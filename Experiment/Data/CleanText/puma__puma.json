{
    "evanphx": "Added in 997c18\n. Part of this has been added by having puma understand the async response pattern to abandon handling of the socket.\n. @rkh So, I looked at what how sinatra used the API and was put off by this: https://github.com/sinatra/sinatra/blob/f2bc346a3c56d61d744fb4419623befc47a7bc65/lib/sinatra/base.rb#L342\nDoesn't that imply that sinatra assumes that if async.callback is set, then EventMachine is being used?\n. None is currently working on it, no.\n\nEvan Phoenix // evan@phx.io\nOn Wednesday, April 10, 2013 at 7:18 AM, Ilya Ostrovskiy wrote:\n\nHi! Has anybody been working on this? If not, I might be able to help out sometime in the future.\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/puma/puma/issues/3#issuecomment-16176763).\n. Mostly because Puma already has support and Sinatra needs to figure out how to manage async.callback when EventMachine isn't used.\n. Done!\n. Applied!\n. Someone needs to release a rails 2 rack adapter.\n. Done!\n. If Merb == Rack, then this is already done! WEE!\n. I rails 1.0 rack handler must be available and likely would require a mutex to run properly. I consider this outside the scope of puma since such a handler should be available as a seperate gem.\n. My point was in probably exists. I'll go ahead and track it down and throw it (and perhaps the rails 2.0 one) in a gem.\n. Seems to work fine for me!\n. I talked to @tenderlove and he indicated that puma didn't need to change anything for this issue.\n. I've applied this and then made it a bit faster. :)\n. That's odd. Could you replace the 2nd line with just \"sleep 10\" and see what happens?\n\nPerhaps\n  a) you're on MRI 1.8 or 1.9 and\n  b) the database adapter you're using uses a C extension (likely) and\n  c) the C extension doesn't release the GVL nor does it use rb_thread_select (likely)\n. Looks like you got it all sorted out! Puma will always work on all ruby implementations, but it being tuned for implementations with full concurrency (Rubinius/JRuby).\nAs @kyledrake so generously offered, I'll always accept pull requests for doc updates to clarify things!\n. What was the request? The parser failed because there was a field that was 80k, which is the expected behavior.\n. @wxmn I need more information about this issue to be able to act on it.\n. What were the HTTP requests? I don't have any ability to replicate the bug right now.\n. The output indicates that puma thinks there is an 80k header value, is that possible in your application?\n. The easiest way is to use dev tools in your browser to look at the request and then paste it to me.\n. @gshilin please open a new issue with the information about how you made the request.\n. I assume you mean add native support for SSL sockets in puma?\n. Gotcha. I should be able to code it up pretty easy.\n. @headius any thoughts on this?\n- Evan // via iPhone\nOn Jan 6, 2012, at 7:56 PM, bporterfieldreply@reply.github.com wrote:\n\nI'm trying to test out concurrent requests w/puma. I can successfully call each of the routes below individually. When I try to call the long one, then the short while the long is executing, one of two things seem to happen:\n1. The short one never returns\n2. The short one returns, then the long one (as I'd expect), but then I get a Java exception.\nCode & exception below:\nRuby (w/RVM):\njruby 1.6.5 (ruby-1.9.2-p136) (2011-10-25 9dcd388) (Java HotSpot(TM) 64-Bit Server VM 1.6.0_29) [darwin-x86_64-java]\nGemfile:\ngem 'rack'\n   gem 'jruby-rack'\n   gem 'puma'\n   gem 'sinatra'\n   gem 'jdbc-mysql'\n   gem 'sequel'\nconfig.ru:\nrequire 'tester'\n   Tester.run!\ntester.rb:\nrequire 'sinatra/base'\n   require 'sequel'\nclass Tester < Sinatra::Base\n     set :server, :puma\n```\n get '/short' do\n   db = Sequel.connect(\"jdbc:mysql://localhost/test?user=root\")  \n   db.run \"select sleep(1)\"\n   \"Hello!\"\nend\nget '/long' do\n   db = Sequel.connect(\"jdbc:mysql://localhost/test?user=root\")\n   db.run \"select sleep(20)\"\n   \"Long Hello!\"\n end\n```\nend\nException:\nException in thread \"RubyThread-5: /Users/bporterfield/.rvm/gems/jruby-1.6.5@2j/gems/puma-0.9.2-java/lib/puma/thread_pool.rb:49\" java.nio.channels.IllegalBlockingModeException\nFull rackup to stacktrace here: http://pastebin.com/PZ22bhx2\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/evanphx/puma/issues/32\n. Stale, closing.\n. Check out https://github.com/evanphx/puma/commit/de7bb0f99702540c1a958ea814d53e56132d383f also, I had to make a few changes to quiet it down.\n. You run \"rake java gem\" to build the java platform gem. Please remove the puma-java.gemspec and use that instead.\n. @headius Any luck on this? Otherwise I'll close the issue since I'm not able to reproduce it.\n. Could you retest with 0.9.5?\n. This problem is extremely odd. There is no reason that Rubinius::AgentRegistry.spawn_thread should be invoked at all. Does rubinius load up fine normally?\n. @mathias Could you provide me with some more details? I'm unable to reproduce this. Does rubinius load, say, into IRB fine? The code you're hitting has nothing to do with puma.\n. It's not always raised actually. You're probably only doing HTTP/1.1 requests, and thus the server sees it because the client said keep-alive, then closed the socket.\n\nIf you do an HTTP/1.0 request (or HTTP/1.1 with Connection: close) then the server finishes the request without hitting any read/write exceptions.\n. @headius Any thoughts on this?\n. Closing since it seems to be stale.\n. This is really a bundler issue. I think for now you need to use the official gem only on jruby.\n. Yep! I'll get one out today just for you!\n. @ehoch Awesome! Could you gist the Procfile you use?\n. @ehoch Just released 0.9.4!\n. Was the first attempt calling puma via \"rails server\"? It looks like it got heavily confused because it's got a port number in there too.\nI did just test it using nginx and it seems to work fine. What nginx configuration do you have it? Does the request show up in puma's log at all?\n. I'm still unable to reproduce this issue. What is in config.ru? \n. What version of nginx are you testing against? Does anything render?\n. Oh interesting! So it was a permission problem. I'll investigate the proper way to set the perms of a unix socket.\n. Is the body in the request or the response?\n. I've been unable to reproduce the issue on either a raw rack app or a rails app. Could you give me some more details about the data that your returning to puma?\n. This is fixed in 2.0.\n. @x3qt Please open a new issue with as much info as you have\n. I mistakenly didn't push the java platform gem. It should be up there now and fix your issue.\n. Fixed in 1.1.1. Not sure why hoe let me release that...\n. I mistakenly didn't push the java platform gem. That has been done and it will fix your issue.\n. Oops! Fixed in 1.1.1!\n. I would prefer that puma not have a detach mode built-in because if someone wants to use in the background, they should really put in under some kind of monitoring (monit, god, bluepill, whatever).\nIn a pinch, the user can simply use & to the shell.\n. I'll add code so that it graceful stops on SIGTERM in which case, yes.\n. Could you launch it with 'ruby -d -S puma' so that the exception location can be seen? I'm unable to replicate this.\n. Set the RAILS_ENV=production env var directly, for instance: RAILS_ENV=production puma -p 3030\n. Do you have an reproduction you could show me? Perhaps a rails app?\n. Any chance you could build a sample rails app that shows the problem? It's quite difficult to derive the situation from just your config files.\n. Puma shouldn't be setting rack.run_once, that's my mistake. I'm finishing up testing of 1.7.0 today and will have it out perhaps shortly and it will include removing setting rack.run_once. I'll also review all the rack options that are set.\n. Yep, I'll have this fixed today and released. Thanks!\n. Fixed by 17219d9ef3d1\n. What version of ActiveRecord are you using?\n. You need to use the ActiveRecord rack middleware ActiveRecord:: ConnectionAdapters::ConnectionManagement so that the connections are closed down properly.\n. No problem!\n\nEvan Phoenix // evan@phx.io\nOn Wednesday, April 11, 2012 at 11:08 AM, Luke Carpenter wrote:\n\nOK, thank you\nSorry, it looks like GitHub didn't get my reply email\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/59#issuecomment-5075018\n. Is there a reason it should? Are you seeing a problem that would be solved by doing so?\n. This causes Rack::Lint to fail and Rack::CommonLogger to blow up, so it doesn't seem proper.\n. It doesn't appear that Rack has fixed the bug even. I think I'll set to \"text/plain\" as the default.\n. You pass -S any_path to puma itself. any_path is just some location on the filesystem that you also pass to pumactl. There is actually no mode to start in the background though, and this is on purpose. If you wish to run this in the background in production, it should be done under a supervisor process like monit, god, bluepill, etc. In development, you can just do puma ... & or use run it in the foreground of a terminal.\n. Bundler currently neuters rubygems so the version outside the bundle won't be found. Because you're executing rackup and not rails s, you have to run it under bundle exec manually.\n\nAll that being said, it's odd that rails s puma didn't work. Maybe rails s isn't loading the Gemfile before looking for the server...\nI'll ask the rails team. None the less, this isn't a puma bug we've figured out.\n. This is an old bug related to puma not removing the unix socket path when it shutdown previously. It is fixed now.\n. What kind of bad crash? I can put some code in to try and detect an unused unix socket, but didn't initially because it has some inherent problems. \n. Well, yeah, if you kill -9 it, no time for cleanup. SIGTERM does a graceful shutdown, which means waiting for all requests to finish. Sounds like I at least need another sig that does a fast (but clean) shutdown. \n- Evan // via iPhone\nOn Apr 29, 2012, at 4:19 PM, Kyle Drakereply@reply.github.com wrote:\n\nFor example if I have to kill -9 the process, it likely won't remove the file. I resolved it for now by running an rm command before I try to start the application. For some reason regular kill is ignored with 1.2.2 on our Ubuntu production box.. very strange. I may be able to get more information about it.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/73#issuecomment-5409750\n. Are you having that problem now? When you do Ctrl-C, is there no other ruby process running on that port, check \"netstat -tln\" to find out.\n. Are you actually binding the main server and the control to the same path? That won't work at all, they must be 2 seperate paths. If you change that, what happens?\n. Which server is using /tmp/.sock? The main one? Try not turning on the control server and see what happens\n. What are the cases where it will still be around?\n\n--Evan Phoenix // evan@fallingsnow.net\nOn Friday, November 16, 2012 at 5:33 AM, Jan wrote:\n\nI actually still encounter quite a few cases where the socket file will still be around, thus puma can't be started without removing it by hand first.\nI checked how unicorn handles this. And it seems they check if the socket file is actually still in use, if not it doesn't matter if it exists.\nI think this is how puma should handle this as well...\nWould that be possible?\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/puma/puma/issues/73#issuecomment-10446900).\n. Oh interesting. I'll check out why JRuby isn't shutting down cleanly.\n. This should be fixed in 2.3.0. Puma now cleans up stale unix socket files.\n. Please open a new issue with your details and backtrace if you still see this.\n. You should instead use something like bluepill which can monitor puma rather than having puma daemonize itself with no parent.\n. I've reversed my position and have added a daemonize option in 2.0. For the very reason you talk about, doing a cap deploy without the ability to daemonize puma is a pain.\n. I'd be happy to do a capistrano recipe too (or would merge a pull request for one!).\n. Why should the pid file be deleted? I've always felt like the pidfile is owned outside of puma, since puma itself doesn't use the pidfile for anything.\n. You can do the same, just run puma instead of thin on the same ports.\n. I don't want a demonizing mode in puma because if you're going to be running it in the background in production, it should be run under a supervisor like bluepill, etc.\n\nIf you want to run in the background in dev mode, just use puma &.\n. Puma requires the client provide a valid Content-Length header in order to provide a body. In that case, there is no ability to stream an unknown number of bytes into puma. For very large files, there could be an option to limit the body size. Most people would do that via nginx and such.\n. This can be implemented now by grabbing the socket from env['puma.socket'] and returning [-1, {}, []] to puma. This allows you to assume full control of the socket and puma will disavow using it. You could then pass the socket to another thread to stream whatever you want to it (perhaps running another event loop in that thread).\n. I'd rather have a websocket rack app that used the new rack hijack or puma.socket to be implemented. It should live outside normal puma so it can be released separately.\n. rake -T details the compile task which does what you want. DONE! :heart:\n. Um. Uh. Are you running it out of a proper puma clone? What version of hoe do you have installed?\n. Are you sure? Those messages make no sense as README.md is right there.\n. What is the output of ls in your clone directory?\n. Can you pop into IRC? I wanna figure this out, something is really weird.\n. Handled via PR!\n. Seems to fail on Rubinius. I don't like the StringIO dep either. Why not just have count be optional and return \"\" if there is no count?\n. Yeah, appears to be a rubinius bug. Anyway, I'd prefer to not use it since it reduces the simplicity of NullIO and it hurts the perf (the whole point of NullIO in the first place).\n. There is a stdout_redirect option in the config you can use. Puma itself has no logs because those come from the application itself.\n. This was added.\n. @ender672 Could you double check that you don't see the hang anymore with my latest change?\n. Sure, at least it can be clarified by the various rubies then.\n. The reason for this is that you've restricted the thread pool to 1 and only 1 thread. Chrome is multiple concurrent requests for /js1 and /js2 and keeping them both alive. Because there can only be 1 thread, puma has to time out the first request before it can move on to the second.\nThis isn't a bug because it's just an effect of restricting the pool size. I'll see if I can't think of a way to avoid this though.\n. @headius Any thoughts on this?\n. Sounds like it was a rack issue.\n. I'm opposed to puma having the ability to daemonize itself because if you want to run it in the background, you should use a monitoring process (god, foreman,etc).\n. @headius Any thoughts? Is Puma doing something wrong wrt Unix sockets for jruby?\n. Fixed with the PR!\n. You can use an ssl socket by using the ssl:// protocol when specifying the server. Check the Readme.\n. Do you have a way that we can reproduce the issue?\n. This is now stale. Please reopen if you have a repro.\n. Merged and committed manually!\n. Add it as a .md file in the docs directory instead and I'll merge that.\n. Yeah, just create one and put the file in there.\n. This was added as nginx.md in docs/\n. Currently, puma does not wait for the new process to start before switching to it. The reason for that is that because puma only uses one process, the pid would end up changing when a hot restart was initiated if the new process was spun up, checked, and then the original one exited.\nI'm currently starting work on a some multiprocess work for puma (a new mode) and I'll keep your use case in mind.\n. 1.6 is under testing, thats why there is no gem for it yet :D\nA bunch of fixes went in yesterday, when did you try it?\n. I just tested against git master and I don't seem to see what you're seeing. I tried the setup you said and it seemed to work fine. Could you give me more details of what you're seeing? Do you see any console output?\n. This has been fixed by working around a buffering behavior in JRuby's OpenSSL.\n. That's very surprising. I need to know exactly what your config.ru is so that I can reproduce what you're seeing exactly.\n. So, I'm getting back around to this. I just pushed a jruby-ssl-bug branch that shows that it seems that OpenSSL in JRuby has a fatal bug. IO.select says there is data but doing a sysread_nonblock always raises IO::WaitReadable, thus going into a loop. @headius could you weigh in here? It seems like a bug I can't work around.\n. I've started an experiment today of not using the stock OpenSSL and instead writing MiniSSL that has just enough of what I need and none of the bugs. Hopefully I'll have it done this week. \n- Evan // via iPhone\nOn Aug 22, 2012, at 5:27 PM, Ben Porterfield notifications@github.com wrote:\n\nThanks for looking into this again! I'll take a look tonight and see if one of the way's I've monkey-patched OpenSSL for my project will make this work for me in it's current state\n\u2014\nReply to this email directly or view it on GitHub.\n. The problem appears to be bugs in how JRuby's jruby-ossl gem in 1.6 handles nonblocking. I guess there is a bunch of fixes in 1.7 though, so you could try that instead.\n\nI'm going to have to implement the MiniSSL engine (which is coming along) to work around these bugs to work on JRuby 1.6 I believe.\n. Yes, the handshake is happening inside the listen loop, and yes we should move it into the initial worker thread. Doing that with the current OpenSSL API is more difficult because parts of that are hidden, but doing it with MiniSSL should be trivial.\n. Turns out that there are just too many bugs in JRuby's OpenSSL right now. I've disabled SSL support on JRuby for now as a result. We'll reevaluate in the future.\n. I'm currently debugging this and plan to make some adjustments so it works out of the box. Should be out soon. \n. Can you show me the errors you're seeing?\n. I'll get these documented soon, I promise!\n. PR merged! We're getting there!\n. Thanks! I've pulled this in and changed a couple things to conditionalize it properly.\n. Oops! I guess I shouldn't have made the release so late last night. All fixed now in 1.6.1.\n. The signals are currently only supported when using the 'puma' command rather than loading via rails's rackup invocation. This is because puma doesn't control the process from the top down, and thus it's unreliable to perform operations like restart.\n. You can expose the Sinatra app as a rack app too in the config.ru, thats the easiest.\n. Do you have a way that I can reproduce this issue?\n. Can you run it as 'ruby -d -S rails s puma -d' and gist the output? That will help. \n- Evan // via iPhone\nOn Aug 25, 2012, at 4:13 PM, Bruno Wagner Gon\u00e7alves notifications@github.com wrote:\n\nI am having this problem too with version 1.6.1 and both rails 3.2.8 and 3.2.3, starting the server with \"rails s puma -d\", if you reload the page several times (hold f5), rails will stop receiving requests (can be seen on the log), I don't use any special rack file and have no weird configurations on my application...\nAnything more you need to know I'll gladly answer\n\u2014\nReply to this email directly or view it on GitHub.\n. Sorry. I meant replicate the issue then gist. \n- Evan // via iPhone\n\nOn Aug 25, 2012, at 5:26 PM, Bruno Wagner Gon\u00e7alves notifications@github.com wrote:\n\nhttps://gist.github.com/3472763\n\u2014\nReply to this email directly or view it on GitHub.\n. Ok. I have a hunch and just need to confirm it. \n- Evan // via iPhone\n\nOn Aug 25, 2012, at 7:17 PM, Bruno Wagner Gon\u00e7alves notifications@github.com wrote:\n\nsorry, I should've noticed the purpose too, I will do it as soon as I can, probably tomorrow morning... anyway it wasn't happening before on versions 1.5 and 1.4...\n\u2014\nReply to this email directly or view it on GitHub.\n. I've just released 1.6.2 that should fix your problem. Could you confirm it does?\n. Wonderful! Thank you for the testing!\n. Sounds like a good idea! Wanna send a pull request?\n. Signals are not supported on Windows (which we want to support) and I want to puma to be able to be controlled remotely (thusly the control tokens for access control).\n. Your change of only using signals means that pumactl no longer works on Windows and can't be used over the network, both of important features.\n. My only gripe is that you've put almost 100% of the logic into one method now, #run. Please break that up to keep it clean.\n. @jpascal If you add optionally using signals, I'll be happy to merge this! I meant to merge it already but I was on vacation.\n. Ok, I've merged this in. I had to rework it because it didn't apply anymore and the style wasn't where I wanted it to be. But it's in now! Thanks!\n. Could I get a little info about the app? I'll need to reproduce the problem. If you can at least tell me what requests are made from the page that cause the problem that should help. Also, which browser did you use?\n. I don't suppose you'd let me run your rails app? :smile_cat:\n\nI haven't yet been able to reproduce this and I've been through the request flow a few times and haven't been able to identify a place where it might happen.\nIf you can let me run your rails app, I can promise to do it in private if you don't wish it to be publicly available.\n. I found the bug. Will have a new release out today. \n- Evan // via iPhone\nOn Nov 16, 2012, at 1:57 PM, Jan notifications@github.com wrote:\n\nSorry about the number of posts, but i think i found a solution to the bug.\nnginx uses HTTP 1.0 by default when proxying requests.\nWhen i configure nginx to use HTTP 1.1 as suggested in the keepalive section in the docs, the weird behavior of puma is gone, see http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive\nSo one needs to add the following to the nginx config:\nlocation @app {\n   ...\n   proxy_http_version 1.1;\n   proxy_set_header Connection \"\";\n   ...\n}\nI have no idea why this works yet and why it doesn't without those settings.\nBut i assume puma expects HTTP 1.1 and there is some trouble when nginx talks 1.0...\nHopefully this will give people who know what they're doing a hint at how to fix this in puma to properly handle HTTP 1.0.\nMeanwhile i'm going to do some reading about the topic ;-)\n\u2014\nReply to this email directly or view it on GitHub.\n. Fail how?\n. Ok, but I need to write a test case for this. Could you give me more info of the failure you see and the code that causes it?\n. It seems like your StreamingExcelDumper is not actually emitting a chunked response. The code you mentioned controls whether or not puma should perform it's own chunking. Since you're doing, it must not, but it sounds like you're code is actually NOT chunking. I've gone through the code paths and I can't see another way this could happen.\n\nIf you can give me a way to reproduce this, I can dig into it more.\n. ActiveRecord has a special middleware that returns a connection back to the thread pool when a request finishes. Perhaps you need to add something like that? I can't see a way that puma would cause this issue, since it just exposes to DataMapper that it's running in multiple threads.\n. Could you try out the 2.0.0.b1 release and see if you see the same behavior?\n. The minor slow request (2.6s) is due to rails loading, etc. There is no getting around that. The other issue was a bug that I fixed where there was a race condition where there could be an outstanding request left inside the thread pool but no workers.\n. 2.0 is going to have some canned capistrano recipes to use, that should help solve your problem. \nIn 1.6.3 it's tricky because there is no daemonization support.\n. So, log wise. I've just pushed a change such that the default logging is only enabled in development mode (which is rational). In that case, the rack app will need to provide it's own logging (and thus configure where to send the logs).\n. Puma does not include a request timeout mechanism, thus no way to change it. :D\n. There is no default timeout because Puma will never terminate a request based on time.\n. Does the server output anything? What if you don't run it in daemon mode?\n. Does this still happen with 2.3.0?\n. This was caused by starting the control server before daemonizing. This has been fixed in 2.3.2.\n. Hm, perhaps TCPSocket.for_fd doesn't work properly on windows...\n. So, it appears that to make the socket descriptors inherit properly into the child process by win32 you have to use DuplicateHandle which it appears MRI doesn't do. I have an idea for a fix, but I'll have to setup a win32 VM to test on. Could you tell me about the setup you're using?\n. That's something that rails has to do, not puma. You can use rails server -s Puma I believe. @tenderlove help!\n. Yeah, it should error out.\n. Great!\n. Dupe!\n. That's a bug. It needs to check the return value and call syswrite again with more of the output. #syswrite goes straight to write(2) so it's limited by the OS. This is a simply oversight.\n. I'll fix it shortly!\n. My guess is that it's threading related and some code you're using isn't thread safe (sharing IO objects across requests, etc).\nWhat is the backtrace for the 2nd error? That's quite odd.\n. Do these happen when the app is first booted or after a restart? Does the \"Missing rack.input\" one always happen or is it random?\n. Interesting! Ok, makes me wonder if these various components below puma aren't thread-safe, especially since you're using JRuby where the threads are truly running in parallel.\n. Ah ha! Concurrent $stdout values being a problem, who would have thought?! :smile_cat: \n. I just merged some other cap PRs that included this language, so I'm going to ahead and close this. If you're not happy with the new language, please submit another PR.\n. Can you post your config/puma.rb? Also, when you see a high CPU load, figure out which process is causing it. The workers have a different name (and likely the higher pid numbers). I need to know if it's the master or a worker (or multiple workers).\n. What version of ruby are you using?\n. Seems #220 has solved this. I didn't realize ruby 2.0 changed this behavior.\n. The puma issue is long fixed. That's something wrong with your OS probably.\nOn Sat, Jul 30, 2016 at 6:51 AM Muhammet notifications@github.com wrote:\n\nis it fixed ?\nI'm getting following error\nbundle/ruby/2.2.0/gems/activesupport-4.2.5/lib/active_support/core_ext/kernel/agnostics.rb:7:in ``': Bad file descriptor (Errno::EBADF)\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/177#issuecomment-236366321, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB-O9zYNlVP7f73-_DsrdkyLLpCgYks5qa1bOgaJpZM4AVIXO\n.\n. I'll accept a PR for this.\n. What version of ruby? I'm not able to diagnose the issue at preset.\n. Is this still happening? I was never able to replicate the issue.\n. @betelgeuse What was the request you sent?\n. It was not a coincidence. Rugged loads an extension which contains the symbol http_parser_init. This is the same symbol name that puma's parser uses. And because dynamic linking is a pain in the ass, when puma invoked http_parser_init, it didn't invoke it's own function by that name but the one that was loaded from rugged. Those 2 functions obviously don't do the same thing and thus the struct was corrupt for puma and it resulted in this obscure error.\n\nMorale of the story: beware dynamic linking.\n. Do the workers start up? What config did you use to test it?\n. Use puma from git in your Gemfile, then you can try it out.\n. WOOO!\n. @concept47 please open a new ticket.\n. @puzanov What configuration are you running puma in? Are you using workers and preload?\n. The problem is, mostly, that Rufus scheduler starts up threads at booted and they don't carry over properly.\n. Hm, I'm not able to reproduce this. The normal kill sends a SIGTERM which causes it to do a graceful shutdown, which means waiting for requests to finish. Perhaps it was just waiting for things to settle down. Using HUP falls back to the OSs default HUP handler since puma doesn't install one currently.\n. I'm going to go ahead and close this. If you still have it, please repoen.\n. Ah yes. I realized this just as I was going to bed last night. I need to add an option to change the directory inside the worker before loading the app. I've got one other feature to add, so I'll go ahead and fix this and release b6 today.\n. It doesn't timeout because I don't think that makes any sense. If your workers can't be started, then puma needs to continue to try and load the workers forever because if it stops trying, you'll be left with the wrong number of workers.\n. What output do you see?\n. What is the output of puma? This works here so I'll need to figure out how to repro your issue.\n. Can you try with not daemonized? There should be output then.\n. If you are seeing this, please open a new issue with errors reported and repro steps.\n. This is fixed in 2.0.\n. Fixed!\n. Can you break into a stuck worker with gdb and run \"bt\"? Perhaps I need to ungracefully kill workers that don't stop after a timeout...\n. Yes, I need a way to reproduce this issue. Can you provide a way to do that?\n. This could very well have been the Process.daemon issue. Let's close and reopen if someone has the issue still.\n. This is almost certainly a thread safety issue with rails, not puma.\n. Fixed in master.\n. What was the query? I need a way to reproduce this to fix it.\n. Stale.\n. Do you have any way for me to reproduce this issue?\n. I believe this is a rails issue. I'll try to reproduce it to help get rails fixed.\n. This is a bug in rails, fixed in PR https://github.com/rails/rails/pull/9789\n. Merged! Thanks!\n. This is a mistake. 2.0.0b7 will include the proper java version. Sorry about that.\n. I pushed a java version of b7.\n. Puma does support a hot restart on jruby by using fork and exec via FFI. I'll update the wiki.\n. Wait, which wiki?\n. Puma doesn't use fork, only exec, so it should be safe.\n. I just fixed a bug that likely prevents it from working in 1.6.3, I'm about to release 2.0.0.b7 with the fix, so i'd try that in about an hour :)\n. Hi @elucid, Could you tell me more about what isn't working for you? Does it restart but no traffic can pass then?\n. Could you gist what the crash looks like? Backtrace, etc.\n. @mrbrdo What is the \"redirect the socket to the new instance\" operation though? The typical way that is done is by sharing a file descriptor between parent/child processes, but thats not available inside JRuby. Could you tell me more about what you're thinking?\n. @asok \"hot restart\" and \"phased restart\" are not the same. \"phased restart\" refers to a feature of cluster mode, when preload_app is off, to be able to kill off and reload workers one at a time, allowing them to load in new code. This feature is fine but hampered by bundler and deployment directory issues.\n\"hot restart\" refers a feature of puma on MRI/Rubinius to be able to hold re-exec puma, load the new app (in perhaps a new directory as well), and have the new process inherit the socket fds from the parent. This means there are no dropped requests while transitioning to the new process.\n. @jeremyhaile I clarified in the issue that puma does support restart on JRuby\n. @jeremyhaile Puma right now supports \"hot restart\" on JRuby but it is at the whim your JVM to support the ability to pass file descriptors between processes properly. I'm unable to test every JVM but it does work in the common cases I've tested.\n. I'd prefer to not drag in kgio nor nio4r. I'll want to address this more later, but your PR solves the majority of the issue.\n. This patch will only run on 1.9+ but puma supports 1.8 as well. We'll need to conditionalize the code.\n. This is already on master, not sure why you didn't see it.\n. Oops! Forgot to push! You should see it now.\n. Please don't hop on issues, open another one. I'm closing this one because SSL isn't suppport on JRuby right now.\n. I'd rather get MiniSSL going on JRuby rather than put the old SSL stuff back in.\n. Because the specific issue detailed was resolved.\n. This is fixed by having 2.0.0 released normally.\n. It's a little weird that the unix inheritor gets passed a TCPServer, but I'll go with it!\n. Yeah, it was a bug where it wouldn't be honored properly.\n. Daemonize is only available in the 2.0 betas.\n. This in now resolved. There are proper java platform versions of the puma gem.\n. Don't specify puma from git. Use the released gem, then everything will be fine.\n. Yeah, thats a bundler bug.\nOn Thu, Aug 7, 2014 at 11:06 AM, David Celis notifications@github.com\nwrote:\n\n@evanphx https://github.com/evanphx Sorry, but is there a reason that\nbundling from git doesn't respect Java vs. C extensions? That seems like a\nbug (albeit probably with bundler)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/238#issuecomment-51509178.\n. Seems like this is that the body wasn't read properly. What is the body of the request?\n. It should be able to read it in fine. I'm not able to replicate the issue here strangely though. What version of puma didi you most recently try with?\n. Daemonize isn't support on JRuby currently. I need to error it out nicely.\n. Fixed!\n. Are you always seeing it from pumactl?\n. puma.gemspec is created by Hoe via a rake task. We need to get Hoe to generate it shorter, since otherwise I'll overwrite your changes next time.\n. Yeah, that's fine to do.\n. Do you have an example I can run to repro it? I wasn't able to yet.\n. Yeah, please give me the haproxy config and I'll take a look.\n. I'm not able to replicate this. What happens when you remove the -d? Does it startup fine?\n. @sabcio Could you open a new issue? This one it sounds like is resolved in the next version by reporting application loading errors before daemonizing.\n. Looks good! Here is some more:\n\nThe thread configuration is per worker, so -w 2 -t 16:16 will be 32 threads.\n. Puma never loads the rails env in the master process so that it is immune to application causes crashes.\non_worker_boot is allow you to do some puma-specific things that you don't want to embed in your application. For instance, you could fire a log notification that a worker booted or send something to statsd.\n. Looks good!\n. This has been fixed. Some bad data was getting into the state yaml. The other point about control server not being active when in cluster mode is true and being fixed.\n. If 0 bytes are returned, then the socket has no data, in which case EAGAIN will be raised and we'll select on the socket, waiting for data.\n. @bnorton Did both workers restart before you tried to access the new route? If you tried it again right away, you the worker that had not yet restarted may have accessed the route.\n. @marcelow Did puma report anything when you issued USR1?\n. Wonderful!\n. Is this still an issue? I haven't been able to replicate it.\n. Crash how? Does changing the thread count change the crash?\n. The init script is provided only as an example.\n. In production, puma doesn't handle any log files, they're all done via whatever rack app you're using. Unicorn has some code that I object to that looks through all IO objects and reopens them but puma won't do that.\n. This is because the worker loads your rails app, dies, and so the master starts a new worker, which dies, repeat. There isn't much puma can do about this since the assumption is that you really do want workers restarted if they fail.\n. This is not a concern that I want puma to have. If you want to have timeout requests, the best way would be to write a Rack middleware that handles it the way you want. The reason I don't want it to be in puma because, imho, it's not a one-size-fits-all situation. Puma is used for long polling as well as short requests, so the problem is better solved an the Rack middleware layer.\n. I need to properly document this situation. JRuby does not have a working TCPServer.for_fd or UNIXserver.for_fd, so there is no way to do seamless restarts.\n. The reason for the little engagement is that it was a limitation that has always existed but it was never properly documented. I'm working on the documentation for it, as well as discussing with @headius if there is a way to make it work.\n. I see how this is probably happening. Does this perhaps happen when you send SIGUSR2 twice in quick succession?\n. @tkoenig What version were you on?\n. @mrbrdo Does this still happen with 2.2.2?\n. If anyone is still seeing this, please open a new issue with backtrace and reproduction information.\n. This is fixed in 2.3.0. The control server now works in cluster mode.\n. What is an H12 error?\n. This has been resolved by adding the preload_app! option.\n. Puma itself doesn't have anything to serve out directories, so that would be something inside Rails. Does this work with other webservers, say, webrick?\n. I won't merge this. This breaks cluster mode because it means that workers won't pick up the new app (used in the phased restart feature).\nThis is basically the unicorn equiv of preload_app. If you add a configuration variable around doing this, I will merge it though. It will have to disable certain other features because they simply won't play well with using preload.\n. Yeah, a configuration variable is the way to go.\n. This is now available via preload_app! (95e469c71e1c489d42f46f8215b5df8742da8204)\n. This will be fixed in the next release (this week!)\n. I have no clue about this issue. When you stop puma, does the process go away entirely?\n. This bubbled all the way into pumas very primitive error handler. If you want to handle exceptions differently, you need to have a middleware that will do so. Puma always considers an exception that hits it a 500.\n. @jrochkind Your analysis is correct. Cluster mode boots the app in each worker so you don't need to worry about worry about reconnecting AR and such.\n@timuckun You want to have one puma manage many different rack apps?\n. That would be something outside the current scope of puma, but it sounds interesting!\n. Fixed in 2.3.0.\n. I need details about the crash. Is there a backtrace? Does the process segfault?\n. Are you using preload? Can you turn that off and try again? I'm thinking that perhaps WickedPDF is inheriting data across processes and crashing itself.\n. Does it happen under MRI too? I'm wondering if perhaps it's something weird\nwicked_pdf does that Rubinius has issues with...\nOn Fri, Dec 27, 2013 at 7:42 AM, Sal Scotto notifications@github.comwrote:\n\nThis happens with or without preload_app. a very easy example is do a\nsimple ls when in clustered mode and it will kill the worker and under\nrubinius it loops restart forever\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/294#issuecomment-31262590\n.\n. This is a thread bug in wicked_pdf, nothing puma can do about that.\n. Rails should be setting that on it's own. Puma sets RACK_ENV but it doesn't have any knowledge of rails so it probably shouldn't set RAILS_ENV.\n. Yes, this is correct. Because the master process is booted within a Gemfile, there is no safe way to transition to a new Gemfile using phased restart. There probably needs to be a separate restart that doesn't use phased that can be used when a Gemfile is updated.\n. You need to use the prune_bundler feature in order to get the Gemfile to refresh properly.\n. That's odd. hm. I wouldn't have expected a different version of psych to be loaded in that scenario... Can you post your Gemfile?\n. Did you try the prune_bundler config option? That is required to reload a Gemfile. \n. Ok, what is your point? Why would reopening this change anything?\n. It's documented here: https://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting\n\nI'm happy to add other docs, what/where should they be?\n. Yeah, they should probably call disconnect first in that case.\n. Probably it should be another hook like unicorn.\n. I'd like to call out this issue as a wonder one. The failing test case made it so easy for me to figure out what was wrong and correct the issue in only about 5 minutes.\n:rocket: :+1: :octocat: \n. Which socket? It's possible that puma is waiting for the requests to finish before stopping.\n. Wait, you're saying that pumactl is removing the application socket file? I've not seen that and pumactl certainly has no code to do that.\n. This is looking to be a bug in MRI. I'm going to release 2.3.2 shortly that should fix it.\n. @nengxu What does pumactl stop output? Does it say the command was successful? What is your puma configuration? Are you using workers? What MRI version? I need a complete picture of your setup because I haven't been able to replicate the issue thus far.\n. @nengxu does this still happen when you don't use -d?\n. Adding workers 1 is a random workaround, it's not solving the real problem.\nI think I've got a lead on this. Are the apps that have having this problem using very long-lived requests?\n. There are many different issues people have presented. I'm closing this one down.\nIf you have issues with the current puma release, please open a new issue.\n. Fixed in 2.3.1, I'm a big idiot. Sorry everyone!\n. Fixed in 2.3.1. I'm a big idiot. Sorry about that.\n. REMOTE_ADDR needs to represent the direct client. I guess for a unix client I could put 127.0.0.1 in there...\n. @mrbrdo That commit is about a month old, could you try 2.3.1?\nI'm not able to replicate this issue with a simple test case. What else is in your config file?\n. @mrbrdo Before were you using workers? The control server wasn't turn on in the worker mode before 2.3, so try that.\n@pawel2105 How are you starting puma?\n. Your config doesn't specify the path to the control file, that's why it doesn't work. The cap tasks expect that you're going to do that on our own and set it up to the path it expects.\n. Puma doesn't have anything internal for serving files, so that would be coming from rails doing that. You'll have to check out the rails configuration. I know rails doesn't serve files in production mode, so you might double check that it's starting in dev mode.\n. I can't merge this because it uses 1.9 only syntax and puma works on 1.8.\n. Added in #346 \n. Yes, it does. We'll add that (or if you'd like, I'd accept a Pull Request!)\n. In puma cluster mode, each worker will it's own pool configured as though it were just a regular threaded server. Using heroku's unicorn example under puma is fine.\nPuma is always multi-thread, cluster mode gives you the ability to have multi-process, multi-thread. You could configure puma's thread pooling to have just 1 thread if you'd like, but there is no reason to do that.\nYou can't share the pool amongst processes (nor would you want to) but you can and should share it amongst threads.\nIf you don't use cluster mode, there is no need for CoW consideration at all because everything is in the one process. If you use cluster mode, you can use the preload_app! option to load the application in the master and thusly have it's memory shared, CoW wise, into the workers.\n. This uses 1.9 only syntax, I can't merge it because puma runs on 1.8\n. I opt'd to merge the other PR for adding phased restart to cap. Thanks for writing this up though!\n. This is now fixed using the prune_bundler configuration option.\n. This is fixed in the latest puma.\n. Yeah, logging on hijack is super weird. The issue is that the middleware stack is what actually does the logging so there isn't an easy way for puma to inject itself genericly. I'm going to make a change to the Rack::CommonLogger patch we use to detect hijacking and do something better, but that won't work in all cases because a different logger might be in the middleware stack.\n. Puma don't write to any log files without being told to, so thats not it writing to production.log.\n. Not sure what \"auto repair\" would do. Did the error happen due the app?\nI could detect that workers are flailing and back off restarting them to try to slow down the flapping.\n. You can probably use those some rack middlewares to monitor a process from within itself. The master process should probably monitor the workers better to detect hung workers and destroy them.\n. I guess I need to dig into what changes newrelic made...\n. Rails is what serves up the files, but it seems like maybe rails is having some problem.\nI need a way to reproduce this issue though, I haven't been able to in the past. Can one of you help out there? Once that is done, it's easy and quick to figure out the fix.\n. @joefiorini  It's most likely that if you're generating assets the first time their served that there is a threading issue causing the problem.\n@sairam Is this still happening in rails 4.0? Perhaps there is a threading issue there. Are you using this asset_compiler middleware too?\n@jloper3 Please open a new issue with the details of the problem you're seeing.\n. README changed to move that text out.\n. @luislavena FTW! :thumbsup: \n. I've attempted to reproduce this with a raw, rack app to isolate it to just puma and I haven't been able to. That points to whatever y'all are running under puma.\nThe best way to start to hone in on the issue is for someone that is having the issue to add this code to their config.ru file:\nThread.new do\n    p ObjectSpace.count_objects\n    sleep 60\nend\nThe standard out of the process now get stats about the heap every 60 seconds. If you can now cause the memory to grow unbounded, do so and then gather up all the stats into a gist and put it here. Then we'll have a better idea of what is going on.\nThanks!\n. Those strings are very likely created by the call to .count_objects itself.\nOn Sun, Jan 12, 2014 at 8:10 PM, Alex Ghiculescu\nnotifications@github.comwrote:\n\nI'm seeing similar results, except I'm getting 40 new strings every 15\nseconds. Here's a quick sample:\nhttps://gist.github.com/ghiculescu/56dc083def5792c4e5d4\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/342#issuecomment-32144510\n.\n. @JamesHarker That is very likely an issue with your app, not puma. Because puma uses threads and expects your app to live a long time, it sounds like you're accumulating objects somewhere.\n\nThis is something that people who have been using unicorn before didn't notice because unicorn was killing off workers and restarting them quite often, never allowing the app's memory to grow much.\n. Which puma version? The memory issue could easily be in ruby, given the 2.1 line has had these kinds of issues. Could you try with ruby 2.0?\n. @tozz I'm going to look into this today. This is totally unexpected and quite strange. How quickly do you see memory increasing?\n. I'm sorry if I came across as flippant, that was not my intention. This issue has haunted me and I haven't been able to make progress on it. For instance, the dead simple repro above using an empty rack app seems like it would be simple enough. Yet when I use all the object memory profiling tools, I see no change, the object load is constant and stable across an infinite number of requests. \nDespite that, I do see the memory increase from the OS. leaks doesn't report any obvious memory leaks (it does find a few in MRI itself but only a few hundred bytes).\n@tozz in your test, do you see the number of objects increasing? Or just the RSS as reported by the OS? I'm going to dig in further and trace the actual mmap calls that are occurring and see where ruby is doing them.\n. @davidelbe How quickly does the memory increase?\n. @tozz Can you please run a test with 100000 requests, 3 times, recording and watching the RSS along the way. The behavior I see here is that yes, it goes up, but it tends to bounce around and stabilize because of how MRI's GC works. I don't see a consistent increase in memory per requests.\n. @tozz There isn't much in your data to suggest an \"object leak\" basically a leak that is created by retaining too many objects. These are the most common memory leaks in ruby, but from the stats you've got there, everything looks to be fine.\nBut because of the nature of MRI, that doesn't give us an accurate picture of the memory usage because String and Array store their actual data in the normal C heap, and thus a giant String and a tiny String look the same in stats.\n@tozz Can you run it again and track ObjectSpace.memsize_of_all over time? Maybe we'll see an increase there that will tell us if we have a strange ballooning Array or String that I'm not seeing.\n. So, I'm starting to wonder if MRI doesn't have a memory leak in it's Thread handling code. The follow script blows up, memory wise, like a fucking sieve.\nruby\nloop do\n  GC.start\n  threads = (0..16).to_a.map { Thread.new { 1 + 1 } }\n  threads.join { |t| t.join }\nend\n. Nevermind. My morning coffee didn't kick in and thusly I didn't notice I had done threads.join instead of threads.map so I had a huge number of runnable threads.\n. Here is some more data from the test I'm running now.\nCommand: puma -q ~/empty.ru\nempty.ru:\n``` ruby\nrequire 'objspace'\nclass App\ndef initialize\n    @i = 0\n    @start = nil\n  end\ndef show_diff\n    @i += 1\n    return if @i < 5000\n    @i = 0\nGC.start\n\n@start ||= ObjectSpace.count_objects\n\ncur = ObjectSpace.count_objects\n\nputs \"============ #{ObjectSpace.memsize_of_all}\"\ncur.each do |kind,cnt|\n  prev = @start[kind]\n  if prev != cnt\n    puts \"#{kind}: #{cnt} => #{prev} (#{cnt-prev})\"\n  end\nend\n\nend\ndef call(env)\n    show_diff\n    [200, {}, []]\n  end\nend\nrun App.new\n```\nab output:\n```\nvagrant@trusty64:~/git/puma$ ab -n 10000000 http://0.0.0.0:9292/\nThis is ApacheBench, Version 2.3 <$Revision: 1528965 $>\nCopyright 1996 Adam Twiss, Zeus Technology Ltd, http://www.zeustech.net/\nLicensed to The Apache Software Foundation, http://www.apache.org/\nBenchmarking 0.0.0.0 (be patient)\nCompleted 1000000 requests\nCompleted 2000000 requests\nCompleted 3000000 requests\nCompleted 4000000 requests\nCompleted 5000000 requests\nCompleted 6000000 requests\nCompleted 7000000 requests\nCompleted 8000000 requests\n```\ntop:\n```\ntop - 18:42:55 up 14 days, 15:42,  3 users,  load average: 1.89, 1.81, 1.75\nTasks:  83 total,   2 running,  81 sleeping,   0 stopped,   0 zombie\n%Cpu(s): 48.2 us, 39.8 sy,  0.0 ni,  0.0 id,  0.0 wa, 12.0 hi,  0.0 si,  0.0 st\nKiB Mem:   1017912 total,   799364 used,   218548 free,    59276 buffers\nKiB Swap:   524284 total,      648 used,   523636 free.   261604 cached Mem\nPID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND                                                           \n 9909 vagrant   20   0  341636 271816   1732 R 28.6 26.7  11:21.08 ab                                                                \n 9489 vagrant   20   0  633552  29156   4356 S 66.2  2.9  30:05.56 ruby                                                              \n 1128 root      20   0 40.625g  21516   4688 S  0.3  2.1  75:16.33 consul                                                            \n  631 root      20   0  474692   9152   1388 S  0.0  0.9   0:46.60 docker\n 1152 root      20   0  133316   6152   1252 S  0.0  0.6   0:11.99 log\n10222 vagrant   20   0   23508   4816   1868 S  0.0  0.5   0:00.29 bash\n10612 vagrant   20   0   23484   4784   1864 S  0.0  0.5   0:00.07 bash\n10571 vagrant   20   0   23464   4740   1844 S  0.0  0.5   0:00.08 bash\n31009 vagrant   20   0   23456   4692   1864 S  0.0  0.5   0:00.11 bash\n30990 root      20   0  105628   4120   3228 S  0.0  0.4   0:00.03 sshd\n  957 root      20   0   10224   2916    608 S  0.0  0.3   0:00.03 dhclient\n    1 root      20   0   33592   2692   1264 S  0.0  0.3   0:01.18 init\n 1694 root      20   0   61364   2688   2008 S  0.0  0.3   0:00.00 sshd\n31008 vagrant   20   0  105628   2208   1304 S  0.0  0.2   0:11.00 sshd\n10221 vagrant   20   0   27752   2160    952 S  0.0  0.2   0:08.03 screen\n  500 syslog    20   0  255840   1980    772 S  0.0  0.2   0:00.15 rsyslogd\n  495 root      20   0   43448   1792   1400 S  0.0  0.2   0:00.08 systemd-logind\n```\nThe RES for ruby has been stable around that value.\n. What my current test suggests is that ab doesn't track the data for the results very efficiently. :smile:\n. @tozz Which debian release are you running your tests on? Maybe some library differences will expose the issue?\n. Hi everyone, if you wish to \"me too\" this issue, please provide some concrete details about what you're seeing. How much the memory increases over what period of time, if you can easily reproduce the issue, etc.\n. Quick update: I'm not able to get a way to reproduce the issue. I've running lengthy stress tests and they show a stable RSS value. If someone can give me access to a system that can reproduce this, I'd love to look at that.\n. I looked at OSX 10.10.2 using rbenv and Ubuntu 14.04 using rbenv. I tried both ruby 2.1 and 2.2. When you reproduce the problem, can you provide a script or exact steps you used and then both what and how you measured the memory? I'm sure there is something to this, we just have to get a reliable way to trigger it. \n. @tozz I need to know the exact steps you took to reproduce this. You used that trivial app, but what did you use to induce load, what did you use to measure the RSS, how many requests did you do, how long, etc.\nI need to know the exact setup to reproduce it.\n. I'm closing this issue as most participants have demonstrated that the issue is in various versions of ruby, not puma itself.\n. @nengxu I understand your point and I do sympathize. But it's never been shown, despite tons of testing, that Puma is the issue here. I'm not sure what puma can do to compensate for certain ruby versions ballooning their memory.\nI don't mean to downplay the issue, it's certainly a problem. I'm just at a loss on what to actually do, puma wise.\n. Maybe @ko1 could take a look at this thread and give us any help?\n. Or @nobu?\n. @ko1 No, that program doesn't represent what users are seeing. Puma caps the number of threads used, it doesn't create new threads forever. It will start new threads if ones are killed (usually by Timeout code) though.\nThe situation is that users report when running an app under puma for a long period of time, the memory footprint continues to grow. A simple example is: https://github.com/puma/puma/issues/342#issuecomment-75843728\nBasically, the memory footprint grows slowly per request, eventually holding on to gigs of memory.\nBecause Puma uses a Thread pool, maybe there is some resources associated with each Thread that are growing and not being cleaned up? That would account for why Puma behaves differently that Unicorn here.\n. @tomas Thank you for the report. I ran your script locally on OS X with Ruby 2.2.2 and puma 2.15.3 and while I can observe memory creeping up, if I also observe the live object counts as well (using GC.stat) and the process RSS over a longer period, both even off. The object counts even off quickly while RSS takes a while.\nThis leads me to believe that what you're observing is side-effects of OS X's malloc implementation. Because MRI uses malloc to store object contents, fragmentation and various effects within how malloc requests and releases memory from the OS is a big factor.\nAdditionally, you might be wondering: \"well, maybe the issue is a true memory leak\". Well, the leaks tool reports: Process 63278: 54 leaks for 2848 total leaked bytes.. (The leaks are all strings created by the VM, like defined guard in gen_random)\nGiven this research as well as @nateberkopec's post, at this stage I'm going to say that your report is incorrect.\nAs always, I'm happy to be wrong. But given what we can observe and measure, thats the most accurate assessment. \n. @tomas You indicate that unicorn's memory leveled off early. Can you tell me how you measured it? Because unicorn uses multiple processes it's more difficult to measure the total RSS used.\n. @tomas What were you using to measure unicorn? Did unicorn restart workers at all?\n. This is a ruby issue, solved in ruby 2.3.0.\n. There isn't anything Puma can do about that. That's normal ruby memory\nhandling.\nOn Thu, Apr 14, 2016 at 1:58 AM lylex notifications@github.com wrote:\n\nI use ruby 2.3.0p0 (2015-12-25 revision 53290) [x86_64-linux], this issue\nreproduced.\nOur server is about 32GB mem, and puma consumes about 2GB.\nI think it seems can be reproduced this way: in a request, generate a huge\narray, repeat again and again. Then the memory will constringe to a high\nmemory consumption level.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/342#issuecomment-209835813\n. Using puma directly isn't discouraged because it's a very easy to to start things up.\n. Very odd it would take that long. Sounds like the SSL layer buffered the request and didn't flush it. When you try without SSL, is it fast?\n. @wesbos On the subsequent requests, is there an error reported? Do you see the requests in the logs?\n. I'm going to do a release today with a bunch of fixes. Sorry for the delay!\n\nOn Tue, Dec 3, 2013 at 8:13 AM, Phillip Calvin notifications@github.comwrote:\n\n@evanphx https://github.com/evanphx Are you planning a 2.6.1 release\nsometime soon for any other reason, or would you consider doing one if I\ncreated a branch that just addressed this bug?\n@goodwill https://github.com/goodwill I looked at the Travis output and\nit appears the build is only failing on Rubinius, and that's as a result of\nrunning it on a more specific version. Nevertheless, I'll let a maintainer\ncomment on whether master is in a state that's safe to try on a production\nsystem.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/352#issuecomment-29723367\n.\n. Send over a PR! You can even do that by editing the file inside github.com.\n. I'd suggest you use custom cap tasks for now. The cap tasks are going to be spun out into their own gem soon.\n. Why would we want this?\n. I disagree. NullIO is very much not the equivalent of nil, it's just an IO-like object that does nothing with what it's given.\n. Yes, I'll accept a PR for this.\n. Could you gist the output and add the url to it?\n. I have a feeling this is a threading bug within rails. puma itself does nothing with assets.\n. @zekus what error do you see when not demonized?\n. Ok, here is the issue:\n\nSome people want puma to load the code BEFORE daemonizing, so that the user can see if there is an error. Other people need it to load AFTER demonizing because they start threads in their initializers and thus need them to survive.\nSo there is this tension between these 2 needs. I'll see about at least making it so that if there is an error like this, it shows up in the stderr output.\n. 2.6.0 out now that should fix this issue!\n. Please open a new issue with the details. This problem is very app and traffic specific.\n. What version of puma? There was some buffering issues at one point, but they should be fixed. Any chance you could get me a way to reproduce the issue?\n. Great!\n. I'll add those APIs. The status is there it's just not nicely exposed.\n. If you must, you could use instance_variable_get to grab the @status ivar. I'd love to get some ideas about what you want to use the status for.\n. Hm, ok. Looking at status isn't going to tell you much. Mostly that the beginning of #run was executed. What do you really want to know? That Puma is serving requests? What are the cases where it would not start?\n. So I added a nice API for you to use! I haven't added docs for it yet, but you can check it out here: https://github.com/puma/puma/commit/b24920d3eda7a7f60da99c006f0d4f082b69d752\nYou'll want to create your own Events object and pass it into Server.new so that you can register callbacks on it. Your blocks/callbacks will be called from the thread puma is running the server in, so you can use a ConditionVariable to block until the server has fully started.\n. So you'd expect puma to print something like \"Internal server error\" then?\n. Ah, the error happens when the app is being loaded? Can you paste the whole backtrace in?\n. @nchelluri I'm confused. What you pasted in looks like exactly what you want, so puma is doing the right thing? Please clarify.\n. Puma has a fallback error handler now and sinatra should also provide a rescue handler for uncaught exceptions running the app.\n. I suspect that Upstart is confused by pidfiles are the like. The Upstart scripts are there more as examples, so you'll have to debug them a bit. Check the Upstart log to see why it decided to restart puma.\n. You should not be demonizing when using Upstart, so thats correct.\n. Are you using jruby?\n. Unable to repro.\n. HUZZAH! The should probably rescue SystemCallError and IOError and swallow them entirely. The Errno usage is really all over the place because ruby doesn't insulate the runtime from OS differences in errno values.\n. 2.6.0 is out now with the fix! Thanks for the PR!\n. If you'd like them linked to off the website, please submit a PR!\n. Yeah, it's OR. Thats a bug in the message.\nOn Tue, Oct 8, 2013 at 6:53 AM, Henry Hwangbo notifications@github.comwrote:\n\nYes, I am running Jruby on Ubuntu. Shouldn't it read Jruby OR Windows if\nwhat you're saying is true?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/374#issuecomment-25891476\n.\n. It's saying \"Cluster mode is not supported on Jruby, nor on Windows\".\nEnglish is hard.\n\nOn Tue, Oct 8, 2013 at 9:00 AM, Evan Phoenix evan@fallingsnow.net wrote:\n\nYeah, it's OR. Thats a bug in the message.\nOn Tue, Oct 8, 2013 at 6:53 AM, Henry Hwangbo notifications@github.comwrote:\n\nYes, I am running Jruby on Ubuntu. Shouldn't it read Jruby OR Windows if\nwhat you're saying is true?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/374#issuecomment-25891476\n.\n. What does the puma config file look like?\n. And is there a config.ru file in WORKING_DIR?\n. That error is when loading the config.ru raised an exception. There should be some output about that exception. Start puma in a terminal and see what you get. Also, try doing bundle exec ruby -d -S puma to have the exceptions printed out as they occur (you can gist the full output for me to look at).\n. So it does run when you execute it directly, it's just when you do it via your init script it doesn't work?\n. The issue has to be within his application, there isn't much that puma can control about the given bug. I'll go ahead and close this.\n. The cap tasks need to be removed from puma, so i'm going to go ahead and just drop this .\n. This is going to make #byteslice crazy slow and potentially screw up apps that depend on the proper #byteslice. Instead, the strings should have their encodings fixed before being sliced.\n. AH! I didn't notice that this would only be active for Ruby 1.9.1 and 1.9.2. I'm fine with this then because most people have moved on to 1.9.3+.\n. Somehow the installation must have been corrupted because puma/app/status.rb is definitely there...\n. This isn't something that puma controls if it's via rails.\n\n\nWhat do you see when running in webrick?\n. Perhaps the cap recipes should be split out into a separate gem so that more people can directly contribute to them. Thoughts +1'ers?\n. @seuros Absolutely.\n. @andrewhavens are you able to get any information about this? I can't take much action based on your report.\n@balauru Sounds like a completely different problem if the process is still running. Please open a new issue.\n. That all sounds correct. Also there were some bad bugs with Process.daemon that I've worked around that were effecting restarts.\nIf this is still an issue, please open a new issue.\n. Technically that violates the rack spec. I'll see what I can do about supporting symbols though.\n. I'm going to say this is an error in your application. \n. Can you paste your config/puma.rb? Puma does wait for requests to end before quitting, so it's possible that the streaming requests are keeping it alive. Would you like puma to just quit and terminate the streams?\n. I'm going to add a timed terminate and some code to request exit and then actually wait for it to exit to 2.7.0. That should fix it.\n. A new option force_shutdown_after can be used to have SSE workers killed after a time.\n. Can you provide the app and harness that you used to do the testing? I'll reproduce it locally to figure out whats wrong.\n. What did you use to test performance? What tool?\n. On JRuby 1.7.6, the current HEAD looks fine. Turning keepalives on gets me 8261.33 r/s and without 2761.13 r/s (2013 Macbook Pro Haswell).\n. This was caused by Process.daemon being broken. 2.7.1 fixes the issue.\n. Looks like rubinius doesn't support symbols there currently. I'm going to stick with numbers.\n. Sorry, I won't. This only causes issues with older rubinius release and doesn't actually achieve anything.\n. Unable to repro. JRuby 1.7.6 seems fine.\n. This was very likely the Process.daemon issue that is now fixed.\n. I think that we're going to move the cap recipes out of puma proper.\n. Once the worker finishes the requests it will terminate.\nOn Fri, Jul 29, 2016 at 6:02 AM Selvamani notifications@github.com wrote:\n\nIf the last worker is responding to a request, then I decrement a worker\nby passing TTOU signal, will it terminate?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/399#issuecomment-236174259, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAABxQM8LyN4AEFL6vDkmsW79TKB3Svks5qafnYgaJpZM4BJR9_\n.\n. Are you asking for a feature to do this?\n. This is most likely due to the Process.daemon issues. Please try with the latest release and reopen.\n. If SIGTERM doesn't stop them, then that probably means that it's waiting for your app's requests to finish. What kind of application is this? Are you using long-lived requests perhaps?\n. This is something with your environment.\n. It's something that the parser inherited from mongrel, but we can certainly change it. What do you think a good value is?\n. This has been changed to 2048.\n. This is an error within your app itself, and thusly not something puma can help you with. Because --preload fixes it, that tells me it's probably an issue with your app loading in the workers and they're somehow conflicting with eachother. I'd suggest getting the celluloid devs to take a look.\n. Ok. I have a lead on this. Could you tell me about your app? I think that the server is waiting for all the requests to end. Is it possible that you're using very long lived requests?\n. Well, it returns that way because it's a request to stop, it's not waiting for the server to stop, the same behavior as sending a signal.\n\nI really need to get access to an app that this is occurring in to figure it out I think, because I have not be able to replicate it at all.\nIs there any chance that I could get access to the app? I'd be happy to keep it private or even debug over ssh on one of your systems only.\n. I agree, I'm going to change it to block because that seems to be the behavior that people expect.\n. @deathbob I'm in Pacific Time, anytime that is good for you I'll make work. Anytime during the afternoon next week looks clear.\n. Wednesday at 11am sounds like a fine time. Please message me with details on how you want to set it up (or if you don't know, let me know and I'll do so).\n. This was Process.daemon being very problematic. This has been fixed as of 2.7.1.\n. The docs need to be clarified that any implementation with a GIL benefits greatly from cluster mode.\n. No, it's not true since 1.9 has native threads but still has a GIL.\n. Agreed that it's a touted feature without really explaining why it's a feature. Really the only feature is the addition of the ability for a C extension to perform a blocking call without locking up the whole runtime (but that extension must notify the system it's about to).\n. @allaire Phased restarted is available when running in cluster mode and NOT using preload_app. This is because it relies on the workers reloading the app.\nI need to do a writeup about the various options and how they work together.\n. As for if you should use cluster mode on a 1 core machine, the conventional wisdom is that it's not going to help the throughput. But you should try because it depends on your application.\n. This has been fixed.\n. This is most likely due to the Process.daemon issue. Should be fixed in 2.7.1.\n. The beginning is best sent to the original stdout because it gives proper feedback to the terminal that requested daemonization. The stdout redirects are designed to provide redirection while an app is running.\n. What version of ruby are you using?\n. @sorentwo Are you using the prune_bundler option? It's documented here: https://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting\n. prune_bundler had a bad bug related to restarts that was fixed in 2.9.2. Did you try it?\n. Tell me more: what to you mean by new context? What does your config look like? Could you provide some output from puma?\n. Hot restart on JRuby isn't possible because the JVM does not allow file descriptors to be shared between a child and parent process.\n. Does it run ok if you remove the daemon flag?\n. The output says starting in cluster mode, but it doesn't appear you actually started it in cluster mode. Are you using a config file as well and didn't indicate that?\n. I'm not able to replicate the issue with a simple rack app. Are you redirecting the output to see if the new process is dying? What ruby version are you using?\n. If you generate a new rails 4 app, do you still see the problem? I did so here and everything looks fine, I'm unable to replicate the issue.\n. Can you please gist the config file and the exact way you're invoking puma (since I don't see the config file specified, I assume you simplified it but I need to see the exact thing).\n. Wonderful! Thank you!\n. If anyone has a way to reproduce this, I'm sure I can fix it quickly. I haven't been able to yet.\n. Found the bug. It's in the pure ruby Process.daemon that is always used now.\n/Users/evanphoenix/git/puma/lib/puma/runner.rb:85:in `reopen': /dev/null can't change access mode from \"w+\" to \"w\" (ArgumentError)\nFixing now, 2.7.1 will be out shortly.\n. Please see issue #202 for information on getting it going.\n. How do you know it blocks all threads? There's nothing in puma that would cause all threads to just stop, especially on JRuby.\nOn Fri, Dec 6, 2013 at 5:38 PM, Peter Kieltyka notifications@github.com\nwrote:\n\nI'm running puma 2.7 with Rails 4.0.2 on JRuby 1.7.8 / jdk7u45 and I've noticed that if Net::Http makes a GET request to a host that its unable to connect to, it will block the entire rails app (all 16 threads) until it times out. Net::Http makes the poor choice of using the generic 'timeout' library when opening a connection (#connect method in Net::Http). The timeout library spawns a thread and sleeps for X seconds (the timeout period) before rejoining the thread. Perhaps this is causing the issue? ..\nIs there something Puma can do so that not all of the threads are blocked?\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/426\n. This was a Rubinius bug.\n. You have to have the key in X509 PEM format, the same format you'd use for other Ruby web servers.\n. Ok. I'd love to get some help from @headius on this, perhaps vetting the\nSSL API using in MiniSSL.java.\n\nOn Wed, Jan 8, 2014 at 9:42 AM, Nathan Agrin notifications@github.comwrote:\n\n@evanphx https://github.com/evanphx the cert is in X509 format. I\nplayed around with this more and it has to do with the Java library\ncurrently in MiniSSL.java not liking self-signed certs unless they are\npackaged into a specific keystore format.\nCan you provide me with any more info about what the status of\nMiniSSL.java is in Puma? We're using an outdated version of Puma under\nJRuby in order to support SSL and I'd like to be able to update us. I'm\ndoing my best to work through the SSL code in MiniSSL.java in order to help\nas much as I can, but any info you could provide me about the status of\nthis code and what remains to be finished would be very helpful.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/435#issuecomment-31858430\n.\n. What version of ruby are you using?\n. @sandelius what version of puma?\n. Please try with the released version of puma, not the one from git.\n\nOn Wed, Jun 25, 2014 at 12:12 PM, Stephan notifications@github.com wrote:\n\nI'm still experiencing this with a normal restart in single mode. pumactl\nrestart kills the process; running it again starts the process.\nruby 2.0.0p451\npuma (2.8.2) from git HEAD:\n  remote: git://github.com/puma/puma.git\n  revision: 5d189197fb8f476ad94bfdbbb63d133f684d0ee3\ntranscript of behavior here:\nhttps://gist.github.com/StephanX/5f8ac46e28a5e58fc917\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/436#issuecomment-47145147.\n. If you disable daemon mode, does the problem still happen?\n\nOn Fri, Jun 27, 2014 at 9:33 AM, Evan Phoenix evan@fallingsnow.net wrote:\n\nPlease try with the released version of puma, not the one from git.\nOn Wed, Jun 25, 2014 at 12:12 PM, Stephan notifications@github.com\nwrote:\n\nI'm still experiencing this with a normal restart in single mode. pumactl\nrestart kills the process; running it again starts the process.\nruby 2.0.0p451\npuma (2.8.2) from git HEAD:\n  remote: git://github.com/puma/puma.git\n  revision: 5d189197fb8f476ad94bfdbbb63d133f684d0ee3\ntranscript of behavior here:\nhttps://gist.github.com/StephanX/5f8ac46e28a5e58fc917\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/436#issuecomment-47145147.\n. @mperham With regards to setting Puma as the default, Rack will find puma automatically already (https://github.com/rack/rack/commit/e6284a3b744fca5373e1119ec37958af5f27f155). Do you think more is needed?\n. I'm guessing it's some kind of thread issue. Though, why is puma involved in a Sidekiq::Worker? \n. I really have no idea. @mperham, any idea?\n. @sudara I see. I was confused by your comment \"Each worker has a number that doesn't change during it's\" to mean \"No worker shares an index with the previous ones\". \n. How did you activate it?\n. Didn't work how? What happens when you go to \"https://127.0.0.1:9292\" ?\n\n\nOn Thu, Jan 23, 2014 at 10:07 AM, beckmx notifications@github.com wrote:\n\nI just did like the example\npuma -b 'ssl://127.0.0.1:9292?key=path_to_key&cert=path_to_cert' but\nit looks like it didnt work\n2014/1/23 Evan Phoenix notifications@github.com\n\nHow did you activate it?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/puma/puma/issues/442#issuecomment-33150058>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/442#issuecomment-33150953\n.\n. You've got the options all confused. Try puma -b 'ssl://....' and see if that works. \n. The error indicates that it can't find the rails app to start. Is it in the correct directory? I'm pretty sure that you're using of sudo -l to run puma will change directories and result in puma not finding a config.ru.\n\nThe bigger issue is that your puma configuration is wrong. You've got application_path = 'path'. That's just setting a local variable, not actually calling a method (same with environment). Change that to directory 'path'.\n. This is a rails problem.\n. You've figured it out. Puma can't help you out if you don't tell it about the redis connection.\n. Yep, you can use actually the same code, though you'll need to wire it into puma differently because @tmm1 patches Unicorn on the fly to inject the code.\nThat being said, puma's use of threads makes the definition of \"out-of-band\" differently than Unicorn. Because when one thread finishes with a request, another thread might be in the middle of handling a request.\nNow that I talk through it, I should see about adding an idle handler to puma. That could be used to trigger @tmm1's OOBGC when there are actually no other requests to handle.\n. Ruby 2.1's GC improves throughput by deferring work, which makes the need for OOBGC less important.\n. It runs whenever the VM decides to run it. Puma does nothing to control that nor can it really.\n. My hope is that 6451a1f should resolve this issue. I'm going to release that fix shortly.\n. There is little else in the response pipeline that could result in this. If 2.9.0 doesn't fix it, please reopen.\n. I'd love to get started on HTTP 2.0 support! It's not in the works yet but I've been thinking it would be nice/fun to have early support. For me, it's been a matter of finding time. If you want to get started, dig in!\nI'm mostly focused on bug fixes and stability right now, so that people have confidence in puma.\n. @tenderlove and I have been discussing it! @tenderlove, do you want me to\nstart looking at add support directly to Puma's core?\nOn Thu, Jul 23, 2015 at 6:03 AM, Nate Berkopec notifications@github.com\nwrote:\n\nIs it time to start the HTTP/2 party yet?\nHere's @tenderlove https://github.com/tenderlove's HTTP2 webrick\nserver: https://gist.github.com/tenderlove/79e7e0de4b2097e43356\nAnd here's his HTTP2 server with Puma!:\nhttps://gist.github.com/tenderlove/d68d9a3c4f7941192c9b\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/454#issuecomment-124088715.\n. Nate's points are (as is to be expected from Nate) right on. Right now, no rack apps are setup to really get much out of HTTP 2.0 so even if puma supported it, you wouldn't likely see much benefit.\n\nNow, you could make the argument that if Puma did have it, that would push Rails et. al to add support for it. So basically I do think something along these lines will eventually happen it's just pretty low priority right now.\n. Do you have a way to reproduce the issue?\n. Turn off preload and it won't load before daemonization. In my opinion, most people don't need preload but I added by popular request.\n. This is an MRI bug.\n. Stats does work, just not when using a pid, because pumactl doesn't know how to connect to that process.\n. The idea here is that a restart means calling execute to start puma again fresh. Because of unix semantics, it's trivial to leave fds open calls to exec(2), and thusly this hook provides a way to cleanup the process before starting the new one.\nA failure to cleanup fds will result in basically a slow fd leak. Eventually, after many restarts, you'll fill up your file descriptor table and the kernel will start to return errors in random places.\n. @concept47 How have you run rufus-scheduler with other servers? In the same way? I'm wondering if it's coded in such a way that it's holding the GIL and starving the process....\n. Sounds like it might be GIL related. Try Rubinius too, see if it pops up.\nOn Mon, Feb 24, 2014 at 11:43 AM, concept47 notifications@github.comwrote:\n\n@concept47 https://github.com/concept47 How have you run\nrufus-scheduler with other servers? In the same way?\nyup, I ran it in threaded mode with thin. And it was the same exact thing,\nthats how I figured the problem was probably not with puma.\nI'm wondering if it's coded in such a way that it's holding the GIL and\nstarving the process....\nThat sounds plausible. what I wasn't able to do is to run it with jruby to\nsee if the same problem surfaced there\n\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/464#issuecomment-35926784\n.\n. This is now fixed.\n. Wrt to #469, you indicate that you kill the workers directly and puma restarts them. To me, of course it restarts them in the old directory because you didn't request a phased upgrade.\n. oh oh, nevermind, I get it now.\n. Why would it need to be required without the test/ prefix? \n. Ok, thats a good reason.\n. Can you make it so the message that goes to stdout is qualified? By that I mean, add\nputs \"An application error was detected:\"\n\nSomething that sets it apart in the output.\n. After considering, I just don't like printing this to stdout. There is support for a user defined low-level exception handler now, so that should be used if you want to capture this situation.\n. Don't worry about the hooks, this is something that should be built-in to puma.\nProbably the right way to handle it is, when doing a worker restart:\n- Attempt to boot as a worker\n- Rescue any exceptions seen while booting\n- Report the exception to the master\n- At this point, the master sees that no other workers will be boot properly either, so the phased restart is aborted and reported.\n. Started it here: https://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting Send PRs to help flesh it out! (like adding a TLDR).\n. @mperham More like written by you! Some great insights seeing a real world config!\n. Rails -b option doesn't pass through to Puma properly. You have to start it via puma instead.\n. What are you looking to have addressed? I dealt with the issue long ago. Are you having a problem with rails -b still?\n. Looks like a bug in rack/commonlogger.rb.\n. I prefer 'and' to '&&'.\n. This is a preload problem. Remove it and the problem should go away.\n. This is an issue with your system. I suspect you built ruby with a different compiler than is now active, thus causing the problem.\n. This is a rack / rails bug, not puma.\n. Run it via puma or set RACK_ENV=production. You're probably starting\npuma via rack via sinatra and so the options aren't being passed through.\nOn Mon, Mar 17, 2014 at 8:47 AM, umito notifications@github.com wrote:\n\nruby backup_sinatra_server.rb -s Puma -e production\nPuma 2.8.1 starting...\n- Min threads: 0, max threads: 16\n- Environment: development <== !!\n- Listening on tcp://0.0.0.0:4567\n  == Sinatra/1.4.4 has taken the stage on 4567 for production with backup from Puma\nIt's a simple Sinatra app with one post url to respond to. Sinatra starts\nin production, but puma somehow keeps on development?\n\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/504\n.\n. Sorry for letting this linger. We removed the rubyforge option entirely on master a while ago. No need for the group option either really.\n. You can use strace against puma to see if it's actually doing work or not. But you can always just look at the CPU usage and see if it's pegging the CPU or not.\n. No, there is no ability to use a config file via rails server, you need to use puma directly.\n. So, here is the reason I don't want to enhance this route: when launched via rails server, puma doesn't control the process from the start. This means that things like hot restart, etc won't work correctly. \n\nAdditionally, I have no clue what has already been loaded so I can't reliably fork, so cluster mode is a no-go.\nIt's just a whole lot of additional headaches with very little upside.\n. @jeremyhaile As for the logging, I've talked with rails-core about that and I need to discuss with them again. We should solve that problem instead. /cc @tenderlove\n. Append the certs together and pass them as the cert file.\n. Can you run netstat -tlpn? It looks like you've already got something listening on port 5000.\n. Yeah, this looks like some kind of foreman bug, where it's not stoping and restarting things properly.\n. It sounds like foreman isn't telling puma to quit properly. I'll try to load it up and see what's it's doing. Could someone post the Procfile they're using?\n. Could you post your config/puma.rb then too?\nOn Mon, Aug 10, 2015 at 10:37 AM, Mike Piccolo notifications@github.com\nwrote:\n\nProcfile:\nweb: puma --config config/puma.rb\nworker1: QUEUE=1 bundle exec rake jobs:work\nworker2: QUEUE=2 bundle exec rake jobs:work\nworker3: QUEUE=3 bundle exec rake jobs:work\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/516#issuecomment-129543773.\n. This should be fixed in the latest release, please update and try again.\n. I considered this but it's super low priority. Is there a reason you need it?\n. Puma now answers immediately that if it detects an Expect: 100-continue. The reason it does this rather than taking the unicorn approach to letting the app handle it is that it's extremely unlikely that an app will know how to handle it and clients might send it to any app, even if they don't handle it. Thusly sending 100 Continue without consulting the app is totally safe.\n\nFuture work includes deferring sending the 100 Continue until the app begins reading the request body (this is how Go handles this).\n. I'm planning to move the init.d scripts out of the main repro because I'm not maintaining them. If you'd like to write init scripts, go right ahead and I'd be happy to send people that way.\n. @billingb Do you have any way to reproduce your problem?\n. I've left my comments over on zerowidth/rack-streaming-proxy#6\n. require_relative is a 1.9 only method and puma still supports 1.8.\n. I suspect this is some kind of bug in the way Live interfaces with puma. /cc @tenderlove help?\n. that's certainly possible. I'll release that fix this weekend.\nOn Fri, Jul 17, 2015 at 8:50 PM, Matthew Draper notifications@github.com\nwrote:\n\nCould this be a manifestation of #743? It seems unlikely that read_body would yield an empty chunk, but the described symptoms (including sleep fixing it) otherwise sound plausible.\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/525#issuecomment-122479893\n. i'm pretty sure the issue was #743. If you still have this issue, please open a new issue.\n. This is a JRuby bug. /cc @headius\n. Can you provide a reproduction of the issue? I don't see why puma would factor into this.\n. Great job! @headius would you might take a quick look?\n. If you set threads 1:1 then multithread request handling is disabled. Puma uses some threads for other things (such as handling slow clients) and that is what you're seeing.\n. @albertnetymk they're not documented right now. What info are you looking for?\n. It's fine, that functionality just isn't available. Use USR2.\n. If you want to truly share the memory (one worker changes it, the others see it) thats outside the scope of puma. If you just want to create something that is then copied into the workers when they fork, you can use preload_app!.\n\nThat being said, puma does not boot workers (and thus apps) nearly as often as unicorn. The workers are mostly long lived so I wouldn't worry the app startup time (because it happens so infrequently you'll probably never see it anyway).\n. Do you have a way to reproduce the issue?\n. Embedding DH params like this is a no-no. It needs to be pulled out.\n. You need to use directory in your puma config file, set to /home/user/apps/app/current. Without that directive, puma is forced to try and figure out the directory. The change in approaches (chdir vs cd) changes how puma works because cd sets an envvar that puma can pick up to figure out value requested (which is important because doing Dir.getwd returns where the symlink resolves to instead of the symlink itself).\n. Yeah, thats the envvar. Not much you can do other than embed the directory in the config file at present. I'm happy to add a feature to puma to deal with this situation better though, we just need to figure out what that feature is.\n. That's a good question. I'm not yet sure because it does alter the behavior a little and I didn't want to surprise anyone with it. Perhaps for 3.0 we do it by default.\n. This is a JRuby specific issue. /cc @headius\n. I'm going to reject this mostly because it would make the restart much less obvious than a strict restart.\n. Can you provide a way to reproduce the issue?\n. @ben-simplee Well, if you've disabled threads then there is no way to call from the application back into itself. Turn threads on and the problem will go away.\n. I think this would be interesting. I'd be happy to merge a PR.\n. This gone done for 3.0!\n. The message should go the same place as the start message, which is not via #log, as that goes to stdout.\n. This should be fixed in 2.9.0. If not, please reopen.\n. I guess the prune_bundler restart is still having problems...\n. @bmorton How are you starting puma?\n. @bmorton I found the bug and just released 2.9.2 with the fix. Could you try it out and see if everything looks ok?\n. It's not serving them twice, the logger is just misconfigured and in the rack middlewares twice. Add -q to disable puma's internal logging and only use sinatras.\n. Do you have the stderr/stdout from puma? It should be in whatever process monitor you're using.\n. @sheharyarn when does it crash? after it's been running a while? At boot?\n. @sheharyarn @MisterFine1 The overall problem with these issues is that there is a 90% chance that it's something within your app. Running out of database connections, deadlocking in some non-threadsafe code, etc. Within puma itself, the only thing that would cause an eventual outage is really a memory leak that causes the system to grind to a halt. While there are people reporting a memory leak, I still am unable to reproduce it and my theory is that it's actually something else within Ruby that is causing the problem.\n. @sheharyarn Are you using workers? Do all the workers crash too? I'm wondering if maybe a gem you're using is crashing and bringing the whole process down.\n. @MisterFine1 Totally true, a deadlock wouldn't cause it to just disappear like that. Since you're not using workers, if it just disappears like that, then it's a JVM crash I assume? Are you using daemonize? If so, can you turn it off and see you can get any more info from the output? The daemonize code for JRuby is squirrelly due to the JVM so we might get more data that way.\n. Please update to the latest release of puma and try ruby 2.3.0. Most users have said that fixes these kinds of issues.\n. I suppose that Puma needs to force the encoding of these strings.\n. This is most likely caused by the the primary thread closing and the worker threads finish up and seeing closed resources. Thread.abort_on_exception isn't really something people should be using with puma.\n. Is the link_to generating an http:// link targeted at the port running SSL? Perhaps that's the issue.\n. on_worker_shutdown gives you the ability to add your own logic to control the lifecycle.\n. I'm sorry about that, I should just remove the start functionality from pumctl. It provides an additional unnecessary vector for confusion.\n. And another vector for bugs. In fact, I should just reimplement it in terms of exec'ing normal puma.\n. @urkle what commands were you running? You didn't paste them in.\n. This should be fixed now, lots has been adjusted in the code loading.\n. When using single (i.e., no workers) mode, the stats endpoint returns the\nbacklog (how many requests are waiting to run) and how many threads are\nrunning requests.\nOn Tue, Jul 29, 2014 at 10:09 PM, Sumit Vij notifications@github.com\nwrote:\n\nIs there an API which allows me to fetch the current no of requests\nexecuting? I was hoping for something like server.running_threads_count. I\nlooked in the docs but couldn't find one.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/565.\n. Hi! Sorry I didn't get back to you get on this. I started some initial investigation but I haven't figured out the cause yet. I suspect there is a memory leak in the minissl code and it's even leaking with the keepalives, it's just a much smaller leak because there is only one connection, rather than 100000.\n\nI'll take a look later tonight and see if I can pinpoint it.\nThanks for your patience!\n. I think I found the problem and committed the fix in d33ae0f050023ee89e20dbf4530f300bad3b30b7. It turned out to be a simple mistake of not freeing the underlying SSL data structures on the cleanup. Could you try this out of git and see if it fixes your problem before I release it?\n. Great! I'll release this tomorrow.\n. Just released 2.9.1 with this fixe.\n. You'll be able to use force_shutdown_after to tell puma to kill any Live running threads in 3.4.0.\n. Very nice! Thanks!\n. I can't really help you there. Looks like something is up with how Ruby is compiled.\n. This is fixed in 2.9.1.\n. That 5 second sleep isn't related to phased-restart, thats the heartbeat for a worker process.\n. Forcing the close of the res_body to happen in a certain time feels like a hack. But I'm not opposed to hacks if it solves real problems. What do you think a reasonable default for the timeout should be?\n. Because it causes many other issues if we don't close it. Looks like it was a rails bug anyway.\n. Can you try using bundle exec puma -E development -p 8080 instead and see if you have the same issue?\n. This must be some weird docker issue. I don't see what puma would behave this way.\n. Ah. FreeBSD must return that error for unix sockets instead of returning nothing like linux does. Easy fix.\n. That's just for dev mode so that the requests are always logged regardless of if the app does it's own logging. Pass -q (for quiet in the config) to disable it. \n. Please remove the \"rubygems-bundler\" gem from the Gemfile.\n. I haven't heard if this is good to merge.\n. Puma has a SIGHUP handler, so I'm not sure why it would crash the process.\n. @renier are you using puma ssl in production?\n. So sorry, fixed now. I did a gem push with the java 2.9.2 version as I ran out the door yesterday and it errored out in an extremely odd way. I just pushed it now so this should be good.\n. I've been pondering this one. My guess is that it's actually an issue with whatever connection pool (probably ActiveRecord) you're using. The reason the issue goes away with unicorn is unicorn workers are killed off and when the new worker is started, the code to clear the connection pool runs. That clearing allows new, fresh connections that do work to come in.\nBecause puma doesn't kill off workers unless they actually totally crash (not hang on database IO like you're getting), that logic doesn't kick in.\nPerhaps @tenderlove has some ideas?\n. I think this is probably something we need the rails folks to look at, since that is likely where the problem is.\n. @quentindemetz be sure your connection pool is large enough.\n. @toymachiner62 Can you tell me some specifics about what you see and what settings you're using?\n. This appears to have a pool set to 1, meaning only one connection is allowed. How many threads do you have puma configured for though?\n. Looks like you're doing it via rails s, so the default of 16. That's probably the issue. Why do you have the pool set to 1?\n. Sounds like the connections aren't being returned to the pool properly...\n. @toymachiner62 What version of rails? Rails handles all it's own poll management and is supposed to be using rack middleware that returns connections to the pool when a request is finished. If the issue happens because of the bad url, perhaps when an exception occurs rails isn't returning the connection.\nAs to why it happens on puma and not webrick, thats certainly a good question. Webrick spins up a new Thread for every new connection, so perhaps there is something with the thread locals...\n. @tenderlove any thoughts on this?\n. Ok, looking at the code that is supposed to handle this (https://github.com/rails/rails/blob/4-1-stable/activerecord/lib/active_record/connection_adapters/abstract/connection_pool.rb) I see it in your backtrace here (http://stackoverflow.com/questions/27801185/activerecordconnectiontimeouterror-could-not-obtain-a-database-connection-wit).\nBecause it's using the BodyProxy hack, if for some reason the body is discarded higher up the middleware stack, the connection won't be returned to the pool.\nI wonder slightly why airbrake and dragonfly are so low in the middleware stack, they're actually below the connection pooler code. Meaning if airbrake accesses the database, the connection won't be returned back into the pool!\nDo you know if airbrake or dragonfly might be accessing the database?\n. @toymachiner62 are you using Rack::Timeout and do you have many timed out requests?\n. @toymachiner62 what version of airbrake are you using? This fix (https://github.com/airbrake/airbrake/commit/21bdff4552d1befba6b87e6400ff595c33710672) is extremely important and might be the cause of the issue. Are you using a version of airbrake with that added line?\n. Ok, so I booted up https://github.com/toymachiner62/devise-connection-failure and update it to the latest puma. I don't see a connection error when accessing localhost:3000/asdf or any other urls. Could the issue be fixed in the newest puma? Can you try to replicate the issue after running bundle update puma?\n. @toymachiner62 wonderful! I'm going to do a little investigate to figure out WHY the upgrade fixed the issue as well.\n. -p is a shorthand for using -b. Puma needs to complain properly about the bind string you specified, since it's misformatted. \n. Is there anything in the log? There isn't anything for me to go on here. Also, what does your config look like?\n. Can you add stdout_redirect \"log/stderr.log\", \"log/stderr.log\", true to your config and see if anything shows up in those file? (You may need to adjust the paths to point to valid locations).\n. You don't need that code because the worker will boot the code itself. \n. everything in on_worker_boot you should remove. \n. You should have that code in an initializer instead if you want to configure the AR pool\n. The error says the problem: You have to install development tools first. You probably need to update Xcode.\n. You need to install the libssl-dev package, then everything will work.\n. Could you say how it becomes a \"zombie\"?\n. Can you provide the patch you made that fixed the issue?\n. @wendelllam Where is the fixe for this?\n. Yes, this is a problem with Spring, I'm not sure what puma would even say about this...\n. @wendelllam can you take a look at that failure?\n. Yup! Thanks!\n. Are you using preload? If so, turn it off and I'll bet the problem goes away. \n. When/how are you calling Rufus::Scheduler? In an initializer?\n. Also, what does your unicorn config look like?\n. How are you starting puma? I don't see any of the typical options in your config file. Also, can you try 2.10 and paste the output from puma when it starts up?\n. Can you show me what puma outputs when you start it up?\n. Update to 2.10.1 and see if Rufus works!\n. Zoinks! I'll take a look!\n. Can you tell me more about your setup? Are you using HTTPS?\n. I've pushed a change that I think will fix this into master. Can you try master on your app and let me know if the issue goes away?\n. @mastfish poke! I'd like to get a fix for this out today and if you can let me know that master is fixed, I'll ship it!\n. 2.10.2 is out now and fixes the issue. Thanks so much for reporting this!!\n. Are you using threads 1, 1 for your app because it's single threaded? I can see how that would deadlock because the workers don't balance balance on the accept loop (as you discovered). I also assume that your goal is to get the 2nd request to hit a different work (and thus different memory space) because your app isn't thread safe.\nThis is a rather unique situation because you really DON'T want threads in the picture at all. You make a good case, but one thing that this breaks is puma's ability to make progress on clients that are going to slowly send their request. The change effectively means there is no queue for the threads to work from, which will adversely affect performance because now clients will be held at the kernel level.\nThe semantics of the accept loop are also not great because basically it will spin, since the listen socket will report itself as always ready for read and your code is choosing to ignore that status.\n. I went ahead and merged #640 which should hopefully satisfy what you need.\n. Are you using the puma gem or via :git in your Gemfile? Bundler has a problem with jruby gems when using :git so I suspect that might be the problem.\n. Those look safe to ignore, though it's odd you're starting minitest in your production code rather than just when the tests are run. That warning was added because people use gems that start threads in there initializer that won't survive a worker fork, so it's important to let them know about the potential issue.\n. @tenderlove Any idea if rails loads minitest all the time?\n. @taf2 That would be a good PR!\n. You're using preload and clustering, which means that those threads don't survive into the workers unless the code that uses those threads automatically restarts them. I suggest you turn off preload.\n. @ghprince this is a warning to you that, because you used preload and workers, a thread was created in the parent process that won't be inherited into the workers. Either turn off preload or be sure to add something in on_worker_boot to recreate the threads.\n. To implement it, the cluster master will need to implement some kind of API it uses to get stats from the workers. If you want it, I'll certainly take a PR to implement it!\n. Where is that background thread started from? There was a bug that is fixed now that loads the code after demonizing, which should fix this problem for you already.\nThe other case is users doing preload!. In those cases, they can use on_worker_boot, though it's easier to just not use preload at all. :)\n. I encourage you to copy the jungle scripts into your own repo and begin to work on them there. I'm likely going to split them out soon anyway.\n. It completely depends what you're using to create the connection pool and how it's managed. If it is ActiveRecord, it won't reuse a connection across threads so you'll see a different error (basically request timeouts).\n. So to answer your question, puma doesn't know!\n. Yes, that's correct. I've had chunked request handling on the todo list, but I haven't gotten to it yet.\n. Why is passing the name of a config file via an environment variable better than directly to puma?\n. Oh, you want to set JAVA_OPTS from a puma config file? There is a chicken and egg problem there, so that probably won't work.\n. Probably.\n. You're probably using preload, don't use it. That will fix the problem.\n. Can you try 2.10.2? Is it ok?\n. Fixed by #628 \n. @dt1973 can you please try 2.11.0 that I released yesterday? It should fix this issue.\n. @jimmycuadra That is expected as puma needs libssl to build.\n. We disabled SSLv3 recently which might have cause an issue. Can you find out what cipher suites you're seeing advertised by your server?\n. This was fixed by #639.\n. All fixed now, thanks!\n. Well, it's not exposed. If you'd like it exposed, I'd take a look at a PR.\n. I think it needs to be abstracted. So something like Puma.clean_for_fork or something that closes the sockets, would be best.\n. We can add a mechanism to allow that to be read. Perhaps a new env key that contains the \"Puma runtime information\".\n. There is configuration for it but no docs because it's generally not used. Feel free to submit a PR with docs.\n. What error are you seeing?\n. Disabling preload is good, I don't mostly recommend people use preload these days (it causes more headaches than it solves).\nYou have to use prune_bundler in order for puma to be able to rotate in new Gemfiles, so if thats what you want to happen, then you're good.\n. @theonegri Those options are not picked up by the the included cap tasks so are you using your own capistrano tasks? Could you show the actual command line or config file that is used?\n. Where is that capistrano/puma file coming from?\n. I suggest you use capistrano-puma, The current capistrano code is going to be removed soon.\n. The jungle scripts are maintained by users, not by myself and are provided mostly as an example.\nWhat kind of issue are you having with RVM?\n. PR accepted!\n. There currently is one because I didn't think it made a whole lot of sense, given it's always on the local machine and thusly behind another load balancer. Can you tell me why you'd want to set it?\n. Using queue_request will increase latency, yes. Could you tell me about your use case? Why are you looking to disable request queueing?\n. These are thread safety related issues with your app. Is this a rails app? What version?\n. If you're able to provide some context (kind of request, backtrace, etc) I can look into the issue within rails as well.\n. @dmkl Ok, I see what might be the problem in rails, but can you tell me what is on app/models/order.rb,  line 428?\n. Ok, well, I'd recomend not calling url_helpers like that. It creates a new Module on every call it appears, which is super expensive, perf wise. I'll dig into it a little more, but seems like you should avoid the issue all together. \n. Everyone having this issue is on MRI yes? Not JRuby on Rubinius?\n. @nicolaracco @PJK can you please gist backtraces?\n. To any future :+1:er, please provide a backtrace. I totally understand this is an issue but between myself and rails core we're not yet able to figure out what the problem is. We'll only be able to figure it out with more detailed info.\n. OMG!! I can't wait to dig into this reproduction!!!! More details very soon!!\n. Ok, update time.\nThis is a bug in MRI. I've opened a bug report with them: https://bugs.ruby-lang.org/issues/10871\nYou can see a quick way to reproduce the bug here: https://gist.github.com/evanphx/6eef92f2c40662a4171b\nTo work around this bug, you must never call Rails.application.routes.url_helpers in threaded code. You should call it in an initializer and then reuse the value in your code.\nThere is a usage of url_helpers in rails via RouteProxy that calls url_helpers on every method_missing. This must be a bug because that means that code is CRAZY slow, so I'm working with rails to get that fixed. /cc @tenderlove\n. As sad as this is, I'm going to go ahead and close this issue. It's a bug in MRI that we'll have to wait for a fix of.\n. This is entirely bundler, not puma.\n. Honestly, don't use daemonize on JRuby at all. It's very difficult to implement a daemon on the JVM and all kinds of weird bugs show up. Use a process monitor instead.\n. It does work, it's just subject to very specific JVM conditions and that's why you get these kinds of weird bugs. In all honesty, I feel like, over all, daemon mode was probably a mistake. It causes more problems and generally let's people get away with deploying without monitoring, which is bad.\n@nachogiljaldo What commands are you running in puma.rb?\n. You have a trailing % with no entity which is a decoding error.\n. Well, the request is invalid, how should puma handle it?\n. I can only guess there is some thread unsafe code in that path that is causing this. When the code works, what is the class? Is it  still that RGeo::Cartesian::PointImpl? Are lon and lat normal methods on it? I can't see a reason the methods would disappear unless there is some metaprogramming there that is run on first use (and thus causes threading issues).\n. Well, I'm not sure how puma would influence this code other than running it in threads. As for why it into happens in puma, threading issues are so finicky it could be any number of factors like the number of threads involved. \n. Does this happen with every request too? \n. Ah! Then yeah, it's not threading. The RGeo library isn't loading properly I guess.\n. So I don't see the lat or lon methods in the RGeo repo even...\n. Ok, I see it now. It's a method on SphericalPointImpl though, not PointImpl. How are you creating the object? Perhaps there is a factory or something that is supposed to be making a SphericalPointImpl but is returning a PointImpl instead?\n. No, puma doesn't have that ability. You can use linux cgroups to limit memory to a set of processes though, if that's what you want.\n. Sounds like perhaps prune_bundler broke. I'll take a look.\nOn Tue, Feb 10, 2015 at 11:16 AM, Nick LaMuro notifications@github.com\nwrote:\n\nWe ran into the same issue on our end. For what it is worth, removing the\nprune_bundler option seemed to allow us to deploy again.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/653#issuecomment-73763532.\n. @indirect would you be willing to chime in here? Did bundler change anything that might break this code: https://github.com/puma/puma/blob/master/lib/puma/cli.rb#L462-L489 ?\n. Hm, I'm not able to reproduce this yet. @SavithaK could you provide some details on what your app is and what the output is?\n. @dvrensk @NickLaMuro @SavithaK Would you be willing to gist your Gemfiles? I want to see if maybe you've got puma configured in such a way that is causing issues.\n\nI was just reviewing the code that I'm noticing that I didn't pull over any load path, which makes me would cause this bug. But then I wonder why reverting back to a previous bundler release fixes it... \nAt any rate, this gives me something to go on. Will report shortly.\n. @dvrensk @NickLaMuro @SavithaK Can you run gem env and gem list in the same environment you run bundle exec puma and gist the output?\n. @dvrensk that gem list output is odd. If you run bundle, does gems like puma and rack show up? @indirect shouldn't running bundle actually install the gems into the gem path? That's what I've always seen... what am I missing?\n. I have isolated the issue and it's a change in a behavior of the value of GEM_HOME between 1.7 and 1.8 when bundle install --path is used. I'm talking with @indirect to figure out which behavior is the correct one.\n. @dvrensk @NickLaMuro can you confirm that you're using bundle install --path?\n. I'm going to go ahead and release this fix since it's active and in the wild. @dvrensk / @NickLaMuro please let me know if 2.11.1 does or does not fix your issue?\n. The backtrace is inside ActiveRecord's mysql adapter making me think that perhaps the connection pool isn't big enough and thus the app is getting starved? Are you accidentally sharing db connections across threads maybe too?\n. Well, first of all, is that pid file correct? You should check that.\nHow are you running these commands? Outside the container or inside?\n. This should be fixed now.\n. That's only possible (and necessary) when preload is used. You're not using preload and thus don't need any on_worker_boot code.\n. The only problem here is that legit uses of bind multiple times will now warn. Issuing the uniq! is probably enough.\n. You're using phased upgrade and the output indicates that the worker isn't stopping. The likely reason for that is that there are requests that haven't finished running and so it's waiting. Are you using web sockets or another kind of persistent request?\n. A client sent you a bad requests, that's what that is.\n. At the moment, no, sorry.\n. Nothing else changed, just changing the version of puma? What does your config look like?\nOn Fri, Mar 6, 2015 at 12:11 AM, Benjamin Fritsch\nnotifications@github.com wrote:\n\nhey everybody,\nyesterday i upgraded puma from 2.10.2 to 2.11.1 and saw an increase (around 10X) of the time spent in the request queue on Heroku.\n\nwhat is the best way to gather information to track this down? i'm not sure where i should start tracking this down. i'm happy to upgrade again for a few hours to collect some data.\nbest,\nben\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/661\n. Hm. The request queue... What exactly is that measuring? /cc @schneems \n. Ok, it's the time from the heroku router getting the request to when NewRelic picks it up.\n\nBetween 2.10.2 and 2.11.1 that are in the request path, but none seem like they'd cause a big problem.\nAre you willing to try using puma from git to see if we can isolate the change that caused this? If so, I'll let you know when I've got a branch setup and we can coordinate. We'll probably want to coordinate in the Gitter room, so let's schedule when we can work on this for about an hour.\n. Is it possible that the request time leaped up because the change was deployed on a dyno with a drifted clock then? Seems like just deploying it again might tell us. \n. Thanks! Great work!\n. I already keep a HISTORY file, I don't see a reason to have a CHANGELOG as well.\n. Use puma -p 80.\n. Done!\nOn Thu, Mar 12, 2015 at 12:54 AM, Juanito Fatas notifications@github.com\nwrote:\n\nP.S. Could you also add some metadata info in this page\nhttps://rubygems.org/gems/puma/edit, please [image: :pray:]\nHere are some I know:\n  URL VALUE   Source Code URL https://github.com/puma/puma  Documentation\nURL http://www.rubydoc.info/gems/puma  Wiki URL\nhttps://github.com/puma/puma/wiki  Mailing List URL N/A  Bug Tracker\nURL https://github.com/github/puma/issues\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/pull/667#issuecomment-78437017.\n. Thank you!! \ud83d\ude18\ud83d\ude18\ud83d\ude18\n\nOn Thu, Mar 12, 2015 at 9:52 AM, Juanito Fatas notifications@github.com\nwrote:\n\nThank you so much, Mr Phoenix!!!!!! :heart: :yellow_heart: :blue_heart: :green_heart: :purple_heart:\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/pull/667#issuecomment-78525075\n. This looks totally fine, both is idea and execution. Was there more you wanted to put into it?\n. The timeout is probably not because of your code. Looks like things aren't behaving properly on 2.2.\n. There are 2 levels of dropping support.\n\n1) When people open bugs related to them, we indicate that we're not testing on those platforms anymore.\n2) We start using features of Ruby that only exist post 2.0\nI'd be fine with the first one, but we don't really have a reason to do the second.\n. Thanks @chulkilee! Great work!\n. I worry this will break anyone that was using it. We should probably deprecate it properly (print out a message when used) before it's removed.\n. I'm closing this PR because there hasn't been any updates on it.\n. This sounds like an issue with your app. Did you test this against a simple rack app? What did you use to execute the requests?\n. Yes, this is just a symptom of the number of connections your browser chooses to make.\n. @Telmo nginx should be doing your SSL termination, not puma.\n. That sounds more like something within your app, perhaps database connections are slow to connect because of some kind of reverse DNS? Does this happen with other webservers?\n. This doesn't sound like a puma issue still.\nOn Fri, Mar 27, 2015 at 11:00 AM, dt1973 notifications@github.com wrote:\n\nReopened #678.\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/678#event-267133213\n. Puma will wind down the number of threads on inactivity, but not processes. Starting up new threads is fast and would not be the source of a delay.\n\nThere is nothing within the puma codebase that would explain what you are seeing. I suggest you try to minimize the reproduction to give yourself more data.\n. @dt1973 perhaps, but this issue is now outside the scope of puma.\n. This is a crash inside libpq and thusly there isn't anything Puma can do about it.\n. I actually don't see where Unicorn sets a STATUS header. What are you using the STATUS header for? Why not just use the HTTP status code?\n. This is most likely #691 as well, which is now fixed.\n. You have a header key that is a Symbol rather than a String. \n. Whoops! Fixed!\nOn Mon, Apr 13, 2015 at 12:44 AM, Petr Kaleta notifications@github.com\nwrote:\n\nPublish also java version, please\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/685.\n. Hm. It's strange that it works on unicorn and thin. I assume that action_dispatch only parses the body if the ContentType is application/x-www-form-urlencoded, though I'm not sure. @tenderlove Any idea if that's true?\n\n@shanaver If you can, can you dump the rails request when run under thin/unicorn and then under puma? Maybe we can use that to pinpoint the difference.\n. @fiftyandfifty That output for thin is really strange. It's jumbled and contains way, way more info than the puma one. Was it generated the same way?\nStrangely enough, the thin one doesn't include anything about content_type anywhere either.\n. This is #695 and will be fixed in the next release.\n. Hm, strange. I can't say I've gotten any reports of users using SSL on\nwindows, so it's possible there are some bugs there. Any chance you could\ntry this exact setup but under linux and see if it works? I'm interested to\nknow if it's mostly just SSL windows support.\nOn Fri, Apr 17, 2015 at 5:15 PM, fatroller notifications@github.com wrote:\n\nI'm trying to run my rails app on puma in development environment on\nwindows box. Without ssl the application works fine..\nHowever when I try to configure ssl for puma, nothing happens. No error is\ngenerated in the log and the web page (https://localhost:3000/) says\nSecure Connection Failed. Below is the output from command line that shows\nthe server started but nothing happened after that..\nE:\\ap01\\dev\\rmmi>foreman start -p 3000\n17:19:31 web.1  | started with pid 3016\n17:19:34 web.1  | * SIGUSR2 not implemented, signal based restart unavailable!\n17:19:34 web.1  | * SIGUSR1 not implemented, signal based restart  unavailable!\n17:19:34 web.1  | *** SIGHUP not implemented, signal based logs reopening unavailable!\n17:19:34 web.1  | Puma starting in single mode...\n17:19:34 web.1  | * Version 2.11.2 (ruby 2.0.0-p353), codename: Intrepid Squirrel\n17:19:34 web.1  | * Min threads: 5, max threads: 5\n17:19:34 web.1  | * Environment: development\n17:19:36 web.1  | * Listening on tcp://0.0.0.0:3000\n17:19:36 web.1  | * Listening on ssl://127.0.0.1:3000?cert=E:/ap01/OpenSSL-Win32/bin/server.crt&key=E:/ap01/OpenSSL-Win32/bin/server.key\n17:19:36 web.1  | Use Ctrl-C to stop\nMy config/puma.rb file is given below -\npath_to_key=\"E:/ap01/OpenSSL-Win32/bin/server.key\"\npath_to_cert=\"E:/ap01/OpenSSL-Win32/bin/server.crt\"\nthreads_count = Integer(ENV['MAX_THREADS'] || 5)\nthreads threads_count, threads_count\npreload_app!\nrackup      DefaultRackup\nport        ENV['PORT'] || 3000\nenvironment ENV['RACK_ENV'] || 'development'\nssl_bind '127.0.0.1', '3000', { key: path_to_key, cert: path_to_cert }\non_worker_boot do\nActiveSupport.on_load(:active_record) do\n  config = ActiveRecord::Base.configurations[Rails.env] ||\n             Rails.application.config.database_configuration[Rails.env]\n   config['pool'] = ENV['MAX_THREADS'] || 5\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\nThe procfile for foreman is given below -\nweb: bundle exec puma -p $PORT -C config/puma.rb\nI've also set config.force_ssl = true in my apps config/application.rb\nfile.\nMy ruby version is - ruby 2.0.0p353 (2013-11-22) [i386-mingw32]\nHas anyone come across similar issue ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/688.\n. Is there a reason you need SSL on Windows then? We can figure out what's wrong but I'm wondering why you need it.\n\nOn Tue, Apr 21, 2015 at 8:51 PM, fatroller notifications@github.com\nwrote:\n\nHi evan,\nI planning on deploying my app on Heroku - so will let you know how I go. But at the moment my dev environment is Windows... Without SSL, Puma works fine on windows...\nIs there a way to debug what really is happening? At the moment there are no errors in the logs and it seems that the browser is unable to connect to the application when SSL is configured...\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/688#issuecomment-94985841\n. This issue is pretty old and I'm sorry I didn't get back to you. Are you still having this issue?\n. Given the age, I'm going to close this issue. Please reopen if it's still a problem.\n. This is fixed on master. I'll release it tomorrow. Sorry about that!\n. Eek! Thanks for the report. I'll check it out shortly.\n. There is no rescue around the block because that has typically been handled by puma's lowlevel handler. This should probably also do it's own management though too.\n. @bpaquet Which version of puma are you using?\n. Sure!\n. Hm. I suppose this would simple set the Content-Type header to if it's absent on a POST?\n. So I don't see unicorn setting the Content-Type anywhere. Can you paste in the request.headers from both?\n. Aaaaah. Ok! So basically if it's empty rails or something else infers it to the right type but if it's set to text/plain it doesn't. Got it!\n. Thit was fixed in a2b02294c8905.\n. I'd definitely be fine turning the logs off entirely (though, if you just want them off, you can always just redirect the output to dev/null, yes?)\n\nAs for the standard logging interface, that would probably be fine. Or at least the ability to, in the configuration, specify a logging object that is used.\n. Great find! Thanks!!\n. Thanks Daniel!\nOn Tue, May 5, 2015 at 10:27 AM, Daniel Marcotte notifications@github.com\nwrote:\n\nThanks @evanphx https://github.com/evanphx! Feel free to let me know\nwhen any of the JRuby stuff needs some attention in the future, and I'd be\nglad to lend a hand.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/pull/698#issuecomment-99151069.\n. Done! Sorry for the delay.\n\nOn Mon, May 18, 2015 at 3:09 PM, Lane LaRue notifications@github.com\nwrote:\n\nCan someone release the gem please?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/pull/698#issuecomment-103228296.\n. @deees Yes, I'll have a release out this week. Sorry for the delay.\n. Seems like we should attack the initially problem, i.e. why threads in the pool are dying. I guess an exception is bubbling up to the top-level?\n. @acrolink Can you tell me more about your setup?\n@emerleite Can you tell me more about your setup?\n. I'm looking into this today.\n. With a totally minimal app I'm not reproducing what you're seeing. I did the following steps:\n1. Add rack = 1.5.2 and puma to a Gemfile\n2. Add prune_bundler and 'workers 2to conf.rb\n3. Addrun lambda { |env| [200, {}, [\"Hello World\"]] }to hello.ru\n4. Runbundle exec puma -C conf.rb hello.ru5. See that puma boots fine and check that it responds\n6. Change the Gemfile to haverack = 1.5.37. Runbundle update rack8. Runkill -USR1 $PUMA_MASTER_PID`\n9. See the workers restart phased properly\n\nCan you try this with your Gemfile and the simple hello.ru and see what the results are?\n. Also, everyone seeing this problem, can you dump in your configuration files? Maybe there is something that is cause this in there.\n. Editing the Gemfile.lock directly to change the version of rack doesn't\nseem to cause a crash.\nOn Wed, Jun 10, 2015 at 12:01 PM, James Le Cuirot notifications@github.com\nwrote:\n\nI'm back from work now so can't test this in a hurry but try modifying the\nrack version in Gemfile.lock by hand instead of locking the Gemfile to a\nspecific version. That's more representative of what's happening in\nproduction.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/705#issuecomment-110879034.\n. @chewi Oh yes, quite right. Let me dig into this and figure out what's going on.\n. @chewi Your solution doesn't work because it leaves the process in a unreliable state because it's simply not possible to load the gems from a Gemfile into the process where another Gemfile is already loaded.\n\nRemoving the rack dependency is really the only way to make puma resilient against Gemfile changes.\n. @chewi Part of the reason that doesn't work (I had to think through the whole flow just now) is would blow up if the users Gemfile did not include rack, which is certainly possible.\n. Mmmm, that is a good point. I started the experiment to remove the rack dependency on a plane with the idea that I wasn't actually using much of rack anyway. I was correct as well, I only used a tiny bit of utils, the builder, and the common logger. If I had used a ton of it, I doubt I'd have gone this direction, but since I was just a little bit, the dependency seemed more cumbersome.\n. @chewi You're right, it is simpler. I think now that I've removed the only gem dependency that puma has I should be able to remove a whole bunch of code from puma-wild (perhaps even all of it).\nIn the future, if gem dependencies are added, I'll likely implement your idea.\n. Remove that bind line from your config, that is causing them to bind to the default port. Since you specify a port on the CLI, you don't want to set one like this in the config.\n. Well, that means that something is already using that socket. That process needs to exit before you can start a new one.\n. Try to hit puma directly and see if it is returning a 500. It's possible that CF or nginx are getting confused about the app as it restarts?\n. I need to have much more information about the crash. What the app is, what it was doing before, what the crash looks like, etc.\n. Hrm, fun. I can make SSL support optional though I wonder if that will confuse folks.\n. The Rack spec indicates that the return headers must be Strings. You can add a simple rack middleware to iterate through the env and print out any keys that are symbols.\n. That's odd! Does it ever work? \n. Sorry for leaving this going so long! Are you still having this issue with current puma?\n. Given the age, I'm going to close this. Please reopen if it's still an issue.\n. I suspect that's because you have other values configured in a config file?\n. This is fixed in d2d1e5b430f.\n. Thanks! That fix is totally fine.\n. What does your puma config file look like?\n. Oh right right. Of course! That doesn't when you don't preload, as you're doing in the second one, the app isn't loaded into the process yet! So you can just delete the on_worker_boot block entirely, you don't even need it when using prune_bundler.\n. Rails isn't loaded yet when parsing the config file, so you can't do that first line. You'll need to remove it. \n. Is this in a rails app? One thing is that puma does not use it's own request logging in production mode, leaving it up to the app to provide that via either Rack::CommonLogger or as Rails does, it's own internal logging.\nWhat mechanism are you using for logging in production?\n. The way stdout_redirect works is just changing where STDOUT/$stdout go to. If you use a logger that sends data to stdout, it will be redirected to the requested file.\nThe logging in Puma is just using a rack middleware called Puma::CommonLogger, so you can add that into your config.ru script to use it.\n. Looks like this has been solved a while back! Sorry about the delay.\n. That's sort of a bug in rack/static for not importing the files it needs. Because Puma no longer requires rack unless the Rack constant is missing, it can't see this error.\nI'm sorry about his breaking change, I considered bumping puma to 3.0.0 to signal this change.\nIf this is a deal breaker for you to change your config.ru to require rack, I can add some work arounds. But the right fix is for you to do require \"rack\"; require \"rack/file\" to deal with the bug in rack/static.\nPlease let me know what you decide to do.\n. I've filed a PR against rack to fix the issue there: https://github.com/rack/rack/pull/911\n. Closing this since it's rack issue now.\n. You need to install libssl. Your distribution of ruby should have a way to install those libraries.\n. Can you gist the mkmf.log file? It seems like ruby can't find your OpenSSL libraries and puma didn't change anything about where it looks for those.\n. What error do you get? I don't have any way to diagnose the issue without more information.\n. What does your config.ru look like? Seems like it's not loading your\nGemfile.\nOn Sun, Jul 19, 2015 at 11:48 AM, Hayden Ball notifications@github.com\nwrote:\n\nI'm also having issues running dashing after upgrading to puma 2.12.2. I\nsee the following in puma.err:\n/usr/local/rvm/rubies/ruby-2.2.2/lib/ruby/site_ruby/2.2.0/rubygems/core_ext/kernel_require.rb:54:in require': cannot load such file -- dashing (LoadError)\n    from /usr/local/rvm/rubies/ruby-2.2.2/lib/ruby/site_ruby/2.2.0/rubygems/core_ext/kernel_require.rb:54:inrequire'\n    from config.ru:1:in block in <module:PumaRackCompat>'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:189:ininstance_eval'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:189:in initialize'\n    from config.ru:innew'\n    from config.ru:in <module:PumaRackCompat>'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:183:ineval'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:183:in new_from_string'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:174:inparse_file'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/configuration.rb:104:in load_rackup'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/configuration.rb:71:inapp'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/runner.rb:123:in app'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/runner.rb:130:instart_server'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:215:in worker'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:inblock (2 levels) in spawn_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in fork'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:inblock in spawn_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:105:in times'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:105:inspawn_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:157:in check_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:423:inrun'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cli.rb:215:in run'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/bin/puma-wild:31:in'\nIn case it is significant, I am bundling dashing from master (rather than\nthe version released to rubygems).\nMy puma.rb:\nDEPLOY_NAME=\"sparkseat_dash\"\ndirectory \"/var/www/#{DEPLOY_NAME}/current\"\nenvironment 'production'\ndaemonize false\npidfile    \"/var/www/#{DEPLOY_NAME}/shared/tmp/puma/pid\"\nstate_path \"/var/www/#{DEPLOY_NAME}/shared/tmp/puma/state\"\nthreads 0, 16\nbind \"unix:///var/www/#{DEPLOY_NAME}/shared/tmp/puma.sock\"\nstdout_redirect \"/var/www/#{DEPLOY_NAME}/shared/log/puma.log\", \"/var/www/#{DEPLOY_NAME}/shared/log/puma.err\", true\n=== Cluster mode ===\nworkers 1\nprune_bundler\n=== Puma control rack application ===\nactivate_control_app\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/735#issuecomment-122694848.\n. This should be fixed in 2.12.3. I'm pretty sure it was because the config.ru wasn't loading in TOPLEVEL_BINDING.\n. This is already fixed in 2.13.\n. Ok, I see what the issue is. I'll see about correcting it shortly.\n. @delwaterman You're not loading a config file at all, you're just running a random ruby file while requiring puma.\n. Sorry about that! 2.12.2 should at least stub it out enough to still work.\n. @rosenfeld This would be better implemented as a standalone gem that something internal to puma. For instance, you could have code in the workers monitor a file and clear the cache when the file is touched.\n. This an invalid URL, that's why it fails.\n. I'll do one this evening.\n\nOn Mon, Aug 3, 2015 at 1:47 PM, Dave Dinh notifications@github.com\nwrote:\n\nDoes anyone know where I can find when the next release will be out?\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/745#issuecomment-127402469\n. This is fixed in 2.12.3.\n. I'd rather not do this. It means balloon the git repo because I have to checkin the jar on every release then. Can bundler fix this eventually? /cc @indirect\n. I've thought about it and I'm not going to check the jar file in. It's just too big. If we can make it so rubygems + mkmf generate the jar, thats the best way forward.\n. Hm. Is this a common thing to do, put constants in the config.ru assuming it's the top-level?\n. This should be fixed now in 2.12.3.\n. What is the puma-manager you're using? The bug is in that entirely.\n. Your system is missing the logger command. In Ubuntu, it's in the bsdutils package it looks like.\n. This will be fixed in 2.13.\n. Fixed! It was only on the branch by accident.\n. Given you wouldn't be able to actually use something that large, I'd rather protect users out of the box and I don't see a big reason to make it configurable.\n. This definitely seems more like an issue with activerecord and database connections across threads. /cc @tenderlove\n. @mwalsher that's correct. It's better you use --tag or tag in your config file to set the tag to something that make sense to you. The default tag is just the directory puma was started from.\n. That's not a URI, thats why. You need to specify an actual file:// uri.\n. @chadwtaylor are you have a problem with the way those docs indicate? Streaming does work fine in Puma.\n. Sorry about that! Should be fixed now.\n. Yup, this is fixed in 2.13.1, out now!\n. This bug was fixed.\n. Newest puma doesn't require openssl.\n. This should be fixed now.\n. Sorry about that and the delay!\n. If anyone can replicate the issue, I'd be happy to fix it. I tried a bunch of combos of rails/puma/ruby this morning and couldn't get it to pop up.\n. @Tonkpils What version of ruby? What OS? Because ruby and the OS translate localhost into an address, I think those are the variables here.\n. Well, puma passes whatever the host you want to bind to directly to TCPServer.new, it doesn't do anything with it other than passing it along. So the handling of localhost as the value is entirely up to Ruby and and the OS.\n. Hi y'all,\n\nI'm going to go ahead and release @himdel's fix, I think it's a perfectly acceptable way to manage this issue. Sorry for the delay, the release will be out later today.\n. Good news everyone! I've just released 2.14.0 which makes SSL optional. So on platforms without OpenSSL (like El Capitan) puma will build without issue but you won't have SSL. Normally that's not an issue on OS X, so there ya go!\n. Send a PR!\n. Puma doesn't have control over which worker gets the request, the OS controls that. If you want to that kind of load balancing, you'll need to use something in front of haproxy or (and this is better) do that work in the background with something that does proper balancing, like sidekiq.\n. I'd rather keep it in the config file personally. Is there a reason you need the command line options?\n. What configuration are you using? How many workers, how many threads?\n. Do you have any logs around when the H12s begin?\n. @larchiu what version of puma? Can you try with the latest version?\n. Great! If you're here because you're having this issue still, open a new issue.\n. Yeah, let\u2019s definitely close this gap. We should probably do another review\nof all the code and see if there are other places this might be popping up.\nIt\u2019s possible this is the source of a memory issue.\nOn Fri, Oct 16, 2015 at 1:16 PM, Richard Schneeman <notifications@github.com\n\nwrote:\nWhen you create a block, it retains references to current variables in\nscope. We are creating blocks while trapping signals here:\nhttps://github.com/puma/puma/blob/d56ad84642d6d31ba8b8ae76e8af23093db3123b/lib/puma/cluster.rb#L332-L334.\nThese blocks are never garbage collected so they will retain references\nindefinitely. This includes a reference to every thread currently running\nvia Thread.list\nI think this should be fine for 99% of the cases, since most threads\nalready running at that point will continue to run. However if you created\na thread on boot that did something insane like read in a gig of data to\nthe thread context and then discard the thread, this would accidentally\nretain a reference to the thread and accidentally retain that data. It's an\nedge case for sure.\nTo fix we could make a new method def self.setup_traps(master) and we\ncould pass in only the main object, though we would need to expose\n@options[:workers] or pass it in.\nIt's not critical, but I think we should close this gap to be safe. What\ndo you think?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/798.\n. You need to manage the connections and the connection pool more tightly if you're having a stale connection problem. This is outside the scope of puma.\n. Do you have any more info about this? How much data is in the logs? What framework are you using? Who is writing the log?\n. @grepruby What log? On defined by the puma config file? Or a Rails log? I need more info.\n. Are you using a log rotator? Did you send it SIGHUP after the logs were rotated?\n. At present I don't have enough data to know what the issue is. If it's still a problem, please reopen this issue.\n. I figured the comment about new relic was best. There are services to help\nyou do this. There isn't another way on Heroku.\nOn Thu, Apr 7, 2016 at 12:11 PM John Bachir notifications@github.com\nwrote:\n[image: :disappointed:]\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/807#issuecomment-207053639\n. #891 provides the required functionality.\n\n@jeffblake That seems possible, yeah. Please open a new issue about it.\n. Is this still an issue?\n. That must be a very, very old linux if 'uint8_t' isn't available.\n. Just released 2.15.1 that fixes this issue. Also, you're using a very old version of libssl, you might want to upgrade that.\n. Sorry about that! I've just released 2.15.3-java properly!\n. Sorry this took so long to merge!\n. If you want to max out the instance, use 4 workers and tune the thread count up until the workers are using 100% each.\n. This is normal encoding issues with rails, not puma specific.\n. Ok, can you provide a backtrace of the error? It looks like it happens at the yield site according to your code above so that's why I figured it was  rails encoding issue.\n. Sorry for the delay in getting back to this. It should definitely work. Does it work using puma via the config file and running with puma?\n. Due to the age of this issue, I'm going to go ahead and close it. If it's still an issue, please reopen. Sorry again about the reply delay.\n. Fixed!\n. I'd be happy to merge a PR that did this. You need to tie into MiniSSL to do it, which should be easier that trying to make the openssl extension do it.\n. Well, I can't rename those now because that would break peoples config files. The docs just need to be be clear that it happens in the master, i'll do that.\n. I fixed the name, because it was confusing, leaving an alias for the old name.\n. I prefer my advise to Heroku's in this case. That's because adding more workers only increases scheduler contention within the kernel. If you have the same number of workers as cores and are using a decent number of threads, then you'll get maximum throughput out of the machine.\n. Yes, thats probably it. Currently puma doesn't support chunked request bodies. I'm going to prioritize adding support for them and you can follow in #620 \n. Yeah, you're missing headers to make OpenSSL work. I suggest you go through one of the puma installation guides. Doing apt-get install libssl-dev might do the trick for you.\n. What about it doesn't work?\n. I'm definitely open to making Rack::Handler::Puma pick up the config file. The reason it didn't was that originally I thought that the point of the rack handlers was quick (dev style) work and thus you wouldn't need a config file.\nThe number of threads could also be controlled by an environment variable that rails can set before invoking the Rack handler, so you don't need the config file. Basically, I'm happy to add whatever is needed to pass the values from rails into puma in the context of it's Rack handler.\n. I don't mean to add more work to this request, but this might be a good place to begin a new Puma::Embedded class that can be used to configure puma within a process. Right now it's a mess and I've been meaning to clean that up for ages.\n. This has been done for 3.0!\n. I like where this is going. Pull the code out of the CLI into the Launcher then having the CLI just use the Launcher means the code paths don't get stale.\n. Yay! I'm not concerned about the yield API. I doubt anyone used it, given\nhow strange and undocumented it was.\nGiven the compat layer you built in, if the tests pass I'm good with it!\nI'll give it a further read tonight and ping you tomorrow with questions.\nOn Thu, Feb 4, 2016 at 3:15 PM Richard Schneeman notifications@github.com\nwrote:\n\nOkay, I got all the tests to pass here. This was WAY more work than I was\nthinking originally. I was successful in extracting out a general purpose\npuma launching class Launcher and re-using that in the CLI and in the\nrack handler. As far as I know this is 100% API backwards compatible minus\none thing:\nThe one backwards incompatible thing: The original Puma rack handler would\nyield to an instance of Puma::Server when called with a block. Since\nwe're not creating a server manually anymore, i'm yielding to an instance\nof Launcher instead. I'm not sure how much people use this behavior of\nthe rack handler, or really even why it exists (though It was handy for\nwriting tests, I added an extra one for doing an integration style test\nwith the rack handler). The Launcher does have access to a \"runner\" either\nan instance of Single or Cluster, each of them have a server object, but\nneither expose it. If you want I could do the work to expose it and then\nkeep the rack handler block API the same, but it would increase the surface\narea of those to classes. So either we could not care about the API that\nmaybe not many people use or I could increase some object surface area,\nit's up to you.\nLet me know if you've got questions. I'm available to chat about this if\nyou want, I think we're both in the Rails contributor basecamp chatroom if\nyou want, or ping me in rubycentral chat. I could also jump on a hangout if\nyou need more fidelity. I'll be around tomorrow and next week after\nWednesday.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/pull/856#issuecomment-180097788.\n. Looks great! I'm going to go ahead and merge it. I'm going to probably add a few helpers as some further documentation to Launcher.\n\nThis weekend I'm going to be adding a plugin system to puma as well. After that, I'm going to go ahead and release all this work as 3.0!\n. Thanks so much for the repo but it doesn't seem to cause the issue for me! Could you list the exact shell commands you run after cloning that repo to cause the issue?\n. Nope, it's not. Should I need to run any bundle commands?\n. Oh, I see that the vendor dir requires me to be running ruby 2.1.0. Let me install that and try again.\n. Ok, so I had to run bundle update puma to get the puma_http11 extension build properly. With that change, I ran ./boot and still don't get the double requires.\n. What version of bundler are you testing this with? Perhaps thats the variable that is keeping it from happening to me (which also means, if you could, update to the latest bundler and see if it still happens for you).\n. Ok! Got it! It's not that rack is installed into a different GEM_HOME, it's that it's installed into the ruby's default gem directory (i.e. gem install rack).\n. Remove daemonizing and Puma will tell you the issue.\nOn Sat, Jan 16, 2016 at 1:39 PM Alexander notifications@github.com wrote:\n\nI am using rails and trying to start puma server on my production server:\nuser@Ubuntu-1510-wily-64-minimal:~/app/current$ puma -e production -b unix:///home/user/app/shared/tmp/sockets/puma.sock\n[9850] Puma starting in cluster mode...\n[9850] * Version 2.15.3 (ruby 2.2.1-p85), codename: Autumn Arbor Airbrush\n[9850] * Min threads: 0, max threads: 16\n[9850] * Environment: production\n[9850] * Process workers: 2\n[9850] * Phased restart available\n[9850] * Listening on unix:///home/user/app/shared/tmp/sockets/puma.sock\n[9850] * Daemonizing...\nNo error. Fine. Is that mean puma server is running? Nope.\nI try:\nuser@Ubuntu-1510-wily-64-minimal:~/app/current$ kill -9 9850\nbash: kill: (9850) - No such process\nOk. Maybe it is not the pid? Let's try:\nuser@Ubuntu-1510-wily-64-minimal:~/app/current$ ps aux | grep ruby\nascii-r+  9887  0.0  0.2   9496  2240 pts/0    S+   22:07   0:00 grep --color=auto ruby\nWhat is going on? Why I can't start puma server?\nNOTE: I successfully did it without .sock file (using RAILS_ENV=production\nbundle exec rails s Puma -d)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/862.\n. Fixed this via https://github.com/puma/puma/commit/c1b85b2b90d05e4d9cff2f83786e69b2bdcbdcdd instead. Thanks!\n. I\u2019m going to fix this today and have a release out. Thanks for your\npatience!\n\nOn Wed, Jan 27, 2016 at 10:53 AM Carl Zulauf notifications@github.com\nwrote:\n\nEncountering this issue too. Would really like to use Puma + SSL +\nActionCable but currently doesn't work. Websocket connections work fine if\nI turn off SSL.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/865#issuecomment-175790823.\n. Is there a reason you're running SSL in puma directly for ActionCable?\n. So I can fix this exact issue but I don't think it will make the system work properly. I'm pretty sure that EM will try to read the encrypted socket but not decrypt it. This is because EM does not use the standard ruby IO mechanism at all (even if Puma used OpenSSL instead of MiniSSL we'd have the same issue, it's not because MiniSSL is in the mix here).\n\nThusly, I'm going to just have to say that, for now, Eventmachine and Puma + SSL are not compatible.\nI'd love to hear more about why people are running Puma + SSL for actioncable rather than running SSL in front of puma though.\n. That would mean it has no ability to run requests. Why would you want that?\nOn Thu, Jan 28, 2016 at 6:21 AM Ed notifications@github.com wrote:\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/866.\n. AAAH! I misunderstood. Yes, it should error out if max_threads equal to or\nless than 0.\n\nOn Thu, Jan 28, 2016 at 4:35 PM Ed notifications@github.com wrote:\n\nI wouldn't want that . . . thats the point I would like this useless\nconfiguration to at least say:\n\"Warning: This configuration of max_threads leaves puma without the\nability to serve requests\"\nor to exit with a similar message.\nI just stayed up all night trying to work out why the puma process that\nlooked like it was running fine, without any errors in the logs was not\naccepting any connections. So its a silly thing really, but 3am me would\nlove some sort of explicit warning why that was happening.\nI can open a PR if you think its a good idea @evanphx\nhttps://github.com/evanphx\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/866#issuecomment-176497005.\n. Probably an exit\n\nOn Thu, Jan 28, 2016 at 4:40 PM Ed notifications@github.com wrote:\n\nWhat do you prefer a warning or an exit\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/866#issuecomment-176497861.\n. No, there is no way to do this. That's not a limitation of puma, it's of all web servers. Even if you looked at the socket directly, it's going to sit in TCP wait state and not indicate that it's closed.\n. Hm. Well, the intention of this feature is that it speeds up very high performance servers from having to resolve the remote address. If it's already set though, thats the fastest path.\n\nCan you tell me why you'd want to set it when it's already set?\n. That\u2019s certainly true, but usually your application will check\nX-Forwarded-For first. The intention of this feature is not to eliminate\npeople using X-Forwarded-For, it\u2019s mostly about a small perf improvement.\nThat being said, if you\u2019d like to use the feature this way, let\u2019s do this.\nAdd an extra optional argument to set_remote_address called\nforce=false, If force is true, then your conditional logic would be\nused. That should give you what you\u2019re looking for.\nOn Mon, Feb 1, 2016 at 10:03 AM mfrank01 notifications@github.com wrote:\n\nWhen using a proxy server, i.e, Nginx, it automatically adds the\nREMOTE_ADDR field as the IP address of the host machine, and not of the\nclient's IP address. (Source:\nhttp://blog.stevesmind.net/2012/10/nginx-not-reading-real-client-ip-address/\n)\nTypically, an HTTP header like X-Forwarded-For would be used to set the\nclient's IP address.\nWithout this change, using nginx + puma will always result in 127.0.0.1\n(or whichever interface to which puma has been bound) for a remote_ip\nmethod call to an instance of ActionDispatch::Request. At least, this has\nbeen my experience.\nIf I'm overlooking something, please let me know. I'd rather minimize code\nchanges.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/pull/873#issuecomment-178099895.\n. Thanks so much for the doc fixes!\n. What version of puma? There was some fixes related to the threads in the Pool dying prematurely that were fixed.\n. Could you try 2.16.0 and see if it still happens?\n\nOn Thu, Feb 4, 2016 at 10:43 AM Ben Osheroff notifications@github.com\nwrote:\n\nI'm running with the gemfile pointed at dbb1932\nhttps://github.com/puma/puma/commit/dbb1932f82df81abf36876eec2a20d4f4e1ef69b\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/876#issuecomment-179994432.\n. @todd Oh geez. So sorry for the confusion. I completely forgot about #770 about the need for directory because of how the current path is otherwise derived.\n\nYou have to use directory \"path/to/release\" because puma can't resolve it properly due to how links and getwd work.\nWhich CLI option do you mean?\nI need to add this to the docs so I don't forget to tell people about it in the future.\n. @todd That would be wonderful!\n. @allaire strangely I can't replicate that behavior with 3.1.1:\nshell\nveritas :: proj/current> bundle exec puma -C puma.rb\n[23132] Puma starting in cluster mode...\n[23132] * Version 3.1.1 (ruby 2.3.0-p0), codename: El Ni\u00f1o Winter Wonderland\n[23132] * Min threads: 0, max threads: 16\n[23132] * Environment: development\n[23132] * Process workers: 2\n[23132] * Phased restart available\n[23132] * Listening on tcp://0.0.0.0:9292\n[23132] Use Ctrl-C to stop\n[23132] - Worker 0 (pid: 23145) booted, phase: 0\n[23132] - Worker 1 (pid: 23146) booted, phase: 0\n[23132] - Starting phased worker restart, phase: 1\n[23132] + Changing to /Users/evan/git/puma/tmp/proj/current\n[23132] - Stopping 23145 for phased upgrade...\n[23132] - TERM sent to 23145...\n[23132] - Worker 0 (pid: 23149) booted, phase: 1\n[23132] - Stopping 23146 for phased upgrade...\n[23132] - TERM sent to 23146...\n[23132] - Worker 1 (pid: 23150) booted, phase: 1\n. @allaire Is puma starting via a normal shell or via upstart/systemd?\n. @allaire Could you paste in your upstart config file? Or at least show how you're invoking puma?\n. Ok, I've figured out the issue. I'll have a new version out this evening to fix it.\n. Ok, 3.2.0 is out now and it fixes this issue. Sorry for breaking this in 3.0 :cry: \n. Here is the commit: https://github.com/puma/puma/commit/8b05d0a76374019a815c7889f147e7c0aa561532\nIt's testable, but not well tested automated right now. If you want to spend a little time on writing those tests, I'd be happy to merge them.\n. That's correct, if you move the log files, you need to tell puma so.\n. Is there any log output? Do they respond to requests?\n. Sounds like your app is just working at max capacity?\n. @simonmorley Thanks for tracking it down! Good for future users to look out for as well.\n.  Great work! I reviewed the locks and it's fine.\n. Well, if doing a simple stop removed it, I suppose the better question is why 2 were started?\n. Closed due to lack of input.\n. :heart_eyes_cat: \n. You need to upgrade to the newest puma. It doesn't require OpenSSL (which was removed from El Cap).\n. If you use hijack, you have to write back the whole response on your own. Hijack basically tells puma to do nothing with the socket because the assumption is that the handler has already done everything with it by the time puma sees it again.\nI have no clue about sinatra's support honestly, maybe @rkh can shed some light on it?\n. 3.4.0, out now, has force_shutdown_after which will kill the threads after a specified time. You can use that mechanism to have puma terminate any SSE/long-poll threads.\n. When you use the port option, it's adding a bind to 0.0.0.0 with the requested port. Remove the port line and this behavior will go away.\n. @thezed Since this has been ongoing for a while and things have changed, would you resummarize what problem you're seeing?\n. Due to this issues age, I'm going to go ahead and close it. If this is still an issue, please reopen it.\n. Are you changing ruby versions during the phased upgrade? That's not going to be supported because you're changing rubygems versions with code loaded, very likely causing this issue.\n. No, that config.ru is fine. You're accidentally loading rubygems twice from multiple paths is my guess, you might want to look at the environment to see what is on the load path.\n. Hiya! There have been various fixes that we've hoped would plug some potential leaks folks have seen, so I'll hope you're one of them. Did you change ruby versions also?\n. It's only following what you asked it to do, so you must have specified that directory but it's not there?\n. Can you post our config?\nOn Fri, Feb 26, 2016 at 7:00 PM Isi Robayna notifications@github.com\nwrote:\n\nAfter upgrading gem to 3.0.2 I get the same error.\nNo configuration changes from previous versions 2.x.x -- something has\nchanged which obviusly is taking some settings in config\\puma.rb in\ndevelopment mode which are only intended for production (at least on my\ncase)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/904#issuecomment-189564174.\n. @irobayna What output do you get? Where is puma trying to change the directory to? Is it not that basedir/current?\n. The state file was never intended to be a portable format used by external tools. I didn't even know about this gem. I'll have to see what they're doing with the state file data...\n. Are you doing '@options.fetch' in your config file?\n. @Casara Why are you reading the max threads? Also, in 3.0+, you can do get(:max_threads) in the config files to read that value.\n. That error is actually inside Rack::Server itself, it's something I have no control over in this case because Puma is started via rails s.\n. In the future, please put the content of the request into the github issue itself.\n. It\u2019s my pleasure.\n\nOn Mon, Mar 14, 2016 at 8:48 AM Matt Di Pasquale notifications@github.com\nwrote:\n\nOK, I sure will. \ud83d\udc4d I'm glad to be of service. \ud83d\ude4f I appreciate you for\nimplementing this request, especially so quickly. That's rare. I'm\nimpressed & humbled. Thank you for your awesome contributions to the Ruby\ncommunity. You are a true legend. :-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/916#issuecomment-196376918.\n. @mattdipasquale In what doesn't it work?. Sorry about that! Fix showing up VERY shortly!\n. 3.1.0 out now should fix this issue! Please let me know if other things are JRuby are busted!\n. @donv: Yeah, I'm working on the JRuby tests this evening.\n. Ok, I have figured out the issue. It was added in 3.0 for puma to load a default config file when launched via rackup. Which means that your case worked in 2.16 because that puma config file was actually never loaded at all! The issue is actually with how Shotgun loads your app and how that interacts with puma. Shotgun appears to load the app in a special non-global constant scope (probably so it can reload it safely), and thus puma can never seen those constantly.\n\nSo basically using any hooks that need to reference your app's code won't work. That's not a puma limitation, that's how Shotgun intends for it to work. \n. Sorry for the delay on this!\n. Sorry, no plans to do this. Puma still works on older ruby versions, I'm just relegating my support of them to a lower tier.\n. Can you show me the error somehow?\n. I have zero idea how that would happen, it's not as though puma has control over those values...\n. Please reopen if you have more info.\n. How are you generating the json? It's not being generated as one big string and returned, I can tell that by the use of chunked encoding. Perhaps there is something there that will shine a light on this.\n. Closed due to lack of input.\n. So, sadly this appears to be a JRuby bug. I've submitted it: https://github.com/jruby/jruby/issues/3799.\nUntil that bug is fixed, you likely should not use unix sockets on jruby.\n. 3.1.1 out now with this fix.\n. I've added a new feature called force_shutdown_after that allows you to tell Puma how long to wait for threads to finish their work before being forced to stop. Would that solve your problem?\n. I think you're going to better off using beanstalk or sidekiq for this. Trying to this kind of very long request in a request oriented server like Puma is always going to be painful.\n. You can run Sidekiq in the same process of Puma, if that helps.\n. @everplays I could set some flag that indicates puma is trying to shutdown, your app could check the flag and take actions. Would that work?\n. @everplays Ok, a new api Puma::Server.current.shutting_down? can now be used to detect if the server is trying to shutdown.\n. I assume that the helper is scope to an instance of the controller created per request. In that case, you should be fine. I don't know if the internals of Grape are thread safe, but most frameworks are pretty good on that at this stage.\n. Running returns the number of threads that are running, not if puma is running or not. Sorry for the confusion!\n. Could you please use 1.8 syntax for the arguments? \n. Are you using puma as a proxy server? Why are you including a URL in the path?\n. @huangxiangdan Ah ok. Just curious. Can you verify for me if the query params are missing in the actual code or is it just the log output?\n. Yes, this an existing issue that I haven't figured out the right way to fix. Puma wants to wait until all the requests are done before shutting down but the Live is just sitting there waiting and thus never shuts down. Part of the issue is that Puma doesn't know that request is Live-style connection. \n. I can certainly give you an option to perform an unclean shutdown, sure. Perhaps force_shutdown_after option that takes the number of seconds to wait for the threads to finish. -1 would be the current behavior, namely wait forever. 0 would mean \"never wait, just exit\" and any positive number would just start a time and exit! when done.\nWould that work for you?\n. @vandrijevik I can have it try to kill the threads, though I still consider that a dicey move because it introduces the equivalent of raising an exception in a random place, and it doesn't work if the code is blocked in some C code (sometimes).\n. Are you sure that ENV['PORT'] is not defined? Add p :env => ENV['port'] and see what is output.\n. Nothing in puma resets environment variables. How are you running puma? What environment? (heroku, upstart, systemd, command line, etc)\n. Ok, can you run me through when it stops working? I need to understand the steps to try and figure out what the issue is.\n. OH, I did not realize you were starting puma via rails s. Yes, via rails s it has control of the port entirely.\n. So the problem is that rails specifies it's own port and passes it into the puma rack handler. Then we load the config file. The rack handler has higher precedence (which is how it should be) and thus we get this behavior. I think the current behavior is actually the correct one, because you're using rails s which has it's own way to specify the port. If you don't want to do that, then use puma directly.\n. It could bind to multiple ports, sure, but I'm doubting people would expect that?\n. Thanks!\n. @richmolj Can you hit localhost:3000 fine by hand? What exactly does haproxy say about that backend in the logs?\n. This might be related to how localhost is resolved. Try using 127.0.0.1 instead of localhost in case there is some weird ipv6 localhost screwing with things.\n. @richmolj I'll see if I can replicate the issue locally.\n. So, the issue is that an OS X (which I'm assuming where you're having this problem, but it could happen on other platforms too), when a socket says to listen on \"localhost\", it's dependent upon /etc/hosts to define what \"localhost\" means. Well, ruby ends up listen on ::1 (IPv6 for localhost) only in this case.\nThere are 2 fixes for you:\n1. In your haproxy config, using ::1:3000 instead of localhost:3000, then haproxy will connect properly.\n2. Use rails s -b 127.0.0.1 to force puma to use the IPv4 localhost only.\n. That's a wonderful use! Could you write up a PR to include this is the examples directory?\n. Closed due to lack of input.\n. I'm sorry, this is not a proper issue. I have no clue what you're reporting.\n. If I were to guess based on your paste, it's about that error message which is not emitted from puma.\n. @freen please open an issue about your problem separately. I have no clue if it's related to this one.\n. Closed due to lack of input.\n. What does it do when you try to start it?\n. What is bootup?\n. @jogaco I'm still very unsure what the issue is. Can you explain further?\n. Ok, so I guess keep it that name? Whats wrong with it being puma_manager.conf?\n. Because of the age, I'm going to go ahead and close this. If it's still an issue, please reopen.\n. I'm betting those are just other Threads running in the process, not the number of request threads in the thread pool. Puma uses a number of other Threads to manage things, such as the IO Reactor, timeouts, etc.\nWhen you started Puma, what did it say about the number of threads?\n. Sorry, that changes how LeveledOptions works entirely. It gives preference to earlier defined options because those are normally set on the command line, which should override ones in the config file.\n. Do you have the puma output doing a restart? Does this happen every time you use phased-restart only only sometimes? It almost sounds like your system isn't picking up the new directories fast enough?\n. I need more information about your issue. What kind of app is this? Is it doing a long poll? Is the browser using all 8 threads without releasing them?\n. You set workers to 0? That doesn't make any sense, if don't want workers, then just leave them out entirely.\nPlease try in production mode and see if the problem still occurs.\n. @u007 i'm sorry, I'm unable to figure out what is going on from what you're reported. Is there any more information you can provide me?\n. No, we don't touch STDERR directly here. You need to use @events.\n. @timabdulla I responded in gitter (btw, no need to ask in an issue and in there). It's not exposed now but I'd merge a PR to expose it. Can you tell me a little about why you want to set it?\n. @timabdulla Sure, sounds like a cromulent reason. Happy to merge that PR!\n. Well, I'm not sure how thats a @, because the decoded version of what you have there is '%'. I can fix puma to allow through the bare % in that position though, since Ruby's URI.decode seems to accept it.\n. This is mostly likely related to keep-alive connections. HAProxy is perhaps terminating the connection. The reason they're showing up now is a recent change to send errors that occur within the reactor to the lowelevel_handler. I'm going to make a change to that code to ignore any IO errors, so these should go away.\n. @kmayer No problem!\n. The answer to these kinds of issues almost always lays in the kind of app it is. Are you using Actioncable or another solution that keeps connections open? When puma refuses connections, is it otherwise idle? What configuration are you using puma in? If you're using non-clustering, then it's possible for a gem that uses a C extension to become a bad actor and prevent all threads from running.\nBasically, I need more information about the app and the circumstances to have a chance of figuring out what is going on.\n. @common-nighthawk Your app is the primary unknown here. Can you tell me about it? Is it using long running connections (SSE, Actioncable, etc)? What happens when you set the threads to 1, 1? Perhaps you have some code that is not thread safe and it's become confused and is locking up.\n. Oh. You know what, I just realized. Those aren't worker processes, those are the threads in the worker! Your top program is just confusing. Can you gist the output of ps ax? I'm betting it doesn't show those because they're threads, not processes.\n. @common-nighthawk I also just realized that probably doesn't matter since you're still having troubles, regardless of if those are threads or not.\nHow can I replicate the issue using your repo? Do you just let it sit there, no requests? Or is there a pattern I can use to recreate the problem.\n. @common-nighthawk As for the memory, there are no known memory leaks in puma, despite many people thinking there are over the years. They've always been either the app or ruby itself. What version of ruby are you on?\n. The threads command sets the number of threads in the thread pool to\nservice requests. There are other threads used to handle other tasks you're\nseeing.\nOn Tue, May 10, 2016 at 8:48 PM Daniel Deutsch notifications@github.com\nwrote:\n\n@evanphx https://github.com/evanphx\nYou are exactly right. Here is the output of ps ax:\nhttps://gist.github.com/common-nighthawk/48ba336df4ae06d7ace10e788cb85731\nWhen I run htop in \"tree\" mode, it's clear the six threads belong to the\none worker. But shouldn't threads 1,1 cap the maximum number of threads?\nI don't think there are any memory leaks in puma. My memory problem is\njust that I'm getting more threads than I want.\nAs for replicating the issue--yes, this problem happens right from the\nstart without any requests. Here is my puma.rb file:\nhttps://gist.github.com/common-nighthawk/e910cb67d6cc4aeff8274657a734301a\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/961#issuecomment-218354777\n. Minor clarification:\n- Threads are set per worker, not per cluster. If you set it to workers: 2, threads: 8 you'll have 16 total threads in the system.\n\n@machty You need more threads basically. I'm currently researching ways to make that not be such a sticking point though.\n. I'm not sure what the errors even are in those cases. I'm not sure what the order of things is, so it's hard for me to have any idea what is going on.\n. If this is still an issue, please reopen the issue.\n. It's causing the machine to crash? As in kernel panic?\n. I really can not imagine what in Puma would be causing that.\n. Sadly there is zero ability for me to reproduce this. If you have a way to reproduce it, I'll get right on it.\n. @nolman I'm totally at a loss still. The kernel panic log doesn't indicate anything about why the ruby process crashed the kernel. Realistically, this has to be an OS X bug because any user process that can crash the kernel is a kernel bug. I know that's not useful to hear but I simply have no other ability to help you :(\n. What do you think the puma bug is? I'm not following.\n. Data isn't saved inside a unix socket, so deleting it doesn't lose any data. Puma removes the unix socket file when it exits.\n. Well, that's interesting. By \"after a while\" do you mean the app is just sitting there or are you trying to restart it?\n. Is there anything in the logs about trying to kill timeout workers maybe?\n. Oh, I'm pretty sure using the same file for both stdout and stderr doesn't work right :( Can you send them to different files?\n. Well, I'm not sure what you're trying to do, but you didn't run this code because printf doesn't return a string and passing a pointer to %s is a crash.\n. @promentus @stmarier is correct, you're missing the proper OpenSSL headers. New versions of OS X do not ship with them, so if you'd like SSL support to work, you'll need to install from elsewhere, perhaps homebrew.\n. This is entirely within your app, it's not puma related. You're referencing a constant that isn't available.\n. Please upgrade to 3.4.0 or later.\n. Did you have something in the real world that was made you think there might be a leak?\nThe reason I ask is because the GC in Ruby 2.3 is lazy now, so GC.start doesn't really perform a full GC anymore. As a result, there might not be a leak, it's just an object that is garbage but not yet collected.\n. Ok, so I used http://ruby-doc.org/stdlib-2.1.0/libdoc/objspace/rdoc/ObjectSpace.html#method-c-dump_all to look at all the live objects in the system. While ObjectSpace.each_object is finding a Puma::Client and ActiveController::Base, they're not live objects. You're seeing garbage objects that just haven't been overridden yet.\n. I'll likely add code to remove it being an option in another year. There was someone asking about the ability to override the cipher settings as well. I might do that.\n. What's in your config?\n. I'm testing this and I'm unable to replicate the issue. Please reply if you're still having this problem.\n. This is really a bundler issue, not something that puma has any control over. But basically I believe rails hardcodes using puma in dev mode now.\n. Thanks!\n. @1c7 What kind of information are you looking for? Your question almost makes it sound like it's not clear what the puma software even does.\n. @netwire88 Not sure what your referring to, please open a separate issue.\n. I'm going to be a doing a bunch of work on puma.io very soon so I'll definitely do this!\n. What was the request? I don't have anything to go on.\n. I'm not really able to give you much comment here. The code used is effectively identical, perhaps @headius can provide some insight.\n. I'd suggest you discuss this with the jruby folks, this isn't a puma issue given the common code.\n. It's daemonizing, which you probably have configured in config/puma.rb.\n. What happens when you run the commands in a normal shell, not inside a rc script?\n. If this is still an issue, please reopen.\n. @mathie could you gist the whole output, I want to see if the output is REALLY the same thing 21 times...\n. Hey @schneems, weren't you looking into rails s not managing listen threads properly perhaps? This looks very similar.\n@mathie: Are you starting puma via rails s or puma? \n. Yeah, it's a fiddly test.\n. At present, there is not, no.\n. Sorry, that's not something I know much about. If you find details, please let me know so I can include them in the docs!\n. This is an issue with your MySQL gem. There are no settings within puma that will help at all because none will influence how/when your MySQL gem makes connections. Are you using mysql2? /cc @brianmario\n. This is an issue with how your database connections are managed and timed out.\n. Thats a platform shell library basically. What version of centos?\n. Ok, looks like the init script excepts lsb stuff and you're probably on a systemd release. So I need to add systemd scripts to have you use instead.\n. We'll track adding systemd support in #1029. For now, I suggest you author your own systemd unit file.\n. Is there a way you can replicate the issue for me? There are 2 ways: either\nprovide me an app that leaks in a repo for me to run, or show that the\nhello.ru trivial rack app leaks.\nOn Mon, Jun 13, 2016 at 2:42 AM ThomasCelen notifications@github.com\nwrote:\n\nHi all,\nFor some time I've been having a memory leak in my application. The\ncurrent setup runs with Ruby 2.3.0, Rails 4.2 and Puma Webserver puma\n2.15.3 It is recommended by Heroku to use Puma, however a lot of debugging\nhas lead me to believe it's actual Puma Webserver which is the cause of all\nthe troubles. During debugging I've removed most of the gems, code etc but\nthe issue kept showing up, RAM Memory increasing until an out of memory\nshowed up. This is why I'm 99,999% sure the leak isn't from my application.\nNext I installed \"Thin\" Webserver on my Development system and also this\nwas not a solution. Then I also upgraded my ruby version to 2.3.0 and the\nleak was immediately solved, RAM usage stayed ok.\nDue to this evolution, I upgraded my production system to Ruby 2.3.0\n(thinking it was only a ruby version issue) but still I'm having the memory\nleak. I'm now at the point that it seems to be a combination of my Puma\nWebserver, together with Ruby 2.3.0...\nDoes anyone of you have any experience with Puma giving memory leaks with\nRuby 2.3.0.\nThanks in advance,\nT\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1000, or mute the thread\nhttps://github.com/notifications/unsubscribe/AAAAB1ajo4kVTcaph6A-3xrj_8LJtvRCks5qLSYGgaJpZM4I0J_o\n.\n. @ThomasCelen Sorry for the delay. I don't see the repository... Where is it?\n. @ThomasCelen Do you have any steps to reproduce the issue?\n. @ThomasCelen Just leave it idle? Do I need to send it request? To what URLs?\n. @ThomasCelen Alrighty, I'm looking now.\n. @ThomasCelen I'm still looking at the issue, but in the mean time, can you disable preload and move your puma_worker_killer out of the initializer and into the puma config file detailed here: https://github.com/schneems/puma_worker_killer\n. @ThomasCelen do you have any stats/graphs of the memory usage you're seeing? I've running some tests and I do see it increase in small amounts but it appears to drop back down once a GC is done.\n. For example:\n\n[90336] PumaWorkerKiller: Consuming 169.87890625 mb with master and 2 workers\nT_OBJECT=18 T_STRING=194 T_ARRAY=120 T_HASH=12 T_DATA=68 T_IMEMO=30 \n[90336] PumaWorkerKiller: Consuming 169.8828125 mb with master and 2 workers\nT_OBJECT=9 T_STRING=109 T_ARRAY=63 T_HASH=7 T_DATA=34 T_IMEMO=15 \n[90336] PumaWorkerKiller: Consuming 169.94921875 mb with master and 2 workers\nT_OBJECT=9 T_STRING=109 T_ARRAY=63 T_HASH=7 T_DATA=34 T_IMEMO=15 \n[90336] PumaWorkerKiller: Consuming 152.5703125 mb with master and 2 workers\n[90336] PumaWorkerKiller: Consuming 135.046875 mb with master and 2 workers\nT_OBJECT=18 T_STRING=194 T_ARRAY=120 T_HASH=12 T_DATA=68 T_IMEMO=30 \n[90336] PumaWorkerKiller: Consuming 135.08203125 mb with master and 2 workers\nT_OBJECT=9 T_STRING=109 T_ARRAY=63 T_HASH=7 T_DATA=34 T_IMEMO=15 \n[90336] PumaWorkerKiller: Consuming 127.75390625 mb with master and 2 workers\n. @schneems he is running PumaWorkerKiller from config/initializers with preload on. Pretty sure that should absolutely NOT be done, correct?\n. @ThomasCelen Please update to 2.3.1 and see what the memory looks like there. There are definitely slow memory growth issues with older versions of ruby and they should be corrected in 2.3.\n. @jjb It can't hurt to go into the official heroku plugin. @schneems thoughts?\n. If it's the same issue as #957, you can configured the persistent_timeout now. Please try that and if it doesn't work, reopen this issue.\n. Hm, ok. I wonder if perhaps on a worker exit persistent sockets aren't being closed. That's not an issue for normal HTTP sockets because the process exit will close the socket, but would be an issue for SSL sockets because the close actually sends data back across telling the other side that it's closed. I'll investigate.\n. Nope, close is called on all sockets. Oh, are you using actioncable or websockets or something else that perhaps uses hijack? If so, I'm betting they're not closing the sockets down on shutdown...\n. Thanks so much for the reproduction! I'm working on it now.\n. @mattyb That should fix your problem. Do you have a way to try it out easily before I release the fix?\n. Hi @alexbrahastoll,\nI'm happy to clear that up and I also need to do a better job explaining it in the README!\nBasically, because MRI's threads do not run in parallel (but do run concurrently) the max throughput of a process is capped by how much IO and such the app does. The reason for that is waiting on IO (talking to a DB, etc) will allow another thread to run.\nTo bump the throughput up, you need to add additional workers. The workers DO run in parallel but obviously consume additional memory to do so.\nYou'll have to do some experimenting with your app to find the right balance, but a good rule of thumb is run as many workers as you have cores in the computer (so long as you have enough memory that is). Then ratchet the number of threads up. If you're coming from Unicorn, I suggest people start with half the number of processes used in Unicorn and 2 threads per worker. That will save you half as much memory as Unicorn and likely your throughput will be the same.\nOnce you're happy with that, begin to experiment until you find a throughput to memory ratio that works for you.\n. The thread pool of your database needs to be at least as long as the number of threads puma is configured to use. Additionally, you need to be sure that the connections are returned to the pool when a request finishes (rails handles that for you but older versions have had bugs where connections were not returned).\nAlso consider turning on clean_thread_locals in your puma config, that can help because it will make sure that Thread locals (where database connections are sometimes stored) are properly cleaned up.\n. @jjb It's been around a while. At some point I'll probably make it the default even though it has a minor performance implication. \n. Could it be because you're passing the socket in the write position for IO.select? Seems like doing IO.select([sock], nil, nil, timeout} is what you want, which will detect if the socket is ready for read, which is what connection signals generally.\n. @sgringwe Ok. I'm at a loss what is going on. Are you sharing the connection across threads accidentally maybe?\n. There is really no difference. The features that require cluster mode (phased-upgrade) require multiple workers to work, so it's really unnecessary.\n. Are you getting any errors on the app side? Those errors are just on the nginx side indicating that there was an issue talking to puma.\n. Does the ruby app show any errors?\n. I'm not sure what is going on here. Basically if nginx is reporting a 502, I'd expect the puma side to also be showing some kind of error. Are the workers getting restarted or killed is some way maybe? Are there OOM entries in the system logs?\n. If this is still ongoing, please reopen.\n. @keithpitt Do which part out of the box? \n. Sorry about that! I'm going to have a message sent to STDERR when that happens now so hopefully that would help you.\n. How can I replicate the issue? What is your puma config? This is most likely a rails issue, not something that puma has control over, but perhaps you're loading the app before the prune talks place. \n. I'm not able to replicate the issue with this config on a new rails app. What ruby version are you using? I have both 2.0.17 and 2.1.0 installed and the rails app starts fine with and without prune_bundler.\n. What if you don't do gem update --system or gem update? Does it still happen then?\n. Yes, you use whatever jruby env vars exist for this (pretty sure it's JAVA_OPTS).\n. Yeah, I'm not sure how/if I should fix this one. Puma uses fork to launch the workers so there is really no way to pickup a new version of ruby. If you upgrade ruby, you need to do a full restart.\n. You need to add Rack::CommonLogger to your rack middleware stack.\n. Basically, either use -b 0.0.0.0 or have something figure out the IP of the interface that you want to listen and pass that. Most people do the former.\n. Oh geez. Ok, I\u2019ll fix this asap and have 3.5.1 out.\nOn Wed, Jul 20, 2016 at 2:46 AM Alexandr Kovtunov notifications@github.com\nwrote:\n\nDowngrading to 3.4.0 works as charm\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1022#issuecomment-233903207, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAByheOt_CQU9hu8-pmogNxalForXIks5qXe6AgaJpZM4JQmLn\n.\n. @AKovtunov i'm curious, what does your /etc/hosts file look like? I'm wondering why localhost returned 127.0.0.1 multiple times..\n. Strange that ENV[\"PWD\"] doesn't work there. I'll use Dir.pwd on cgywin.\n. @fera2k My pleasure!\n. You can set worker_timeout to some comically large value, say 1,000,000,000 and that should do the trick.\n. @banister My pleasure.\n. This just forces puma to listen on your devices public address. I'm not going to have puma always do that. If you want to do that, most folks will just have it bind to 0.0.0.0 so that it can be accessed remotely.\n. Er, your devices non-localhost addresses.\n. Sounds more like that gem is creating sockets and not cleaning them up. How\nare you using that gem?\nOn Fri, Jul 22, 2016 at 4:56 AM Marco Colli notifications@github.com\nwrote:\nAt the moment is an hour that I have disabled the LogStashLogger gem and\nPuma seems to work.\nMy guess (it's just a guess) is that if an exception is raised by the gem\nand propagates up to Puma, it will try to start the application again, thus\ncreating the same error again and again until the socket limit is reached.\nHowever I think that Puma isn't managing that situation properly: Puma\nshouldn't create that many puma.sock (until it eventually reaches the\nlimit).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1026#issuecomment-234525203, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB2_f7IxrXDQSyQmHSDYL1gLu-u87ks5qYK_7gaJpZM4JSowv\n.\n. Looking at the lsof dump from the other issue, that is pretty odd. Did you restart puma a bunch of times? How long was it running? The sockets there could easily be client sockets connected from nginx as well, how many worker_connections did you configure in the nginx config?\n. Ah. So the other gem basically locked up all the Threads. Nginx kept sending in requests and puma was just queueing them internally, creating the proliferation of descriptors for puma.socket.\n\nPerhaps puma needs to limit it's internal queueing in this case, so that if the threads are really locked up, it's not making the situation worse by continuing to accept connections. On the other hand, the situation is pretty dire it sounds like, it's unlikely those Threads were going to come back anyway and so even if puma was limiting it's internal queue, the process was probably dead.\nLimiting the internal queueing would allow nginx to see that puma could no longer accept connections and deal with the situation (though not sure what that remediation would be either).\n. There isn't a way to cleanup all the thread's sockets though the GC will do that anyway, closing sockets when they're garbage collected.\n. I'm going to close this issue while I consider if/how puma should limit it's internal queueing.\n. Fixed! Thanks for pointing it out, I agree (and thusly followed suit with the code change) that it should catch more exceptions here.\n. I've done that before though I'm generally against it because it also catches NoMemoryError, for which there is really no recovery and the process needs to just die.\nThough, ThreadPool does rescue Exception so, actually, I should probably change it to Exception.\n. Sorry about that! #968 incorrectly set the default to VERIFY_PEER, which is a really bad default for a server since most users are NOT using client certs (which is what this controls).\n. Does systemd need a pid file? If puma runs in the foreground it shouldn't be necessary, right?\n. @t27duck I did start that direction but wanted to get someone that has a production unit to at least weigh in.\n. @dekellum If you feel that the doc is fine (and it's fine by me), then I'll add docs to point people do it more readily.\n. It's not actually rdoc format, it's adhoc. I'd rather convert it to\nmarkdown actually.\nOn Mon, Jul 25, 2016 at 5:33 PM Stefan Wrobel notifications@github.com\nwrote:\n\nHistory.txt appears to be in rdoc format, so it should have that\nextension. Not having an extension breaks changelog parsers like vandamme\nhttps://github.com/tech-angels/vandamme\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/puma/puma/pull/1031\nCommit Summary\n- Rename History.txt to History.rdoc\n- Update History filename in Manifest.txt\n- Merge pull request #1 from swrobel/patch-3\nFile Changes\n- R History.rdoc\n  https://github.com/puma/puma/pull/1031/files#diff-0 (0)\n- M Manifest.txt\n  https://github.com/puma/puma/pull/1031/files#diff-1 (2)\nPatch Links:\n- https://github.com/puma/puma/pull/1031.patch\n- https://github.com/puma/puma/pull/1031.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1031, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAABwBM9aPr94A_i2nC6ejZf4umqocoks5qZVXHgaJpZM4JUrbi\n.\n. No there is no puma imposed limit. Those errors are from your OS, basically that machine can't handle that many workers/threads. \n. There is no set max, it's a matter of resources, mostly memory in the\nsystem. It is also influenced by file descriptors, but I don't think that's\nan issue here.\nOn Thu, Jul 28, 2016 at 7:04 AM Daniel Naves de Carvalho \nnotifications@github.com wrote:\nAny way to check max number of workers/threads my OS supports or how to\nincrease that number?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1033#issuecomment-235904006, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB4sW0PZo_QOzNWEv7J2WcG3Ms5-hks5qaLb3gaJpZM4JWLRD\n.\n. I don't have any stats handy on max worker/threads because it varies so much by app and machine configuration. But you're definitely running out of resource, as the background shows with lines like Resource temporarily unavailable. I don't have much else to do give you to go on because puma is doing it's best to fulfill the numbers you want, but the machine just doesn't have the resources.\n. The other issue is most likely somehow 2 rubygems are getting loaded into the process. Does this happen everytime for you or was it just this once?\n. Yeah, so Puma has a work around for this issue. Basically demonization in jruby is really hard because the JVM works very hard to make it difficult. Puma allows it use some funky exec techniques. These techniques cause the issues you're seeing though.\n\nSo, to get around this, use the env var PUMA_JRUBY_DAEMON_OPTS instead of JRUBY_OPTS. Puma will detect that env var and move the options to JRUBY_OPTS in the final process only, making the JVM happy.\n. Oh, quite right. I'll get that fixed and released tomorrow.\n. Those paths are set by a puma configuration, not used from the rails configuration. I'll agree though, the init scripts need some work and documentation. I'll add that to my todo list.\n. It's probably run under the wrong user and without the rbenv bits loaded. You'll need to add the bits to load rbenv to the init scripts.\n. The number is highly volatile and requires additional mechanisms to have the the master and the workers communicate. I'll put this on the feature request bin.\n. I'll leave some comments here as well as on the Rack issue. I'm all for adding support for this, mostly because currently the websocket support is wired in a sort of weird way for puma anyway. Puma already has all the infrastructure to support evented websocket support so I'm all for it.\nIn fact, I might spike an implementation shortly.\n. Yeah, I need to fix that test, it's racey.\n. Do you have the stdout and stderr of puma available? Usually when this happens it's ruby crashing because of a bad C extension.\n. @wdrury That looks like puma is being told to quit. You might want to check for a bad log rotation script?\n. @CaptainStiggz Can you paste in your config file?\n. @CaptainStiggz Ok, thanks for the config. That all looks fine so my guess is you're getting a process crash due to a bad extension. Since you're in production, I'd suggest uncommenting the workers line and using at least 2 workers. That will at least shield you from the crashes a little because the other worker will be able to handle traffic while the crashed one is automatically restarted.\n. Did you update to the latest version of puma-dev? That was the fix for that issue.\n. Can you run 'puma-dev -V' and also check the full path to puma-dev running\nvia 'ps'\nOn Wed, Aug 17, 2016 at 2:26 PM Ryan Blakely notifications@github.com\nwrote:\n\n@evanphx https://github.com/evanphx yes I installed the latest version\nof puma-dev and received the error above\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1046#issuecomment-240554032, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB4QugF7mxINHwTUvs3aq-h_wlzADks5qg3yNgaJpZM4JjcxD\n.\n. @ryan-ol Could you run ps ax | grep puma for me? Puma-dev isn't showing up at all in the normal ps because of the way process sessions work.\n. You have something very odd going on with spring because puma-dev isn't using that mechanism anymore, no should the app be launched via rails s which it appears to be in the backtrace.\n\nOh, I reread your docs and you're using rails restart. I'm not certain that will work properly, try using the native mechanism of touching tmp/restart.txt.\n. Thank you for the details, i'll go about reproducing and getting some data shortly.\n. @schneems I'm starting to wonder if it's something about the malloc that heroku is using. Could you tell us a little about how the heroku ruby's are compiled?\n. @robinsingh-bw Is that app receiving any traffic?\n. No jemalloc or anything?\n. Hm. ok. \n. @robinsingh-bw I'm confused why it doesn't shutdown due to being idle then...\n. @robinsingh-bw Ah ok. I'm adding a curl every few minutes to keep the free one up and testing now. RSS creep like this is likely the behavior of malloc(2) rather than something inside ruby/puma based on my experience.\nIn your regular app, do you see memory grow and grow until the instance hits the ceiling?\n. @robinsingh-bw My concern is that this is actually malloc fragmentation rather than a leak in puma.\n. So I've been running a dyno for the last week and there memory has not grown out of control. It's a sawtooth pattern but I never saw it go above 65MB and generally started around 62MB.\n@robinsingh-bw On the smaller one, did you see the memory actually grow out of control? Or was it just the other app?\n. Why does heroku restart it?\nOn Mon, Sep 5, 2016 at 12:49 PM robinsingh-bw notifications@github.com\nwrote:\n\n@evanphx https://github.com/evanphx It just happens with the other app,\nsmaller one just goes up ~10mb before heroku restarts the dyno.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1047#issuecomment-244804440, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAByeIY1o81UFdNzxxkfKf6HsR0SzPks5qnHJfgaJpZM4Jj6T9\n.\n. Ah, gotcha. On the app, how quickly does the memory ramp up and what size\ndyno are you using?\nOn Mon, Sep 5, 2016 at 12:51 PM robinsingh-bw notifications@github.com\nwrote:\nIts the 24 hour rolling restart.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1047#issuecomment-244804658, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB05h09yndeYCYgVe1Gfq4BZjjxEgks5qnHLdgaJpZM4Jj6T9\n.\n. That kind of growth doesn't seem outside the norm how malloc(2) will hold on or release memory back to the system.\n\nThe long and short of it is that I've audit the puma codebase many times now looking for memory leaks using all kinds of tools. So at this stage I'll have to say that this is MRI's interaction with malloc(2) and how malloc(2) is able to release memory back to the system. Because MRI uses malloc(2) to allocate the memory for most data structures, it's susceptible to fragmentation.\n. @robinsingh-bw No problem on this end. I'm going to continue to investigate because I don't like saying \"sorry not, my problem.\" Did you try the malloc arena setting recommended above?\n. You don't \ud83d\ude04. There is no mechanism to do so and you generally shouldn't be doing that anyway.\n. No thanks. I like History. \n. We don't have any specific docs at the moment, but most folks get a good base image with their preferred ruby and just bundle install into it. There are probably some tutorials elsewhere.\n. I'm totally willing to give it a look. If it's too specific, we could probably build some hooks in for you to be able to implement the logic properly.\n. Being MRI only isn't a blocker, we'll just need to to add JRuby support before releasing.\n. Hm. Can you gist the output when doing this? The pid file should be removed, yes.\n. I believe that means you already have something listening on that port, not that it's not working. Can you check if there is something else listening on 3000?\n. Oddly those line numbers don't match up but I agree, there are some large Hashes in const that can be cleaned up. Let me see about moving where it's required as well.\n. Glad to know that will help sidekiq processes!\nOn Mon, Sep 5, 2016 at 12:43 PM Mike Perham notifications@github.com\nwrote:\n\nSeems like I need to move my APIs to autoload to minimize the memory\noverhead of unused features.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1063#issuecomment-244803859, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB2AQscRZHid4cNdyIJ1mIXM-qRjrks5qnHDGgaJpZM4Jxx5-\n.\n. It's only autoloaded by puma.rb. All other usages will load everything as\nsoon as it's clear that puma is being used (rack handler, cli, etc). This\nchange just prevents puma from loading everything when Bundler requires it.\nOn Mon, Sep 5, 2016 at 1:47 PM Richard Schneeman notifications@github.com\nwrote:\nFor apps that need the constants loaded, moving to autoload can increase\nmemory since it's no longer taking advantage of CoW.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1063#issuecomment-244809826, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB22PE-gFbJQFYr-6N0zhBkxwmOUJks5qnH_5gaJpZM4Jxx5-\n.\n. But why should this be done instead of the current code?\n. Ah ok. Your update makes much more sense! Thanks!\n. How does this fix the warning?\n. Hi @janko-m,\n\nThere are 2 fixes to make this work, one long term, one short term:\nShort term: An option that will control passing the body even if the connection was closed while reading it. I'm totally open to this and it would fix your use case.\nLonger term: Make the request body into a streaming reader so that it's not read at all until you start doing so, which will mean you'll be able to implement your logic on what to do. This is a bit of work and it's a little confusing for users because they're not expecting the #read from the body to return an IOError.\nI'm happy to implement the short term one now, are you fine with that?\n. It's only launched internally, it's not meant to be run directly.\n. I don't see an error here. What is the issue?\n. @junaruga Any update on this?\n. That output is not an issue, it just means that you don't have OpenSSL\ninstalled and puma won't have SSL support. Why do you think it's an issue?\nOn Mon, Sep 19, 2016 at 5:57 AM Jun Aruga notifications@github.com wrote:\n\nHi @evanphx https://github.com/evanphx\nI found the way to reproduce above error easily.\nmkmf.log is created from below command.\n$ ruby ext/puma_http11/extconf.rb\n$ grep error mkmf.log\nconftest.c:13:57: error: \u2018BIO_read\u2019 undeclared (first use in this function)\nconftest.c:13:57: error: \u2018SSL_CTX_new\u2019 undeclared (first use in this function)\nCould you check this?\nThanks\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1074#issuecomment-247985765, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB1BsHSp2VB-7T_0i3ROp9yRJkA48ks5qrobSgaJpZM4Jz8U-\n.\n. What version of OpenSSL? I'm trying to figure out why BIO_read is missing....\n. Wait wait, so does SSL support work or not?\n\nOn Mon, Sep 19, 2016 at 10:24 AM Jun Aruga notifications@github.com wrote:\n\n@evanphx https://github.com/evanphx you haven't seen this message in\nyour environment?\nOpenSSL version is 1.0.2h (openssl-devel-1.0.2h-1.fc23.x86_64)\nI am using Fedora 23.\nAnd it is the latest version of master branch.\n$ git log --pretty=oneline | head -1\n0b3626091e7ca7013312586a54a2f6c150f861b7 Provide write as <<. Fixes #1089\n$ gcc --version\ngcc (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n$ ruby -v\nruby 2.3.1p112 (2016-04-26 revision 54768) [x86_64-linux]\nBy the way $ bundle exec rake compile would succeed in spite of the\nmkmf.log message.\nAnd also all the test was passed except the issue #1044\nhttps://github.com/puma/puma/issues/1044 .\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1074#issuecomment-248060061, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAABwdV6zNVyAymVo8NTNcieQsqSnaTks5qrsVbgaJpZM4Jz8U-\n.\n. Oh. Don't worry at all about mkmf's log, those are just tests being run.\n. There is no logging by default with puma in production mode, it's whatever the app uses. You'll need to see how lotusrb is logging to answer this question.\n. @adamdavis40208 What is your whole puma config for when you have the issue?\n. Sounds like there is a threading issue in your applications code that is causing the hang. What is the app?\n. I believe this is wrong because it remove the call to Thread#raise when 0 is used.\n. The tcp there just means it's bound to that port and will be doing http over it.\n\nTCP mode (I'm realizing now that the name is confusing) is a separate mode for using puma to run non-HTTP apps entirely.\nNow, the real issue here is that you can't connect. If you typed tcp://0.0.0.0:9292 that won't work. Does http://127.0.0.1:9292 work?\n. @sepastian You didn't indicate if 0.0.0.0 or localhost work with puma. Do they? It sounds like this might be a  IPv4/IPv6 issue.\n. That's really odd. What about using just a hello world rack app? Does that work? Try this: https://raw.githubusercontent.com/puma/puma/master/test/hello.ru\n. Could you be a bit more specific with what the issue is? Are you saying that when you request a phased restart nothing happens?\n. Does the log have any indication what happened? I don't have anything to\ndiagnose what you're seeing.\nOn Sat, Sep 17, 2016 at 8:16 PM James notifications@github.com wrote:\n\nwith the pumactl command for restart nor phased-restart,\nit seems all instance are shutted down (not sure forcefully or\ngracefully),\nbut it does not spawn new instance\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1093#issuecomment-247822300, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB0JgMhXgONAD2IiC17UvQICUeq-tks5qrK0kgaJpZM4J-oTi\n.\n. Can you please gist the output of puma when you perform a phased restart? It should not stop all the workers at once, it should be doing them one at a time.\n. Ok, that makes sense as requiring relative to the top-level dir, when it's not a normal ruby dir, is a little weird.\n. No, it's not connecting to test.com, it's just sending that as the Host header. There isn't really a reason to be doing that either, that's just a really old test.\n. You can not use those when running it via Rackup, because puma doesn't have control of the process so there is no ability to do that. Is there a reason you're not launching your app via puma?\n. Ok, I suggest you use Puma::Launcher instead of Rack::Server then. You have access to the Puma configuration DSL that way and can set all those things.\n. You can control that output by passing events: Puma::Events.strings to Launcher.new. You can see how the events are created here: https://github.com/puma/puma/blob/master/lib/puma/events.rb\n\nPuma's internal output all goes through an event object, using a default event object that uses stdout.\n. This must be a JRuby bug? /cc @headius \n. Oh, I didn't think that's what was meant by that. If it fails under MRI, do you have a backtrace or error output? MRI doesn't have a \"problem when accepting\" error I've seen.\n. How are you starting puma? It appears that the bind param in the config file isn't being picked up...\n. Ah yup, there ya go.\nThere was a change so that the command line overwrote the config file proper in 3.x. You've specified you want it to listen on TCP port 5000 AND the bind param.\nIf you remove the -p 5000 from the upstart script, it should work fine for you.\n. @fidalgo You can't send it TTIN in single mode, so it's basically an invalid signal, that's why it just exits.. The code in the ThreadPool that's sleeping is just the autotrim functionality. It checks the size of the pool at a fixed interval of seconds to see if it should be sized down, so it doesn't indicate a problem with performance at all.\nThe code blocking in connection_pool usually indicates that the size of your connection pool doesn't match the size of the thread pool. Those should be matched otherwise threads will wait for a connection (which is what you're seeing here).\n. Sorry for the delay on this. Can you guard the test to not run on jruby? It doesn't run properly there and we don't need it tested there atm.\n. You're trying call out to Rails in your puma config file but you don't have rails loaded/preloaded. Can you paste your config file?\n. If the jruby SSL impl requires nothing else, this should be fine!. @nateberkopec That's my fault. We're missing tests in a bunch of places as you've seen \ud83d\ude04 . :shipit: . This is incorrect as it's not uncommon to allow puma to make those files when it needs them.. Oh der. I misread the patch, it's checking that the directory exists.. :shipit: . Rather than accepting and closing in a loop (which is pretty rude and can easily confuse load balancers), the right thing to do is have the listening socket closed. I thought that was already done though..... Dealt with in #1685 . Hi all, I'm suggest the fix in #1274 for now. It's not perfect but it will get y'all working again.. @djones Just to confirm, websockets not over SSL work fine?. @djones I just pushed a change to #1274 after consulting the actioncable code. Would you be willing to try the fix and see if things work for you?. @djones So I went through the actioncable code that made the changes to this code path and found that they're using a read_nonblock(data, exception: false) variant that minissl didn't support either. I've made the change to support that and pushed to that PR. Wanna give it a spin again?. @djones WOONDERFUL! I'll merge that in and get a release out tomorrow with the fixes. I'll chat with the actioncable folks about not masking these sorts of errors (which they must be).. Is there any output? Any indication of it dying? I don't have much to go on.. Totally good catch, yes, it's waiting for the request to finish uploading. This was an oversight when I added support for this, looking to add the simplest version possible to satisfy the requirement. It should be fixed.. @junaruga Rather than remove it like this, I'd be happy to accept a patch that conditionalized requiring. Perhaps if ENV[\"NO_BUNDLER\"] as the check.. Thanks!. @robertomiranda That's correct, it did cause #1154, but reverting it is not the correct fix. @schneems and I discussed the right way to handle it.. The better way is to detect if HTTP parsing began at all, and if not, then silently return. If you're using a TCP check, then there should be no data at all.. @sirupsen I left a comment about a frozen string usage above, otherwise this is fin.\nPer @SamSaffron's discussion though, I think exploring the ability for puma to advertise that the client has disappeared is totally fine and should be done separate from this PR.\nI'd prefer something more generic that checking the socket directly, like env['client_status'].call == :alive, which would be implemented by checking the socket or whatever the server things is the relevant thing (not everything will have directly connected sockets, for instance, and might use a separate signal to implement this check.\nDespite all that, it's still a separate feature than this. Make the frozen string change and I'll merge this.. That code should only be being run on linux..... Weird that some linuxes (linuxi?) don't support it...\nBasically we'd need to rescue any error seen by this and disable it for the process. I'll go ahead and make that change since this is a breaking issue.. Sorry about that, fixed!. Unsure what this is actually testing.... @mwpastore Since you use that autotune script to set the workers and threads, I can't know their exact values on the machine. Could you confirm what they were? You said 100 workers and 4 threads per work, is that correct?. @mwpastore Ok. I suspect the issue isn't really something that puma can correct sadly. What I believe is happening is that the workload is causing ruby threads to fight for the GVL because there is enough work being done in ruby, especially if the queries don't really interleave, but rather they all return results at the same time. In that case, all 4 threads would be asleep, waiting on IO, then all wake up at the same time to process the results.\nBecause wrk does not simulate a \"normal\" traffic pattern but just tries requests as fast as possible you'll see results like this. You might consider trying to generate staggered requests, which better represent a normal traffic pattern. That situation should be better.. Absolutely, leave it open. It's good for documentation. I'll try to think of a way to detect and verify my explanation as well. . The reason for the database latency spikes is that while the thread that did the query was waiting, another thread began to run. And only once the data from the query was returned did the original thread go back into the run queue. Because you're traffic load is causing all threads to run, there is just more perceived time waiting for the database to return, when really what you're seeing is thread scheduling delay. The question is: are you ok with that?\nIn the unicorn case, the worker process can monopolize the database connection all to itself, so when the database results come back, it begins to work immediately.. It's certainly possible that is happening, though less so because puma now attempts to not accept new sockets it can not work on (https://github.com/puma/puma/blob/5a7d884bdc300f477d0559181af1d52f37eaa4d3/lib/puma/server.rb#L399).\nBut that change doesn't solve the condition you're talking about, which is where a a process that keeps having 1 thread slot available and filling it as opposed to where another process has 5 and rarely fills it.\nImplement a mechanism for having only the least loaded worker accept new connections would require some sort of communication between the workers (where is none atm) or having the master process accept and then distribute the socket to one of the workers it chooses. Even in that later case, the worker processes would have to communicate their load to the master so it can make the proper decision.\nI'll take a look at how Passenger solves this.. It appears that Passenger attempts to solve this problem by having the master process accept sockets and hand them out to the clients not based on load, but in just a simple round-robin process, with the hope that each request takes the same amount of work. Ergo it's still possible for passenger to be handing sockets to backends that would not be the most efficient one. But knowing which one is the most efficient requires that master <-> worker communication about load.. Hi y'all, a few questions:\n\nYou started with a premise that is missing: why would people use async within puma?\nI presume that your reason for running a loop per thread is because the work is done in the context of the loop, is that right?. For 1, but why does puma need that? What does it provide to users? Or do you feel that it would improve puma itself without users caring.. Ok, I went and looked at async. It appears to mostly be a wrapper around Fiber, is that right? If so, that's a no-go since Fiber on JRuby isn't really performant.\n\nWhat I guess I miss is how you intend for for a Fiber to give up control back to your scheduler? For instance, when RestClient blocks, waiting for data, how do you switch Fibers to another one?. Ok, I see. While it sounds interesting, it's not something I'm looking to add to puma. The amount of things that can go wrong and the support that would generate is just too large. For a webserver that is more niche, something you personally have control over and use, sure. But this doesn't sound like something I'd want to maintain for my users.. I'm not sure how this fixes it, you appear to have removed the code that runs the data through the SSL engine.. Oh, nevermind. readnonblock is running it through as expected.. Doing prune_bundler not in clustered mode is more of a mess sadly so I'd prefer to restrict it to only clustered mode.. So I would ask that the code in rb_thread_purge_queue have a version guard be added at the top of it, that exits early if the version doesn't need this fix. The reason for this is that you're making puma depend on the exact memory layout of MRI's thread struct and it could easily break in the future and segfault.\nSo if this was introduced in 2.3 and will be fixed in the release after 2.4, we should only run this code when one of those releases is running.. Moving away from this code in master anyway.. The Travis errors appear to be from the compiler being updated, we'll need to fix those but this PR didn't cause that.. There were a couple of other changes tonight that should fix this. I closed it with the above commit because @dayudodo mentions an issue where he's getting SSLErrors which is what that commit fixes.. Can this be merged? Is anything waiting on it?. I fixed this via 49ed71ad3ea3e173a05de5c621c712549b496587 instead.. I'm going to reject this PR because it changes the meaning of SIGHUP for existing users.. This looks fine. I merged a similar PR that caused this to need to be rebased. Can you do that and then I'll merge it.. . Can you provide a backtrace for this? Never seen a frozen IO.. I'm going to say no to this change. The usage of moving to a big set of Hash's for configuration coupled with moving the most important part of the file to a different file doesn't help with understanding, imho.. That would mean sharing a global variable across 2 different processes, which is not something ruby supports.. While I appreciate this is a bug and that assets can be really tough to get right, this doesn't appear to be a puma bug.. Sadly that is a ruby bug, not much puma can do about it.. This is super good and a great idea!\nThinking through it, I wonder if the main issue here is that if all workers have at least one thread working and a client connects, it might take up to 1 second before any worker realizes there is a new client and goes off to do the accept. I could see that happening in the case of a well loaded server, with the workers sometimes happening to check only a few milliseconds after the client tries to connect and then random it taking a full second. People would see observer that externally as a random 1 second latency in requests, which would be pretty odd.\nThis is a really good idea and we just need to think through the logic to be sure that new clients are served quickly.. @wjordan That's great news! So I took a crack at improving the logic a bit and pushed the change to a f-even-accept branch. Check out the commit https://github.com/puma/puma/commit/e326b75f638ef881a9ae0bb536702952a6de5d3d.\nWould you be open to trying out my change on your production system and reporting back on the numbers?. That's incorrect, nothing defaults daemonize to true. The example file is just that, an example. The method you see in the DSL makes it so that if you call daemonize in your configuration, it means that you do want daemonization. But it is not used unless you actually call that method.. Without any other details, there isn't much we can do here.. They are all likely in tests, not in the code base itself. I've never heard of hakiri until today.. Yup, double checked and 100% of them are either rack or jruby-openssl, so nothing about puma itself.\n. There is currently no way to do that.. You can implement this yourself pretty easily by just starting a thread in the after_fork block.. You can implement this with a middleware, puma will not do it itself.. Given the age, I'm going to say that for ruby 2.0, you'll need to stay on an older puma release instead.. Unsure what a better way to handle this is. The point of the cluster mode is that the workers are restarted because they might have crashed.. Rebased and picked up the existnig fix!. In that case, we'd need to have some pretty explicit documentation about why a person would use it.\nAnd also decide if the parent process should be performing cleaning, such as terminating the other workers and closing sockets, etc.. Should be fixed with the above PR.. This sounds like a bug in ruby 2.6.. @guilleiguaran Can you give me more info about your macOS setup? I can't replicate the issue here.. @atitan I need to get more information about your setup. Can you provide your config.rb and any info about your rails app?. @atitan Sorry, still unable to reproduce it with an empty rails app. Can you send your Gemfile?. Still unable to repro. Maybe can you push the rails app up to github and I can try it exactly as you have it?. Here is what mine looks like:\n~/tmp/puma-test master* 9s\n\u276f bundle exec puma -C config/puma.rb\n[38384] Puma starting in cluster mode...\n[38384] * Version 3.12.1 (ruby 2.6.2-p47), codename: Llamas in Pajamas\n[38384] * Min threads: 5, max threads: 5\n[38384] * Environment: development\n[38384] * Process workers: 2\n[38384] * Preloading application\n[38384] * Listening on tcp://0.0.0.0:3000\n[38384] ! WARNING: Detected 4 Thread(s) started in app boot:\n[38384] ! #<Thread:0x00007fb93049b7e0@/Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/internals/thread_pool.rb:6 sleep> - /Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/rb-fsevent-0.10.3/lib/rb-fsevent/fsevent.rb:44:in `select'\n[38384] ! #<Thread:0x00007fb93049b380@/Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/internals/thread_pool.rb:6 sleep> - /Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/event/config.rb:19:in `sleep'\n[38384] ! #<Thread:0x00007fb93032faf0@/Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/internals/thread_pool.rb:6 sleep> - /Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/record/entry.rb:42:in `realpath'\n[38384] ! #<Thread:0x00007fb93032f848@/Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/internals/thread_pool.rb:6 sleep> - /Users/evan/.rbenv/versions/2.6.2/lib/ruby/gems/2.6.0/gems/listen-3.1.5/lib/listen/event/config.rb:19:in `sleep'\n[38384] Use Ctrl-C to stop\n[38384] - Worker 0 (pid: 38395) booted, phase: 0\n[38384] - Worker 1 (pid: 38396) booted, phase: 0\n^C[38384] - Gracefully shutting down workers...\n[38384]     worker status: pid 38395 exit 0\n[38384]     worker status: pid 38396 exit 0\n[38384]     worker shutdown time:   0.51\n[38384] === puma shutdown: 2019-03-19 21:37:58 -0700 ===\n[38384] - Goodbye!. Feel free to open a PR!. Thanks! looks like we picked up the same fix elsewhere.. Could you rebase this on master? There have been a few fixes around this code merged lately.. Is this still a WIP?. @headius Would you mind giving this a review?. If you're getting that error, that means that one of the servers isn't even listening on the port you gave. It was was listening and not accepting the connection, you'd like get a hang.\nIf for some reason you're making so many connections that the accept limit is reached, you might be over your thread limit. What do you have it set to? If you're making a request from puma => goliath => puma, you'll need at least 2 available threads to make that happen.. It appears you have a broken relationship between your ruby implementation and your C compiler. Ruby is added flags via mkmf that your compiler doesn't understand. I suggest you reinstall ruby and try again.. You have a broken complier / libc-dev setup. uint8_t is a system type.. There are people maintaining it here, we're all just volunteers, so please be kind.\nAs for this issue, it's something about your app startup, not puma. Switch to another webserver won't change the situation.. @MSP-Greg Given my recent merge-fest, this will need rebasing now.. This is the ruby 2.6 bug.. Yes, that output appears to be on puma's stdout, not in the response.. Open a PR to add a section in the Readme would be best!. Feel free to open that PR.. @MSP-Greg Happy to consider other options, for sure. On Windows it will fall back to IO.select I believe, which means Windows would stay as-is. Would be nice to allow Windows to take advantage of this change, but it's net neutral otherwise.. This is almost certainly the ruby 2.6.0 wait bug. Why are you doing workers 1? Might as well avoid the overhead of using the clustering in that case and just comment it out and use the single process model.. Sadly this isn't a puma bug. Your ruby implementation is crashing for an unknown reason. I'd suggest opening a rails or ruby core bug report.. Your ruby install is broken, the etc standard library module is missing.. This is the wrong way to fix the issue. We need to actually wait for the child to exit, not just try and give up right away.. Process.waitpid will never hang if the pid doesn't exist, it will raise an exception.. The reason this patch is fixing other things is because the code is no longer actually waiting on anything really, it's just doing one time attempt to reap child and if the child hasn't exitted, it gives up. I don't think that's the right behavior.. #1738 isn't right I don't think. I merged #1736.. puma does not log anythingn by default, you must use a rack middleware for that.. I don't see why we need this. Is Process.wait without WNOHANG fundementally broken on ruby now? This PR just implements a busy polling loop which is unnecessary if a blocking Process.wait works the way it should, which is that it waits until a child is finished.. This is absolutely a bug in ruby, not a behavior change. Looks like they attempted to fix it here: https://github.com/ruby/ruby/commit/9e66910b3bd85de32e95cf019ed285a36aecfd9e. I'm pretty annoyed that ruby-core has broken a fundamental and simple unix function and hasn't fixed it.\nConditionalizing it on ruby version is fine.. I'd be happy to move these under the puma site as well.. I'm going to fix this by just moving the const in question.. What's the reason to not use Yaml.load_file?\n. Why not use File.read then?\n. Yes, let's at least require 1.8.7 and use File.read. I've used File.read elsewhere as well.\n. Why is this and the ruby.rb one below explicitly loading a .jar? That doesn't seem correct.\n. Rather than requiring a file per engine, please just special case jruby and otherwise load the extension.\n. This is wrong because add_listener returns the SSLServer object, but the TCPServer object should be.\n. The hot restart feature requires that the server sockets are not closed between restarts, so that the kernel can properly queue connections.\n. We could try that but doing all that work doesn't justify you just needing to return the proper object in inherited_ssl_listener.\n. Since this is going into 2.0, let's use the new -d (daemonize) rather than backgrounding it.\n. Why is this here?\n. We can add it in elsewhere, just send a PR!\n. Please use a real block rather than Symbol#to_proc.\n. Please use a real block rather than Symbol#to_proc.\n. It's probably not best to set $0 here. It should be done inside cli.rb instead, since Server can be embedded in another program and we shouldn't mess with their $0.\n. Can you fix the formatting of this?\n. This line really hurts the performance of this acceptor loop because it means searching through all the sockets again on each iteration. In order to not penalize other platforms, I'd prefer to put this and the change on line 28 behind methods that are defined differently on maglev than other platforms. On other platforms, the methods would be noops that just return the value given. On maglev, the transformations would be applied.\n. Can you make this more than 1 line? It's super hard to read.\n. Is there a reason you removed the default value for force?\n. I'd prefer you just call @stderr.puts multiple times rather than make an array and then join it.\n. Whoops! Thanks.\nOn Sat, Aug 20, 2016 at 7:57 PM Tony Arcieri notifications@github.com\nwrote:\n\nIn Gemfile https://github.com/puma/puma/pull/1056#discussion_r75588178:\n\n@@ -10,4 +10,6 @@ gem \"test-unit\", \"~> 3.0\"\n gem \"rack\", \"< 2.0\"\n gem 'minitest', '~> 5.8'\n+gem \"nio\"\n\nI think you want nio4r here: https://rubygems.org/gems/nio\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1056/files/d8bf71a9ef09acd25932a4b528812a58bd8ac090#r75588178,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAAB1Z6Sro-20ZyMFv7RlgepDc9K2IZks5qh76sgaJpZM4JpNhI\n.\n. I think this approach is fine, no need to combine them for now.. Please pull that \"C\" out into a constant, as puma doesn't use the frozen string pragma. . I don't mind pulling it out into a variable because that value will never change for the lifetime of the process. Shouldn't be an instance variable though, just use a local.. Probably best to remove this in this PR.. Yup, done.. One of my fav underused constructs!. \n",
    "rkh": "I'm not sure if this shouldn't be just handled by a reverse proxy.\n. What needs to be done to fully support the async API currently used by Thin, Rainbows!, Zbatery and Ebb:\n- wrap the app.call in a catch(:async) and treat it like status -1 (status -1 is essentially not usable, as Rack::Lint and other middleware will go berserk if they see it)\n- add env['async.callback'] which is a proc (or anything responding to call) for setting status code, headers, and body object\n- if the body object responds to callback and errback, don't close the connection after being done with the each call, instead call those methods with a block for closing the connection (i.e. body.callback { connection.close })\n- add env['async.close'] which responds to callback and errback. these methods store the blocks passed to it and trigger them if the connection is being closed (for instance by the client)\n- make sure the block passed to each can be called at any later point in time (before the connection has been closed) and will still send out the message to the client\nAn example Rack application using this protocol could look like this:\n``` ruby\nclass Body\n  def <<(msg)\n    @each[msg] if @each\n  end\n# close is reserved by the rack spec\n  def close!\n    @callback.call\n  end\ndef each(&block)\n    @each = block\n  end\ndef callback(&block)\n    @callback = block\n  end\nalias errback callback\nend\nrequire 'sinatra'\nstreams = []\nget '/' do\n  stream = Body.new\n  streams << stream\n  env['async.close'].callback { streams.delete(stream) }\n  stream\nend\npost '/' do\n  streams.each { |stream| stream << params[:message] }\n  [201, 'message send']\nend\ndelete '/' do\n  streams.each(&:close!)\n  streams.clear\n  'closed all connections'\nend\n```\nThis is just for demo reasons (i.e. I didn't run the code, it's obviously not thread-safe, etc). A proper implementation can be found in this example or within Sinatra (used for instance in a real time chat example).\nI'm super swamped with work at the moment, but might be able to implement this (if no one else takes care of it) once we've released pre-tested Pull Requests for Travis CI.\n. Yeah, I can \"fix\" that. But otherwise streaming doesn't work on EventMachine based servers (which all the servers supporting async.callback are atm).\n. Rails 2.3.x should be supported, we might have to add docs there.\n. Sinatra is supported. It's even in the docs. I have successfully run a couple of Sinatra applications on Puma.\n. Sinatra 1.4.0 will pick up Puma automatically if it is installed.\n. If you use rackup:\nyaml\nweb: rackup -s puma -p $PORT\nIf you use Sinatra 1.3 (current release) without rackup:\nyaml\nweb: ruby app.rb -s puma -p $PORT\nIf you use the (not yet released) Sinatra 1.4 and you have puma in your Gemfile:\nyaml\nweb: ruby app.rb\n. also https://github.com/travis-ci/travis-api/blob/master/script/server\n. Merb should be supported, since Rack is supported. Dunno if it's worth adding docs for Merb.\n. Rails 1 is not thread-safe.\n. Right.\n. RACK_ENV=puma rackup .... Try running with /?a=1&a%5Ba%5D=1 as path.\n. Ah, we're on an outdated puma.\n. Still seeing this with Puma 2.1.0\nruby\nrun proc { |env|\n  [307,\n    {'Location' => '/?a=1&a%5Ba%5D=1', 'Content-Type' => 'text/plain'},\n    [Rack::Request.new(env).params.inpect]]\n}\nRan with rackup -s puma -E production.\n\n. <3\n. ",
    "jamesotron": "Keen to help with this.\n. ",
    "iostat": "Hi! Has anybody been working on this? If not, I might be able to help out sometime in the future.\n. ",
    "maccman": "Would definitely love to see this.\n. ",
    "lucaong": "Was this closed because the feature was discarded, or because a better strategy to support async request handling is/will be available?\n. ",
    "Nerian": "@evanphx Is there an example somewhere I can read about how to do async calls with puma?. ",
    "ghost": "I was able to get Puma working with Rails 2.3.14 by doing the following:\nFreeze Rails\n$ rake rails:freeze:gems\nModify action_controller in file: vendor/rails/actionpack/lib/action_controller.rb by changing:\ngem 'rack', '~> 1.1.0'\nTo:\ngem 'rack', '~> 1.4.0'\nCreate config.ru file in project root:\nhttp://guides.rubyonrails.org/v2.3.11/rails_on_rack.html\nThen instead of using script/server, use:\n$ rackup -s puma\nDisclaimer: There may be issues running Rails 2.3 with Rack 1.4???\nThese links may be of use:\nhttp://wiki.dreamhost.com/Ruby_on_Rails#Rails_2.3.5_-Rack_1.0_already_activated.28fix.29\nhttp://stackoverflow.com/questions/1814856/bypassing-rack-version-error-using-rails-2-3-5\n. Okay, thanks evan.\n. That issue is probably still valid, if that is the root cause of this problem, and if the file number puma is passing to for_fd() did not come from IO#fileno - JRuby internally fakes the fd range.\nI looked a way to fix IO.for_fd() in JRuby a while back, but never got around to implementing it.  I'll see if I can circle back to it for 1.7.2 or 1.7.3.\n. indeed, the TTIN and TTOUT are useful to know about.  Is there a wiki or doc page dedicated to signals?  unicorn, for example has a really nice page that lays all of these out in one place: http://unicorn.bogomips.org/SIGNALS.html\nCertainly happy to consult the source code, but a doc page like this would be a great reference.\nThanks @seuros for the info\n. nice; thanks @schneems ; I appreciate the documentation\n. Completely understand if this was a bit too verbose or not the preferred verbiage, but figured I'd throw this out there since these options weren't included in the sample config.\n. Great, thanks @evanphx .  Not seeing this issue with 2.9.0\n. On El Capitan, @robertjpayne's answer works.\n. I installed puma 2.11.3 good following this docs:\nhttps://devcenter.heroku.com/articles/getting-started-with-ruby#introduction\nFirst download OpenSSL. Next, extract the lzma download with 7Zip, and then extract the tar file to C:\\openssl.\nI downloaded OpenSSL for win64bit.\nBut with puma 2.12 it doesn't work.\n. Et voil\u00e0!\n```\nhave_library: checking for BIO_read() in -lcrypto... -------------------- no\n\"x86_64-w64-mingw32-gcc -o conftest.exe -IC:/Ruby21-x64/include/ruby-2.1.0/x64-mingw32 -IC:/Ruby21-x64/include/ruby-2.1.0/ruby/backward -IC:/Ruby21-x64/include/ruby-2.1.0 -I. -DFD_SETSIZE=2048 -D_WIN32_WINNT=0x0501 -D__MINGW_USE_VC2005_COMPAT -D_FILE_OFFSET_BITS=64   -O3 -fno-omit-frame-pointer -fno-fast-math -g -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration conftest.c  -L. -LC:/Ruby21-x64/lib -L.      -lx64-msvcrt-ruby210  -lshell32 -lws2_32 -liphlpapi -limagehlp -lshlwapi  \"\nchecked program was:\n/ begin /\n1: #include \"ruby.h\"\n2: \n3: #include \n4: #include \n5: int main(int argc, char argv)\n6: {\n7:   return 0;\n8: }\n/ end /\n\"x86_64-w64-mingw32-gcc -o conftest.exe -IC:/Ruby21-x64/include/ruby-2.1.0/x64-mingw32 -IC:/Ruby21-x64/include/ruby-2.1.0/ruby/backward -IC:/Ruby21-x64/include/ruby-2.1.0 -I. -DFD_SETSIZE=2048 -D_WIN32_WINNT=0x0501 -D__MINGW_USE_VC2005_COMPAT -D_FILE_OFFSET_BITS=64   -O3 -fno-omit-frame-pointer -fno-fast-math -g -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration conftest.c  -L. -LC:/Ruby21-x64/lib -L.      -lx64-msvcrt-ruby210 -lcrypto  -lshell32 -lws2_32 -liphlpapi -limagehlp -lshlwapi  \"\nconftest.c: In function 't':\nconftest.c:16:57: error: 'BIO_read' undeclared (first use in this function)\nconftest.c:16:57: note: each undeclared identifier is reported only once for each function it appears in\nconftest.c:16:32: warning: variable 'p' set but not used [-Wunused-but-set-variable]\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: #include \n 4: #include \n 5: \n 6: /top/\n 7: extern int t(void);\n 8: int main(int argc, char argv)\n 9: {\n10:   if (argc > 1000000) {\n11:     printf(\"%p\", &t);\n12:   }\n13: \n14:   return 0;\n15: }\n16: int t(void) { void ((volatile p)()); p = (void (()()))BIO_read; return 0; }\n/ end /\n\"x86_64-w64-mingw32-gcc -o conftest.exe -IC:/Ruby21-x64/include/ruby-2.1.0/x64-mingw32 -IC:/Ruby21-x64/include/ruby-2.1.0/ruby/backward -IC:/Ruby21-x64/include/ruby-2.1.0 -I. -DFD_SETSIZE=2048 -D_WIN32_WINNT=0x0501 -D__MINGW_USE_VC2005_COMPAT -D_FILE_OFFSET_BITS=64   -O3 -fno-omit-frame-pointer -fno-fast-math -g -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration conftest.c  -L. -LC:/Ruby21-x64/lib -L.      -lx64-msvcrt-ruby210 -lcrypto  -lshell32 -lws2_32 -liphlpapi -limagehlp -lshlwapi  \"\nconftest.c: In function 't':\nconftest.c:16:1: warning: implicit declaration of function 'BIO_read' [-Wimplicit-function-declaration]\nc:/ruby21-x64/devkit/mingw/bin/../lib/gcc/x86_64-w64-mingw32/4.7.2/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -lcrypto\ncollect2.exe: error: ld returned 1 exit status\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: #include \n 4: #include \n 5: \n 6: /top/\n 7: extern int t(void);\n 8: int main(int argc, char argv)\n 9: {\n10:   if (argc > 1000000) {\n11:     printf(\"%p\", &t);\n12:   }\n13: \n14:   return 0;\n15: }\n16: int t(void) { BIO_read(); return 0; }\n/ end /\n\nhave_library: checking for BIO_read() in -llibeay32... -------------------- no\n\"x86_64-w64-mingw32-gcc -o conftest.exe -IC:/Ruby21-x64/include/ruby-2.1.0/x64-mingw32 -IC:/Ruby21-x64/include/ruby-2.1.0/ruby/backward -IC:/Ruby21-x64/include/ruby-2.1.0 -I. -DFD_SETSIZE=2048 -D_WIN32_WINNT=0x0501 -D__MINGW_USE_VC2005_COMPAT -D_FILE_OFFSET_BITS=64   -O3 -fno-omit-frame-pointer -fno-fast-math -g -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration conftest.c  -L. -LC:/Ruby21-x64/lib -L.      -lx64-msvcrt-ruby210 -llibeay32  -lshell32 -lws2_32 -liphlpapi -limagehlp -lshlwapi  \"\nconftest.c: In function 't':\nconftest.c:16:57: error: 'BIO_read' undeclared (first use in this function)\nconftest.c:16:57: note: each undeclared identifier is reported only once for each function it appears in\nconftest.c:16:32: warning: variable 'p' set but not used [-Wunused-but-set-variable]\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: #include \n 4: #include \n 5: \n 6: /top/\n 7: extern int t(void);\n 8: int main(int argc, char argv)\n 9: {\n10:   if (argc > 1000000) {\n11:     printf(\"%p\", &t);\n12:   }\n13: \n14:   return 0;\n15: }\n16: int t(void) { void ((volatile p)()); p = (void (()()))BIO_read; return 0; }\n/ end /\n\"x86_64-w64-mingw32-gcc -o conftest.exe -IC:/Ruby21-x64/include/ruby-2.1.0/x64-mingw32 -IC:/Ruby21-x64/include/ruby-2.1.0/ruby/backward -IC:/Ruby21-x64/include/ruby-2.1.0 -I. -DFD_SETSIZE=2048 -D_WIN32_WINNT=0x0501 -D__MINGW_USE_VC2005_COMPAT -D_FILE_OFFSET_BITS=64   -O3 -fno-omit-frame-pointer -fno-fast-math -g -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration conftest.c  -L. -LC:/Ruby21-x64/lib -L.      -lx64-msvcrt-ruby210 -llibeay32  -lshell32 -lws2_32 -liphlpapi -limagehlp -lshlwapi  \"\nconftest.c: In function 't':\nconftest.c:16:1: warning: implicit declaration of function 'BIO_read' [-Wimplicit-function-declaration]\nc:/ruby21-x64/devkit/mingw/bin/../lib/gcc/x86_64-w64-mingw32/4.7.2/../../../../x86_64-w64-mingw32/bin/ld.exe: cannot find -llibeay32\ncollect2.exe: error: ld returned 1 exit status\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: #include \n 4: #include \n 5: \n 6: /top/\n 7: extern int t(void);\n 8: int main(int argc, char argv)\n 9: {\n10:   if (argc > 1000000) {\n11:     printf(\"%p\", &t);\n12:   }\n13: \n14:   return 0;\n15: }\n16: int t(void) { BIO_read(); return 0; }\n/ end /\n\n```\n. Everything good with puma 2.12.1.\nI can close.\n. hello @gonace, I have got the same error as your now. What procedure that you did to solve the problem?. ",
    "hron84": "It needs more support than dropping a config.ru file into RAILS_ROOT?\n. @Hareramrai you just missed a very important point of the documentation.\nThe documentation says the following:\n\nThis should point at your applications config.ru, which is automatically generated by Rails when you create a new project.\n\nThe second part of the sentence is simply not true in your case, since Rails 2.3.x does not generate config.ru automatically, you have to write one for yourself.\nI recommend you to read this thread on StackOverflow about the topic, both the opening post and the linked solution too.\n. I am not agree. This implicitly says Puma integration with Capistano is seamless. These tasks can be named later.\n. ",
    "Hareramrai": "Hi there,\nI am trying to update the our application server to use puma from unicorn. I am using Rails 2.3.18 & Ruby 1.9.3. I have followed all steps given in https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server . But I got the bellow error in our server log.\nforeman start -p 3000\n15:10:55 web.1            | started with pid 5395\n15:10:55 worker.1         | started with pid 5397\n15:10:55 monitor_daemon.1 | started with pid 5400\n15:10:58 web.1            | [5399] Puma starting in cluster mode...\n15:10:58 web.1            | [5399] * Version 2.11.3 (ruby 1.9.3-p484), codename: Intrepid Squirrel\n15:10:58 web.1            | [5399] * Min threads: 1, max threads: 1\n15:10:58 web.1            | [5399] * Environment: development\n15:10:58 web.1            | [5399] * Process workers: 3\n15:10:58 web.1            | [5399] * Preloading application\n15:10:58 web.1            | [5399] ERROR: No application configured, nothing to run\n15:10:58 web.1            | exited with code 1\n15:10:58 system           | sending SIGTERM to all processes\n15:10:58 worker.1         | terminated by SIGTERM\n15:10:58 monitor_daemon.1 | terminated by SIGTERM\nPlease help me on this.\nThanks,\nHare \n. ",
    "fbjork": "What should I put in my Procfile to use Puma on Heroku?\n. Thanks! Unrelated question, has anyone found a good number for the thread setting on Heroku? I'm using the default 0:16 now.\n. @ehoch I'd be interested in knowing your experience with Puma on MRI and Heroku. How does it compare to Thin and Unicorn?\n. @ehoch so you are running with Unicorn and threadsafe! enabled?\n. Also, why would Puma affect rake tasks?\n. ",
    "mindscratch": "here's an example starting a Sinatra app (gollum) using puma with a control server: https://gist.github.com/4664524\n. @bporterfield - I've added my vote to that JIRA issue, as I'm also in the same boat as you.\n. ",
    "guilhermesimoes": "Is there a way to set the number of threads on the Procfile, on Heroku? I've tried using\nweb: bundle exec rackup -s puma config.ru -p $PORT -t 0:4\nbut that yields me invalid option: -t.\n. ",
    "9mm": "@guilhermesimoes  did you ever figure out how to set config options? I've been trying to do this for ages and can't figure it out.... To anyone that found this thread I solved it here:\nhttps://stackoverflow.com/questions/45195708/how-to-pass-config-file-to-puma-sinatra. ",
    "nateberkopec": "You should be using bundle exec puma, not bundle exec rackup.. @prcongithub The last line of your stacktrace in the Rails router. Report it at the Rails repo, this is not a Puma issue.. @brianlaw033 This is a really old issue, please open a new one with a reproduction case.. > 70mb over 16 hours \nI agree, and maybe someone (@schneems?) can clarify here how we should expect worker and thread settings to affect memory growth over time.\nFor example, a lot of memory growth over time comes from long-lived objects, like the AR query cache. How many copies of long-lived objects should we expect to exist in a setup like @dirkdk's? Surely at least 2 - since workers of course are not sharing memory. But what about threads? Do all threads in a puma server share all long-lived objects? Is that 100% of time the case? It would seem to me this can' be true - otherwise adding a thread to a Puma server would not cost any significant amount of memory.\nIt seems to me @dirkdk (and others) should be thinking of \"70mb over 16 hours\" as really \"35mb over 16 hours\" (per worker), and maybe even less depending on how we should evaluate the memory growth of puma threads over time.\n. @tomas I cannot replicate this locally.\nWith ab -n 10000000 -c 1 http://0.0.0.0:9292/ I see no memory growth in 15 minutes, pretty much like Koichi's result above.\nTry using a benchmarking tool that can put out more requests per second, like Apache Bench. When using your provided command-line loop, I couldn't even trigger a major GC, which is likely what was leading to the \"leak\" behavior you're seeing. When using super-simple replication cases like this, you need to send more requests to trigger (major) garbage collection.\nEDIT: Confirmed on a fresh AWS instance. Sending enough requests to trigger a major GC shows this isn't a leak.\n. That Ruby bug looks like a problem in Ruby 2.2 only. This issue thread goes way back to 2.1 days, so if there is a leak problem in Puma (or Puma aggravates an existing bug in Ruby MRI) that is likely not the only thing happening here.\nHowever, that Ruby bug sounds like it could be the right area (the bug is re: stack overflows in new threads causing GC shutdown) to cause problems for Puma. However, total GC shutdown sounds much, much worse than the sort of leaks reported here (although @bbozo's does sound pretty bad). I would encourage anyone with issues in this thread to try the Ruby 2.3 preview release to see if anything has changed for them. The Ruby preview releases are usually pretty stable.\n. @bbozo You can check on GC status by calling GC.enable though that will of course, enable GC. If GC.enable returns true, GC was disabled but is now re-enabled. Vice versa for false. I don't believe there is an idempotent way to check GC status in CRuby.\nHowever, you'll note that MRI checks several variables when determining when to GC - GC.enable only toggles one of them. I think the bug linked was with a different variable, so GC.enable may still report true when toggled when this bug is present :|\n. @jrafanie Very interesting link, thanks for sharing that. \nI find Heroku's testing on MALLOC_ARENA_MAX very interesting. When MALLOC_ARENA_MAX is set to the number of CPU cores (4 or 8 I assume), memory usage is almost 2x when MALLOC_ARENA_MAX is set to 2, for only a 10% performance gain. I'm not sure why they haven't set it to 2 by default, 10% speed gains for a 2x loss in scalability/instances-per-dyno seems like an awful tradeoff to me.\n. If you're experiencing unbounded growth on Ruby 2.2+ and are using Puma in multi-process/cluster mode, you may find this gem interesting. It seems like all you have to do is drop it into the Gemfile and go.\n. Is it time to start the HTTP/2 party yet? \nHere's @tenderlove's HTTP2 webrick server: https://gist.github.com/tenderlove/79e7e0de4b2097e43356\nAnd here's his HTTP2 server with Puma!: https://gist.github.com/tenderlove/d68d9a3c4f7941192c9b\n. Not to self promote my own article here, but for those wondering why HTTP/2.0 isn't available on your application-server-of-choice yet, you may want to read this.\nTL;DR: This problem really needs to be solved at the Rack level before Puma can support HTTP/2 meaningfully. Use an HTTP/2.0 enabled CDN and/or reverse proxy to get 80% of the benefits of HTTP/2 today.\n. I'll link #1403 here, as this is probably the most concrete progress we've made on HTTP 2 features in the last year.. @jrochkind I think what you're seeing is #1200.. @jrochkind Give it a shot against master, tell me what happens.. OK, weird. Obviously I'm going to need you to figure out what that difference is, but it's clearly not the issue I mentioned.. No, we don't mind. Something in the docs for the threads dsl command would be appropriate.. People reading this may also be interested in https://github.com/schneems/puma_worker_killer\n. @liamwhite I understand your pain but is it really that difficult to maintain a fork with a single number changed?\n80% of actions that take 10,000 characters should probably be POSTs, and the remaining 20% are too edge-casey or could probably be worked around in other ways. . For Heroku specifically, there's another concern.\nHeroku's routing layer is \"dumb\" - that is, it randomly selects dynos to route your requests to. Puma's routing is a little more \"smart\", because it's basically routing requests to a \"free\", non-busy Puma worker when possible.\nI've seen huge response queueing improvements on Heroku when reducing the number of dynos and increasing the number of workers-per-dyno. For example, moving from 30 1x dynos with 2 workers each to a single Perf-L dyno with ~20-30 workers will usually greatly improve response queueing and result in fewer timeouts.\nTechnically I think Performance dynos on Heroku have 8 cores, the normal ones have 4. But again, due to routing, I've seen improvements at even 2-3x the core count.\n. hey @omitter we're fixing this via https://github.com/rails/rails/pull/28137. @gamut It can't be exactly the same because this was a Rails bug, so the Puma version shouldn't have any effect. Please open a new issue.. See #1128 and #1126, this is the same issue. @biow0lf it sounds like you don't have SSL configured correctly for Puma.\n. Maybe Puma could provide a better error message here.\n. This is super common in dev environments where sometimes people have a project on port 3000 which uses SSL, and then they switch to one that doesn't and the browser tries an SSL connection.. Changing to docs/feature to reflect that this issue remains open for providing better/more informative error messages here (\"hey, this looks like SSL but I'm not accepting SSL connections!\"). @nicolasmlv I think you have to clear out Chrome's HSTS cache. chrome://net-internals/#hsts. > Would you consider taking a PR for this?\nI think so, yes.. @ThomasCelen's graph looks normal to me. \n@ce07c3 - do you have some \"RUBY_\" environment variables set? Your graph shows you have no free heap slots, which is abnormal. I'm guessing you have some GC variables tuned to incorrect values, probably RUBY_GC_HEAP_GROWTH_FACTOR.\n. If you have both 127.0.0.0 and ::1 pointing to localhost, puma will try to bind to both and then fail. I'm not sure this is really a bug, it's doing exactly what it's supposed to do. Pointing ::1 to localhost is user error IMO, you should just not do that. LMK if there's some reason why you have to/must do this, otherwise I'm closing.. > running Ubuntu in Docker\nOh alright. I'll reopen until I get a repro. I'm looking at this in #1318.. Fixed in #1318. FWIW I think a \"thread utilization\" metric would be extremely useful for most people. I've seen lots of pumas with threadcounts that are waaaaay too high for their work.. I'm not sure what signal we could use. SIGINFO feels natural, but Linux doesn't have it.. I've done some poking around, here's what I've learned:\n- I can reproduce this memory creep on Heroku using the steps provided in the original issue.\n- I cannot reproduce this memory creep locally on macOS Sierra. This may be due to the differences between Linux and MacOS memory management.\n- I cannot reproduce this memory creep on Heroku with thin. It does seem to be a Puma-specific issue.\n- This memory creep occurs regardless of using glibc malloc or jemalloc via the jemalloc buildpack.\n- Tuning any number of MALLOC env variables, such MALLOC_ARENA_MAX or even MALLOC_MMAP_THRESHOLD etc has no effect.\n- I cannot reproduce unbounded RSS creep in any environment, though I can reproduce (seemingly) unbounded total memory creep. See next point.\n- Eventually, the process starts swapping, slowly as shown in this comment. I can reproduce this. RSS grows slightly, then decreases as memory is swapped out. RSS does not grow unbounded.\n- Locally, I have observed some interestingly different behavior between puma and thin. Logging GC.stat to the console every 5 seconds, live_slots seems to increase slowly but consistently with puma, but not with thin. So Puma is clearly creating some ruby object garbage every X seconds that thin does not. @evanphx may be able to tell us where this occurs. Aside: In fact, with thin, heap_free_slots seems to slowly increase without running a GC. I am not sure how this is possible.\n- If I run GC.start every 5 seconds, causing a major GC, heap_live_slots does not increase under Puma. So there are no managed object leaks. Also, when running major GC every 5 seconds, I can almost completely negate all memory growth.\nSo, here is my hypothesis for what is happening:\n- Puma generates some Ruby object garbage when idling, which other webservers do not, probably because they make greater use of C-extensions and manage their memory outside of the Ruby VM.\n- Because of this, Puma slowly and gradually fills up free heap slots (marking them as \"live\").\n- This Ruby object garbage is not a leak, because running a major GC cleans it up.\n- However, generating this Ruby object garbage moves memory from the virtual set into the resident set as memory locations are accessed and memory mappings are created. This is what causes the RSS growth, not any memory leak.\n- puma can decide whether or not this memory growth is a problem, but the fact is that there is no leak demonstrated by this issue, what is being demonstrated is ordinary memory behavior in a UNIX system.\nIf you do not understand the difference between resident set size, virtual memory and physical memory I would encourage you to read this. Please do correct me if my understanding is wrong.\nImportant note: Several people in this issue have claimed large amounts of unbounded RSS growth, though no one has produced a repro app for this problem. \nI used the following code to run/track GC:\nruby\nThread.new do\n  # hash = {}\n  while true\n    sleep(5)\n    # puts GC.stat(hash)\n    GC.start\n  end\nend\nEDIT: After writing this, I learned that MRI does not use MMAP, so thats why that setting had no effect.\n. A couple of open questions I still have re: what I posted yesterday\n- This \"problem\" (~10-20MB of memory growth over 24 hours or so from an idle process) is also frequently raised on the Sidekiq repo. Is what we're seeing here (and my explanation for why) the same issue?\n- Why are we seeing this slow memory growth on Heroku but not on macOS? BSD's memory management is very different from Linux, but I still don't know exactly why there's a difference in the end result here.\n- I should be able to come up with a very simple example Ruby app to replicate this behavior - it should generate ~100 or so garbage objects every few seconds w/o GC, and then we should see the same memory creep occurring.\n- Because Heroku is Heroku, I couldn't see if the virtual set size of the process was increasing or not. I should try to replicate this issue in a non-containerized Linux environment, and maybe get some better memory introspection with smaps or something.\n. > My internal theory for why this is normal is that this +10-20mb is where the Ruby process actually wants to be to be \"comfortable\" with the amount of memory it has. Ruby processes start with notoriously small heaps\nThe problem with this theory is that the number of heap pages is not increasing.\n\nAlso different OS tools might report memory differently.\n\nthis is true. on Mac I was using ps, Heroku reports <cgroupfs>/memory.stat\n\none difference could be the implementation of malloc, perhaps OS X is more aggressive about free-ing\n\nIf my hypothesis is correct, the difference is actually in the MMU/kernel, not malloc. Worth noting that using jemalloc does not change the demonstrated behavior.\n. > Can you confirm if you're using the defaults for min_threads (0), max_threads(16), and workers (0) (clustered mode disabled)?\nI was using Rails 5, which I believe has a min_threads setting of 5 and max-threads of 5 by default. Workers were still 0, so no forking.\n\nAre you using glibc 2.10+?  \n\nI don't believe the glibc version matters because this behavior can be shown with glibc malloc or jemalloc.\n. > So, on that note, I'm wondering if an idling puma is creating a small enough number of Ruby objects to not trigger the GC, but that are backed by much larger heap allocated memory blocks.\nHmm, interesting idea. \nTo simplify the reproduction of this behavior, check out https://github.com/nateberkopec/pumagarbotest which is a bare rack app that repros the original issue.\n. > The GC is not aware of the underlying size of the Ruby objects, you can create a single Ruby object backed by 1GB of heap allocated memory and Ruby has no idea, so it won't kick off a GC. It's Ruby's world, you only created one object in the Ruby heap, and there are plenty of slots leftover.\nThis isn't correct. Try the following code:\nruby\nThread.new do\n  prev_result = {}\n  while true\n    sleep(5)\n    str = \"0\" * 9999999\n    hash = GC.stat\n    diff = hash.merge(prev_result) { |k,ov,nv| 0 - (nv - ov) }\n    puts diff.reject! { |k,v| v == 0 }\n    prev_result = hash\n    #GC.start\n  end\nend\nBasically we're allocating a humongous string every 5 seconds, then diffing GC.stat with the previous run. Note how a GC is triggered every ~20-30 seconds or so.\nThis behavior of GC-triggered-by-heap-allocation is governed by GC_MALLOC_LIMIT and GC_OLDMALLOC_LIMIT in Ruby 2.3. In particular I think the GC is triggered here.\n\nSo, on that note, I'm wondering if an idling puma is creating a small enough number of Ruby objects to not trigger the GC, but that are backed by much larger heap allocated memory blocks.\n\nThat said, I think this is probably the case. Puma is creating some heap-backed objects (long strings, etc) while idling. This calls malloc (I think here) which causes that slow RSS creep we're seeing.\n. Alright, I'm 99% convinced this is normal behavior of heap growth outside the objectspace. No one's actually shown any behavior/produced a repro app that points to a memory leak. \nThe fact that the repro app only repros on Heroku (Ubuntu) and not on macOS just further proves that this is just the behavior of malloc and the kernel, not anything in Rubyland.\n. > Does the hack of just running GC.start every 5 seconds on every worker actually work?\nTo be 100% clear, you Should Not Do This in production. That is not a workaround. I only included it in the script for demonstration purposes. . @nicatronTg If a GC reduces memory usage, you don't have a leak. The definition of a leak is an unmanaged growth on the heap. If you can free memory by garbage collecting it, you've just got either poor GC settings or a weird workload.\nIf running GC.start fixes your problems, you may want to to watch my 2016 RubyConf talk about 12 ways to reduce memory usage in Ruby applications.. I'm going to close this issue. If you have an issue with memory growth on Puma please try the following:\n\nReduce your thread count from the default to something like 3-5 threads.\nChange MALLOC_ARENA_MAX.\nTry jemalloc (3.6 tends to work better with Ruby, don't use 4.x).\n\nIf you've tried all of the above and still have problems and can provide a reproduction application with just Puma that grows significantly in a short amount of time, I am happy to reopen.. I need to \"learn to C\" before I can review this properly, but the use case and the Ruby seems fine to me. Can you rebase please?\n. Ping!. tag @NickolasVashchenko. @mieko Can you rebase against master?. Closing as stale. There's not much for the maintainers to go on in this thread. We need some kind of reproduction app.. There's a 10% chance that this fixes anything, but can anyone experiencing this issue try adding:\nruby\nqueue_requests false\n... to their puma.rb files or try the latest master and report back if that fixes anything?\nIf that doesn't help, try and follow this guide from New Relic about debugging stuck Ruby processes and post any output/information you can pull out of your stuck processes.. @ryanfields sounds like autoloading deadlocks. Please try http://api.rubyonrails.org/classes/ActionDispatch/DebugLocks.html.. @shssoichiro That looks like a Rails issue, I'd report it over there. If specifying the params key like this solves the issue, that would be a red flag.. I'm going to close and lock this thread, so that this message isn't buried.\nIf you experience this behavior, try using the DebugLocks middleware in Rails. If it shows there's an issue in Puma (or shows nothing at all), please open a new issue.. Makes sense to me - clever use of 0.times.\n. Fixed by #1118 \n. TTOU works as expected, but it isn't logging anything. I'll open a separate issue.\n. Reopening after the revert for 3.6.2. . Fixed on master, just forgot about this issue \ud83d\udc4d . @fidalgo Sorry, can you expand on what you expect to happen? This issue is for TTIN signals, which should increase worker count.. Paging @schneems \n. @Dombo can you expand on your use-case? Same as @daveallie's?. > The mechanism by which puma chooses which worker in a cluster will handle a request is unclear to me\nI'm pressed for time right now and can't dig into this issue in full, but Puma uses the select system call to manage this, so $ man select may give some insight here.\n. Also, it's very relevant that your test controller using sleep. Sleep unblocks the MRI interpreter and allows other threads to do work. You will probably get different results if your controller looks like the following:\nruby\ndef slow\n    time = Time.now\n    while Time.now <= time + 20\n      # do nothing, spin CPU cycles\n    end\n    render plain: 'slow'\n  end\n. Closed by #1278. Total garbage, who would ever merge this?\n. LOLJK\n. This file is literally just an example puma config file. It doesn't actually get used anywhere. AFAICT is was linked from #1211 because the directory setting is useful.. @montdidier Can you remove the Puma version bump and Puma::StateFile commits (so leaving only b7a68791405729f5e). We're gonna fix the others separately. (cc #1138)\n. @montdidier LGTM but I don't think we have test coverage for the failure case? Can you add one?\n. Brilliant! We'll take it from here.. I'm going to say this is probably blocking on getting test_timeout_in_data_phase fixed, otherwise we have no coverage for this.\n. Heh me neither right now, sorry :) Not your fault/problem, we just need to fix that before merging.. I don't think so, it's a deadlock that doesn't ever resolve.. Alright, you know what. This is just adding a config point, your use case is valid, not a big deal. Sorry this took so long to merge.. @parhs You're running puma off master? Bless your soul, open source needs more people like you that catch problems like this \ud83d\udc4d . Should be fixed by #1120 / puma 3.7.0 (coming soon).. Excellent catch!\n. @prathamesh-sonpatki rebase please, build is green now\n. As soon as this gets merged, we'll release 3.6.1. See https://github.com/puma/puma/projects/1\n. I think this is probably more appropriate as a separate gem, since it's platform-specific and fixes problems caused by other gems.. Yeah. Puma's doing the right thing here, webrick is wrong. See https://github.com/puma/puma/blob/f5f23aaac7aaccff1b6b138d93dd4b1755ebf1c2/ext/puma_http11/http11_parser_common.rl#L13\n. @edwardmp You may be experiencing #1085.. Feel free to comment/reopen if this is still occuring, but we've done a lot of bug squashing in this area in the last year.. Fixed in 74e791a93d\n. @dawidof Did clearing cookies/history work for you, like you said in #1128 ?\n. @dawidof Try asking Gitter for a faster response. You probably just have a mistake in your config.\n. I think what may have happened is previously you were hosting a site at localhost:3000 which was served over SSL (or maybe had HSTS enabled). Your browser is attempting to open a connection to localhost:3000 over SSL because of that.\n. Closing as stale. The fact that you can't find a ruby C api function here makes me think this is a problem with your build environment, sorry.. Lucky for you, my Rubyconf talk was exactly about this. Here are the notes from that talk: https://gist.github.com/nateberkopec/2b1f585046adad9a55e7058c941d3850\nIs there a reason you're running three separate app servers instead of running Puma in clustered mode?\n. \ud83d\udc4d 145mb isn't bad to be honest. Check out the suggestions linked and see if any make any difference, but you're currently already at \"above-average\" memory performance for a Ruby web process.\n. Ah, I have no idea how that benchmark was conducted but it was probably just conducted with a bare Rack app, which your apps definitely aren't. Your Mileage Will Vary.\n. If I'm reading this correctly, this should be reproducible by just serving a 0-length response over SSL? Can anyone confirm that?. Great! Sounds like somebody can write a test and we can be on our way here \ud83d\udc4d . Hm, I dunno. The datastructure isn't that large.\nFor comparison, look at the mime-types library, which is basically a huge list of constants for use by other libraries. It's enormous!\nI just don't think the size of the constant is worth the effort and extraction.\n. Aha! I figured out why.\nSee https://github.com/puma/puma/commit/537bc21593182cd9c4c0079a3936d05b1f91fe14 and #705.\n. Yeah, that already exists in Rack::Utils (which is where Puma's status codes are directly copy-pasted from).\nThe reason Puma doesn't use Rack::Utils directly is in the issues linked above.\n. I believe that would re-open #705 if I understand it correctly, because now Puma would be replacing a rack dependency with a webrick dependency, possibly breaking Rails code reloading as mentioned in that issue. \nAlso, as you already noted, the status codes in Webrick don't exactly match. If you think this is a problem, you should take it up with Ruby core.\n. I want to dig in further to #705 and the solution there before closing this.\n. Looks like it's coming to a Ruby Core near you: https://bugs.ruby-lang.org/projects/ruby-trunk/repository/revisions/58801\nWe can check if this constant is defined and  use it instead of our own: https://github.com/ruby/ruby/blob/bd73d374715ae8ca6e53ebd4a32f3ae2d6542352/lib/net/http/status.rb#L21. > Wouldn't it be best to just require the file in ruby 2.5 or higher and always use it, and keep the file as a fallback to previous rubies?\nThat's exactly what I'm proposing.\nruby\nbegin \n require \"net/http/status\"\n HTTP_STATUS_CODES = Net::HTTP::STATUS_CODES\nrescue LoadError\n   HTTP_STATUS_CODES = # manual copy of the 2.5 codes here\nend. Ha yes I think I thought require/rescue LoadError but wrote is defined? Brain fart!. Sounds good to me. It's right there in the RFC:\n\nAll transfer-coding values are case-insensitive\n. After a quick lookaround the code it looks like this is the only place we actually read transfer-encoding, so this LGTM.\n. Thanks for your contribution!\n. I wonder if this is related to #1184 . Closing and moving discussion to #1060.. Can't repro, but I haven't tried Rails yet (which it looks like you're doing, using the Rails bundler binstub?)\n. > When using Rubinius, there\u2019s currently an issue with selecting the correct version in RVM in our build environment, but only when specifying rbx as your version. As a workaround, specify rbx-2 instead.\n. See #1044\n. \ud83d\ude06  You still get your PR ;)\n\nMerging as soon as it passes...\n. I've always found travis/rvm fuzzy matching to be super buggy, so I don't even bother with it.\n. Tada!. No more tests skipped on master \ud83d\udc4d . LGTM. Praise be. Hopefully this shouldn't affect outstanding PRs too much.. just checked, doesnt look like any are affected . It looks like these were introduced in 46416cb49 but never used.. > I'd guess because the -p option is no longer as important as the value in config/puma.rb but that's definitely a breaking change.\nYeah, that's definitely not great. Unintentional side effect of #1118.. I think I'll just revert #1118 and we'll fix TTIN properly in a future release.. Fixed in 3.6.2 - thanks for your patience everyone! Just getting used to the codebase so I'm still learning \"if you push this lever over here...\". @evanphx meh no one's perfect, we've all certainly seen much worse!. @raldred Probably, 3.8 + Rails 5.1 will fix this issue permanently.. Yay, #1203 does this.. \ud83d\udc96 . He's on fire!. \ud83d\udca5 . > multiple TTOU signals can be handled at once\nCan you explain a little more about how/why this might occur?. > and maybe even by some automated process that wants to scale out by say 5 workers\nAh right, this is probably a fairly common case.\nI have no opinion on whether or not a self pipe is better or worse here. Not including a self pipe with this PR would not make the code any worse than it already stands, so let's do the changes separately.. You heard the man! MERGE!. Good point, I don't think there is at the moment.. If you could provide some feedback on #1318 please. It looks like you've already got a question going on SO, so I'm going to close this. We generally try to reserve the issue tracker for bugs and features rather than support/questions.. @olleolleolle I just added it to the commit message \ud83d\udc4d . Workers == OS processes, so no objects are directly shared between them.. Hi - we generally try to reserve the issue tracker for bugs and features rather than support. You should ask this question on gitter or StackOverflow, since this question is about general Ruby behavior rather than being Puma-specific.. I'm not super familiar with systemd, but more documentation is usually better than less, even if it's not 100% on the cutting-edge. @bdewater if you've got something to add, would love to look at a PR.. @zouqilin Please try with Puma 3.8.1, we've changed a lot of the restart code since you opened this issue.. Yeah. @jnimety let's try to line up more with what Rack does here: https://github.com/rack/rack/blob/c1437097dcdf92d53a692ca8135a3391791fbca3/lib/rack/request.rb#L186. Closing as dead in favor of #1491. @jjb Have you changed autoload_paths?. Looks like this affects LibreSSL as well.. Fixed by #1192 . Looks related to #1180 . This is probably Rails autoloading (https://github.com/rails/rails/issues/27455). Anyone in this thread experiencing the issue, please report back if running Puma with one thread fixes the problem, or if you are experiencing this issue in production (i.e. config.eager_load = true).. Okey dokey. This looks like Rails-related deadlocks.\nIf you are experiencing this issue, use DebugLocks and report to Rails.\nIf you are not using Rails, this happens for you in production mode, or if this happens even if threads are set to 1, please open a new issue.. I wonder why someone would even want the current behavior (i.e. why even make this configurable).. I'll defer to @evanphx on whether or not it's a good idea. IMO, if this doesn't break people's apps, it should be default and no need for a config point.. Paging @schneems too. > I'm unfamiliar with socket stuff, but would closing the listening socket cut off existing open connections that Puma is waiting for during the graceful shutdown period?\nNot sure, but writing a test might help clarify :). Waiting on @evanphx to either do it or give me ownership on rubygems.org.. Donezo.. Working on a release now, closing because it's merged on master.. Hi folks - this was merged here https://github.com/puma/puma/commit/7cd363f7990f956334364ccdecf0b21f5fe65143 and is available in 3.9.0+. @manishval Sounds like you're most of the way there. Open a PR?. This could be any of a million issues. We're going to need some kind of reproduction steps to help you. (Please reopen when you have them).. Praise be! Please re-open if someone experiences this on puma 3.7+. If I'm reading this correctly, I believe the setting you're looking for is queue_requests, which you should set to false. See #640. If you do this, you're removing slow-client protection and keep-alive support, so you should place Puma behind a reverse proxy such as Nginx.\n. Please do reopen if you feel the above is not adequate.. Why would you expect nginx to accept any response from Puma at all if your proxy_read_timeout is 5 and the response takes 10 seconds to generate?. There are actually two backlogs in Puma - one at the socket (which is what you have configured with bind 'tcp://0.0.0.0:9292?backlog=8' and one at each worker (the thread pool's backlog). The threadpool backlog. Disabling queue_requests disables the threadpool backlog, which seems to be the behavior you expect. Does that answer your question?. @faucct I'm taking a look at your Vagrant example r/n to see if I can duplicate the \"greater than 30 seconds\" behavior.. So I think what's happening here is nginx is timing out the requests (causing them to finish from curl's perspective), but your last request fails because Puma is still processing requests from its backlog. If you just vagrant up and time curl localhost everything works fine.. See #1200 for resolution.. Yeah @ayghor got it right, for some reason I reverted on the 3.6.2 branch but never on master. My very, very bad. Fixed by above.. Gonna push out 3.7.1 with this fix ASAP.. @remi Yes, sorry! had to have a meeting with Evan last week to discuss how the release process works, working on a 3.7.1 now.. See #1200 as resolution.. @roeintense Puma version?. Wonderful, thanks @twalpole. Taking a look.. @bikramwp see #1229.. Closed by #1200 . Dupe of #1181.. @nibygro You need openssl-dev installed.. We just discussed this. The team definitely thinks there could be a race condition here, but we're not sure we've got the mechanism correctly figured out.. \n. See #1200 for resolution.. Works for me on Puma (all versions) and Mac.\nEDIT: Could certainly be linux-specific.. Fixed on master, releasing in 3.7.1.. I'll let @schneems and @evanphx weigh in, but my gut reaction is this is a \"no\". Bundler is sort of the de facto in Ruby now, so I don't see any gain in removing it.\n. Thanks so much for a repro, linking this issue to #1202.. I think we're blocking on @schneems to implement that fix since he seems to understand the problem/solution the best, but IIRC it involved changes in Rails/Rack.. Which is being released right now! . Hey @aasmith - I'm trying your repro on latest master and it isn't working, even with Puma::Const::FIRST_DATA_TIMEOUT = 0.1. Can you take another look? Maybe we fixed this somewhere else.. Thanks!. Closing for now, but definitely willing to reopen if it can be reproduced on latest Puma.. tag @NickolasVashchenko. You have a server still running. Please spring stop and/or kill all existing puma processes.. You're going to need to give a repro app then, because the steps you provided do not reproduce.. Also you're trying to bind to a WAN address, not sure what's up with that.. Looks like a bug.. I'm somewhat partial to reducing the noise here. I don't think malformed requests are errors which require the developer's attention most of the time.. @evanphx Can you expand on \"detect if HTTP parsing began at all,\". I think I understand now. @sasso we can merge if, instead of always logging to debug, we just don't call parse_error when receiving a TCP check.\nA test will be essential here as well.. I don't think there's anything you can do in the event of a segfault. You'd need some kind of external process-monitoring solution (e.g. Monit).\nClosing because this isn't a problem w/Puma.. I'm not sure setting this to 0.0 when max_threads=1 is a good idea. Couldn't slow clients basically get perma-locked-out of a worker if other, faster traffic is constantly ahead of them?\nI'm OK with a faster timeout, but 0 seems like it could lead to a lot of edge cases. @evanphx . @davidor Can you share your test setup from here so I can repro?. Hullo Simon!\nThanks for such a detailed PR. Will take a look!. > I wonder if rack protocol should allow exposing this as well\nWhat's stopping Puma from just adding it to the environment? . after_worker_boot only runs in cluster mode (more than 1 worker).. shrug Just pointing out that it's clearly documented that this is a cluster_mode only callback: https://github.com/puma/puma/blob/f5f23aaac7aaccff1b6b138d93dd4b1755ebf1c2/lib/puma/dsl.rb#L337\nWhat's your use case for doing something after boot in single mode?. Hm, here's the output of git diff --stat v3.7.0 v3.7.1 on my machine:\nnodachi:puma nateberkopec$ git diff --stat v3.7.0 v3.7.1\n .hoeignore                   |  2 --\n .travis.yml                  |  3 ++-\n History.md                   |  6 ++++++\n Manifest.txt                 |  2 ++\n lib/puma/binder.rb           |  2 +-\n lib/puma/configuration.rb    |  8 +++++++-\n lib/puma/const.rb            |  2 +-\n test/config/settings.rb      |  2 ++\n test/test_config.rb          | 11 +++++++++++\n tools/jungle/init.d/run-puma |  4 ++++\n 10 files changed, 36 insertions(+), 6 deletions(-)\nWhich looks correct to me.. Also git tag -v v3.7.0:\n```\nnodachi:puma nateberkopec$ git tag -v v3.7.0\nobject f413b61f4ba9a453528b1ea44a0dbdd4a6f030f1\ntype commit\ntag v3.7.0\ntagger Evan Phoenix evan@phx.io 1485563714 -0800\nTagging v3.7.0.\nerror: no signature found\n```. Paging @evanphx . No problem, I agree that our history should be valid and point to objects that actually exist!. \ud83e\udd37\u200d\u2642\ufe0f Can't really fix this.. I don't think this is a Puma issue. You'll have better luck getting an answer on StackOverflow. \nI can reopen if you can provide a full repro app, otherwise we're just debugging someone else's app in the dark.. Could you please post your config/puma.rb?. @schneems is this done now?. @Fudoshiki done in a different commit 49c1466cfdb08e99265fe01ca5171bbe81f49981. LGTM \ud83d\udea2 . Now that we're the Official Web Server Of Rails (tm) we should probably test on Windows. Don't need to do the full matrix or whatever but just a smoke test of the latest Ruby on windows would be good.. I still think we should do windows tests, but based on #1241 this may not be Windows-specific.. Folding into #1241.. Are you running on Windows a la #1240 ?. It looks like sockopts vary between distros then. We should at least rescue a failure here, though finding a more cross-distro way to deal with this would be ideal.. paging @sirupsen . Please upgrade to 3.8.1.. Usually, the best practice is to put your timeout code as close as possible to where the actual slow code is. So, in your case, a timeout on ActiveRecord reads would probably be appropriate. \nThat said, rack-timeout could work well for you too.. Yes - this is a multithreading issue. Since this is not a bug with Puma, I'm closing. There are many resource for multithreaded programming in Ruby and Rails online.. Puma doesn't cause multithreading errors, it merely enables them to occur ;)\nBut really, this is probably a pg issue. Puma doesn't have anything to do with it. If you can bring a reproducible case that says otherwise, happy to take a look.. Cannot reproduce. 99% certain this is an issue on your end, rather than Puma. What happens if you send SIGKILL? pkill -9 -f puma. Also post output of ps aux | grep puma after you've tried to terminate and it doesn't work.. I'm going to need more information or a reproducible container to help you, as I cannot reproduce a failure to respond to SIGTERM. My guess if you've got something else in your app that's causing this behavior.. I'm not sure if we can really do anything for you here. Both events are logged at the same level (by the same logger). You can provide your own Logger to Puma which filters it out I suppose. I'll reopen if more people want this, but surpressing startup while wanting the request log is a little weird.. Hi @seanvree, it looks like you're having some trouble with some of your Rails/Ruby code. This is not a problem or bug in Puma. We try to reserve the issue tracker here for problems with Puma itself.\nYou should take your question to Stack Overflow, the Rails Gitter chat, or a similar venue. Thanks!. The problem is that binder.close is never called.. ^^^ What he said. Please use puma -d.. @mwpastore Couldn't TechEmpower just delay requests to create a more realistic arrival pattern?. @mwpastore I'm interested to see how the threadpool changes in 3.9 affect our benchmark results. Paging @schneems . Does this happen with a USR2 restart as well?. Can you just add a comment here explaining that it's bad to set Rack env keys to nil?. This feels like the sort of change where you think \"this couldn't possibly break anyone's apps!\" and then it totally does.. I know that's really vague (and it is strange that someone might expect the ENV to not be reverted on a USR signal restart), but it just seems like this could mess up apps. . I definitely agree that this is reasonable behavior, but I just worry about people relying on the unreasonable behavior to set up their ENV.\n. Looks like empty values are supported in the HTTP spec, so we should support them too. LGTM, can you add a test?. Somewhere in https://github.com/puma/puma/blob/master/test/test_puma_server.rb?. This isn't really an issue w/Puma as noted in https://github.com/charliesome/better_errors/issues/341. Yes, we end up including @app in the stacktrace, but that's not really our fault. In any case, binding_of_caller is basically abandonware and so this isn't likely to be fixed on their side anytime soon, so I'll leave this open.. I don't think there's anything we can do about this on our side. Recommend people affected follow https://github.com/charliesome/better_errors/issues/341.. Totally. Need someone else to review this though as I am not a systemd user.. Literally anyone that uses capistrano and Puma, if you could just give this a read and let us know if it's sane, happy to merge.\nIn the long term, maybe we could move this stuff to wiki?. Thanks!. Yes.. Re: bundler rescue load-error, I mean I think that was added as a request by debian (?) or some other package manager team that wants to run the tests w/o bundler.. The only change here I'm really ok with is the one to def hit and the Bundler rescue comment.\n\nI don't want to change the color output, minitest-pride is fine. Sorry if you disagree.\ntest_helper is a common idiom and doesn't need to be changed. . @grosser \n\n\nyou still like pride more than rg ?\n\nYes\n\nis moving to helper ok ? (\n\nAlright, it's OK.. > am i right that once a server has started, the socket type can't change?\n^^^ @evanphx . The answer appears to be \"we don't know either.\". Thanks!. \ud83d\udc4d  Needs a test though.. Ping, happy to merge with a test \ud83d\udc4d . I think #1340 is a more complete solution.. Sorry, what I meant was that the entire require test_helper line should remain the same. There's no reason to use require_relative here.. Fair enough\n. Does anything in #1189 help you out? Also, try latest master.\n. Please try with 3.9.1 and reopen if not fixed, thanks!. I think this change would be OK. Please open a PR.. This particular issue looks like the above ^^^ If config.eager_load = true doesn't fix it for you, please open a new issue.. Linking #1107. I think this could really improve performance for a lot of people, so I'm going to start looking at packaging up a new release.. \ud83d\udc4d since the Jruby bug on master was fixed, I think I can push a 3.9.0 soon.. @eprothro \n\nTo clarify, my understanding is that there is still benefit in having the queue_requests option, as this allows requests to be received without blocking a worker thread from handling on another request that has already been received. This has not changed, simply how many requests could be in this state (being handled by the queue_requests thread before being handed to a worker thread) has changed.\n\nThat is correct.. See #1072. This works for me:\nruby\nAPP = Rails.application\non_worker_boot do\n  client = ::Rack::MockRequest.new APP\n  %w[/].each do |url|\n    client.get url\n  end\nend. Do you have libssl-dev installed?. Pardon, I think on CentOS it's openssl-devel. release is on my todo for this week unless someone else gets to it first \ud83d\udc4d . Hm, do we need a version guard here as well?. This is a problem with your environment. Try gem pristine.. I can't repro this on master. \nruby\nlowlevel_error_handler do |error, env|\n  puts \"lowlevel_error_handler is being invoked\"\n  puts error\n  puts env\nend\n...followed by \ncurl -v -X P \"http://localhost:3000\"\n...and the error handler is invoked.. Pinging @schneems . > In order to test it, I forced an exception in my ApplicationController\nYeah, lowlevel_error_handler isn't called in that case. It's only called in particular cases internal to Puma.. @lucascartaxo Just search the Puma source for areas where it's called. There's not too many.. @jjb can you retry on 3.10.0?. I can't reproduce this. Can you please provide your entire config?\nIt's worth noting what that line actually does:\nruby\nTCPSocket.gethostbyname \"localhost\". All for it except the forced major on checking stats. There are times I want to do that, other times I don't.. Yeah, either that or just remove the GC.start from the PR.. Definitely already broken and not your fault.. Rubocop failures. Get rid of those filthy, filthy tabs.. For tabs? Yes, Rubocop fails otherwise.. > macOS kernel bugs that look similar to this \no great. It looks like it happens on Travis too, so probably not a kernel bug. Actually, they seem to happen on CRuby too, so not sure why it passes for me locally.. Can confirm this started after #1260 was merged.. Hey @grosser, can you take a look at these failures on JRuby 9k? Related to your PR above.. This error never occurs on our JRuby builds. Could be MRI specific, which does point to #1206, but merging that patch didn't fix the CRuby builds, so I'm not sure.. Here's @respire's proposed solution: https://github.com/respire/puma/commit/4fe40c2904433a390cab0e12ddaf631286ea66d4. Anyone experiencing this bug is advised to add https://rubygems.org/gems/stopgap_13632 to their gemfile, or wait until Ruby 2.3.5 or 2.4.2 are released, which will (probably) contain the fix for this.. @krishna-rpx Can you post the full backtrace? Also, what Ruby version?. Also, are you using https://rubygems.org/gems/stopgap_13632?. \ud83d\udc4d Thanks for your contribution.. I'm closing this because Rails 4.2 is now on \"severe security updates only\", so I don't see a point in taking maintainer time for trying to patch this for Rails 4.2.. I do this all the time, not a problem.\nFWIW you should be smart about HTTP caching and use a CDN - which are things you should be doing with Passenger or any other web server anyway.. @lkananowicz Puma and Rails are involved with serving static files, via http://api.rubyonrails.org/classes/ActionDispatch/Static.html. LibreSSL fix has been pushed \ud83d\udc4d . Yup!. So @NickolasVashchenko's description of this bug in Ruby in #1206 was definitely on the money, but I'm not sure I've got a solution yet. Please check @evanphx to make sure we can actually do this, also I need a solution for why STDERR is getting closed, which is why I had to comment it out in this PR.. @evanphx said since only IO related errors can come out of io.close, we can actually just rescue nil. . Interrupt doesnt inherit from standarderror tho, so rescue nil won't catch it.. Anyway @NickolasVashchenko want to take another swing? I'm trying to get rid of all the errors on master related to this :|. @NickolasVashchenko Yeah. As an example, look at ruby-head: https://travis-ci.org/puma/puma/jobs/237347708 and all of the \"stream closed in another thread\" errors. \nYour original patch doesnt seem to fix everything on CI any longer, and my patch here requires disabling the STDERR.puts stuff, so I'm not happy with either solution right now.. > It does not affect any of the other Ruby versions that have ever been released.\nI'm confused by this. We're seeing errors on 2.2, 2.3, 2.4 and ruby-head.. Wow, that was quick. Thanks to your thorough report, I think! I'll take another look.. Closing, let's move discussion to #1293.. Hi! I was just looking at that async-io C10K benchmark the other day. Wicked awesome stuff!\nRe: async-io, have you seen https://github.com/puma/puma/pull/1056?\nI'm not sure you one-reactor-per-thread design would really work with Puma, though.\n. @ioquatix I think ultimately the goal would be for Puma to pass a C10k benchmark (in preparation for native websocket support). I think NIO was just a stab at that. AFAIK @evanphx doesn't have any strong feeling on how we accomplish that.\n\nI'm not sure you one-reactor-per-thread design would really work with Puma, though.\n\nMaybe I'm just misunderstanding how this would work. Don't we need a reactor to manage the reactors, then?\n. Thanks! All the \"network programming\" aspects of Puma are very new to me. \"Where the reactor lives\" was recently discussed in meatspace at Railsconf. ctrl-f for Puma here for a summary. After playing with Passenger a bit during client work, I really like their \"first-unused-process\" routing approach for distributing work in a process cluster. In the long term, I'm more in favor of the \"one reactor process, many worker processes\" approach.. Sorry, I think I'm using the word reactor imprecisely, partly because the only reactor I'm familiar with is Puma's. \nWhat I mean is, in cluster mode, I'd like all of the request buffering function currently performed by Puma::Reactor to be done in a \"master process\" which is the only process listening on the socket. . > My understanding is that a Ruby thread has 256KB of overhead\nI thought it was 8? Where did you get 256kb?\n\nActually, my goal for this is zero. It should be completely transparent.\n\nWe all try to ship bug-free software, but that doesn't stop us from writing bugs \ud83d\ude06 \n\nRight, and my question wasn't how to add this to puma, but how to integrate it on that basis e.g. as a plugin. \n\nI don't think the reactor is pluggable right now. I mean, it's Ruby, you can undefine Puma::Reactor and put your own in there. Not sure if that would work for you.. @ioquatix for a microbenchmark like that, on MRI, yes, probably one process per core.\nTechEmpower gets 110k rps for a Hello World roda app on Puma. That's on 80 hyperthreads, 40 true cores.. I believe we've addressed your questions \ud83d\udc4d Do post back with any puma-related findings.. I think I did. The reactor/other internals isn't currently designed to be pluggable, so you're stuck monkey patching. You can see for yourself how limited the plugin infrastructure is, so I don't think that's going to get you anywhere.. I'm not sure I completely understand how deeply you want to modify Puma, but, for example, Reactor only has 6 public methods. If you monkeypatch that and decide it works or is interesting, I'm happy to find a way to make parts of Puma internals more pluggable and open to extension.. > Is it possible to supply a block which runs around worker threads, for example?\nTo be explicit, the answer to that is no.\nAlso, I've been concentrating on the Reactor, but I think you probably actually want to integrate in Puma::ThreadPool#spawn_thread.. Thanks for fixing this so quickly!. I did! that was failing before you, but if you want to fix that too I won't complain :D. That's annoying :( It looks like it should work, maybe we should make a report.. Works for me. I love how all the stars aligned and we didnt see any ioerrors on the CRuby parts of this built.. Creating parent/child processes may still be useful due to the copy-on-write savings. However, I think this option could reduce some of the complexity in the Reactor and improve capacity.\nWindows does not support SO_REUSEPORT.. pinging @grosser who recently touched this. @grosser how could I repro this with a rails app? rails s, bin/puma, bundle exec puma, puma all work for me.. Still nothing.. Closed by #1385. > ensure no other puma versions are around so we actually fail as described in the issue\nCI is failing on this new check.. CI is completely borked r/n because of #1206 . @grosser JRuby 9000 should have a green CI, it doesn't: https://travis-ci.org/puma/puma/jobs/238616761. Run gem install bundler, make sure bundler -v says 1.15.0, then try again.. What's the best way for us to solve this? You can't do optional dependencies, so I guess we just have to have a code path for \"bundler is defined but is old\". @wangyuan99 @matobinder Can either of you try again with Puma master and tell me if that fixes the problem for you? \nruby\ngem 'puma', github: 'puma/puma'. Alright, I'll roll up a patch release in a few hours.. Fixed in 3.9.1, out now.. @grosser makes sense to me, waiting on the test.. @grosser Shouldn't we be replacing with_clean_env in prune_bundler as well?. This seems to reliably cause the Ruby 2.1 integration tests to fail.. I'm fine with the test changes but mangling RUBYOPT just seems like a recipe for disaster.. Much better, thx.. Ah, I think I get it now. Linking #782.. I'm also going to link #1022 here, because the original fix here caused that issue, maybe I can satisfy everyone.. Looks like it's in 1.9.3, so no.. Also, JRuby returns the same result as CRuby does here, though it prints the full ipv6 address rather than the short ::1.. @grosser just a heads up, I've created a new restart tag on issues: https://github.com/puma/puma/issues?q=is%3Aopen+is%3Aissue+label%3Arestart . \ud83d\udc4d \n. @olleolleolle Unfortunately we already use TTIN to increase worker count :(. Maybe I can do both SIGINFO and a pumactl command. On prod/Linux you can use the pumactl command, on dev you can just ctrl-t or SIGINFO.. Great! Needs a test.. . New failures on JRuby and 2.1. I can repro the 2.1 failure locally sometimes:\n\n. Wonderful!\n\nLet's add this to the default rake task. task :default => [:rubocop, :test]\nLet's add this to CI. script: \"TESTOPTS=-v bundle exec rake\" rather than rake test.. Rubocop needs to be added to the 2.1 gemfile as well. It looks like Rubocop is running against vendored stuff. I think you need to add them to the excludes in both rubocop.yml and the Rakefile.. Thanks!. Let's also look at #1264 . No prob \ud83d\udc4d Thanks.. Please open PRs against master \ud83d\udc4d . Please open all pull requests against master.. If you register a handler for lowlevel_error, it will be called and you'll have access to the request environment when the parse error occurs.. Release is blocked on #1344 . No. I'm not going to release something I know is broken. You can use master branch.\n\n. > Issuing a restart when workers = 0 would result in errors like the following observable in the puma_errors.log\nWhich restart type? USR1 or USR2? /tmp/restart? And can you confirm this happens with a basic Rack app or do I need to do anything else to reproduce it?. I'm not familiar with the tool you've screenshotted there, so I can't tell you how to interpret it's output. There were issues prior to 3.9 with incorrect configurations being picked up (or defaults overriding options being set elsewhere), please do upgrade.\nAs for thread counts, it's something of a guess - w/CRuby, you want only as many threads as could possibly be used at the same time. That number is very application-dependent. Rails' default of 5 threads is sane.. Closed by #1385. Brilliant! Paging @evanphx.. > The reason for this is that you're making puma depend on the exact memory layout of MRI's thread struct and it could easily break in the future and segfault.\nDespite the excellent and heroic effort by @NickolasVashchenko on tracking down this bug and providing a fix, I'm starting to think we should just wait for Ruby to backport and fix this.\nFrom what I can tell, 2.2 will never get this backported since normal maintenance finished in March. Hey @unak, when do you think 2.3.5 and 2.4.2 will be released, with the fix for https://bugs.ruby-lang.org/issues/13632? Please say November or earlier, so I don't have to do anything \ud83d\ude06 . Thanks, @unak! I understand that you are only estimating. That's all I needed!\nWell, if it's even possible that a fix may not actually make it ever into 2.3, maybe we really do need to merge this.\n@NickolasVashchenko in that case, where appropriate, please add windows to the build matrix: https://docs.travis-ci.com/user/multi-os/ . @NickolasVashchenko Yeah, if we add it to the Gemfile, that'd be great. Would make our builds green again.. \ud83d\udc4d Goodness it feels good to see those tests all pass again! \nI'm thinking we'll remove this code about ~6-months to 1 year after Ruby releases patches for this, so sometime in fall next year most likely.. Now I have to get those Appveyor tests actually working.... The docs are pretty clear that you have to provide the redirect_stdout option. There are a limited amount of signals available, so we sometimes have to make them do a bit more work than just one thing. . For future reference, @NickolasVashchenko and I discussed this change over Skype.. Closing in favor of #1385. Hm, we already do this here. I don't understand, is there a problem with this? . Thank you!. Yeah, the error message is pretty self explanatory. You're binding to the same socket 4 times. You can't do that.. Puma expects its binds to adhere to RFC 3986. According to that, you have to put ipv6 literals inside square brackets:\nrackup -p 1234 -o [::]. \ud83d\ude05 Thanks!. I think your UNIX address for the upstream is wrong. I don't know why you have three slashes after unix:. Docs say just one is necessary, e.g. unix:/tmp/backend2;\nIn any case, this is a problem with your NGINX config, not Puma.. ```\nevents { worker_connections 1024; }\nhttp {\n  include /etc/nginx/sites-enabled/*;\nupstream demo {\n    server unix:/tmp/puma.sock;\n  }\nserver {\n    listen 127.0.0.1:8000;\n    server_name  localhost;\n    access_log /tmp/demo.access.log;\n    error_log /tmp/demo.error.log;\nlocation / {\n  proxy_pass http://unix:/tmp/puma.sock:/uri/; # match the name of upstream directive which is defined above\n  proxy_set_header Host $host;\n  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n  proxy_redirect off;\n  proxy_set_header Upgrade $http_upgrade;\n  proxy_set_header Connection \"upgrade\";\n}\n\n}\n}\n```\na config.ru\nrun(proc { |_env| ['200', { 'Content-Type' => 'text/plain' }, ['A barebones rack app']] })\nand running Puma 3.9.1 under JRuby with...\npuma -b unix:///tmp/puma.sock config.ru\n...works fine for me. I need you to give me a repository with an nginx.conf that I can run against the config.ru and puma command above to reproduce.. No. This file has a bunch of stuff related to your application that I will have to delete/modify. What I want is an nginx.conf that reproduces your issue against a basic Rack app and puma command (like above).\nI've proven that Puma works properly over a Unix socket with nginx. I can't just debug your nginx.conf for you, so please give me a simple nginx.conf which reproduces the issue.. Binding to the unspecified address won't work for your use case? puma -b tcp://0.0.0.0 config.ru. Thanks. I am not a systemd guy, so the real answer is \"IDK\"! I was just trying to see if any of the behavior we already support re: binding could fill this use case.\nFWIW, we tend to support a few more things/weirder scenarios when you use puma directly, rather than rails server.. Closed by #1377. Yeah, starting a server via Rack::Server like this and then expecting the port to be remembered is Not Supported.\nExample of something you could do:\n```ruby\nrequire 'puma/cli'\ncli = Puma::CLI.new([\"--port=12345\"])\ncli.run\n```\nAn alternative might be to use Puma::Launcher directly with a configuration object.. > Are you saying this is the expected behavior, the command line arguments are expected to be erased between restarts?\nWell, it does work when you use Puma::CLI, which is what is supported. . @perlun Puma::CLI.new takes an array of arguments: Puma::CLI.new([\"--port=12345\", config_path]). Yes. Single mode is --worker 0.\nIn single mode, there is only one process. In cluster mode, you'll see a worker and master process.. Closing this since you merged the warning in #1373.. Can do.. Thanks guys!. Fixed in #1383, I think @schneems just forgot to close.. @schneems fixed this via #1383. Wow! Awesome. Thanks, I'll take a look soon.. Sorry it took so long for me to merge this. Anyone else can open up a badge PR!. Please upgrade your Puma version. OpenSSL 1.1 support was added on 3.7 or 3.8, I can't remember.. I don't understand. You think this is a bug in Puma?. Please reopen if you think there's a bug here, but we can't debug your build for you.. Obviously our error messages kind of suck (thanks, c extensions!) but you have something misconfigured in your ssl, obviously. There's nothing more you can tell from the error message than that.. Thanks for reporting back!. Can you try 3.10?. Closing until someone has this issue with recent (3.10+) version of Puma, otherwise I believe this is solved.. If someone can get get a reproduction app going which can cause a failure to graceful shutdown, please open a new issue.. \n. Yeah, this looks like it does the same thing as #1348 with a much smaller change.\n@grosser why don't we .dup ORIGINAL_ENV. Looks like you fixed this on master . > My requests were big JSON uploads, which take some time unfortunately. \nThis could definitely have something to do with it.\nCan you share any kind of test application for this? Just a Rack app that accepts some file uploads, I guess?. Thanks! No, we're not using the Hoe stuff anymore. There is an open issue for it's removal #1328.. Linking #1133, because we've have to add 418 back when importing the stdlib's status codes.. Merging this, because at least one Ruby web framework supports 418, so if they send down a 418 via Rack we want to append the correct message. Our HTTP status code hash should basically reflect whatever the popular frameworks will send down to us. (I'm not going to just add everything Hanami does though, they've got a few in there that actually conflict with current IANA codes, e.g. 451: https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml) \nThis change has been made to ensure the highest standards of scalability, accuracy and reliability in teapot deliverability.. Not a Puma bug. Not sure where you're putting the logger config, but adding:\nruby\n  config.logger = ActiveSupport::TaggedLogging.new(ActiveSupport::Logger.new(STDOUT))\n...to development.rb, for example and everything works fine.. Hm, failing on appveyor w:\nThe system cannot find the path specified.\nrake aborted!\nCommand failed with status (1): [cd test/shell; sh run.sh...]. Looks promising, just want to get https://travis-ci.org/puma/puma/jobs/265280976#L855 fixed.. > Would this be something we could extend off the existing MiniSSL implementation currently in place?\nI think so, yes.. Sorry, this is kind of out of scope for the Puma issue tracker. What you're describing is generic UNIX process management, something that isn't specific to Puma.. If the behavior hasn't changed since 2.15.3 to 3.10, it's probably a locked mutex and not a problem with Puma.\nIn Rails, we have DebugLocks. I would give that a shot.\nIf that doesn't turn up anything, you need to dig in with dtrace. You could also try #1320. Please reopen if you've got anything reproducible.. @dnd yeah that's part of the reason I didn't merge it yet. we're kind of \"out of signals\". it's a pretty simple patch, you can change it to respond to any signal that we don't already (see docs/signals.md). DebugLocks is Rails 5+ only.. > Instead of using a signal, since pickings are slim is this something the control server could have an endpoint for and return?\nPerhaps? Haven't investigated.. The reason I was using signals was because that was the original feature request.. This kind of reminds me of 9faca801b07cfd4cda5d4bf69dafb0. > I wasn't expecting multiple instances of my rack app in a worker\nThis doesn't happen if you run Puma with 1 thread, correct?. :clap: Give me a while to look at it, headed to Japan soon so not sure how much time I'll have. Last resort we can talk about it at Kaigi? . >  I can understand if you don't want to be used for experimentation, but I would like to get this in to production systems sooner rather than later.\nalso I think I speak for the whole Puma team when I say that we have no problem with experimentation. hell Evan wanted to add native websockets and rewrite the entire reactor. so adding an optional feature...pretty tame \ud83d\ude06 . Need to look at this one more time when I'm not jetlagged but I think this is ready.. @tenderlove If I'm reading the HTTP 103 spec correctly:\n\nA server MUST NOT include Content-Length, Transfer-Encoding, or any\n   hop-by-hop headers ([RFC7230], section 6.1) in the informational\n   response using the status code.\nA client MAY speculatively evaluate the headers included in the\n   informational response while waiting for the final response.  For\n   example, a client may recognize the link header of type preload and\n   start fetching the resource.  However, the evaluation MUST NOT affect\n   how the final response is processed; the client must behave as if it\n   had not seen the informational response.\n\nSo, 103 status CANNOT contain a Connection, Content-Length or Transfer-Encoding header, but MAY contain any other header?\nIf I'm getting that right, your change makes sense. But should anyone be responsible for omitting the prohibited headers? Puma's job or someone elses?. Can you post the actual response headers you're getting from 3.8.1 and 3.9?. Also, relevant commit & test: https://github.com/puma/puma/commit/eea4dbb4379011dbb78d0a957836cff20dcebc51. 3.9 is showing the correct behavior. You set the header to an empty string, which is a valid header value. In 3.8 we omitted the header, which was incorrect.\nInstead of setting the header to an empty string, delete the key:\nruby\nconfig.action_dispatch.default_headers.delete('X-Frame-Options'). Your last comment looks almost like \"expected behavior\" if ~100 requests show up at once and each take 10-300 ms to process. The backlog will immediately get pretty long and take a while to process with a single Puma worker.. @stereobooster Ah, but you can: https://github.com/puma/puma/blob/4db1dcd28181f83bca94db41f736d3d6f6aad8ed/lib/puma/binder.rb#L102. Worked on restart. > Could it be reasonable to compile the C extensions using a compiler version which does not emit warnings? \nSorry what are you getting at here? That we don't currently emit all possible warnings and we should?\nb/c if we're not omitting warnings on CI I agree, we should.. > (I'm sorry to make you have this low-value, pedantic conversation.)\nI'm a maintainer, my job is to have many low-value conversations over and over UNTIL THE END OF TIME \ud83d\ude06 \nI don't think there's any value compiling with things people don't use anymore.. > then \"what Travis defaults to\" could be said to constitute usage\nMeh, but our true usage is \"what people bundle/gem install on\".. This warning is related to https://github.com/puma/puma/issues/1421 and the reason why creating threads before forking is dangerous is explained here. I believe the warning to be valid here. \n. I'll try to repro this, but there about 1 million variables here so it's not going to be useful, probably.\nFrom Puma's perspective, wrk on an external address is the same as wrk on a local address, so whatever is happening here is almost certainly outside of Puma's responsibility.. Thanks!. Merged in a different PR, but thanks for submitting.. You haven't given me a lot to go on here, I can't help you without some clue as to how to reproduce.. Perhaps you don't have enough memory to run the number of workers required for a proper phased restart? https://github.com/puma/puma/issues/671\nDoes a USR2 restart work?\n. Relevant Unicorn thread https://bogomips.org/unicorn-public/20170804191023.GA28511@dcvr/T/. I'm with Eric. I have no idea how ObjC factors into this.. Great work @ticky! I won't be able to take a closer look at this until next week - holding off on my own High Sierra upgrade until I am back home and can make some backups first.. Great post @FooBarWidget. Just to clarify some things from Puma's perspective:\n\nI usually side with the user-experience side as well, and since Puma is the default for Rails I think it's extra important that we Just Work. I'm generally just watching and waiting this thread and the Ruby issue tracker to see what consensus emerges. If it looks like Ruby will quickly implement a fix, we may not do anything in Puma. But we've done some wild workarounds for Ruby bugs in the past too so I'm not against merging something to fix this here.\nSince v2.1, Puma warns the user if it detects additional Ruby threads created during preloading.. Yeah, I agree with what @ticky and @boazsegev said. Also, since there is an environment variable setting which gives you \"the old behavior\", it's easier for us to just point users to that instead.\n\nIMO \"upgrade your ruby patchlevel\" or \"use this environment variable to go back to the way things were pre-2017\" is an OK solution.. Okay, because of the quick fix here I'm going to close this from our perspective. This will be fixed in Ruby 2.4.3 (which will probably be around Christmas), as of this writing backports to other Ruby minor versions are unknown.\nTo work around this issue today, I suggest just setting the OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES environment variable when working with Puma. This gets you the old, \"unsafe\" behavior. To understand why this is an imperfect solution, read @FooBarWidget's blog. Try not to set it permanently or in your .bash_profile or anything, it's a genuinely useful feature that you should probably be using - instead maybe add it to a Procfile or something like that.. > this potentially masks other issues which this new behaviour is attempting to mask\nI think you meant \"attempting to unmask\" right?\nMy view is that Apple basically turned a feature into a bug here and made a helpful safety check cause a crash. Puma already warns when multiple threads exist before forking, so the safety check here is not as helpful as it might otherwise be. If people want the additional safety check Apple wants to provide here, they can upgrade to a newer Ruby version (when it comes out). Until then, they can live with the old behavior via the environment variable workaround.. > The safety check won\u2019t apply to any C extensions which may fork, though, right?\nCorrect, we're only checking the Ruby Thread.list.. > GemRequireError, unable to load gem red cloth\nThis is a problem with your application, please try the #ruby IRC channel or another avenue for support.. Please upgrade to Puma 3.10. There have been a number of bugfixes since 3.8.2. \nYou may also be experiencing this known bug.\nClosing for now, please re-open if you have any additional information that could lead to a reproduction, otherwise this is the best I can do.. Thanks for the repro. My gdb-fu isn't strong enough for this, are you getting a line number that we're stalled on in try_to_finish?. Probably not. If you want one process, use single mode.. Whoops, yeah in the docs that should say something like:\nruby\npumactl -C 'tcp://127.0.0.1:9293' --control-token foo restart\nor\nruby\npumactl -p <puma-pid> --control-token foo restart\nWe're just using OptionParser so it's too dumb to allow \"command-before-arguments\", even though it feels more natural here. All options must come before the command.. Thanks so much @MSP-Greg.. @ktimothy What part of the docs are you talking about exactly?. @williamweckl Please read @stereobooster's explanation.. > If I have other free web workers, why not to use them? \nYou have an application with two workers, Worker A and Worker B. Worker A makes a request to itself (localhost:3000 or whatever). As soon as this happens, thanks to Puma's multithreaded Reactor architecture, Worker A's thread releases the GVL and is \"free\". \nThe reason this doesn't happen in Unicorn is because Worker A would never go back and listen again on the socket, but in Puma, it will. Worker A tries to connect to itself, and voila, a lock. \nNote how the original issue said that increasing the number of workers decreases the chances of a lock. This is because sometimes Worker A will get the request from the kernel, sometimes Worker B will. That's up to the OS, not Puma.. LGTM. Sneaky! Thanks.. Yes - thanks you for opening, @shayonj.. :wave: Thanks.. This is a Rails issue, not Puma. See above.. (Marking as fixed, please re-open if it isn't). > Trying to isolate/debug I noticed that the default backlog for unix sockets is nil  \nHm, that does seem wrong.. @eprothro I talked to Evan about this, he said this behavior is not intentional and is definitely a bug. We should change the default Unix backlog to 1024.. Fair enough. Needs a test.. Really high quality PR, thanks very much for this.. One quick change there, but otherwise LGTM and thank you so much for this!. I feel like I've seen this issue before...it involved system calls being slower after a fork. What's your OS?\n. Here's what I was thinking of.. Ugh, I might just move 2.2 to allowed failures.. \ud83e\udd37\u200d\u2642\ufe0f Looks like the socket isn't writable. Please reopen if you have a reproduction or can show this is Puma's fault, otherwise there's nothing we can really do here \ud83d\ude22 . I'm honestly not sure. I'd open an issue with the JRuby repo - not b/c it's a JRuby bug, but I think JRuby could provide a better error message here.. Hi - this doesn't sound like a bug but more of a support issue. You can use our Gitter channel or contact the person that wrote that tutorial to get help.. Donezo https://github.com/puma/puma/releases/tag/v3.11.0. Ah, wonderful! Thank you!. I'm confused - we don't propose a config.ru file anywhere for Sinatra. Sinatra doesnt typically use rackup files (which is why the README says to use ruby app.rb -s Puma).. Looks like you compiled Puma's extensions without OpenSSL configured correctly. \nDid you recently upgrade your OS version or SSL package? Uninstall puma completely and reinstall, see if that fixes.\nPlease try StackOverflow or Gitter. . Sorry, we can't do support on this repository. Please use StackOverflow or Gitter.. Hm, SpaceBeforeBlockBraces failing on 2.1? . I believe you have to edit gemfiles/2.1-Gemfile.. Thanks!. Thanks. I need to look at the implementation, but if we're gonna merge this, it will need docs and a test.. Thanks for the PR. I'm taking some time off during the holidays and probably won't be able to merge until 2018.. @ranupratapsingh You're experiencing something different, please open a different issue.. Sorry, but this looks like you're asking for support with a particular Rails application. This issue tracker is for Puma bugs and requests. I think you would be better off asking a general Rails forum or chat.. @jxa Sorry for the delay here, I've been taking a break from Ruby.\nCan you rebase? I think we fixed the build issues.. This definitely looks like the right approach, just needs the things you already pointed out to get it over the line.. Yeah, we have several buffers which have to have limits on their size. We chose a different one than nginx. You should be using a POST here. Marking as wontfix.. VSZ isn't important, though. On a 64-bit system virtual address space is practically infinite.\nClosing because this is outside the scope of the Puma issue tracker.. Cannot reproduce on a basic Rails app. Please re-open if you still have this problem and have any more clues/reproduction steps.. Reopening until @tuwukee's comment is addressed.. Is this related to #1480?. > If we'd change ruby in the shebang line to puma\nOther way around but this is correct. Puma is the correct shebang line here.. re: memory usage, yes, I believe that's because you are starting a pumactl server every 60 seconds.\n. > Is there a better way to access the Puma::Launcher instance?\nNot that I'm immediately aware of. Keeping this open because I too use these numbers quite a bit.. This is not a Puma bug. We reserve the issue tracker for bug reports, feature requests and the like. This looks like you have some sort of misconfiguration with Faye.. Yeah, what @vfonic said.. Puma doesn't have any awareness of RAILS_ENV.\n. I think what you're seeing is probably that Puma's default environment is \"development\". Puma isn't reading RAILS_ENV.. Puma catches the TERM/KILL signal before it bubbles up to Rails. There's really nothing we can do about this, I would say the current behavior is what is expected.. No, you've judged the situation correctly - rack-timeout is the proper tool here. If it has a memory leak, you'll have to open an issue with them.. Cannot reproduce. Can you confirm tmp/sockets/puma.sock does not exist? $ ls tmp/sockets/.. This is performing as documented. Phased restarts are done with pumactl phased-restart, not USR1/USR2. See here.. Unfortunately, I don't think Puma will ever be able to fix this for you, given the way we do all of the downtime-free deploys.\nThis works in Unicorn, though? How did you accomplish this with Unicorn?. Doesn't this change mean SIGHUP will no longer call stop?. SIGWHYDONTWECALLTHEWHOLETHINGOFF. Your understanding is correct - this was a change in how Rails works though, not Puma.. If you look around you can probably find the commit in Rails that changed this. I think it was @matthewd's commit.. Thanks for sharing.. Sorry, we only have the time/resources to deal with bug reports and feature requests on the Puma issue tracker. Please try StackOverflow.. > Apologize if there have any English mistake.\nDon't worry about it, your English is great.\nNot sure if this is a misconfiguration or a bug, but given the information you've provided I'm guessing it has to be a misconfiguration somewhere or we'd have a lot more bug reports about restarts.\n. Thanks!. Thanks. Please reopen if you have some sort of code that can reproduce the problem.\nIf your application isn't returning an XHR response, my guess is that Puma is not the issue here.. > Is this a feature the maintainers would be interested in having in Puma?\n\ud83d\udc4d Absolutely.\n. Paging previous contributors @vipulnsward @michaelsauter @snow @kwilczynski . Can you confirm that starting a basic app with just the fog gem in the Gemfile reproduces the error?. Nope, not intentional! I should've moved it somewhere else in the docs folder but I obviously missed it.. Probably something to do with your browser.\nDoes $ curl -X GET localhost:9292 & curl -X GET localhost:9292 & curl -X GET localhost:9292 & curl -X GET localhost:9292 & curl -X GET localhost:9292 (where 9292 is the correct port obviously) behave as expected for you? Because it does for me.. @schneems thoughts? Seems reasonable.. @perlun Am I correct in understanding that this was fixed by https://github.com/jruby/jruby/issues/5134?. I agree, but this should be in allow-failures.. This feature should be optional.\n@schneems do you have any thoughts about we should organize stuff like this in general? Add to Puma.stats?. Hm, I think we should do this as well. Thoughts @deivid-rodriguez ?. @schneems. 99% of memory \"leak\" issues in Ruby are:\n\nFragmentation.\nAn old C extension in your dependencies.\n\nThere are already several things in this thread that make me think that many of you are seeing thread-related fragmentation:\n\nFor testing purposes, i replaced Puma and installed Passenger.\n\nYou replaced a multi-threaded application server with a single-threaded one (I'm assuming you would have said something specifically if you were using Passenger Enterprise, which allows threads). This eliminates the majority of memory fragmentation in Ruby.\n\nWe have also observed this with almost similar versions but on Ubuntu 16.04. OSX does not have this.\n\nThe fragmentation issue linked to above seems to be much worse on Linux than on Mac, possibly due to OSX shipping older versions of malloc or possibly just better memory management in the Mach kernel. Many BSD-based operating systems also do not report fragmentation issues w/Ruby.\nI must close this issue, as without a minimal verifiable example, we can't do anything about it. I've fixed memory \"leak\" issues on dozens of Puma applications, and they all come down to the two issues I mentioned at the beginning.. \ud83d\ude47 Thanks anyway! Merged the other one.. Standard 1x dynos and performance-M dynos have roughly the same logical cores available (2).\nTry Perf-L with ~8-12 Puma workers.. > I expect that Puma can serve 10 times more requests on a server that costs 10 times and has 10 times the computing power compared to another server.\nThis is more of a problem with the way Heroku explains and prices their dyno types than Puma. What matters for vertical scaling is the total number of logical cores. Perf-M and Standard dynos have access to (roughly) the same number.. Heroku Perf dyno pricing is definitely wonky, you don't have to run a benchmark to see that. Perf-M dynos have 1/4 of the CPU threads and 1/6 the memory of Perf-L dynos, but cost 1/2 as much, for example. \nYour benchmark sounds very I/O light and CPU-heavy. That's why you're not seeing better results once you add more processes beyond the amount of hyperthreads available (8 on a Perf-L).\nSounds like you're just seeing the underlying economics of the Heroku platform. Otherwise everything about your benchmark results are as expected. Note also that Standard dynos technically \"share\" 8 cores with whoever else is on your dyno. I don't exactly know how the Heroku hypervisor works but it's entirely possible you could have access to 8 hyperthreads at once, the same as you get on Perf-L (you just get reduced \"time\" on them), which could be why standard dynos are performing well here.\nHappy to continue to discuss this over email but I'm going to close as there's no issue with Puma here.. Actually, I have no idea if this is a performance sensitive path or not, but using casecmp will be ever so slightly faster here (and allocates one fewer object):\nruby\nCHUNKED.casecmp(te) == 0\n. Why does this one have to be require_relative?. I'd prefer _buffer. Is there really ever a case where we should cull and spawn? . So this ends up delaying the response to TTOU until the next run of check_workers, right?. Maybe this should be rearchitected so that cull_workers and spawn_workers takes an integer argument, and then you calculate the value to give to those methods here. You could then call cull_workers(1) in your TTOU signal trap.. Derp, you said that in your description.. Can you leave a comment here explaining this? Otherwise it looks pretty magical.. Maybe even just include a permalink to this pull, since your explanation is very complete here. TIME_WAIT isn't really applicable to the described use-case, correct? . Why wouldn't max_threads already be an integer?. What's the purpose of an instance variable? Why not just call the accessor?. I would say this: https://github.com/puma/puma/pull/1225/files/b373d26a37eaac35ed943c72b183b11f99406104#diff-ef4c610f0ce605eb90f2810856394fd5R487 is an accessor. I'm asking what the purpose of the instance variable is when you could just call fast_track_ka_timeout.. I'm loathe to cache anything unless we actually have a benchmark proving it is a significant speed difference. In this case, making a comparison against an integer.\nI'm not even sure max_threads is always static in all cases.. Paging @evanphx for a quick gut check on my logic above.. Aha, yeah, this is probably the correct fix.. Can't we just yield?. Just use @options?. Already set. samesies. no biggie, I just have a habit of using yield. I think this was added for a reason, I can't recall the issue.. What's wrong with test_helper? That's a pretty common convention.. No reason to remove IMO.. fine with this. Typo?. Raises as if STDERR was closed if not commented out.. isn't this a required attribute?. why remove?. why remove?. from what I can tell removing this would change the default output for gem install --document puma.. looked it up, this is the default value, so \ud83d\udc4d . removing this would remove the require ruby version check from bundler AFAICT, I don't want to do that.. Why not .original_env?. Ah I see, nvm. Should we be cloneing this like Bundler does though?. why not \"unless token = 'none'\" and set the token to 'none'? a little more meaningful I think.. this is weird, what was this even supposed to test? . I guess it's testing that the server should not respond within 2 seconds? That's...not really testing whether or not the server restarted lol.. i guess the restarting is already tested by the wait_for_server_to_boot, because it looks for \"Use Ctrl-C to stop\" which won't appear if the server didn't restart.. Aren't we missing a comparison operator here?. Hmmm... are there scenarios where people's Rubyopt will get mangled by this?. This still makes me nervous. Why aren't we just appending this?. Shouldn't we be fixing this for 2.2.0 through 2.2.6, 2.3.0 through 2.3.3 and 2.4.0 as well?. Is the reason we're not patching other Ruby versions the little weird subtleties like this one? Basically our struct here has to exactly match the thread struct of the Ruby version we're patching, right?. Actually, because of the way this works, maybe we should only patch the latest teeny, because that's all we test on CI. This patch could potentially break on a teeny change (since we're so deep in the MRI internals here), so maybe targeting to really specific versions isn't a bad idea.. Why is this necessary? I guess I don't understand.. It would be nice to have this lambda live somewhere other than the body of this method.. I know our DSL style is really inconsistent, but for futureproofing I'd prefer:\nruby\ndef early_hints(answer=true)\n    @options[:early_hints] = answer\nend. Should we auto-detect this part? I did some of this in rack-http-preload... I think you were copying tcp_mode!, but that's a ! method because it doesn't make sense for a server to be in tcp_mode and then not be in tcp mode later. Although I can't think of why anyone ever would (restarting and disabling early hints as the config changes?), it's more plausible that a server could toggle its support of early_hints. So this should probably just be treated like a straight-up accessor. . Rubocop is barfing on your squiggly heredoc.. It's also possible we may not want to auto-detect as at the puma level. I just wanted to ask the question.. I think the reason is explained adequately in the OP?. it's -> its. In the case of a single-threaded worker, @waiting will be 0, @todo.size will be 1, @max will be 1 and @spawned will be 1, so the condition evaluates to 1 < 1 - 1, which is zero, so Puma waits here.. \"worker pool\" -> \"worker's thread pool\". > Should we file a new issue?\nPlease do.. \ud83d\udc4d . \ud83d\udc4d but the comment isn't necessary. @grosser What I meant was you eliminated this branch of code, and I don't understand why.. I think it was important to #1377?. maybe we can think of something more descriptive than m. maybe this could be it's own method. ",
    "damm": "Merb == Rack.  You should not need Merb support if you support Rack.\n. mongrel had pid support\n. ",
    "mperham": "In fact, Rails < 2.2 is not thread-safe.\n. Looks like puma changes the default development logging when started this way:\n$ bin/puma -C config/puma.rb \n[20512] Puma starting in cluster mode...\n[20512] * Version 2.7.1, codename: Earl of Sandwich Partition\n[20512] * Min threads: 2, max threads: 4\n[20512] * Environment: development\n[20512] * Process workers: 2\n[20512] * Phased restart available\n[20512] * Listening on tcp://0.0.0.0:3000\n[20512] Use Ctrl-C to stop\n[20512] - Worker 20514 booted, phase: 0\n[20512] - Worker 20513 booted, phase: 0\n[20513] 127.0.0.1 - - [17/Jan/2014 09:23:06] \"GET / HTTP/1.1\" 200 - 3.5813\n[20513] 127.0.0.1 - - [17/Jan/2014 09:23:06] \"GET /application.css?body=1 HTTP/1.1\" 304 - 0.0787\n[20513] 127.0.0.1 - - [17/Jan/2014 09:23:06] \"GET /application3.css?body=1 HTTP/1.1\" 304 - 0.0600\n[20513] 127.0.0.1 - - [17/Jan/2014 09:23:06] \"GET /jquery.tools.min.all-1.2.6.js?body=1 HTTP/1.1\" 304 - 0.0534\n[20513] 127.0.0.1 - - [17/Jan/2014 09:23:06] \"GET /jquery_ujs.js?body=1 HTTP/1.1\" 304 - 0.0742\nbut not when we start it via bin/rails s puma.  How do we use a config file while also getting normal Rails development stdout logging?\n. Any thoughts about setting puma as the default server, a la unicorn-rails?  We love it.\nhttps://github.com/samuelkadolph/unicorn-rails/blob/master/lib/unicorn/rails.rb#L35\n. I'd prefer not to require a separate gem.  I don't know why an app would have multiple app servers bundled but maybe there's a rare edge case.\n. Sounds good, we're probably on an old rack version. \n. Sidekiq::Processor only runs in the sidekiq process. It has nothing to do with the puma server. \n. @seuros That only adjusts the process count, yes, not thread count per process?\n. Yep.  ;-)\n. Man, it's like that Migrating from unicorn section was written for us!\nI'd add an explicit section on how to set up capistrano if possible. \n\nOn Feb 28, 2014, at 17:10, Evan Phoenix notifications@github.com wrote:\nStarted it here: https://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting Send PRs to help flesh it out! (like adding a TLDR).\n\u2014\nReply to this email directly or view it on GitHub.\n. Here's the impl:\n\nruby\n      when 'TTIN'\n        Thread.list.each do |thread|\n          Sidekiq.logger.warn \"Thread TID-#{thread.object_id.to_s(36)} #{thread['label']}\"\n          if thread.backtrace\n            Sidekiq.logger.warn thread.backtrace.join(\"\\n\")\n          else\n            Sidekiq.logger.warn \"<no backtrace available>\"\n          end\n        end\n      end\n. Awesome.  Testing the results now.  Before:\n```\nMeasuring objects created by gems in groups [:default, \"production\"]\nTotal allocated: 4246065 bytes (34727 objects)\nTotal retained:  437421 bytes (4179 objects)\nallocated memory by gem\n1348174  puma-3.6.0\n    749251  sidekiq/lib\n    538517  pro/lib\n    397726  ent/lib\n    390783  sqlite3-1.3.11\n    366598  redis-3.3.1\n    143032  bundler-1.12.5\n    119089  redis-namespace-1.5.2\n     84124  statsd-ruby-1.3.0\n     52623  connection_pool-2.2.0\n     25766  ruby-2.3.0/lib\n     25330  activesupport-5.0.0.1\n      4972  railties-5.0.0.1\n        80  derailed_benchmarks-1.3.1\nallocated memory by file\n376210  /Users/mike/src/sidekiq/lib/sidekiq/redis_connection.rb\n328169  /Users/mike/.gem/ruby/2.3.0/gems/puma-3.6.0/lib/puma/server.rb\n297164  /Users/mike/.gem/ruby/2.3.0/gems/puma-3.6.0/lib/puma.rb\n267768  /Users/mike/.gem/ruby/2.3.0/gems/puma-3.6.0/lib/puma/launcher.rb\n237737  /Users/mike/src/sidekiq/lib/sidekiq.rb\n220417  /Users/mike/.gem/ruby/2.3.0/gems/redis-3.3.1/lib/redis.rb\n198843  /Users/mike/.gem/ruby/2.3.0/gems/sqlite3-1.3.11/lib/sqlite3/database.rb\n190144  /Users/mike/.gem/ruby/2.3.0/gems/puma-3.6.0/lib/puma/configuration.rb\n189821  /Users/mike/src/pro/lib/sidekiq/pro/api.rb\n187532  /Users/mike/src/ent/lib/sidekiq-ent.rb\n184554  /Users/mike/src/ent/lib/sidekiq-ent/limiter.rb\n164701  /Users/mike/src/pro/lib/sidekiq/batch.rb\n149197  /Users/mike/src/pro/lib/sidekiq-pro.rb\n142872  /Users/mike/.gem/ruby/2.3.0/gems/bundler-1.12.5/lib/bundler/runtime.rb\n108896  /Users/mike/.gem/ruby/2.3.0/gems/sqlite3-1.3.11/lib/sqlite3.rb\n 93205  /Users/mike/.gem/ruby/2.3.0/gems/redis-3.3.1/lib/redis/connection.rb\n 90357  /Users/mike/.gem/ruby/2.3.0/gems/redis-namespace-1.5.2/lib/redis-namespace.rb\n 79214  /Users/mike/.gem/ruby/2.3.0/gems/puma-3.6.0/lib/puma/reactor.rb\n 77809  /Users/mike/.gem/ruby/2.3.0/gems/statsd-ruby-1.3.0/lib/statsd-ruby.rb\n 61038  /Users/mike/.gem/ruby/2.3.0/gems/puma-3.6.0/lib/puma/const.rb\n\n```\n. DAAAAAAMN, SON.  Making me look bad!  After:\n```\n$ bundle exec derailed bundle:objects\nMeasuring objects created by gems in groups [:default, \"production\"]\nTotal allocated: 2900975 bytes (23618 objects)\nTotal retained:  309364 bytes (2885 objects)\nallocated memory by gem\n749250  sidekiq/lib\n538517  pro/lib\n397726  ent/lib\n390823  sqlite3-1.3.11\n366518  redis-3.3.1\n144092  bundler-1.12.5\n119089  redis-namespace-1.5.2\n 84124  statsd-ruby-1.3.0\n 52623  connection_pool-2.2.0\n 25766  ruby-2.3.0/lib\n 25330  activesupport-5.0.0.1\n  4972  railties-5.0.0.1\n  2065  puma-5fdf337790a6\n    80  derailed_benchmarks-1.3.1\n\nallocated memory by file\n376209  /Users/mike/src/sidekiq/lib/sidekiq/redis_connection.rb\n237737  /Users/mike/src/sidekiq/lib/sidekiq.rb\n220337  /Users/mike/.gem/ruby/2.3.0/gems/redis-3.3.1/lib/redis.rb\n198883  /Users/mike/.gem/ruby/2.3.0/gems/sqlite3-1.3.11/lib/sqlite3/database.rb\n189821  /Users/mike/src/pro/lib/sidekiq/pro/api.rb\n187532  /Users/mike/src/ent/lib/sidekiq-ent.rb\n184554  /Users/mike/src/ent/lib/sidekiq-ent/limiter.rb\n164701  /Users/mike/src/pro/lib/sidekiq/batch.rb\n149197  /Users/mike/src/pro/lib/sidekiq-pro.rb\n143932  /Users/mike/.gem/ruby/2.3.0/gems/bundler-1.12.5/lib/bundler/runtime.rb\n108896  /Users/mike/.gem/ruby/2.3.0/gems/sqlite3-1.3.11/lib/sqlite3.rb\n 93205  /Users/mike/.gem/ruby/2.3.0/gems/redis-3.3.1/lib/redis/connection.rb\n 90357  /Users/mike/.gem/ruby/2.3.0/gems/redis-namespace-1.5.2/lib/redis-namespace.rb\n 77809  /Users/mike/.gem/ruby/2.3.0/gems/statsd-ruby-1.3.0/lib/statsd-ruby.rb\n 49577  /Users/mike/.gem/ruby/2.3.0/gems/connection_pool-2.2.0/lib/connection_pool.rb\n 42500  /Users/mike/.gem/ruby/2.3.0/gems/sqlite3-1.3.11/lib/sqlite3/statement.rb\n 35237  /Users/mike/src/sidekiq/lib/sidekiq/client.rb\n 33343  /Users/mike/src/sidekiq/lib/sidekiq/worker.rb\n 27540  /Users/mike/.gem/ruby/2.3.0/gems/redis-namespace-1.5.2/lib/redis/namespace.rb\n 25328  /Users/mike/.gem/ruby/2.3.0/gems/redis-3.3.1/lib/redis/connection/ruby.rb\n 24850  /Users/mike/.gem/ruby/2.3.0/gems/activesupport-5.0.0.1/lib/active_support/core_ext/class/attribute.rb\n 23423  /Users/mike/src/pro/lib/sidekiq/pro/config.rb\n 20798  /Users/mike/.rubies/ruby-2.3.0/lib/ruby/2.3.0/forwardable.rb\n 18244  /Users/mike/src/sidekiq/lib/sidekiq/util.rb\n\n```\n. Seems like I need to move my APIs to autoload to minimize the memory overhead of unused features.\n. Oh crap. Subscribe me, github!. Is there some reason why this shouldn't be passed up to MRI instead of fixed in each individual project?  For instance, MRI on OSX could touch the Obj-C runtime on startup and all this goes away, right?. ",
    "drnic": "Are there docs on the intertubes for rackifying Rails 1 apps?\nCheers\nNic\nDr Nic Williams - VP Engineering\nEngine Yard\nThe Leading Platform as a Service\nMobile: +1 415 860 2185\nSkype: nicwilliams\nTwitter: @drnic\nOn Jan 6, 2012, at 9:07 AM, Evan Phoenixreply@reply.github.com wrote:\n\nI rails 1.0 rack handler must be available and likely would require a mutex to run properly. I consider this outside the scope of puma since such a handler should be available as a seperate gem.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/evanphx/puma/issues/15#issuecomment-3387407\n. Or Is your reply \"it probably exists but I don't know\"?\n\nCheers\nNic\nDr Nic Williams - VP Engineering\nEngine Yard\nThe Leading Platform as a Service\nMobile: +1 415 860 2185\nSkype: nicwilliams\nTwitter: @drnic\nOn Jan 6, 2012, at 9:07 AM, Evan Phoenixreply@reply.github.com wrote:\n\nI rails 1.0 rack handler must be available and likely would require a mutex to run properly. I consider this outside the scope of puma since such a handler should be available as a seperate gem.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/evanphx/puma/issues/15#issuecomment-3387407\n. Did mongrel used to have pid support and puma removed it, or did mongrel never have it?\n. @blanchma if it works, could you be a champion and create a wiki page on Puma & Apache?\n@dariocravero want to create one for Puma & Nginx? You too then get bonus points towards being a champion :)\n. \n",
    "burningTyger": "Since 2.14 I noticed the logger has returned. Need to use the '-q' or '--quiet' paramter to turn it off again.\n. ",
    "yob": "my bad, my testing methodology was flawed. Testing with ab shows the expected results.\nOn MRI 1.9.3 both:\nsleep(n)\nand (with mysql2):\nProduct.connection.select_all(\"select sleep(#{n})\")\nresult in fully concurrent requests. Thanks!\n. I was able to build puma on debian sid once I installed the new libssl1.0-dev package.\nInstalling it might be a challenge, as it conflicts with libssl-dev (v1.1.0). You should only need it installed while puma builds though, then you can re-install libssl-dev. It's a hack, but might be helpful as an interim solution.. Thanks for this - I was able to leverage it to build a small puma plugin that passes the numbers to statsd every few seconds: https://github.com/yob/puma-plugin-statsd. ",
    "mrrooijen": "+1: That would make it usable on Heroku as well since you have to bind to a port on their Cedar stack. They do not support UNIX/TCP sockets as far as I know. Also, Bamboo used HTTP 1.0, but the new Cedar stack uses HTTP 1.1.\nJust not sure whether Puma was meant to run under using Rubinius rather than Ruby 1.9.x because of the GIL. Heroku only allows for Ruby 1.9.2 at this time. But, if it still offers a performance (concurrency) improvement even under Ruby 1.9.2 that would be nice.\nStill, I would imagine this would be great to run under RBX with the GIL removed. Would love to try that on a VPS/EC2, and hope Heroku will support RBX at some point.\n. Actually if you run it through the Rails executable you can assign a port with -p. But, I'm not sure whether you're able to set any Puma-specific options along with it.\nrails server puma -p 5000\nBut if you wanted to increase the amount of threads it's allowed to spawn, not sure if you can. The -t option isn't available because this is actually the rails cli, not the puma cli.\nThough, in any case I guess it might be better to be able to bind to a port directly from Puma itself.\n. > Like many other servers, it uses the rapid HTTP parser that comes with Mongrel. It also uses a dynamically sized thread-pool for processing requests in parallel. With Puma, you now have a go to choice when it comes to deploying web applications on Rubinius. And since it does not contain any Rubinius specific code, it also works quite well on JRuby or CRuby.\nThis is it. Full of win. Thanks! I didn't see that article.\n. @kyledrake thanks!\n. @evanphx great! Thanks a lot for building it. I suspect this will really push web developers towards more (ram/cpu) efficient deployments using threads, and Rubinius in general. Process-based scaling is just crazy in my opinion, and becomes quite expensive in the long run in various ways. Rather than using REE with CoW friendliness which is stuck in Ruby 1.8.7 performance/features but spawns processes more memory efficient, it just doesn't beat spawning these ultra light threads and having a better GC.\nCheers!\n. Yup seems to be working now. Thanks!\n. ",
    "nono": "FYI, Konstantin Haase has posted http://www.engineyard.com/blog/2012/my-summer-of-open-source/ with some informations about Puma.\n. ",
    "kyledrake": "I will add a detailed explanation of what Puma provides and update the README via pull request. These are great questions!\n. Update: Nope, it's not nginx related at all. I made a different socket file that nginx was not aware of.\n. I'm still having this issue! It's not deleting the socket file after bad crashes and then refuses to start instead of overriding it. This is not a safe default, as it will not start back up after a failure!\n. For example if I have to kill -9 the process, it likely won't remove the file. I resolved it for now by running an rm command before I try to start the application. For some reason regular kill is ignored with 1.2.2 on our Ubuntu production box.. very strange. I may be able to get more information about it.\n. Sorry, I'm in the middle of a hackathon right now so I'm not providing very useful information :)\n. I believe this is related: https://github.com/puma/puma/issues/73\n. I'm assuming @calavera was having the same issue I was, which was that Puma was refusing to start because of the pid file being present. If that's been fixed, then it's probably correct to leave the file alone.\n. Fixed, thanks!\n. ",
    "davidhorsak": "Got same error today:\n```\n2012-10-11 14:54:32 +0200: HTTP parse error, malformed request (): #\n2012-10-11 14:54:32 +0200: ENV: {\"rack.version\"=>[1, 1], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"1.6.3\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2012-10-11 14:54:32 +0200: HTTP parse error, malformed request (): #\n2012-10-11 14:54:32 +0200: ENV: {\"rack.version\"=>[1, 1], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"1.6.3\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2012-10-11 14:54:48 +0200: HTTP parse error, malformed request (): #\n2012-10-11 14:54:48 +0200: ENV: {\"rack.version\"=>[1, 1], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"1.6.3\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n```\nSwitched from Unicorn to Puma. On Unicorn the app runs without problems. I got there nothing special. Only using ActiveAdmin for apps administration interface. The app is also connected to Solr server through Sunspot gem. Any ideas? If you think I can provide you with more information, please, tell me where to look or what you need. Thanks! \n. Can you please tell me how to find it out? I am sure I don't modify headers anywhere in the app (not sure about gems).\n. Google Chrome gives me \"No data received\" and Firefox \"The connection was reset\". I can get any information from both Chrome's developers tools and Firebug. If I try curl my localhost:port, it raises \"Empty reply from server\". Any other ideas?\n. Actually, I got this error with config.threadsafe! commented out and was running puma in development mode. Never tried connecting to it using SSL.\n. ",
    "kenips": "This only happens when config.threadsafe! is uncommented AND connecting via SSL. From the other logs it seems like that to be the case as rack.multithread is true in both cases. Not sure if they are using SSL though.\n. Still happening with Puma 2.4.0\u2026 any idea?\n. Still has 3GB in free - must be enough right?\n. @evanphx any more information that you need to fix this? At 2.5.1 now\u2026\n28-08-2013 14:53: Read error: #<Errno::EPIPE: Broken pipe>\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/server.rb:762:in `syswrite'\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/server.rb:762:in `fast_write'\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/server.rb:589:in `handle_request'\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/server.rb:343:in `process_client'\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/server.rb:242:in `block in run'\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/thread_pool.rb:92:in `call'\n/var/projects/twitter/vendor/bundle/ruby/2.0.0/gems/puma-2.5.1/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\nEdit: for those who ask about memory, 16GB total, 8GB used, 7GB free.\n. Great! Thanks!\n. ",
    "stantona": "I had the same experience as @DaveTsunami, I got this error running in development mode. I had config.threadsafe! defined in my production.rb file only, and it wasn't until I also added it to development.rb did the error go away. \n. This was actually an issue with the RMagick gem being lazy loaded in development environment.\ngem 'RMagick', require: false \nSetting it to:\ngem 'RMagick'\ngot it working again.\nI'm not sure why in that circumstance it would be firing a SIGTRAP. Will have to have a deeper look into it. \nAnyways, I will close this.\n. ",
    "cj": "I'm getting this error and it's todo with characters it doesn't like passed in the url.  Add % to your url and you'll be able to reproduce this error.\n. ",
    "dlindahl": "I am getting this error in my Rails 4/Ruby 2 app with a DELETE request and a very basic query:\nhttp://my-app.dev/api/widgets/3362107/claim\nLog:\n13:34:46 web.1       | Started DELETE \"/api/widgets/3362107/claim\" for 127.0.0.1 at 2013-10-11 13:34:46 -0400\n13:34:46 web.1       | Processing by Api::ClaimsController#destroy as */*\n13:34:46 web.1       |   Parameters: {\"widget_id\"=>\"3362107\"}\n13:34:46 web.1       | Completed 200 OK in 86ms (Views: 78.0ms | ActiveRecord: 0.6ms)\n13:34:46 web.1       | 2013-10-11 13:34:46 -0400: HTTP parse error, malformed request (): #<Puma::HttpParserError: Invalid HTTP format, parsing fails.>\n13:34:46 web.1       | 2013-10-11 13:34:46 -0400: ENV: {\"rack.version\"=>[1, 2], \"rack.errors\"=>#<IO:<STDERR>>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"2.5.1\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n13:34:46 web.1       | ---\n. ",
    "masterkain": "It is possible to log the error's requests (by modifying puma)? I'm also experiencing a similar issue in Rails 4.0.1, Ruby 2.1.0-preview1 but I can't reproduce through a browser, as I have no clue which path the client is trying to hit.\nI only get this:\nHTTP parse error, malformed request (): #<Puma::HttpParserError: Invalid HTTP format, parsing fails.>\nENV: {\"rack.version\"=>[1, 2], \"rack.errors\"=>#<IO:<STDERR>>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"2.6.0\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n. I'm having this problem with MRI 1.9.3-p374, OSX 10.8.2\n```\nenvironment \"development\"\nbind \"tcp://0.0.0.0:5000\"\nbind \"unix:///tmp/puma.sock\"\npidfile \"/tmp/puma.pid\"\ndaemonize true\nworkers 2\nthreads 1, 16\nrackup \"config.ru\"\nactivate_control_app \"tcp://127.0.0.1:9293\", { auth_token: \"foo\" }\n```\nStarting puma the first time is fine, CTRL+C, starting again I get:\n```\n^C[97402] - Gracefully shutting down workers...\n[97402] - Goodbye!\n$ bundle exec puma -C config/puma/development.rb\n[97520] Puma 2.0.0.b6 starting in cluster mode...\n[97520] * Process workers: 2\n[97520] * Min threads: 1, max threads: 16\n[97520] * Environment: development\n[97520] * Listening on tcp://0.0.0.0:5000\n[97520] * Listening on unix:///tmp/puma.sock\n/Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:234:in initialize': Address already in use - /tmp/puma.sock (Errno::EADDRINUSE)\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:234:innew'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:234:in add_unix_listener'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:96:inblock in parse'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:64:in each'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:64:inparse'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/cli.rb:637:in run_cluster'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/lib/puma/cli.rb:391:inrun'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bundler/gems/puma-2e2440024623/bin/puma:10:in <top (required)>'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bin/puma:19:inload'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bin/puma:19:in <main>'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bin/ruby_noexec_wrapper:14:ineval'\n  from /Users/kain/.rvm/gems/ruby-1.9.3-p374@mygemset/bin/ruby_noexec_wrapper:14:in `'\n```\nUsing master.\n. Long standing bug, I noticed that Rainbows! cleans the previous socket file upon starting, perhaps it's better than trying to clean it at shutdown?\n. It seems to be working for me on rails 3.2-stable, puma 2.3.2 unless I'm missing something:\n```\n$ curl -vvv -F \"query=zoooom\" --header \"Transfer-Encoding: chunked\" http://my.puma.host/test\n About to connect() to my.puma.host port 80 (#0)\n   Trying 192.81.211.xxx...\n connected\n Connected to my.puma.host (192.81.211.xxx) port 80 (#0)\n\nPOST /test HTTP/1.1\nUser-Agent: curl/7.24.0 (x86_64-apple-darwin12.0) libcurl/7.24.0 OpenSSL/0.9.8x zlib/1.2.5\nHost: my.puma.host\nAccept: /\nTransfer-Encoding: chunked\nExpect: 100-continue\nContent-Type: multipart/form-data; boundary=----------------------------aeec04bbf822\n< HTTP/1.1 200 OK\n< Content-Type: text/html; charset=utf-8\n< X-UA-Compatible: IE=Edge,chrome=1\n< ETag: \"7215ee9c7d9dc229d2921a40e899ec5f\"\n< Cache-Control: max-age=0, private, must-revalidate\n< X-Request-Id: 578958d76357ed3e5caa3d299aa453d4\n< X-Runtime: 0.024167\n< Content-Length: 1\n< \n* Connection #0 to host my.puma.host left intact\n * Closing connection #0\n```\n. Getting this without tampering anything, but no report in newrelic_rpm (3.6.5.130), Rails 3-2-stable, Ruby 2.0.0-p247\nSame config worked out of the box with Rainbows!:\n\nINFO : Starting the New Relic agent in \"staging\" environment.\nINFO : To prevent agent startup add a NEWRELIC_ENABLE=false environment variable or modify the \"staging\" section of your newrelic.yml.\nINFO : Reading configuration from config/newrelic.yml\nINFO : Enabling the Request Sampler.\nINFO : Environment: staging\nINFO : Dispatcher: puma\nINFO : Application: Myapp (Staging)\nINFO : Installing ActiveRecord instrumentation\nINFO : Installing Sidekiq instrumentation\nINFO : Installing Sinatra instrumentation\nINFO : Installing Net instrumentation\nINFO : Installing middleware-based Excon instrumentation\nINFO : Installing Dalli Memcache instrumentation\nINFO : Installing Rails 3 Controller instrumentation\nINFO : Installing Rails 3.1/3.2 view instrumentation\nINFO : Installing Rails3 Error instrumentation\nINFO : Finished instrumentation\nINFO : Reporting to: https://rpm.newrelic.com/accounts/xxxx/applications/xxx\n. I see lot of this in the puma err log:\n==> rails/log/puma.err.log <==\nThe following keys are invalid: :newrelic_trace_data\nmight be a hint.\nAlso at the same time:\n==> rails/log/newrelic_agent.log <==\n[07/15/13 15:45:26 +0000 staging (19945)] ERROR : Caught exception in trace_method_execution footer.\n[07/15/13 15:45:26 +0000 staging (19945)] ERROR : RuntimeError: unbalanced pop from blame stack, got net_http, expected method_tracer\n. preload and cluster, yes.\n. Same:\n\n. Once done I suggest to document and provide a real world example (redis, activerecord, other descriptors) of what goes in those blocks. In my experience this proved to be a pain point for many users who doesn't know what to do in similar situations.\n. Good points.\nWhat I'd like is a mention how to properly configure in these blocks and project a redis connection in puma cluster mode, for example, or other popular gems.\nOn the various sidekiq, resque, rainbows, unicorn trackers this is always source of confusion.\n. Yes, that would work, it's 127.0.0.1 also in rainbows.\n. Can you try a cap deploy:setup and cap deploy:check first?\n. can you try $stdout.sync = true in the sinatra app?\n. @kinaan-khan-confiz sorry, can you try STDOUT.sync = true in config/environments/development.rb worth a shot, that's how I configured it in mine, but the issue mentioned may be totally unrelated to output buffering.\n. ",
    "Sija": "Same happens to me while issuing DELETE requests via ajax. Riding Ruby 2 with Rails 3.2.15.\n01:34:14 web.1       | Started DELETE \"/signout\" for 127.0.0.1 at 2013-11-20 01:34:14 +0000\n01:34:14 web.1       | Processing by SessionsController#destroy as */*\n01:34:14 web.1       | WARNING: Can't verify CSRF token authenticity\n01:34:14 web.1       |   User Load (0.5ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 5 LIMIT 1\n01:34:14 web.1       |    (0.1ms)  BEGIN\n01:34:14 web.1       |    (0.1ms)  COMMIT\n01:34:14 web.1       | Completed 204 No Content in 8.4ms (ActiveRecord: 0.7ms)\n01:34:14 web.1       | 2013-11-20 01:34:14 +0000: HTTP parse error, malformed request (): #<Puma::HttpParserError: Invalid HTTP format, parsing fails.>\n01:34:14 web.1       | 2013-11-20 01:34:14 +0000: ENV: {\"rack.version\"=>[1, 1], \"rack.errors\"=>#<IO:<STDERR>>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"2.6.0\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n01:34:14 web.1       | ---\n. ",
    "ryandotsmith": "Is there a flag we can use to have Puma dump the request to stdout/stderr? If there was, I suspect it would make resolving these issues simpler.\n. Is it worth the effort of making this configurable via puma config? I am in a situation where I would like to have this value be much larger than 2048.\n. rfc2616 3.2.1\n\nThe HTTP protocol does not place any a priori limit on the length of a URI. Servers MUST be able to handle the URI of any resource they serve, and SHOULD be able to handle URIs of unbounded length if they provide GET-based forms that could generate such URIs. A server SHOULD return 414 (Request-URI Too Long) status if a URI is longer than the server can handle (see section 10.4.15).\n\nhttp://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html\n. ",
    "lloydmeta": "For what it's worth, I got this error when I mistakenly made requests to localhost/127.0.0.1 with HTTPS (I hadn't set this up) instead of HTTP. I was using Puma.\n. ",
    "LouisSayers": "Also discovered that I got the error with an https url - where ssl isn't set up locally.\n. ",
    "michaelvobrien": "I got the same \"HTTP parse error, malformed request\" errors. I wanted Sinatra, Puma, and SSL for a test server. The following code gave me the errors.\n``` ruby\nRack::Handler::Puma.run(SinatraApp, rack_opts) do |server|\n  ctx             = Puma::MiniSSL::Context.new\n  ctx.key         = ENV['SSL_KEY']\n  ctx.cert        = ENV['SSL_CERT']\n  ctx.verify_mode = Puma::MiniSSL::VERIFY_NONE\nserver.add_ssl_listener(rack_opts[:Host], rack_opts[:Port], ctx)\nend\n```\nThe Puma Rack Handler had the follow lines:\nruby\nserver.add_tcp_listener options[:Host], options[:Port]\nserver.min_threads = min\nserver.max_threads = max\nyield server if block_given?\nBecause the yield came after the add_tcp_listener, the code called server.add_ssl_listener too late. The add_tcp_listener had already bound to the port, I think.\nTo fix it, I just changed the port for the ssl listener.\n. ",
    "mgharbik": "@lloydmeta you saved my day, Thanks! I was calling localhost over https\n. I am not using rugged but my server raises the same error when sending a post request, it's curious because this happens only when the request contains a full base64 code of a file. Any suggestions?\nFull error:\n```\n2014-10-31 07:17:30 +0100: HTTP parse error, malformed request (): #\n2014-10-31 07:17:30 +0100: ENV: {\"rack.version\"=>[1, 2], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"2.9.1\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\", \"REQUEST_METHOD\"=>\"POST\", \"REQUEST_PATH\"=>\"/developers/offers\"}\n\n```\n. ",
    "gshilin": "I'm using ruby 2.1.2p95, rails 4.1.8, puma 2.9.2. For the following plain GET request\nhttp://localhost:3000/download/files/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98%2020132%20personal%20ECG%20% I'm getting\n```\n2015-02-05 17:36:30 +0200: HTTP parse error, malformed request (): #\n2015-02-05 17:36:30 +0200: ENV: {\"rack.version\"=>[1, 2], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"2.9.2\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\", \"REQUEST_METHOD\"=>\"GET\"}\n\n```\nNothing fancy, just simple GET\n. That's not me, that's come from outside. And I'm not getting it because of puma error.\n. To supply configuration option: where to route invalid requests to\n-- OR --\nTo reply with 404\n-- OR --\nTo look what others [webrick, thin, etc.] are doing in such a situation\n. ",
    "gerrywastaken": "For others hitting this page as a result of a https error. You are more than likely specifying tcp:// instead of ssl:// as the binding address:\ne.g.\nbundle exec puma config.ru -b 'tcp://0.0.0.0:3456?key=/path/to/key.key&cert=/path/to/cert.crt'\ninstead of\nbundle exec puma config.ru -b 'ssl://0.0.0.0:3456?key=/path/to/key.key&cert=/path/to/cert.crt'\n. ",
    "vanboom": "Thanks for the help about changing the binding address to ssl!  However, when I configure with ssl://... I receive this error when my client tries to connect...\nRack app error: #<TypeError: no implicit conversion of Puma::MiniSSL::Socket into Integer>\n/rails_apps/helphub/vendor/bundle/ruby/2.1.0/gems/eventmachine-1.0.8/lib/eventmachine.rb:762:inattach_fd'`\nI am using the private .key file I generated when applying for my certificate, and the .crt file that was given to me by Comodo.   Thanks!\n. ",
    "rda1902": "@vanboom \nI solved this problem by replacing puma on thin server\n. I solved this problem by replacing puma on thin server\n. @Tom-Tom \nI used this manual https://www.driftingruby.com/episodes/actioncable-on-production\n. ",
    "pavan461": "Hi i'm facing this issue with puma gem, Can anyone help out me to fix this issue\n=> Booting Puma\n=> Rails 4.2.5 application starting in development on http://localhost:3000\n=> Run rails server -h for more startup options\n=> Ctrl-C to shutdown server\nPuma starting in single mode...\n- Version 3.4.0 (ruby 2.3.1-p112), codename: Owl Bowl Brawl\n- Min threads: 0, max threads: 16\n- Environment: development\n- Listening on tcp://localhost:3000\n  Use Ctrl-C to stop\n```\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n```\n. @evanphx @biow0lf  How to fix this issue, can you explain why this occurs?\n. Hi i'm facing this issue with puma gem, Can anyone help out me to fix this issue\n=> Booting Puma\n=> Rails 4.2.5 application starting in development on http://localhost:3000\n=> Run rails server -h for more startup options\n=> Ctrl-C to shutdown server\nPuma starting in single mode...\nVersion 3.4.0 (ruby 2.3.1-p112), codename: Owl Bowl Brawl\nMin threads: 0, max threads: 16\nEnvironment: development\nListening on tcp://localhost:3000 Use Ctrl-C to stop\n```\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n2016-11-08 11:03:34 +0530: HTTP parse error, malformed request (): #\n2016-11-08 11:03:34 +0530: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.4.0 Owl Bowl Brawl\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n\n```\n. @dawidof Is your above issue fixed, i'm also facing same issue!\n. @dawidof Thanks. After clearing the cookies, cache browsing history it working fine, may i know why this occurs?\n. ",
    "rainerborene": "@pavan461 make sure not to use https:// in development, or if you do configure it appropriately.. ",
    "felixbuenemann": "I had this happening for a health check, because the the rails app was configured with config.force_ssl = true, but the check was bypassing the load balancer terminating ssl, so the check received a redirect to something like https://localhost:5000/ping causing the error.\nI had to do some low-level debugging with wireshark to find the issue, so maybe this helps others.\nThe solution in this case was to add a request header X-Forwarded-Proto: https to the health checks.. Shouldn't the dhparam be configurable? Eg. With 2048 Bit RSA keys it's recommended to use a 2048 dhparam. Alternatively, the code could embed multiple params and choose the size matching the key, but that would lead to incompatibility with java 6 which some users might care about.\n. The comment by @memiux relates to the rating by ssllabs, a great tool for verifying SSL configuration.\n. ",
    "beitaz": "When i use http://localhost:3142/api/v1/users got the same error HTTP parse error, malformed request (): #<Puma::HttpParserError: Invalid HTTP format, parsing fails.>, but access 127.0.0.1:3142/api/v1/users is ok.. ",
    "bporterfield": "Yes thanks, a-la thin's options: \n\"--ssl\", \"Enables SSL\"\n\"--ssl-key-file PATH\", \"Path to private key\"\n\"--ssl-cert-file PATH\", \"Path to certificate\"\n\"--ssl-verify\", \"Enables SSL certificate verification\"\nI may be an unusual case, since most people will probably have something in front of puma in production that supports ssl, but for me it's an important option to have. \n. excellent, ty!\n. FYI running puma w/ssl could be difficult on Jruby until https://github.com/jruby/jruby-ossl/issues/20 is fixed.\n. Apologies for delay. You're correct, I do not need jruby-rack.\nI'm still seeing similar issues with a much simpler case, although I'm not familiar with threading and Ruby so I may be off with this test. Please let me know.\nGemfile\n```\nsource :rubygems\ngem 'rack'\ngem 'puma', :git => \"https://github.com/evanphx/puma.git\"\ngem 'sinatra'\n```\nconfig.ru\nrequire 'tester'\nTester.run!\ntester.rb\n```\nrequire 'sinatra/base'\nclass Tester < Sinatra::Base\n    set :server, :puma\nget '/short' do\n    sleep(1)\n    \"Hello!\"\n end \nget '/long' do\n    sleep(10) \n    \"Long Hello!\"\n  end\nend\n```\nI start this with: bundle exec rackup, first calling /long and then /short. The result is that /short seems to wait for /long before returning. My expectation is that it would not?\nI thought maybe my idea of how sleep & threading worked together could be wrong, so I tried to print out current thread during each request using Thread.current. It appears as though each request uses the same thread (which would explain the delay before the second request returns, but was not what I was expecting).\n. Testing again today, I did receive the thread exception again. I made a few long/short requests (from code above) and at some point (after probably 5 or 6 requests) the error occurred. The server did not seem to handle concurrent requests before the error either\nbundle exec rackup\nPuma 0.9.3 starting...\n- Min threads: 0, max threads: 16\n- Listening on tcp://0.0.0.0:3000\n  == Sinatra/1.3.2 has taken the stage on 3000 for development with backup from Puma\n  Exception in thread \"RubyThread-7: /Users/bporterfield/.rvm/gems/jruby-1.6.5.1@puma2/bundler/gems/puma-c2e6206e599e/lib/puma/thread_pool.rb:49\" java.nio.channels.IllegalBlockingModeException\n  at java.nio.channels.spi.AbstractSelectableChannel.configureBlocking(AbstractSelectableChannel.java:257)\nfull stack trace: http://pastebin.com/1DKtdAU6\n. reproduced with -Xbacktrace.style=raw ->\njruby -Xbacktrace.style=raw -S bundle exec rackup\nPuma 0.9.3 starting...\n- Min threads: 0, max threads: 16\n- Listening on tcp://0.0.0.0:3000\n  == Sinatra/1.3.2 has taken the stage on 3000 for development with backup from Puma\n  Exception in thread \"RubyThread-11: /Users/bporterfield/.rvm/gems/jruby-1.6.5.1@puma2/bundler/gems/puma-c2e6206e599e/lib/puma/thread_pool.rb:49\" java.nio.channels.IllegalBlockingModeException\n  at java.nio.channels.spi.AbstractSelectableChannel.configureBlocking(AbstractSelectableChannel.java:257)\n  at org.jruby.util.io.SelectBlob.tidyUp(SelectBlob.java:335)\n  at org.jruby.util.io.SelectBlob.goForIt(SelectBlob.java:86)\n  at org.jruby.RubyIO.select_static(RubyIO.java:3297)\n  at org.jruby.RubyIO.select(RubyIO.java:3293)\n  at org.jruby.RubyIO$s$0$3$select.call(RubyIO$s$0$3$select.gen:65535)\n  at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:69)\n  at org.jruby.ast.CallManyArgsNode.interpret(CallManyArgsNode.java:59)\n  at org.jruby.ast.LocalAsgnNode.interpret(LocalAsgnNode.java:123)\n  at org.jruby.ast.IfNode.interpret(IfNode.java:111)\n  at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\nfull stack trace: http://pastebin.com/TmgZrqJx\n. Using https://github.com/evanphx/puma/pull/39, the requests return as expected. However, I am still able to reproduce the exception (although it seems to take quite a bit more requesting for it to happen).\n. Just an update - I still consistently see this every time I start the server (within a few requests). It does not appear to affect the functionality, but is a little worrying :D\nAlso reproduced with jruby 1.6.6\n. I saw the malloc error when I used the puma gem direct from github with bundler. After some poking around I realized that bundling using git (gem 'puma', :git => https://github.com/puma/puma.git) does not build the java extension - it only builds the c extension. Forking the gem myself, building the java extension manually, and pointing bundler at my local path (gem 'puma', :path => ~/stuff/puma) resolved the issue. I'm guessing that Jruby's experimental c compilation stuff actually kinda worked, and that's why the error isn't consistent? \nWhen puma 1.0 was released, the java platform gem was not pushed right away (https://github.com/puma/puma/issues/49) so a re-install of the gem may be all you need.\n. Re-opened issue #42 I had closed previously: https://github.com/puma/puma/issues/42\n. This apparently is the result of my own misunderstanding with bundler. I assumed it would know the difference and compile appropriately. Pulling puma from rubygems does result in the gem being compiled with java extensions. So, \ngem 'puma'\npulling from rubygems works fine in jruby, while\ngem 'puma', :git => 'https://github.com/evanphx/puma.git'\ndoes not, because bundler compiles the c extension\n. Re-opening. Don't know if this is puma or bundler problem really though - I'm not sure if there's anything you can add to enforce bundler's compilation of java extension? \n. Thanks so much for the quick update :D\n. Ah! Didn't realize it hadn't been released yet. Bad assumption on my part :D\nI pulled a few hours ago to test, so I reckon I've tried with the most recent.\n. Sure. I've cloned the repo from master into ~/puma and built puma_http11.jar using rake compile. I can't just use bundler and gem 'puma', :git => path_to_repo b/c of #42 (which I think is a bundler issue and not yours).\nGemfile:\nsource :rubygems\ngem 'jruby-openssl'\ngem 'puma', :path => \"~/puma\"\nI'm using the keys from examples/puma. Using the config.ru I provided above, the working case (http://localhost:3000) looks like:\nbporterfield@ ~/dev/pumatest$bundle exec puma -b tcp://127.0.0.1:3000 config.ru\nPuma 1.6.0 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://127.0.0.1:3000\nUse Ctrl-C to stop\ncall\n127.0.0.1 - - [25/Jul/2012 18:37:03] \"GET / HTTP/1.1\" 200 - 0.0030\ncall\n127.0.0.1 - - [25/Jul/2012 18:37:03] \"GET /favicon.ico HTTP/1.1\" 200 - 0.0010\nand the failure case(https://localhost:3000) looks like:\nbporterfield@ ~/dev/pumatest$ bundle exec puma -b 'ssl://127.0.0.1:3000?key=puma/puma_keypair.pem&cert=puma/cert_puma.pem' config.ru\nPuma 1.6.0 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on ssl://127.0.0.1:3000?key=puma/puma_keypair.pem&cert=puma/cert_puma.pem\nUse Ctrl-C to stop\ncall\nIn the failure case, Chrome just spins until I hit the stop button, after which call is output.\nruby -v:jruby 1.6.7 (ruby-1.9.2-p312) (2012-02-22 3e82bc8) (Java HotSpot(TM) 64-Bit Server VM 1.7.0_04) [darwin-x86_64-java]\nChrome version: Version 20.0.1132.57\nIn Firefox the results look slightly different:\nbporterfield@ ~/dev/pumatest$ bundle exec puma -b 'ssl://127.0.0.1:3000?key=puma/puma_keypair.pem&cert=puma/cert_puma.pem' config.ru\nPuma 1.6.0 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on ssl://127.0.0.1:3000?key=puma/puma_keypair.pem&cert=puma/cert_puma.pem\nUse Ctrl-C to stop\ncall\n127.0.0.1 - - [25/Jul/2012 19:01:05] \"GET / HTTP/1.1\" 200 - 0.0030\n2012-07-25 19:01:05 -0700: Read error: #<IOError: Broken pipe>\norg/jruby/ext/openssl/SSLSocket.java:602:in `syswrite'\n/Users/bporterfield/.rvm/gems/jruby-1.6.7@global/gems/jruby-openssl-0.7.6.1/lib/1.9/openssl/buffering.rb:317:in `do_write'\n/Users/bporterfield/.rvm/gems/jruby-1.6.7@global/gems/jruby-openssl-0.7.6.1/lib/1.9/openssl/buffering.rb:335:in `write'\n/Users/bporterfield/dev/puma/lib/puma/server.rb:512:in `handle_request'\n/Users/bporterfield/dev/puma/lib/puma/server.rb:301:in `process_client'\n/Users/bporterfield/dev/puma/lib/puma/server.rb:214:in `run'\norg/jruby/RubyProc.java:258:in `call'\n/Users/bporterfield/dev/puma/lib/puma/thread_pool.rb:94:in `spawn_thread'\nSame story here, Firefox spins until I hit stop, after which there is output from puma to the console (starting with call). Firefox v10.0.2.\nIn Safari, the request works immediately. Safari Version 5.1.7 (7534.57.2).\nLet me know if there's anything else I can provide!\n. So far from my testing the problem seems to remain - at least on Chrome and Firefox. I've pulled the latest this AM and am running master branch. Anything additional info can provide?\n. Sure - maybe I'm just hitting some strange bundler/rvm issue or something. At any rate, my config.ru:\nclass Test\n  def call(env)\n    puts 'call'\n    return [200, {}, [\"hello\"]]\n  end\nend\nrun Test.new\nGemfile:\n```\nsource :rubygems\ngem 'rack'\ngem 'jruby-openssl'\ngem 'puma', :path => \"~/dev/puma2\", :branch => 'master'\n```\nstarting the server and hitting it with chrome:\n```\nbundle exec bin/puma -b 'ssl://127.0.0.1:3000?key=puma_keypair.pem&cert=cert_puma.pem' config.ru\nPuma 1.6.0 starting...\n Min threads: 0, max threads: 16\n Environment: development\n* Listening on ssl://127.0.0.1:3000?key=puma_keypair.pem&cert=cert_puma.pem\nUse Ctrl-C to stop\n```\nChrome is now spinning, and once I hit stop on chrome, \"call\" is output.\njruby version:\nruby -v\njruby 1.6.7 (ruby-1.9.2-p312) (2012-02-22 3e82bc8) (Java HotSpot(TM) 64-Bit Server VM 1.7.0_04) [darwin-x86_64-java]\n. Anything else I can provide here to help?\n. Thanks for looking into this again! I'll take a look tonight and see if one of the way's I've monkey-patched OpenSSL for my project will make this work for me in it's current state\n. I've attempted to create a simple gist of this bug: https://gist.github.com/3531448\nWorks in 1.9.3, raises 'read would raise' in JRuby 1.7-pre2.\nCan you confirm that this accurately represents the problem before I submit to JRuby issues? This stuff is pretty new to me so another opinion is helpful.\n. I actually was testing in 1.7-pre2 - looks like it still exists there. I'll submit the bug. Thanks for the work on MiniSSL stuff, that will be great!\nOn that note - today in Puma, SSL handshake happens in the accept call to an SSLServer - this suggests to me that handshakes block the listen loop, so lots of SSL handshakes could slow down the server. Would moving the handshake to the worker thread increase performance?\n. Thanks @headius for taking a look! You're correct, the reported bug (https://gist.github.com/bporterfield/3531448) is no longer broken in JRuby 1.7+.\nHowever, @evanphx wrote the reactor changes in a way that is still broken in JRuby because of a different JRuby ossl bug: https://gist.github.com/bporterfield/3557313.\nI've got a ticket filed for this bug: https://jira.codehaus.org/browse/JRUBY-6874. Would love some attention to it - we're using a much much older version of Puma as a consequence.\n. This is because of http://jira.codehaus.org/browse/JRUBY-6874\nPlease comment on the bug if you want to get Puma support for SSL + JRuby!\n. @headius actually the bug listed above is the exact problem Puma hits - I actually discovered it trying to figure out why Puma with SSL was breaking. This code from Puma will look very similar to the code submitted with the bug: https://github.com/puma/puma/blob/master/lib/puma/reactor.rb#L31\n(bug code: https://gist.github.com/bporterfield/3557313)\n. quick note: the linked code in Puma is actually wrapped by another IO.select in server.rb (which is why the puma code shown above is not exactly same as bug listed). If you combined the code in server.rb and reactor.rb, you'd have code that results in the bug.\n. @headius there is a ticket filed for the JRuby SSL bug that stops SSL from working. The bug can be found here: http://jira.codehaus.org/browse/JRUBY-6874\n. ",
    "dnagir": "I am not sure if that's related, but aren't you suppose to mange the sequel connection if you en it on every request?\nhttp://sequel.rubyforge.org/rdoc/files/doc/opening_databases_rdoc.html\nIs there a similar issue without using sequel at all?\n. ",
    "headius": "Poking at it now.\n. If you can add -Xbacktrace.style=raw to JRuby we might get more information. That exception is normally from the JDK's IO APIs...a either the IO channel in play can't be set back to blocking while it is still registered for \"select\", or it can't be set to nonblocking in the first place.\nNeed more input. Ideally you should reproduce this without Sequel involved, since the issue is unlikely to be related to Sequel (and if it is, it would be a Sequel bug).\n. rake-compiler seems to be incompatible with either moving the extension (under puma/) or with removing the PumaHttp11Service.java file entirely. I'm trying to find another way to build.\n. Taking a few steps back...\npuma-0.9.3 does not appear to work on JRuby at all because of the extension move. Puma build from git and installed has the same problem. If I add Thread.abort_on_exception=true early in load, I get the following:\n```\nNameError: uninitialized constant Puma::Server::HttpParser\n   const_missing at org/jruby/RubyModule.java:2609\n  process_client at /Users/headius/projects/jruby/lib/ruby/gems/shared/gems/puma-0.9.3-java/lib/puma/server.rb:206\n             run at /Users/headius/projects/jruby/lib/ruby/gems/shared/gems/puma-0.9.3-java/lib/puma/server.rb:143\n            call at org/jruby/RubyProc.java:269\n            call at org/jruby/RubyProc.java:219\n    spawn_thread at /Users/headius/projects/jruby/lib/ruby/gems/shared/gems/puma-0.9.3-java/lib/puma/thread_pool.rb:66\n            call at org/jruby/RubyProc.java:269\n            call at org/jruby/RubyProc.java:223\nNameError: uninitialized constant Puma::Server::HttpParser\n   const_missing at org/jruby/RubyModule.java:2609\n  process_client at /Users/headius/projects/jruby/lib/ruby/gems/shared/gems/puma-0.9.3-java/lib/puma/server.rb:206\n             run at /Users/headius/projects/jruby/lib/ruby/gems/shared/gems/puma-0.9.3-java/lib/puma/server.rb:143\n            call at org/jruby/RubyProc.java:269\n            call at org/jruby/RubyProc.java:219\n    spawn_thread at /Users/headius/projects/jruby/lib/ruby/gems/shared/gems/puma-0.9.3-java/lib/puma/thread_pool.rb:66\n            call at org/jruby/RubyProc.java:269\n            call at org/jruby/RubyProc.java:223\n```\nThis is because the new location for the puma_http11.jar does not correspond to the package naming for the class PumaHttp11Service that it looks to load. The require is 'puma/puma_http11', so JRuby looks for the PumaHttp11Service class under the 'puma' package.\nUnfortunately, just adding the package specification and moving the .java file doesn't work either, since rake-compiler appears to only look for PumaHttp11Service.java and not the packaged version. I will try to figure out how to make it do that.\nIn the future it would be helpful if you could test the JRuby version of the gem when there have been structural changes, or feel free to ask me or another JRuby person to test it before it is released.\n. Modifying the rake-compiler task to be \"puma/puma_http11\" does not appear to work. It ends up with a bad Java command line. I may have to remove rake-compiler from the build under JRuby.\n. Ok, I figured out the minimal change necessary: adding the package.\nThe package causes the PumaHttp11.class file to get generated into the right package structure. The location of the PumaHttp11Service.java file is a bit nonstandard (not in package structure) but it appears to work ok. I've removed the other commits and forced this one. The gem appears to work correctly again.\n. @luislavena Can you toss me an email and we'll talk about rake-compiler there? The way it builds extensions is the \"classic\" way, but I've been thinking of (and using) different mechanisms for loading ext. Since you're an ext/building expert, I'd like your thoughts :)\n. So it seems like this might be Rack double-initializing, but I haven't traced why it's doing it.\n. Poking at it again today. Gotta get this tab count down :)\n. I'm closing this for now...don't have time to investigate, @evanphx wasn't able to reproduce, and it's non-fatal anyway.\n. Would be nice for someone to file a separate bug for either Puma or Bundler to get the git repo to build the java version properly under that Gemfile setup.\n. Since someone's having an issue with MRI as well, perhaps this is simply the closed connection taking a while to go away? Or is something getting spun up that doesn't die right away?\nCtrl-C on JVM would normally do a forced shutdown of the JVM, but it's possible through normal Ruby signal APIs to rebind it. I'm not sure what Puma does, but Mongrel did bind it to a shutdown sequence.\n. UNIX sockets underwent a rewrite in 1.7.0, so it's probable there's a regression here (or otherwise just a bug). Probably should just shift this to a JRuby bug (GH issues is fine).\n. This could probably be reevaluated. I will try to get to it myself some day, but if someone else wants to poke at it we'd appreciate it.\nEither the new ossl stuff @evanphx wrote or the now-bundled-and-fixed-and-working ossl library in JRuby 1.7+ should be able to support this.\n. JRuby's SSL support should be compatible with Puma now, but I think some footwork is required to get Puma back on JRuby's SSL...\n. Perhaps we can get SSL support turned on? Since you had to turn it off, we've iterated several times getting nonblocking working, and I think standard OpenSSL logic ought to work ok now. If it doesn't, I should be able to fix it. What do I need to do to move that forward?\n. Yeah, I spent several hours trying to figure out how to fix 6874 properly only to realize the problem was in buffering.rb, where a Ruby-land buffer is filled up, draining both SSLSocket's buffers and the underlying stream.\nThe test case I used for JRuby also fails in Ruby 1.9 and 2.0. As far as I can tell there's no way to reliably use select with an SSLSocket due to the many layers of buffering and stream-draining logic.\nPerhaps this is why @evanphx would rather get MiniSSL working in JRuby.\n. UNIXServer.for_fd could now be implemented in JRuby, but not TCPServer.for_fd. The former already is native and uses an fd, while the latter uses Java IO logic.\n. Filed jruby/jruby#940.\n. The implementation in MiniSSL.java looks correct at a glance. We have also done a lot of work in the past year on getting OpenSSL's SSLSocket to work properly, so that may be an option where it was not before.\nIf you'd rather proceed with MiniSSL.java, I'm standing by to help. Is there something that's not working at present?\n. If this was not reported to JRuby, and it's still possible to do so...please do so :-) This is a bug in the JIT...it is picking a fixed-size scope for some body of code when it actually needs a variable-size scope. This also may be gone in more recent versions of JRuby.\n. Sorry I've been away...looking now.\n. It's a lot of code to review but it looks ok at a glance. @evanphx if you want a deeper review, I can try to do it this week, but @dmarcotte's testimonial about this being solid is good enough for me right now.\n. Could be a JRuby bug, but 9.0.1.0 is over a year old, and there have been hundreds of fixes since then.\n@SimonKaluza Can you try with JRuby 9.1.5.0?\n. @SimonKaluza Hopefully I haven't missed out on your run for the night, but if you pass -Xbacktrace.style=full to JRuby we might be able to get more context out of any trace that results.\nOh, and I just noticed you said this fails in Ruby 2.2.2 as well? If that's the case, it really takes the spotlight off JRuby and puts it back on Puma, eh @evanphx?\n. What Java version is this? That looks like an internal JDK bug.\n. Do you get this just starting up puma with JRuby? Maybe we can try to reproduce with a trivial app.\nIf you can, try a more recent JDK, like any recent Java 8 (1.8).\n. @SimonKaluza Well, that's good news! It seems to support my belief that the issue you saw with 9.1.5.0 + JDK 7 was a JDK bug. Here's hoping the SocketError is gone too, or else I'll have to convince @etehtsea to help us investigate this :-D\n. Oops, never saw this. @enebo or I would be fine, but I'm traveling now.\nOn Mon, Jan 21, 2019, 01:00 Olle Jonsson <notifications@github.com wrote:\n\n@enebo https://github.com/enebo Would you be able to give this a\nreview?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1691#issuecomment-455894772, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAnl0dBS4Evc2LOjRQwiBlgecv3eKOoks5vFMPKgaJpZM4ZcCAV\n.\n. \ud83d\udc4d Yay, thanks for getting a release out!. That was a typo on my part. I'm removing those files as you suggest below.\n. \n",
    "nicksieger": "@bporterfield do you need jruby-rack in your gemfile?\n. ",
    "iconara": "I see this too in a basic pure-Rack app (run lambda { |env| [200, {}, []] }). I fire it up in Puma and hit reload as fast as I can in the browser, this error is printed quite a lot:\nException in thread \"RubyThread-3: /Users/theo/.rvm/gems/jruby-1.6.7@bomcl/gems/puma-0.9.5-java/lib/puma/thread_pool.rb:55\" java.nio.channels.IllegalBlockingModeException\n    at java.nio.channels.spi.AbstractSelectableChannel.configureBlocking(AbstractSelectableChannel.java:257)\n    at org.jruby.util.io.SelectBlob.tidyUp(SelectBlob.java:348)\n    at org.jruby.util.io.SelectBlob.goForIt(SelectBlob.java:87)\n    at org.jruby.RubyIO.select_static(RubyIO.java:3366)\n    at org.jruby.RubyIO.select(RubyIO.java:3362)\n    at org.jruby.RubyIO$s$0$3$select.call(RubyIO$s$0$3$select.gen:65535)\n    at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:69)\n    at org.jruby.ast.CallManyArgsNode.interpret(CallManyArgsNode.java:59)\n    at org.jruby.ast.LocalAsgnNode.interpret(LocalAsgnNode.java:123)\n    at org.jruby.ast.IfNode.interpret(IfNode.java:111)\n    at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n    at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n    at org.jruby.ast.IfNode.interpret(IfNode.java:119)\n    at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n    at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n    at org.jruby.ast.IfNode.interpret(IfNode.java:117)\n    at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\n    at org.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n    at org.jruby.ast.WhileNode.interpret(WhileNode.java:131)\n    at org.jruby.ast.NewlineNode.interpret(NewlineNode.java:104)\nI see it both in Java 1.6 and 1.7u4 running JRuby 1.6.7\n. I see this error too after a while when load testing with around 500 concurrent clients. Same version of Puma, same error:\nError in reactor loop escaped: undefined method `timeout_at' for nil:NilClass (NoMethodError)\n/media/ephemeral0/apps/dada-light/unpacked/gems/puma-1.6.3-java/lib/puma/reactor.rb:78:in `run'\n/media/ephemeral0/apps/dada-light/unpacked/gems/puma-1.6.3-java/lib/puma/reactor.rb:95:in `run_in_thread'\n. Can't say for sure since this was a concurrency issue, but I haven't been able to force this issue on v2.0.0.b7 so it looks like the patch did the trick. Thanks! \n. ",
    "Overbryd": "Same issue here. Seems to be not related to a specific requests, it just happens occasionally.\nIf there is anything I can help to fix this issue, I'd be happy to help you out!\n@headius @evanphx Is this issue still on your radar? How can we help?\nI was using 1.6.7 in 32bit mode:\njruby -v\njruby 1.6.7 (ruby-1.9.2-p312) (2012-02-22 3e82bc8) (Java HotSpot(TM) Client VM 1.6.0_31) [darwin-i386-java]\nStacktrace: https://gist.github.com/2658217\n. ",
    "scizo": "I'm also seeing this in my logs. I am running jruby 1.6.7 and puma 1.4.0. It has no effect on the functionality of my app.\n. ",
    "dgies": "My app recently switched to puma and this exception is cropping up every time we run tests with cucumber/selenium, causing all tests to fail.  Using puma 1.5.0 with jruby 1.6.7.2.\njruby 1.6.7.2 (ruby-1.8.7-p357) (2012-05-01 26e08ba) (Java HotSpot(TM) 64-Bit Server VM 1.6.0_45) [darwin-x86_64-java]\n. ",
    "luislavena": "Updated, cheers!\n. Awesome, thank you!\n<3 <3 <3 <3 !!!\n. > Modifying the rake-compiler task to be \"puma/puma_http11\" does not appear to work. It ends up with a bad Java command line. I may have to remove rake-compiler from the build under JRuby.\n@headius would you mind report this to rake-compiler? I would love get that fixed at the root instead of bandaids on all the projects.\n. @pickerel seems you need OpenSSL headers and libraries to be able to build Puma.\nPlease take a look to RubyInstaller list and search for latest OpenSSL 1.0.0k:\nhttps://groups.google.com/d/topic/rubyinstaller/0rcD9Jlerh4/discussion\nAnd here on how to extract and use knapsack packages:\nhttps://groups.google.com/d/topic/rubyinstaller/4ulo3o1bQOM/discussion\nTake a look to the instructions, not the package versions.\nThen you can do:\ngem install puma --pre -- --with-opt-dir=C:/knapsack/x86-windows\nThat should do the trick.\n. @jtomaszewski hello,\nWhat is this:\nC:/Ruby193/bin/ruby.exe extconf.rb --with-opt-dir=C:/Ruby193/devkit/x64\nAre you trying to use 64bits DevKit with Ruby 1.9.3? Ruby 1.9.3 is only 32bits (x86).\nThis is indicated in RubyInstaller download page:\nhttp://rubyinstaller.org/downloads\nLast (but no least) you need to use the 32bits package of OpenSSL, not mixing with 64bits.\nMakes sense?\n. @chenzx File uploads are not allowed on GitHub anymore, you can find the download URLs for OpenSSL in RubyInstaller repository:\nhttps://github.com/oneclick/rubyinstaller/blob/master/config/dependencies.rb#L18-L29\nRoot URL: http://packages.openknapsack.org/openssl\nPackage files:\n- openssl-1.0.0k-x86-windows.tar.lzma\n- openssl-1.0.0k-x64-windows.tar.lzma\nFiles are hosted on S3/Amazon, hope is not blocked that too.\n. The warning is caused by rdoc generating documentation. Simply ignore it or\nupdate rdoc gem (gem updated rdoc)\nSorry for top posting. Sent from mobile.\nOn May 21, 2013 11:45 PM, \"Chen Zhixiang\" notifications@github.com wrote:\n\nThanks for response.\nI've successfully install puma-2.0.1 on Windows 7(64bit machine),\nRuby-2.0. as a x86 Win32 mode.\nThere is a warning:\nC:\\knapsack\\x86-windows>gem install puma --pre --\n--with-opt-dir=C:/knapsack/x86-windows\nBuilding native extensions with: '--with-opt-dir=C:/knapsack/x86-windows'\nThis could take a while...\nSuccessfully installed puma-2.0.1\nParsing documentation for puma-2.0.1\nunable to convert \"\\x90\" from ASCII-8BIT to UTF-8 for\nlib/puma/puma_http11.so, skipping\nI think it should not matter.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/202#issuecomment-18254439\n.\n. You need to separate gem options from gem compile ones, use double dashes:\n\ngem install puma -- --with-opt-dir=...\n. Hello @kdnmih, to build puma you will need OpenSSL library and headers.\nRubyInstaller binaries are build with OpenSSL, but the headers and libraries to link against are not shipped with it.\nYou will need to download the 64bits package of openssl that is used with RubyInstaller, extract it into a folder and then, when installing Puma, indicate it where to find it.\nIf you search at RubyInstaller group, you will see several posts mentioning Knapsack packages, OpenSSL is one.\nTo give you a summary of things you need to do:\n- Download OpenSSL package for x64-windows (since you're using x64 version of Ruby): http://packages.openknapsack.org/openssl/openssl-1.0.0k-x64-windows.tar.lzma\n- Extract the package, as indicated in several RubyInstaller posts\n- Proceed again with gem installation and point to the directory where you extracted OpenSSL\nThe commands will be something like this:\nC:\\>mkdir C:\\Knapsack\\x64-windows\nC:\\>cd C:\\Knapsack\\x64-windows\nC:\\Knapsack\\x64-windows>bsdtar --lzma -xf openssl-1.0.0k-x64-windows.tar.lzma\nC:\\>gem install puma --platform=ruby -- --with-opt-dir=C:/Knapsack/x64-windows\nPlease let us know if this works for you.\n. @lozandier sqlite3 no longer requires building since 1.3.8, neither pg gem since 0.17.0\nPerhaps is worth updating your Gemfile and bundle update those dependencies.\n. C:/row/Ruby21/bin/ruby.exe -r ./siteconf20150202-6560-i9u6l7.rb extconf.rb --with-opt-dir=C:/Knapsack/x86-windows\nLooks like you have RubyGems 2.4.x, which is broken for Windows.\nDowngrade to 2.3.0 or 2.2.x to correct this.\n. See commit message: \nhttps://github.com/luislavena/puma/commit/935a5569eaf0a805bd68ba7886c0bcc44849d171\n. gemspec didn't specify minimum Ruby version, File.read is from 1.8.7 (not 1.8.6), will fail for ppl still using 1.8.6.\nI can change gemspec to be 1.8.7 and greater and then use File.read instead, cool?\n. > gemspec didn't specify minimum Ruby version, File.read is from 1.8.7 (not 1.8.6), will fail for ppl still using 1.8.6.\nIgnore my stupid comment, 1.8.6 did had File.read, my brain is confused. Will change the code.\n. ",
    "celsodantas": "Same here.\n```\n $  ruby -v\n jruby 1.6.6 (ruby-1.8.7-p357) (2012-01-30 5673572) (Java HotSpot(TM) 64-Bit Server VM 1.6.0_29) [darwin-x86_64-java]\n$  rails s puma \n= > Booting Puma\n=> Rails 3.2.1 application starting in development on http://0.0.0.0:3000\n=> Call with -d to detach\n=> Ctrl-C to shutdown server\nPuma 0.9.3 starting...\n Min threads: 0, max threads: 16\n Listening on tcp://0.0.0.0:3000\n```\nI just created a new rails app, added gem 'puma', ran bundle and ran rails s puma and I can't access the application.  It just hung there. No response.\nAnd same thing on a Windows machine.\nIf I find any news, I'll post here.\n. It works if I downgrade to 0.9.2.\n. I did it and it works fine!  =D\nboth on Windows 7 and Mac 10.7.\nJRuby 1.6.7\nRails 3.2.1 and 3.2.2\nThanks!\n. ",
    "jrochkind": "While I'm not sure if it can be responsible for that kind of 'malloc' exception, I suspect you really need to set config.threadsafe! in a rails app used with puma, no?  Otherwise puma is going to be sending overlapping concurrent requests to the same rails app instance (right?), but rails is not neccesarily running such that request/action processing is actually threadsafe for this use. \n(On the other hand, I'm not entirely sure what config.threadsafe! actually does under the hood these days, it might not actually do anything important. but it (or the other config variables it triggers, like config.allow_concurrency) is documented to be neccessary for concurrent overlapping request processing. Search for threadsafe! in http://guides.rubyonrails.org/configuring.html)\nYou might want to see if this is still reproducible with config.threadsafe!.  And puma README probably ought to say in the Rails section that you want to config.threadsafe! (as well as be sure you aren't doing anything in app code that's not threadsafe under concurrent request handling)\n. After more reading and exploring... I think all of these are a non-issue, and clustered mode should Just Work without worrying about any of that. As in puma, each forked process is essentially booted all the way from zero.  \nThey should behave just as if they were completely seperate processes (say, old-style mongrel cluster), with no need for any special on_fork.  \nUnlike unicorn, that boots the rails app and then does an actual unix fork of an already booted rails app into multiple processes. That's what requires the gymnastics. But puma doesn't do that. \nI'm not sure if any special activity is required in the on_restart for the rolling restarts... I think maybe not there either, but it's still confusing for me to think about. \n. Thanks!\n. Phusion's writeup comparing puma and passenger  says that puma does support copy-on-write virtual memory for memory savings. \nI think maybe that's not actually true?  I can't find any mention of that in puma docs, and in my (potentially flawed) understanding, \"booting the app in each worker\" would be incompatible with actually getting any memory savings from copy-on-write, no?\n. I'm not having luck googling, any progress on this in other issues or other places?\nPuma with multiple threads/workers messes up my byebug use, which is crucial for my development workflow. \nIs there any way to set puma configuration in rails server dev mode?  Or any way to tell rails to go back to using webrick even if puma is in the gemfile? Or other solutions? . Thanks friends. \nHuh, so if Rails 5 is supposed to read the config... a) someone should prob close this issue, so it's more clear to someone googling like I was, and b) I wonder why it's not for me. \nNot working for me. puma 3.6.0. rails 5.0.1.  Config changes to workers/threads in ./config/puma.rb have no effect on rails server. Indeed, even if I add a raise or syntax error in ./config/puma.rb, it is not triggered, the puma.rb does not seem to be loaded at all from rails server -- even though the puma startup messages are in the console. \nThis is an app that has been upgraded many times and used to be rails4, sometimes that matters because some important piece of information was left out of initializers or config files, but I can't find or think of one here. \nNot really sure where to go from here. . oops, this issue was closed, but closed with a \"won't fix\", where in fact it was fixed? Okay then.... I have an older config file, gisted here which uses PUMA_MAX_THREADS instead of RAILS_MAX_THREADS. \nHowever, no matter what I set PUMA_MAX_THREADS to (or RAILS_MAX_THREADS), or what I set PUMA_WORKERS to: After starting in dev mode with rails server, I have one worker and 2 threads, I can get neither more nor less. Not even by hard-coding them in the ./config/puma.rb. Pretty certain the ./config/puma.rb is not being used at all. \n~~~\n[35547] Puma starting in cluster mode...\n[35547] * Version 3.6.0 (ruby 2.3.0-p0), codename: Sleepy Sunday Serenity\n[35547] * Min threads: 2, max threads: 2\n[35547] * Environment: development\n[35547] * Process workers: 1\n[35547] * Preloading application\n[35547] * Listening on tcp://localhost:3000\n~~~. @nateberkopec I'm not sure, that problem seems to be about options in config/puma.rb overwriting other options? My problem is kind of the reverse, options in config/puma.rb seem to be getting ignored, the entire config/puma.rb file seems to have no effect. \nBut I'll try it out when 3.7.1 is released. . So, in a different app (Rails 5.0.1, puma 3.6.2) that does not exhibit the problem, changes i make in config/puma.rb are reflected in puma startup logs when running rails server, and additionally if I put a byebug in config/puma.rb, then I do get a breakpoint triggered when running rails server.  All as expected and desired. \nIn the problematic app -- and of course I don't know what the difference is, but the problematic app is a much older app that has been upgraded through various versions of rails and puma but is currently rails 5.0.1 -- changes I make in config/puma.rb are not reflected in puma startup logs on rails server, and additionally a byebug in config/puma.rb does not trigger a breakpoint. It's as if config/puma.rb is not being loaded at all. \nIn the problematic app (Rails 5.0.1), this problem exhibits with puma: 3.6.0, 3.6.2, 3.7.0 and master at 6b96a9. Ugh, ignore me. Thanks for the replies, my fault. I got to debugging and discovered I had a config/puma/development.rb, so that was being used instead of config/puma.rb.  I didn't even know that was a puma feature. Some previous developer on this project must  left it there. Sorry, thanks!. OK, thanks. The problems did become repeatable, and I did eliminate them by switching my capybara web server driver from puma to webrick. But I can believe it's puma triggering a bug in MRI. \nI'm just a bit despondent of anyone figuring out what it is or doing anything about it. Will probably just keep using webrick rather than puma in my capybara tests. . ",
    "sorentwo": "Was this issue essentially covered by issue #42 ? If so can this be closed?\n. The most recent version cited here is 1.4.0. Can any of you confirm if this is still an issue with the 2.0 betas?\n. The app I was testing this out with is our production app, and I can't share it exactly. I can share the gemspec and environment files though.\nhttps://gist.github.com/2375692\nOnly the development config is included, but this would happen in all environments.\n. Truthfully I returned to using Unicorn after running into the issue, so it took me some time to get a free moment to investigate this.\n@ezkl Those precise steps don't quite work out, but not because of anything with Puma. With the asset pipeline disabled no precompilation happens at all, and with it enabled no stylesheets are precompiled, making the exercise futile.\n@evanphx I'll try to do this soon. Because the image mixup is random on different servers, and we're loading 20+ images on the affected pages I fear it would be difficult to replicate.\n. Yeah, the app is using Rack::Cache. In staging and production it is configured with Memcached/Dalli, nothing is configured in development.\n. Thanks @jtblin, @rtomayko and @evanphx. I never got back to trying to work through this and it haunted me a bit. Awesome to see that this got resolved though.\n. This request is over 9 months old now. I would guess the benchmarking code probably isn't forthcoming.\nAnyhow, all of the benchmarking is on the puma.io site, which has a separate repositorty: https://github.com/puma/puma.io\nI'd recommend asking over there, or maybe putting together a small project for automating the benchmarks. I doubt the numbers are consistent with the current version of each server now.\n. The speed comparison is now a chart rather than a table, meaning this doesn't seem to be relevant anymore.\n\n. It looks like this has been fixed here https://github.com/puma/puma/commit/cc91b6cd511787b3a6d37df7c89d45facadef75f https://github.com/puma/puma/issues/234\n. How large is your app? More specifically, are you loading a lot of gems? In my experience it is definitely possible to have requests queued while a dyno has loaded but the app isn't in a state to handle requests yet. More pertinent to the issue is that I have seen the behavior when running unicorn, more often than I have when running puma. \nMy hunch is that this is a heroku issue, not a puma issue. \n. anything tested and Travis looks finicky.\nOn a separate note, I'd highly recommend anybody to use their OS's init system to handle start/stop/reload tasks. That isn't really the job of Puma, and I'd argue that cap tasks don't even belong in this repo.\n. Ha, I read this as Sep 20, not \"Sep 20 2013\". Sorry!\n. @evanphx: Awesome to hear those are getting dropped.\n. @pawel2105 I'm having a somewhat similar problem, although I am using entirely custom deploy tasks (as we are using Capistrano 3). The server always appears to restart, but after some indeterminate period of time Nginx starts getting 502 errors. I can't tell if workers are dying, I'm really not sure.\nWould you mind sharing your custom cap tasks? I'd like to see how similar they may be to mine and go from there.\n. This is definitely still an issue. It's the only reason our apps are still using Unicorn right now.\n. @jcoleman: thanks, that is really helpful to know about. I wasn't aware of the specific point of failure here (the gems/Gemfile), it just seemed like we had random restart failures. \n. @evanphx: Nope, I wasn't. In fact, I'd never seen that wiki page. The last time I was setting things up was in November 2013, a little before the page was available. That's an excellent tip though. I'll try an experiment next week and let you know how things go.\n. Has this been resolved for you? I imagine you've moved on since the end of May.\n. There was lengthy discussion related to this in #416, which ended up in @evanphx mentioning https://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting. I've recently set up another clustered deployment and avoided using preload! and phased restarts, which has worked out beautifully so far.\n. 2.9.2 is out now, this doesn't seem relevant anymore.\n. The root cause is that something is already running on the port. Can you verify no other servers are running and then start up puma by itself, without spring or rails server? bin/puma -C config/puma.rb -b tcp://0.0.0.0:3000 Or simply try changing your port to something like 3001?\nIt doesn't look like this is a puma specific issue.\n. You may be interested in https://github.com/ngauthier/tubesock and https://github.com/ngauthier/sock-chat\n. @pctj101: Can you share which version of rails you saw this with?\n. @pctj101: Thanks, I was mostly asking to try and get the info for @tenderlove \n. You have preload_app! in your config, which you don't want to use with phased restart. There is a little about this in deployment\n. This seems like much more of a spring issue than a puma issue. Surely the same would hold for thin, unicorn, passenger, etc.\n. This is throwing an error at https://travis-ci.org/puma/puma/jobs/41340172#L186. You'll probably want to fix that up.\n. ",
    "ehoch": "Any shot some of these recent changes can warrant a version bump soon for gem users?\n. Kick ass!  Been loving puma even on my lame MRI implementation on Heroku.  Thanks!\n. Honestly, I've switched back to Unicorn for now.  I've run into some threadsafe! issues that I need to work out with my app before I can switch from processes to models.  \nThe main thing puma did for me was allow a second user to access my Dyno during IO blocking like a large image upload.  It was awesome for that.\nThat was until some of my rake tasks mysteriously broke from Rails / MRI threading problems..\n. No, I turned off threadsafe! and just run 3 unicorn workers.  It was the threadsafe! that was wrecking my rake tasks...\n. ",
    "toovs": "huh ok so I tried puma -b unix://tmp/sockets/puma.sock config.ru, but doesn't seem to be playing well with nginx, just gives me a bad gateway error. works fine if I don't use a socket and just use a custom port.\n. and my nginx setup works fine with unicorn's sockets.\n. ",
    "sxua": "+1\nalso tried to pair it with nginx, but all I've got is \"Bad Gateway\"\nthis configuration works well with Unicorn, but not with Puma\nnginx.conf:\nupstream puma {\n  server unix://tmp/puma.sock fail_timeout=0;\n}\nconfig/puma.rb:\nruby\nbind \"unix:///tmp/puma.sock\"\npidfile \"/tmp/puma.pid\"\nrunning as:\n~$ puma -C ./config/puma.rb config.ru\nPuma 1.0.0 starting...\n* Min threads: 0, max threads: 16\n* Listening on unix:///tmp/puma.sock\nUse Ctrl-C to stop\n. Just default Rails app\nruby\nrequire ::File.expand_path('../config/environment',  __FILE__)\nrun TestApp::Application\n. nginx/1.0.13\nOops, sorry, my bad. nginx logs shows me: *1 connect() to unix://tmp/puma.sock failed (13: Permission denied) while connecting to upstream\nchmod a+rw /tmp/puma.sock does the trick\n. OK, but what about killing server? Will it gracefully stop on KILL in \"&\" case?\n. ",
    "xpepermint": "Related to this #73\n. ",
    "isc": "The response body.\n. I've created a repository containing a dummy rails app which demonstrates the issue : https://github.com/isc/puma-content-length .\n. ",
    "kfatehi": "I get a similar error (Listen loop error), but with SSL, using Ruby 1.9.3p125\nMon Apr 23 15:43:08 -0700 2012: Listen loop error: #<OpenSSL::SSL::SSLError: SSL_accept SYSCALL returned=5 errno=0 state=SSLv3 read client certificate A>\n/Users/spike/.rvm/rubies/ruby-1.8.7-p357/lib/ruby/1.8/openssl/ssl-internal.rb:166:in `accept'\n/Users/spike/.rvm/rubies/ruby-1.8.7-p357/lib/ruby/1.8/openssl/ssl-internal.rb:166:in `accept'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/lib/puma/server.rb:217:in `run'\n(eval):3:in `each_without_optional_block'\n(eval):3:in `each'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/lib/puma/server.rb:213:in `run'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/lib/puma/server.rb:204:in `initialize'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/lib/puma/server.rb:204:in `new'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/lib/puma/server.rb:204:in `run'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/lib/puma/cli.rb:428:in `run'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/gems/puma-1.2.1/bin/puma:10\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/bin/puma:19:in `load'\n/Users/spike/.rvm/gems/ruby-1.8.7-p357@myapp/bin/puma:19\nIt is nonfatal, the request still is handled correctly, there are no noticeable effects as a result, but I see this in the log. My guess is that Puma is trying to read the client certificate, but there is none, and instead of failing that routine gracefully it outputs this error?\nThanks\n. ",
    "goddamnhippie": "I get the same error with MRI 1.9.3p194, even on puma 1.4.0\n2012-06-07 21:23:31 +0000: Listen loop error: #<OpenSSL::SSL::SSLError: SSL_accept SYSCALL returned=5 errno=0 state=unknown state>\n/usr/local/lib/ruby/1.9.1/openssl/ssl-internal.rb:164:in `accept'\n/usr/local/lib/ruby/1.9.1/openssl/ssl-internal.rb:164:in `accept'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-1.4.0/lib/puma/server.rb:217:in `block (2 levels) in run'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-1.4.0/lib/puma/server.rb:213:in `each'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-1.4.0/lib/puma/server.rb:213:in `block in run'\n. BTW I'm using Ubuntu Server 12.04 (32bit) and the site works, but that gets printed to the logs.\n. ",
    "rumblinthebronx": "I am getting a similar error while running puma-1.6.3 java on jruby,\n`` ruby\nError in reactor loop escaped: Detected invalid array contents due to unsynchronized modifications with concurrent users (ConcurrencyError)\norg/jruby/RubyArray.java:2494:indelete'\n/home/developer/.rvm/gems/jruby-1.7.2/gems/puma-1.6.3-java/lib/puma/reactor.rb:51:in run'\norg/jruby/RubyArray.java:1613:ineach'\n/home/developer/.rvm/gems/jruby-1.7.2/gems/puma-1.6.3-java/lib/puma/reactor.rb:26:in run'\n/home/developer/.rvm/gems/jruby-1.7.2/gems/puma-1.6.3-java/lib/puma/reactor.rb:95:inrun_in_thread'\n\ndeveloper@localhost ~ $ jruby --version\njruby 1.7.2 (1.9.3p327) 2013-01-04 302c706 on OpenJDK 64-Bit Server VM 1.7.0_09-b30 [linux-amd64]\ndeveloper@localhost ~ $ gem list puma\n LOCAL GEMS \npuma (1.6.3 java)\n```\n. ",
    "x3qt": "Experienced same issue on heroku during stress and security testing. Using rails 4.0,  ruby 2.0.0p247 and puma 2.5.0.\nError message:\n``\n2013-08-14T10:06:48.859102+00:00 app[web.1]: Error in reactor loop escaped: closed stream (IOError)\n2013-08-14T10:06:48.959732+00:00 app[web.1]: /app/vendor/bundle/ruby/2.0.0/gems/puma-2.5.0/lib/puma/reactor.rb:99:inclose'\n2013-08-14T10:06:49.061355+00:00 app[web.1]: /app/vendor/bundle/ruby/2.0.0/gems/puma-2.5.0/lib/puma/reactor.rb:99:in ensure in run'\n2013-08-14T10:06:49.166861+00:00 app[web.1]: /app/vendor/bundle/ruby/2.0.0/gems/puma-2.5.0/lib/puma/reactor.rb:100:inrun'\n2013-08-14T10:06:49.272039+00:00 app[web.1]: /app/vendor/bundle/ruby/2.0.0/gems/puma-2.5.0/lib/puma/reactor.rb:107:in `block in run_in_thread'\n```\nUpdate: Saw puma 2.5.1 was released. Will update and report soon.\nUpdate: 2.5.1 also affected.\n. Cast @evanphx \n. Seems to be fixed in 2.6.\n. @seuros :+1: \n. Yep, 2.7.1 affected.\n. ",
    "carlhoerberg": "i don't know.. :P \n``` sh\n$ gem install puma\nSuccessfully installed puma-1.1.0-java\n1 gem installed\n$ puma\nLoadError: no such file to load -- puma/compat\n  require at org/jruby/RubyKernel.java:1042\n  require at /Users/carl/.rvm/rubies/jruby-1.6.7/lib/ruby/site_ruby/1.8/rubygems/custom_require.rb:36\n   (root) at /Users/carl/.rvm/gems/jruby-1.6.7/gems/puma-1.1.0-java/lib/puma/server.rb:8\n  require at org/jruby/RubyKernel.java:1042\n  require at /Users/carl/.rvm/gems/jruby-1.6.7/gems/puma-1.1.0-java/lib/puma/server.rb:36\n   (root) at /Users/carl/.rvm/gems/jruby-1.6.7/gems/puma-1.1.0-java/lib/puma/cli.rb:4\n  require at org/jruby/RubyKernel.java:1042\n  require at /Users/carl/.rvm/gems/jruby-1.6.7/gems/puma-1.1.0-java/lib/puma/cli.rb:36\n   (root) at /Users/carl/.rvm/gems/jruby-1.6.7/gems/puma-1.1.0-java/bin/puma:6\n     load at org/jruby/RubyKernel.java:1068\n   (root) at /Users/carl/.rvm/gems/jruby-1.6.7/gems/puma-1.1.0-java/bin/puma:19\n``\n. but if i comment out line 8 inlib/puma/server.rb` it seems to work fine :)\n. ",
    "jlxw": "I'm getting this on latest HEAD cc91b6cd5117 any ideas?\n. ",
    "ywjno": "I also encountered this problem when uesd 1.1.0 version, so I used 1.0.0 version now.\n. ",
    "amolpujari": "RAILS_ENV=production not working with me\n. amol@sinhgad:~/plotwatt$ RAILS_ENV=production bundle exec puma -p 3000\nPuma starting in single mode...\n- Version 2.4.0, codename: Crunchy Munchy Lunchy\n- Min threads: 0, max threads: 16\n- Environment: development\n. ",
    "catsby": "Can you elaborate? Do you get errors, nothing, or maybe it runs in dev\nmode? Need more information.\nOn Friday, August 9, 2013, Amol Pujari wrote:\n\nRAILS_ENV=production not working with me\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/56#issuecomment-22413285\n.\n. Ah, seems Puma only cares about RAILS_ENV when used with capistrano. Can you use RACK_ENV or use -e instead? That should work:\n\nRACK_ENV=production bundle exec puma -p 3000\nor \nbundle exec puma -p 3000 -e production\nSee here https://github.com/puma/puma/blob/master/lib/puma/cli.rb#L252-L262 , where the environment is set from the RACK_ENV environment variable if no -e is given. \n. I just un-idled an app running puma 1.6.3 with no issue\n. Should be included in #254 , where I asked for clarification of the same. \n. This project uses Hoe to manage releases among other things, and depends on those files being named as they are. \n. Care to close this then? If you have changes to the content maybe open another pr separate from this one.\n.  I suspected it was generated, just didn't find where. I'll close this and look into shortening it with Hoe.\n. So, the only way I can see to do this with Hoe is to update the README and put shorter content in Description,  then put the existing description under another header. So, one is brief and the other has more detail. Thoughts on this? \n. Refactored:\n- shorter description under Description header\n- original description under new Built For Speed & Concurrency header\n- rendering: https://github.com/ctshryock/puma/blob/update_gem_description/README.md\nIn doing this I saw the gemspec updated with more things, I suspect a new release was done and the newest gemspec not committed to master? \nExample of the diffs:\ndiff\n-  s.rubygems_version = \"1.8.25\"\n-  s.summary = \"Puma is a simple, fast, and highly concurrent HTTP 1.1 server for Ruby web applications\"\n+  s.rubygems_version = \"2.0.3\"\n+  s.summary = \"Puma is a simple, fast, threaded, and highly concurrent HTTP 1.1 server for Ruby/Rack applications\"\n. Cool, I suspected that. I'll incorporate that and update. What about on_worker_boot use cases, or the multi-core / Rubinius / JRuby thing? \n. Updated with your note on thread-per-worker count.\n. Good questions. From my brief scanning of the code I believe they are both \"no\", but @evanphx would know better.\n. Updated with a comment about on_worker_boot. @evanphx look good to you? https://github.com/catsby/puma/blob/cluster_docs/README.md\n. Would you support the addition of said hook, or just recommend ActiveRecord::Base.disconnect! in the on_worker_boot block, so that each worker calls it?\n. The current README gives a brief example in the Clustered Mode section\nIt would likely be connection setup like this: https://devcenter.heroku.com/articles/concurrency-and-database-connections#multi-process-servers\n. Very true. Unfortunately I don't really know those answers :(\n. I sent a PR for this, but I'm confused by something. When I figure it out after some code spelunking I'll either send a new PR or update that one with tests. \n. It was my understanding that CoW benefits are really only applicable on Ruby 2, right?\nOtherwise the preload option would ensure the workers don't waste time loading the app before serving requests, correct? \nIn terms of documentation I feel like the latter has more significance here, assuming(?) that Ruby 2 adoption is smaller than 1.9.x. \n. Sure, and I agree, but Heroku isn't the only place to run ruby apps, and \"some other benefits\" are vague when we're talking about adding documentation :) \n. > question: is the preload_app! option the same as using --preload?\nyes\n. Something I've been wondering though: what is the advantage or implications, if any, of specifying min threads? I've seen people recommend matching them (ex. 5:5 min:max), but without much explanation.\n. This was just answered on Twitter in this thread:\n- https://twitter.com/dropmeme/status/356903963775344640\nIn short: preallocating allows faster immediate service; specifying a min number lets them be spun down when not needed. Sound accurate? \n. Are either of you using JRuby though? Clustered mode is not available on JRuby and not available on Windows. The current wording of \"JRuby and Windows\" may be misinterpreted as \"JRuby on Windows\", but it's my understanding to be \"JRuby and also Windows\". /cc @evanphx for clarity there, but JRuby/Windows lack fork(2) which Puma uses for workers.\n. Have you tried running in development mode? When I run an app with RACK_ENV=development puma config.ru I get development mode and stack traces. \n```\n$ RACK_ENV=development puma config.ru \nPuma 2.1.1 starting...\n Min threads: 0, max threads: 16\n Environment: development\n* Listening on tcp://0.0.0.0:9292\nUse Ctrl-C to stop\nStarted GET \"/\" for 127.0.0.1 at 2013-10-02 11:04:38 -0500\nProcessing by HomeController#index as HTML\nCompleted 500 Internal Server Error in 20ms\nNoMethodError (undefined method render_text' for #<HomeController:0x007fdac68c9508>):\n  app/controllers/home_controller.rb:3:inindex'\nRendered /Users/clint/.rbenv/versions/2.0.0-p247/lib/ruby/gems/2.0.0/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/_source.erb (0.8ms)\n  Rendered /Users/clint/.rbenv/versions/2.0.0-p247/lib/ruby/gems/2.0.0/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/_trace.erb (1.2ms)\n  Rendered /Users/clint/.rbenv/versions/2.0.0-p247/lib/ruby/gems/2.0.0/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/_request_and_response.erb (1.0ms)\n  Rendered /Users/clint/.rbenv/versions/2.0.0-p247/lib/ruby/gems/2.0.0/gems/actionpack-4.0.0/lib/action_dispatch/middleware/templates/rescues/diagnostics.erb within rescues/layout (22.3ms)\n```\nI think WEBrick assumes this mode, since WEBrick is a development server.\n. > However running puma with rails server does show a stack trace.\nWould setting something like config.consider_all_requests_local in your config/environments/production.rb address this? \nhttp://guides.rubyonrails.org/configuring.html\nAs Evan said, Puma likely isn't doing anything here, it's the Rails app dictating. \n. It's difficult to prove without seeing your app code, but I believe this is an application issue and not a Puma one. I cannot reproduce, I tried my own Sinatra test app, a Rails test app, and the example app you provided:\n\nCan you try capturing some request logs? What is the error you mention? \n. Same results, everything looks fine:\n\nNote that I ran RAILS_ENV=production bundle exec rake assets:precompile first. \n\nmaybe it is a problem with assets pipeline and caching, which is handled differently by other servers (thin, webrick)\n\nCan you expand on this? Puma gets a request, then serves it. Rails dictates the assets and caching. \n. I don't agree that this is necessary. Can you or @evanphx elaborate your reasoning / thoughts here? It's my understanding that --preload helps with loading the app before workers are forked, but the CoW benefits are inherit of the fork call regardless. \n. That does make sense now. I had some processes confused in my head regarding loading of the app. \nI would like this section to explain why preloading is a good idea, and mention that in MRI 2.0.0 you also gain the advantage of CoW\n. Would that require a separate gem, puma-rails or similar? Or something into Rails, like how Rails will pick up Thin automatically? I would love to see the latter. \n. @schneems I know I told you this was normal; I can't find the explanation I received. Strangely enough, on Puma 2.7.1 with 3 workers I'm only seeing 4 processes (as expected):\nconsole\n$ ps aux | grep puma         \nclint            4642   0.0  0.0  2432784    608 s000  S+    3:04PM   0:00.00 grep puma\nclint            4521   0.0  0.6  2508288  52352   ??  S     3:04PM   0:01.23 puma: cluster worker: 4499\nclint            4520   0.0  0.6  2517504  51608   ??  S     3:04PM   0:01.23 puma: cluster worker: 4499\nclint            4519   0.0  0.6  2518528  52636   ??  S     3:04PM   0:01.23 puma: cluster worker: 4499\nclint            4499   0.0  0.3  2479068  25536   ??  S     3:04PM   0:00.82 ruby /Users/clint/.rbenv/versions/2.1.1/bin/puma -p 5000 -C config/puma.rb -e development\n. ah, I was using the go version of Foreman, which would explain why I didn't see the extra ruby processes when using it locally... \n. How is this different than the current on_worker_boot? \nIt was my (possibly wrong) understanding that on_worker_boot is invoked after the worker has booted. \nMy experimentation suggests this is the case as well, so if you have code that demonstrates the difference that would be rad too :) \n. @rubencaro thanks for expounding!\n. Great point. Are you suggesting further refactoring the method or :-1: the PR? \n. No worries! Minor nitpick, minor effort :)\nThanks for review \n. OK, so it wouldn't affect production install. Is there draw back to moving them to group development? \n. OK, thanks again\n. Are you experiencing errors in your logs indicating requests aren't begin fulfilled because of no database connection? Are you doing load testing, such that you know all 64 threads are actively hitting the database? Are you using Clustered mode in Puma?\n. >  So, I expect all of the threads to spool up and create a database connection upon server boot, and stay like that. Is that not the expected behavior?\nI believe that's the expected thread behavior, but not the expected database connection behavior. The connections are not eager loaded, so a new connection is only open if needed. It's likely that some of those threads are idling because there's aren't enough requests coming in to need them, thus they aren't requesting and using a connection from the pool. \n. ",
    "MassMadankumar": "A server is already running\nroot@instance-2:/mnt/zapstore.com/shopifyapp/zap# rails s\n=> Booting Puma\n=> Rails 5.1.4 application starting in development\n=> Run rails server -h for more startup options\nA server is already running. Check /mnt/zapstore.com/shopifyapp/zap/tmp/pids/server.pid.\nExiting. this dir of the server. root@instance-2:/mnt/zapstore.com/shopifyapp/zap# rails s\n=> Booting Puma\n=> Rails 5.1.4 application starting in development\n=> Run rails server -h for more startup options\nPuma starting in single mode...\nVersion 3.11.0 (ruby 2.4.2-p198), codename: Love Song\nMin threads: 5, max threads: 5\nEnvironment: development\nListening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\nany solution?. ",
    "ezkl": "@sorentwo Do you still see the issue if you turn off the asset pipeline by setting config.assets.enabled = false in config/application,  run RAILS_ENV=development bundle exec rake assets:precompile and then run the server?\n. There are a few different ways. See the last half of the README or the code that implements the functionality it describes:\n- https://github.com/puma/puma/blob/master/bin/pumactl\n- https://github.com/puma/puma/blob/master/lib/puma/control_cli.rb\n- https://github.com/puma/puma/blob/master/lib/puma/app/status.rb\n. Out of curiosity, do you have puma listed in your Gemfile?\n. @TrevorBramble Try giving bundle exec rails s puma and bundle exec rackup -s puma a go. I just ran into a similar issue myself.\n. ",
    "lukesutton": "I've seen the same issue myself. I found Rack Cache seemed to be misbehaving, although I didn't get to diagnose the specific cause. \n@sorentwo Are you using Rack Cache? I do know Rails 3 injects it into the middleware stack by default. \n. ",
    "jtblin": "I can repro with Rack::Cache. It is quite simple to repro, just add rack-cache gem, sets config.consider_all_requests_local       = true, config.serve_static_assets = true and config.static_cache_control = \"public, max-age=2592000\" development.rb in a project where you have css, images, etc. Just refresh the pages a few time.\nIt seems to occur because puma sets env['rack.run_once'] to true and Rack::Cache determines if it's going to make a thread safe request based on this, e.g. /lib/rack/cache/context.rb:48\ndef call(env)\n      if env['rack.run_once'] \n        call! env\n      else\n        clone.call! env\n      end\nend\nEither puma should not set env['rack.run_once'] to true or Rack::Cache should test if env['rack.multithread'] is not true as well, i.e.\nif env['rack.run_once'] && !env['rack.multithread']\n   call! env\nelse\n   clone.call! env\nend\nI'll propose this fix to rack::cache project and let this thread know if they accept my pull request.\n. Thanks a ton @rtomayko and @evanphx!\n. ",
    "rtomayko": "I just merged @jtblin change in rtomayko/rack-cache#71 so that rack.multithread is checked but puma setting rack.run_once seems pretty wrong here. What's the reasoning behind setting it true in this case?\n. ",
    "seamusabshere": "this should fix #54\n. hey @evanphx it's also possible that you don't want #unknown_error to accept env... either way, here's where the problems are:\nlib/puma/server.rb:\n  182                client.close rescue nil\n  183              rescue Object => e\n  184:               @events.unknown_error self, e, \"Listen loop\"  # <- 3 args\n  185              end\n  186            end\n  ...\n  287        # Server error\n  288        rescue StandardError => e\n  289:         @events.unknown_error self, env, e, \"Read\"          # <- 4 args\n  290  \n  291        ensure\n  ...\n  295            # Already closed\n  296          rescue StandardError => e\n  297:           @events.unknown_error self, env, e, \"Client\"      # <- 4 args\n  298          end\n  299        end\n. perish the thought! I'll look more into this.\n. yo @spastorino, interesting, do you have a benchmark?\n. aha, awesome :)\n. hey @paneq, you also need to fix line 184 of server.rb:\nlib/puma/server.rb:\n  182                client.close rescue nil\n  183              rescue Object => e\n  184:               @events.unknown_error self, e, \"Listen loop\"\n  185              end\n  186            end\nYou could change it to\n@events.unknown_error self, nil, e, \"Listen loop\"\n. ",
    "rxbynerd": "OK, thank you\nSorry, it looks like GitHub didn't get my reply email\n. ",
    "philostler": "For the record I get the same in JRuby (1.7.0.preview2)\nFixed by adding use ActiveRecord::ConnectionAdapters::ConnectionManagement before all my Sinatra apps in my config.ru file\n. ",
    "davesag": "Thanks! I was getting this too but only occasionally.\n. ",
    "cyrusg": "\"use ActiveRecord::ConnectionAdapters::ConnectionManagement\" solved my issues with ClearDB on Heroku.\n. ",
    "paneq": "Technically it should allow to bind to port even when it is in TIME_WAIT state what might happen because of crash or killed process. EventMachine mentions in its code comments that it helps with unexpected restarts. Otherwise there are problems with binding again for some time.\nI also use Puma for testing communication with external application and Puma is started and shutdown in every acceptance test. The problem is that such quick on/off workout leaves the socket in TIME_WAIT state and puma server (playing the role of external webservice) cannot start on the same port in subsequent tests. REUSEADDR should fix that but as you can see from my gist it does not work well for me.\nI had the same issue when using Webrick but it works fine on Thin (which uses EM which sets this option in C code directly instead of via Ruby code). I guess this option might not work at all in Ruby but it is also possible that I am doing something wrong. I was hoping that someone smarter could try to use it and tell me if it behaves correctly or not.\n. Closing because it was not a successful refactoring.\n. Ah, I didn't notice that subtle difference and the tests were passing. Am I right that TCPServer instances are only used in cli for restarts ? Why not just close and open new socket ? To prevent closing opened connections ?\n. As an alternative solution maybe we could borrow something from haproxy:\nhttp://haproxy.1wt.eu/download/1.2/doc/architecture.txt point 4.3 :\nFor this reason, a new hot reconfiguration mechanism has been introduced in\nversion 1.1.34. Its usage is very simple and works even in chrooted\nenvironments with lowered privileges. The principle is very simple : upon\nreception of a SIGTTOU signal, the proxy will stop listening to all the ports.\nThis will release the ports so that a new instance can be started. Existing\nconnections will not be broken at all. If the new instance fails to start,\nthen sending a SIGTTIN signal back to the original processes will restore\nthe listening ports. This is possible without any special privileges because\nthe sockets will not have been closed, so the bind() is still valid. Otherwise,\nif the new process starts successfully, then sending a SIGUSR1 signal to the\nold one ensures that it will exit as soon as its last session ends.\nThe way I understand it is that we could unbind old sockets (instead of closing them) and then new sockets on same tcp port can be started. I leave the decision to you. Maybe that way could lead to less code.\n. I am sure it doesn't justify. I was just generally speaking about solutions. And I also think that Server should not return TCPServer objects to the caller. I would consider it something private. Since my refactoring was not successful I will just close the pull request.\n. ",
    "dustalov": "If it is important to investigation, I'm running Ruby 1.9.3 (with falcon patch) under Fedora 16 (x86_64).\n% ruby -v\nruby 1.9.3p125 (2012-02-16 revision 34643) [x86_64-linux]\n% uname -a\nLinux tazik 3.3.1-3.fc16.x86_64 #1 SMP Wed Apr 4 18:08:51 UTC 2012 x86_64 x86_64 x86_64 GNU/Linux\n. It's a kind of plot.\n. ",
    "trobrock": "@seamusabshere I'm running into the same issue where CONTENT_TYPE is nil, have you found out anymore about this?\n. ",
    "brandur": "~~I'm seeing a Puma failure out of the box on this. I'll probably end up hacking in a fix, but I'd also be interested to know what the people consider to be the \"correct\" solution.~~\nEdit: I locked my Rack dependency and as a result was pulling down some ancient version of Puma. Things look great on 1.6.2.\n. ",
    "dariocravero": "I'd also add Passenger to this. I had just moved our servers from Passenger to Puma and the difference is huge! I'll be posting more on this soon... In the meantime, thanks Evan and the Puma team for making this happen!! :)\n. Hey @blanchma,\nHave you tried a vhost definition like this one:\n<VirtualHost *:80>\n    NameVirtualHost 99.99.99.99\n    ServerName yourapp.com\n    ServerSignature Off\n    ProxyRequests Off\n    <Proxy *>\n        Order Allow,Deny\n        Allow from all\n    </Proxy>\n    ProxyPass / http://localhost:3000/\n    ProxyPassReverse / http://localhost:3000/\n    ProxyVia On\n</VirtualHost>\nTo start your app with tcp instead of a socket, run something like puma -b tcp://127.0.0.1:3000.\nI don't know if Apache works with unix sockets, Nginx definitely does. If you need a config for it, here's a very simple one:\n```\nupstream myserver {\n  server unix:///path/to/web/tmp/puma.sock;\n}\nserver {\n  listen 80;\n  server_name myserver.com;\nroot /exordo-dev/web/public;\naccess_log /path/to/web/log/nginx.access.log;\n  error_log /path/to/web/log/nginx.error.log;\ntry_files $uri @app;\nlocation @app {\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host $http_host;\n    proxy_redirect off;\n    proxy_pass http://myserver;\n  }\n}\n```\nBoth vhosts definitions are quite simple, i.e., they don't go any further than serving the app and if you go on production you may want your webserver to handle some stuff like some assets instead of your ruby server. Here's a better production ready example for nginx we use - it's adapted from a capistrano recipe.\nHope that helps :),\nCheers!\nDar\u00edo\n. Hey @drnic, here's the example as a wiki page. I just can't put it into puma's repository - I reckon it's because I don't have permissions to do so. I've also tried doing a pull request but it looks like the wiki pages don't count for that matter...\n. @evanphx sorry I never came back to this. I was going to put the file now but can't find a docs directory at all... Where is it? Or should I create one?\n. Hi @mattdbridges,\nAll the options are documented in Puma::Configuration::DSL.\nI agree with you that it would be easier to have them in the documentation itself, so I started writing them into the readme and adding a few extra comments to make it easier to understand and when I was just about to finish github decided to fail and loose all of my changes :(. I don't have any more time now but I'll try to do this tomorrow.\nCheers,\nDar\u00edo\n. Just set your preferred server in app.rb to puma:\nset :server, :puma\n```\nclass Pumatra < Sinatra::Base\nset :server, :puma\nget '/' do\n    return 'It works!'\n  end\nend\n```\n. @vimutter do you have an example to test it against?\n. ",
    "ixti": "Guys, if you are not interested in providing additional benchmarks on your website, please, give us at least the code you used for benchmarks, so we could made our own benchmarks on local environments just for ourselves...\n. Sorry for necroposting, but, how is Puma doing the right thing here, if according to URI spec, ; is a valid char in path? o_O. Oh, indeed, I was referencing updated URI spec, which actually clarifies reserved chars situation by splitting it into 2 groups:\n```\n      reserved    = gen-delims / sub-delims\n  gen-delims  = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n\n  sub-delims  = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n              / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n\n```\nAnd then explicitly allowing sub-delims in paths:\n```\n      path          = path-abempty    ; begins with \"/\" or is empty\n                    / path-absolute   ; begins with \"/\" but not \"//\"\n                    / path-noscheme   ; begins with a non-colon segment\n                    / path-rootless   ; begins with a segment\n                    / path-empty      ; zero characters\n  path-abempty  = *( \"/\" segment )\n  path-absolute = \"/\" [ segment-nz *( \"/\" segment ) ]\n  path-noscheme = segment-nz-nc *( \"/\" segment )\n  path-rootless = segment-nz *( \"/\" segment )\n  path-empty    = 0<pchar>\n  segment       = *pchar\n  segment-nz    = 1*pchar\n  segment-nz-nc = 1*( unreserved / pct-encoded / sub-delims / \"@\" )\n                ; non-zero-length segment without any colon \":\"\n\n  pchar         = unreserved / pct-encoded / sub-delims / \":\" / \"@\"\n\n```. ",
    "cypher": "Yeah, I think this can be closed. @evanphx \n. I can't speak for @evanphx obviously, but I'm guessing it's because JRuby doesn't support fork by default. \nBut, according to the JRuby wiki, you can enable experimental fork support on platforms that support it by setting jruby.fork.enabled=true (see https://github.com/jruby/jruby/wiki/PerformanceTuning). That way you should be able to use hot restart under JRuby (with the caveat of fork being an experimental feature).\n. I think you have it backwards. Puma is the app server that hosts your Rails app, so any Puma config would be required before Rails even loads.\nI suggest that you use something like Foreman, and have the following line in your Procfile:\nweb: bundle exec puma -p 3000 -e $RACK_ENV -C config/puma.rb\nAlong with any other services you need, like Redis or RabbitMQ or\u2026\nThat way, you can simply run foreman start and have Puma use the config from config/puma.rb, and running on port 3000. \n. If that happens, what would be the appropriate way to handle it? Raise an exception after io.syswrite returned 0 more than a certain number of times, or have a timeout around the whole thing (ie. if the string can't be written within x seconds, fail)?\n. ",
    "svetogor": "thanks for your help!\n. btw, when I trying to start pumactl, it asked for -S parameter (path to \"state file\"). so, where can I find it?\n. ",
    "explainer": "I have been working on a rails app with puma, and found some documentation which defined this command to start puma:\nbundle exec puma -e production -p 9292 --pidfile tmp/pids/puma.pid -d\nIs this correct? If not, can you point me to a shell script which can start puma as a daemon (in production)?\n. ",
    "TrevorBramble": "@ezkl I do indeed. Just gem 'puma', no version.\n. @ezkl Yup. Those work. Thanks!\nStrange, same versions of rack and puma are available outside of the bundle.\nNot sure if this requires updated docs or any other code change, so leaving the issue open for a maintainer to review.\n. ",
    "linuxbender": "FYI: same issue same error msg..but if you run only puma (without: rails s puma) in the rails project dir - it works fine - and the rails project runs without any problem - in my case.\n. ",
    "bratsche": "I have an issue similar to this when I start puma and Ctrl-C it to stop.  Then when I try to restart it refuses\nIOError: bind failed: Address already in use\n. I'm trying to listen with a unix socket, fwiw.  And now I'm not even sure how I got it working earlier, now I can't seem to get puma started at all.  It always insists that the address is already in use.  I'm trying to start with:\nRAILS_ENV=production bundle exec puma -b 'unix:///tmp/.sock' -S /some/path/puma.state --control 'unix:///tmp/.sock'\nI tried running netstat -x and I get:\nActive UNIX domain sockets (w/o servers)\nProto RefCnt Flags       Type       State         I-Node   Path\nunix  4      [ ]         DGRAM                    7425     /dev/log\nunix  3      [ ]         STREAM     CONNECTED     8164     \nunix  3      [ ]         STREAM     CONNECTED     8163     \nunix  2      [ ]         DGRAM                    7990     \nunix  3      [ ]         STREAM     CONNECTED     7632     /var/run/dbus/system_bus_socket\nunix  3      [ ]         STREAM     CONNECTED     7631     \nunix  2      [ ]         DGRAM                    7543     \nunix  3      [ ]         STREAM     CONNECTED     7404     /var/run/dbus/system_bus_socket\nunix  3      [ ]         STREAM     CONNECTED     7403     \nunix  3      [ ]         STREAM     CONNECTED     7389     \nunix  3      [ ]         STREAM     CONNECTED     7388     \nunix  3      [ ]         STREAM     CONNECTED     6587     @/com/ubuntu/upstart\nunix  3      [ ]         STREAM     CONNECTED     6583     \nunix  3      [ ]         DGRAM                    6261     \nunix  3      [ ]         DGRAM                    6260     \nunix  3      [ ]         STREAM     CONNECTED     6200     @/com/ubuntu/upstart\nunix  3      [ ]         STREAM     CONNECTED     6197\nI've restarted my box and tried changing from /tmp/.sock to /tmp/puma.sock but it's still telling me Address already in use now.  I have no idea how I got it to start up earlier today but not now.\n. D'oh, that was really stupid.  So yeah, puma starts up just fine now.  If I Ctrl-C it and then restart it, I get the bind failed error though.  If I remove /tmp/.sock then restart it then it seems to start just fine.\n. ",
    "haberbyte": "I actually still encounter quite a few cases where the socket file will still be around, thus puma can't be started without removing it by hand first.\nI checked how unicorn handles this. And it seems they check if the socket file is actually still in use, if not it doesn't matter if it exists.\nI think this is how puma should handle this as well...\nWould that be possible?\n. I just realized that this only happens with JRuby. On MRI i indeed don't have a problem at all.\nWith JRuby i just have to start puma, let it bind on a unix socket and exit out with CMD-C.\nOn MRI you will see `- Gracefully stopping, waiting for requests to finish``\nOn JRuby it just exists, killing the process and leaving the socket behind.\n(JRuby 1.7.0 is the version i use)\n. My guess is there is a connection to this jruby issue: http://jira.codehaus.org/browse/JRUBY-4637\nFixing this probably isn't that easy because the JVM handles Ctrl-C?\nMaybe @headius knows.\nAnyway, i still think all of this wouldn't be a problem if an unsused .sock file wouldn't cause \"socket already in use\" and puma would check that it is not used first.\n. I'm having this issue now after upgrading to JRuby 1.7.0.\nDoes the rack 1.4.1 fix it? How would i force rails to use rack 1.4.1? It seems it depends on 1.4.0!\n. Had the same issue. It really only works for me if the env variable NEWRELIC_DISPATCHER=puma is set...\nIt would be nice if a \"dispatcher: puma\" line in rewrelic.yml would suffice.\nNot sure why it doesn't work without the env var!?\n. I have the same issue.\nI hope we can reproduce it so @evanphx has a place to look into.\nWhat i found so far is that it seems to happen when binding to a unix socket.\nI'm using JRuby 1.7.0, so that might play a role, too.\n. OK I have put together a very small hello world rack app that demonstrates the problem.\nHave a look here: https://github.com/habermann24/pumabug\nTo really reproduce it you should try listening on a socket and use nginx.\nSometimes you will see the \"Hello, World!\" text appear, other times the status code \"HTTP/1.0 200\".\nThe more text you add to config.ru's body output, the more HTTP header you will get :)\nI explicitly said \"and use nginx\" because this doesn't happen if you forward a TCP port to that socket with socat like this:\nsocat TCP-LISTEN:1234,reuseaddr UNIX-CLIENT:/tmp/puma.sock\nupdate:\ninteresting discovery: if i tell puma to use n minimum threads, then on the n+1's request i get the header.\nFor example:\nbundle exec puma config.ru -b unix:///tmp/puma.sock -t 4:8 would mean after the 4th request i get the header displayed\n. Sorry about the number of posts, but i think i found a solution to the bug.\nnginx uses HTTP 1.0 by default when proxying requests.\nWhen i configure nginx to use HTTP 1.1 as suggested in the keepalive section in the docs, the weird behavior of puma is gone, see http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive\nSo one needs to add the following to the nginx config:\nlocation @app {\n   ...\n   proxy_http_version 1.1;\n   proxy_set_header Connection \"\";\n   ...\n}\nI have no idea why this works yet and why it doesn't without those settings.\nBut i assume puma expects HTTP 1.1 and there is some trouble when nginx talks 1.0...\nHopefully this will give people who know what they're doing a hint at how to fix this in puma to properly handle HTTP 1.0.\nMeanwhile i'm going to do some reading about the topic ;-)\n. That is awesome!!!\nMy guess is it is exactly what's stated here under \"The connection header\":\nhttp://www2.research.att.com/~bala/papers/h0vh1.html\nPuma needs to remove the \"hop-by-hop\" headers that nginx sends, or something like that :-)\nOn 16.11.2012, at 20:16, Evan Phoenix notifications@github.com wrote:\n\nI found the bug. Will have a new release out today. \n- Evan // via iPhone \nOn Nov 16, 2012, at 1:57 PM, Jan notifications@github.com wrote: \n\nSorry about the number of posts, but i think i found a solution to the bug. \nnginx uses HTTP 1.0 by default when proxying requests. \nWhen i configure nginx to use HTTP 1.1 as suggested in the keepalive section in the docs, the weird behavior of puma is gone, see http://nginx.org/en/docs/http/ngx_http_upstream_module.html#keepalive \nSo one needs to add the following to the nginx config: \nlocation @app { \n... \nproxy_http_version 1.1; \nproxy_set_header Connection \"\"; \n... \n} \nI have no idea why this works yet and why it doesn't without those settings. \nBut i assume puma expects HTTP 1.1 and there is some trouble when nginx talks 1.0... \nHopefully this will give people who know what they're doing a hint at how to fix this in puma to properly handle HTTP 1.0. \nMeanwhile i'm going to do some reading about the topic ;-) \n\u2014 \nReply to this email directly or view it on GitHub. \n\u2014\nReply to this email directly or view it on GitHub.\n. Same issue on jruby 1.7.0\n\n\n```\njan$ bundle exec puma -S puma.state --control unix:///tmp/puma.sock &\n[1] 48559\njan-lan:docmago jan$ include_class is deprecated. Use java_import.\ninclude_class is deprecated. Use java_import.\nPuma 1.6.3 starting...\n Min threads: 0, max threads: 16\n Environment: development\n Listening on tcp://0.0.0.0:9292\n Starting status server on unix:///tmp/puma.sock\nUse Ctrl-C to stop\njan$ pumactl -S puma.state restart \n* Restarting...\nRequested restart from server\njan-lan:docmago jan$ Errno::EFAULT: Bad address\n  chdir_exec at /Users/jan/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-1.6.3-java/lib/puma/jruby_restart.rb:18\n    restart! at /Users/jan/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-1.6.3-java/lib/puma/cli.rb:114\n         run at /Users/jan/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-1.6.3-java/lib/puma/cli.rb:464\n      (root) at /Users/jan/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-1.6.3-java/bin/puma:10\n        load at org/jruby/RubyKernel.java:1045\n      (root) at /Users/jan/.rbenv/versions/jruby-1.7.0/bin/puma:23\n[1]+  Exit 1                  bundle exec puma -S puma.state --control unix:///tmp/puma.sock\njan$ ruby -v\njruby 1.7.0 (1.9.3p203) 2012-10-22 ff1ebbe on Java HotSpot(TM) 64-Bit Server VM 1.6.0_37-b06-434-11M3909 [darwin-x86_64]\n```\n. ",
    "mcmoyer": "I randomly have the socket file stick around after the server crashes on MRI 1.9.3p327 on Ubuntu 12.04.\n. I know it's not optimal, but the best solution I've found so far is to listen on a port instead of the socket file.  I've not been able to track down a consistent pattern of when it happens.  It does seem that it's about 50% of the time.\n. ",
    "radicaled": "I'm having the same issue: OS X 10.8, MRI 1.9.3-p385. Running 2.0.0.b6\n. - Does Puma's cluster-mode load up the Rails environment in the supervisor process, like Unicorn, requiring you to explicitly assign shared resources in the on_worker_boot block? \n- Does it even have a supervisor process, or does it just fire-and-forget the required instances?\n  - If it does, what's the behavior for handling misbehaving workers?\n(both questions and suggestions as to what to put in the docs)\n. ",
    "pedroarnal": "Happens to me on debian 7, MRI 1.9.3-p392 running 2.0.0b7.\n. ",
    "MartinodF": "Same issue here. Ubuntu 10.04, Ruby 2.0.0-p0, puma 2.0.0b7.\nI am running the app using your own jungle/upstart files and managing restarts with puma/capistrano.rb.\nCapistrano automatically runs puma:restart, which sometimes results in the puma.sock and puma.state files not being deleted.\n. Until puma is fixed, the upstart script could clean the sockets on shutdown or check if the process is still running on startup and clean them if it's not. The latter solution is what I implemented as a stopgap measure.\n. ",
    "rrrodrigo": "Happens to me too, albeit randomly.  Ruby 2.0.0-p0, puma 2.0.0b7 on current Gentoo, Rails 4 app deployed via Capistrano. Socket file sticks around after restart (implemented as pumactl -S puma.state restart) and prevents new instance of server from starting.\nError msg: /shared/bundle/ruby/2.0.0/gems/puma-2.0.0.b7/lib/puma/binder.rb:234:in 'initialize': Address already in use - \"/tmp/puma-production.sock\" (Errno::EADDRINUSE)\n. ",
    "mrThe": "Same to me, with puma 2.0.0.b7, ruby 1.9.3-p125, rails 3.2.11. Socket do not removing after stop server(with kill -9 [pid]) and sometimes after restart (kill -s USR2 [pid])\n. ",
    "lothar59": "Same here using jungle. Removing the puma tmp files then stopping/starting puma (and in my case force-reloading nginx) ensured that everything works again after redeploying the application. \n. ",
    "balinterdi": "This happens to me, too, intermittently. I use cap deploy and have require 'puma/capistrano in my deploy.rb so puma is automatically restarted by bundle exec pumactl -S /path/to/app/shared/sockets/puma.state restart. This works 4 out of 5 times. When it does not, puma is stopped but not started and the socket file is not removed.\n. ",
    "mpalenciano": "Hi. This happens to me too. The socket never gets removed. In OSX and Ubuntu 12.10.\nI use to start:\nbin/puma -C config/puma.rb -S tmp/sockets/puma.state (configuration is fine)\nI stop with:\nbin/pumactl -S tmp/sockets/puma.state stop\nWhen I try starting again it fails with the 'Address already in use' (Errno::EADDRINUSE)\n'restart' always works for me.\nRegards and thanks !\n. Just tested on OSX and Ubuntu 12.10 (with puma 2.3.1)\nWorks great\nThanks a lot !\n. ",
    "ppoektos": "As I understand, after executing \"pumactl ... stop\" the .pid, .sock and .state files must go away, but they don't.\n[root@redmine rm2.3.0]# puma -C ./config/puma.rb \n[6292] Puma 2.0.1 starting in cluster mode...\n[6292] * Process workers: 1\n[6292] * Min threads: 0, max threads: 16\n[6292] * Environment: production\n[6292] * Listening on unix://./tmp/puma/puma.sock\n[root@redmine rm2.3.0]# \n[root@redmine puma]# ls -la\ntotal 16\ndrwxr-xr-x  2 nginx root 4096 Jun 19 09:25 .\ndrwxr-xr-x 11 nginx root 4096 Jun 18 16:07 ..\n-rw-r--r--  1 root  root    5 Jun 19 09:25 puma.pid\nsrwxrwxrwx  1 root  root    0 Jun 19 09:25 puma.sock\n-rw-r--r--  1 root  root  613 Jun 19 09:25 puma.state\n[root@redmine puma]# puma -V\npuma version 2.0.1\n[root@redmine puma]# ps ax | grep puma\n 6297 ?        Sl     0:00 /usr/local/rvm/gems/ruby-1.9.3-p327/bin/puma                                                                                     \n 6300 ?        Sl     0:03 puma: cluster worker: 6297                                                                                                       \n 6341 pts/1    S+     0:00 grep puma\n[root@redmine rm2.3.0]# pumactl -P ./tmp/puma/puma.pid status\nPuma is started\n[root@redmine rm2.3.0]# pumactl -P ./tmp/puma/puma.pid stop\nCommand stop sent success\n[root@redmine rm2.3.0]# pumactl -P ./tmp/puma/puma.pid status\nNo pid '6297' found\n[root@redmine puma]# ls -la\ntotal 16\ndrwxr-xr-x  2 nginx root 4096 Jun 19 09:25 .\ndrwxr-xr-x 11 nginx root 4096 Jun 18 16:07 ..\n-rw-r--r--  1 root  root    5 Jun 19 09:25 puma.pid\nsrwxrwxrwx  1 root  root    0 Jun 19 09:25 puma.sock\n-rw-r--r--  1 root  root  613 Jun 19 09:25 puma.state\n[root@redmine rm2.3.0]# puma -C ./config/puma.rb \n[6426] Puma 2.0.1 starting in cluster mode...\n[6426] * Process workers: 1\n[6426] * Min threads: 0, max threads: 16\n[6426] * Environment: production\n[6426] * Listening on unix://./tmp/puma/puma.sock\n/usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/binder.rb:235:ininitialize': Address already in use - ./tmp/puma/puma.sock (Errno::EADDRINUSE)\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/binder.rb:235:in new'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/binder.rb:235:inadd_unix_listener'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/binder.rb:96:in block in parse'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/binder.rb:64:ineach'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/binder.rb:64:in parse'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/cli.rb:652:inrun_cluster'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/lib/puma/cli.rb:406:in run'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/gems/puma-2.0.1/bin/puma:10:in'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/bin/puma:19:in load'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/bin/puma:19:in'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/bin/ruby_noexec_wrapper:14:in eval'\n        from /usr/local/rvm/gems/ruby-1.9.3-p327/bin/ruby_noexec_wrapper:14:in'\n[root@redmine rm2.3.0]#`\n. You may want to try this one: https://gist.github.com/niwo/4526179\nBut it doesn't work for me.\n. Same for me in Linux:\nhttps://github.com/puma/puma/issues/73#issuecomment-19667336\n. ",
    "wingrunr21": "I did the same thing as @mcmoyer: switched to using ports. Even adding correct cleanup code occasionally gave me problems. I'd rather take the perf hit than have to deal with the sockets.\n. ",
    "jure": "Same issue. How can I help?\n. :+1: on more thought.\n. ",
    "cmer": "I have this issue in 2.3.2.\n. Issue is also present in 2.4.0. JRuby 1.7.4 if it matters.\n. Would love to see this reopened and fixed.\n. ",
    "dmorrison": "I'm having the same problem with the same versions of Puma and JRuby as @cmer mentioned.\n. ",
    "adamhunter": "I am having the same problem with Puma 2.5 and JRuby in 1.7.4\n. ",
    "rhietala": "Same here as @adamhunter, Puma 2.5.1 and JRuby 1.7.4 running on Debian 7.0.\n. ",
    "plentz": "@evanphx I agree with you, but for simpler environments, a simple daemonized proccess would do the job as well. If I write a recipe for capistrano to be used as sidekiq's recipe - wich is just an require 'sidekiq/capistrano' in the deploy.rb, for example, it could be accepted or that's not something that you want?\nI really liked the project and I think that's something that could help the project adoption.\n. @evanphx is that an yes for the capistrano recipe? :)\n. btw, I think that would be cool if that kind of information(socket location, etc) could stay at project's config.rb. maybe we can check for the config file existence and - if it exists - just use it, otherwise we use our sane defaults. thoughts?\n. well, using -d option to daemonize the process worked fine, but I can't figure out how to get the output of that process to redirect it's log to our log file.\n. @AhmedElSharkasy do you still have this problem? looks like it doesn't have anything to do with puma :)\n. it can be done just by removing the last & and adding the -d, right? anyway, we should add the -d and --daemon to the -h option list :)\n. nevermind. just found that the daemon option isn't released to rubygems yet.\n. I've worked more hours then I should yesterday :)\n. ",
    "travisbot": "This pull request passes (merged 2f55c55b into d8c5a0bd).\n. This pull request passes (merged 29a71026 into 208c2f56).\n. This pull request passes (merged bf541b1a into 208c2f56).\n. This pull request passes (merged 2d2814f5 into 2cc4b517).\n. This pull request passes (merged 6767859c into 2cc4b517).\n. This pull request fails (merged 5fbd942f into f6b4363b).\n. This pull request passes (merged a1eee5aa into f6b4363b).\n. This pull request passes (merged c077e632 into 5c2bd100).\n. This pull request passes (merged 3dc2a762 into 7af141b9).\n. This pull request fails (merged 9af4c644 into 7af141b9).\n. This pull request passes (merged c0b6481b into 7af141b9).\n. This pull request passes (merged 0bfd7070 into 28489307).\n. This pull request passes (merged b7337c01 into 7f5a0311).\n. This pull request passes (merged 8102eae7 into c96cee6b).\n. This pull request passes (merged ff2200c0 into c96cee6b).\n. This pull request passes (merged 740e0291 into c96cee6b).\n. This pull request passes (merged efb7fad9 into 2459d3b7).\n. This pull request passes (merged 0728ce5b into fc0aebd3).\n. This pull request fails (merged 8d33d334 into fc0aebd3).\n. This pull request passes (merged 62ee378a into ca211116).\n. This pull request fails (merged 01ff40f0 into ca211116).\n. This pull request passes (merged 22a770ea into ca211116).\n. This pull request passes (merged 68b5877c into ca211116).\n. This pull request fails (merged d29ddd5f into ca211116).\n. This pull request passes (merged 5b789381 into ca211116).\n. This pull request passes (merged 26f2d462 into ca211116).\n. This pull request passes (merged 074ac5f3 into ca211116).\n. This pull request passes (merged fb4e23d6 into ca211116).\n. This pull request passes (merged 5c6facde into ca211116).\n. This pull request fails (merged 79994c37 into ca211116).\n. This pull request passes (merged 8efc32c8 into ca211116).\n. This pull request fails (merged 4efbc707 into ca211116).\n. This pull request fails (merged 0531b9c4 into ca211116).\n. This pull request fails (merged 8225dd41 into ca211116).\n. This pull request passes (merged 267a99d9 into ca211116).\n. This pull request passes (merged 6a458da4 into cd83b2f3).\n. This pull request fails (merged 880df272 into 48593aeb).\n. This pull request passes (merged 43327bea into d356c099).\n. This pull request fails (merged 380818e3 into e191003f).\n. This pull request fails (merged 129fb41e into e191003f).\n. This pull request fails (merged 6a5b30ea into e191003f).\n. This pull request fails (merged ffa0c434 into e191003f).\n. This pull request fails (merged 58354c9f into b2550acf).\n. This pull request passes (merged 11bcdba3 into b2550acf).\n. This pull request passes (merged d0127d40 into b2550acf).\n. This pull request passes (merged 93d07eb9 into b2550acf).\n. This pull request fails (merged 55b7c688 into b2550acf).\n. This pull request passes (merged ed559f02 into 074adfbf).\n. ",
    "schneems": "Let's close this. Feature requests should be submitted in the form of pull requests. This issue has been open for over 2 years with only one comment. Regardless of whether it works or not let's close this issue. If it's a documented feature and it's broken, then we can open up a bug report issue. \nIf it works and it's not documented...let's see a documentation PR.\ntl;dr close this issue\n. You can use rack timeout to do this https://github.com/kch/rack-timeout if you really need it\n. No activity in 7 months. Is this still an ongoing issue? Can you build a run-able script that reproduces the problem?\n. Hey @carllerche it's been quiet in here for the last 8 months. What do other servers thin/unicorn/mongrel do in this scenario? Is this still the current behavior of Puma?\n. Is this still an issue? The original issue reads somewhat like a feature request. Can we close this issue?\n. 4 months no comment. Let's see a reproducible case or close the issue.\n. Check out https://github.com/schneems/puma_worker_killer it's designed to detect memory leaks and restart workers no matter where they're coming from. It is currently running in production on codetriage.com and seems to work well. Make sure you're using 0.0.2 or above.\nIf you can reproduce the unbounded memory growth with an insanely simple rack app let us know:\n``` ruby\nrequire 'rack'\nrequire 'rack/server'\nclass HelloWorld\n  def response\n    [200, {}, ['Hello World']]\n  end\nend\nclass HelloWorldApp\n  def self.call(env)\n    HelloWorld.new.response\n  end\nend\nrun HelloWorldApp\n```\nPending a reproducible example, let's close this issue.\n. @Kagetsuki can you share your simple app you benchmarked with?\nIf we're looking for only a leak in Puma. Maybe it should be a Rack hello world app\n```\nrequire 'rack'\nrequire 'rack/server'\nProc.new {|env|  [200, {}, ['Hello World']] }\n```\n. There's not rack support yet, care to comment here:\nhttps://github.com/schneems/derailed_benchmarks/pull/1\nOn Tue, Nov 11, 2014 at 12:13 PM, \u5f71\u6708 \u96f6 notifications@github.com wrote:\n\nOK, I'm still getting a memory leak with github master.\n@schneems https://github.com/schneems I can't seem to get\nderailed_benchmarks to run with a simple rack app. I get\nLoadError: cannot load such file -- application\nfrom line 22 of lib/derailed_benchmarks/tasks.rb .\nMore than that, can you confirm to me that you do not see this leak with a\nplain Rails setup and 2.1.4? I'm testing on my own here and getting these\nresults but that doesn't mean there isn't something else in my environment\nthat is causing these issues. I think it would be more helpful if someone\nelse could run these tests once and report their results than me just\nrunning more tests. The fact is I am seeing an observable leak in Rails\nwith puma and 2.1.4 - seeing if that leak is present with Rack could help\nnarrow the field to observe but it doesn't change the fact that I am\ngetting these results. If someone else does not get these results then we\ncan close this issue for now and I can try and figure out what it is that's\nwrong (the fact that I'm on cedar-14 and using an Ubuntu 14 machine coupled\nwith the the new memory allocation models in both Ruby 2.1 and Ubuntu 14\nfor example...).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/342#issuecomment-62590698.\n. > So I would like to see some progress in this issue.\n\nI hear what you're saying but you need to consider that puma is open source, there is nothing preventing you from finding and fixing the issue. Evan doesn't get paid to work on this and has actively been making an effort to try to reproduce the problem. Your choice of webserver on your personal project has not impact on the validity of this issue and doesn't help get it solved any faster. Your patience, and help are appreciated. \n. @evanphx I vote we lock this thread as it seems to be unproductive. I would encourage people who can reliably get puma to leak memory with a trivial rack app to open up a new issue and give reproduction instructions. At this point, there are so many conflicting theories and \"me too\" anect-data on this thread, it's difficult to pull meaningful information from this issue. I appreciate that people on this thread have genuine memory issues, but it's not very productive to the Puma maintainers and it hasn't brought us any closer to reproducing, isolating, or fixing any supposed memory leaks. \n. @fesplugas I'm pretty sure that example is demonstrating normal MRI garbage collector behavior, and it's more an indication of the memory costs of using Puma Worker Killer (PWK) than puma. I think there's a fundamental misunderstanding of what exactly constitutes a memory leak. A true memory leak will go up forever until the entire system's memory is consumed. \nHere's a whole talk I did on Ruby's memory http://www.schneems.com/2015/05/11/how-ruby-uses-memory.html\nBasically each tick causes PWK to create objects that it uses to determine the  memory usage of the cluster (strings, arrays, whatever). Ruby won't try to get rid of these objects in memory until it has run out of available slots, so what you're seeing is a normal amount of memory growth. I would expect if you left this example running for days the memory will continue to increase until it reaches about 45 - 60 mb of memory then a GC will be triggered and all those junk objects will be collected and memory will be brought back down. I'm fairly certain if you leave this run for days/weeks/months, it will not eat up all the memory on the system. Even if you do, it would demonstrate a leak in puma worker killer and not puma.\nWhen someone opens a ticket at Heroku claiming they've got memory leaks, 9 times out of 10 they don't. Instead the system is using more memory than they would like. I've got some more info on determining if a system leaks memory here: https://github.com/schneems/derailed_benchmarks#is-my-app-leaking-memory. \n. 1) I'm rebutting a specific \"repro\" which uses PWK.\n2) I'm not saying there isn't a problem. I'm saying so far this thread hasn't lead to any info that will let us repro the problem in a simple and reliable way. Without a repro there isn't much we can do. Chiming in with \"me too\" and no runable examples doesn't help.\n. I actually ran that script that I previously commented on all afternoon, it did eventually go down, it's currently at around 27mb https://gist.githubusercontent.com/schneems/c43a7dba190f5fbac647/raw/gistfile1.sh\n. Try this out https://github.com/schneems/puma_worker_killer/pull/16 in your Gemfile add:\ngem 'puma_worker_killer', github: 'schneems/puma_worker_killer', branch: 'schneems/rolling-restart'\nThen\n$ echo \"PumaWorkerKiller.enable_rolling_restart\" >> config/initializers/puma_worker_killer.rb\nYou should see output in your log when you run this locally. \n. 70mb over 16 hours doesn't seem like a large growth in memory, what is it right after it boots and takes a few requests? Is it in the 320mb ballpark? Memory growth is expected to grow and level out over time. https://devcenter.heroku.com/articles/ruby-memory-use#memory-leaks\n. @ponny Running with more threads than what you have in your DB pool is bad if you're using Rails. Rails acquires a thread per request, it means that you have requests sitting around idle waiting for another request to finish so it can gain access to the DB. Running 2 puma threads with only 1 DB connection, doesn't buy you anything over running 1 puma thread over 1 DB connection. We state this in our docs https://devcenter.heroku.com/articles/concurrency-and-database-connections#threaded-servers\nRegarding people reporting memory bloat/leaks. Please don't comment unless you have an app you can share that reproduces the problem. This thread is noisy enough as is. Adding anectdata isn't helping anyone get any closer to the issue.\n. Want to throw out that you should be using be using a more recent version than Ruby 2.2.0. It is almost a year old at this point and has bugs in it. I would recommend 2.2.3, the most recent patchlevel of 2.2.\n. Can anyone else reproduce with this rack app ?\n\nRichard Schneeman\nhttp://www.schneems.com\nOn Sat, Dec 26, 2015 at 4:06 AM -0800, \"joanblake\" notifications@github.com wrote:\n@mkilling  Did tested with minimal rack application and it unfortunately still leaks. Even this hopeful bugfix has no effect.\n\u2014\nReply to this email directly or view it on GitHub.\n. \"The\" issue and \"this\" issue are both likely not the same issue and probably not caused by puma. Why would I say that? To date no one has provided a minimal example that reproduces a problem and shows puma as the culprit.\nThe first step in fixing a problem is reproducing it. This means multiple people on multiple machines seeing the same behavior. To date we don't even have that.\nRichard Schneeman\nhttp://www.schneems.com\nOn Sun, Dec 27, 2015 at 3:40 AM -0800, \"Marvin Killing\" notifications@github.com wrote:\nI can confirm that the issue is still present with Ruby 2.3. This is our memory usage in production (2 dynos, 1 worker each, 5 threads each):\n\u2014\nReply to this email directly or view it on GitHub.\n. Yes reproducing an issue is hard. Fixing memory issues are hard too. Even harder is having to do both at the same time. There are people happy to work on fixing issues, but we need help finding them.\nI've spent over a year at this point writing memory debugging tools and articles. I'm happy to help with education or with tooling. But none of it helps unless people use it (or other tools) to reproduce and find problems.\nTo date this thread hasn't been helpful for finding problems or helping to fix them. My worry is that people see \"oh there is an open issue\" and assume someone will fix it eventually. There is very little quid-pro-quo that is needed for isolating and fixing anything.\nI recommend that everyone here try to use any of the tools and techniques I've posted. Or please post new tools and articles. Please try debugging your app and post results back here. Who knows you could get lucky and hit the debugging jackpot and find a huge problem that is fixed and makes everyone's app run faster. We will throw you a parade. Even if you don't find a problem with puma, you'll probably speed your app up at bare minimum.\nI've already asked for reproductions but I think that is too hard. Instead what I think we should do is turn this thread into debugging/repro tips & how to.\nWhat have people done to debug your memory problems and what questions do you have?\n\nRichard Schneeman\nhttp://www.schneems.com\nOn Mon, Dec 28, 2015 at 7:16 AM -0800, \"Joshua T. Mckinney\" notifications@github.com wrote:\nUse an analytics service that can monitor GC like NewRelic. \nAnything can be tested if enough effort is put forth by the victims in this issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. We did some benchmarking on that setting cc @hone. Changing it can have drastic consequences for your app if you don't need it lowered. For example on one test app:\nPerc 95\nCedar-14 with `MALLOC_ARENA_MAX=1`: 829.99ms\nCedar-14 with `MALLOC_ARENA_MAX=2`: 607.54ms\nWhen you lower the value you decrease performance as well as decreasing memory consumption. The default is the best value for if you want a high performance app. In one of the test apps we used it never went over 512mb regardless of the setting so it didn't matter if the memory was increased. \nWe decided to keep the default value since customers who don't have memory problems can benefit significantly from a higher value. Customers who do see problems can manually set that value. Here's the article https://devcenter.heroku.com/articles/tuning-glibc-memory-behavior. I also added a link to that article from https://devcenter.heroku.com/articles/ruby-memory-use so it should be more discoverable. So while in this case it would make sense to set that value it is application dependent.\n. 5 months no comments. This is a feature request. Please provide a working PR we can review.\nLet's close this issue.\n. 5 months is this an inherent bug in puma? Can we close this issue?\n. Can anyone provide a simple script or program that can be used to reproduce the behavior?\n. This is a 2 months old feature request. If you were going to extract this into a new gem, i'm thinking it would have happened by now. I don't se the benefit of keeping this ticket open.\n. In rails/rails we close all feature requests, it's impossible to greenlight a feature without actually seeing code. If you're interested fork the project, make a PR. If it's liked it will get merged. If there's some issues the worst thing that happens is you now have a custom fork of code you can use and you've likely learned a bunch in the process. I don't use capistrano so I have no feelings on the contents of this issue, other than we should close it and re-visit the issue if a PR comes in. \n. It's been 24 days, no update. Are you able to continue working on this PR?\n. Thanks for making changes i'm :+1: on this functionality\n. :heart:  :heart_eyes: \n. We've gotten a few bumps here, any chance someone can throw together a script that demonstrates the problem?\n. I fail to see how puma is involved here, can we close this issue?\n. Seems okay unless this was done intentionally\n. Looks like @beckmx got SSL working. Can we close this issue? \n. I'm not quite sure what you're looking for here. You should be able to configure your apache server to proxy to puma. Here is an example of doing this with Thin http://www.rackspace.com/knowledge_center/article/ubuntu-apache-rails-and-thin#Stating_thin\n. @thehappycoder how about doing this on a fork and maybe showing some benchmarks. I agree with 2.1.0 it's less important though there could still be some speed benefits.\n. > Be warned - this is a bad idea if you're using puma in threaded mode.\nWe probably wouldn't take something that doesn't work for everyone. You would need to introduce some kind of lock across all threads to let them know when you want to do a GC, wait till they're done responding, run GC. Then remove the lock to let them start receiving requests again.\n. > Much as I like threads I really don't see much shift towards thread safety in a lot of Ruby gems\nBetween puma and sidekiq and passenger almost any mainstream gem out there is threadsafe.\nNot sure if you saw but GitHub actually turned off their OOBGC and got something a 10% decrease in overall CPU utilization. While GC pauses might add some time to a request it is likely minuscule between incremental GC, lazy sweep, and generational GC an individual GC run shouldn't be taking a significant portion of a request.\nIf you're really gung-ho about adding OOBGC support let me know what kind of hooks Puma could introduce to allow you to build it as a third party gem.\n. I wasn\u2019t saying that puma and sidekiq are threadsafe but rather apps that use puma and sidekiq in production are threadsafe. The dependencies that those apps use are either already threadsafe or have been found to have threading bugs in them and then been fixed.\nIf you don\u2019t want to use threads with puma I would recommend using unicorn which will give you better performance as long as it is running behind nginx then you\u2019re protected from slow clients. As a bonus then you can use the already written OOBGC.\n. Weird, can you produce a simple script that demonstrates the problem?\n. :+1:\n. > Are there other HTTP 2.0 implementations to copy from? \nGood question.\n. Did quite a bit more digging, and it looks like this behavior goes away if I do not \"wait\" for the process to exit.\nTo repro I tried using only existing puma internals (signals TTOU and TTIN called repeatedly should do roughly the same thing as puma worker killer) https://gist.github.com/schneems/9276057 I was unable to reproduce behavior. I then moved back to puma worker killer. I added an explicit wakeup! call after sending term to a worker, this did nothing. The only other difference was not calling Process.wait on the child worker PID, removing this functionality \"fixed\" the behavior in puma_worker_killer:\nhttps://github.com/schneems/puma_worker_killer/blob/master/lib/puma_worker_killer/puma_memory.rb#L17\nHonestly not entirely sure of the mechanics here, but we can close this issue i think. Somehow waiting for the process to finish causes puma cluster master to not recognize that it is gone.\nIf you're reading this and you're a process guru maybe give me some hints or some debugging steps. I hate when I find a fix but don't understand the mechanics\n. As explained to me via @dpiddy \"Process.wait and get back a result the Process.waitpid on line 132 (https://github.com/puma/puma/blob/master/lib/puma/cluster.rb#L132) won't get it. so then it won't be deleted from @workers, then won't be respawned by spawn_workers\" which seems to make sense. Thanks!\n. Build is failing, do tests pass for you locally? https://travis-ci.org/puma/puma/jobs/19053241\n. phantom failure, maybe CI needs to be restarted.\n. Ping. It's been 2 months can you remove the extraneous white space?\n. I didn't know about TTIN and TTOUT thanks! I'm interested in a way to control threads as well.\nIn the vein of controlling processes could one of you take a look at: https://github.com/puma/puma/pull/467\n. @ckuttruff i started a doc here, take a look:  #481\n. ```\n$ bundle install\n2.1.0  ~/documents/projects/puma (schneems/fix-child-term)\n$ bundle exec rake\ninstall -c tmp/x86_64-darwin13.0/puma_http11/2.1.0/puma_http11.bundle lib/puma/puma_http11.bundle\ncp tmp/x86_64-darwin13.0/puma_http11/2.1.0/puma_http11.bundle tmp/x86_64-darwin13.0/stage/lib/puma/puma_http11.bundle\n/Users/schneems/.rbenv/versions/2.1.0/bin/ruby -w -Ilib:bin:test:. -e 'require \"rubygems\"; require \"minitest/autorun\"; require \"test/test_app_status.rb\"; require \"test/test_cli.rb\"; require \"test/test_config.rb\"; require \"test/test_http10.rb\"; require \"test/test_http11.rb\"; require \"test/test_integration.rb\"; require \"test/test_iobuffer.rb\"; require \"test/test_minissl.rb\"; require \"test/test_null_io.rb\"; require \"test/test_persistent.rb\"; require \"test/test_puma_server.rb\"; require \"test/test_rack_handler.rb\"; require \"test/test_rack_server.rb\"; require \"test/test_tcp_rack.rb\"; require \"test/test_thread_pool.rb\"; require \"test/test_unix_socket.rb\"; require \"test/test_ws.rb\"' --\n/Users/schneems/Documents/projects/puma/lib/puma/daemon_ext.rb:6: warning: method redefined; discarding old daemon\nRun options: --seed 57991\nRunning tests:\n.........................................................................................\nFinished tests in 41.047504s, 2.1682 tests/s, 5.0429 assertions/s.\n89 tests, 207 assertions, 0 failures, 0 errors, 0 skips\n```\n. Seems good to have more debugging information\n. Here's my config:\n``` sh\n$ cat config/puma.rb\nthreads Integer(ENV['MIN_THREADS']  || 1), Integer(ENV['MAX_THREADS'] || 16)\nworkers Integer(ENV['PUMA_WORKERS'] || 3)\nrackup DefaultRackup\nport ENV['PORT'] || 3000\nenvironment ENV['RACK_ENV'] || 'development'\npreload_app!\nThread.abort_on_exception = true\non_worker_boot do\n  # worker specific setup\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\n# If you are using Redis but not Resque, change this\n  if defined?(Resque)\n    ENV[\"OPENREDIS_URL\"] ||= \"redis://127.0.0.1:6379\"\n    uri = URI.parse(ENV[\"OPENREDIS_URL\"])\n    Resque.redis = Redis.new(:host => uri.host, :port => uri.port, :password => uri.password)\n    Rails.logger.info('Connected to Redis')\n  end\nend\n```\nI'm not setting PUMA_WORKERS\n```\n2.1.1  ~/documents/projects/codetriage (master)\n$ echo $PUMA_WORKERS\n```\nWhen I start puma:\n2.1.1  ~/documents/projects/codetriage (master)\n$ foreman start web\n13:16:28 web.1    | started with pid 89339\n13:16:29 web.1    | [89339] Puma starting in cluster mode...\n13:16:29 web.1    | [89339] * Version 2.7.1, codename: Earl of Sandwich Partition\n13:16:29 web.1    | [89339] * Min threads: 1, max threads: 16\n13:16:29 web.1    | [89339] * Environment: development\n13:16:29 web.1    | [89339] * Process workers: 3\n13:16:29 web.1    | [89339] * Preloading application\n13:16:31 web.1    | /Users/schneems/Documents/projects/codetriage/config/initializers/git_hub_bub.rb:2: warning: already initialized constant GitHubBub::Request::USER_AGENT\n13:16:31 web.1    | /Users/schneems/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/git_hub_bub-0.0.2/lib/git_hub_bub/request.rb:7: warning: previous definition of USER_AGENT was here\n13:16:31 web.1    | /Users/schneems/Documents/projects/codetriage/config/initializers/git_hub_bub.rb:3: warning: already initialized constant GitHubBub::Request::RETRIES\n13:16:31 web.1    | /Users/schneems/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/git_hub_bub-0.0.2/lib/git_hub_bub/request.rb:12: warning: previous definition of RETRIES was here\n13:16:31 web.1    | /Users/schneems/Documents/projects/codetriage/config/initializers/git_hub_bub.rb:4: warning: already initialized constant GitHubBub::Request::GITHUB_VERSION\n13:16:31 web.1    | /Users/schneems/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/git_hub_bub-0.0.2/lib/git_hub_bub/request.rb:8: warning: previous definition of GITHUB_VERSION was here\n13:16:31 web.1    | [89339] * Listening on tcp://0.0.0.0:3000\n13:16:31 web.1    | [89339] Use Ctrl-C to stop\n13:16:31 web.1    | [89339] - Worker 89340 booted, phase: 0\n13:16:31 web.1    | [89339] - Worker 89341 booted, phase: 0\n13:16:31 web.1    | [89339] - Worker 89342 booted, phase: 0\nI see an extra process:\nhttps://www.dropbox.com/s/2lpr448ti4dxoon/Screenshot%202014-02-25%2013.17.23.png\nWhen I kill puma all the Ruby processes go away.\n. I bet it's foreman. That makes sense. Thanks!\u2014\nRichard Schneeman\nOn Tue, Feb 25, 2014 at 3:13 PM, Abdelkader Boudih\nnotifications@github.com wrote:\n\n@schneems : I'm running 2.7.1 as well.\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/479#issuecomment-36058684\n. :+1:\n. Thanks @prathamesh-sonpatki :heart:\n. Related? https://twitter.com/evanphx/status/439535654863896576\n. I removed the warning.\n. \"Request queuing\" in new relic is an inherently flawed measurement. They're comparing the clock of two different computers and only stop measuring once the new relic middleware is hit. It's not only measuring queuing time it also measures any rack middleware before the new relic middleware and the server request processing time. This is in addition to any time spent actually queuing the request.\n. Can you give me a simple app that I can reproduce the problem with?\n. Or you could run off of 1 thread and multiple puma workers which is essentially doing the same thing as Unicorn.\n. You should be releasing your lock in an ensure block. Rack timeout might be causing the exception but it is your applications responsibility to clean up when an exception is raised\n. Got it. Active Record should be handling exceptions properly. If not, well we'll need to reproduce the bug and file an issue. Do you have a suggested edit for the devcenter? Rack Timeout has many many problems and caveats, however it's still the best (only?) way to keep your app from being locked up when you get too many requests (as far as I know).\n\n@evanphx awhile ago you were toying with some worker based timeouts. What ever happened to all that?\n. If you have logs from an endpoint so you know where in your app the problem is coming from, you could try a tool like siege. However you'll also need to detect the failure condition of when a response is not returned by your server. I'm not sure if siege gives us good insights into this, maybe a custom curl script might be better, idk. You can also try to force the problem by decreasing the rack timeout. \nOnce you can reproduce, from there you'll need to pull out the problem query into a new app we can share with contributors and some reproduction instructions.\nIt will be a lot of work and won't be easy. It might end up that we can't reproduce locally, if that's the case then maybe there's something else happening. This is one of those cases where you either decide to timebox it and shoot the moon or look for workarounds.\n. I had super crazy high memory on one app and all the errors you mentioned. It turned out to be an includes that was loading thousands of models that weren't being used. https://github.com/codetriage/codetriage/commit/facfbd775513609024465d70ebd77a14042059e7\n. With H12 you are seeing a request that is taking longer than 30 seconds. You need info on what your app is doing when that happens, the best way we have to do that for now is Rack::Timeout. When the timeout fires you'll get a backtrace, use that backtrace to see where in your code you are stuck. Is it the same place every time?  If you don't kill the request it will stay stuck and other requests will back up behind it and cause more H12s. \nIf you think you're hitting noisy neighbors, you can try performance dynos which are on a dedicated VPS instead of a shared runtime.\n. Any update? Did you try performance dynos? Did you try out Rack::Timeout to get backtraces?\nI've been thinking about writing another gem that doesn't actually kill the thread (which can cause it's own new problems) but instead only outputs debug information when a request goes over a certain time.\nRelatedly, i'm wondering if some kind of a long request worker restart for puma would be effective. Basically when a request goes over some limit the master process sends a \"quit processing new requests\" to all threads. Once each thread stops minus the one that is stuck then the whole worker is restarted. There's two problems i'm trying to address. One is hung/stuck requests. When these happen they pile up other hung requests after them where the client disconnects before a response is written, however puma doesn't know and keeps processing old requests. Currently the only solution to this is to use Rack::Timeout. Unfortunately this throws exceptions in arbitrary threads. It's not safe. See http://www.mikeperham.com/2015/05/08/timeout-rubys-most-dangerous-api/ for more context about why raising on another thread is bad. In some cases adding Rack::Timeout causes more problems than it solves. \n. You could try running your app directly AWS if you can't afford to run a Performance-M dyno for a few days. They're pro-rated so you only pay for what you use. Generally the pattern is to run more puma workers on fewer instances. So if you're running 5 regular dynos, with 1 worker each, you could maybe get the same throughput by running 1 Performance M dyno (2.5 GB of Ram) with 5 workers. I don't have a budget for debugging performance problems, would be nice if I did. If the problem happens this intermittently you could probably get away with something like Puma Worker Killer's rolling restarts.\n. We've seen an issue on a few apps where the \"mysterious\" behavior always happens after a restart. The best we can tell is that it happens because lots of requests are being held by our router while the new app boots. After it boots there are too many requests to catch up and even with rack timeout the backlog never entirely catches up. \nI'm not 100% sure exactly what the failure mode is, but I can tell you that everyone we've asked to use rack-timeout and preboot https://devcenter.heroku.com/articles/preboot have seen the problem go away.\n. I was curious if we could write tooling around this. We would need to detect when a proc scope is created and what the local and instance variables are when it is created. It looks like we can get that from TracePoint:\n``` ruby\nTracePoint.new(:b_call) do |tp|\n  puts tp.binding.local_variables.inspect\nputs self.instance_variables.inspect\nfile_line = \"#{tp.path}:#{tp.lineno}\"\n  puts file_line.inspect\nend.enable\na = nil\n@boo = \"foo\"\n->() {}.call\n```\nThen we would need to know what block objects are retained. If we know the block objects being retained, we can work backwards find out what variables the blocks have reference to, and boom...we've built an automated context leak detector. \nUnfortunately i've not figured out this second part. i.e. how do you ask Ruby if an object is retained?\nYou can use ObjectSpace to grab objects:\n``` ruby\nrequire 'objspace'\nObjectSpace.trace_object_allocations do\na = Proc.new {}\nb = -> {}\ndef foo(&block)\n    return block\n  end\nc = foo do\n  end\nend\nObjectSpace.each_object(Proc) do |obj|\n  puts obj\nend\n```\nHowever the ObjectSpace.trace_object_allocations block will trace all allocated objects, even if they're not referenced anymore. I'm going to be at Keep Ruby Weird all day tomorrow. I wanted to write down this as a note in case anyone else was interested in fiddling with Ruby inspection and seeing if they had any ideas.\n. :boom: thanks!\n. :rocket:  thanks!\n. :rocket: :boom: i poked around a bit and i'm going to either need to expose some functionality in Puma::CLI (currently private methods) or move some things around to make them accessible by multiple things. I'll play around some more and see what I come up with. Thanks for the feedback. cc/ @pixeltrix\n. What did you have in mind with Puma::Embedded class? A good amount of the config does stuff on boot, instead of dynamically. I think there's a few places that we could change on the fly such as worker count as there is a reactor that constantly looks to see if we have more or less workers than needed. Other changes would either require code changes or a restart. Would you want to restrict to the values that make sense to set today or default to restarting the server on any config change or what? I could see using something like this in Puma worker killer or similar. \nI want to pull all the server starting business out of CLI into it's own class. Runner was already taken, so i'm thinking Launcher. That way CLI is still responsible for parsing input and orchestrating a launched process, but now we can re-use the server starting portion inside of Rack::Puma.\n. cc/ @pixeltrix\n. Okay, I got all the tests to pass here. This was WAY more work than I was thinking originally. I was successful in extracting out a general purpose puma launching class Launcher and re-using that in the CLI and in the rack handler. As far as I know this is 100% API backwards compatible minus one thing:\nThe one backwards incompatible thing: The original Puma rack handler would yield to an instance of Puma::Server when called with a block. Since we're not creating a server manually anymore, i'm yielding to an instance of Launcher instead. I'm not sure how much people use this behavior of the rack handler, or really even why it exists (though It was handy for writing tests, I added an extra one for doing an integration style test with the rack handler). The Launcher does have access to a \"runner\" either an instance of Single or Cluster, each of them have a server object, but neither expose it. If you want I could do the work to expose it and then keep the rack handler block API the same, but it would increase the surface area of those to classes. So either we could not care about the API that maybe not many people use or I could increase some object surface area, it's up to you.\nLet me know if you've got questions. I'm available to chat about this if you want, I think we're both in the Rails contributor basecamp chatroom if you want, or ping me in rubycentral chat. I could also jump on a hangout if you need more fidelity. I'll be around tomorrow and next week after Wednesday. \n. Any questions or concerns? We could add a deprecation to the CLI methods if we want to decrease surface area in the future, but I don't think it's needed. I don't see any downside to keeping those delegate methods in there. \nThe other thing I wanted to do, but it is very ambitious is to not pass the Launcher object to Single and Cluster and instead pass an interchange object. Ideally it would have a smaller API and if we wanted to expose some kind of intermediate control layer directly to the app I would rather we did it without going directly through the Launcher object. I spent a little time investigating that change, and it is pretty large. The classes that inherit from Runner directly use the @cli object pretty liberally, and they also directly use the @cli.options hash. There wasn't an immediate benefit from going down that road so I stopped.\n. \ud83c\udf89\ud83c\udf89\ud83c\udf89 let me know when 3.0 goes out and I can update the Gemfile that rails generates to use a compatible version of Puma. Thanks for your help!\n. Hey, thanks for the report can you open up a separate issue so we can track this? What do you think the problem is with shotgun? That constant should be defined when you require the puma rack handler.\n. Looks like this is still failing on jruby :(\nI'm guessing the addr[1] interface isn't replicated. I'll try to repro locally.\n. I made some mistakes in the tests, fixed in https://github.com/puma/puma/pull/877\n. It should be read in via the rack handler, I think we should re-open this issue\n. This works for me\n$ env PORT=4000 rails server\n=> Booting Puma\n=> Rails 5.0.0.beta3 application starting in development on http://localhost:4000\n=> Run `rails server -h` for more startup options\n=> Ctrl-C to shutdown server\n/Users/richardschneeman/Documents/projects/codetriage/config/initializers/git_hub_bub.rb:2: warning: already initialized constant GitHubBub::Request::USER_AGENT\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/git_hub_bub-0.0.6/lib/git_hub_bub/request.rb:7: warning: previous definition of USER_AGENT was here\n/Users/richardschneeman/Documents/projects/codetriage/config/initializers/git_hub_bub.rb:3: warning: already initialized constant GitHubBub::Request::RETRIES\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/git_hub_bub-0.0.6/lib/git_hub_bub/request.rb:12: warning: previous definition of RETRIES was here\n/Users/richardschneeman/Documents/projects/codetriage/config/initializers/git_hub_bub.rb:4: warning: already initialized constant GitHubBub::Request::GITHUB_VERSION\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/git_hub_bub-0.0.6/lib/git_hub_bub/request.rb:8: warning: previous definition of GITHUB_VERSION was here\n[59980] Puma starting in cluster mode...\n[59980] * Version 3.0.0.rc1 (ruby 2.3.0-p0), codename: Schneems Sleak Shoes\n[59980] * Min threads: 5, max threads: 5\n[59980] * Environment: development\n[59980] * Process workers: 2\n[59980] * Preloading application\n[59980] * Listening on tcp://localhost:4000\n[59980] Use Ctrl-C to stop\n[59980] - Worker 0 (pid: 60018) booted, phase: 0\n[59980] - Worker 1 (pid: 60019) booted, phase: 0\nAre you using Puma 3.0 or above?\n. Okay, so the reason that worked is that rails server sets port for you via ENV[\"PORT\"] https://github.com/rails/rails/blob/9d87ce34f865fa9d7a24ef9f1f1b0d4dedfd3fbf/railties/lib/rails/commands/server.rb#L92-L92\nWhat we're seeing here is that puma is picking up the Port option that is passed in, and prefering it to the port that is set in the config/puma.rb. I think we want the config/puma.rb to be authoritative i.e. if there is a conflict (at least with the rack handler) then the config file should win.\nHere is where it is set to 3000 based on the rails server input hash https://github.com/puma/puma/blob/d2da2caf49932827f07994c8b65b1f99001c5fb9/lib/rack/handler/puma.rb#L39\nI changed my config file to this\nport 4000\nThen when I boot the app still run s with \n[63654] * Listening on tcp://localhost:3000\nThe DSL value is being picked up, curiously it is set to 4000 after it is set to 3000, but for some reason it does not get persisted. Here is the caller of where port is being called from when it is set to 4000 (from the file). \nconfig/puma.rb:7:in `_load_from'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/dsl.rb:26:in `instance_eval'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/dsl.rb:26:in `_load_from'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/dsl.rb:9:in `load'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/configuration.rb:204:in `block in load'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/configuration.rb:201:in `each'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/configuration.rb:201:in `load'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/puma/launcher.rb:62:in `initialize'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/rack/handler/puma.rb:47:in `new'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/puma-3.4.0/lib/rack/handler/puma.rb:47:in `run'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/rack-2.0.0.alpha/lib/rack/server.rb:296:in `start'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/commands/server.rb:78:in `start'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/commands/commands_tasks.rb:90:in `block in server'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/commands/commands_tasks.rb:85:in `tap'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/commands/commands_tasks.rb:85:in `server'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/commands/commands_tasks.rb:49:in `run_command!'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/command.rb:20:in `run'\n/Users/richardschneeman/.gem/ruby/2.3.0/gems/railties-5.0.0.beta3/lib/rails/commands.rb:18:in `<top (required)>'\nI think this is an issue in Puma, we should re-open the issue. Any idea why port being called after might not set the binds correctly?\n. When I output the value of @options here https://github.com/puma/puma/blob/d2da2caf49932827f07994c8b65b1f99001c5fb9/lib/puma/launcher.rb#L64\nI see that binds is set to 4000 in that object\n``` ruby\n2, :min_threads=>6, :max_threads=>6, :rackup=>\"config.ru\", :binds=>[\"tcp://0.0.0.0:4000\"], :environment=>\"development\", :preload_app=>true, :before_worker_boot=>[#Proc:0x007f8dfbb06018@config/puma.rb:11]}, @set=[{:environment=>\"development\", :pid=>\"/Users/richardschneeman/Documents/projects/codetriage/tmp/pids/server.pid\", :Port=>3000, :Host=>\"localhost\", :AccessLog=>[], :config=>\"/Users/richardschneeman/Documents/projects/codetriage/config.ru\", :DoNotReverseLookup=>true, :daemonize=>false, :caching=>false, :log_stdout=>true, :server=>nil}, {:log_requests=>false, :environment=>\"development\", :binds=>[\"tcp://localhost:3000\"], :app=>#:user, :scope_defaults=>{}, :default_strategies=>{:user=>[:rememberable, :database_authenticatable]}, :intercept_401=>false, :failure_app=>#}, @app=#>>>, @cache_control=\"max-age=0, pri\n...\n```\nHowever\n``` ruby\nputs @options[:binds]\n=> tcp://localhost:3000\n```\nSo it looks like something funny is going on with LeveledOptions\n. I'm wondering if maybe there's an orthogonal solution. Could we bind to both ports? It looks like puma can do that. Maybe we can warn when you specify different ports and we only bind to one. Something like\n\nRack passed in a different port than specified in config/puma.rb to use this port instead run the puma command directly\n\nSomething like that?\n. Agreed binding to multiple ports would be non-obvious. \nI'm thinking either we could make it more clear where the values are coming from, maybe listing out the flags that are passed in via the options hash. Alternatively we could add some logic in Rails to not specify port if puma is bundled and a config/puma.rb file exists.\n. Yes over at https://github.com/rails/rails/issues/24990#issuecomment-223629453\nThe issue is that the Listen gem boots before the puma forks boot, so puma gives that warning. It's loosely related to a problem that i'm working on i.e. if we weren't booting before the workers get spawned then there wouldn't be an issue or a warning. I don't know if we'll be able to do that.\nI would say yes this is definitely under the rails/rails umbrella of things to worry about and not Puma, also this warning isn't something to be immediately concerned with right now. However you might want to subscribe to that linked thread\n. Not a puma issue at all. Let's close this thread.\n. That looks like normal growth\n. Here's a primer http://www.schneems.com/2015/05/11/how-ruby-uses-memory.html\nNo, GC should not reduce memory. Up to Ruby 2.3 it will not free aggressively, or not enough to matter for most apps. It also takes a larger memory overhead before it would start to reclaim memory. There is a GC setting for 2.4 that can be set, but it won't impact this use case.\nEven if the app isn't doing any thing there are still event loops that create objects and do things like set timeouts, wait for incoming requests, etc. Not just from puma but other places in your code or libraries can run loops in new threads.\nYour seeing memory grow and then level off. It's totally normal.\n. Interesting. I guess disconnecting before forking has the same effect as manually connecting after the fork. I prefer that method otherwise if there's a weird race condition where you've done a query in a separate thread or something you might accidentally create another connection maybe?\nI don't see adding it as a bad thing, however I also don't entirely think it's necessary. The establish_connection should fix any connection pool problems.\nActually, re-read that comment and I think we should be doing it. It's about connection leaks. Right now you might be left with X connections to the database open that you'll never use for each dyno. Not a big deal for a few dynos but if you've got a lot of them connections in PG aren't free. We should probably add it. Can you send a patch to put it in the plugin @jjb ?\n. We use a ubuntu image, cedar 14 uses ubuntu 14. I think it's available via docker hub but not really kept up to date https://hub.docker.com/u/heroku/. We don't do anything special with malloc that I know of. @hone might be able to say more.\n. Nope https://github.com/heroku/heroku-buildpack-ruby/issues/328\n. That's 361.5 to 372.02 mb (not much). I'm guessing it levels out.\n. Can you reproduce the memory increase locally? Can you provide us with an example app that shows the problem?\n. Sometimes I feel like memory work is like working with divining rods.\nMy internal theory for why this is normal is that this +10-20mb is where the Ruby process actually wants to be to be \"comfortable\" with the amount of memory it has. Ruby processes start with notoriously small heaps and it depending on the random place in your code when a major GC gets kicked off, you may or may not need those \"junk\" objects. It keeps on increasing until it finds its max.\nI'm not totally convinced that this isn't happening on a mac or another OS. Few people leave open processes running for 24 hours and have a minute-by-minute graph of the different memory statistics (no I don't think that new relic is reliable). Also different OS tools might report memory differently. Maybe macs give a starting process more memory than it thinks it will need by default and just happens to be in the 10-20mb range so you never observe memory going up, even though utilized memory inside of the process is going up. \nIf this memory increasing problem really isn't happening on mac, one difference could be the implementation of malloc, perhaps OS X is more aggressive about free-ing. This would make more sense for a general purpose OS that doesn't want one long lived process to eat all the resources versus linux where it is very common and desirable to have a handful of extremely long lived processes that you want to consume all the resources (a webserver for example, you want to have max performance, hence max resources). \nAnother difference could be the LXC container we are running things in.\nThese are all theories.\n. For apps that need the constants loaded, moving to autoload can increase memory since it's no longer taking advantage of CoW.\n. Great! Thanks for the patch! \ud83d\ude80\ud83d\udc4f\n. Hi, I wrote puma worker killer. Does it work if in a before_block ? https://github.com/schneems/puma_worker_killer/issues/43\nI've not used prune_bundler before. Sounds like we want to do it if you're updating your gems so that bundler will update your load path. I would think that we want this to work with all git dependencies out of the box and not just special ones that we define. Does bundler have any ways to \"refresh\" a load path from an updated Gemfile?\n. \ud83d\ude02\ud83c\udf89\n. Can you give us an example app that reproduces the problem? Are you running the latest version of puma?\n. Looks like i'm getting this (and/or) another error over on CI on this PR. Any ideas on a fix? https://github.com/puma/puma/pull/1604. Closing in favor of #986 ping if it's still an issue and clearing cache or browsing in a new \"private mode\" doesn't fix the issue, then we can re-open.\n. Going to close this in favor of #986 appreciate any thoughts on a better error message there.\n. Can you give us an example app that reproduces the problem? Is the error coming from nginx or from Puma? \n. The concept is sound, i'm not sure about the implementation. I 100 % agree with this:\n\nRather than accepting and closing in a loop (which is pretty rude and can easily confuse load balancers), the right thing to do is have the listening socket closed. I thought that was already done though....\n\nAt Heroku a thing i've seen before is that when a dyno is scaled down there seems like there might be a slight race condition where the the dyno gets told to shut down before the router gets told to stop sending requests to that dyno. \nIn that case I want to avoid even accepting any connections. So rather than accepting and closing individual connections. \nWe also need to be wary of other settings such as drain_on_shutdown. I think what I would ideally like is to first tell the socket to stop accepting new connections, then drain whatever is left in the queue. Unfortunately, I don't know if that's possible (via the socket API).\nI'm fine with making that default behavior, but likely only after a release and some in-the-wild testing. Also assuming we can get this working the way we want.\nI see this PR is quite stale.\n. Github won't let me do it. Says you closed the issue.. Doesn't the UI allow you reopen this PR?. I am a maintainer. I think I want to know when an HTTP request is malformed. \nDo you know why health checks in TCP mode with kubernetes result in malformed requests? Can we fix it there instead?. Thanks! Tests on master were failing, they've been fixed. Can you pull from master please?\nI'm wondering if we want to better support the case of multiple servers running from the same directory at the same time. We could possibly add the port to the path of the PID file, though that might break anyone's existing tooling. \nSo in this current PR all servers will be restarted if we touch the tmp/restart.txt file correct? \nYou are also correct there are no tests here. We should add some. I'm thinking an integration style test where we have a config file and actually boot up a server or several servers. I have a lib I wrote for integration testing such things https://github.com/schneems/wait_for_it, though Puma already has a few tests that boot servers. We should probably re-visit them sometime as it's relatively easy for them to be left in a bad state and need to be killed manually.\n. Great work here. This is going to go out soon in 3.8.0.\n. Yes, please review. Ping me with any questions.\n. I'm not sure. Can you try with 3.9.0 and if it's still broken open a new issue with an example app?\n. Default options are not allowed to be callable during runtime. They are finalized and turned into static values when clamp is called on the Configuration object.\nrespond_to isn't the fastest thing and calling procs has overhead. I'm leery to introduce slow-ish things that won't be widely used.\nI don't use phased restarts, but based on this issue https://github.com/puma/puma/issues/770 config changes should be picked up between restarts? Is that not the case?\nShooting from the hip here, could you call the tag method from within the before_fork block? \n. thanks!. Thanks, I'm on it.. Fixed and released in 3.8.1. Thanks for the report.\n. Is this testing existing functionality, or a filing test for a bug? Master is failing on CI.. Thanks, master is failing so a failing CI on a PR isn't unusual right now.. Well \ud83d\udca9. Thanks for the issue. I'll be able to take a look but it probably won't be for a few days I'm currently a bit sick.\n. So. I'm seeing a different behavior:\n$ rails s -b 0.0.0.0\n/Users/schneems/.gem/ruby/2.4.1/gems/activesupport-5.0.1.rc2/lib/active_support/xml_mini.rb:51: warning: constant ::Fixnum is deprecated\n/Users/schneems/.gem/ruby/2.4.1/gems/activesupport-5.0.1.rc2/lib/active_support/xml_mini.rb:52: warning: constant ::Bignum is deprecated\n[tunemygc] not enabled\n=> Booting Puma\n=> Rails 5.0.1.rc2 application starting in development on http://0.0.0.0:3000\nCan you give me an example app that reproduces the problem?. Able to repro with rails master\n$ rails s -b 0.0.0.0\n=> Booting Puma\n=> Rails 5.2.0.alpha application starting in development on http://0.0.0.0:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.4.1-p111), codename: Sassy Salamander\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:9292. The issue is how we're doing defaults\nself.set_host_port_to_config(options[:Host], options[:Port], config: user_config)\n          self.set_host_port_to_config(default_options[:Host], default_options[:Port], config: default_config)\nWhat is happening is options[:Host] is 0.0.0.0. However options[:Port] is blank, this is because 3000 is a default option and not supplied explicitly. It is passed along via default_options[:Port] which is 3000.\nHere's that method\n```\n      def self.set_host_port_to_config(host, port, config:, defaults:)\n        if host && (host[0,1] == '.' || host[0,1] == '/')\n          config.bind \"unix://#{host}\"\n        elsif host && host =~ /^ssl:\\/\\//\n          uri = URI.parse(host)\n          uri.port ||= port || ::Puma::Configuration::DefaultTCPPort\n          config.bind uri.to_s\n        else\n      if host\n        port ||= ::Puma::Configuration::DefaultTCPPort\n      end\n\n      if port\n        host ||= ::Puma::Configuration::DefaultTCPHost\n        config.port port, host\n      end\n    end\n  end\n\n```\n. Interesting. I don't use the restart feature due to Heroku. So I have little operational opinions on it. This seems like reasonable behavior on the surface. You're telling the server to go away and a new one to come up. My only concerns oils be about why hooks have people used to load or set env vars. If it's something that wouldn't get called after a restart then this could break.. maybe we could do a diff of the environment and output it as a warning/debug-info?. If you think without is the way to go, maybe leave a comment with that code in this issue so we could add it back if lots of people are confused by the behavior.. Looks like master is failing with the same error. Thanks for the PR.. >  apps that are not yet threadsafe?\nI actually don't recommend puma for apps that aren't threadsafe, unless it's a step to getting the app threadsafe. I recommend the rack-freeze gem for threadsafety in the middleware (it's a great gem). \nI've seen performance issues with Puma using X workers and 1 thread compared to Unicorn with X workers. I'm not totally sure why, but i've seen this several times. You could also try passenger.\nI don't think this patch would hurt if you're only using 1 thread, but i've seen other perf issues with that setup. I would recommend either putting in leg work to get the app threadsafe, or moving to a different server.\n. > nvm ... we need to not lose requests ... so I guess this is still on :D\nYou're good to go with this PR? I think it's fine.\n. Done \ud83d\udea2 \ud83c\udf89 . Can you try with puma master? I think this is already fixed\n\nhttps://github.com/puma/puma/pull/1277\nhttps://github.com/puma/puma/pull/1290\n. Broken on 3.9 as well?. We can do something like this to re-raise the exact exception https://stackoverflow.com/questions/29568298/run-code-when-signal-is-sent-but-do-not-trap-the-signal-in-ruby. Hmm, it should have been closed automatically by github. Apparently I got my issue number wrong. Thanks for the eyes. Ahh, multiple duplicate issues. If you remove PWK does the problem go away?. I have another metric I want to expose via Puma.stats probably next week. I\u2019m fine if this goes out before then though.. I cut a release can this Issue be closed?. This broke master, the scope of the variable @host was changed to host in another commit. Going to try working on a fix on #1542. Does #1532 meet all your needs here?. I don\u2019t have plans to emit to stdout. A plug-in would be good for now.. That\u2019s great!. @jjb great! I'm trying to log backlog as an optional metric at Heroku https://github.com/heroku/barnes/pull/14. I took a look at puma-stats-logger originally and was surprised to find out the hoops that had to be jumped through to get the info. Seems like this patch already satisfies a few use cases :)\n. Where is Puma::DSL::PumaStats coming from? Can you give me an example app that reproduces the problem and open a new issue?. Thanks for the PR!. Looks like it was:\n\nTestIntegration#test_restart_closes_keepalive_sockets_workers = /home/travis/build/puma/puma/lib/puma/server.rb:395:in `close': can't modify frozen IOError (RuntimeError)\n    from /home/travis/build/puma/puma/lib/puma/server.rb:395:in `ensure in handle_servers'\n    from /home/travis/build/puma/puma/lib/puma/server.rb:399:in `handle_servers'\n    from /home/travis/build/puma/puma/lib/puma/server.rb:328:in `block in run'\n1.64 s = .\nI'm sure there's a bug or a race condition in the tests, somewhere, but \ud83e\udd37\u200d\u2640\ufe0f\nI merged in your windows fixes to master. Since this commit doesn't touch windows or appveyor, i'm going to merge in even though that one system test is failing.\nThanks for the pr!\n. 2.2 continues to fail randomly on master. When I restart it, it generally works. Should we switch back to 2.8 for consistency/sanity sake?. I merged the other PR into master. Thanks for this one though.. I\u2019m fine with this.. Thanks for the PR!. Do you know what if any effect this will have on the backlog metric?\nOn Thu, Apr 19, 2018 at 10:14 AM jasquat notifications@github.com wrote:\n\nThanks @josler https://github.com/josler and @dannyfallon\nhttps://github.com/dannyfallon. This does seem to have fixed my issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1563#issuecomment-382773885, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AADpYLjAUaltP5lfErkYtKx09gEK7Otnks5tqKm4gaJpZM4TY7FV\n.\n. Thanks for the work and the explanation!\n\nIt looks like #1471 provides a test case for this behavior. Can you confirm the issue is gone with this patch?\n\nI expect that we'll never have @waiting > 0 and backlog > 0 which I believe was the goal of the original PR\n\nAhh, so if there is a free thread to process requests, there shouldn't ever be a request waiting in the backlog. Makes sense. I'm currently using backlog as a proxy for \"are we keeping up with the work\". It sounds like this won't impact that?\n. One issue to consider is the \"slow client\" attack problem. If we're only ever allowing the same number of connections into the reactor as are in the current thread pool then it will be easy to exhaust that and  force the server to become unresponsive. \n. Yes. Nginx has its own reactor that will handle slow clients. However many people are using puma over unicorn explicitly for the slow client protection (so they get it without also having to run Nginx).  We could consider vendoring in nginx like passenger. Also I think there was one proposal to run Puma via passenger awhile ago. I do not think this is the kind of thing we can just quietly drop support for. \n. Okay I've read through a lot of the Puma source code and added docs in #1575 and #1576. So now I think i actually understand what's going on in Puma (in general). At least enough to say more comprehensible things about this PR. \nModifying this does not impact the ability to deal with slow clients at all, yay! Because the @todo size will only increase when a request a fully formed body makes its way through the reactor. While this is happening the wait_until_not_full will continue to not wait, which continues to allow the Puma::Server to add new requests to the reactor. This will happen until one of those requests is fully buffered and is then passed to the thread pool.\nBased on my new found understanding of how puma works, I think this PR should be merged. \ud83c\udf89\ud83c\udf89\ud83c\udf89\nHonestly this is a tangent but since i'm thinking out loud i'll mention it here: This does present a problem for measuring the backpressure of the server as with this patch the backlog will always be at or close to zero. I don't think we can get  metrics from the TCP socket as to how many requests are waiting. Even if we could, they might not be fully buffered requests, so that wouldn't really be a good indicator anyway. I'll open up another issue.\nThanks for the PR. I'll see what I can do to get this merged.\n. Thanks a ton for the PR and bearing with me while I figure out how Puma works!!. Puma.stats is mostly for high level server metrics. Also everything in there is pretty cheap to compute.\nThis would be a per request metric. The only good way to expose that would be either through the env hash like this or some kind of more architected notification API where people could subscribe to certain events.\nCheck out https://github.com/puma/puma/issues/1577 and the attached PR. The goal is to give a high level indication that the app needs to scale out with more servers. Is that kinda what you\u2019re trying to achieve with these measurements?\n. I added it when I made this change https://github.com/puma/puma/commit/995ed8bd4585574543977a2fd3f80de55cff2236. Though it broke some stuff and got modified later like in https://github.com/puma/puma/pull/1340/files. The comment might not be true anymore. It\u2019s hard to keep track of how all the config works which is why I tried leaving all the comments. This change doesn\u2019t break tests so that\u2019s a good sign. Can you give me an example of what exactly you\u2019re trying to do and how it\u2019s not working? In general I think the dsl load is not a very public facing feature. \nIs it possible for you to write a test that fails on master but passes on your branch?\n. We need a test to merge otherwise we risk accidentally reverting this capability.\nFor your use case you can maybe build it into a plugin. \nAlso be careful with this measurement, if you\u2019re running on a shared host like Heroku\u2019s 1x dynos then it will show 8 processors even though none of them are dedicated to your app.. @jjb \n\ncan't we now consider @todo.size - @waiting to be that number to watch? \n\nThe issue is in the implementation. @todo only grows when there are threads that are not doing work. The only time that @todo should be higher than zero is during the slight race condition where a request is added to the @todo array, but is not yet picked up by an idle worker. With this patch it means that @todo should always be small, usually zero but sometimes one. If we use @todo - @waiting then we should always get zero.\nPreviously that metric should have been close to zero, but there was essentially a bug that when the above mentioned race condition happened it could let say 6 requests in before trying to process them. Basically even before this patch, the metric wasn't actually all that useful. Or at least wasn't doing what I thought it was.\n@SamSaffron  Thanks for the info. that's helpful. I also didn't know you used unicorn. Is discourse threadsafe?\n\nI don't know if you require NGINX in your architecture or not, but fronting stuff with it simplifies this so so much.\n\nThat's a good question and something to look into. We do not require nginx in front of Puma. We talked about using it as a reactor at last RailsConf (phoenix) but punted on it. \nI'm curious if @foobarwidget has any insights. Is there a way to measure this kind of \"backpressure\" in passenger? Can you get any meaningful metrics out of nginx directly? \n. Another idea: instead of measuring the capacity we don't have which is what i'm trying to do with \"backpressure\" we could instead measure the capacity that I do have. Basically we could report idle time and when all servers are reporting zero (or low) idle time for a period then we know we are at capacity. \nFor example if I know that I have 2 workers and 5 threads, if my service isn't getting hit much then maybe it's telling me my idle capacity is 10. If I get hit with a New York times mention (or whatever) then suddenly I see that my idle capacity is down to zero, then if I scale out by adding another server I should get some slack re-introduced into my system.\n. Yep!. > Where did this idea come from? Discussion in another ticket/channel/IRL? Does this resolve the \"we can't access a meaningful metric given the current design\" conundrum discussed in #1577, or is it more of a stopgap solution?\nI think this is the thing TM that \"fixes\" #1577. Though if you disagree, i'm happy to re-open. \nAhh, i see i missed some of your comments over there, sorry about that. I had intended conversation about this specific interface to happen on this PR. I'm using this in production right now and it is providing good feedback. \n\nWhat is the highest value that this can be?\n\nThat's already been asked, I can add that in as another metric. Seems worthwhile. Per worker it would be max-threads. It's a good idea to let the stat tell us that directly. While i'm doing that I can also expose the current number of threads per worker. The two values are related.\n\n(related to above) How (if at all) is it affected by 0:N vs. N:N thread configuration?\n\nGoing down to 0 in \"pool capacity\" is bad in the N:N config. There may be a race condition with 0:N case where all your workers drop to zero (because you've not yet hit N threads) and then the stat fires and reports that you have 0 capacity, then they all dynamically create a new thread and the capacity would actually be 1 times the number of processes.\nI think this is mitigated by the fact that we would need to use a stream of this metric rather than just one value. I.e. even if it's showing 0 right now, under the same sustained load it would show a positive number the next time it reports. Even if we took some scaling action based on the 0 indicator, a future stream of positive numbers would perhaps indicate we could safely scale back down. \n. Let me know what you think!\nThe other major question to answer with metrics is \"am I using a good number for my thread count?\".\nI'm not totally sure about that one. My current best suggestion is to pick a magic number and live with it. Alternatively, adjust it up or down and note if the average response time increases or decreases.\nI guess I want some combination of a CPU metric (want to be close to 100% utilization) but also some kind of a \"contention\" metric. For example, you can certainly saturate an app with 1000 threads, but your contention will be through the roof.\nMaybe contention could be time spent idle per thread. I don't know if it's possible to get that from Ruby, it might require a patch to ruby/ruby and wouldn't be useful for at least a year until it gets released.\nI'm open to other thoughts/suggestions. Maybe we should open another issue on it for some brainstorming.\n. Interesting. That could be really cool. There's lots of apps that can't run that many workers. I also worry about large apps that might need to tightly control DB connection counts.\nI think this is a good experiment. We could maybe manually do this in a \"on_worker_boot\" block of an app, though would need a way to determine thread multiplier for each process.\nI'm not sure how to log and record throughput of each individual worker in that scenario.\n\nDoing this would require that the logic which distributes requests to processes is very accurate in its determination of if a process has available capacity. I don't know to what extent this is possible.\n\nPuma 3.12 does a good job of this AFAIK after a recent patch. You can read the docs about how this all works https://github.com/puma/puma/pull/1576.\nLet me know if you've got any other ideas. The value of a brainstorming session usually comes from the number of ideas. With quantity comes quality.\n. If you can send a PR that passes the test I see no reason not to use it.\n\nAFAIK this has possibly significant benefits with no drawbacks.\n\nI've done memory benchmarking with derailed benchmarks and never found a hotspot inside of puma. That being said, every little bit helps as long as it doesn't cause significant pain or errors, i'm for it \ud83d\udc4d.. Both CI's are failing.. This is my failure:\nTestBinder#test_binder_parses_jruby_ssl_options = rake aborted!\nCommand failed with status (1)\n/home/travis/build/puma/puma/vendor/bundle/jruby/2.3.0/gems/rake-12.3.1/exe/rake:27:in `<main>'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/cli/exec.rb:1:in `(root)'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/cli/exec.rb:74:in `kernel_load'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/cli/exec.rb:28:in `run'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/cli.rb:424:in `exec'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor/command.rb:27:in `run'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor/invocation.rb:126:in `invoke_command'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor.rb:387:in `dispatch'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/cli.rb:27:in `dispatch'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor/base.rb:466:in `block in start'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/cli.rb:18:in `start'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/exe/bundle:30:in `<main>'\n/home/travis/.rvm/gems/jruby-9.1.17.0/gems/bundler-1.16.2/lib/bundler/friendly_errors.rb:124:in `<eval>'\n/home/travis/.rvm/gems/jruby-9.1.17.0/bin/jruby_executable_hooks:24:in `<main>'\nTasks: TOP => default => test:all => test\n(See full trace by running task with --trace)\nAny ideas? Seems unrelated.. Can't run locally due to different unrelated error:\n```\n\u26c4  2.3.3 \ud83d\ude80  ~/documents/projects/puma (schneems/max-thread-metric)\n$ ruby -v\njruby 9.1.17.0 (2.3.3) 2018-04-20 d8b1ff9 Java HotSpot(TM) 64-Bit Server VM 25.40-b25 on 1.8.0_40-b25 +jit [darwin-x86_64]\n$ be rake test TEST=test/test_binder.rb\nmkdir -p tmp/java/puma_http11\njavac -extdirs \"/Users/rschneeman/Library/Java/Extensions:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/ext:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java\" -target 1.5 -source 1.5 -Xlint:unchecked  -cp \"/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_40.jdk/Contents/Home/jre/classes:/Users/rschneeman/.rubies/jruby-9.1.17.0/lib/jruby.jar:\" -d tmp/java/puma_http11 ext/puma_http11/PumaHttp11Service.java ext/puma_http11/org/jruby/puma/Http11.java ext/puma_http11/org/jruby/puma/Http11Parser.java ext/puma_http11/org/jruby/puma/MiniSSL.java\nwarning: [options] bootstrap class path not set in conjunction with -source 1.5\nwarning: [options] source value 1.5 is obsolete and will be removed in a future release\nwarning: [options] target value 1.5 is obsolete and will be removed in a future release\nwarning: [options] To suppress warnings about obsolete options, use -Xlint:-options.\nNote: ext/puma_http11/org/jruby/puma/Http11.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n4 warnings\ntouch tmp/java/puma_http11/.build\njar cf tmp/java/puma_http11/puma_http11.jar -C tmp/java/puma_http11 org/jruby/puma/Http11\\$1.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$2.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$3.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$4.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$5.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$6.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$7.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$8.class -C tmp/java/puma_http11 org/jruby/puma/Http11\\$9.class -C tmp/java/puma_http11 org/jruby/puma/Http11.class -C tmp/java/puma_http11 org/jruby/puma/Http11Parser\\$ElementCB.class -C tmp/java/puma_http11 org/jruby/puma/Http11Parser\\$FieldCB.class -C tmp/java/puma_http11 org/jruby/puma/Http11Parser\\$HttpParser.class -C tmp/java/puma_http11 org/jruby/puma/Http11Parser.class -C tmp/java/puma_http11 org/jruby/puma/MiniSSL\\$1.class -C tmp/java/puma_http11 org/jruby/puma/MiniSSL\\$2.class -C tmp/java/puma_http11 org/jruby/puma/MiniSSL\\$MiniSSLBuffer.class -C tmp/java/puma_http11 org/jruby/puma/MiniSSL\\$SSLOperation.class -C tmp/java/puma_http11 org/jruby/puma/MiniSSL.class -C tmp/java/puma_http11 puma/PumaHttp11Service.class\ninstall -c tmp/java/puma_http11/puma_http11.jar lib/puma/puma_http11.jar\nRun options: --seed 60297\nRunning:\nSSSrake aborted!\nCommand failed with status (1)\n/Users/rschneeman/.gem/jruby/2.3.3/gems/rake-12.3.0/exe/rake:27:in <main>'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/cli/exec.rb:1:in(root)'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/cli/exec.rb:74:in kernel_load'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/cli/exec.rb:28:inrun'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/cli.rb:424:in exec'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor/command.rb:27:inrun'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor/invocation.rb:126:in invoke_command'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor.rb:387:indispatch'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/cli.rb:27:in dispatch'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/vendor/thor/lib/thor/base.rb:466:inblock in start'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/lib/bundler/cli.rb:18:in start'\n/Users/rschneeman/.gem/jruby/2.3.3/gems/bundler-1.16.2/exe/bundle:30:in'\n/Users/rschneeman/.gem/jruby/2.3.3/bin/bundle:23:in `'\nTasks: TOP => test\n(See full trace by running task with --trace)\n```. Tests on master are borked for jruby, needs investigation.. Great PR! Thanks!\nWhat's a good way to test this with another tool? Does curl allow me to send chunked requests? What would a good terminal command be for me to test this out?\n. Awesome! thanks for the working repro case. I confirm this fails without your patch and passes with it.\nskylight (1.6.0) lib/skylight/probes/middleware.rb:10:in `call'\nrack-mini-profiler (0.10.7) lib/mini_profiler/profiler.rb:282:in `call'\nskylight (1.6.0) lib/skylight/probes/middleware.rb:10:in `call'\n/Users/rschneeman/.gem/ruby/2.5.1/bundler/gems/raven-ruby-9649fa5a50e5/lib/raven/integrations/rack.rb:51:in `call'\nskylight (1.6.0) lib/skylight/probes/middleware.rb:10:in `call'\nrailties (5.2.0) lib/rails/engine.rb:524:in `call'\n/Users/rschneeman/.gem/ruby/2.5.1/bundler/gems/puma-7f71af4b45ba/lib/puma/configuration.rb:225:in `call'\n/Users/rschneeman/.gem/ruby/2.5.1/bundler/gems/puma-7f71af4b45ba/lib/puma/server.rb:658:in `handle_request'\n/Users/rschneeman/.gem/ruby/2.5.1/bundler/gems/puma-7f71af4b45ba/lib/puma/server.rb:472:in `process_client'\n/Users/rschneeman/.gem/ruby/2.5.1/bundler/gems/puma-7f71af4b45ba/lib/puma/server.rb:332:in `block in run'\n/Users/rschneeman/.gem/ruby/2.5.1/bundler/gems/puma-7f71af4b45ba/lib/puma/thread_pool.rb:133:in `block in spawn_thread'\n2018-08-15 12:51:24 -0500: HTTP parse error, malformed request (): #<Puma::HttpParserError: Invalid HTTP format, parsing fails.>\nThanks a ton!. I don't honestly know. I can try to queue up a relase. In the mean time can you use the github source in your gemfile?. Tests for travis are good. Windows tests seem borked for a totally unrelated reason. Thanks a ton for the PR!\nWindows error:\nbundle --version\nC:/Ruby24/bin/bundle:23:in `load': cannot load such file -- C:/Ruby24/lib/ruby/gems/2.4.0/specifications/gems/bundler-1.16.1/exe/bundle (LoadError)\n    from C:/Ruby24/bin/bundle:23:in `<main>'\nCommand exited with code 1\nruby -v\n. This is great! Thanks!\nI\u2019ll take a look at the other PR shortly. I like the change for skipping certain suites. We merged in a change that makes the required version of Ruby 2.2 or higher so we could make the str a kwarg.\n. Thanks!\n. Thanks a ton!. > SignalException should not be raised in this case.\nThe exception is used by many libraries to clean up on SIGTERM. Firing that exception forces all the ensure blocks of the application to fire, for example telling libraries to safely close references to files etc. I don't have a problem with this being a configurable option, but I do worry about the unintended side effects of enabling this for long term system stability.\n. Thanks for your PR I totally missed it. You're pretty fast on the gun. Need this for https://github.com/puma/puma/pull/1700. Hey @MSP-Greg. I'm having some issues with my local install of jruby. I need to get that figured out to release a jruby gem, so we have a little time.\nCan you give me details on how to grab and push the binaries for windows? It's not a thing I have much experience with.\nAs a related side note, not providing vendored versions actually has allowed teams (unintentionally) to vendor gems and still deploy to Heroku. As long as none of the gems are pre-compiled, then it doesn't matter if you're installing from windows/mac and deploying to linux.\n. Tried again today to release, and I'm still getting jruby errors. Not sure what's going on\n$ be rake java gem\njavac -extdirs \"/Users/rschneeman/Library/Java/Extensions:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/ext:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java\" -target 1.6 -source 1.6 -Xlint:unchecked  -cp \"/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/resources.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/rt.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/sunrsasign.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/jsse.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/jce.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/charsets.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/lib/jfr.jar:/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/classes:/Users/rschneeman/.rubies/jruby-9.2.6.0/lib/jruby.jar:\" -d tmp/java/puma_http11 ext/puma_http11/PumaHttp11Service.java ext/puma_http11/org/jruby/puma/Http11.java ext/puma_http11/org/jruby/puma/Http11Parser.java ext/puma_http11/org/jruby/puma/MiniSSL.java\next/puma_http11/PumaHttp11Service.java:5: cannot access org.jruby.Ruby\nbad class file: org/jruby/Ruby.class(org/jruby:Ruby.class)\nclass file has wrong version 52.0, should be 50.0\nPlease remove or make sure it appears in the correct subdirectory of the classpath.\nimport org.jruby.Ruby;\n                ^\nrake aborted!\nCommand failed with status (1): [javac -extdirs \"/Users/rschneeman/Library/...]\n/Users/rschneeman/.gem/jruby/2.5.3/gems/rake-compiler-1.0.7/lib/rake/javaextensiontask.rb:98:in `block in define_compile_tasks'. Figured out the issue, or at least worked around it with a reinstall of java via homebrew cask. Released 3.12.1. . Thanks @MSP-Greg I will take a look, btw are you going to be going to any Ruby Conferences this year? RailsConf or Kaigi or Euruko?. I'm not sure where to start with this one. Does it happen if the server is under load? What command do you use to boot the server? Can you reproduce this in a docker container? Approximately how long are you having to wait for? If you change the worker or thread number does it no longer crash?\n\nRuby version: 2.4.0\n\nThe first thing I would have you do is upgrade to the latest minor version which would be 2.4.5.\n\nRails version: 5.0.2\n\nSame here, upgrade to the latest version of Rails, you didn't mention Rails earlier, does this only happen on one specific Rails project? Does it happen without Rails?\n\nPuma version: 3.12.0\n\nI just released 3.12.1 today, give that a shot, probably won't help. But it likely won't hurt.\n. It looks like that line in the control_cli was added in https://github.com/puma/puma/commit/274350a2e69cc86e3fa9d8f30ac8dea9f4afeef0 about 6 years ago. I generally don't like having requires in methods unless there's a really good reason. \nI don't see a downside to having the require be outside of the method and at the top of the file, though feel free to let me know if that causes issues.\n. Can you try out a fix on a branch and we can see if the tests pass? I'm fine with an autoload, but it does seem like overkill for that one little constant. ~I think once we get this fix in then we can probably ship another version I'm working on a history.md now.~ Actually looks like there are other release plans. \n. > nio4r? etc?\nLikely, a slightly more purposeful release rather than just the cobbled together remnants of master. Ideally targeting RailsConf timeframe.\n. I think this is a bundler/rubygems bug and not an issue with Puma. Does the bundle command fail with other commands such as bundle exec ruby -v or only with bundle exec puma ...?\nIf it fails across the board I would recommend building a reproduction application that you can share with the bundler team over at bundler/bundler.\n. > No this issue only occurs with bundle exec puma\nThat's weird. Makes me think the problem might be with your binstubs. Check bin/puma maybe it's accidentally pointing to a different version of Ruby.\n. What if we use Process.setproctitle if the target is running Ruby 2.1.0 or above?\n. We could make this a bit simpler\nruby\nProcess.respond_to?(:setproctitle) ? Process.setproctitle(process_title) : $0 = process_title\n. Can you remove the extra whitespace? :scissors: \n. That was a copy pasta error. Will fix. Sure if you prefer that.. thing => think. What's this comment referring to? Is it extra?. Is there an issue that needs to be fixed? Are we not testing something with the handler that we should?. This should fix it https://github.com/puma/puma/pull/1546. I'm really jumpy around this line of code. This controls a disproportionate amount of what puma does compared to it's size. While the method is only a few lines of code there's 30 lines of docs (most of which I wrote).\nOriginal code \n@todo.size - @waiting < @max - @spawned\nCan be re-written as \n@todo.size - @waiting + @spawned < @max\nor flipping it\n@max > @todo.size - @waiting + @spawned\nre-aranging\n@max > @spawned - @waiting + @todo.size\nAnd finally \nbusy_threads = @spawned - @waiting + @todo.size\n@max > busy_threads\nSo that looks fine. Just makes me nervous.\n. The other question to ask: is this the correct logic. \n@waiting is the count of threads that have been spawned but are idle. So busy count would be the spawned count minus the idle count. \nWe then need to handle for the case where a request has been processed but not picked up yet so we subtract the backlog. \nSo that makes sense.. > after only a single request was added to the worker-process\nWas this a web page load or did you make the request via curl? if it's a webpage don't forget there are assets that each require a different request.\n\nthere might be some concurrency edge-case not handled perfectly by the current logic.\n\nYes there's a known race condition between the time where a request is added to the @todo array and when a new thread picks it up and starts working on it. So it's possible that @todo.size might be greater than 0 but all threads be idle. \n\nAnyway, it's certainly possible to leave the equation untouched by this PR if that makes it any less nerve-wracking. I figured making the 'busy threads' count more explicit in this method makes the logic easier to follow, but the change isn't strictly necessary.\n\nI'm fine with the change just wanted to be sure I talked through it out loud and it preserved behavior.. This threw me off, wanted to mention in case anyone else didn't see what was happening. This else only gets fired if there's no exception https://blog.newrelic.com/engineering/weird-ruby-2-rescue-interrupt-ensure/. ",
    "bai": "Hey folks, I was curious if it's something that's in the works or planned?\n. ",
    "MikeAski": "+1\n. ",
    "sigursoft": "I believe you should be able to restart server by SIGUSR2 even if control stuff is not configured and initialized.\n. ",
    "jc00ke": "I was going to ask for an opinion on making the Content-Type always JSON, but I figure for the 403 & 404 responses it makes a lot more sense to return text/plain\n. I wish it did...\n\u2234 rake -T\n** README.md is missing or in the wrong format for auto-intuiting.\n   run `sow blah` and look at its text files\n** History.txt is missing or in the wrong format for auto-intuiting.\n   run `sow blah` and look at its text files\nManifest is missing or couldn't be read.\nThe Manifest is kind of a big deal.\nMaybe you're using a gem packaged by a linux project.\nIt seems like they enjoy breaking other people's code.\n. Yes, proper clone.\n```\n\u2234 gem list hoe\n LOCAL GEMS \nhoe (3.0.3, 3.0.0)\n. 100%. I've seen it before with Hoe.\n.\n\u2234 ls\nbin  COPYING  examples  ext  Gemfile  Gemfile.lock  History.txt  lib  LICENSE  Manifest.txt  puma.gemspec  Rakefile  README.md  test  TODO  tools\n```\n. Patch on its way\n. Sunnova... DUPE!\n. An H12 is a request timeout (request took longer than 30s)\nhttps://devcenter.heroku.com/articles/error-codes#h12-request-timeout\nSorry, should have been more clear in the issue description.\n. I'm loading ~45 gems, so nothing crazy.\n@evanphx told me that the OS distributes requests using accept(2) so I wonder how a worker would receive requests even though it hasn't bound itself to a port? Is that what's happening?\n. :thumbsup: so far no H12s\n. :heart_decoration: \n. ```\ncurrent directory: /home/jesse/.asdf/installs/ruby/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.3.0/ext/puma_http11\n/home/jesse/.asdf/installs/ruby/2.3.1/bin/ruby -r ./siteconf20180706-736-l02lxz.rb extconf.rb\nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\nchecking for openssl/bio.h... yes\ncreating Makefile\nTo see why this extension failed to compile, please check the mkmf.log which can be found here:\n/home/jesse/.asdf/installs/ruby/2.3.1/lib/ruby/gems/2.3.0/extensions/x86_64-linux/2.3.0-static/puma-3.3.0/mkmf.log\ncurrent directory: /home/jesse/.asdf/installs/ruby/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.3.0/ext/puma_http11\nmake \"DESTDIR=\" clean\ncurrent directory: /home/jesse/.asdf/installs/ruby/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.3.0/ext/puma_http11\nmake \"DESTDIR=\"\ncompiling mini_ssl.c\nmini_ssl.c: In function \u2018get_dh1024\u2019:\nmini_ssl.c:90:5: error: dereferencing pointer to incomplete type \u2018DH {aka struct dh_st}\u2019\n   dh->p = BN_bin2bn(dh1024_p, sizeof(dh1024_p), NULL);\n     ^~\nmini_ssl.c: In function \u2018engine_init_server\u2019:\nmini_ssl.c:161:3: warning: ISO C90 forbids mixed declarations and code [-Wdeclaration-after-statement]\n   DH dh = get_dh1024();\n   ^~\nmini_ssl.c:165:3: warning: ISO C90 forbids mixed declarations and code [-Wdeclaration-after-statement]\n   EC_KEY ecdh = EC_KEY_new_by_curve_name(NID_secp521r1);\n   ^~~~~~\nmini_ssl.c: In function \u2018engine_init_client\u2019:\nmini_ssl.c:192:3: warning: \u2018DTLSv1_method\u2019 is deprecated [-Wdeprecated-declarations]\n   conn->ctx = SSL_CTX_new(DTLSv1_method());\n   ^~~~\nIn file included from /usr/include/openssl/ct.h:13:0,\n                 from /usr/include/openssl/ssl.h:61,\n                 from mini_ssl.c:15:\n/usr/include/openssl/ssl.h:1645:1: note: declared here\n DEPRECATEDIN_1_1_0(__owur const SSL_METHOD DTLSv1_method(void)) / DTLSv1.0 /\n ^\nmake: ** [Makefile:239: mini_ssl.o] Error 1\nmake failed, exit code 2\n```\nThis would indeed be handy. I'm trying to install Puma in Ruby 2.3.1 with LibreSSL (or OpenSSL 1.0.1f but I can't get Ruby to install correctly, anyway). ",
    "tomykaira": "Is this problem fixed?\nI got into the same error with rbx-head while hue works with MRI 1.9.3.  Do you have an idea?\n. The CI failure seems to be the temporary problem of travis CI.\nIn my environment, all tests pass.\nPlease re-run CI, if possible.\n. ",
    "dstrelau": "@evanphx the failure looks like a bug in Rubinius' StringIO to me. I used StringIO since it was already required as part of puma/server, and it seemed the simplest way to ensure it behaved /exactly/ like IO#read.\n```\n$ ruby -v -rstringio -e 'puts StringIO.new(\"\").read(0).inspect'                                                                                                              \nruby 1.9.3p194 (2012-04-20 revision 35410) [x86_64-darwin12.0.0]\n\"\"\n$ ruby -v -rstringio -e 'puts StringIO.new(\"\").read(0).inspect'                                                                                                              \nrubinius 2.0.0dev (1.9.3 a8a5f52d yyyy-mm-dd JI) [x86_64-apple-darwin12.0.0]\nnil\n``\n. Okay, how's that? It won't raise the correct exception if someone does something stupid (eg,read(-4)`), but should be close enough.\n. Thanks!\n. ",
    "codeblooded": ":+1: I think it would be helpful as well.\n. :+1: Thanks for this!\n. ",
    "nizhib": "You launch it as puma <options> <rackup file>. So puma /path/to/config.ru is what you need, isn't it?\n. ",
    "ender672": "Here is a version of the above which correlates the lines to puma's source code: https://gist.github.com/2638291\n. I tested with a few more ruby versions:\njruby-1.6.7 - no deadlock\nrbx-2.0.0dev - soft deadlock (requires SIGINT)\n1.8.7-p352 - no deadlock\n1.9.2-p290 - hard deadlock (requires SIGKILL)\n1.9.2-p320 - hard deadlock\n1.9.3-p0 - hard deadlock\n1.9.3-p194 - hard deadlock\n. With that patch I cannot reproduce the hang with any ruby versions.\nIs it worth reporting this issue upstream as an interpreter problem?\n. After thinking this over a little, I think it is the wrong approach. Should probably be pursued with rack and then rails.\n. ",
    "michelson": "i have changed the server to trinidad and i got the same problem, i've found that the problem was on rack gem on session generator,  so i've updated to rack 1.4.1 and the problem gone. \ni will try this on puma too to confirm it\n. +1\n. ",
    "hemju": "@michelson is this issue still valid?\n. ",
    "czarneckid": "OK, I'll open the issue over there.\n. ",
    "kenkeiter": "Actually, there might be a better way to do this. I'll resubmit.\n. @tarcieri Good points, all of them. I'm simply looking for supervision chains, since it's a long-running process. I wish that Celluloid had a way to supervise existing threads because having to patch Puma and include Celluloid is sub-optimal. \nAt the same time, I do find it a bit odd that Puma starts its runloop in another thread by default.\n. ",
    "tarcieri": "Just a note on the example code here: I'm not really sure it's wise to try to force Celluloid upon code that wasn't really designed to use it. Celluloid will work just fine side-by-side with other multithreaded Ruby code, so there's no reason why you can't write your code with Celluloid and let Puma manage its own threads.\nThe Rack middleware API also makes it quite easy to use up all of the available stack space on YARV fibers as the stacks are quite small, and Celluloid presently runs all methods in fibers (the next release will let you opt out of this for certain methods). So if you do try to patch Celluloid in like this, you'll definitely want to avoid using Rack middleware if possible.\n. You'll probably want to add a @selector.close here:\nhttps://github.com/puma/puma/blob/d8bf71a9ef09acd25932a4b528812a58bd8ac090/lib/puma/reactor.rb#L164\n. @ioquatix virtual memory, not physical memory (until you actually use the stack space). I think you want nio4r here: https://rubygems.org/gems/nio\n. You can replace this with the NIO::Selector#wakeup mechanism, which is basically doing the same thing behind the scenes\nEdit: oh, looks like you're also passing some data along... carry on then\n. ",
    "spastorino": "No idea what github did here, opening a PR again\n. @seamusabshere ask @evanphx for a nice explanation :), he is the one that started using the approach\n. @zacksiri have you tried the latest puma version?. Can you give any pointers about how to reproduce the issue?. \nBecause I can't :(.\n. It's working fine for me even asking for json format. If you can upload an example to reproduce the issue I will fix it.\n. ",
    "chanks": "I had a similar problem a month or so ago with Puma (intermittently) ignoring the _method param in Rails forms, resulting in routing errors. It only occurred in production though (on Heroku) - I could never recreate it on my development machine. I switched back to thin and things have worked fine since then. Didn't file an issue because I couldn't recreate it, but this might be related.\n. I just diagnosed a problem with an app wherein Rack::Timeout's errors (which descend from the Exception class instead of StandardError) weren't being caught by anything and were killing Puma's workers. Then new requests that came in wouldn't be serviced and would timeout themselves.\nI'm going to open an issue with rack-timeout, but in the meantime I wrote a quick middleware to catch those errors and reraise them in a way that Puma can recover from. Haven't deployed to production yet, but it seems to work alright in testing.\n```\nclass TimeoutRecovery\n  def initialize(app, options = {})\n    @app, @options = app, options\n  end\n# Rack::Timeout, as of 0.2.4, raises errors that descend from the Exception\n  # class when a request takes too long - however, this is a very strong\n  # exception level and will kill Puma's worker if we let it continue down the\n  # stack. So instead, catch it and wrap it in a recoverable StandardError.\nclass Error < StandardError; end\ndef call(env)\n    @app.call(env)\n  rescue Rack::Timeout::Error => e\n    raise Error, \"Original Error: #{e.class.inspect}, #{e.message.inspect}, #{e.backtrace.inspect}\"\n  end\nend\n```\n(Edit: Be sure that it comes before Rack::Timeout in your middleware stack, naturally.)\n(Edit #2: https://github.com/heroku/rack-timeout/issues/76)\n. ",
    "zacksiri": "I will try updating my puma gem and report back. I am making the call with JSON format maybe that will help reproducing it?\n. OK I am still getting this issue, I should add that i am namespacing my controller? here is my controller code\nyou can view the gist here https://gist.github.com/3292307\ni upgraded to puma 1.5.0\n. I am going to create a simple app and put it up on github\n. Ah ok wow thx for this, could never figure this out!\n. ",
    "acorcutt": "I also had this problem with my dev setup, I tracked it down to be from using a custom proxy configuration to map a port to localhost such as: https://gist.github.com/766693\nTurning this off and using a standard etc/hosts setup fixed it.\n. ",
    "blanchma": "I'll give it a try. Thanks\nOn 31/07/12 08:22, Dar\u00edo Javier Cravero wrote:\n\nHey @blanchma,\nHave you tried a vhost definition like this one:\n```\n     NameVirtualHost 99.99.99.99\n     ServerName yourapp.com\n     ServerSignature Off\n     ProxyRequests Off\n     \n         Order Allow,Deny\n         Allow from all\n     \n     ProxyPass / http://localhost:3000/\n     ProxyPassReverse / http://localhost:3000/\n     ProxyVia On\n\n``````\nTo start your app with tcp instead of a socket, run something like puma -b tcp://127.0.0.1:3000.\nI don't know if Apache works with unix sockets, Nginx definitely does. If you need a config for it, here's a very simple one:\n```upstream myserver {\n   server unix:///path/to/web/tmp/puma.sock;\n}\nserver {\n   listen 80;\n   server_name myserver.com;\nroot /exordo-dev/web/public;\naccess_log /path/to/web/log/nginx.access.log;\n   error_log /path/to/web/log/nginx.error.log;\ntry_files $uri @app;\nlocation @app {\n     proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n     proxy_set_header Host $http_host;\n     proxy_redirect off;\n     proxy_pass http://myserver;\n   }\n}\n``````\nBoth vhosts definitions are quite simple, i.e., they don't go any further than serving the app and if you go on production you may want your webserver to handle some stuff like some assets instead of your ruby server. Here's a better production ready example for nginx we use - it's adapted from a capistrano recipe.\nHope that helps :),\nCheers!\nDar\u00edo\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/125#issuecomment-7395581\n. \n",
    "codykrieger": "Thanks a lot Evan! Much appreciated.\n. ",
    "bnorton": "@evanphx Is there an update on this new mode?\n. I'm not seeing this happen. What version and are you using -e production when starting puma?\n. anybody have a chance to try this out?\n. All workers restarted just fine (saw the new pids for the worker processes) but puma did not have any output.\nAs of 2.2.2 this seems to be resolved.\n. The new thing that I have noticed is that the lib folder is not reloaded :frowning: \n. back to square 1 as puma reports\nbash\n$ puma --version\npuma version 2.2.2\npuma.log\n[21122] - Starting phased worker restart, phase: 1\n[21122] - Stopping 21126 for phased upgrade...\n[29551] + Changing to /app/releases/20130714234127\n[21122] - Worker 29551 booted, phase: 1\n[21122] - Stopping 21130 for phased upgrade...\n[29662] + Changing to /app/releases/20130714234127\n[21122] - Worker 29662 booted, phase: 1\nBut is not reloading routes nor code.....\n. finally getting somewhere, I noticed that the Changing to /app/releases/blah is NOT the same path that is linked to /app/current\n```\n$ ls -al\nlrwxrwxrwx  1 deploy deploy   30 Jul 15 16:20 current -> /app/releases/20130715161933\n$ ls releases\n20130713192226  20130714231056  20130714232511  20130714234127  20130715161933\n``\n. phased restart is supposed to reload the code though?\n. has anyone else verified what @such did?\n. For future reference the--dir` flag to puma MUST be set if the 'current directory' at the time of puma boot is going to be a symlinked directory. \nwhen booting puma (without the --dir flag) in a directory /app/current --> /app/releases/2013448394 then the cached value for the directory inside the puma primary process would be /app/releases/2013448394. \nSo for my config change was adding --dir /app/current\nSee: https://github.com/puma/puma/commit/40af41e5ef13c6e0c9343f4fe40bafca9441b80b#L0R212\n. ",
    "steverandy": "@bnorton on the latest beta you can use phased restart with USR1.\n. Any documentation about this feature and cluster mode?\n. This is the output, and it was just stuck there.\n* 2013-02-09 16:42:41 executing `puma:start'\n  * executing \"cd /home/admin/rails/current && bundle exec puma -d -e production -b 'unix:///home/admin/rails/shared/sockets/puma.sock' -S /home/admin/rails/shared/sockets/puma.state --control 'unix:///home/admin/rails/shared/sockets/pumactl.sock'\"\n    servers: [\"linobank.com\"]\n    [linobank.com] executing command\n ** [out :: linobank.com] Puma 2.0.0.b6 starting...\n ** [out :: linobank.com] \n ** [out :: linobank.com] * Min threads: 0, max threads: 16\n ** [out :: linobank.com] \n ** [out :: linobank.com] * Environment: production\n ** [out :: linobank.com] \n ** [out :: linobank.com] * Listening on unix:///home/admin/rails/shared/sockets/puma.sock\n ** [out :: linobank.com] \n ** [out :: linobank.com] * Starting status server on unix:///home/admin/rails/shared/sockets/pumactl.sock\n ** [out :: linobank.com]\n. However if I add this line >> #{current_path}/log/puma.log 2>&1 &, the deploy script will complete.\nBut puma having -d option to daemonize, that line should not be needed.\n. ",
    "1c7": "Hi, can anyone post a update info on this? (how to do hot reload with Puma)\nit's 2018, previous comment here is 2013\nI want 0 downtime deploy. @evanphx \nMaybe put a longer version tutorial, FAQ, common error section on http://puma.io/?\nhow to deploy puma with nginx, how to config them, stuff like that?\nactually these info in README.MD can be break into a lot little piece and put into  http://puma.io/\nGood example I think is Jekyll  https://jekyllrb.com/\ntheir document just feel better.\nThanks for you guys write puma. awesome software.\nopen source rock!\n:D\n. @dawidof solution work! Thanks!\nI found out that I set config.force_ssl to true and I forgot about it.\nso it just keep redirecting me to https. @shirkavand have you solve your problem?\nit's been 4 months now (2018,9~2019,1). @shirkavand \nI am on subway now. reply you later. Hi @shirkavand \n\nDo you have any insight of whether these new versions solve the problem or not?\n\nI am using Rails 5.2.2, Ruby 2.5.1\nI haven't try to reproduce what you describe. like @wjordan  did.  \nBut I have a story for you (it's about Docker) (it may help, not sure)\nAfter I type it all out, I realize it may be too long. \nif you don't want to read all of that, the point is just use Docker:\n0 downtime & limit memory & auto restart when crash happen\nDetail\nYou can write config in docker-compose.yml to limit memory usage, \nAnd add healthcheck to restart it automatically if somehow it crash.\nYou can think of Docker as poor man's Heroku haha.\nEven though Docker is much much more than that.\nDocker is free and even you are only 1 person+1 production machine. you should still use it.\nDocker is not big or difficult to learn. (Kubernetes is difficult, but not docker&docker swarm)\nStory begin:\nCouple months ago I was trying to do 0 downtime deployment\n(or blue/green deployment, same things)\nby the way I use mina, not cap\n. ## Try 1:  Puma Phased Restart \ud83d\ude1e \n\n\nMachine A: works perfectly\n       Mean that it restart without downtime in 5-15 seconds\n\n\nMachine B: Fail. (Machine B is production server)\n         Mean that each time puma restarts, it takes 100% CPU for 5-7 minute.\n\n\nBoth machine configs:\n\nUbuntu 18.04 (or 16.04, can't remember, it's been quite long)\n4G memory\n2 virtual CPU\n\nCloud Provider: UCloud\n(not AWS or Azure or Google Compute Platform)\nSure, It still successfully start-up after 5-7 minute. \nBut I just don't like it take this long. and cause 100% CPU\nI try to work out what's the difference between A and B.\n\nEnvironments variable? LANG=en_US.utf8\nPuma config file (Am I config the worker number right? preload_app? what's that?)\n\nI tried everything, everything I can think of.  Can't solve it.\nI just tire of this. I don't care. I just want it up and running. \n(Insert memes here)\nMission Impossible 6 meme: \"Why do you have to make things so fucking complicated?\"\n(Henry Cavill yelling to that bad guy)\n(Disclaminer: I still thanks people behind puma open source project, I am just mad at the problem itself. not the people behind the project)\nI ask for help. didn't work out.\nGithub issue here\nSo I just run 2 copy of Rails 5 App on the production server\n\nOne on port 9292\nOne on port 9696\n\nit didn't solve that 5-7 minute (100% CPU) start-up time.\nbut I just don't care, I have to move on.    \nAnd I use Nginx load balancer. config looks like this: \nupstream app_witt {\n  server 127.0.0.1:9292;\n  # server 127.0.0.1:9696;\n}\nYou can see # here, \nbecause I just switch 9292 and 9696 back and forth.\nManually change Nginx config and apply config withsudo nginx -s reload\nConclusion\nMina + Nginx + Puma\nIt works OK for a while. until I tire of it.\nit's just a stupid & temporary solution.\nthat 5-7 minute start-up time is still there\nEvery time I made some change to the codebase and want to deploy.\n\nssh to the server\nkill -9 [PID] and restart the other Rails app\nChange Nginx config, point to that Rails app. apply Nginx config\n\nIt is very manual and cost me at least 5 minutes.\nAfter doing that for 100+ time. I am just sick of it.  \n. ## Try 2: Passenger \ud83d\ude22 \nPassenger work OK. start-up fast. like 5 to 10 second.    \nGood, they have 0 downtime\n\nScreenshot link here\nBut, 0 downtimes require \"Passenger Enterprise\"\n\nScreenshot link here\nOK.. so... how much is that \"Passenger Enterprise\"?\n\n\n\nNow I get it.\n\"Passenger Enterprise\" mean let them handle everything.\nBut I have my cloud server! I don't want pay that much money. I want use my machine!\nSo I don't use 0 downtime provider by Passenger.\nBack to the old way: Nginx Load balancer\nConclusion:\nMina + Nginx + Passenger\nProblem\n\nStill not solving the real issue: Deployment is too manual, Very annoying\nNot a generic solution: Dependent on Passenger\nExpensive for solo dev working on side project: I rather use Heroku at that price point.\nNot enough time: not interesting in learning the passenger free edtion. I don't care these config. \n\nWhat's next?\n. ## Try 3: Docker  \ud83d\ude04 \n1. Write Dockerfile to pack my rails app into Docker Image\n2. Write docker-compose.yml for local dev\ndocker-compose.yml use Dockerfile mention in Step 1 to run Rails 5. and it spin up redis and sidekiq\n3. Write docker-stack.yml for deploy\n0 downtime for FREE!\n(You need write \"healthcheck\" to get this working)\nContainer Orchestration\nI am using Docker Swarm.  \nfor solo developer like me, Kubernetes is just too much for me at this point. \nI don't have 2-4 week to spend on learning Kubernetes\nNote\nI didn't provide example for Dockerfile, docker-compose.yml and docker-stack.yml.\nbecause this is about high-level view, not implementation detail.\nBut of course, there are still some problem\nLogging, Storage, Secret, Config, Metric Monitoring.  \nBut\nI am so much happy now. Docker is a generic solution. it didn't tie to Ruby, Rails, or puma/passenger.\nReverse Proxy.\nI try Traefik for a bit.\nBut give up, See this Github issue\nSo I still use Nginx.\nI freaking hate that TOML syntax\nConclusion:\nNginx + Docker Swarm\n. ## I hope what I wrote to provide some value \ud83d\udcb0 \nAnd not being seen as some guys on the internet wrote a long ranting.\nI spend 10:00 AM to 11:29 AM writing these.. \nI don't know what it took so long\nI should put these into my blog: 1c7.me for better SEO haha. export LANGUAGE=en_US.UTF-8\nexport LANG=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\ndoesn't work. A very very strange thing is, on server A it work perfectly, only take 10 second or so to restart.\nbut on server B no matter how I try (check utf-8 setting everywhere) it just keep taking 100% CPU.\n. ## Problem only happen in restart. only happen when code is update (push a new Rails code to server and restart puma). ## too many worker maybe?\n\n. ## My suspicion\nwhen restarting puma worker, it somehow didn't take encoding setting like\nexport LANGUAGE=en_US.UTF-8\nexport LANG=en_US.UTF-8\nexport LC_ALL=en_US.UTF-8\nso worker keep pooping invalid byte sequence in US-ASCII (ArgumentError)\n. ## Gemfile\n```ruby\nfrozen_string_literal: true\nsource \"https://rubygems.org\"\ngit_source(:github) { |repo_name| \"https://github.com/#{repo_name}.git\" }\ngem \"listen\"\ngem 'hashids'\ngem 'annotate'\ngem 'fast_jsonapi'\ngem 'devise-jwt', '~> 0.5.6'\ngem 'active_model_serializers'\ngem 'qiniu', '>= 6.8.0'\ngem 'aliyun-sms'   # https://github.com/VICTOR-LUO-F/aliyun-sms\ndeploy \u90e8\u7f72\ngem 'mina-puma', require: false, github: 'untitledkingdom/mina-puma'\ngem 'mina'\ngem \"jbuilder\", github: \"rails/jbuilder\"\ngem 'bootsnap', '~> 1.3', '>= 1.3.1'\ngem 'rails', '~> 5.2.0'\ngem 'puma', '~> 3.12'\ngem \"rails_autolink\"\ngem \"sanitize\" # https://github.com/rgrove/sanitize\nWhitelist-based Ruby HTML and CSS sanitizer.\nPostgreSQL\ngem \"pg\"\ngem \"pghero\" # https://github.com/ankane/pghero\nA performance dashboard for Postgres\ngem \"rack-attack\"\nhttps://github.com/kickstarter/rack-attack\ngem \"http_accept_language\" # https://github.com/iain/http_accept_language\nOAuth Provider\ngem \"doorkeeper\" # https://github.com/doorkeeper-gem/doorkeeper\ngem \"doorkeeper-i18n\"\ngem \"bulk_insert\" # https://github.com/jamis/bulk_insert\n\u4e0a\u4f20\u7ec4\u4ef6\ngem \"carrierwave\"\ngem \"carrierwave-aliyun\"\nLazy load\ngem \"mini_magick\", require: false\n\u7528\u6237\u7cfb\u7edf\ngem \"devise\"\ngem \"devise-encryptable\"\n\u901a\u77e5\u7cfb\u7edf\ngem \"notifications\" # https://github.com/rails-engine/notifications\ngem \"ruby-push-notifications\" # https://github.com/calonso/ruby-push-notifications\n\u8d5e\u3001\u5173\u6ce8\u3001\u6536\u85cf\u3001\u5c4f\u853d\u7b49\u529f\u80fd\u7684\u6570\u636e\u7ed3\u6784\ngem \"action-store\" # https://github.com/rails-engine/action-store\nhttps://ruby-china.org/topics/32262\ngem \"kaminari\"\n\u4e09\u65b9\u5e73\u53f0 OAuth \u9a8c\u8bc1\u767b\u9646\ngem \"omniauth\"\ngem \"omniauth-github\"\nPermission\ngem \"cancancan\" # https://github.com/CanCanCommunity/cancancan\nRedis\ngem \"hiredis\"\ngem \"redis\"\ngem \"redis-namespace\"\ngem \"redis-objects\"\nCache\ngem \"second_level_cache\"\nhttps://github.com/hooopo/second_level_cache\nSetting\ngem \"rails-settings-cached\"\ngem 'rails-settings-cached', '~> 0.7.0'\nhttps://github.com/huacnlee/rails-settings-cached\nhttps://github.com/ledermann/rails-settings\n\u961f\u5217\ngem \"sidekiq\"\ngem \"dalli\" # High performance memcached client for Ruby\nhttps://github.com/petergoldstein/dalli\nAPI cors\ngem \"rack-cors\", require: \"rack/cors\"\ngem \"rack-utf8_sanitizer\"\ngem \"exception-track\" # https://github.com/rails-engine/exception-track\ngem \"status-page\" # https://github.com/rails-engine/status-page\ngem \"bundler-audit\", require: false # https://github.com/rubysec/bundler-audit\ngroup :development do\n  gem \"derailed\" # https://github.com/schneems/derailed_benchmarks\n  # Better Errors\n  gem \"better_errors\"  # https://github.com/charliesome/better_errors\n  gem \"spring\"\n  gem \"spring-commands-rspec\"\nend\ngroup :development, :test do\n  gem \"capybara\"\n  gem \"database_cleaner\"\n  gem \"factory_bot_rails\"\n  gem \"letter_opener\"\ngem \"rspec-rails\"\n  gem \"rubocop\", \">= 0.49.0\", require: false\nend\n``. ## 2018-8-3 Update on this issue\nI give up debug this issue. \nonserver Ait work fine, onserver B` the problem just keep happening.\nI can't waste time on this anymore\nMy Goal: I just want 0 downtime deployment for my Ruby on Rails 5.2.0 project\nMy Solution\nuse Nginx load balancer point to different Puma.\nI run 2 Puma, one at :9292  one at: 9696\nI just wrote this in my nginx config file.\nupstream app_witt {\n  # server 127.0.0.1:9292;\n  server 127.0.0.1:9696;\n}\nand next time after I update the code\nupstream app_witt {\n  server 127.0.0.1:9292;\n  # server 127.0.0.1:9696;\n}\nand then\nsudo nginx -t\ncheck if nginx config file correct\nsudo nginx -s reload\ntell nginx reload config file. so our change to nginx config file would work after reload. I close this issue now. If you guys don't have time/energy to deal with this issue.\nUse Docker and a simple curl healthcheck. so Docker would restart it for you.\n(I am not the maintainer or a contributor of this project, I am just some guy pass by trying to help)\nI recently learn docker and adopt it into my workflow.\nfor me(1 person & 1 production machine)\nit works perfectly. save me a lot of time on deploy & maintain environment . ### How big is our codebase?\n60 model, 40 controller or so. still a fairly small project\nHow many gem in Gemfile?\n```ruby\nfrozen_string_literal: true\nsource \"https://rubygems.org\"\nruby \"2.5.1\"\ngem \"mail\"\ngem \"skylight\"\ngem 'bcrypt'\ngem \"rails-settings-cached\"\ngem \"syslogger\", \"~> 1.6.0\"\ngem \"lograge\", \"~> 0.3.1\"\nhttps://github.com/roidrage/lograge\ngit_source(:github) { |repo_name| \"https://github.com/#{repo_name}.git\" }\ngem \"whenever\", require: false\ngem \"httparty\"\nSentry Error Tracking\nhttps://sentry.io/for/rails/\ngem \"sentry-raven\"\nBlockchain (Nebulas)\ngem 'bitcoin-secp256k1', '~> 0.4.0' # gem 'neb' \u4f9d\u8d56\u7684\u5e93 # https://rubygems.org/gems/bitcoin-secp256k1\ngem 'neb', '0.1.3'\ngem \"hashids\"\ngem \"fast_jsonapi\"\ngem \"devise-jwt\", \"~> 0.5.6\"\ngem \"active_model_serializers\"\ngem \"qiniu\", \">= 6.8.0\"\ngem \"aliyun-sms\"   # \u77ed\u4fe1\u53d1\u9001 https://github.com/VICTOR-LUO-F/aliyun-sms\ngem \"mina\", require: false\ngem 'bootsnap', '~> 1.3', '>= 1.3.2', require: false\ngem 'rails', '~> 5.2', '>= 5.2.2'\ngem \"puma\", \"~> 3.12\"\ngem \"pg\"\ngem \"pghero\" # https://github.com/ankane/pghero # A performance dashboard for Postgres\ngem \"rack-attack\"\nhttps://github.com/kickstarter/rack-attack\ngem \"http_accept_language\" # https://github.com/iain/http_accept_language\nOAuth Provider\ngem \"doorkeeper\" # https://github.com/doorkeeper-gem/doorkeeper\ngem \"doorkeeper-i18n\"\ngem \"bulk_insert\" # https://github.com/jamis/bulk_insert\ngem \"carrierwave\"\ngem \"carrierwave-aliyun\"\nLazy load\ngem \"mini_magick\", require: false\ngem \"devise\"\ngem \"devise-encryptable\"\ngem \"notifications\" # https://github.com/rails-engine/notifications\nCache\ngem \"second_level_cache\"\nhttps://github.com/hooopo/second_level_cache\ngem \"action-store\" # https://github.com/rails-engine/action-store\nhttps://ruby-china.org/topics/32262\ngem \"kaminari\"\ngem \"omniauth\"\ngem \"omniauth-github\"\nPermission\ngem \"cancancan\" # https://github.com/CanCanCommunity/cancancan\nRedis\ngem \"hiredis\"\ngem \"redis\"\ngem \"redis-namespace\"\ngem \"redis-objects\"\ngem 'redis-rails', '~> 5.0', '>= 5.0.2' # https://github.com/redis-store/redis-rails\nSetting\ngem \"rails-settings-cached\"\ngem 'rails-settings-cached', '0.7.0'\nhttps://github.com/huacnlee/rails-settings-cached\nhttps://github.com/ledermann/rails-settings\n\u961f\u5217\ngem 'sidekiq', '~> 5.2', '>= 5.2.5'\ngem \"dalli\" # High performance memcached client for Ruby\nhttps://github.com/petergoldstein/dalli\nAPI cors\ngem \"rack-cors\", require: \"rack/cors\"\ngem \"rack-utf8_sanitizer\"\ngem \"bundler-audit\", require: false # https://github.com/rubysec/bundler-audit\ngroup :development do\n  gem \"derailed\" # https://github.com/schneems/derailed_benchmarks\n  gem \"better_errors\"  # https://github.com/charliesome/better_errors\n  gem \"spring\"\n  gem \"spring-commands-rspec\"\n  gem \"faker\"\n  gem \"binding_of_caller\"\n  gem \"annotate\"\n  gem \"listen\"\nend\ngroup :development, :test do\n  gem \"capybara\"\n  gem \"database_cleaner\"\n  gem \"factory_bot_rails\"\n  gem \"letter_opener\"\ngem \"rspec-rails\"\n  # gem \"rubocop\", \">= 0.49.0\", require: false\nend\n```. I want to solve this myself,\nbut I just don't know where to start.\nwhat config I should fine tune.\nwhich component eat up a lot of time etc.\nIf anyone can provide some insight that would be very helpful\nThanks you in advance. Ahh.. Still don't know how to fix\nconsidering change to Unicorn / Passanger.\n. A lot of Github Issue have no reply at all. \nSeem like no one maintaining puma? . One example: \nI haven't build CI/CD system yet. (would get it done soon)\n(and I haven't write any test, I know it's bad..)\nsometime a bug happen in Production and I can fix it in 20 second.\nbut boot up puma alone take 4-5 minute. @evanphx oh my native language is not English.\nso if there are tone problem I am sorry, I didn't mean it.\nAnd Thanks for the suggestion!. Let me try Unicorn / Passenger see if problem still exist.\nIf so. there must be something wrong in my code.\nI not. probably because wrong server config / wrong puma config / both / some other weird thing\nThank again. would close this issue within a week. Remove bootsnap and puma\nAdd passenger, somehow seem working. ( take only couple second to start )\nDoing further investigate. maybe somehow bootsnap case problem?\nUpdate: 2019-2-15\nbecause I use Docker now. the problem goes away.\nI suspect that when I am not using Docker. the problem come from my server.\nprobably environment variable wrong or some config wrong or my code wrong. who knows\nanyway. now the problem gone.\n. ",
    "steveklabnik": "This shit was crazy complicated. :/\n. @hlegius it's been a while, and it seems like this issue is no longer an issue. If you can give me a way to reproduce, I'll re-open this, but for now, I'm giving it a close.\n. This is just a PSA, but you can use :github => \"puma/puma\" if you're on Bundler 1.2. :)\n. What is HTTP_X_FORWARDED_PROTO?\n(and wow, what a terrible name)\n. ",
    "rafaelss": "after almost a whole day trying to figure this out, I got this working.\nnewrelic gem doesn't recognize puma as a dispatcher, so you need to configure it manually. you can do that by adding dispatcher: puma to newrelic.yml or setting NEWRELIC_DISPATCHER env variable, like NEWRELIC_DISPATCHER=puma rails s puma.\n. weird. I can't reproduce this anymore. I'll keep using beta1 to see if the issue shows up again.\n. ",
    "kostia": "In my case the environment variable helped. But I had to add following initializer config/initializers/newrelic.rb:\nruby\nif defined?(NewRelic) && defined?(Puma)\n  NewRelic::Agent.after_fork force_reconnect: true\nend\nI also had to put gem 'newrelic_rpm'outside of the group :production section, for otherwise the assets wont compile...\n. @davidray\n1. Rails-Version?\n2. RPM-Version?\n3. Puma-Version?\nPlease try following:\n1. Add NEWRELIC_DISPATCHER=puma to /etc/environment\n2. Add the above Initializer\n3. Move gem 'newrelic_rpm outside of the group production\n4. Add dispatcher: 'puma' to config/newrelic.yml\n5. Restart, reload everything...\n. ",
    "davidray": "I can't get this to work with any of these approaches\n. #1 and #4 fixed it - I already had the other two things in place. I did #1 a bit differently though - I set the value in my command to start up puma (we use foreman to manage it) and it's all fixed. Thanks!\n. ",
    "jpascal": "+1 NewRelic work with NEWRELIC_DISPATCHER=puma.\n. Hello again! =)\nWhy you use HTTP request to control server? \nMay be perfect to use signals to control process SIGTERM/SIGUSR2?\n. HOE always gathers the files are not needed for the performance bundle.\nSuchshe would be used for this command 'git ls-files' for it?\n. No problem. I will add this feature.\n. Done! =) \n. I am waiting too.\n. evanphx: where are you?\n. ",
    "davidcelis": "Even with the ENV variable or dispatcher: key in my newrelic.yml file, as well as the after_fork block in an initializer, I can't get the newrelic_rpm agent to report data. Any status on this?\n. @rengland-newrelic I've tried all \"solutions\" listed in this thread: the environment variable, setting it manually in the start-up script, including dispatcher: puma in newrelic.yml. None of them have worked.\nEDIT: false alarm. it looks like setting the environment variable in my startup script did work. I'm reporting data. Cool.\n. @concept47 based on that stack trace, it's likely a bug in the Ruby agent rather than in puma. We're taking a look.\n. @evanphx Sorry, but is there a reason that bundling from git doesn't respect Java vs. C extensions? That seems like a bug (albeit probably with bundler)\n. For posterity: bundler/bundler#3127\n. @evanphx They directed me back here (or to RubyGems) from that issue, but upon further inspection it looks like the problem is with how the puma repository is set up. The gemspec file that is checked in (built via a Rake task) specifies C extensions, so bundling from git causes JRuby to attempt to build those. Perhaps there's a way we could change that generated gemspec to better support both platforms? How is the gem currently built for JRuby?\n. @narutosanjiv Yup. I'm trying to figure out how the JRuby version of Puma is built, since cloning the repository and running gem build puma.gemspec && gem install puma.gem under JRuby attempts to build C extensions.\n. Ruby 2.0.0-p195, Rails 4.0.0.rc1 with Rails-API, and Ubuntu 13.04. Same issue.\n. ",
    "rengland-newrelic": "There has been a report that manually including NEWRELIC_DISPATCHER=puma in the actual start-up script of puma as opposed to an environment variable was required, in one instance\n. ",
    "tscolari": "I got this error message when using the above initializer:\nPlugin not initialized\nI was trying to deploy with capistrano (it was from running assets precompile rake task)\n. ",
    "ehlertij": "Adding ENV['NEWRELIC_DISPATCHER'] = 'puma' to my config/boot.rb and dispatcher: 'puma' to config/newrelic.yml did the trick for me.\nRails 3.2.8, Puma 1.6.3\n. The 2.0.0.b1 version does fix the issue. Thanks!\n. ",
    "concept47": "This doesn't work out the box for me with puma 2.0.0.b4, rails 3.2.11 and ruby 2.0.0.rc1 \nWorks with Ruby 1.9.3 but not Ruby 2.0.0.\nHere's the stack trace \nhttps://gist.github.com/4592477\n. @davidcelis ... you're absolutely right, I ran it by newrelic and its an issue with ruby 2.0.0 and their newrelic agent\n. I can confirm it works now.\nFrickin awesome!!!\n. I'm having the same problem as @pawel2105 \nRuby 2.0.0 p247, rails 3.2.12, puma 2.3.2\nnew relic log looks like this\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Environment: production\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Dispatcher: puma\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Application: xxxxx\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Installing Net instrumentation\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Installing ActiveRecord instrumentation\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Installing Rails 3 Controller instrumentation\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Installing Rails 3.1/3.2 view instrumentation\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Installing Rails3 Error instrumentation\n[07/13/13 22:15:29 +0000 xxx (27173)] INFO : Finished instrumentation\n[07/13/13 22:15:30 +0000 xxx (27173)] INFO : Reporting to: https://rpm.newrelic.com/accounts/xxxx/applications/xxxx\nbut I'm getting no data to newrelic whatsoever\nWhen I downgrade to puma 2.0.1, it works just fine.\n. you might have to specify 1 worker in the puma startup command, to get it to work\n. This is still happening btw ... and I'm using the latest from master\nusing\nbundle exec puma -d -e production -S tmp/status.txt -b unix:///path_to_my_app/tmp/puma.sock\n. Here is the schedule.rb I'm running from my config/initializers directory (I've snipped out most of the jobs except a printout of \"I\"VE Started\" that I'm using to test this. Its the same one I've used for years, and it works just fine when puma isn't daemonized.\nI also think it might be related to this mongrel bug with rufus scheduler https://groups.google.com/forum/?fromgroups=#!topic/rufus-ruby/8ToQAH8-Rv0\n```\nrequire 'rufus/scheduler'\ncheck for console or rake before proceding, this stops all of the scheduling from running when we're at the console or running a rake task\nif !defined?(IRB) && !defined?(Rake)\n   #setup ---------------------------------------------------------------\n    scheduler = Rufus::Scheduler::PlainScheduler.start_new\nlogfile = File.open(Rails.root.join('log', 'scheduler.log'), 'a')\nlogfile.sync = true\n$schedule_logger = Logger.new(logfile, 2, 1024000)\n$schedule_logger.info \"------------------------------------------------------------------------\"\n$schedule_logger.info \"------------------------------------------------------------------------\"\n$schedule_logger.info \"[Scheduler] started at #{Time.now.strftime('%B %d, %Y at %I:%M %p')}\\n\\n\"\n\ndef scheduler.handle_exception (job, exception)\n    $schedule_logger.error \"------------------------------------------------------------------------\"\n    $schedule_logger.error \"[Scheduler] Exception at #{Time.now.strftime('%B %d, %Y at %I:%M %p')}\"\n    $schedule_logger.error \"job #{job.job_id} running every #{job.t} caught exception\"\n    $schedule_logger.error \"'#{exception}'\"\n    $schedule_logger.error \"#{exception.backtrace.join(\"\\n\")}\\n\\n\"\n\nend\nschedules ------------------------------------------------------------\nFOR DEBUGGING STUFF\nscheduler.in '5s' do ## same logic here, put it as far away from start as we can\n    $schedule_logger.info \"I STARTED!!! #{Time.now.strftime('%B %d, %Y at %I:%M %p')}\"\nend\n\n```\n\"[Scheduler] started at ... \"  never prints out to the log\n. Hey Evan, is there a way I can try this out?\n. IT WORKS!!!! WOOHOOOO!!!!\n. Hi Evan, \nSomehow this stopped working in 2.2.1+ :\\\n. Sorry about that. it was fixed in 2.3.2 so not need.\nThanks Evan!\n. In my case the server simply doesn't restart and when I go to start it again, I get the error above\n. No output, it actually looks like it worked but puma never comes back up.\n. restarts just fine when not daemonized :\\\n. Works for me now using Puma 2.0.0.b7, ruby 1.9.3-p374, \n. @evanphx I think you should document that or be clearer in the error message. \nStats not available via pid only\nis confusing at best :\\\n. I'm also running rufus scheduler (2.0.19), which I don't think should cause any problems, but I just wanted to mention it just in case\n. @tecnobrat I'm so glad I'm not the only one, I've been going crazy over this thinking I was hallucinating or something. Please post back with your findings whenever you get a second\n. so it turns out that running rufus-scheduler in the foreground (inside puma) with :blocking => true was what was causing the outages. It basically forces scheduler jobs to run in a single thread queueing up pending jobs unti the currently executing job is done. I guess the \"pileup\" of jobs was causing the outages somehow.\nI removed the blocking setting so that all the jobs would execute in their own threads and weirdly enough, that made the problem way worse. Outages went from once every one or two days to a couple of times a day. \nSo I wound up just making rufus scheduler only execute the jobs in sidekiq (background processing) and that seems to have stopped the outages completely. \nI'm really not sure why the rufus scheduler jobs would be causing puma to be unresponsive in that manner especially since it works in a single thread all on its own, and the site is pretty low traffic, but everything seems back to normal. \nlet me know if you need any more info, if not, feel free to close this out @evanphx.\nThanks for your hard work on puma, trying to find an alternative with passenger and then thin made meappreciate how awesome, and easy to use puma is.\n. > @concept47 How have you run rufus-scheduler with other servers? In the same way?\nyup, I ran it in threaded mode with thin. And it was the same exact thing, thats how I figured the problem was probably not with puma.\n\nI'm wondering if it's coded in such a way that it's holding the GIL and starving the process....\n\nThat sounds plausible. what I wasn't able to do is to run it with jruby to see if the same problem surfaced there\n. Interesting. In my case rufus scheduler connects to a site to do some screen scraping, so connections timing out are not a rare occurrence, could that mean ruby sits there waiting in a situation like that. I thought this could be a problem, but I'm using ruby 2.1.0 and my understanding is that ruby threads shouldn't block on i/o.\n. ",
    "aaronchi": "No longer working under ruby 2.0.0\n. I have issues serving assets with 2.0 beta but not 1.6.3\n. ",
    "mpoisot": "Looks like New Relic Agent 3.5.8.72 added support for Ruby 2.0.0. Agent 3.6.0.78 added support for \"thread safety\" and specifically mentions Puma. I'm still on Ruby 1.9.3 so I can't confirm anything.\nUnfortunately looks like I still have to set NEWRELIC_DISPATCHER=puma.\nhttps://newrelic.com/docs/releases/ruby\n. ",
    "samg": "I wanted to let you all know I've just pushed a beta version of newrelic_rpm which should work in puma without needing to set NEWRELIC_DISPATCHER variables or other shenanigans.\nMore specifically, I've simplified the logic that controls when the agent attempts to start, and switched to blacklisting environments (e.g. rails console, irb, certain rake tasks) instead of trying to detect if the agent is in a \"supported environment\".\nI'd be very interested in getting some feedback on whether this works for your puma apps and simplifies their deployment.\nhttps://rubygems.org/gems/newrelic_rpm/versions/3.6.1.85.beta\nThanks!\n. @jjb - thanks for the update.  I sounds like that's the intended behavior.  The recent change I described makes it so the agent will start up even if it doesn't detect a \"known dispatcher\" as long as it's in a monitored environment.  It will still log the message you described.  I haven't added any specific code to detect puma.  I've just made it so we don't need to detect every web server out there in order to start.\nThanks!\n. ",
    "jjb": "@samg it's working for my app when running in a webserver, but seems to not be detected during asset compilation when deploying to heroku. \"No known dispatcher detected\"\n. @ctshryock okay great, closing this then\n. cool. well, it's probably just one sentence and you can probably write it better than me, so go for it :D \n. ruby 2 is the default on heroku and is growing in popularity. also i think preloading has some other benefits.\n. great thanks.\n\nPuma is always multi-thread, cluster mode gives you the ability to have multi-process, multi-thread. You could configure puma's thread pooling to have just 1 thread if you'd like, but there is no reason to do that.\n\nYes there is -- if my app isn't threadsafe, and I want to use puma instead of unicorn for multi-worker. --threads 0:1 --workers 4 :-D\nquestion: is the preload_app! option the same as using --preload?\n. If the code is loaded into memory after the fork, then the fork doesn't\nhave the opportunity to reuse any memory object, because they don't exist\nyet.\nOn Thu, Oct 17, 2013 at 5:19 PM, Clint notifications@github.com wrote:\n\nI don't agree that this is necessary. Can you or @evanphxhttps://github.com/evanphxelaborate your reasoning / thoughts here? It's my understanding that\n--preload helps with loading the app before workers are forked, but the\nCoW benefits are inherit of the fork call regardless.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/pull/387#issuecomment-26554013\n.\n. I'll investigate further -- sorry for the noise\n\nOn Tue, Nov 5, 2013 at 12:22 AM, Evan Phoenix notifications@github.comwrote:\n\nClosed #405 https://github.com/puma/puma/issues/405.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/405\n.\n. @evanphx true-- but before that, look at the quote in my OP above -- is that sentence correct?\n. ahhh yes, that's right. although i've never seen anyone give an explanation/example of when those native threads can possibly be better than 1.8 threads, since they can never run in parallel. (i think maybe it's for a very small handful of scenarios when a C extension is using certain system calls).\n\nin fact performance might even be worse because L2 cache won't be shared between threads on different CPUs.\n. There's probably no benefit for clustered mode on a machine with 1 core.\nOn Mon, Feb 3, 2014 at 9:30 AM, Mathieu Allaire notifications@github.comwrote:\n\n@evanph so that means that even with Ruby MRI 2.1.0, we should put the\nnumber of workers == to the nomber of cores?\nAlso, I know that the cluster mode is enabled when specifying the workersoption. Currently, I set it to\n1 since my VPS only has 1 core, do I get any benefits or workers need to\nbe >= 2 to get all the advantages of the clustered mode?\nI would really appreciate some insight [image: :+1:]\nThank you!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/410#issuecomment-33958633\n.\n. I wasn't aware of preload offering a benefit to restarts... any docs on\nthis?\n\nOn Mon, Feb 3, 2014 at 10:18 AM, Mathieu Allaire\nnotifications@github.comwrote:\n\n@jjb https://github.com/jjb But then i guess I can have the preload_app\nand to have phased restart\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/410#issuecomment-33964145\n.\n. #480\n. @evanphx sure, but i don't quite understand the goal. it already has a distinct string, \"Puma caught this error\". your suggested string doesn't mention that the log line is coming from puma -- is that intentional?\n. @evanphx everyone who encounters the maytag message is going to google it and then find this issue thread. If you don't want to put the error message into the output, perhaps explain better what is going on and tell the user what they can do to get more detail?\n\nOr at the very least link to documentation of \"support for a user defined low-level exception handler\" from this thread... :)\n. In this Heroku article it describes the setting as such:\n\n'reaping_frequency' is telling Active Record to check to see if connections are hung or dead every 10 seconds.\n\nSo that would suggest that it somehow intelligently detects hung or dead connections and doesn't disconnect all connections.\n. Rails docs confirm this:\n\nRemoves dead connections from the pool. A dead connection can occur if a programmer forgets to close a connection at the end of a thread or a thread dies unexpectedly.\n. > Are you experiencing errors in your logs indicating requests aren't begin fulfilled because of no database connection? Are you doing load testing, such that you know all 64 threads are actively hitting the database? Are you using Clustered mode in Puma?\n\nNo to all of those.\nI'm not experiencing any problems, but the stats don't correlate with what I expected to be the behavior. Note that I set my minimum and maximum threads to be the same thing. So, I expect all of the threads to spool up and create a database connection upon server boot, and stay like that. Is that not the expected behavior?\n. to be clear, i did not observe threads idling. i was only wondering if they hadn't spooled up, based on the db connection stats.\n\nThe connections are not eager loaded\n\nThis might be part of the answer -- I assumed that they were indeed eager loaded.\nHowever, if that's the answer, it would also mean that even when all the puma threads are eager loaded, some of them are still never used unless they are needed. I would be surprised if puma somehow went out of its way to do this (I suppose it might do so using the same logic used to determine whether to create a thread in the N:M case where N<M). There's no benefit I can think of, and in fact might make for worse performance.\n@evanphx: in the N:N threads case, does puma distribute requests evenly amongst the N threads, regardless of load?\n. @evanphx done https://github.com/puma/puma/pull/686/\n. @brianr of Rollbar recommends adding request data:\nruby\nRollbar.scope(request: request_data_here).critical(e)\n@evanphx in the context of the handler, how would we grab the request info from the Rack environment?\n. if anyone finds this thread-- here's a more complete solution i came up with for Rollbar\n```ruby\nclass PumaRollbarTools\n  include Rollbar::RequestDataExtractor\nend\nlowlevel_error_handler do |error, env|\n  Rollbar.scope(request: PumaRollbarTools.new.extract_request_data_from_rack(env)).critical(error)\n  [ 500, {},\n    [ \"An error has occurred, and our engineers have been informed. Please reload the page. If you continue to have problems, contact support@example.com\\n\" ]\n  ]\nend\n``\n. :disappointed: \n. :cool: \n. @Taytay I'd love to hear what your overall solution to this was / currently is.. I created a method for logging these stats to stdout on heroku, would love anyone's feedback! https://github.com/puma/puma/issues/1512. JRuby will take much longer to boot up and so the first request will seem to take much longer and affect the rest of the test. Maybe add acurl http://localhost:9292/hellobefore each invocation ofwrk`, to normalize the results a bit.\n. @ThomasCelen that looks like pretty typical behavior of a rails app\nthe thrashing around and then slow growth could be from a lot of things\nto get a more focused look at the \"real\" memory growth, try this:\n- set the worker count to 1 (you may of course need to scale the # of dynos accordingly)\n- heroku config:set RUBY_GC_HEAP_GROWTH_FACTOR=1.10 (reference: https://devcenter.heroku.com/articles/ruby-memory-use#gc-tuning)\nleave that running for a day or two, the graph should be more informative.\nand may as well update to ruby 2.3.1, if possible.\n. (and also perhaps remove PumaWorkerKiller)\n. done!\n. See discussion about rails 5.2 here, in my large unmerged \ud83d\ude2d rails guide https://github.com/rails/rails/pull/29807. @jrafanie thank you for your thoughts and info!\nTo clarify: I was not trying to spin down threads in order to release db connections. I was noticing that I was leaking connections, and when I stopped allowing threads to spin down, the problem was fixed.\nSo my vague suspicion is that the threads \"aren't really\" being spun down, so AR is still holding onto them. Meanwhile, new ones are spun up. Eventually, there are no more available connections in the pool because the old, unused threads are holding onto them all and not doing anything with them.\n. @jrafanie @gjastrab thanks for your info. at this point my problem is completely solved by pinning the thread number, so I won't be able to invest any time in exploring this in the near future. \n\nso that there is more overhead above the 2 * 6 + 3\n\nyou have your math wrong, the pool is per-puma-worker and per-sidekiq-instance. so for rails, the pool should be 6 or more and for sidekiq the pool should be 3 or more.\n\nwhen you notice the leaking happening would you see the workers logging ! Detected parent died, dying ?\n\nno, this was never in my logs\n. @jrafanie are you addressing me, @gjastrab, or either? :)\n. In my case, if I allow threads to go up and down, then I eventually always get the \"active record can't find an available db connection in the pool\" error. this is regardless of pool size. typical config is 16 threads, 20 db connection pool size. \n. > The thread pool of your database needs to be at least as long as the number of threads puma is configured to use. Additionally, you need to be sure that the connections are returned to the pool when a request finishes (rails handles that for you but older versions have had bugs where connections were not returned).\nBoth of these things were the case when I was experiencing the problem.\n\nAlso consider turning on clean_thread_locals in your puma config, that can help because it will make sure that Thread locals (where database connections are sometimes stored) are properly cleaned up.\n\nI'd never heard of this, I'll try it next time I have the problem. Seems very magical and useful! How long has it been around?\n. @nateberkopec no, why?. When this bug occurs does it leave puma in a bad state or does it only affect one http request?. ruby\nplugin :heroku. Okay, commenting that out fixed the problem :-D \ud83e\udd26\u200d\u2642\ufe0f . Reopening this because I'm realizing there's a larger question. rails s or rails s Puma ignores config in puma.rb. Is this by design? Unavoidable because of how Rails interacts with servers? Is there a common workaround? Am I doing something wrong?\nHere's my puma.rb\nruby\nbind \"tcp://0.0.0.0:8080\"\nrails starting:\nplatform:/vagrant$ bundle exec rails s Puma\n=> Booting Puma\n=> Rails 5.0.2 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.7.1 (ruby 2.4.0-p0), codename: Snowy Sagebrush\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://localhost:3000\nUse Ctrl-C to stop. Okay, I found discussion here: https://github.com/puma/puma/issues/939#issuecomment-209000933\nclosing again.... More info if anyone finds this ticket with a similar situation. My final setup was actually quite simple. I kept my puma.rb as:\nruby\nplugin :heroku\nI've also been using dotenv-rails. in Gemfile\nruby\ngem 'dotenv-rails', groups: [:development, :test]\nI removed the code in my boot.rb shown above, and added this to my .env:\nPORT=8080\n\ud83c\udf89 everything works now. related: #1277 #1290. @nateberkopec thanks for trying, I will attempt to make a minimal reproduction. What does -X P do? Send an unhandleable request? curl manpage doesn't seem to explain P... thanks!. When I remove the heroku plugin, behavior returns to normal. I think maybe a bug was introduced in 3.8 which changed how plugins share config with the rest of the config. Here are some possible clues.\nI put in these log lines (all the ps)\nlib/puma/dsl.rb\n```ruby\nmodule Puma\n  class DSL\n    def initialize(options, config)\np \">>> initializing with #{options.object_id}\"\n...\ndef lowlevel_error_handler(obj=nil, &block)\n\np \">>> setting lowlevel error handler on #{@options.object_id}\"\n...\n```\nlib/puma/configuration.rb\n```ruby\nmodule Puma\n  class UserFileDefaultOptions\n    def initialize(user_options, default_options)\np \">>> initializing new UserFileDefaultOptions\"\n...\ndef []=(key, value)\n\np \">>> setting #{key}=#{value}\"\n...\n```\nlib/puma/server.rb\nruby\nmodule Puma\n  class Server\n    def lowlevel_error(e, env)\n      if handler = @options[:lowlevel_error_handler]\np \">>> there is a handler\"\n...\n      end\np \">>> there is not a handler.\"\n...\nHere's the output when I start my server:\n$ rails s\n=> Booting Puma\n=> Rails 5.1.0 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\n\">>> initializing new UserFileDefaultOptions\"\n\">>> initializing with 23033980\"\n\">>> initializing with 36279040\"\n\">>> initializing with 36279260\"\nstarting puma config\n\">>> setting lowlevel error handler on 36279040\"\nending puma config\n\">>> setting environment=development\"\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.4.1-p111), codename: Sassy Salamander\n* Min threads: 1, max threads: 1\n* Environment: development\n* Listening on tcp://0.0.0.0:8080\nUse Ctrl-C to stop\nnote that\n\nPuma::DSL is initialized 3 times\nlowlevel error handler is not set on the most recent one\nthe \"setting key=val\" message is never printed for lowlevel_error_handler\n\nhere's the output when i make a request which raises a lowlevel error:\n```\n2017-06-01 02:53:11 +0000: Rack app error handling request { GET / }\n\nconfig.ru:23:in call'\n/var/lib/gems/2.4.0/gems/puma-3.8.2/lib/puma/configuration.rb:226:incall'\n/var/lib/gems/2.4.0/gems/puma-3.8.2/lib/puma/server.rb:600:in handle_request'\n/var/lib/gems/2.4.0/gems/puma-3.8.2/lib/puma/server.rb:435:inprocess_client'\n/var/lib/gems/2.4.0/gems/puma-3.8.2/lib/puma/server.rb:299:in block in run'\n/var/lib/gems/2.4.0/gems/puma-3.8.2/lib/puma/thread_pool.rb:120:inblock in spawn_thread'\n\">>> there is not a handler.\"\n```\nNote that \"there is not a handler\" is printed.\nI didn't do an analogous set of print statements for 3.7. If the above isn't enough to make you know what the problem is, then I can work on making a standalone rails app demonstrating the problem.\nThanks!. Let me know if the info above isn't sufficient to make a good guess about what happened, then I'll try to make a minimal reproduction.. Still broken on 3.10.0\nWith this config.ru\n```ruby\nrequire_relative 'config/environment'\nclass LowLevelTester\n  def initialize(app)\n    @app = app\n  end\ndef call(env)\n    raise \"low level error\"\n@app.call(env)\n\nend\nend\nuse LowLevelTester\nrun Rails.application\n```\nand this puma.rb\n(puma-heroku plugin is gem \"puma-heroku\", github: \"jjb/puma-heroku\", ref: \"78c14f4\")\n```ruby\nputs \"starting puma config\"\nplugin :heroku\nclass PumaRollbarTools\n  include Rollbar::RequestDataExtractor\nend\nlowlevel_error_handler do |error, env|\n  puts \"lowlevel_error_handler is being invoked\"\n  Rollbar.scope(request: PumaRollbarTools.new.extract_request_data_from_rack(env)).critical(error)\n  [ 500, {},\n    [ \"An error has occurred, and our engineers have been informed. Please reload the page. If you continue to have problems, contact support@example.com\\n\" ]\n  ]\nend\nputs \"ending puma config\"\n```\n3.7.1\n``\n$ rails s\n=> Booting Puma\n=> Rails 5.1.4 application starting in development \n=> Runrails server -h` for more startup options\nstarting puma config\nending puma config\nPuma starting in single mode...\n Version 3.7.1 (ruby 2.4.1-p111), codename: Snowy Sagebrush\n Min threads: 1, max threads: 1\n Environment: development\n Listening on tcp://0.0.0.0:8080\nUse Ctrl-C to stop\n2017-09-20 21:22:43 +0000: Rack app error handling request { GET / }\n\nconfig.ru:11:in call'\n/var/lib/gems/2.4.0/gems/puma-3.7.1/lib/puma/configuration.rb:232:incall'\n/var/lib/gems/2.4.0/gems/puma-3.7.1/lib/puma/server.rb:578:in handle_request'\n/var/lib/gems/2.4.0/gems/puma-3.7.1/lib/puma/server.rb:415:inprocess_client'\n/var/lib/gems/2.4.0/gems/puma-3.7.1/lib/puma/server.rb:275:in block in run'\n/var/lib/gems/2.4.0/gems/puma-3.7.1/lib/puma/thread_pool.rb:120:inblock in spawn_thread'\nlowlevel_error_handler is being invoked\n```\n\u2794 curl http://platform:8080/\nAn error has occurred, and our engineers have been informed. Please reload the page. If you continue to have problems, contact support@example.com\n3.10.0\n``\n$ rails s\n=> Booting Puma\n=> Rails 5.1.4 application starting in development \n=> Runrails server -h` for more startup options\nstarting puma config\nending puma config\nPuma starting in single mode...\n Version 3.10.0 (ruby 2.4.1-p111), codename: Russell's Teapot\n Min threads: 1, max threads: 1\n Environment: development\n Listening on tcp://0.0.0.0:8080\nUse Ctrl-C to stop\n2017-09-20 21:28:17 +0000: Rack app error handling request { GET / }\n\nconfig.ru:11:in call'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/configuration.rb:225:incall'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/server.rb:605:in handle_request'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/server.rb:437:inprocess_client'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/server.rb:301:in block in run'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/thread_pool.rb:120:inblock in spawn_thread'\n```\n\u2794 curl http://platform:8080/\nPuma caught this error: low level error (RuntimeError)\nconfig.ru:11:in `call'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/configuration.rb:225:in `call'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/server.rb:605:in `handle_request'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/server.rb:437:in `process_client'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/server.rb:301:in `block in run'\n/var/lib/gems/2.4.0/gems/puma-3.10.0/lib/puma/thread_pool.rb:120:in `block in spawn_thread'%. @nateberkopec \ud83c\udf89 I've made a minimal reproduction repo: https://github.com/jjb/reproduce-puma-1287\nsee the readme for how to set up and reproduce.. @nateberkopec maybe whoever deals with the code regarding plugins could look at this?. This bug is still happening on 3.11.0. I've updated my repo to reflect this.. @evanphx thanks for taking a look at my puma-heroku PRs! Maybe you could also take a look at this issue. It makes lowlevel_error_handler (and maybe other/all config?) not work when using a plugin. I have a recreation repo linked above.. Alright, great news. I figured out how to access the puma stats without running the control app and invoking pumactl.\nIt's simply this:\napp/lib/puma_stats_logger.rb\n```ruby\nclass PumaStatsLogger\ndef self.run\n    launchers = []\n    ObjectSpace.each_object Puma::Launcher do |l|\n      launchers << l\n    end\n    raise unless launchers.one?\n    @launcher = launchers.first\nsleep 10 # wait for puma to boot\n\nThread.new do\n  loop do\n    begin\n      stats = JSON.parse @launcher.stats, symbolize_names: true\n      stats[:worker_status].each do |worker|\n        puts \"puma worker threads - pid: #{worker[:pid]} #{worker[:last_status]}\"\n      end\n      STDOUT.flush\n    rescue => e\n      Rollbar.error e\n    end\n\n    sleep 60\n  end\nend\n\nend\nend\n```\ninteresting part of puma.rb\nruby\nbefore_fork do\n  ActiveRecord::Base.connection_pool.disconnect!\n  PumaStatsLogger.run\nend\nIt seems to be working. Of course, my method of getting the Puma::Launcher instance is hacky. However I think it will be reliable.\nIs there a better way to access the Puma::Launcher instance?. Great to hear you like my methodology. I'm thinking of making it into a puma plugin, what do you think?. @schneems yes! I no longer need to do the hacky accessing of the launcher. So new recipe using puma master is below.\nIs any of the work you're doing going to facilitate logging this into to STDOUT, like my recipe is doing? If not, I'll put this together into a puma plugin (although I can't use plugins until #1287 is fixed).\n```ruby\nclass PumaStatsLogger\n  def self.run\n    sleep 10 # wait for puma to boot\nThread.new do\n  loop do\n    begin\n      stats = JSON.parse Puma.stats, symbolize_names: true\n      stats[:worker_status].each do |worker|\n        puts \"puma worker threads - pid: #{worker[:pid]} #{worker[:last_status]}\"\n      end\n      STDOUT.flush\n    rescue => e\n      Rollbar.error e\n    end\n\n    sleep 60\n  end\nend\n\nend\nend\n```. I still have the eventual goal to turn this into a plugin. In the meantime wanted to share the latest incarnation, using the new API in Puma 3.11.4\n```ruby\nclass PumaStatsLogger\ndef self.run\n    sleep 10 # wait for puma to boot\nThread.new do\n  loop do\n    begin\n      stats = JSON.parse Puma.stats, symbolize_names: true\n      stats[:worker_status].each do |worker|\n        Rails.logger.info \"puma worker threads - pid: #{worker[:pid]} #{worker[:last_status]}\"\n      end\n      Rails.logger.flush\n    rescue => e\n      Rollbar.error e\n    end\n\n    sleep 60\n  end\nend\n\nend\nend\n```. Hi @schneems  - have you seen my work here?\nhttps://github.com/puma/puma/issues/1512#issuecomment-362669497\nI accessed stats by looking through memory for the instance of Puma::Launcher. \ud83d\ude06 \nThe work you are doing is going to make this much less hacky -- awesome!. Finally got around to using the new API. Thanks @schneems!\n\ud83d\ude02 \n\n. backing up a little: if formerly we considered @todo.size to be the number to watch because it caused waiting to happen, can't we now consider @todo.size - @waiting to be that number to watch? And since we formerly didn't watch @waiting at all even though it had the same effect on this loop, and now @waiting's standalone role has been removed, maybe @todo.size - @waiting is in fact more meaningful than standalone @todo.size was before?\n(but I'll admit I didn't yet think deeply about your request lifecycle description-- I'm first simply comparing what we were looking at before to what we can look at now in the same place). > Puma::Server meters request based on threadpool capacity. In an ideal world it will never try to add a request if there is not capacity.\ncan you point me to the area of code where this happens? does the Server decide which thread gets the request, or does the threadpool do that? what do you mean by \"meter\"?\nif i were to naively design such a system myself, i would make\n a global queue of requests coming in (probably implemented with Queue)\n a pool of threads which take requests off of the que\nAre there such structures in Puma? If not, is that for a particular reason?. okay i found this in ThreadPool\n\nEach thread in the pool has an internal loop where it pulls a request from the @todo array and proceses it.\n\nSo I guess you are saying that the size of this queue doesn't matter and there is something further up which indicates lack of capacity. So the answers to my other questions above will lead me in the right direction. Thanks!. A thought/realization I just had: in https://github.com/puma/puma/issues/1512, I have min threads set to 1 and max threads set to whatever (5 or 6 typically). from my understanding of things, the delta between running and max_threads is meaningful. (even though that's not clear from my ticket, which only shows examples with max_threads of 1).\nSo if that's true, then the part of puma that decides to spool up and down threads (when min and max are not identical) also \"knows\" if capacity is available.\nis this interesting?. sweet! a few questions\n\nWhere did this idea come from? Discussion in another ticket/channel/IRL?\nDoes this resolve the \"we can't access a meaningful metric given the current design\" conundrum discussed in #1577, or is it more of a stopgap solution?\nWhat is the highest value that this can be?\n(related to above) How (if at all) is it affected by 0:N vs. N:N thread configuration?. I've been thinking about this problem for years. Ideally one would like to try different thread counts under similar circumstances and see how performance and CPU load are affected (assuming that processes are scaled up to use available ram, and the amount of ram more threads take is not an issue).\n\nBut that's hard to do because it's hard to get directly comparable traffic circumstances, and also cumbersome to schedule the experiment and directly compare results.\nBut... I just now had this idea: One could have, on the same machine,  Puma processes with different thread counts. Then the behavior of each can be compared. Most important is probably simply throughput. If a process with 4 threads serves almost 2 times as many requests as a process with 2 threads, 4 threads is worth the DB connections. If a process with 8 threads serves only 20% more requests than a process with 4 threads, 8 threads is too many and a waste of DB connections (of course one would would to have enough incoming traffic to feel confident there was a sufficient test... and maybe having enough would require scaling down the number of processes for some period of time until there is a tad bit of backpressure).\nDoing this would require that the logic which distributes requests to processes is very accurate in its determination of if a process has available capacity. I don't know to what extent this is possible.\nIf it is possible, then we can add features to Puma to facilitate this sort of experiment.\nWe could even eventually create some sort of autoscaling.. (above comment was edited a lot after posting, please read on website). done in #1649 \ud83c\udf89 . @schneems ever get a chance to take a look at this?. I haven't seen this in a while. I think I was wrong that it correlated with a version upgrade.. ",
    "pawel2105": "Using Ruby 2.0.0-p247, Rails 4.0.0 and newrelic_rpm (3.6.5.130)\nI've added dispatcher: 'puma' to the newrelic.yml file and added ENV['NEWRELIC_DISPATCHER'] = 'puma' to the boot.rb file and the newrelic log file on the server reports:\n[07/11/13 14:32:44 +0000 domain.com (12336)] INFO : Installing Sidekiq instrumentation\n[07/11/13 14:32:44 +0000 domain.com (12336)] INFO : Installing Net instrumentation\n[07/11/13 14:32:44 +0000 domain.com (12336)] INFO : Installing Dalli Memcache instrumentation\n[07/11/13 14:32:44 +0000 domain.com (12336)] INFO : Installing Rails 4 Controller instrumentation\n[07/11/13 14:32:45 +0000 domain.com (12336)] INFO : Installing ActiveRecord 4 instrumentation\n[07/11/13 14:32:45 +0000 domain.com (12336)] INFO : Installing Rails4 Error instrumentation\n[07/11/13 14:32:45 +0000 domain.com (12336)] INFO : Installing Rails 4 view instrumentation\n[07/11/13 14:32:45 +0000 domain.com (12336)] INFO : Finished instrumentation\n[07/11/13 14:32:45 +0000 domain.com (12336)] INFO : Reporting to: https://rpm.newrelic.com/accounts/xxx/applications/xxx\n[07/11/13 14:32:47 +0000 domain.com (12336)] INFO : Installing Sinatra instrumentation\nStill not seeing anything for the application even after another cap deploy. Production mode does have monitor_mode: true set in the yml file.\n. @jasonrclark Hi but I don't use a puma config file, instead I issue this command via Capistrano:\nrun \"cd #{current_path} && #{fetch(:puma_cmd)} -t 1:16 -q -d -e #{puma_env} -b '#{fetch(:puma_socket)}' -S #{fetch(:puma_state)} --control 'tcp://127.0.0.1:9998'\", :pty => false\nI did add require: false to the Gemfile. I added this to my deploy.rb file:\n```\nafter 'puma:restart', 'new_relic:start'\nnamespace :new_relic do\ndesc 'Manually start New Relic'\n  task :start, :roles => :app do\n    run \"cd #{deploy_to}/current && /usr/bin/env rake new_relic:start_monitoring RAILS_ENV=production\"\n  end\nend\n```\nI also created lib/tasks/start_new_relic.rake:\n```\nrequire 'newrelic_rpm'\nnamespace :new_relic do\n  desc \"start recording application stats for New Relic\"\n  task :start_monitoring => :environment  do\n    NewRelic::Agent.manual_start\n  end\nend\n```\nNow the NewRelic log file displays:\nINFO : Reporting to: https://rpm.newrelic.com/accounts/xxx/applications/xxx\nINFO : Installing Sinatra instrumentation\nINFO : Installing Sidekiq instrumentation\nINFO : Installing Net instrumentation\nINFO : Installing Dalli Memcache instrumentation\nINFO : Installing Rails 4 Controller instrumentation\nINFO : Installing ActiveRecord 4 instrumentation\nINFO : Installing Rails4 Error instrumentation\nINFO : Installing Rails 4 view instrumentation\nINFO : Finished instrumentation\nWhereas last time it only got up to installing Sinatra instrumentation.\nUnfortunately no app server stats still.\n. @jasonrclark Done!\n. On Puma 2.4.0 this is now fixed\n:+1: \n. +1\nAlso getting No such file or directory - \"/home/deploy/myapp/shared/sockets/pumactl.sock\" when cap puma:start runs using puma 2.3.1\n. @evanphx \nI've got this in config/puma.rb:\n```\nrails_env = ENV['RAILS_ENV'] || 'development'\nthreads 4,4\nbind  \"unix:///data/apps/myapp/shared/tmp/puma/myapp-puma.sock\"\npidfile \"/data/apps/myapp/current/tmp/puma/pid\"\nstate_path \"/data/apps/myapp/current/tmp/puma/state\"\nactivate_control_app\n```\nThis in script/rails:\nAPP_PATH = File.expand_path('../../config/application',  __FILE__)\nrequire File.expand_path('../../config/boot',  __FILE__)\nrequire 'rack/handler'\nRack::Handler::WEBrick = Rack::Handler.get(:puma)\nrequire 'rails/commands'\nI've also got https://gist.github.com/joneslee85/5844933 in script/puma.sh\nAnd I've got require 'puma/capistrano' in my deploy.rb file\n. Any news on that pull request?\n. I'm experiencing the same thing. Everything was fine, not sure what's changed but when I do a cap deploy:cold then Puma starts up properly and the server's shared/sockets folder contains the following files as required:\npumactl.sock\npuma.state\npuma.sock\nWhen I do a regular cap deploy or cap deploy:migrations then the cap task finishes without errors but I get a 502 on nginx because the Puma server is down. When inspecting the sockets directory it only has puma.sock after these types of migrations.\nHere is the output of a cold deploy:\n** transaction: commit\n  * 2013-11-18 10:20:59 executing `deploy:migrate'\n  * executing \"cd /home/deploy/my_app/releases/20131118082039 && bundle exec rake RAILS_ENV=production      db:migrate\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 6827ms\n  * 2013-11-18 10:21:06 executing `deploy:start'\n    triggering after callbacks for `deploy:start'\n  * 2013-11-18 10:21:06 executing `puma:start'\n  * executing \"cd /home/deploy/my_app/current && bundle exec puma -q -d -e production -b 'unix:///home/deploy/my_app/shared/sockets/puma.sock' -S /home/deploy/my_app/shared/sockets/puma.state --control 'unix:///home/deploy/my_app/shared/sockets/pumactl.sock'\"\n    servers: [\"IP\"]\n    [IP] executing command\n ** [out :: IP] Puma starting in single mode...\n ** [out :: IP] \n ** [out :: IP] * Version 2.6.0, codename: Pantsuit Party\n ** [out :: IP] \n ** [out :: IP] * Min threads: 0, max threads: 16\n ** [out :: IP] \n ** [out :: IP] * Environment: production\n ** [out :: IP] \n ** [out :: IP] * Listening on unix:///home/deploy/my_app/shared/sockets/puma.sock\n ** [out :: IP] \n ** [out :: IP] * Daemonizing...\n ** [out :: IP] \n    command finished in 4846ms\nHere's some output after a regular deploy done right after that:\n* 2013-11-18 10:27:16 executing `deploy:config_symlink'\n  * executing \"cp /home/deploy/my_app/shared/config/database.yml /home/deploy/my_app/releases/20131118082709/config/database.yml\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 705ms\n  * executing \"cp -r /home/deploy/my_app/shared/imports /home/deploy/my_app/releases/20131118082709/\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 1666ms\n  * 2013-11-18 10:27:18 executing `deploy:create_symlink'\n  * executing \"rm -f /home/deploy/my_app/current &&  ln -s /home/deploy/my_app/releases/20131118082709 /home/deploy/my_app/current\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 1029ms\n ** transaction: commit\n  * 2013-11-18 10:27:19 executing `deploy:restart'\n  * executing \"touch /home/deploy/my_app/shared/tmp/restart.txt\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 1705ms\n    triggering after callbacks for `deploy:restart'\n  * 2013-11-18 10:27:21 executing `puma:restart'\n  * executing \"cd /home/deploy/my_app/current && bundle exec pumactl -S /home/deploy/my_app/shared/sockets/puma.state restart\"\n    servers: [\"IP\"]\n    [IP] executing command\n ** [out :: IP] Command restart sent success\n    command finished in 3095ms\n  * 2013-11-18 10:27:24 executing `deploy:cleanup'\n  * executing \"ls -1dt /home/deploy/my_app/releases/* | tail -n +6 |  xargs rm -rf\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 1521ms\n    triggering after callbacks for `deploy'\n  * 2013-11-18 10:27:26 executing `rvm:trust_rvmrc'\n  * executing \"rvm rvmrc trust /home/deploy/my_app/releases/20131118082709\"\n    servers: [\"IP\"]\n    [IP] executing command\n ** [out :: IP] Marked /home/deploy/my_app/releases/20131118082709/.ruby-version as trusted\n    command finished in 1418ms\n  * 2013-11-18 10:27:27 executing `puma:force_restart'\n  * executing \"cd /home/deploy/my_app/current && bundle exec pumactl -S /home/deploy/my_app/shared/sockets/puma.state stop; true\"\n    servers: [\"IP\"]\n    [IP] executing command\n ** [out :: IP] Command stop sent success\n    command finished in 2592ms\n  * executing \"rm -f /home/deploy/my_app/shared/sockets/puma*\"\n    servers: [\"IP\"]\n    [IP] executing command\n    command finished in 1412ms\n  * executing \"cd /home/deploy/my_app/current && bundle exec puma -w 1 -t 1:8 -q -d -e production -b 'unix:///home/deploy/my_app/shared/sockets/puma.sock' -S /home/deploy/my_app/shared/sockets/puma.state --control 'tcp://127.0.0.1:9998'\"\n    servers: [\"IP\"]\n    [IP] executing command\n ** [out :: IP] [4695] Puma starting in cluster mode...\n ** [out :: IP] [4695] * Version 2.6.0, codename: Pantsuit Party\n ** [out :: IP] [4695] * Min threads: 1, max threads: 8\n ** [out :: IP] [4695] * Environment: production\n ** [out :: IP] [4695] * Process workers: 1\n ** [out :: IP] [4695] * Phased restart available\n ** [out :: IP] [4695] * Listening on unix:///home/deploy/my_app/shared/sockets/puma.sock\n ** [out :: IP] [4695] * Daemonizing...\n    command finished in 3180ms\nI wonder if the order of the cap tasks has anything to do with it?\nThis is on Puma 2.6.0 but was also happening on 2.4.0 and ruby-2.0.0-p247\n. By the way this issue is resolved for me by removing any custom puma cap tasks in my deploy file from the deploy process.\n. Urg. I had a typo. should be shared/tmp instead of tmp/shared\n. Removing the before_fork part removes the error:\n:before_fork:\n    - !ruby/object:Proc {}\n. After some digging I've found that Ubuntu now doesn't support the upstart scripts you'd expect to work from /etc/init/. I see that https://github.com/puma/puma/blob/master/docs/systemd.md does a bit of an intro but I'm still lost to be honest, because I'm not too familiar with systemd.\nWonder if the docs could become idiot-proof for idiots like me?\n. ",
    "ehutzelman": "I'm seeing the same thing. Using Ruby 2.0.0-p247, Rails 4.0.0 and newrelic_rpm (3.6.5.130). In New Relic, there is no data except for it detecting a single instance pegged at 100MB. I'm running 4 workers.\n. ",
    "jasonrclark": "Hey all. I'm an engineer on the Ruby agent team at New Relic and I'm looking into this today.\nCan those who are seeing the issue confirm whether they're using clustered mode with multiple workers? If so, we've encountered an issue with that recently. While we're getting a fix ready, you can work around it by:\nAdding :require => false to your gem 'newrelic_rpm' line in your Gemfile\nIn your puma configuration file, add a block like this:\non_worker_boot do\n  require 'newrelic_rpm'\n  NewRelic::Agent.manual_start\nend\nIf anyone is not running in clustered mode but has problems with recent versions, I'm especially interested to hear about that too.\n. @pawel2105 Yeah, that doesn't look like the same clustered mode issue other folks have seen. What version of puma are you running?\nIt would help on my end if you opened a support case. You can reference my comment here and ask them to escalate the ticket directly to me. That'll let me look more closely at what's happening on our end.\nI appreciate your help, and will definitely report back here any additional information so others can benefit.\n. @grk Hadn't heard about that case, but I'll see whether the fixes I'm doing for clustered mode will get it fixed. Based on how your workaround was structured, I'm hopeful what I'm working on might be applicable there too. Thanks for mentioning it!\n. We've released version 3.6.6.147 of newrelic_rpm which now supports Puma cluster mode. Anyone following the instructions from my comment https://github.com/puma/puma/issues/128#issuecomment-21050609 should be able to upgrade and remove both the :require => false and the on_worker_boot workaround code.\nIf this new version doesn't work with cluster mode, or you were having different symptoms that I missed, please feel free to comment here or enter a support ticket referencing this thread and we'll take a closer look.\n. @ayrton The log message you're seeing is coming from a known bug that will be fixed in the upcoming 3.6.7 release (commit's already visible at https://github.com/newrelic/rpm/commit/dc09a06cf5a518f8296297cac8f4e7f5046d8021).\nAdditionally, as @concept47 indicated you might need to set things up per #335. newrelic_rpm still doesn't properly support daemonize mode for Puma, and on that thread you can find instructions for working around that. I haven't checked it yet, but that workaround might avoid the cli_config failure, but that crash will definitely be addressed in the next release either way.\nDefinitely let us know if that doesn't get things working for you, and I'll be updating here when we do have a fix to properly start the agent when daemonized.\n. @ondrejbartas Glad that seems to have gotten some part of it working for you.\nWhere are you seeing just one instance listed? We have a couple possible ways you could see that in the UI, so I want to be able to check where you're actually looking.\n. @ondrejbartas Glad to hear it cleared up eventually. Thanks for the image as well. I hadn't thought that data could potentially lag in that part of the UI, but good to know.\n. @haggen Yeah, shotgun isn't going to work out--it's a known limit we've got.\nOn the environment variables, you're totally right. More recent agent versions actually support NEWRELIC or NEW_RELIC as prefixes for any the keys because it was so common for to mix them up. We're trying to consistently only document them as NEW_RELIC, but it should still be happy with the other.\n. @mrbrdo All of that seems odd to me. That Unicorn detection should be relatively harmless if the server isn't running, but I don't see why it would get found without the unicorn gem installed.\nDispatcher setting should never be required past the 3.6.1 version of newrelic_rpm, so that's odd in itself.\nAny chance you could email me your Gemfile and puma configs? jclark [at] newrelic [dot] com\n. @ponny If you set the daemonize option true in your puma configuration, newrelic_rpm as of 3.6.9.171 doesn't detect properly. We're working on a fix, but don't have it ready yet.\n@madwork's suggestion or something similar should fix things. As long as Puma doesn't daemonize itself, you should be fine--another process managing Puma can daemonize/background all it wants, as long as Puma itself isn't calling Process.daemon.\nAn alternative some people have had success with is setting workers 1 in your puma configuration, but that requires Puma 2 and you said you're using 1.6.3, so that's probably not an easy option.\n. Hey @grk. I'd mentioned in another thread that I was hopeful this might be addressed by our cluster mode fix. Turns out we're only partway there.\nThe newly released 3.6.6.147 version of the Ruby agent has cluster mode support. From my testing, you shouldn't need to :require => false or add the on_worker_boot workarounds anymore, but you do continue to need to set cluster mode with 1 worker to get things working right.\nIf you're interested, I'd love to hear whether that works for you. I've also entered the Puma daemonize problem onto our issues list, so at some point we can hopefully get it fixed and comment back here.\nYou're absolutely right that an after_fork hook in Puma itself would make this a lot cleaner for us in the agent, but we might be able to find a fix without requiring Puma to change.\n. Hey @Helio-Pereira \nWe don't have a fix to this yet, but it is definitely on our list to address.\nThat said,  spent a few minutes researching this morning and there's a hook since 2.3 for on_booted after the Process.daemon call that could be what we need. However, I'm not seeing where we can get to it from the agent (https://github.com/puma/puma/blob/master/lib/puma/events.rb#L95-L97) since I haven't found a way to access the current Runner or CLI objects Puma's running with.\nAnyone more familiar with Puma have any ideas if there's a good way to do that I'm missing?\n. @evanphx Happy to help out in any way I can.\nWhat we did with cluster-mode was to have our instrumentation automatically add an on_worker_boot handler. https://github.com/newrelic/rpm/blob/master/lib/new_relic/agent/instrumentation/puma.rb\nI think we could do something similar for daemonize if we had a way to tap into the on_booted event, but haven't figured how to get to it from the agent.\n. Just released newrelic_rpm 3.7.1.180 which contains a (currently optional) mode that I believe should get the New Relic thread restarted cleanly even after daemonizing. If this works out for folks, we may enable it by default and be able to get away from having to register hooks in Puma itself, which would be awesome!\nTo enable the feature, update your agent to 3.7.1.180 and add this to your newrelic.yml\nrestart_thread_in_children: true\nKeen for any feedback on whether that helps. Thanks!\nNote: original comment had the wrong key name (restart_threads_in_child) which would not work. I've updated the comment here so anyone finding it won't have to follow the comment trail to find my typo.\n. :disappointed: ack! I've revised my comment above with a note so anyone stumbling on it will hopefully get the right key name first.\n. ",
    "rafbm": "Thank you so much @jasonrclark for this timely intervention. Just switched to clustered mode and was experiencing this issue. Everything just works when following your instructions!\n. ",
    "exviva": "I think the problem has to do with the --preload option - when I removed it, everything started working as expected again. This seems to make sense - when using preload, the agent is attached to the master process, which never receives any requests.\nCan you guys confirm that you've been using --preload?\n. Another solution would be to detect if Bundler was required as a gem, and if so, pass -I$path_to_gem_installed_bundler to the restarted process.. Here's my idea for a fix: https://github.com/puma/puma/pull/1706.. Hmm, updating rubygems to 3.0.2 seems to have fixed the issue...:thinking:. Thanks for the review @olleolleolle . Turns out updating rubygems to 3.0.2 fixed the issue, so closing this.. ",
    "grk": "@jasonrclark I described my issue in #335. puma 2.3.2, newrelic_rpm 3.6.5.130, no data when using daemonize, but not clustered mode.\n. I'll try to get an isolated test case, but so far it happens only when starting a new process. I've got more details about our setup in https://github.com/puma/puma/issues/197.\n. I tried making an isolated test case but didn't get the mixed results I was getting on production.\nI created a simple rails app, with same Gemfile as our prod app, and a test script. When running this, the only weird thing that I was getting was this error:\nArgumentError (A secret is required to generate an integrity hash for cookie session data. Use config.secret_token = \"some secret phrase of at least 30 characters\"in config/initializers/secret_token.rb):\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/cookies.rb:319:in `ensure_secret_secure'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/cookies.rb:284:in `initialize'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/cookies.rb:231:in `new'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/cookies.rb:231:in `signed'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/session/cookie_store.rb:53:in `block in unpacked_cookie_data'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/session/abstract_store.rb:57:in `stale_session_check!'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/session/cookie_store.rb:51:in `unpacked_cookie_data'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/cookie.rb:107:in `extract_session_id'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/session/abstract_store.rb:53:in `block in extract_session_id'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/session/abstract_store.rb:57:in `stale_session_check!'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/session/abstract_store.rb:53:in `extract_session_id'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:43:in `load_session_id!'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:32:in `[]'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:267:in `current_session_id'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:273:in `session_exists?'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:107:in `exists?'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:127:in `load_for_read!'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/session/abstract/id.rb:59:in `[]'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/notice.rb:371:in `find_session_data'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/notice.rb:138:in `initialize'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake.rb:155:in `new'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake.rb:155:in `build_notice_for'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake.rb:117:in `notify_or_ignore'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/rails/middleware.rb:42:in `notify_airbrake'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/rails/middleware.rb:17:in `rescue in call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/rails/middleware.rb:14:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/debug_exceptions.rb:16:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/show_exceptions.rb:56:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/railties-3.2.12/lib/rails/rack/logger.rb:32:in `call_app'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/railties-3.2.12/lib/rails/rack/logger.rb:16:in `block in call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/activesupport-3.2.12/lib/active_support/tagged_logging.rb:22:in `tagged'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/railties-3.2.12/lib/rails/rack/logger.rb:16:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/actionpack-3.2.12/lib/action_dispatch/middleware/request_id.rb:22:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/methodoverride.rb:21:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/runtime.rb:17:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/activesupport-3.2.12/lib/active_support/cache/strategy/local_cache.rb:72:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-1.4.5/lib/rack/lock.rb:15:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in `forward'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:in `fetch'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in `lookup'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:in `call!'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/rack-cache-1.2/lib/rack/cache/context.rb:49:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/user_informer.rb:16:in `_call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/airbrake-3.1.8/lib/airbrake/user_informer.rb:12:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/railties-3.2.12/lib/rails/engine.rb:479:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/railties-3.2.12/lib/rails/application.rb:223:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/railties-3.2.12/lib/rails/railtie/configurable.rb:30:in `method_missing'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/puma-2.0.0.b6/lib/puma/configuration.rb:66:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/puma-2.0.0.b6/lib/puma/server.rb:350:in `handle_request'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/puma-2.0.0.b6/lib/puma/server.rb:233:in `process_client'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/puma-2.0.0.b6/lib/puma/server.rb:135:in `block in run'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/puma-2.0.0.b6/lib/puma/thread_pool.rb:92:in `call'\n  /Users/grk/.rvm/gems/ruby-1.9.3-p392/gems/puma-2.0.0.b6/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\n. Oh, forgot to link, the test app is at https://github.com/grk/puma-test\n. @jasonrclark that's great to hear! This fixes the issue for our customer, since we can run puma in clustered mode for him. However, clustered mode doesn't work on jruby, so a fix for daemonize would be nice. Luckily so far noone used a combination of jruby + puma + newrelic yet.\n. ",
    "ayrton": "Today I've switched to Puma coming from Passenger and I noticed that I no longer see any data flowing in newrelic.\nI currently see the following in my newrelic logs:\n[08/27/13 21:00:20 +0200 staging01 (2058)] INFO : Installing Puma cluster mode support\n[08/27/13 21:00:20 +0200 staging01 (2058)] ERROR : Error while installing puma instrumentation:\n[08/27/13 21:00:20 +0200 staging01 (2058)] ERROR : NoMethodError: undefined method `cli_config' for Puma:Module\nThis was before and after I've set dispatcher: 'puma' in my newrelic config file, after some googling I couldn't find anything related but this issue.\nI'm using puma 2.5.1 and newrelic_rpm 3.6.6.147\nFinally, my puma config file:\n```\n!/usr/bin/env puma\nSet the environment in which the rack's app will run. The value must be a string.\nenvironment 'staging'\nDaemonize the server into the background. Highly suggest that\nthis be combined with \u201cpidfile\u201d and \u201cstdout_redirect\u201d.\ndaemonize\nBind the server to \u201curl\u201d. \u201ctcp://\u201d, \u201cunix://\u201d and \u201cssl://\u201d are the only accepted protocols.\nbind 'tcp://0.0.0.0:9292\n```\nAny ideas @jasonrclark?\n. ",
    "ondrejbartas": "Hello, I tried  https://github.com/puma/puma/issues/335 this solution - set workers to 1 and new relic started to receive data. Only one problem. I set 3 workers and in new relic I see only 1 instance.\nOndrej\n. @jasonrclark I was too rush. Because of 1 puma worker could manage all request, no other workers register in new relic web ui:\n\nBut after longer time all instances are shown correctly.\nThanks\n. Hello, I had same issue. Daemonize and 0 workers.\nWhen I set workers to 3 (didn't try 1) it just started to work. \nThank you guys. This really helped me.\n. ",
    "haggen": "Why is it NEWRELIC_DISPATCHER and all other variables NEW_RELIC_... ? This really kills discoverability and leads to mistakes.\nBy the way, dispatcher: puma and environment variable didn't work here.\nUpdate.\nDarn it! My mistake. I was using shotgun not puma to launch my Sinatra app in development.\n. Hey @ponny how is your launch script, I mean what do you use to boot up the server ? Also, do you know the version of your new_relic Gem ?\n. ",
    "mrbrdo": "@jasonrclark is it normal for newrelic to be detecting unicorn when running on puma?\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Puma cluster mode support\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Sidekiq instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing deferred Rack instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing ActiveRecord 4 instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Net instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Unicorn instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Detected Unicorn, please see additional documentation: https://newrelic.com/docs/troubleshooting/im-using-unicorn-and-i-dont-see-any-data\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Rails4 Error instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Rails 4 Controller instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Installing Rails 4 view instrumentation\n[09/20/13 01:45:14 +0200 distro (14810)] INFO : Finished instrumentation\nI did previously have unicorn in my Gemfile, additionally to puma, but I've now removed it in hopes of solving this--didn't seem to help. It seems to work fine though (reporting data and all). As you can see I'm using clustered mode (8 workers). Also, I think I had to add NEW_RELIC_DISPATCHER to make it work (I also added dispatcher to newrelic.yml), before that it did not send any data (but it did start judging from log). Newest stable puma and newrelic_rpm.\n. Hm, today I restarted puma on the server just in case and now it's not detecting unicorn anymore.  I think I did a stop & start yesterday too, but maybe I just did a restart, not sure. Anyway it seems newrelic was then detecting unicorn because the gem was present (even though puma was present too).\nBefore I added the environment variable, I didn't get any data in newrelic (but logs were showing that newrelic did start).\n. Sorry for reviving an old issue, but do we need to use at least 2 cluster workers for this to work, or is it supposed to work in pure threaded mode too? Because for me it is not working with threads only, I send a phased-restart via pumactl and a normal restart is made (gateway error until the app is started again). Logs also don't mention anything about a hot or phased restart.\n. Yes you are right, but it would still be possible on JRuby to spawn another puma worker/instance, wait for it to boot, and then redirect the socket to the new instance, then stop the old instance. I don't see why fork is required for this.\nNo-downtime restarts is something we need for JRuby too.\n. @evanphx I was thinking something along the lines of what you just described for phased-restart. I know cluster mode is not available for JRuby right now. What is the reason?\nThe problem which I am trying to address is to be able to do a zero-downtime deploy/restart. It can probably be done via reverse proxy like @nathansamson already mentioned, but it is a complex setup and everyone has to do it on their own.\nAlso I know @headius was talking some time ago that they are looking into using some native code to support accessing real FDs, but I think this is not yet complete, if it has even been started yet. Since he is currently working on a Rails preloader for JRuby I do not think that fork will be supported any time soon.\n. @evanphx any updates on this? I'm really in need of a zero-downtime-deploy option for one of my projects.\n. it was caused by newrelic, not related to puma\n. Yes, but I'm not on Heroku. However I am having similar problems with Unicorn too (+ my workers are even dying on Unicorn), so maybe this is a problem with my app. Any idea what it could be?\nPS: does it even make sense to use multiple threads per worker on MRI? how many?\n. Let me get back to you at the end of the week, I'll do some testing. Thanks!\n. Ok I am back on Puma. I still have this problem though. I think the 'slowness' coincides with this error in my puma error log:\n2013-06-18 00:03:52 +0200: Read error: #<Errno::EPIPE: Broken pipe>\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `syswrite'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `fast_write'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:475:in `handle_request'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:243:in `process_client'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:142:in `block in run'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/thread_pool.rb:92:in `call'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\n2013-06-18 00:05:06 +0200: Read error: #<Errno::EPIPE: Broken pipe>\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `syswrite'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `fast_write'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:475:in `handle_request'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:243:in `process_client'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:142:in `block in run'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/thread_pool.rb:92:in `call'\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\nIt seems when this happens the client has to wait a long time before he gets a response. I did try 8:8 thread config (and cluster of 8 workers also). I am on MRI 1.9.3-p429, nginx, running on VPS 32bit ubuntu with 2GB RAM, 1.4GB used. Would appreciate any speedy ideas as I'm running this in production so it's kinda awkward for me :) The above is the only problem I can see in my logs. I'm using puma master branch from about a week ago.\nThanks!\n. @PetrKaleta some more info, it seems the EPIPE error in puma error log conincides a bit with errors from nginx log.\nnginx:\n2013/06/18 10:18:55 [error] 12597#0: *3670 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 81.139.159.67, server: test.x.com, request: \"GET / HTTP/1.1\", upstream: \"http://unix:///x/web/shared/sockets/puma.sock:/\", host: \"users.x.com\"\npuma:\n2013-06-18 10:19:34 +0200: Read error: #<Errno::EPIPE: Broken pipe>\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `syswrite'\nnginx:\n2013/06/18 10:23:46 [error] 12597#0: *3678 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 81.139.159.67, server: test.x.com, request: \"GET / HTTP/1.1\", upstream: \"http://unix:///x/web/shared/sockets/puma.sock:/\", host: \"users.x.com\"\npuma:\n2013-06-18 10:24:25 +0200: Read error: #<Errno::EPIPE: Broken pipe>\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `syswrite'\nHowever it's not always the case... Sometimes nginx has this error instead:\nnginx:\n2013/06/18 03:55:40 [info] 12597#0: *2323 client prematurely closed connection, so upstream connection is closed too while sending request to upstream, client: 71.177.224.171, server: test.x.com, request: \"GET / HTTP/1.1\", upstream: \"http://unix:///x/web/shared/sockets/puma.sock:/\", host: \"users.x.com\", referrer: \"http://x.com/\"\npuma:\n2013-06-18 03:55:49 +0200: Read error: #<Errno::EPIPE: Broken pipe>\n/x/web/shared/bundle/ruby/1.9.1/bundler/gems/puma-bd97b6ec3e24/lib/puma/server.rb:613:in `syswrite'\nI'm not sure if these are related, but that's all I could find. It seems like in some cases my worker can't start in time to serve the request (only my assumption!). I don't understand why it has to start anyway, since this is not happening after puma restart, it's happening on random requests when puma is already \"hot\". Is there any way I could find out if my workers are dying and why?\n. Also I should update the title of the issue, the bigger problem for me is that sometimes I get an error page from nginx (I presume), basically the page loads for some time and then I get an error, probably 504 from nginx.\n. Master at the time. Maybe it could have been a low memory issue?\n. Not sure but I am not getting any reports about issues anymore... I did upgrade RAM on the server though. Im still on master I think\n. For me it was too little RAM on a VPS I think. Do you have enough RAM?\n. I suggest to open a new issue since it's not the same problem, I was getting these errors seemingly randomly not when performing hot/cold restart. So it's not the same issue.\n. Well the Read error: #<Errno::EPIPE: Broken pipe - Broken pipe> is definitely a problem since I am getting several of these every day too. I can't really say what it looks like on the client side when this happens since I am unsure if I have experienced it myself. I haven't gotten any reports of problems but it may be that users just retry if this happens. Even in case it is harmless I would argue it's still a bug since then it's not really an error that should be logged. So definitely something to look into.\n. Any update on this?\n. @evanphx could we reopen this? It seems it is possible to achieve this but there is no easy-to-understand solution provided here. If this is possible it would be nice to have it in puma documentation or for puma itself to take care of it.\nI understand the part about the master processes but I don't understand how ENV[\"BUNDLE_GEMFILE\"] in combination with prune_bundler helps here or what it does.\n. I did not try it but that was not the point of what I was trying to say.\n. That it should be documented or implemented. It's not like it is some obscure thing that never happens.\n. That's great, I did not see that. I will give it a shot and if I have issues I'll report back. Thanks!\n. I am using prune_bundler with phased-restart, but I get errors when my Gemfile changes, e.g.:\n/opt/rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.9/lib/bundler/runtime.rb:34:in `block in setup': You have already activated tzinfo 0.3.42, but your Gemfile requires tzinfo 0.3.43. Prepending `bundle exec` to your command may solve this. (Gem::LoadError)\n  from /opt/rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.9/lib/bundler/runtime.rb:19:in `setup'\n  from /opt/rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.9/lib/bundler.rb:122:in `setup'\n  from /opt/rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.9/lib/bundler/setup.rb:17:in `<top (required)>'\n  from /opt/rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'\n  from /opt/rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_require.rb:55:in `require'\n...\nAfter I stop and start puma it begins working correctly. I start puma itself with bundle exec (using the puma jungle init script). I am using rbenv without gemsets. Bundler is setup to install into vendor/bundle (as per usual).\nAfter stop/start I do get this warning (but it works fine):\nWARN: Unresolved specs during Gem::Specification.reset:\n      tzinfo (~> 0.3.37)\nWARN: Clearing out unresolved specs.\nPlease report a bug if this causes problems.\nDo I need to set BUNDLE_GEMFILE? It's not mentioned here https://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting or in the puma jungle script.\n. this has been working fine for me, but today I set up a new server, using the same Chef script I used with the previous server, however now I cannot boot puma if I use prune_bundler. It works fine without that, but with it I get the attached error.\nI am using rbenv 0.4.0-146-g7ad01b2 and puma 2.9.0. The version of puma is the same on the old server. rbenv is 0.4.0-143 there. I also tried export BUNDLE_GEMFILE=/path/to/Gemfile before running the command but that did not change anything. The gems are installed via the usual deployment way (into vendor/bundle, not into system gems).\nI am using the puma \"jungle\" scripts that are included in the repo. So I use an init script with run-puma to start puma.\nbash\n$ bundle exec puma -C config/puma.rb\n* Pruning Bundler environment\n/opt/rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/dependency.rb:298:in `to_specs': Could not find 'rack' (= 1.5.2) among 9 total gem(s) (Gem::LoadError)\n  from /opt/rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/dependency.rb:309:in `to_spec'\n  from /opt/rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_gem.rb:53:in `gem'\n  from /u/apps/myapp_production/current/vendor/bundle/ruby/2.1.0/gems/puma-2.9.0/bin/puma-wild:10:in `block in <main>'\n  from /u/apps/myapp_production/current/vendor/bundle/ruby/2.1.0/gems/puma-2.9.0/bin/puma-wild:8:in `each'\n  from /u/apps/myapp_production/current/vendor/bundle/ruby/2.1.0/gems/puma-2.9.0/bin/puma-wild:8:in `<main>'\nEDIT: If I set GEM_HOME to /path/vendor/bundle/ruby/2.1.0 that works, but obviously that's not ideal, and it works without that on the old server. Any ideas?\n. It seems like the problem could be related to bundler version (old: 1.7.9 new: 1.9.1). I checked the changes in rbenv between the two versions and it is basically just a single commit and it's very unlikely it's causing the problem.\n. My problem is that pumactl stop does not delete the .sock file, and when I try to start puma back up later I will get an error \"IOError: Connection refused\". Same thing with 2.5.1 and 2.6.0. I am running on JRuby, I belive this problem might not be happening on MRI.\n. Do you use jruby? I think it might matter. I definitely still have the issue on 2.6.0-java.\n. not mine.\n. @evanphx yeah I'm using clustered mode. Will try upgrading, thanks!\n. whoops nevermind, wrong issue\n. Was not using JRuby when I was getting this error so it can't be that\n. Good job, will be interesting if this solves the other issues people were having, exhibiting this error. Thanks!\n. Still experiencing this, 2.6.0, JRuby 1.7.6.\npuma was running, I stopped it through pumactl, when I tried to start it again (through puma -C ...), I got this error.\nYou should really check if the socket is open, if not delete it, because this is annoying. I have to SSH to the server and manually delete the sock file so that I can start puma. I am on JRuby so no cluster mode of course.\n* Listening on unix:///deploy/shared/sockets/puma.sock\nIOError: Connection refused\n                new at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/binder.rb:261\n  add_unix_listener at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/binder.rb:261\n              parse at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/binder.rb:114\n               each at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/binder.rb:82\n              parse at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/binder.rb:82\n      load_and_bind at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/runner.rb:111\n                run at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/single.rb:41\n                run at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/lib/puma/cli.rb:442\n             (root) at /deploy/releases/20/vendor/bundle/jruby/1.9/gems/puma-2.6.0-java/bin/puma:10\n               load at /deploy/shared/bundle/jruby/1.9/bin/puma:23\n             (root) at /deploy/shared/bundle/jruby/1.9/bin/puma:23\n! Error starting new process as daemon, exitting\n. Perhaps by making it possible to use workers/cluster mode on JRuby, which sounds like an easy modification. If this would be done, then step 2 would be to simply have a cluster of 1, and on hot restart add another worker, wait for it to boot, and then remove the old worker.\nI never said the implementation needs to be the same as with MRI, what I am asking is for restarts without downtime. Current state makes puma almost useless in non-cloud environments if you deploy often.\n. ",
    "ponny": "What exactly do I need to do now?  I've added ENV['NEWRELIC_DISPATCHER'] = 'puma' to boot.rb and dispatcher: puma to my newrelic.yml.\nIs there anything else I need to do?  I'm getting practically no data (just a cpm count - everything else is zero).\nUpdate: Puma 1.6.3 Ruby 1.9.3 Rails 4.0.1 w/ Resque Sinatra app\n. Hi @haggen I use some jungle scripts from the puma project.  New relic gem v 3.6.9.171\n@seuros Except when it doesn't ;-)\n. @madwork So what exactly do I need to do to fix it?  I don't see how switching to upstart would make a difference.\n. Here's my puma config, if that helps: \nrails_env = ENV['RAILS_ENV'] || 'development'\nthreads 4,16\nbind  \"unix:///home/rails/MyApp/shared/tmp/puma/MyApp-puma.sock\"\npidfile \"/home/rails/MyApp/current/tmp/puma/pid\"\nstate_path \"/home/rails/MyApp/current/tmp/puma/state\"\nactivate_control_app\n. Issue was pointing at shared/tmp instead of current/temp for my socket\n. It worked for a while but a deploy/restart seems to have stopped the puma server starting again:\n/etc/init.d/puma status MyApp\n- Status Puma rack web server puma\n- --> About to status puma /home/rails/MyApp/current\n  No such file or directory - /tmp/puma-status-1363814285081-27251\nI was able to get a working status before when I closed this issue previously but now it's refusing to run.\n. Blew away files in $app/tmp/puma.  init.d restarted and it worked.  Status worked too.\nDid a deploy that runs an init.d restart and the app fell over again.\nBlew away files to make it run/status again.  \nAny ideas what I've done wrong that is causing changes to kill it?\n. We had some similar problems for our app.  We wrote our own cron/bash killer for them.  I had a look at the rbtrace and saw that it looked like all objects from an action were being kept around - json, controllers, models, views, connections, etc.  No smoking gun for my app doing the wrong thing.  I recently tried different combinations of workers/threads/db pool.   We're using Rails 4.2, puma 2.11.3, postgres, Ruby 2.1.6.\n4 workers, 128 threads, 8 db pool size => leaks like a sieve - we'd sometimes go from 4gig to 8gig in an hour.  Graph with manual restart near the end: https://dl.dropboxusercontent.com/spa/wdptakhwk6xo3zk/po7q3ww2.png\n4 workers, 8 threads, 8 db pool size => Very slow increase in usage.  Not sure if it'll level off or continue increasing.  Performance on par with 128 threads.  Graph with restart at start: https://dl.dropboxusercontent.com/spa/wdptakhwk6xo3zk/n-81noli.png\n1 worker, 32 threads, 32 db pool size => This is the only setting that GC seems to work reliably.  Doesn't seem to be leaking any more.  Only used one CPU though so performance was degraded.  Graph: https://dl.dropboxusercontent.com/spa/wdptakhwk6xo3zk/-s3yqtfd.png\nMy 2c from this is to set the number of threads to the same as the db pool.\n. @Kagetsuki Not quite. Running with more threads than I have db pool seems to cause massive leakage for me.  Using the same number of threads as db pool in production sees pretty consistent memory usage.  It might be leaking a small amount.  You can see my latest memory usage here with a deploy in the middle that bounced the app: https://dl.dropboxusercontent.com/spa/wdptakhwk6xo3zk/gd089von.png\n. ",
    "seuros": "@ponny  New Relic gem work out of the box with puma.\n. same issue here!\n. Maybe puma get started before another service that is required (eq: database).\n. +1\n. To fix that, i had to create another file 'capistranov3.rb' since the hooks are not the same .\nI can send a pull request if wanted.\n. Sorry guys, i didn't have email notification set here.\nI didn't send the pull request after noticing that it will break existing setups.\nI end up using the jungle script init and a custom .cap file (not compatible with the current puma/capistrano flow). However if you want use puma with the new capistrano , you can place a puma.cap inside your capistrano/tasks folder with \n```\nnamespace :puma do\n  set(:puma_cmd, \"#{fetch(:bundle_cmd, 'bundle')} exec puma\")\n  set(:pumactl_cmd, \"#{fetch(:bundle_cmd, 'bundle')} exec pumactl\")\n  #set(:puma_state, \"#{shared_path}/puma/state\") <-- this line don't work\n  desc 'Start puma'\n  task :start do\n    on roles (fetch(:puma_role, :app)) do\n      within current_path do\n        execute \"#{fetch(:puma_cmd)} #{fetch(:start_options)}\"\n      end\n    end\n  end\n%w[stop restart phased-restart].each do |command|\n    desc \"#{command} puma\"\n    task command do\n      on roles (fetch(:puma_role, :app)) do\n        within current_path do\n          execute \"#{fetch(:pumactl_cmd)} -S #{fetch(:state_path)} #{command}\"\n        end\n      end\n    end\n  end\nend\n```\ndon't forget to set  _start_options and _state_path in your deploy.rb. I could not make the commented line work, it seem that deploy_to is not loaded before the capfiles are imported. I might be wrong.\n. It can then include the jungle script init script, and many variants of the cap recipes. \n. I will be releasing a gem that will handle this issue probably tomorrow. I included the tasks to start, stop, status, restart puma, and also you can install the jungle scripts.\nI'm currently testing and parameterizing everything so you it can suit most people installation.\nIf you have any request, please tell me it so i can see if i can implement it. \n. Here you have : https://github.com/seuros/capistrano-puma . Basic tasks are working and partial support for jungle.\n. Did you try preload_app! in the config file ?\n. If you have puma in cluster mode, try to disable and test again.\nMy puma instances serve 10 times more data without any problem.\n. That seem like a jruby issue and not puma's\n. +1\n. The log can be set inside the $config file.\n. @stas you have to use a gem like capistrano3-puma for that.\n. That command don't work and should be removed.\nhttps://github.com/puma/puma/blob/master/lib/puma/control_cli.rb#L200\n. You can add/remove workers by sending TTIN/TTOUT signals to the master. \n. @mperham right! \nI'm looking now if i can change the thread count somehow.\n. I will update capistrano3-puma gem to support these signals soon.\n. capistrano 2 ?\n. Workers should reload directory automatically, it make no sense to add a extra command to reload the path. \nI think this behavior should be chain-linked to phased-restart.\n. People who care about phased-restart should have at least tested if their app is booting before sending it to production.\nIt will be more hard to notice the error if the application still using the old code base but your change are not reflected.\n. Thank you.\n. Sorry, i have difficulties explaining myself. \nWhat i wanted to do is that instead of executing 2 command, Puma should do it automatically do this if a phased-restart is called.\nIt should restart and only show a error message if it fail : \nFailure scenario : \nCommand phased-restart sent success\n[2014-02-25 15:03:11] Error: Unable to boot with new codebase, using old one.\n Phased-restart failed\nSuccess scenario :\nCommand phased-restart sent success\n. Check #483 \n. I'm wrong, this callback is not usefull since it will never be called if the application fail to boot. \nalso it will be run under another thread/context.\n. @evanphx Any help here ?\n. I have 2 workers and i see 3 process. Master + 2 Workers.\n. can you do a ps aux  and paste the  lines with the workers/master ?\n. @schneems : I'm running 2.7.1 as well.\n. +1\n. @evanphx from which version will feature be implemented ? \n. I'm sorry to say it but :-1: \nThank you for your effort though.\n. No, since they are required for tests and doc.\nIn a gem , the dependencies are in the gemspec \nhttps://github.com/puma/puma/blob/master/puma.gemspec\n. No, Gemfile is not used at all when puma is required as a gem. \nRefactoring it not necessary and may cause  problems for people that don't use default envs.\n. There is nothing wrong with this. The port is used, you can't have 2 servers in the same port.\n. @evanphx the working scripts can be found in capistrano3-puma\n. Only if your gemfile changed.\n. it works only if you using cluster mode.\n. No operation as far as i know.\n. :+1: \n. What about having it === puma shutdown: 2014-07-01 14:13:10 +0100 === ?\n. cc @evanphx \n. @finist try systemd\n. You can't.\n. Yes, please send a PR.\n. :green_heart: \n. ",
    "madwork": "@ponny newrelic agent v3.6.9.171 don't work (no auto_instrument) with daemonize option (#335) in single mode (workers 0), I did not test in cluster mode.\nI use upstart to daemonize puma and newrelic agent works as expected.\n. @ponny Set daemonize option to false will fix the newrelic agent issue, but puma will not start as a daemon, so to fix this behavior, I use upstart to daemonize puma (and start/stop/monitor puma process).\nI made a gist for this issue: http://git.io/UjuxwQ\n. ",
    "thehappycoder": "Setting workers to 1 worked for me as well and -w 2 works too.\nnewrelic_rpm-3.7.1.182\npuma-2.6.0\npuma -t 8:50 -e production -b unix:///tmp/utro.0.sock -w 1 -d && puma -t 8:50 -e production -b unix:///tmp/utro.1.sock -w 1 -d\nbut this also works\npuma -t 8:50 -e production -b unix:///tmp/utro.0.sock -w 2 -d\n. Where can I read about it more? How to configure it?\n. I had similar issue which I could only reproduce at load test:\nNoMethodError: undefined method `_save_callbacks' for #Class:0x007fdf99ebca00>\nSeems that ruby-2.3.0 fixed it :)\n. Thanks!\n. ",
    "nTraum": "Can confirm that setting workers > 0 fixes the issue.\nbundle show | egrep \\(relic\\|puma\\)\n  * capistrano-newrelic (0.0.8)\n  * capistrano3-puma (0.2.2)\n  * newrelic_rpm (3.7.3.204)\n  * puma (2.8.1)\n. ",
    "hlegius": "I can start up a testing server on Heroku for you, ok? Resuming the issue is: after Puma return from idle status, no CSS nor Javascript loads again.\nAnyway I'll load a server for you and will gave all information about.\nThanks!\n. ",
    "capitalist": "I'm having the same problem. For me, my javascript gets served on a request to screen.css after idle.\n. ",
    "DouweM": "Is this still a problem? I was about to switch one of my Heroku apps to puma, but this unresolved and unanswered issue kinda scares me.\n. ",
    "mdespuits": "@dariocravero Thanks. I really do appreciate it. I understand where to look for configuration options like that (I already had), but I was thinking about those who are not as familiar on how to determine options like that...\n. ",
    "baroldgene": "I would very much like to see this done.  Even an example configuration file with most of the options would be great.  I can find no documentation on this and am having trouble parsing the above linked file.\n. ",
    "simonc": "I agree, when coming to puma the first time, it's hard to see that a config file is usable (only a small reference to the -C option is written). Knowing what is the format to use and which option are usable in a readable way is important.\n. Adding a little note for each option to indicate at which version of puma it appeared would be very useful, I had to bundle open the gem to see that some current options weren't present yet.\n. Hi, great job :)\nI don't think the # Examples comment brings anyting though. For instance I think the following would be enough :)\n``` ruby\nThe directory to operate out of.\ndirectory '/u/apps/lolcat'\n```\n\nExamples could also be simplified. The app do; ...; end option doesn't need the lambda example. If someone wants to use it this way, that means he knows how to do it already ;)\n\nIndicating the default values would be a good thing too, I guess.\n. That much better ! Thank you for doing this :)\n. ",
    "sashap": "Having the same problem - I'm new to puma and having to figure out which options are available in config could be easier.\n. ",
    "mkempe": "Hi there,\ni just opened a pull request (https://github.com/puma/puma/pull/205), any suggestion is welcome.\n. Hi Simon,\nthanks for your comments and you are right. I just streamlined the sample configuration and moved it into the existing one. You can see the sample configuration at: https://github.com/mkempe/puma/blob/83b6d9c8071d941d0f904f3c1f5a90cfd5aed9ae/examples/config.rb\nFor the default value, I'll see what I can do.\n. Hi Evan,\nMaybe I overlooked something, or am just too impatient, but I can not see the changes.\nPS: Thanks again for all your work.\n. Just ignore my last comment :)\nHave a nice day.\n. ",
    "nanaya": "sorry, I didn't notice yesterday's commits.\n. looking again, I noticed that the logging behavior can just be fixed by explicitly setting logger output to STDOUT (which is what commonly suggested for production as well). Though in this case there's logger timestamp making the output a bit noisier than simply from rails server.\nExtra: quiet_assets gem also stops functioning.\nExtra 2: timestamp comes from rails' it seems.\n. It's still the same noisy format. I haven't looked into log formatting enough to fix this.\nAlso it makes rails console output duplicated:\n```\ndefault logger\nirb(main):001:0> Post.count\n   (43.5ms)  SELECT COUNT(*) FROM \"posts\"\n=> 298121\nSTDOUT logger\nirb(main):002:0> Post.count\n   (48.7ms)  SELECT COUNT() FROM \"posts\"\nD, [2015-03-09T14:11:59.271945 #15239] DEBUG -- :    (48.7ms)  SELECT COUNT() FROM \"posts\"\n=> 298121\nirb(main):003:0>\n```\n(though practically speaking, I've returned to using plain webrick for development so this issue doesn't affect me anymore)\n. ~require: false should also work assuming you're launching puma directly through its command instead of rails server.~ or maybe not.\nApparently rails 5 do read the config though.. This is how my app looks like. Note threads number.\n$ RAILS_MAX_THREADS=17 bundle exec rails s\n/home/edho/apps/ruby24/lib/ruby/gems/2.4.0/gems/activesupport-5.0.1/lib/active_support/xml_mini.rb:51: warning: constant ::Fixnum is deprecated\n/home/edho/apps/ruby24/lib/ruby/gems/2.4.0/gems/activesupport-5.0.1/lib/active_support/xml_mini.rb:52: warning: constant ::Bignum is deprecated\n=> Booting Puma\n=> Rails 5.0.1 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\n/home/edho/apps/ruby24/lib/ruby/gems/2.4.0/gems/activesupport-5.0.1/lib/active_support/core_ext/numeric/conversions.rb:138: warning: constant ::Fixnum is deprecated\nPuma starting in single mode...\n* Version 3.6.2 (ruby 2.4.0-p0), codename: Sleepy Sunday Serenity\n* Min threads: 17, max threads: 17\n* Environment: development\n* Listening on tcp://localhost:3000\nUse Ctrl-C to stop\nconfig file (default from rails 5 iirc).. whoops I dum\n. ",
    "adamyonk": "Thanks!\nOn Aug 12, 2012, at 10:15 AM, Evan Phoenix notifications@github.com wrote:\n\nOops! I guess I shouldn't have made the release so late last night. All fixed now in 1.6.1.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "ralph": "OK, makes sense, thanks!\n. ",
    "emassip": "BTW, running on MRI 1.9.3p125.\n. Thanks!\n. Works again:\nStarted GET \"/sys/fetch_changesets?key=XXX\" for 10.132.21.139 at 2012-08-28 12:31:31 +0200\nProcessing by SysController#fetch_changesets as */*\n  Parameters: {\"key\"=>\"XXX\"}\n  Rendered text template (0.0ms)\nCompleted 200 OK in 6625ms (Views: 0.0ms | ActiveRecord: 453.1ms)\n. The exact same error happens using the -S option so this issue may be some duplicate of #165 (but with 1.6.3).\n. Well, as I said, ESXed Windows 2003 server with MRI 1.9.3p125 installed using RubyInstaller but I think that any win32 setup would lead to the error.\nAbout the software setup I don't think this will change anything either but I was running the Redmine trunk @ http://svn.redmine.org/redmine/trunk with additional Gemfile.local file containing a single 'gem \"puma\"' line in app root dir.\n. ",
    "bwagnerr": "I am having this problem too with version 1.6.1 and both rails 3.2.8 and 3.2.3, starting the server with \"rails s puma -d\", if you reload the page several times (hold f5), rails will stop receiving requests (can be seen on the log), I don't use any special rack file and have no weird configurations on my application...\nAnything more you need to know I'll gladly answer\n. https://gist.github.com/3472763\n. sorry, I should've noticed the purpose too, I will do it as soon as I can, probably tomorrow morning... anyway it wasn't happening before on versions 1.5 and 1.4...\n. I sent the gist to you by email, since it has sensitive information, hope it can help, meanwhile I will go back to version 1.5\n. I just tried it, apparently everything is smooth, I will let it run for the day and see how it goes... Thanks for the attention!\n. All is fine after a whole day of operation, I think we can say the issue is fixed on 1.6.2\nThanks!\n. ",
    "stanislaw": "Current Puma's pumactl is not able to send -9 or USR2 signals to a server, its stop and restart work only if activate_control_app is present and this commit allows controlling restart/stop of server both by using signals or by using control app, do I understand it right?\nIf so, my strong +1 on this work!\n. If this functionality were present, then puma/tools/jungle/* scripts could be significantly simplified (I've just finished russian tutorial using these scripts modified for CentOS - too much complication there: https://github.com/stanislaw/tutorials/blob/master/ru-running-several-ruby-micro-apps-on-one-webserver.md). Running just 'pumactl' to control app by signals without all these \"bundle exec ...\" and manual kills would be much more like PostgreSQL having pg_ctl or other well-known packages of this sort.\n. Any updates here? I am waiting for this to have been merged and released, to suggest a pull request containing simpler init.d scripts relying on pumactl usage. \n. I've checked out the latest work done by @jpascal and tested if pumactl is able to control an app by sending signals - it works perfectly!\nI see the only improvement that can be done on top of this nice work:\nCurrent pumactl does not have start command so it is still coupled with puma. Currently we need puma to be run before restart, stop and other commands of pumactl may be used.\nThe obvious suggestion here is to add run command to pumactl. This task also implies a reading of a config file, as puma does it with -C option.\nIt would be great to have a possibility of running pumactl completely outside of an app's directory, i.e. it should be enough to just pass a config file containing all necessary paths (pid, state, rackup) to pumactl. \nThen, if this functionality was implemented, running puma in bundle exec environment would be an important option (as for me personally, I'd like to see it as a default behavior).\nI hope this is clear enough.\nThanks!\n. +1, I am waiting too for this issue to make any progress.\n. ",
    "nickbarth": "It appears to be adding the JRuby command twice to argv removing that line fixes the issue for me.\npuma-1.6.3-java/lib/puma/jruby_restart.rb\n``` ruby\nrequire 'ffi'\nmodule Puma\n  module JRubyRestart\n    extend FFI::Library\n    ffi_lib 'c'\nattach_function :execlp, [:string, :varargs], :int\nattach_function :chdir, [:string], :int\n\ndef self.chdir_exec(dir, cmd, *argv)\n  chdir(dir)\n  # argv.unshift(cmd)\n  argv = ([:string] * argv.size).zip(argv).flatten\n  argv <<:int\n  argv << 0\n  puts dir\n  execlp(cmd, *argv)\n  raise SystemCallError.new(FFI.errno)\nend\n\nend\nend\n```\n. https://github.com/puma/puma/pull/151\n. Works with RVM. That's good enough for me.\n. ",
    "kyledecot": "I'm also experiencing this w/ my Rails app. It doesn't happen on every page load though. It's very intermittent. Here are a couple of screenshots. As you can see sometimes the raw HTML w/ the headers is spit out and others it seems to inject the headers in the page page.\nhttps://www.evernote.com/shard/s222/sh/0ac8606b-ca14-4a1c-b035-c90c672f7f87/80896e6d533206a8b29d5c564cc9290b\nhttps://www.evernote.com/shard/s222/sh/de4a3520-84f5-4bc7-a588-a105c4a430da/570c451fdfefc5077c80e1e44cec0881\nWhat information about my setup would be useful in tracking down the source of this? I'll start you off w/ my nginx config:\n```\nupstream production {\n  server unix:///home/deployer/production/shared/sockets/puma.sock fail_timeout=0;\n}\nserver {\n    server_name theflowskatepark.com;\n    rewrite ^(.*) http://www.theflowskatepark.com$1 permanent;\n}\nserver {\n  listen 80;\n  server_name www.theflowskatepark.com;\n  root /home/deployer/production/current/public;\nlocation ^~ /assets/ {\n    gzip_static on;\n    expires max;\n    add_header Cache-Control public;\n  }\ntry_files $uri/index.html $uri @production;\n  location @production {\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header Host $http_host;\n    proxy_pass http://production;\n  }\nerror_page 500 502 503 504 /500.html;\n  client_max_body_size 4G;\n  keepalive_timeout 5;\n}\n``\n. Certainly, though I haven't seen the issue when running my project locally. It only seems to happen on my linode server. I'm wondering if this is because locally I runrails s puma` and on production I am running:\ncd /home/deployer/production/current && RAILS_ENV=production bundle exec puma -d -b 'unix:///home/deployer/production/shared/sockets/puma.sock' -S /home/deployer/production/shared/sockets/puma.state --control 'unix:///home/deployer/production/shared/sockets/pumactl.sock' >> /home/deployer/production/shared/log/puma-production.log 2>&1\nIn any case my project is located at https://github.com/kyledecot/the-flow-skatepark.\n. Case #1\nRenders somewhat properly but the headers seems to be displayed in within the page\nhttps://www.evernote.com/shard/s222/sh/0ac8606b-ca14-4a1c-b035-c90c672f7f87/80896e6d533206a8b29d5c564cc9290b\nhttp://snipt.org/vgik0\nCase #2\nRaw HTML rather then a rendered page\nhttps://www.evernote.com/shard/s222/sh/de4a3520-84f5-4bc7-a588-a105c4a430da/570c451fdfefc5077c80e1e44cec0881\nhttp://snipt.org/vgil1\nI should also note that case #1 is much easier to reproduce (perhaps 1/5 page loads). Case #2 might only only happen every 20 page loads. \n. Any progress on this issue?\n. Thanks for all the hard work on this guys!\n. +1\n. ",
    "duyleekun": "Can you share the safari html source of the page when that happens? It may help pinpoint the issue\n. Bump?\n. ",
    "jinzhu": "In your start script, please add '-e production' option. \n. BTW, you need to require 'bundler/setup' before require 'puma/capistrano' in your deploy.rb\n. ",
    "linjunpop": "@jinzhu Thanks, add -e production solve this.\n. @evanphx, @plentz Also update README file?\n. I think these commands should be\nshell\n$ cap puma:start\n$ cap puma:restart\n$ cap puma:stop\nSo it will only trigger Puma tasks. Exclude those tasks which also hook into deploy:stop/start/restart.\n. ",
    "arkadiyk": "If works perfectly fine if I just comment out the allow_chunked = false right after when TRANSFER_ENCODING in the lib/puma/server.rb. \n. The failure is kinda difficult to see, at least using the code I have. \nMost likely the server sends response with encoding set to \"Chunked\" but no boundaries in it.\nBelow is my controller snippet which causes the failure.\n``` ruby\ndef as_excel\n  self.headers[\"Content-Disposition\"] = \"attachment; filename=#{filename}\"\n  self.headers['Last-Modified'] = Time.now.ctime.to_s\n  self.headers[\"Cache-Control\"] = \"no-cache\"\n  self.headers['Transfer-Encoding'] = 'chunked'   # this makes the response unrecognizable by browser\nself.content_type = 'text/xml; charset=utf-8'\ndumper = StreamingExcelDumper.new    # implements \"each\" method for chunked response\nself.response_body = dumper\nend\n```\n. ",
    "alexanderadam": "I'm very sorry to comment on this, quite old issue. I'm not sure whether it is exact the same issue but for me it looks pretty much like it and I didn't want to create a duplicated issue.\nI tried to reproduce a minimal example mentioned here:\n```ruby\nresponse.headers['X-Accel-Buffering'] = 'no'\nresponse.headers['Cache-Control'] = 'no-cache'\nresponse.headers['Transfer-Encoding'] = 'chunked' # this breaks the request\nresponse.headers.delete('Content-Length')\nself.response_body = Enumerator.new do |lines|\n  1000.times do\n    lines << [rand, rand].to_csv\n  end\nend\n```\nMaking a request with chrome results in a Network Error and making the request with cURL results in:\ncurl: (18) transfer closed with outstanding read data remaining\nI don't think it is related to #1480 because petr666 explained that the issue was introduced in version 3.7. But this code above doesn't work in 3.6 either.\n. ",
    "pazustep": "This is not new for jruby-1.7.0, and also happens on jruby-1.6.8\n\u03bb gmaonrails \u2192 \u03bb git master* \u2192 puma -t0:32 -b tcp://127.0.0.1:3000\nPuma 1.6.3 starting...\n* Min threads: 0, max threads: 32\n* Environment: development\n* Listening on tcp://127.0.0.1:3000\nUse Ctrl-C to stop\n* Restarting...\nErrno::EFAULT: Bad address\n  chdir_exec at /Users/pazu/.rvm/gems/jruby-1.6.8@gmaonrails/gems/puma-1.6.3-java/lib/puma/jruby_restart.rb:18\n    restart! at /Users/pazu/.rvm/gems/jruby-1.6.8@gmaonrails/gems/puma-1.6.3-java/lib/puma/cli.rb:114\n         run at /Users/pazu/.rvm/gems/jruby-1.6.8@gmaonrails/gems/puma-1.6.3-java/lib/puma/cli.rb:464\n      (root) at /Users/pazu/.rvm/gems/jruby-1.6.8@gmaonrails/gems/puma-1.6.3-java/bin/puma:10\n        load at org/jruby/RubyKernel.java:1087\n             at /Users/pazu/.rvm/gems/jruby-1.6.8@gmaonrails/gems/puma-1.6.3-java/bin/puma:19\n        eval at org/jruby/RubyKernel.java:1112\n      (root) at /Users/pazu/.rvm/gems/jruby-1.6.8@gmaonrails/bin/ruby_noexec_wrapper:14\n\u03bb gmaonrails \u2192 \u03bb git master* \u2192 ruby -v\njruby 1.6.8 (ruby-1.8.7-p357) (2012-09-18 1772b40) (Java HotSpot(TM) 64-Bit Server VM 1.7.0_09) [darwin-x86_64-java]```\n. ",
    "dentarg": "Same thing is happening on Puma 2.0.0.b3:\n```\n$ bundle exec puma -b tcp://127.0.0.1:3000 -S tmp/puma_state -e production\nPuma 2.0.0.b3 starting...\n Min threads: 0, max threads: 16\n Environment: production\n Listening on tcp://127.0.0.1:3000\nUse Ctrl-C to stop\n Restarting...\nErrno::EFAULT: Bad address\n  chdir_exec at /Users/dentarg/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-2.0.0.b3-java/lib/puma/jruby_restart.rb:18\n    restart! at /Users/dentarg/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-2.0.0.b3-java/lib/puma/cli.rb:107\n  run_single at /Users/dentarg/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-2.0.0.b3-java/lib/puma/cli.rb:467\n         run at /Users/dentarg/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-2.0.0.b3-java/lib/puma/cli.rb:372\n      (root) at /Users/dentarg/.rbenv/versions/jruby-1.7.0/lib/ruby/gems/shared/gems/puma-2.0.0.b3-java/bin/puma:10\n        load at org/jruby/RubyKernel.java:1045\n      (root) at /Users/dentarg/.rbenv/versions/jruby-1.7.0/bin/puma:23\n$ ruby -v\njruby 1.7.0 (1.9.3p203) 2012-10-22 ff1ebbe on Java HotSpot(TM) 64-Bit Server VM 1.6.0_37-b06-434-11M3909 [darwin-x86_64]\n```\n. We also noticed this in an application running on Heroku (the exception information was sent over to Sentry). We upgrade the application from Puma 2.16.0 to 3.4.0 today. Have only seen it once though, and I have no repro for it. If it continues to be a problem I will report back here.\nEOFError: end of file reached\n  from puma/client.rb:166:in `read_nonblock'\n  from puma/client.rb:166:in `try_to_finish'\n  from puma/reactor.rb:73:in `block in run_internal'\n  from puma/reactor.rb:42:in `each'\n  from puma/reactor.rb:42:in `run_internal'\n  from puma/reactor.rb:144:in `block in run_in_thread'\nPuma::ConnectionError: Connection error detected during read\n  from puma/client.rb:170:in `rescue in try_to_finish'\n  from puma/client.rb:165:in `try_to_finish'\n  from puma/reactor.rb:73:in `block in run_internal'\n  from puma/reactor.rb:42:in `each'\n  from puma/reactor.rb:42:in `run_internal'\n  from puma/reactor.rb:144:in `block in run_in_thread'\n. We continue to see Puma::ConnectionError: Connection error detected during read errors.\n. @evanphx any plans in the near future on making a new release with the fix here?\n. @ioquatix I think you are seeing https://bugs.ruby-lang.org/issues/15499\nWe see it for our app using Puma cluster mode, it does not exit on SIGTERM. On Heroku, we are getting R12 - Exit timeout errors.. > This sounds like a bug in ruby 2.6.\nI think so too, 2.6 changed the behaviour of our specs, that started and stopped our application using Puma in cluster mode. I've extracted the code to https://github.com/dentarg/gists/tree/master/gists/ruby-bug-15499 to reproduce the problem. Before 2.6 our specs was able to stop Puma without leaving any Ruby processes behind.\nI made some comments about this in https://bugs.ruby-lang.org/issues/15499 but it hasn't gone anywhere since. I've been thinking about opening a new bug, but we switched our application to use Puma in single mode so the issue hasn't been pressing. I can chime in if anyone here opens a new bug.\n(Also, the Heroku R12 errors went away when switching to single mode.). @MSP-Greg I have tested #1741, see https://github.com/dentarg/gists/tree/master/gists/ruby-bug-15499#261--httpsgithubcompumapumapull1741. Seems like it behaves as 2.5.3 does.. Yes, I think so, at least some of it. Something I didn't do but could be done: try to reproduce the problem on Heroku (but it looks like the original post is doing something similar). Linking this issue to our discussion in https://github.com/puma/puma/issues/1674#issuecomment-451973891. https://bugs.ruby-lang.org/issues/15499 may be the cause of the test failures.. ",
    "glebm": "Same on jruby 1.7.1 + puma 2.0.0.b3\n. Getting this (jruby 1.7.1 + ubuntu 12.04):\n* Restarting...\nErrno::EFAULT: Bad address\n  chdir_exec at /usr/justlanded/apps/deploy/shared/bundle/jruby/1.9/gems/puma-2.0.0.b3-java/lib/puma/jruby_restart.rb:18\n    restart! at /usr/justlanded/apps/deploy/shared/bundle/jruby/1.9/gems/puma-2.0.0.b3-java/lib/puma/cli.rb:107\n  run_single at /usr/justlanded/apps/deploy/shared/bundle/jruby/1.9/gems/puma-2.0.0.b3-java/lib/puma/cli.rb:467\n         run at /usr/justlanded/apps/deploy/shared/bundle/jruby/1.9/gems/puma-2.0.0.b3-java/lib/puma/cli.rb:372\n      (root) at /usr/justlanded/apps/deploy/shared/bundle/jruby/1.9/gems/puma-2.0.0.b3-java/bin/puma:10\n        load at org/jruby/RubyKernel.java:1046\n      (root) at /usr/justlanded/apps/deploy/shared/bundle/jruby/1.9/bin/puma:23\n. Could it be because for_fd is used on unix sockets (https://github.com/puma/puma/blob/master/lib/puma/binder.rb#L246 and https://github.com/puma/puma/blob/master/lib/puma/binder.rb#L144)? There is this bug from 2009 in jruby issue tracker: http://jira.codehaus.org/browse/JRUBY-4099\nUpdate Might be invalid anymore, because it looks like for_fd is there: https://github.com/jruby/jruby/blob/master/src/org/jruby/ext/socket/RubySocket.java#L128\n. A few more random errors:\nRuntimeError (Missing rack.input): https://gist.github.com/426c41bda8ee595ede49\nIn puma ( https://gist.github.com/4240962 ):\nRack app error: #<NoMethodError: undefined method `>' for nil:NilClass>2012-12-08 17:41:03 +0100: Rack app error: #<NoMethodError: undefined method `>' for nil:NilClass>\nIn journey ( https://gist.github.com/4235007 ),  ticketed here: https://github.com/rails/journey/issues/54\nNoMethodError (undefined method `names' for nil:NilClass):\nIn puts to stdout (!) ( https://gist.github.com/4240917 ):\nActionView::Template::Error (not opened for writing)`\n. These happen when the app is first booted (and after restart)\nAll of these errors are random and only happen when I run a lot of concurrent requests (the minimum to reproduce for me is 30)\n. rack-cache is not thread-safe (trunk may be, see https://github.com/rtomayko/rack-cache/commit/40a7c50e76c9b2939b8258312dd9155da74aaf00)\nRails cache is not thread-safe yet either: https://github.com/rails/rails/pull/6917\nI wonder what else is not thread-safe...\nUpdate: Seeing fewer errors after switching to rack-cache from trunk\nThe closed stream error is there a lot \n. Got another error: Bad file descriptor\nhttps://gist.github.com/4254783\n. New info. At the moment of  (IOError) closed stream request env rack.input is a Puma::NullIO! Hope this is useful!\n. 100% haml issue fully identified :) https://github.com/haml/haml/issues/613\n. Yes lol :) Currently devising a non-mutex solution :)\n. Same versions, same problem on startup (first few requests crash like this), cluster mode\n. These are threads. Shift+h will hide userland threads in htop.\n. ",
    "sairam": "The current code does not create sockets directory if it does not exist. (Setup should be handled to do that )\n. Was this ever solved ? #313 also reports similar issue \nI also am facing the same problem with rails 4.\n. I set serve static assets to true in rails and its working .\n. ",
    "RKushnir": "I think, rack_env is a very different thing than rails_env, so it's not correct to derive one from another. See https://github.com/rack/rack/blob/575bbcba780d9ba71f173921aa1fcb024890b867/lib/rack/server.rb#L157-L162\n. ",
    "ramsees": "What's the default timeout then?\n. Thank you.\n. ",
    "sobrinho": "It's fixed on master, closing.\n. @evanphx there is no output on daemon mode.\nI will check tomorrow what happens without the daemon mode, I'm without access to the server right now.\n. ",
    "gfrancesco": "Same here, on 2.0.0.b3 I noticed the following:\n- using cap puma:start task won't start puma, process is stuck. I have to ditch the --control ..... part\n- with that modified recipe puma works, but pid in puma.state is not the real one (from ps aux), so when restarting with cap puma:restart I got \"No pid found\"\n. ",
    "scottkf": "I have the same issue on 2.0.0.b4, but only when daemonized. If I removed the -d switch it'll work just fine. In production I've been just backgrounding it using &\n. ",
    "toru": "Jumping on the wagon. I'm also using 2.0.0.b4 (ecdb3c870c4b38a1584029bdb6fda311fed2dae4) and experiencing the same problem (server becomes totally unresponsive).\nThis issue only arises when daemonizing the process while supplying the --control server option.\nFranky I'm happy to run Puma in the foreground and use something like god, supervisord, daemontools or whatever for reliably running a  daemon but it would be fantastic if this issue could be resolved.\n. ",
    "kirantpatil": "Hey delete the puma.socket and then start the server.\n. ",
    "Juanmcuello": "It still happens with 2.3.0. The process never stops when sending a 'SIGTERM'.\nIt's easy to reproduce, just start the server demonized and with the control server:\n$ bundle exec puma -b unix:///tmp/puma.sock --control unix:///tmp/pumactl.sock -d\n$ kill {pid_number}\nAnd the servers does not stop, it continues running.\nIf you start the server without the control server (or not demonized):\n$ bundle exec puma -b unix:///tmp/puma.sock --control unix:///tmp/pumactl.sock\n$ kill {pid_number}\nYou will see:\n- Gracefully stopping, waiting for requests to finish\n- Goodbye!\n. I rebased this PR due to  0fc3bde80a3ee57347448e2b33295a0b24321b1f. This PR introduces phased_restart task.\nI also removed unnecessary fetch methods. If I'm not missing something, these vars will always exist as they are defined at the top, so there is no need to set fallback values (so no need to use fetch).\n. Ups, sorry. Changed to ruby 1.8 syntax.\n. I think this is duplicate of #383.\nAlso take a look at https://github.com/seuros/capistrano-puma\n. ",
    "mattscilipoti": "I see that the start command within railties (lib/rails/commands) calls super, but I can't find what the various options for 'super' are. I have also reviewed many references to Rails within 'thin'.\nI found a Changelog entry entitled \"Added Thin support to script/server. #488 [Bob Klosinski]\" from Oct. of 2008, but that code area has changed significantly since that commit (a93ea88c0623b4f65af98c0eb55924c335bb3ac1).\nIf someone could direct me to the right section of code, that would be very helpful.\n. Thanks.  I couldn't tell if it was done from within Thin (via monkey patch, similar to an Engine) or Rails.\n. ",
    "rubiii": "If you think it should error out, I could take a look at how to do it. In case you want me to.\n. Does this work for you?\n. Duplicate of #163.\n. @dbussink see #174\n. thanks dude!\n. ",
    "dbussink": "Guess this should also be done for JRuby\n. The pull request doesn't change it for JRuby though\n. @evanphx Looks like this was not included in the reorganization of these files? Do you think it could be added back safely? Seems to cause performance regressions compared to Puma 1.6.3 for example.\n. Oh, wait, nvm, it seems it's not included for other files such as io_buffer.c that seems the cause.\n. ",
    "nixme": "I could write a patch? Should all usage of syswrite be checked and looped until fully written, or just the parts that are variable sized?\n. Thanks for the quick turnaround!\n. Just hit this to for a customer of ours, but we don't see it locally (either dev or prod) for the same JSON responses. Chrome. Also using Oj. Wondering if there's a miscount somehow on the response. I'm going to dig into it.\n. A question for everyone who's hit this: Are you using Heroku? Have you seen it at all in any other hosting environment?\n. ",
    "nicolai86": "I've just found a reference to https://github.com/puma/puma/issues/126 .\nThat's exactly my issue. So I guess it'll be addressed\n. thanks!\n. @steverandy I wrote a short blog post about how to use phased restarts just yesterday:  http://blog.nicolai86.eu/posts/2013-02-06/phased-restarts-using-puma/\ndoes that help you to get going?\n. ",
    "prcongithub": "Is this resolved? I am getting the below error with JRuby-9.1.7.0 and Puma-3.7.0-java. This happens pretty randomly. Restarting Puma fixes the issue.\nNoMethodError (undefined method names' for nil:NilClass):\n  actionpack (4.2.5.1) lib/action_dispatch/journey/router.rb:116:inblock in find_routes'\n  org/jruby/RubyArray.java:2517:in map!'\n  actionpack (4.2.5.1) lib/action_dispatch/journey/router.rb:113:infind_routes'\n  actionpack (4.2.5.1) lib/action_dispatch/journey/router.rb:30:in serve'\n  actionpack (4.2.5.1) lib/action_dispatch/routing/route_set.rb:815:incall'\n  exception_notification (4.2.1) lib/exception_notification/rack.rb:32:in call'\n  warden (1.2.7) lib/warden/manager.rb:36:inblock in call'\n  org/jruby/RubyKernel.java:1120:in catch'\n  warden (1.2.7) lib/warden/manager.rb:35:incall'\n  rack (1.6.5) lib/rack/etag.rb:24:in call'\n  rack (1.6.5) lib/rack/conditionalget.rb:25:incall'\n  rack (1.6.5) lib/rack/head.rb:13:in call'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/params_parser.rb:27:incall'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/flash.rb:260:in call'\n  rack (1.6.5) lib/rack/session/abstract/id.rb:225:incontext'\n  rack (1.6.5) lib/rack/session/abstract/id.rb:220:in call'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/cookies.rb:560:incall'\n  activerecord (4.2.5.1) lib/active_record/query_cache.rb:36:in call'\n  activerecord (4.2.5.1) lib/active_record/connection_adapters/abstract/connection_pool.rb:653:incall'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/callbacks.rb:29:in block in call'\n  activesupport (4.2.5.1) lib/active_support/callbacks.rb:88:inrun_callbacks'\n  activesupport (4.2.5.1) lib/active_support/callbacks.rb:778:in _run_call_callbacks'\n  activesupport (4.2.5.1) lib/active_support/callbacks.rb:81:inrun_callbacks'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/callbacks.rb:27:in call'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/remote_ip.rb:78:incall'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/debug_exceptions.rb:17:in call'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/show_exceptions.rb:30:incall'\n  railties (4.2.5.1) lib/rails/rack/logger.rb:38:in call_app'\n  railties (4.2.5.1) lib/rails/rack/logger.rb:20:inblock in call'\n  activesupport (4.2.5.1) lib/active_support/tagged_logging.rb:68:in block in tagged'\n  activesupport (4.2.5.1) lib/active_support/tagged_logging.rb:26:intagged'\n  activesupport (4.2.5.1) lib/active_support/tagged_logging.rb:68:in tagged'\n  railties (4.2.5.1) lib/rails/rack/logger.rb:20:incall'\n  actionpack (4.2.5.1) lib/action_dispatch/middleware/request_id.rb:21:in call'\n  rack (1.6.5) lib/rack/methodoverride.rb:22:incall'\n  rack (1.6.5) lib/rack/runtime.rb:18:in call'\n  activesupport (4.2.5.1) lib/active_support/cache/strategy/local_cache_middleware.rb:28:incall'\n  rack (1.6.5) lib/rack/sendfile.rb:113:in call'\n  railties (4.2.5.1) lib/rails/engine.rb:518:incall'\n  railties (4.2.5.1) lib/rails/engine.rb:-1:in call'\n  railties (4.2.5.1) lib/rails/application.rb:165:incall'\n  puma-3.7.0 (java) lib/puma/configuration.rb:226:in call'\n  puma-3.7.0 (java) lib/puma/server.rb:578:inhandle_request'\n  puma-3.7.0 (java) lib/puma/server.rb:415:in process_client'\n  puma-3.7.0 (java) lib/puma/server.rb:275:inblock in run'\n  puma-3.7.0 (java) lib/puma/thread_pool.rb:120:in `block in spawn_thread'\n. ",
    "cryo28": "Sure\n```\nrequire 'pathname'\nAPP_PATH  = Pathname.new('/srv/projects/come2me')\nCURRENT   = APP_PATH.join('current')\nSHARED    = APP_PATH.join('shared')\nsocket    = \"unix://\" + CURRENT.join('tmp/sockets/puma.sock').to_s\nSee Puma::Configuration::DSL\nbind        socket\npidfile     SHARED.join(\"pids/puma.pid\")\nthreads     1, 16\nstate_path  SHARED.join('pids/puma.state')\nworkers 4    # --- uncommenting this causes 100% cpu usage\ndirectory   CURRENT\nrackup \"config.ru\"\n```\nAnd this is the output of htop pointed at the upstart script running puma server. Please note 100% cpu is eaten by the master puma process, not the individual workers which are idle. \n. If this helps, strace and ltrace outputs are as follows: https://gist.github.com/4396552\n. @evanphx 1.9.3-p125\n. @evanphx It looks like the problem is somewhere around this code in Puma::CLi#run_cluster:\nruby\nwhile !stop\n  begin\n     IO.select([read], nil, nil, 5)\n     check_workers\n  rescue Interrupt\n     stop = true\n  end\nend\nWhen puma cluster starts some of the child processes sends \"!\" on SIGCHLD signal to read IO. So IO.select immediately returns something like \nruby\n [[#<IO:fd 6>], [], []]\nHowever, this \"!\" is never read from the IO:fd6 stream, and 5-second timeout to IO.select becomes worthless as read stream contains this unread \"!\" character and the code above runs infinite loop eating up 100% CPU.\nChanging to the following code seems to have done the trick for me. However, I am uncertain whether this doesn't break child-master signaling and hot restarts.\nruby\nwhile !stop\n  begin\n    result = IO.select([read], nil, nil, 5)\n    log result.flatten.first.read_nonblock(255) if result\n    check_workers\n  rescue Interrupt\n    stop = true\n  end\nend\n. @Wijnand Not yet. I returned to non-clustered mode for the time being.\n. ",
    "Wijnand": "Hi @cryo28, \ndid you already find out if this breaks signaling or hot restarts?\n. @nicolai86 thank you for that blogpost!\n@evanphx thank you for this commit, this was the last piece of the puzzle I needed to switch to puma\n. ",
    "ozzyaaron": "I'm hitting this problem with the latest Puma (2.11.3) in development after I have used the debugger. There is an issue in byebug that seems to be getting worked on https://github.com/deivid-rodriguez/byebug/issues/144\n. ",
    "gregmolnar": "I have a similar issue on Ubuntu with master and ruby 2.0.0.p0. I am using a socket instead of a tcp port so my error is\n/.../shared/bundle/ruby/2.0.0/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:246:in `for_fd': Bad file descriptor (Errno::EBADF)\nfrom /.../shared/bundle/ruby/2.0.0/bundler/gems/puma-2e2440024623/lib/puma/binder.rb:246:in `inherit_unix_listener'\nfrom /.../shared/bundle/ruby/2.0\n. hi Ben,\nCan you provide some more details please? Maybe a simple PoC app?\n. ",
    "brentkirby": "+1 on this, same exact message as masterkain above, but using sockets\n. ",
    "alindeman": "Can you release a version to rubygems.org with this fix?\n. ",
    "flackou": "+1 (using TCP port, not socket)\n. ",
    "muhammet": "is it fixed ?\nI'm getting following error on ubuntu for production\nbundle/ruby/2.2.0/gems/activesupport-4.2.5/lib/active_support/core_ext/kernel/agnostics.rb:7:in ``': Bad file descriptor (Errno::EBADF)\n. ",
    "joeczucha": "If anyone else comes here searching for a solution to the above \n/home/deploy/.rbenv/versions/2.3.1/lib/ruby/gems/2.3.0/gems/activesupport-4.2.6/lib/active_support/core_ext/kernel/agnostics.rb:7:in ``': Bad file descriptor (Errno::EBADF)\nIt was being caused on my system by me having missed out a comma in the stdout_redirect line.\ni.e.\nstdout_redirect \"#{root}/shared/log/puma.log\" \"#{root}/shared/log/puma_error.log\"\nin my config/puma.rb, file when it should be:\nstdout_redirect \"#{root}/shared/log/puma.log\", \"#{root}/shared/log/puma_error.log\"\nHope this helps someone :)\n. ",
    "Gaolz": "@joeczucha Thanks so much, Your answer solves my problem.\n. ",
    "revathskumar": "@joeczucha Thanks, For me it was log folder which was missing. \n. ",
    "straight-shoota": "I stumbled upon the same error as @muhammet and @joeczucha and @revathkumar, but it's neither a missing comma nor log folder. And the error message was really not helping in any way :/\nFull log output:\n[17434] Puma starting in cluster mode...\n[17434] * Version 3.6.2 (ruby 2.3.1-p112), codename: Sleepy Sunday Serenity\n[17434] * Min threads: 0, max threads: 16\n[17434] * Environment: production\n[17434] * Process workers: 1\n[17434] * Preloading application\n[17434] * Listening on tcp://0.0.0.0:0\n[17434] * Listening on unix:///data/web/kath-cms/shared/puma.sock\n[17434] Use Ctrl-C to stop\n/home/kathmin/.gem/ruby/2.3.1/gems/activesupport-5.0.1/lib/active_support/core_ext/kernel/agnostics.rb:7:in ``': Bad file descriptor (Errno::EBADF)\n        from /home/kathmin/.gem/ruby/2.3.1/gems/activesupport-5.0.1/lib/active_support/core_ext/kernel/agnostics.rb:7:in ``'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/source/git/git_proxy.rb:161:in `block (2 levels) in git'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/source/git/git_proxy.rb:233:in `block in capture_and_filter_stderr'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/2.3.0/tempfile.rb:295:in `open'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/source/git/git_proxy.rb:231:in `capture_and_filter_stderr'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/source/git/git_proxy.rb:161:in `block in git'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/shared_helpers.rb:72:in `with_clean_git_env'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/source/git/git_proxy.rb:160:in `git'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/source/git/git_proxy.rb:88:in `full_version'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/env.rb:78:in `git_version'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/env.rb:22:in `report'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/friendly_errors.rb:74:in `request_issue_report_for'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/friendly_errors.rb:40:in `log_error'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/friendly_errors.rb:102:in `rescue in with_friendly_errors'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/friendly_errors.rb:100:in `with_friendly_errors'\n        from /opt/rubies/ruby-2.3.1/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/exe/bundle:26:in `<top (required)>'\n        from /opt/rubies/ruby-2.3.1/bin/bundle:23:in `load'\n        from /opt/rubies/ruby-2.3.1/bin/bundle:23:in `<main>'\npuma config:\n````\n!/usr/bin/env puma\nAnsible managed\nenvironment ENV['RAILS_ENV'] || ENV['RACK_ENV'] || 'production'\nshared = \"/data/web/kath-cms/shared\"\ndirectory \"/data/web/kath-cms\"\nrackup \"/data/web/kath-cms/current/config.ru\"\npidfile \"#{shared}/puma.pid\"\nstate_path \"#{shared}/puma.state\"\nactivate_control_app \"unix://#{shared}/pumactl.sock\", { no_token: true }\nport 0\nbind \"unix://#{shared}/puma.sock\"\npreload_app!\nworkers Integer(ENV['PUMA_WORKERS'] || 1)\nthreads Integer(ENV['MIN_THREADS']  || 0), Integer(ENV['MAX_THREADS'] || 16)\nstdout_redirect '/data/web/kath-cms/logs/puma.log', '/data/web/kath-cms/logs/puma.error.log', true\non_restart do\n  puts 'Refreshing Gemfile'\n  ENV[\"BUNDLE_GEMFILE\"] = \"/data/web/kath-cms/current/Gemfile\"\nend\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\nAllow puma to be restarted by touching tmp/restart.txt\nplugin :tmp_restart if respond_to? :plugin # if an app is running an older version of puma\n````\nThe reason was: The log files were not writable for the executing user. I missed this at first, because the log files are symlinked.\nIt would be nice if puma could offer a meaningful error message that could actually assist in solving a problem.. ",
    "niwo": "+1\nI would like to see a init script for RedHat/CentOS too. Are there any examples around?\n. I got something working on CentOS 6 bases on this blogpost: http://blog.hostpro.ua/cloud-hostpro-zapusk-neskolkih-ruby-mikro-prilozheniy-na-odnom-veb-servere/\nsee https://gist.github.com/4526179\n. ",
    "leehambley": "I have the same, I couldn't diagnose it; I've temporarily switched to thin, it's only a development question.\n. Same p194\nSent from my Galaxy S3.\nOn 20 Mar 2013 20:30, \"Petteri R\u00e4ty\" notifications@github.com wrote:\n\n@evanphx https://github.com/evanphx MRI 1.9.3\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/179#issuecomment-15198114\n.\n. I moved away from Puma, Evan. Please feel free to close it, I'll come and reopen whenever I can reproduce, if ever!\n. \n",
    "betelgeuse": "@evanphx MRI 1.9.3\n. MRI 2.0.0p195 with puma 2.2.2 on rails 3.2.12 does not show the issue. Testing some other combos.\n. Still an issue with MRI 1.9.3:\n$ ruby -v\nruby 1.9.3p392 (2013-02-22 revision 39386) [x86_64-darwin12.3.0]\n$ ./script/rails s puma -b localhost \n=> Booting Puma\n=> Rails 3.2.12 application starting in development on http://localhost:3000\n=> Call with -d to detach\n=> Ctrl-C to shutdown server\nPuma 2.2.2 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://localhost:3000\n2013-07-05 16:30:18 +0300: HTTP parse error, malformed request (): #<Puma::HttpParserError: HTTP element HEADER is longer than the (1024 * (80 + 32)) allowed length.>\n2013-07-05 16:30:18 +0300: ENV: {\"rack.version\"=>[1, 1], \"rack.errors\"=>#<IO:<STDERR>>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"CONTENT_TYPE\"=>\"text/plain\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"2.2.2\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n. The same as in original report.\nOn 6.7.2013, at 7.39, Evan Phoenix notifications@github.com wrote:\n\n@betelgeuse What was the request you sent?\n\u2014\nReply to this email directly or view it on GitHub.\n. My app also loads rugged so maybe that is not a coincidence.\n\nOn 18.7.2013, at 19.17, Samuel Flores notifications@github.com wrote:\n\nI'm facing the same problem in a Sinatra application. In my environment the issue only happens when I require the rugged gem, even if I do nothing with the lib.\nHere's an application that reproduces the problem: https://github.com/samflores/puma_issue_179\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "samflores": "I'm facing the same problem in a Sinatra application. In my environment the issue only happens when I require the rugged gem, even if I do nothing with the lib. \nHere's an application that reproduces the problem: https://github.com/samflores/puma_issue_179\n. Thanks for the quick solution! :heart: :heart: \n. ",
    "brianlaw033": "I\u2019m running into the same issue as @mgharbik\nAny solutions?. ",
    "wellington1993": "I have the same problem:\n2018-08-24 12:11:06 -0300: HTTP parse error, malformed request (): #\nYes, I'm receiving large headers.. Can I solve this?\nThanks!. ",
    "sekmo": "Did you happen to run a workaround for these long GET requests?. any news from the front?. Nothing new from the front?. ",
    "jhsu": "I can verify this happening too. Instead of -d, i'm passing a config (-C) with:\nruby\ndaemonize true\n. ",
    "rrouse": "If you move write_pid and write_state under redirect_io in both run_single and run_clustered, it will write the right PID.\nThere are some issues with the clustered side, but those problems were already there prior to this change.\n. ",
    "dylansm": "Still happening with latest from master.\n. Update: I've narrowed it down to specifying workers in app/config/puma.rb \u2014 even workers 1 will do it, so it has something to do with how the status file is written in cluster mode.\n. ",
    "roeme": "Same issue here. it might work in a non-cluster mode, but it still fails under cluster mode. See details here:\nhttps://github.com/puma/puma/blob/master/lib/puma/cluster.rb#L265-L276\n. Uhm yes, to clarify, this is not about how to specify the log file, but the failure of puma to use stdout_redirect when daemonizing. Maybe the issue text was too verbose.\n. ",
    "gamead": "It looks like Rufus Scheduler doesn't run with daemonized puma again,I'm in version 2.8.1,any help?\n. ",
    "djoos": "FYI: same issue thing on 2.9.0.\nTemp workaround: omit the --daemon flag and use nohup instead? #sneaky\nHTH!\n. ",
    "puzanov": "Still not working with Puma 2.9.1\nRufus version is 3.0.7\n. Rufus 3.0.9 is also not working with Puma 2.9.1\nPlease help! \n. Anyone?\n. How to fix that?\nOn Thu, Oct 16, 2014 at 10:03 PM, Evan Phoenix notifications@github.com\nwrote:\n\nThe problem is, mostly, that Rufus scheduler starts up threads at booted\nand they don't carry over properly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/183#issuecomment-59386038.\n. \n",
    "ike-bloomfire": "try \nsudo kill -9 <pid>\n. ",
    "tolot27": "Version 2.0.0.b6, as I wrote in the issue text. ;-)\nYou mean the closed issue #180. Unfortunatelly, it is not fixed.\nBut I cannot verify the version of puma directly. If I run 'puma\n--version' it writes 'puma: version unknown'.\nBut \"gem list puma\" returns:\npuma (2.0.0.b6)\nHence, the bug #180 is not fully fixed.\n2013/2/8 Evan Phoenix notifications@github.com\n\nA bug that sounds just like this was fixed in b5. What version did you try with?\n. \n",
    "pointlessone": "Looks like I'm hitting this too.\nSymptoms are as described by @concept47. pumactl emits \"Command restart sent success\" and I see in top how puma tries to restart but then just exits. stderr.log has the following backtrace.\n```\nAn exception occurred running /home/deployer/.rvm/gems/rbx-head@myapp/bin/ruby_noexec_wrapper\n    Address already in use - bind(2) (Errno::EADDRINUSE)\nBacktrace:\n                          Errno.handle at kernel/common/errno.rb:19\n    UNIXSocket(UNIXServer)#unix_setup at /home/deployer/.rvm/rubies/rbx-head/lib/19/socket.rb:1162\n                UNIXServer#initialize at /home/deployer/.rvm/rubies/rbx-head/lib/19/socket.rb:1219\n       Puma::Binder#add_unix_listener at /home/deployer/.rvm/gems/rbx-head@myapp/gems/puma-2.0.0.b7/lib/puma/binder.rb:234\n            { } in Puma::Binder#parse at /home/deployer/.rvm/gems/rbx-head@myapp/gems/puma-2.0.0.b7/lib/puma/binder.rb:96\n                            Array#each at kernel/bootstrap/array.rb:68\n                   Puma::Binder#parse at /home/deployer/.rvm/gems/rbx-head@myapp/gems/puma-2.0.0.b7/lib/puma/binder.rb:64\n                 Puma::CLI#run_single at /home/deployer/.rvm/gems/rbx-head@myapp/gems/puma-2.0.0.b7/lib/puma/cli.rb:414\n                        Puma::CLI#run at /home/deployer/.rvm/gems/rbx-head@myapp/gems/puma-2.0.0.b7/lib/puma/cli.rb:402\n                    Object#script at /home/deployer/.rvm/gems/rbx-head@myapp/gems/puma-2.0.0.b7/bin/puma:10\n                   Kernel(Object)#load at kernel/common/kernel.rb:586\n             { } in Object#script at /home/deployer/.rvm/gems/rbx-head@myapp/bin/puma:19\n Rubinius::BlockEnvironment#call_on_instance at kernel/common/block_environment.rb:75\n                   Kernel(Object)#eval at kernel/common/eval.rb:75\n                    Object#script at /home/deployer/.rvm/gems/rbx-head@myapp/bin/ruby_noexec_wrapper:14\n      Rubinius::CodeLoader#load_script at kernel/delta/codeloader.rb:68\n      Rubinius::CodeLoader.load_script at kernel/delta/codeloader.rb:118\n               Rubinius::Loader#script at kernel/loader.rb:615\n                 Rubinius::Loader#main at kernel/loader.rb:816\n```\n- Rubinius 2.0.0.rc1 (rubinius/rubinius@84224c2cc544fc0303791121f5e478e4feb0b76a)\n- Puma 2.0.0.b7\nCommand for restart is:\nbin/bundle exec pumactl --state tmp/pids/puma.state restart\n. ",
    "nicolas-brousse": "I have a similar problem when I use a config file.\nIf I do this command bundle exec puma -e production -b unix://tmp/sockets/puma.sock and I stop process with CTRL + C the unix socket file is removed.\nBut if I set my configs into a file and I run bundle exec puma -C ./config/puma.rb and I stop process. Unix socket is not removed.\n. ",
    "robotmay": "Happening for me on Ruby 2.0, Puma 2.0.1 run in a similar way to @nicolas-brousse. Anyone got a better solution than forcibly deleting the sockets as part of my deploy script? :\\\nEdit: Tried it in 2.1.0 too, still no dice.\n. ",
    "soupmatt": "This is broken for me as well on puma 2.4.0 under both mri 2.0 and jruby 1.7.4.\n. This has been fixed under 2.5.1\n. ",
    "phemmer": "I'm having this exact same issue as well. It also occurs on shutdown (SIGTERM to master). It's completely random. Sometimes all workers shut down, sometimes just one of them.\n- Ruby 1.9.3p0 from ubuntu 12.04\n- Rails 3.2.13\n- Puma 2.0.1\nExecuting with bundle exec puma --daemon --bind tcp://127.0.0.1:3000  --pidfile /path/to/pidfile -w 3\nHere is a backtrace of one of the workers that didn't shut down after a SIGTERM stuck in the futex call:\n\n0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n(gdb) bt\n#0  0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n#1  0x00007f23954121a9 in native_cond_wait (cond=, mutex=) at thread_pthread.c:307\n#2  0x00007f2395415f90 in native_sleep (th=0x603520, timeout_tv=0x0) at thread_pthread.c:908\n#3  0x00007f239541782b in sleep_forever (th=0x603520, deadlockable=1) at thread.c:855\n#4  0x00007f239541789d in thread_join_sleep (arg=140733465907824) at thread.c:688\n#5  0x00007f239531844b in rb_ensure (b_proc=0x7f2395417860 , data1=140733465907824, e_proc=0x7f2395411fa0 , data2=140733465907824)\n    at eval.c:736\n#6  0x00007f2395414078 in thread_join (target_th=0x2d9ad40, delay=) at thread.c:721\n#7  0x00007f23954141a2 in thread_join_m (argc=0, argv=0x7f239429e208, self=) at thread.c:802\n#8  0x00007f239540abe1 in vm_call_cfunc (me=0x719170, blockptr=0x0, recv=, num=0, reg_cfp=0x7f239439da90, th=) at vm_insnhelper.c:404\n#9  vm_call_method (th=, cfp=0x7f239439da90, num=, blockptr=0x0, flag=, id=, me=0x719170, recv=58423080)\n    at vm_insnhelper.c:534\n#10 0x00007f2395401b52 in vm_exec_core (th=, initial=) at insns.def:1015\n#11 0x00007f239540755d in vm_exec (th=0x603520) at vm.c:1220\n#12 0x00007f239540d385 in invoke_block_from_c (cref=0x0, blockptr=0x0, argv=0x0, argc=0, self=10399800, block=0x7f239439dc18, th=0x603520) at vm.c:624\n#13 vm_yield (th=0x603520, argv=0x0, argc=0) at vm.c:654\n#14 rb_yield_0 (argv=0x0, argc=0) at vm_eval.c:740\n#15 rb_yield (val=6) at vm_eval.c:747\n#16 0x00007f239531820a in rb_protect (proc=0x7f239540cef0 , data=6, state=0x7fff103e581c) at eval.c:711\n#17 0x00007f239537dd02 in rb_f_fork (obj=) at process.c:2792\n#18 rb_f_fork (obj=) at process.c:2780\n#19 0x00007f239540abe1 in vm_call_cfunc (me=0x6fab40, blockptr=0x7f239439dc18, recv=, num=0, reg_cfp=0x7f239439dbf0, th=) at vm_insnhelper.c:404\n#20 vm_call_method (th=, cfp=0x7f239439dbf0, num=, blockptr=0x7f239439dc18, flag=, id=, me=0x6fab40, recv=10399800)\n    at vm_insnhelper.c:534\n#21 0x00007f2395401b52 in vm_exec_core (th=, initial=) at insns.def:1015\n#22 0x00007f239540755d in vm_exec (th=0x603520) at vm.c:1220\n#23 0x00007f239540d15e in invoke_block_from_c (cref=0x0, blockptr=0x0, argv=0x7fff103e5b08, argc=1, self=10399800, block=0x7f239439dd20, th=0x603520) at vm.c:624\n#24 vm_yield (th=0x603520, argv=0x7fff103e5b08, argc=1) at vm.c:654\n#25 rb_yield_0 (argv=0x7fff103e5b08, argc=1) at vm_eval.c:740\n#26 rb_yield (val=3) at vm_eval.c:750\n#27 0x00007f2395351891 in int_dotimes (num=7) at numeric.c:3290\n#28 0x00007f239540abe1 in vm_call_cfunc (me=0x6978a0, blockptr=0x7f239439dd20, recv=, num=0, reg_cfp=0x7f239439dcf8, th=) at vm_insnhelper.c:404\n#29 vm_call_method (th=, cfp=0x7f239439dcf8, num=, blockptr=0x7f239439dd20, flag=, id=, me=0x6978a0, recv=7)\n    at vm_insnhelper.c:534\n#30 0x00007f2395401b52 in vm_exec_core (th=, initial=) at insns.def:1015\n#31 0x00007f239540755d in vm_exec (th=0x603520) at vm.c:1220\n#32 0x00007f239540e7f8 in rb_iseq_eval (iseqval=17853160) at vm.c:1447\n#33 0x00007f2395319d35 in rb_load_internal (fname=17855800, wrap=) at load.c:310\n#34 0x00007f2395319e98 in rb_f_load (argc=, argv=) at load.c:383\n#35 0x00007f239540abe1 in vm_call_cfunc (me=0x706da0, blockptr=0x0, recv=, num=1, reg_cfp=0x7f239439df08, th=) at vm_insnhelper.c:404\n#36 vm_call_method (th=, cfp=0x7f239439df08, num=, blockptr=0x0, flag=, id=, me=0x706da0, recv=6719200)\n    at vm_insnhelper.c:534\n#37 0x00007f2395401b52 in vm_exec_core (th=, initial=) at insns.def:1015\n#38 0x00007f239540755d in vm_exec (th=0x603520) at vm.c:1220\n#39 0x00007f239540e8df in rb_iseq_eval_main (iseqval=17840040) at vm.c:1461\n#40 0x00007f23953170b2 in ruby_exec_internal (n=0x11037a8) at eval.c:204\n#41 0x00007f2395317b9d in ruby_exec_node (n=0x11037a8) at eval.c:251\n#42 0x00007f239531974e in ruby_run_node (n=0x11037a8) at eval.c:244\n#43 0x00000000004007db in main (argc=11, argv=0x7fff103e6508) at main.c:38\n\nHere's info threads\n\n(gdb) info threads\n  Id   Target Id         Frame \n  15   Thread 0x7f23958ef700 (LWP 4290) \"ruby1.9.1\" 0x00007f2394fea033 in select () from /lib/x86_64-linux-gnu/libc.so.6\n  14   Thread 0x7f2391640700 (LWP 4291) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  13   Thread 0x7f2390084700 (LWP 4705) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  12   Thread 0x7f2386807700 (LWP 4706) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  11   Thread 0x7f2386685700 (LWP 4707) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  10   Thread 0x7f2386503700 (LWP 4708) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  9    Thread 0x7f2386381700 (LWP 4709) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  8    Thread 0x7f23861ff700 (LWP 4710) \"ruby1.9.1\" 0x00007f2394cee89c in __lll_lock_wait () from /lib/x86_64-linux-gnu/libpthread.so.0\n  7    Thread 0x7f238607d700 (LWP 4711) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  6    Thread 0x7f2385efb700 (LWP 4712) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  5    Thread 0x7f2385d79700 (LWP 4713) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  4    Thread 0x7f2385bf7700 (LWP 4720) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  3    Thread 0x7f2385a75700 (LWP 4727) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n  2    Thread 0x7f23859f4700 (LWP 4728) \"SignalSender\" 0x00007f2394cedfd0 in sem_wait () from /lib/x86_64-linux-gnu/libpthread.so.0\n* 1    Thread 0x7f23958e5700 (LWP 4288) \"ruby1.9.1\" 0x00007f2394cebd84 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib/x86_64-linux-gnu/libpthread.so.0\n\nI can reproduce this fairly easily if you need more info.\n. The behavior of the cd and chdir is the same. If /home/user/apps/app/current is a symlink, both methods will run the application within the symlink's target directory.\nHowever here's where it gets interesting.\nWhen updating the symlink, and sending a SIGUSR2, the application does properly change to the new directory, however it's still running the code from the previous directory.\n```\ncat /etc/init/puma_app-web-1.conf\nstart on starting puma_app-web\nstop on stopping puma_app-web\nrespawn\nsetuid testuser\nchdir /tmp/workdir\nexec bundle exec puma --dir /tmp/workdir --config config/puma.rb\nls -l /tmp/workdir\nlrwxrwxrwx 1 root root 1 May 26 06:41 /tmp/workdir -> a\ngrep 'This is' /tmp/{a,b}/config/puma.rb\n/tmp/a/config/puma.rb: body = 'This is: AAAAAAAA'\n/tmp/b/config/puma.rb: body = 'This is: BBBBBBBB'\ninitctl start puma_app\npuma_app start/running\ncurl http://localhost:8000\nThis is: AAAAAAAA\nls -l /proc/7249/cwd\nlrwxrwxrwx 1 testuser testuser 0 May 26 06:44 /proc/7249/cwd -> /tmp/a\nrm /tmp/workdir\nln -s b /tmp/workdir\nkill -USR2 7249\ncurl http://localhost:8000\nThis is: AAAAAAAA\nls -l /proc/7249/cwd\nlrwxrwxrwx 1 testuser testuser 0 May 26 06:44 /proc/7249/cwd -> /tmp/b\n```\nNotice how after the SIGUSR2, the app is still responding with This is: AAAAAAAA, even though /proc/7249/cwd shows it's running out of /tmp/b.\nSomething is going wrong in puma. But I don't know what.\nI know upstart does do some evil stuff in regards to how it handles processes. With the previous method, you had su running sh running puma. With the new upstart both the su and sh are gone, meaning upstart is directly running puma now. It's entirely possible the way upstart works is interfering with the way puma works.\n. ",
    "leifcr": "@evanphx I seem to experience this when using this testing application: https://github.com/leifcr/capistrano-testing\nThe paths for the deployment script is horrible, as it is used for devel/testing.\nI am running puma under runit, but it seems to happen occasionally when running puma manually as well.\nI still haven't found a way to reproduce it consistantly, but I do think a \"kill after timeout x\" is the best option, as that should take care of workers that are just hanging.\nCould this be triggering if db connections and or logs are not properly closed?\nIt seems to trigger more often if I am reloading the app (curl) just when the phased restart is going to happen\nSeems to do the same for restart and phased restart\n. This is a bundle error and not directly related to puma, although a solution would be appreciated.\nWhen you fire up a command using bundle exec, it \"locks\" the running task to run in that folder using that specific Gemfile.\nSince current_path is a symlink to a release folder, bundle will look for the Gemfile it is running with in the folder you originally started puma using \"bundle exec puma ....\"\nI think there must be a way to get around this. \nIdeas:\n- Start puma without bundle exec\n- Have a \"stable\" Gemfile, and place it in your shared_path, only symlinking it into the current_path\n- Forcefully restart puma if changing the gemfile.\nThe same will happen to other services running under bundle exec, so there might be some other ideas on how to solve this for other services.\n. I think this is related to issue #300, and as a comment there says, the architecture needs to be replaced in order for this to work.\n. Thanks @evanphx\n. Are you preloading the app in cluster mode? If not it is logical that the usage goes up over time, as the app is loaded by the worker when it is needed.\nAre you getting the same results when adding preload_app! to your puma config?\n. @tozz Ruby 2.2.1 ? I cannot see memory growth on any of my applications running in production. (All monitored using monit, and I would be alerted if they were restarted...)\n. @kartikluke the following setups :\nUbuntu 12.04\nRvm\nRuby 2.1.2\nVarious rails apps with mysql or mongodb. \nSame setup on Ubuntu 14.04. \n. I'm seeing this in staging on 2.8.1 as well with Rails 3.2.18\nOutput: \n``\n2014-05-12_11:57:50.09844 2014-05-12 11:57:50 +0000: Rack app error: #<ThreadError: deadlock; recursive locking>\n2014-05-12_11:57:50.10117 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-1.4.5/lib/rack/lock.rb:14:inlock'\n2014-05-12_11:57:50.10118 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-1.4.5/lib/rack/lock.rb:14:in call'\n2014-05-12_11:57:50.10119 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:inforward'\n2014-05-12_11:57:50.10119 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:in fetch'\n2014-05-12_11:57:50.10119 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:inlookup'\n2014-05-12_11:57:50.10120 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:in call!'\n2014-05-12_11:57:50.10120 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:incall'\n2014-05-12_11:57:50.10121 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/airbrake-3.1.16/lib/airbrake/user_informer.rb:16:in _call'\n2014-05-12_11:57:50.10122 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/airbrake-3.1.16/lib/airbrake/user_informer.rb:12:incall'\n2014-05-12_11:57:50.10123 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/skylight-0.3.12/lib/skylight/middleware.rb:50:in call'\n2014-05-12_11:57:50.10124 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/railties-3.2.18/lib/rails/engine.rb:484:incall'\n2014-05-12_11:57:50.10125 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/railties-3.2.18/lib/rails/application.rb:231:in call'\n2014-05-12_11:57:50.10126 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/railties-3.2.18/lib/rails/railtie/configurable.rb:30:inmethod_missing'\n2014-05-12_11:57:50.10127 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/rack-1.4.5/lib/rack/deflater.rb:13:in call'\n2014-05-12_11:57:50.10128 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/puma-2.8.2/lib/puma/configuration.rb:71:incall'\n2014-05-12_11:57:50.10383 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/puma-2.8.2/lib/puma/server.rb:490:in handle_request'\n2014-05-12_11:57:50.10418 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/puma-2.8.2/lib/puma/server.rb:361:inprocess_client'\n2014-05-12_11:57:50.10444 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/puma-2.8.2/lib/puma/server.rb:254:in block in run'\n2014-05-12_11:57:50.10452 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:incall'\n2014-05-12_11:57:50.10459 /usr/local/rvm/gems/ruby-2.1.2@nyb_staging/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\n```\nI think it's related to these bugs:\nhttps://github.com/rack/rack/issues/658 and https://github.com/rack/rack/issues/349\nPuma configuration: \n```\nPuma configuration for:\nnyb running as deploy in environment staging\nworkers 2\nthreads 2, 8\nbind 'unix:///web/staging/nyb/shared/sockets/puma.sock'\npidfile '/web/staging/nyb/shared/pids/puma.pid'\nstate_path '/web/staging/nyb/shared/sockets/puma.state'\nenvironment 'staging'\nactivate_control_app 'unix:///web/staging/nyb/shared/sockets/pumactl.sock'\npreload_app! # Yes, I know this isn't ideal, but I need it.\ndirectory '/web/staging/nyb/current'\n```\n. This patch by @gregclermont works for rails 3.2.18 and rack 1.4.5 https://gist.github.com/gregclermont/9793651\nBasically, If you are on rails 3.2.18 which has strict requirements on rack 1.4.5, you will at one point see this issue. the bug is rack/rails related and not puma related.\nEither rack needs to backport as in rack/rack#658 or rails needs to relax their rack dependency rails/rails#9488\nUpdate: The bug is rack related to deflating gzip'ed content. Rack team is on top of the issue.\n. See also https://github.com/charliesome/better_errors/issues/341 and https://github.com/banister/binding_of_caller/issues/59. ",
    "ahayworth": "Hi all -\nWe're starting to run Puma in production, and I think we're running into this bug. I've attached a backtrace, a thread-level backtrace, and a ruby backtrace. They're unfortunately each from different processes, as we're running in cluster mode and the master started killing timed-out workers while I investigated. We're running ruby 1.9.3-p484, and puma 2.8.1. We've not yet found a reliable way to reproduce:\n```\nhayworth@sdg24:~$ sudo strace -p 1486\n[sudo] password for hayworth: \nProcess 1486 attached - interrupt to quit\nfutex(0x87a83b0, FUTEX_WAIT_PRIVATE, 1113, NULL^C \nProcess 1486 detached\nhayworth@sdg24:~$ sudo strace -p 4364\nProcess 4364 attached - interrupt to quit\nfutex(0x87a83b0, FUTEX_WAIT_PRIVATE, 1129, NULL^C \nProcess 4364 detached\nhayworth@sdg24:~$ sudo strace -p 21879\nProcess 21879 attached - interrupt to quit\nfutex(0x87a83b0, FUTEX_WAIT_PRIVATE, 1179, NULL^C \nProcess 21879 detached\nhayworth@sdg24:~$ \n(gdb) bt\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf773bfcf in __pthread_cond_wait (cond=0x8aa23ac, mutex=0x8aa23f8) at pthread_cond_wait.c:153\n2  0x08168fe2 in native_cond_wait (cond=0xfffffe00, mutex=0x6b) at thread_pthread.c:309\n3  0x0816e78f in native_sleep (th=0x8aa2358, timeout_tv=) at thread_pthread.c:909\n4  0x0816ecbe in sleep_forever (arg=4288293932) at thread.c:868\n5  thread_join_sleep (arg=4288293932) at thread.c:701\n6  0x0805bc91 in rb_ensure (b_proc=0x816ec20 , data1=4288293932, e_proc=0x8168c30 , data2=4288293932) at eval.c:744\n7  0x0816c8ed in thread_join (target_th=0xe178f58, delay=1e+30) at thread.c:734\n8  0x0816ca50 in thread_join_m (argc=0, argv=0xf7502128, self=219428620) at thread.c:815\n9  0x081535f5 in call_cfunc (func=0x816c9e0 , recv=107, len=128, argc=0, argv=0xfffffe00) at vm_insnhelper.c:317\n10 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581d1c, num=0, blockptr=0x0, flag=0, id=4744, me=0x8b33338, recv=219428620) at vm_insnhelper.c:404\n11 vm_call_method (th=, cfp=0xf7581d1c, num=0, blockptr=0x0, flag=0, id=4744, me=0x8b33338, recv=219428620) at vm_insnhelper.c:530\n12 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n13 0x0815ed7f in vm_exec (th=) at vm.c:1236\n14 0x08165f45 in invoke_block_from_c (val=6) at vm.c:640\n15 vm_yield (val=6) at vm.c:670\n16 rb_yield_0 (val=6) at vm_eval.c:777\n17 rb_yield (val=6) at vm_eval.c:784\n18 0x0805be08 in rb_protect (proc=0x8165d10 , data=6, state=0xff9a31bc) at eval.c:719\n19 0x080bf6d5 in rb_f_fork (obj=149192700) at process.c:2814\n20 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581dcc, num=0, blockptr=0xf7581de0, flag=8, id=8288, me=0x8b238f0, recv=149192700) at vm_insnhelper.c:404\n21 vm_call_method (th=, cfp=0xf7581dcc, num=0, blockptr=0xf7581de0, flag=8, id=8288, me=0x8b238f0, recv=149192700) at vm_insnhelper.c:530\n22 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n23 0x0815ed7f in vm_exec (th=) at vm.c:1236\n24 0x08165d60 in vm_yield (val=1) at vm.c:670\n25 rb_yield_0 (val=1) at vm_eval.c:777\n26 rb_yield (val=1) at vm_eval.c:787\n27 0x0808e301 in int_dotimes (num=3) at numeric.c:3328\n28 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581e50, num=0, blockptr=0xf7581e64, flag=0, id=3352, me=0x8af03e8, recv=3) at vm_insnhelper.c:404\n29 vm_call_method (th=, cfp=0xf7581e50, num=0, blockptr=0xf7581e64, flag=0, id=3352, me=0x8af03e8, recv=3) at vm_insnhelper.c:530\n30 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n31 0x0815ed7f in vm_exec (th=) at vm.c:1236\n32 0x0815f2c5 in rb_iseq_eval (iseqval=147564920) at vm.c:1464\n33 0x081afaff in rb_load_internal (fname=, wrap=) at load.c:310\n34 0x081afc48 in rb_f_load (argc=1, argv=0xf750202c) at load.c:384\n35 0x081535f5 in call_cfunc (func=0x81afbd0 , recv=107, len=128, argc=1, argv=0xfffffe00) at vm_insnhelper.c:317\n36 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581f84, num=1, blockptr=0x0, flag=8, id=6048, me=0x8b29bc8, recv=145578700) at vm_insnhelper.c:404\n37 vm_call_method (th=, cfp=0xf7581f84, num=1, blockptr=0x0, flag=8, id=6048, me=0x8b29bc8, recv=145578700) at vm_insnhelper.c:530\n38 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n39 0x0815ed7f in vm_exec (th=) at vm.c:1236\n40 0x0815f162 in rb_iseq_eval_main (iseqval=146688500) at vm.c:1478\n41 0x0805c192 in ruby_exec_internal (n=) at eval.c:204\n42 0x0805de72 in ruby_exec_node (n=0x8be49f4) at eval.c:251\n43 ruby_run_node (n=0x8be49f4) at eval.c:244\n44 0x0805b3ea in main (argc=8, argv=0xff9a3ed4) at main.c:38\n(gdb) \nThread 9 (Thread 0xf703db70 (LWP 10806)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf764a361 in select () at ../sysdeps/unix/syscall-template.S:82\n2  0x081706d4 in rb_fd_select (max=9, read=0xf703ce24, write=0x0, except=0x0, timeout=0x0) at thread.c:2446\n3  do_select (max=9, read=0xf703ce24, write=0x0, except=0x0, timeout=0x0) at thread.c:2640\n4  rb_thread_fd_select (max=9, read=0xf703ce24, write=0x0, except=0x0, timeout=0x0) at thread.c:2788\n5  0x080821ef in select_internal (arg=4144221716) at io.c:7709\n6  select_call (arg=4144221716) at io.c:7779\n7  0x0805bc91 in rb_ensure (b_proc=0x8081d80 , data1=4144221716, e_proc=0x8071fb0 , data2=4144221716) at eval.c:744\n8  0x08072211 in rb_f_select (argc=1, argv=0xf6ece024, obj=145555180) at io.c:8033\n9  0x081535f5 in call_cfunc (func=0x8072160 , recv=0, len=-153809056, argc=1, argv=0xfffffdfe) at vm_insnhelper.c:317\n10 0x08158d8f in vm_call_cfunc (th=, cfp=0xf6f4df84, num=1, blockptr=0x0, flag=0, id=1552, me=0x8b15398, recv=145555180) at vm_insnhelper.c:404\n11 vm_call_method (th=, cfp=0xf6f4df84, num=1, blockptr=0x0, flag=0, id=1552, me=0x8b15398, recv=145555180) at vm_insnhelper.c:530\n12 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n13 0x0815ed7f in vm_exec (th=) at vm.c:1236\n14 0x0815ffa6 in rb_vm_invoke_proc (th=0x91a3580, proc=0x8e73448, self=149192700, argc=0, argv=0x8fb8ad8, blockptr=0x0) at vm.c:686\n15 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n16 0x0816f78e in thread_start_func_1 (th_ptr=0x91a3580) at thread_pthread.c:657\n17 0xf7737955 in start_thread (arg=0xf703db70) at pthread_create.c:300\n18 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 8 (Thread 0xf713fb70 (LWP 10935)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf773c4d2 in pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/i386/i686/../i486/pthread_cond_timedwait.S:179\n2  0x0816e71e in native_cond_timedwait (th=0x9c36198, timeout_tv=0x53) at thread_pthread.c:327\n3  native_sleep (th=0x9c36198, timeout_tv=0x53) at thread_pthread.c:911\n4  0x0816e88c in sleep_timeval (th=0x9c36198, tv=...) at thread.c:908\n5  0x080b8b7b in rb_f_sleep (argc=1, argv=0xf692f084) at process.c:3472\n6  0x081535f5 in call_cfunc (func=0x80b8b30 , recv=83, len=128, argc=1, argv=0xfffffdfc) at vm_insnhelper.c:317\n7  0x08158d8f in vm_call_cfunc (th=, cfp=0xf69aee50, num=1, blockptr=0x0, flag=8, id=8320, me=0x8b23c70, recv=162798840) at vm_insnhelper.c:404\n8  vm_call_method (th=, cfp=0xf69aee50, num=1, blockptr=0x0, flag=8, id=8320, me=0x8b23c70, recv=162798840) at vm_insnhelper.c:530\n9  0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n10 0x0815ed7f in vm_exec (th=) at vm.c:1236\n11 0x0815ffa6 in rb_vm_invoke_proc (th=0x9c36198, proc=0xa0caa50, self=163039300, argc=1, argv=0x9b76fb4, blockptr=0x0) at vm.c:686\n12 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n13 0x0816f78e in thread_start_func_1 (th_ptr=0x9c36198) at thread_pthread.c:657\n14 0xf7737955 in start_thread (arg=0xf713fb70) at pthread_create.c:300\n15 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 7 (Thread 0xf7756b70 (LWP 11022)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf764a361 in select () at ../sysdeps/unix/syscall-template.S:82\n2  0x081712b4 in thread_timer (p=0x8aa200c) at thread_pthread.c:1159\n3  0xf7737955 in start_thread (arg=0xf7756b70) at pthread_create.c:300\n4  0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 6 (Thread 0xf7241b70 (LWP 11393)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf773c4d2 in pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/i386/i686/../i486/pthread_cond_timedwait.S:179\n2  0x0816e71e in native_cond_timedwait (th=0xe259170, timeout_tv=0x9d) at thread_pthread.c:327\n3  native_sleep (th=0xe259170, timeout_tv=0x9d) at thread_pthread.c:911\n4  0x0816e88c in sleep_timeval (th=0xe259170, tv=...) at thread.c:908\n5  0x080b8b7b in rb_f_sleep (argc=1, argv=0xf68ae02c) at process.c:3472\n6  0x081535f5 in call_cfunc (func=0x80b8b30 , recv=157, len=128, argc=1, argv=0xfffffdfc) at vm_insnhelper.c:317\n7  0x08158d8f in vm_call_cfunc (th=, cfp=0xf692df84, num=1, blockptr=0x0, flag=8, id=8320, me=0x8b23c70, recv=149192700) at vm_insnhelper.c:404\n8  vm_call_method (th=, cfp=0xf692df84, num=1, blockptr=0x0, flag=8, id=8320, me=0x8b23c70, recv=149192700) at vm_insnhelper.c:530\n9  0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n10 0x0815ed7f in vm_exec (th=) at vm.c:1236\n11 0x0815ffa6 in rb_vm_invoke_proc (th=0xe259170, proc=0xd86c3f0, self=149192700, argc=1, argv=0xd2d7878, blockptr=0x0) at vm.c:686\n12 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n13 0x0816f78e in thread_start_func_1 (th_ptr=0xe259170) at thread_pthread.c:657\n14 0xf7737955 in start_thread (arg=0xf7241b70) at pthread_create.c:300\n15 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 5 (Thread 0xf6ecdb70 (LWP 11394)):\n0  0x080fe8d9 in st_foreach (table=0xb401548, func=0x80668f0 , arg=4142701396) at st.c:825\n1  0x08067f09 in mark_hash (objspace=0x8aa25c0) at gc.c:1630\n2  gc_mark_children (objspace=0x8aa25c0) at gc.c:1951\n3  gc_mark_stacked_objects (objspace=0x8aa25c0) at gc.c:1506\n4  gc_marks (objspace=0x8aa25c0) at gc.c:2578\n5  0x080684ca in garbage_collect (objspace=0x8aa25c0) at gc.c:2602\n6  0x08068fdf in garbage_collect_with_gvl (objspace=0x8aa25c0, size=129) at gc.c:731\n7  vm_malloc_prepare (objspace=0x8aa25c0, size=129) at gc.c:761\n8  vm_xmalloc (objspace=0x8aa25c0, size=129) at gc.c:795\n9  0x08104c9f in rb_str_buf_new (capa=128) at string.c:745\n10 0x080fe0c5 in rb_enc_vsprintf (enc=0x8aa4918, format=0x81c9891 \"%s:%d:in `%s'\") at sprintf.c:1170\n11 rb_enc_sprintf (enc=0x8aa4918, format=0x81c9891 \"%s:%d:in `%s'\") at sprintf.c:1190\n12 0x08154220 in vm_backtrace_push (arg=0xf6ec9d7c, file=158623600, line_no=68, name=156549120) at vm.c:849\n13 0x0815544b in vm_backtrace_each (th=, lev=, init=, iter=0x81541c0 , arg=0xf6ec9d7c) at vm.c:815\n14 0x08155d2b in vm_backtrace () at vm.c:868\n15 rb_make_backtrace () at vm_eval.c:1659\n16 0x0805e2cd in setup_exception (th=, tag=, mesg=240983040) at eval.c:390\n17 0x0805e838 in rb_longjmp (mesg=240983040) at eval.c:452\n18 rb_exc_raise (mesg=240983040) at eval.c:465\n19 0x08158a28 in raise_method_missing (th=0xe2adf58, argc=, argv=, obj=4, last_call_status=) at vm_eval.c:559\n20 0x08158af6 in rb_method_missing (argc=1, argv=0xf5635a04, obj=4) at vm_eval.c:499\n21 0x081535f5 in call_cfunc (func=0x8158ad0 , recv=4142701396, len=34, argc=1, argv=0x4908b9e9) at vm_insnhelper.c:317\n22 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b3a34, num=1, blockptr=0x0, flag=136, id=384, me=0x8aed740, recv=4) at vm_insnhelper.c:404\n23 vm_call_method (th=, cfp=0xf56b3a34, num=1, blockptr=0x0, flag=136, id=384, me=0x8aed740, recv=4) at vm_insnhelper.c:530\n24 0x0815c842 in vm_exec_core (th=, initial=) at insns.def:1054\n25 0x0815ed7f in vm_exec (th=) at vm.c:1236\n26 0x081604dc in vm_call0 (th=0xe2adf58, recv=, id=, argc=1, argv=0xe5d1c6c, me=0x9d3a920) at vm_eval.c:66\n27 0x081610b7 in rb_call0 (recv=4, mid=384, argc=, argv=0xe5d1c6c, scope=CALL_FCALL, self=6) at vm_eval.c:236\n28 0x08161813 in rb_call (args=0xf6eca574) at vm_eval.c:456\n29 rb_funcall2 (args=0xf6eca574) at vm_eval.c:671\n30 check_funcall_exec (args=0xf6eca574) at vm_eval.c:253\n31 0x0805bf61 in rb_rescue2 (b_proc=0x81617b0 , data1=4142703988, r_proc=0x8166580 , data2=4142703988) at eval.c:647\n32 0x08160d4e in check_funcall (recv=4, mid=, argc=, argv=0x0) at vm_eval.c:302\n33 0x08098b3f in convert_type (val=4, type=7, tname=0x81cb54d \"Array\", method=0x81cb546 \"to_ary\") at object.c:2063\n34 rb_check_convert_type (val=4, type=7, tname=0x81cb54d \"Array\", method=0x81cb546 \"to_ary\") at object.c:2100\n35 0x081837d7 in rb_check_array_type (ary=240983200, level=, modified=) at array.c:474\n36 flatten (ary=240983200, level=, modified=) at array.c:3740\n37 0x08183a57 in rb_ary_flatten (argc=0, argv=0xf56359d4, ary=240983200) at array.c:3846\n38 0x081535f5 in call_cfunc (func=0x81839d0 , recv=4142701396, len=34, argc=0, argv=0x4908b9e9) at vm_insnhelper.c:317\n39 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b3ab8, num=0, blockptr=0x0, flag=0, id=4912, me=0x8b0a4c8, recv=240983200) at vm_insnhelper.c:404\n40 vm_call_method (th=, cfp=0xf56b3ab8, num=0, blockptr=0x0, flag=0, id=4912, me=0x8b0a4c8, recv=240983200) at vm_insnhelper.c:530\n41 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n42 0x0815ed7f in vm_exec (th=) at vm.c:1236\n43 0x08165d60 in vm_yield (val=168864160) at vm.c:670\n44 rb_yield_0 (val=168864160) at vm_eval.c:777\n45 rb_yield (val=168864160) at vm_eval.c:787\n46 0x0817b35d in rb_ary_each (array=240983260) at array.c:1495\n47 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b3b94, num=0, blockptr=0xf56b3ba8, flag=264, id=424, me=0x8b08f30, recv=240983260) at vm_insnhelper.c:404\n48 vm_call_method (th=, cfp=0xf56b3b94, num=0, blockptr=0xf56b3ba8, flag=264, id=424, me=0x8b08f30, recv=240983260) at vm_insnhelper.c:530\n49 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n50 0x0815ed7f in vm_exec (th=) at vm.c:1236\n51 0x081604dc in vm_call0 (th=0xe2adf58, recv=, id=, argc=1, argv=0xf6ecadf0, me=0xa7232c8) at vm_eval.c:66\n52 0x081610b7 in rb_call0 (recv=219243760, mid=384, argc=, argv=0xf6ecadf0, scope=CALL_FCALL, self=6) at vm_eval.c:236\n53 0x08161475 in rb_call (th=0xe2adf58, id=, recv=, num=0, blockptr=0x0, opt=0) at vm_eval.c:456\n54 rb_funcall2 (th=0xe2adf58, id=, recv=, num=0, blockptr=0x0, opt=0) at vm_eval.c:671\n55 vm_method_missing (th=0xe2adf58, id=, recv=, num=0, blockptr=0x0, opt=0) at vm_insnhelper.c:454\n56 0x08158e84 in vm_call_method (th=, cfp=0xf56b3c9c, num=0, blockptr=0x0, flag=, id=38255, me=0x0, recv=219243760) at vm_insnhelper.c:673\n57 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n58 0x0815ed7f in vm_exec (th=) at vm.c:1236\n59 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xbd70620, self=220747560, argc=0, argv=0xf6ecb240, blockptr=0x0) at vm.c:686\n60 0x0816004e in vm_call_bmethod (th=0xe2adf58, recv=220747560, argc=34, argv=0xf6ecb240, blockptr=0x0, me=0xbd70650) at vm_insnhelper.c:429\n61 0x08159020 in vm_call_method (th=, cfp=0xf56b3d4c, num=0, blockptr=0x0, flag=270, id=242712, me=0xbd70650, recv=220747560) at vm_insnhelper.c:562\n62 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n63 0x0815ed7f in vm_exec (th=) at vm.c:1236\n64 0x08165d60 in vm_yield (val=196449880) at vm.c:670\n65 rb_yield_0 (val=196449880) at vm_eval.c:777\n66 rb_yield (val=196449880) at vm_eval.c:787\n67 0x0817b35d in rb_ary_each (array=238700660) at array.c:1495\n68 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b3e28, num=0, blockptr=0xf56b3e3c, flag=264, id=424, me=0x8b08f30, recv=238700660) at vm_insnhelper.c:404\n69 vm_call_method (th=, cfp=0xf56b3e28, num=0, blockptr=0xf56b3e3c, flag=264, id=424, me=0x8b08f30, recv=238700660) at vm_insnhelper.c:530\n70 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n71 0x0815ed7f in vm_exec (th=) at vm.c:1236\n72 0x081604dc in vm_call0 (th=0xe2adf58, recv=, id=, argc=1, argv=0xf563573c, me=0xe191fd8) at vm_eval.c:66\n73 0x0805f6b4 in rb_method_call_with_block (argc=1, argv=0xf563573c, method=240178920, pass_procval=4) at proc.c:1444\n74 0x08061fef in rb_method_call (argc=1, argv=0xf563573c, method=240178920) at proc.c:1411\n75 0x081535f5 in call_cfunc (func=0x8061fb0 , recv=4142701396, len=34, argc=1, argv=0x4908b9e9) at vm_insnhelper.c:317\n76 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b4140, num=1, blockptr=0x0, flag=2, id=6016, me=0x8b2b530, recv=240178920) at vm_insnhelper.c:404\n77 vm_call_method (th=, cfp=0xf56b4140, num=1, blockptr=0x0, flag=2, id=6016, me=0x8b2b530, recv=240178920) at vm_insnhelper.c:530\n78 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n79 0x0815ed7f in vm_exec (th=) at vm.c:1236\n80 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xbd577d0, self=220747560, argc=1, argv=0xf6ecbe10, blockptr=0x0) at vm.c:686\n81 0x0816004e in vm_call_bmethod (th=0xe2adf58, recv=220747560, argc=34, argv=0xf6ecbe10, blockptr=0x0, me=0xbd57810) at vm_insnhelper.c:429\n82 0x08159020 in vm_call_method (th=, cfp=0xf56b4198, num=1, blockptr=0x0, flag=0, id=238056, me=0xbd57810, recv=220747560) at vm_insnhelper.c:562\n83 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n84 0x0815ed7f in vm_exec (th=) at vm.c:1236\n85 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xd6b7ef8, self=220334340, argc=0, argv=0xf56354a0, blockptr=0x0) at vm.c:686\n86 0x08062054 in proc_call (argc=0, argv=0xf56354a0, procval=220457780) at proc.c:555\n87 0x081535f5 in call_cfunc (func=0x8062000 , recv=4142701396, len=34, argc=0, argv=0x4908b9e9) at vm_insnhelper.c:317\n88 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b46c0, num=0, blockptr=0x0, flag=0, id=6016, me=0x8b2a3a0, recv=220457780) at vm_insnhelper.c:404\n89 vm_call_method (th=, cfp=0xf56b46c0, num=0, blockptr=0x0, flag=0, id=6016, me=0x8b2a3a0, recv=220457780) at vm_insnhelper.c:530\n90 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n91 0x0815ed7f in vm_exec (th=) at vm.c:1236\n92 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xcfb2dd8, self=212351180, argc=2, argv=0xf5635454, blockptr=0x0) at vm.c:686\n93 0x08062054 in proc_call (argc=2, argv=0xf5635454, procval=173672680) at proc.c:555\n94 0x081535f5 in call_cfunc (func=0x8062000 , recv=4142701396, len=34, argc=2, argv=0x4908b9e9) at vm_insnhelper.c:317\n95 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b4770, num=2, blockptr=0x0, flag=6, id=6016, me=0x8b2a3a0, recv=173672680) at vm_insnhelper.c:404\n96 vm_call_method (th=, cfp=0xf56b4770, num=2, blockptr=0x0, flag=6, id=6016, me=0x8b2a3a0, recv=173672680) at vm_insnhelper.c:530\n97 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n98 0x0815ed7f in vm_exec (th=) at vm.c:1236\n99 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xe21b1b8, self=221925740, argc=1, argv=0xf5635270, blockptr=0x0) at vm.c:686\n100 0x08062054 in proc_call (argc=1, argv=0xf5635270, procval=221925700) at proc.c:555\n101 0x081535f5 in call_cfunc (func=0x8062000 , recv=4142701396, len=34, argc=1, argv=0x4908b9e9) at vm_insnhelper.c:317\n102 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b4b64, num=1, blockptr=0x0, flag=0, id=6016, me=0x8b2a3a0, recv=221925700) at vm_insnhelper.c:404\n103 vm_call_method (th=, cfp=0xf56b4b64, num=1, blockptr=0x0, flag=0, id=6016, me=0x8b2a3a0, recv=221925700) at vm_insnhelper.c:530\n104 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n105 0x0815ed7f in vm_exec (th=) at vm.c:1236\n106 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xdd67568, self=221085720, argc=2, argv=0xf5635040, blockptr=0x0) at vm.c:686\n107 0x08062054 in proc_call (argc=2, argv=0xf5635040, procval=221083460) at proc.c:555\n108 0x081535f5 in call_cfunc (func=0x8062000 , recv=4142701396, len=34, argc=2, argv=0x4908b9e9) at vm_insnhelper.c:317\n109 0x08158d8f in vm_call_cfunc (th=, cfp=0xf56b4f84, num=2, blockptr=0x0, flag=2, id=6016, me=0x8b2a3a0, recv=221083460) at vm_insnhelper.c:404\n110 vm_call_method (th=, cfp=0xf56b4f84, num=2, blockptr=0x0, flag=2, id=6016, me=0x8b2a3a0, recv=221083460) at vm_insnhelper.c:530\n111 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n112 0x0815ed7f in vm_exec (th=) at vm.c:1236\n113 0x0815ffa6 in rb_vm_invoke_proc (th=0xe2adf58, proc=0xda7c628, self=221083700, argc=0, argv=0xd2d7454, blockptr=0x0) at vm.c:686\n114 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n115 0x0816f78e in thread_start_func_1 (th_ptr=0xe2adf58) at thread_pthread.c:657\n116 0xf7737955 in start_thread (arg=0xf6ecdb70) at pthread_create.c:300\n117 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 4 (Thread 0xf55b3b70 (LWP 11395)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf764a361 in select () at ../sysdeps/unix/syscall-template.S:82\n2  0x081706d4 in rb_fd_select (max=19, read=0xf55b2e24, write=0x0, except=0x0, timeout=0xf55b2c48) at thread.c:2446\n3  do_select (max=19, read=0xf55b2e24, write=0x0, except=0x0, timeout=0xf55b2c48) at thread.c:2640\n4  rb_thread_fd_select (max=19, read=0xf55b2e24, write=0x0, except=0x0, timeout=0xf55b2c48) at thread.c:2788\n5  0x080821ef in select_internal (arg=4116393492) at io.c:7709\n6  select_call (arg=4116393492) at io.c:7779\n7  0x0805bc91 in rb_ensure (b_proc=0x8081d80 , data1=4116393492, e_proc=0x8071fb0 , data2=4116393492) at eval.c:744\n8  0x08072211 in rb_f_select (argc=4, argv=0xf55b4044, obj=145555180) at io.c:8033\n9  0x081535f5 in call_cfunc (func=0x8072160 , recv=0, len=235994800, argc=4, argv=0xfffffdfe) at vm_insnhelper.c:317\n10 0x08158d8f in vm_call_cfunc (th=, cfp=0xf5633f58, num=4, blockptr=0x0, flag=0, id=1552, me=0x8b15398, recv=145555180) at vm_insnhelper.c:404\n11 vm_call_method (th=, cfp=0xf5633f58, num=4, blockptr=0x0, flag=0, id=1552, me=0x8b15398, recv=145555180) at vm_insnhelper.c:530\n12 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n13 0x0815ed7f in vm_exec (th=) at vm.c:1236\n14 0x0815ffa6 in rb_vm_invoke_proc (th=0xd8b9c40, proc=0xdf766b8, self=221082420, argc=0, argv=0xd2d71ac, blockptr=0x0) at vm.c:686\n15 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n16 0x0816f78e in thread_start_func_1 (th_ptr=0xd8b9c40) at thread_pthread.c:657\n17 0xf7737955 in start_thread (arg=0xf55b3b70) at pthread_create.c:300\n18 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 3 (Thread 0xf54b1b70 (LWP 11396)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf773c4d2 in pthread_cond_timedwait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/i386/i686/../i486/pthread_cond_timedwait.S:179\n2  0x0816e71e in native_cond_timedwait (th=0xdfd78b8, timeout_tv=0x2f3) at thread_pthread.c:327\n3  native_sleep (th=0xdfd78b8, timeout_tv=0x2f3) at thread_pthread.c:911\n4  0x0816e88c in sleep_timeval (th=0xdfd78b8, tv=...) at thread.c:908\n5  0x080b8b7b in rb_f_sleep (argc=1, argv=0xf54b2024) at process.c:3472\n6  0x081535f5 in call_cfunc (func=0x80b8b30 , recv=755, len=128, argc=1, argv=0xfffffdfc) at vm_insnhelper.c:317\n7  0x08158d8f in vm_call_cfunc (th=, cfp=0xf5531f84, num=1, blockptr=0x0, flag=8, id=8320, me=0x8b23c70, recv=221081720) at vm_insnhelper.c:404\n8  vm_call_method (th=, cfp=0xf5531f84, num=1, blockptr=0x0, flag=8, id=8320, me=0x8b23c70, recv=221081720) at vm_insnhelper.c:530\n9  0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n10 0x0815ed7f in vm_exec (th=) at vm.c:1236\n11 0x0815ffa6 in rb_vm_invoke_proc (th=0xdfd78b8, proc=0xe270180, self=221081720, argc=0, argv=0xd2d6f40, blockptr=0x0) at vm.c:686\n12 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n13 0x0816f78e in thread_start_func_1 (th_ptr=0xdfd78b8) at thread_pthread.c:657\n14 0xf7737955 in start_thread (arg=0xf54b1b70) at pthread_create.c:300\n15 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 2 (Thread 0xf53afb70 (LWP 11397)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf773bfcf in __pthread_cond_wait (cond=0x8aa202c, mutex=0x8aa2010) at pthread_cond_wait.c:153\n2  0x0816dc2c in native_cond_wait (vm=0x8aa2008, th=) at thread_pthread.c:309\n3  gvl_acquire_common (vm=0x8aa2008, th=) at thread_pthread.c:64\n4  gvl_acquire (vm=0x8aa2008, th=) at thread_pthread.c:82\n5  0x0816feb7 in blocking_region_end (th=0xe41f068, region=0xf53aec2c) at thread.c:1060\n6  0x081706f9 in do_select (max=17, read=0xf53aee24, write=0x0, except=0x0, timeout=0x0) at thread.c:2640\n7  rb_thread_fd_select (max=17, read=0xf53aee24, write=0x0, except=0x0, timeout=0x0) at thread.c:2788\n8  0x080821ef in select_internal (arg=4114279956) at io.c:7709\n9  select_call (arg=4114279956) at io.c:7779\n10 0x0805bc91 in rb_ensure (b_proc=0x8081d80 , data1=4114279956, e_proc=0x8071fb0 , data2=4114279956) at eval.c:744\n11 0x08072211 in rb_f_select (argc=1, argv=0xf53b0044, obj=145555180) at io.c:8033\n12 0x081535f5 in call_cfunc (func=0x8072160 , recv=7989, len=128, argc=1, argv=0xfffffe00) at vm_insnhelper.c:317\n13 0x08158d8f in vm_call_cfunc (th=, cfp=0xf542ff58, num=1, blockptr=0x0, flag=0, id=1552, me=0x8b15398, recv=145555180) at vm_insnhelper.c:404\n14 vm_call_method (th=, cfp=0xf542ff58, num=1, blockptr=0x0, flag=0, id=1552, me=0x8b15398, recv=145555180) at vm_insnhelper.c:530\n15 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n16 0x0815ed7f in vm_exec (th=) at vm.c:1236\n17 0x0815ffa6 in rb_vm_invoke_proc (th=0xe41f068, proc=0xe150fd8, self=221085720, argc=0, argv=0xd2d6d4c, blockptr=0x0) at vm.c:686\n18 0x0816f6a5 in thread_start_func_2 (th=, stack_start=) at thread.c:466\n19 0x0816f78e in thread_start_func_1 (th_ptr=0xe41f068) at thread_pthread.c:657\n20 0xf7737955 in start_thread (arg=0xf53afb70) at pthread_create.c:300\n21 0xf76511de in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:130\nThread 1 (Thread 0xf75836c0 (LWP 10803)):\n0  0xf7759430 in __kernel_vsyscall ()\n1  0xf773bfcf in __pthread_cond_wait (cond=0x8aa23ac, mutex=0x8aa23f8) at pthread_cond_wait.c:153\n2  0x08168fe2 in native_cond_wait (cond=0xfffffe00, mutex=0x8b) at thread_pthread.c:309\n3  0x0816e78f in native_sleep (th=0x8aa2358, timeout_tv=) at thread_pthread.c:909\n4  0x0816ecbe in sleep_forever (arg=4288293932) at thread.c:868\n5  thread_join_sleep (arg=4288293932) at thread.c:701\n6  0x0805bc91 in rb_ensure (b_proc=0x816ec20 , data1=4288293932, e_proc=0x8168c30 , data2=4288293932) at eval.c:744\n7  0x0816c8ed in thread_join (target_th=0xe41f068, delay=1e+30) at thread.c:734\n8  0x0816ca50 in thread_join_m (argc=0, argv=0xf7502128, self=221080920) at thread.c:815\n9  0x081535f5 in call_cfunc (func=0x816c9e0 , recv=139, len=128, argc=0, argv=0xfffffe00) at vm_insnhelper.c:317\n10 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581d1c, num=0, blockptr=0x0, flag=0, id=4744, me=0x8b33338, recv=221080920) at vm_insnhelper.c:404\n11 vm_call_method (th=, cfp=0xf7581d1c, num=0, blockptr=0x0, flag=0, id=4744, me=0x8b33338, recv=221080920) at vm_insnhelper.c:530\n12 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n13 0x0815ed7f in vm_exec (th=) at vm.c:1236\n14 0x08165f45 in invoke_block_from_c (val=6) at vm.c:640\n15 vm_yield (val=6) at vm.c:670\n16 rb_yield_0 (val=6) at vm_eval.c:777\n17 rb_yield (val=6) at vm_eval.c:784\n18 0x0805be08 in rb_protect (proc=0x8165d10 , data=6, state=0xff9a31bc) at eval.c:719\n19 0x080bf6d5 in rb_f_fork (obj=149192700) at process.c:2814\n20 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581dcc, num=0, blockptr=0xf7581de0, flag=8, id=8288, me=0x8b238f0, recv=149192700) at vm_insnhelper.c:404\n21 vm_call_method (th=, cfp=0xf7581dcc, num=0, blockptr=0xf7581de0, flag=8, id=8288, me=0x8b238f0, recv=149192700) at vm_insnhelper.c:530\n22 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n23 0x0815ed7f in vm_exec (th=) at vm.c:1236\n24 0x08165d60 in vm_yield (val=1) at vm.c:670\n25 rb_yield_0 (val=1) at vm_eval.c:777\n26 rb_yield (val=1) at vm_eval.c:787\n27 0x0808e301 in int_dotimes (num=3) at numeric.c:3328\n28 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581e50, num=0, blockptr=0xf7581e64, flag=0, id=3352, me=0x8af03e8, recv=3) at vm_insnhelper.c:404\n29 vm_call_method (th=, cfp=0xf7581e50, num=0, blockptr=0xf7581e64, flag=0, id=3352, me=0x8af03e8, recv=3) at vm_insnhelper.c:530\n30 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n31 0x0815ed7f in vm_exec (th=) at vm.c:1236\n32 0x0815f2c5 in rb_iseq_eval (iseqval=147564920) at vm.c:1464\n33 0x081afaff in rb_load_internal (fname=, wrap=) at load.c:310\n34 0x081afc48 in rb_f_load (argc=1, argv=0xf750202c) at load.c:384\n35 0x081535f5 in call_cfunc (func=0x81afbd0 , recv=139, len=128, argc=1, argv=0xfffffe00) at vm_insnhelper.c:317\n36 0x08158d8f in vm_call_cfunc (th=, cfp=0xf7581f84, num=1, blockptr=0x0, flag=8, id=6048, me=0x8b29bc8, recv=145578700) at vm_insnhelper.c:404\n37 vm_call_method (th=, cfp=0xf7581f84, num=1, blockptr=0x0, flag=8, id=6048, me=0x8b29bc8, recv=145578700) at vm_insnhelper.c:530\n38 0x0815b23e in vm_exec_core (th=, initial=) at insns.def:1018\n39 0x0815ed7f in vm_exec (th=) at vm.c:1236\n40 0x0815f162 in rb_iseq_eval_main (iseqval=146688500) at vm.c:1478\n41 0x0805c192 in ruby_exec_internal (n=) at eval.c:204\n42 0x0805de72 in ruby_exec_node (n=0x8be49f4) at eval.c:251\n43 ruby_run_node (n=0x8be49f4) at eval.c:244\n44 0x0805b3ea in main (argc=8, argv=0xff9a3ed4) at main.c:38\n(gdb) call (void) close(1)\n(gdb) call (void) close(2)\n(gdb)  call (int) open(\"/dev/pts/0\", 2, 0)\n$1 = 1\n(gdb)  call (int) open(\"/dev/pts/0\", 2, 0)\n$2 = 2\n(gdb) call (void)rb_backtrace()\n    from /usr/lib/ruby/gems/1.9.3/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:in block in spawn_thread'\n    from /usr/lib/ruby/gems/1.9.3/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/puma-2.8.1/lib/puma/server.rb:254:in block in run'\n    from /usr/lib/ruby/gems/1.9.3/gems/puma-2.8.1/lib/puma/server.rb:361:inprocess_client'\n    from /usr/lib/ruby/gems/1.9.3/gems/puma-2.8.1/lib/puma/server.rb:490:in handle_request'\n    from /usr/lib/ruby/gems/1.9.3/gems/puma-2.8.1/lib/puma/configuration.rb:71:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/dispatcher.rb:106:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/rack-1.1.0/lib/rack/lock.rb:11:incall'\n    from :10:in synchronize'\n    from /usr/lib/ruby/gems/1.9.3/gems/rack-1.1.0/lib/rack/lock.rb:11:inblock in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/failsafe.rb:26:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/cache/strategy/local_cache.rb:25:incall'\n    from /var/local/meraki/manage-released/lib/rack_pageviews.rb:137:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/connection_adapters/abstract/connection_pool.rb:361:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/query_cache.rb:28:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/query_cache.rb:9:incache'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/connection_adapters/abstract/query_cache.rb:34:in cache'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/query_cache.rb:29:inblock in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/session/abstract_store.rb:177:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/params_parser.rb:15:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/rack-1.1.0/lib/rack/methodoverride.rb:24:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/rack-1.1.0/lib/rack/head.rb:9:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/string_coercion.rb:25:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/newrelic_rpm-3.7.2.192/lib/new_relic/rack/agent_hooks.rb:32:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/newrelic_rpm-3.7.2.192/lib/new_relic/rack/agent_hooks.rb:32:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/dispatcher.rb:130:inblock in build_middleware_stack'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/dispatcher.rb:121:in _call'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/dispatcher.rb:87:indispatch'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/routing/route_set.rb:438:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/base.rb:386:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/base.rb:391:in process'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:606:inprocess_with_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/base.rb:532:in process'\n    from /usr/lib/ruby/gems/1.9.3/gems/newrelic_rpm-3.7.2.192/lib/new_relic/agent/instrumentation/controller_instrumentation.rb:341:inperform_action_with_newrelic_trace'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/flash.rb:151:in perform_action_with_flash'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/rescue.rb:160:inperform_action_with_rescue'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/benchmarking.rb:68:in perform_action_with_benchmark'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/core_ext/benchmark.rb:17:inms'\n    from /usr/lib/ruby/1.9.3/benchmark.rb:295:in realtime'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/core_ext/benchmark.rb:17:inblock in ms'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/benchmarking.rb:68:in block in perform_action_with_benchmark'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:610:inperform_action_with_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:615:in call_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:635:inrun_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:184:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:182:inevaluate_method'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:182:in call'\n    from /var/local/meraki/manage-released/app/controllers/cf_controller.rb:21:inblock in '\n    from /var/local/meraki/manage-released/app/controllers/application.rb:514:in log_pageview'\n    from /var/local/meraki/manage-released/app/controllers/application.rb:514:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:638:in block in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:615:incall_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:635:in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:186:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:178:in evaluate_method'\n    from /var/local/meraki/manage-released/app/controllers/application.rb:289:incapture_request_binding'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:638:in block in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:615:incall_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:635:in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:186:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:178:in evaluate_method'\n    from /var/local/meraki/manage-released/app/controllers/application.rb:296:inblock_html_json'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:638:in block in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:615:incall_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:635:in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:186:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:178:in evaluate_method'\n    from /var/local/meraki/manage-released/app/controllers/application.rb:121:inset_log_level_to_info'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:638:in block in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:615:incall_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:635:in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:186:incall'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:178:in evaluate_method'\n    from /var/local/meraki/manage-released/app/controllers/application.rb:121:inset_log_level_to_info'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:638:in block in run_before_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/filters.rb:617:incall_filters'\n    from /usr/lib/ruby/gems/1.9.3/gems/actionpack-2.3.18/lib/action_controller/base.rb:1333:in perform_action'\n    from /var/local/meraki/manage-released/app/controllers/apple_mdm_controller.rb:131:inios'\n    from /var/local/meraki/manage-released/lib/pcc/apple_mdm/checkin_helpers.rb:23:in ios_handle_plist'\n    from /var/local/meraki/manage-released/lib/pcc/apple_mdm/checkin_helpers.rb:63:inios_update_checkin_meta'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/transactions.rb:200:in save_with_transactions!'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/transactions.rb:208:inrollback_active_record_state!'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/transactions.rb:200:in block in save_with_transactions!'\n    from /usr/lib/ruby/gems/1.9.3/gems/newrelic_rpm-3.7.2.192/lib/new_relic/agent/method_tracer.rb:544:intransaction_with_trace_ActiveRecord_self_name_transaction'\n    from /usr/lib/ruby/gems/1.9.3/gems/newrelic_rpm-3.7.2.192/lib/new_relic/agent/method_tracer.rb:281:in trace_execution_scoped'\n    from /usr/lib/ruby/gems/1.9.3/gems/newrelic_rpm-3.7.2.192/lib/new_relic/agent/method_tracer.rb:549:inblock in transaction_with_trace_ActiveRecord_self_name_transaction'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/transactions.rb:182:in transaction'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/connection_adapters/abstract/database_statements.rb:136:intransaction'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/transactions.rb:200:in block (2 levels) in save_with_transactions!'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/dirty.rb:87:insave_with_dirty!'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/validations.rb:1099:in save_with_validation!'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/base.rb:2602:insave!'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/callbacks.rb:250:in create_or_update_with_callbacks'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/base.rb:2935:increate_or_update'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/callbacks.rb:283:in update_with_callbacks'\n    from /usr/lib/ruby/gems/1.9.3/gems/activerecord-2.3.18/lib/active_record/callbacks.rb:344:incallback'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:276:in run_callbacks'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:92:inrun'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:92:in each'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:93:inblock in run'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:166:in call'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:196:inshould_run_callback?'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/callbacks.rb:196:in flatten'\n    from /usr/lib/ruby/gems/1.9.3/gems/activesupport-2.3.18/lib/active_support/whiny_nil.rb:48:inmethod_missing'\n(gdb)\n```\n. ",
    "nachogiljaldo": "I just noticed this issue and mine might be similar: #649\n. Got some more info:\nbash\nProcess 7521 attached\nrestart_syscall(<... resuming interrupted call ...>) = -1 ETIMEDOUT (Connection timed out)\nfutex(0x7fd61800b728, FUTEX_WAKE_PRIVATE, 1) = 0\nwrite(1, \"Continues here 7521.\", 20)    = 20\nwrite(1, \"\\n\", 1)                       = 1\nwrite(1, \"None is null\", 12)            = 12\nwrite(1, \"\\n\", 1)                       = 1\nwrite(1, \"Ey still here\", 13)           = 13\nwrite(1, \"\\n\", 1)                       = 1\nlseek(4, 1257095, SEEK_SET)             = 1257095\nread(4, \"PK\\3\\4\\24\\0\\10\\10\\10\\0\\vq\\33E\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0\\0.\\0\\0\\0\", 30) = 30\nlseek(4, 1257171, SEEK_SET)             = 1257171\nread(4, \"\\215SmO\\323`\\24=\\3176\\3261\\213TD\\231\\210\\10R\\331@\\245\\202\\202/#&\\10[X\\334\\213\"..., 616) = 616\nfutex(0x7fd61800bb54, FUTEX_WAIT_PRIVATE, 525, NULL\nIt seems file descriptor 4 corresponds to:\nbash\njose@jose-laptop ~/Documentos/proyectos/cuponeitor/cuponeitor-web $ ls -al /proc/7521/fd | grep -i \" 4 \"\nlr-x------ 1 jose jose 64 Feb  2 23:56 4 -> /home/jose/.rvm/rubies/jruby-1.7.14/lib/jruby.jar\n. I found that I can (or so it seems) prevent the deadlock by making sure the process is not a daemon in my config/puma.rb and initializing just the strictly necessary things in case it is not.\n. If it doesn't work, wouldn't it make sense disabling it and/or adding it to the documentation? I spent a lot of time investigating this issue (which was fun and since this is a personal project I could do it without much time pressure), but it would have been nice to know!\nAnyway, if it helps anyone, the issue seems to be gone (or at least I could not reproduce it) if you run certain commands in the puma.rb only if the current process is the daemon one:\nruby\nalready_daemon = ENV.key?(\"PUMA_DAEMON_PERM\") || ENV.key?(\"PUMA_DAEMON_RESTART\")\nI have the feeling it is somehow related to the socket file, because usually deleting the file was fixing the problem and the next time it would restart...\n. ",
    "pimeys": "Now I tested with the 1.6.3 and seeing the same error there also.\n. It's also happening here. When adding lots of traffic, some requests leak responses to other requests.\n. ",
    "kreynolds": "@evanphx Got a report of this still being broken as of 2.0.1. Here are some details that were forwarded to me.\nhttps://gist.github.com/kakekake89/5743129\n. Same here, Centos 7, ruby 2.3.3, puma 3.6.2, with or without --daemon, all specified via the command line, no config file. No state file either, not sure if there should be one.\npuma config.ru -t 0:4 --pidfile /home/user/puma.pid -S /home/user/puma.state. ",
    "calavera": "@steveklabnik \n\na de facto standard for identifying the originating protocol of an HTTP request, since a reverse proxy (load balancer) may communicate with a web server using HTTP even if the request to the reverse proxy is HTTPS\n\nhttp://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Common_non-standard_request_headers\n. @steveklabnik let me explain you with an example why this is important.\nLet's say I want to use puma to serve this application:\nruby\nproc do |env|\n  request = Rack::Request.new(env)\n  [200, {}, [request.url]]\nend\nIt only prints the incoming url as text, but it could also return a json document with hypermedia links.\nNow, let's say I want to use Nginx and ssl to serve the content of this app. This is the most common nginx configuration to do that, skipping configuration directives for brevity:\n```\nupstream simple-app {\n  server 127.0.0.1:9292;\n}\nserver {\n  listen localhost:443 default_server;\n  ssl on;\n  location / {\n    proxy_pass http://simple-app\n...\n```\nNginx serves responses in the port 443 and passes the incoming requests to puma using http.\nSo, then you write this url in your browser https://localhost. You'd expect the rack application to return that same url in the body, but instead it returns this https://localhost:80.\nAs you can see, we didn't set any port when we typed the url in the browser, because it assumes we're using the default https port, 443, but the application says that the request came from the port 80. This is because rack is not generating the incoming url properly here:\nhttps://github.com/rack/rack/blob/master/lib/rack/request.rb#L312\nIn that method, the scheme is https but the incoming port is 80. Since 80 is not the default port for https, rack sets the port in the url to 80, here;\nhttps://github.com/rack/rack/blob/master/lib/rack/request.rb#L314\nBut, why rack thinks that the port is 80 when we didn't set any port in the url? Because puma is hardcoding the incoming port to 80.\nThe way load balancers, and web servers in general, have to tell application servers that the request was forwarded from a secure connection is the header HTTP_X_FORWARDED_PROTO. And application servers must respect this header to return properly formatted urls.\n. ",
    "jtomaszewski": "I had exactly the same problem.\nAfter resolving this with luislavena's solution, now I got these errors:\n``` log\nC:/Ruby193/bin/ruby.exe extconf.rb --with-opt-dir=C:/Ruby193/devkit/x64\ncreating Makefile\nmake\ngenerating puma_http11-i386-mingw32.def\ncompiling http11_parser.c\next/http11/http11_parser.rl: In function 'http_parser_execute':\next/http11/http11_parser.rl:111:3: warning: comparison between signed and unsigned integer expressions\ncompiling io_buffer.c\nio_buffer.c: In function 'buf_to_str':\nio_buffer.c:118:3: warning: pointer targets in passing argument 1 of 'rb_str_new' differ in signedness\nc:/Ruby193/include/ruby-1.9.1/ruby/intern.h:644:7: note: expected 'const char ' but argument is of type 'uint8_t '\ncompiling mini_ssl.c\nIn file included from mini_ssl.c:2:0:\nc:/Ruby193/include/ruby-1.9.1/ruby/backward/rubyio.h:2:2: warning: #warning use \"ruby/io.h\" instead of \"rubyio.h\"\nmini_ssl.c: In function 'engine_read':\nmini_ssl.c:110:14: warning: unused variable 'n'\nmini_ssl.c: In function 'engine_write':\nmini_ssl.c:133:8: warning: unused variable 'buf'\ncompiling puma_http11.c\nlinking shared-object puma/puma_http11.so\nmini_ssl.o: In function engine_inject':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:88: undefined reference toBIO_write'\nmini_ssl.o: In function engine_free':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:15: undefined reference toBIO_free'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:16: undefined reference to BIO_free'\nmini_ssl.o: In functionengine_read':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:114: undefined reference to SSL_read'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:120: undefined reference toSSL_want'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:122: undefined reference to SSL_get_error'\nmini_ssl.o: In functionraise_error':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:100: undefined reference to SSL_get_error'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:101: undefined reference toERR_error_string'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:103: undefined reference to ERR_clear_error'\nmini_ssl.o: In functionengine_write':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:140: undefined reference to SSL_write'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:145: undefined reference toSSL_want'\nmini_ssl.o: In function raise_error':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:100: undefined reference toSSL_get_error'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:101: undefined reference to ERR_error_string'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:103: undefined reference toERR_clear_error'\nmini_ssl.o: In function engine_alloc':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:26: undefined reference toBIO_s_mem'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:26: undefined reference to BIO_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:27: undefined reference toBIO_ctrl'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:29: undefined reference to BIO_s_mem'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:29: undefined reference toBIO_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:30: undefined reference to BIO_ctrl'\nmini_ssl.o: In functionengine_init_client':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:70: undefined reference to DTLSv1_method'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:70: undefined reference toSSL_CTX_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:71: undefined reference to SSL_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:72: undefined reference toSSL_set_verify'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:74: undefined reference to SSL_set_bio'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:76: undefined reference toSSL_set_connect_state'\nmini_ssl.o: In function engine_alloc':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:26: undefined reference toBIO_s_mem'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:26: undefined reference to BIO_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:27: undefined reference toBIO_ctrl'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:29: undefined reference to BIO_s_mem'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:29: undefined reference toBIO_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:30: undefined reference to BIO_ctrl'\nmini_ssl.o: In functionengine_init_server':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:48: undefined reference to SSLv23_server_method'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:48: undefined reference toSSL_CTX_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:51: undefined reference to SSL_CTX_use_certificate_file'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:52: undefined reference toSSL_CTX_use_PrivateKey_file'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:55: undefined reference to SSL_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:60: undefined reference toSSL_set_bio'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:62: undefined reference to SSL_set_accept_state'\nmini_ssl.o: In functionengine_extract':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:160: undefined reference to BIO_ctrl'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:162: undefined reference toBIO_read'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:165: undefined reference to BIO_test_flags'\nmini_ssl.o: In functionraise_error':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:100: undefined reference to SSL_get_error'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:101: undefined reference toERR_error_string'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:103: undefined reference to ERR_clear_error'\nmini_ssl.o: In functionengine_alloc':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:26: undefined reference to BIO_s_mem'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:26: undefined reference toBIO_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:27: undefined reference to BIO_ctrl'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:29: undefined reference toBIO_s_mem'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:29: undefined reference to BIO_new'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:30: undefined reference toBIO_ctrl'\nmini_ssl.o: In function raise_error':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:100: undefined reference toSSL_get_error'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:101: undefined reference to ERR_error_string'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:103: undefined reference toERR_clear_error'\nmini_ssl.o: In function Init_mini_ssl':\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:176: undefined reference toSSL_library_init'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:177: undefined reference to SSL_library_init'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:178: undefined reference toSSL_load_error_strings'\nC:\\Ruby193\\lib\\ruby\\gems\\1.9.1\\gems\\puma-2.0.0.b7\\ext\\puma_http11/mini_ssl.c:179: undefined reference to `ERR_load_crypto_strings'\ncollect2: ld returned 1 exit status\nmake: *** [puma_http11.so] Error 1 \n```\nSeems like the openssl file [it's version?] doesnt really fit?\n. Makes huge sense =) I have mistakenly installed 64bit OpenSSL instead of x86.\nNow I have successfully installed Puma 2.0.1 correct. Thanks.\n. ",
    "chenzx": "Can you put the package file in GitHub? groups.google.com is blocked in china. Thanks!\n. Thanks for response.\nI've successfully install puma-2.0.1 on Windows 7(64bit machine), Ruby-2.0. as a x86 Win32 mode.\nThere is a warning:\nC:\\knapsack\\x86-windows>gem install puma --pre -- --with-opt-dir=C:/knapsack/x86-windows\nBuilding native extensions with: '--with-opt-dir=C:/knapsack/x86-windows'\nThis could take a while...\nSuccessfully installed puma-2.0.1\nParsing documentation for puma-2.0.1\nunable to convert \"\\x90\" from ASCII-8BIT to UTF-8 for lib/puma/puma_http11.so, skipping\nI think it should not matter.\n. Also, i've installed puma-2.0.1 successfully on Ruby-2.0-x64:\nC:\\knapsack\\x64-windows>bsdtar --lzma -xf openssl-1.0.0k-x64-windows.tar.lzma\nC:\\knapsack\\x64-windows>SET PATH=C:\\Knapsack\\x64-windows\\bin;%PATH%\nC:\\knapsack\\x64-windows>SET CPATH=C:\\Knapsack\\x64-windows\\include\nC:\\knapsack\\x64-windows>SET LIBRARY_PATH=C:\\Knapsack\\x64-windows\\lib\nC:\\DevKit-x64>C:\\Ruby200-x64\\bin\\gem dk.rb init\nERROR:  While executing gem ... (Gem::CommandLineError)\n    Unknown command dk.rb\nC:\\DevKit-x64>C:\\Ruby200-x64\\bin\\ruby dk.rb init\nInitialization complete! Please review and modify the auto-generated\n'config.yml' file to ensure it contains the root directories to all\nof the installed Rubies you want enhanced by the DevKit.\nC:\\DevKit-x64>C:\\Ruby200-x64\\bin\\gem install puma\nFetching: rack-1.5.2.gem (100%)\nSuccessfully installed rack-1.5.2\nFetching: puma-2.0.1.gem (100%)\nBuilding native extensions.  This could take a while...\nSuccessfully installed puma-2.0.1\nParsing documentation for rack-1.5.2\nInstalling ri documentation for rack-1.5.2\nParsing documentation for puma-2.0.1\nunable to convert \"\\x90\" from ASCII-8BIT to UTF-8 for lib/puma/puma_http11.so, skipping\nInstalling ri documentation for puma-2.0.1\n2 gems installed\n. ",
    "jonahx": "@luislavena tyvm!\n. ",
    "datnguyen0606": "I got the problem\nError: While executing gem...OptionParser::InvalidOption\ninvalid option: --with-opt-dir=C:/knapsack/x86-windows\nAnyone knows how to deal with it?\n. @luislavena yes, that is exactly what I type in command prompt: gem install puma -- --with-opt-dir=c:\\openssl.\nHowever, I know the problem, I used Windows PowerShell to type the command, it makes double dashes become weird characters, so Gem could not parse the command, and raised the error.\nThanks for your help.\n. ",
    "nicohvi": "I just wanted to thank @luislavena for the solution, worked ilke a charm! :+1: \n. ",
    "pancakeCaptain": "@luislavena \nThank you so much for the solution you provided, this was extremely helpful!\n. ",
    "EvilFaeton": "@grk Look at this, it's not only puma problem https://github.com/rails/rails/issues/4652\n. ",
    "NoICE": "I'm just fighting similar problem, but under heavy load. Request sometimes aren't even logged in rails log, but the cliend asked for it gets response for previous one.\nSometimes, it logs both requests, e.g. GET /api/next_batch.json, then GET /image.gif, but client asking for that JSON gets GIF89a... and crashes...\nI'm desperate. I don't know how to debug this, or who is causing it :-(\nEDIT: using puma 2.8.1 and rails 4.0.3 and 4.0.10 (happens in both of them, both seem have that patch mentioned in this issue)\nEDIT: I found similar issue and it could be it: my custom new middleware seems not to be threadsafe. So consider this whole comment useless... Thanks for your time anyway :-) May it help someone else crossing it.\n. ",
    "mkwiatkowski": "I have a similar problem with 2.0.0b5 and 2.0.0b6, under rvm with jruby-1.7.2. The exception is somewhat different in my case:\nLoadError: load error: puma/puma_http11 -- java.lang.UnsatisfiedLinkError: failed to load shim library, error: /home/mk/.rvm/rubies/jruby-1.7.2/lib/native/i386-Linux/libjruby-cext.so: cannot open shared object file: No such file or directory\n  require at org/jruby/RubyKernel.java:1027\n   (root) at /home/mk/.rvm/gems/jruby-1.7.2/gems/puma-2.0.0.b5/lib/puma/server.rb:16\n  require at org/jruby/RubyKernel.java:1027\n   (root) at /home/mk/.rvm/gems/jruby-1.7.2/gems/puma-2.0.0.b5/lib/puma/cli.rb:1\n  require at org/jruby/RubyKernel.java:1027\n   (root) at /home/mk/.rvm/gems/jruby-1.7.2/gems/puma-2.0.0.b5/lib/puma/cli.rb:4\n     load at org/jruby/RubyKernel.java:1046\n   (root) at /home/mk/.rvm/gems/jruby-1.7.2/gems/puma-2.0.0.b5/bin/puma:1\n     eval at org/jruby/RubyKernel.java:1066\n   (root) at /home/mk/.rvm/gems/jruby-1.7.2/bin/ruby_noexec_wrapper:14\nIt seems 2.0.0b5 introduced a C extension that causes the problem.\n. @sabcio @joneslee85 Lack of proper exit code is pretty annoying, I'm gonna look into this and will put a pull request.\n. This should be closed then?\n. ",
    "estill01": "I just got the same problem as @habermann24 with 2.0.0.b7 on JRuby 1.7.3, specifically:\njruby 1.7.3 (1.9.3p385) 2013-02-21 dac429b on Java HotSpot(TM) 64-Bit Server VM 1.6.0_37-b06-434-11M3909 [darwin-x86_64]\nLoadError: load error: puma/puma_http11 -- java.lang.RuntimeException: C extension initialized against invalid ruby runtime\nI've switched to 2.0.0.b4, and that works, but it would be great to use the latest build.\nAny guidance? Thanks.\n. It seems this is all due to the fact that there are two versions of Puma 2.0.0.b7: One for JRuby that doesn't require C-extensions, and one for MRI Ruby that does.\nHere's the appropriate gem to use with JRuby:\nhttp://rubygems.org/gems/puma/versions/2.0.0.b7-java\nHere's the tweet exchange/explanation:\nhttps://twitter.com/gopumago/status/326484700866695168\n. Puma 2.0.0.b4 does not require using JRuby C-extensions and is the latest version of Puma that I've been able to get to work.\nIf you want to use a newer version (i.e. 2.0.0.b7), you can get Bundler to install Puma with JRuby by turning on JRuby's C-extension support:\n$ export JRUBY_OPTS=\"-Xcext.enabled=true\"\n$ bundle install\nBUT, then you'll hit a different issue when you go to actually turn on the server (See: https://github.com/puma/puma/issues/207#issuecomment-16822678).\n. Apparently there are two versions of the Puma 2.0.0.b7 build; one for JRuby that doesn't require C-Extensions, and one for MRI Ruby that does.\nHere's the appropriate gem you should install:\nhttp://rubygems.org/gems/puma/versions/2.0.0.b7-java\nHere's the tweet explaining the situation:\nhttps://twitter.com/gopumago/status/326484700866695168\n. ",
    "tisba": "I'm hitting the same issue. I'm also having a simple hello world rack app, using puma 1.6.3 on jruby 1.7.3...\n. ",
    "johnae": "+1\n. This doesn't work on JRuby (1.7.3). I just tested 2.0.0.b7 and when I send USR2 to the puma process it restarts, however any request coming in during the restart is dropped so it's not like Unicorn, more like a standard restart. Puma also seemed to hang completely every other restart, not responding to requests at all and the Puma process at 200-600% cpu (I've got 8 cores). Not sure if this is supposed to keep the same PID, it does though.\n. ",
    "peterdk": "Sorry, I mean it's the readme of the github root.\n. ",
    "mohamedhafez": "Fork & Exec using FFI is error prone (see the 'Update' at the bottom of headius' blog post: http://blog.headius.com/2009/05/fork-and-exec-on-jvm-jruby-to-rescue.html), though it looks like if you use posix_spawn or the Spoon gem it would solve the problem mentioned in the blog post and be perfectly reliable\n. awesome! well, i guess i've got no more excuses left to put off switching over:)\n. oh one last question: is this feature available as of the last stable release (1.6.3), or is this only starting with 2.0? \n. Playing around with things a bit more, it looks like puma already does graceful shutdown for the cli, the reason I wasn't seeing it is because i was sending TERM and INT to rails s, where it just fails immediately, at least on JRuby 9.0.4.0.\n. ",
    "elucid": "Apologies if this is obvious to others, but do hot restarts with JRuby actually work? That is, was the README changed to include JRuby as a supported hot restart runtime because it was actually tested, or simply because it was learned that JRuby supports exec via FFI?\nI ask because I have spent some time trying to get it to work with various versions of Puma and have had no luck whatsoever. I really hope that there is something obvious I am missing in which case I would be happy to update the README with more specific instructions for JRuby. If someone has a working hot restart setup with JRuby please do post a gist.\nHowever, if this is not the case or there are still unresolved issues, I think it would be more appropriate to at the very least provide some caveats in the README or possibly even take JRuby out of the list of runtimes that support hot restarts.\n. Hi @evanphx, I send USR2, the restart message prints in the log, and once the first request hits the new version of the app it crashes.\n. I put together a trivial Rails app and ran it on JRuby 1.7.3 with Puma 2.0.0.b7 and hot restarts worked without causing a crash :blush:\nMy original test had used Puma 1.6.3 (which is currently the default version that is installed by Rubygems and/or Bundler) and lead to crashes like:\nSyntaxError: /home/vagrant/.rvm/rubies/jruby-1.7.3/bin/jruby:1: Invalid char `\\177' ('\u007f') in expression\u007fELF`4\u00e4\u00b24  ( 444  TTT \u00ac \u00ac\u00f0\u00ae\u00f0\u00f0\u00b0\n                                                                                                                                         \u00e8\u00e8hhhDDQ\u00e5tdR\u00e5td\u00f0\u00ae\u00f0\u00f0/lib/ld-linux.so.2GNUGNU\ufffd\u00b3S\\]HM\u00d9\u00d8\u0099Q\u00e8_|-Y\u00d1a $aPY^`\u00a2%\u0091y\u00c8\u0081\n^\nI then found this thread and saw that I should be using 2.0.0.b7. However, instead of installing it from Rubygems I tried to build Puma from the master branch in the Github repo. I had trouble getting it to build correctly.\nAnyway, TL;DR is to use 2.0.0.b7 from Rubygems and things will work correctly.\nAside from the trivial app I was also able to run and hot-restart the full JRuby app I had originally experimented with. I did not run into the aforementioned issue with Puma sometimes hanging after restarts. I did however see that requests were dropped while restarts were in progress.\n. ",
    "asok": "I'm also confused about the state of the \"hot\" restart in puma. Please correct me if any of these statements are wrong:\n- the \"hot\" restart is the synonym of \"phased-restart\" in the puma world\n- the \"phased-restart\" can be initiated via pumactl -S puma.state phased-restart, or kill -s USR2 pumapid\n- the \"phased-restart\" is only possible when running puma in the clustered mode\n- the \"phased-restart\" is only available on platforms which support fork. It does not work on jruby, neither on windows\n- the last point means that on jruby there is no \"hot\" restart feature - when application is restarting the pending request are dropped.\n. ",
    "nathansamson": "I think the spawning is EXACTLY the problem. (AKA as forking not being\navailable on JRuby/WIndows).\nYou can do this sort of manually current (assuming you have nginx or\nanother reverse proxy in front of your webserver).\nStart up puma on socket X. Have proxy point to socket X. (or TCP on port X)\n...\nDeploy new version, start up puma on socket Y, wait for puma to become\navailable\nMove proxy to socket Y\nReload proxy; Old connections should still have gone through socket X.\n(Depending on your proxy/config it might kill them...)\nKill old puma process (and let it wait till it handled all connections with\na given timeout).\n...\nRepeat\nRegards,\nNathan\nOn Mon, Jan 27, 2014 at 3:56 PM, Jan Berdajs notifications@github.comwrote:\n\nYes you are right, but it would still be possible on JRuby to spawn\nanother puma worker/instance, wait for it to boot, and then redirect the\nsocket to the new instance, then stop the old instance.\nNo-downtime restarts is something we need for JRuby too.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/210#issuecomment-33374064\n.\n. I think this is actually quit important. I noticed I was \"leaking\" a database connection per master process today. If you only have very limited connections every one of them should count...\n\nQuoting Heroku's docs\n\nMulti-process servers\nFor a forking server such as Unicorn, the master process will boot your rails applications (and execute any initializers) and then fork workers. For this reason it\u2019s necessary to disconnect in your master process in the before_fork and then re-establish the connection in an after_fork block:\n. Hi,\n\nI had a user reporting that (sometimes) they viewed the source code of the website. Sometimes HTML, sometimes the JavaScript AJAX response. This seemsed to have been caused by using the back button and caches etc...\nI've never found a way to reproduce this reliably (although I've seen this myself), but it occured in different browsers (Firefox, Chrome at least) and different OS (linux, windows).\nCould this problem been caused by this?\nMy setup is (F5) -> nginx -> Puma (rails 3.2)\nNote that F5 was not always present.\nI thought I already posted this, so if I posted thissomewhere else my apologies...\nRegards,\nNathan\n. I am having a similar issue with puma 2.3.2 but didn't test without multiple workers or another app server, so it might be some other gem, although this is very suspicious.\n. Correct code is obviously:\nThread.new do\n    while true do\n            p ObjectSpace.count_objects\n            sleep 60\n        end\nend\nI did this and ran a very small test (running the webserver for 5 minutes, doing NO requests at all).\n(Note every 60 seconds it prints the output 4 times, since I have 4 workers).\nhttps://gist.github.com/nathansamson/8123477\nNote that while :TOTAL is being more or less consistent over time the number of strings is slightly increasing over time (~40 every 60seconds).\nThis is without doing anything on the puma webserver....\nThe same observation (40 extra strings every 60s) was made with an empty rack app. (still the same Gemfile though).\n. https://github.com/puma/puma/blob/master/History.txt\n. While I understand above's comment I think we should also take into account\nthat it might be useful to know how many workers in total were spawned by\nthe master process.\nOne approadch could be to have a name-like index (describing aboves\nbehaviour), plus a real number that follows the current behaviour.\nRegards,\nNathan\nOn Mon, Jan 27, 2014 at 12:24 PM, Sudara notifications@github.com wrote:\n\nOne little wrinkle here. Lets say we are running 4 workers:\npuma: cluster worker 0: 14488\n  puma: cluster worker 1: 14488\n  puma: cluster worker 2: 14488\n  puma: cluster worker 3: 14488\nIf I kill off worker #2 https://github.com/puma/puma/issues/2 it ends\nup being replaced with a worker #4 https://github.com/puma/puma/issues/4\npuma: cluster worker 0: 14488\n  puma: cluster worker 1: 14488\n  puma: cluster worker 3: 14488\n  puma: cluster worker 4: 14488\nHowever, normally one would have monit config with named pids like\npuma_worker_0.pid and depend on there being a static set of workers, in\nthis case named 0 through 3.\ncheck process puma_worker_0\n  with pidfile /myapp/tmp/pids/puma_worker_0.pid\n  if totalmem is greater than 130 MB for 2 cycles then exec \"/etc/monit/scripts/puma kill_worker 0\"\ncheck process puma_worker_1\n  with pidfile /myapp/tmp/pids/puma_worker_1.pid\n  if totalmem is greater than 130 MB for 2 cycles then exec \"/etc/monit/scripts/puma kill_worker 1\"\ncheck process puma_worker_2\n  with pidfile /myapp/tmp/pids/puma_worker_2.pid\n  if totalmem is greater than 130 MB for 2 cycles then exec \"/etc/monit/scripts/puma kill_worker 2\"\n...\nA fix for this situation would be have next_worker_index always return\n\"the next lowest index available between 0 and @options[:workers] like so:\ndef next_worker_index\n  all_positions =  0...@options[:workers]]\n  occupied_positions = @workers.map { |w| w.index }\n  available_positions = all_positions - occupied_positions\n  available_positions.first\nend\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/440#issuecomment-33359700\n.\n. Booting rails has a huge advantage (especially with preload_app and/or Copy-On-Write rubys) in memory consumption.\n\nI believe it is well documented that you should open connection on on_worker_boot to get conneciton pooling working.  I think it could be clarified more that in fact you should do something similar for every connections you have to 3rd party server (eg Redis, Memcache, ...)\n```\nconfig/puma.rb\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\n```\n. Looks good to me.\nSince ActiveRecord is reaping connection (apparantly after 10s in your\ncase), it only will go up to 64 if there are actually 64 concurrent\nrequests.\nIf this is not the case there is no need to create the connections.\nAt elast this is my understanding of how ActiveRecord works...\nRegards,\nNathan\nOn Fri, Mar 14, 2014 at 10:31 PM, John Bachir notifications@github.comwrote:\n\nThe number of open database connections doesn't correlate with what I\nexpect given my configuration. I'm on Heroku running 2 dynos. My config is:\nan initializer, puma.rb\nthreads(ENV[\"WEB_THREADS_MIN\"] || 16, ENV[\"WEB_THREADS_MAX\"] || 16)\nanother initializer, database_connection.rb:\nRails.application.config.after_initialize do\n  ActiveRecord::Base.connection_pool.disconnect!\nActiveSupport.on_load(:active_record) do\n    config = Rails.application.config.database_configuration[Rails.env]\n    config['reaping_frequency'] = ENV['DB_REAP_FREQ'] || 10 # seconds\n    config['pool']              = ENV[\"WEB_THREADS_MAX\"] || 16\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\nmy Procfile line which starts puma:\nweb: bundle exec puma --port $PORT --environment $RAILS_ENV --config config/puma.rb\nMy heroku ENV config has WEB_THREADS_MIN and WEB_THREADS_MAX both set to\n32. I am running 2 dynos. So, expected database connections is 2x32=64. I\nalso have a sidekiq worker running, which should be an additional 32\nthreads and connections. Here is what librato shows me that heroku is\nreporting:\n[image: screen shot 2014-03-14 at 5 28 02 pm]https://f.cloud.github.com/assets/6097/2426070/90defb94-abbf-11e3-906c-0f30e1fe6571.png\nSo, something is clearly not right here. Any ideas?\n\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/502\n.\n. I think your list is displaying 12 (4*3) threads in 3 processes. Seems\nconsider consistent with config.\nOp 24 apr. 2014 04:09 schreef \"cheng\" notifications@github.com:\nMy puma config\nworkers 3\npreload_app!\nthreads 4, 8\nps aux|grep puma|grep master\n1002      5434  0.0  0.5 274564 90044 ?        Sl   09:54   0:00 puma 2.8.2 (unix:///opt/www/my_project/shared/sockets/puma.socket)\nps aux|grep puma|grep worker\n1002      3427  0.0  0.5 822696 94648 ?        Sl   09:51   0:00 puma: cluster worker 0: 5026\n1002      3439  0.0  0.5 822888 94704 ?        Sl   09:51   0:00 puma: cluster worker 0: 6968\n1002      3442  0.0  0.5 822888 94728 ?        Sl   09:51   0:00 puma: cluster worker 1: 6968\n1002      3445  0.0  0.5 822888 94704 ?        Sl   09:51   0:00 puma: cluster worker 2: 6968\n1002      3451  0.0  0.5 822868 94680 ?        Sl   09:51   0:00 puma: cluster worker 0: 9115\n1002      3454  0.0  0.5 822868 94676 ?        Sl   09:51   0:00 puma: cluster worker 1: 9115\n1002      3457  0.0  0.5 822868 94688 ?        Sl   09:51   0:00 puma: cluster worker 2: 9115\n1002      3516  0.0  0.5 822696 94624 ?        Sl   09:51   0:00 puma: cluster worker 1: 5026\n1002      3519  0.0  0.5 822696 94628 ?        Sl   09:51   0:00 puma: cluster worker 2: 5026\n1002      5437  0.0  0.5 817356 89524 ?        Sl   09:54   0:00 puma: cluster worker 0: 5434\n1002      5441  0.0  0.5 817356 89492 ?        Sl   09:54   0:00 puma: cluster worker 1: 5434\n1002      5450  0.0  0.5 817356 89492 ?        Sl   09:54   0:00 puma: cluster worker 2: 5434\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/526\n.\n. I actually had the same problem (at least very similar).\n\nSometimes when users did a Ajax request, and went to another page (normal\nlink), and then pressed back button or something, the Ajax response would\nbe shown as plaintext.\nI've resolved that issue by fixing a few cases where a Ajax request would\nreturn in a HTML response (eg. an unaunthenticated response that only had a\nHTML template, and not a JS template).\nFixing that seemed to have solved the problems.\nThis was with Ruby 2.0, puma relatively old, and rails 3.2\nRegards,\nNathan\nOn Thu, May 15, 2014 at 4:23 PM, Robert Kranz notifications@github.comwrote:\n\nHi there,\ni kinda have a problem with puma :(\nOccasionally my javascript assets from my rails app get served as plain\ntext from puma. If i switch to webrick, everything is fine.\nBut i can't reproduce that bug, sometimes it works, sometimes not. Chrome\ntells me, he interpreted it as script anyway, but because the http headers\ngot written in the file somehow, i get a javascript error.\nDid anyone experienced such a problem before and has an idea how to fix\nthat?\nEnvironment Infos: Windows 64 Bit, Ruby 2.0.0, Puma 2.8.2, Rails 4.1.0\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/535\n.\n. Would this solve any real world problems? Or is just to make sure it doesn't cause any issues?\n. Hi,\n\nWe have noticed similar behaviour. Our PostgreSQL server was timing out the connection after some inactivity. After this timeout the web server would be unresponsive.\nFirst fix was to  use unicorn, which resolved it for us, although we really would like to use Puma.\nAnother fix was to enable Keep alive messages, which also seem to have solved the problem, although we didn't use that in our production deploy yet to validate if this really fixes the problem.\nI am still confused why Puma (or Unicorn) would be meddling with Database connections, as that seems to be a rails task to keep the DB Pooling alive...\n. @evanphx  yes that would make sense. And yes we are using ActiveRecord.\nBut I was under the impression that ActiveRecord would kill off broken connections itself... It seems to have troubles recovering from suddenly broken (TCP) connections though...\n. Hi\nI had a similar problem a while ago. Every time my app would crash the\nconnection was not returned and after 5 crashes (pool size) my app would\nhang\nI believe my problem went away by adding / modyfing my puma config with the\nfollowing settings\nbefore_fork do\n\ndefined?(ActiveRecord::Base) and\n    ActiveRecord::Base.connection.disconnect!\nend\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    config = ActiveRecord::Base.configurations[Rails.env] ||\n                Rails.application.config.database_configuration[Rails.env]\n    config['reaping_frequency'] = ENV['DB_REAP_FREQ'] || 10 # seconds\n    config['pool']            =   ENV['DB_POOL'] || 2\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\n\nNot sure if that would help though...\nI can go look back in my version control to look at the exact changes I\nmade if this is not enough\nNathan\nOn Wed, Jan 13, 2016 at 6:27 PM, Tom Caflisch notifications@github.com\nwrote:\n\nI can give you access to my production app if you'd like. You can really\nsee it then.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/595#issuecomment-171371869.\n. This was the fix. The upgrade to puma 2.9 was probably irrelavant\nfix.txt\n\nbefore this change I constantly had problems, afterwards no issues anymore. I don't have a good explanation why this would fix that issue but for me it did...\n. What happens if all workers are busy?\nNathan\nOn Mon, Dec 1, 2014 at 10:01 AM, Emanuel Rietveld notifications@github.com\nwrote:\n\nI absolutely love puma and I'm using it for all my applications. Usually,\nthose applications are threadsafe. Sometimes they are not. When an\napplication is running single threaded, either with threads 1, 1 in puma\nconfig or allow_concurrency=false in application config, certain kinds of\nrequests can deadlock.\nFor example, let's say I have a application that generates a PDF from a\npage server side. The first request is for the PDF, then the PDF creator\ntool (e.g. phantomjs) internally does a second request to the app. If\nyou're unlucky, and the second request is routed to the same puma worker,\nthe second request will not be handled. It will either block in rack::lock\nor in puma itself until the first request is finished. However the first\nrequest will not finish until the second request is handled.\nAnother example is a git http server like grack, which will deadlock if\nyou have pre-receive hooks that make calls to your application.\nI realize puma may not be the best application server for a single\nthreaded application but I really like puma and I hope I can standardize\nour application server on it. With the below patch, and config threads 1,\n1, the problem is solved for us because puma workers that are 'busy' don't\naccept new requests. Therefore the request will be routed to another worker.\nHowever I don't understand puma code that well so I'm worried I've broken\nsome other things with this patch. Please advise.\ndiff -ur puma-2.10.2.orig/lib/puma/server.rb puma-2.10.2/lib/puma/server.rb\n--- puma-2.10.2.orig/lib/puma/server.rb 2014-12-01 08:48:32.322000000 +0100\n+++ puma-2.10.2/lib/puma/server.rb      2014-12-01 08:49:48.792000000 +0100\n@@ -184,7 +184,7 @@\n                 break if handle_check\n               else\n                 begin\n-                  if io = sock.accept_nonblock\n-                  if !pool.busy and io = sock.accept_nonblock\n                   client = Client.new io, nil\n                   pool << client\n                 end\n  @@ -293,7 +293,7 @@\n               break if handle_check\n             else\n               begin\n-                  if io = sock.accept_nonblock\n- if !pool.busy and io = sock.accept_nonblock\n               client = Client.new io, @binder.env(sock)\n               pool << client\n             end\ndiff -ur puma-2.10.2.orig/lib/puma/thread_pool.rb puma-2.10.2/lib/puma/thread_pool.rb\n  --- puma-2.10.2.orig/lib/puma/thread_pool.rb    2014-12-01 08:48:32.333000000 +0100\n  +++ puma-2.10.2/lib/puma/thread_pool.rb 2014-12-01 09:45:59.449000000 +0100\n  @@ -114,6 +114,16 @@\nprivate :spawn_thread\n-    # A thread may become available after busy leaves the critical section\n-    # this means that a request may not be handled even though it can be.\n-    # However, since there is only one thread that adds work, the calling thread,\n-    # the reverse should never happen.\n-    def busy\n-      @mutex.synchronize do\n-        (@todo.size - @waiting) >= (@max - @spawned)\n-      end\n-    end\n  +\n   # Add +work+ to the todo list for a Thread to pickup and process.\n   def <<(work)\n     @mutex.synchronize do\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/612.\n. This looks like your browser limiting number of requests per host.\nOp 22 mrt. 2015 6:54 AM schreef \"thiaguerd\" notifications@github.com:\nsingle rack\nbuttons, click in the button to execute command\nI am doing like this\n: rails new test_app\n: add \"gem 'puma'\" on Gemfile\n: rails g controller multiple index slow\n: change method slow to\ndef slow\nsleep 5\nend\n: export SECRET_KEY_BASE='MY_SECRET_KEY'\n: on prodiuction.rb\nconfig.assets.compile = true\n: add on routes\npost ':controller/:action'\nget ':controller/:action'\n: index.html.erb\n<% 20.times do %>\n<%= button_to \"Go!\", {controller: \"multiple\", action: \"slow\"}, remote:\ntrue %>\n<% end %>\n: mv slow.html.erb slow.js.coffee\n: in slow.js.coffee\nconsole.log \"done\"\nusing WEBrick: 1 at a time\nusing puma: 6 at a time\nwhat I need to execute all 20 or more request at a time?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/675#issuecomment-84527206.\n. Hi,\n\nExcept for different ports you probably also need different PID & state\nfiles (if you are using pumactl).\nCan you post the logs of both apps at the time of boot/shutdown. Also the\nconfig for both would be helpful, startup commands .\nI've been running multiple pumas on the same VM without problems, and\ntechnically there is no reason why one instance would affect the other one,\nunless they share state / config.\nNathan\nOn Thu, Jun 4, 2015 at 10:42 AM, trekr5 notifications@github.com wrote:\n\nHi,\nI have a scenario where I run two web apps powered by puma server running\non a linux box. However when one server starts, the other shuts down.\nCan someone please tell me how to enable puma to run multiple instances. I\nalso tried running the puma servers on different ports but have been\nunsuccessful.\nMany thanks\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/708.\n. looks good, any logs we can have a look at?\n\nNathan\nOn Thu, Jun 4, 2015 at 11:28 AM, trekr5 notifications@github.com wrote:\n\nHi Nathan,\nI've created two dashbaords both powered by puma servers on the same linux\nbox.\nThese dashboards are stored in different locations on the same box and the\nstart puma command for each dashboard run from these directories.\nThis is my config.ru file for both:-\nconfig.ru\nrequire 'dashing'\nconfigure do\nset :auth_token, 'YOUR_AUTH_TOKEN'\nset :bind, '0.0.0.0'\nhelpers do\ndef protected!\nPut any authentication code you want in here.\nThis method is run before accessing any resource.\nend\nend\nend\nmap Sinatra::Application.assets_prefix do\nrun Sinatra::Application.sprockets\nend\nand my command to start puma server 1 is:-\n\"bundle exec puma -d config.ru tcp://0.0.0.0:9292 --pidfile\n/var/run/puma.pid\"\nwith pid stored as /var/run/puma.id\nMy command to start puma server 2 is:\nbundle exec puma -d config.ru tcp://0.0.0.0:3030 --pidfile\n/var/run/puma-3030.pid\nwith the pid stored as /var/run/puma-3030.pid\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/708#issuecomment-108805622.\n. Are you testing this in development?\n\nIn development rails will refuse (at least in default config) to allow multiple requests at the same time.\n. This option was only added in v2.13, you are running 2.11. Upgrade to the\nlatest version of puma to resolve this issue.\nRegards,\nNathan\nOn Wed, Feb 10, 2016 at 9:31 AM, vincenzor notifications@github.com wrote:\n\nHi,\nI'm using capistrano/puma and I have a problem when trying to add a\nbefore_fork on my puma.rb config file. I'm using Ruby 2.3.0 with RVM.\nI run:\ncap production puma:start\nthis is the command that is executed:\nRACK_ENV=production /usr/local/rvm/bin/rvm default do bundle exec puma -C\n/PATH_TO_MY_APP/shared/puma.rb --daemon\nHere is the log:\nDEBUG [6ef5b3ad]    : undefined method before_fork' for #<Puma::DSL:0x00000001133db8> (NoMethodError)\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/dsl.rb:20:ininstance_eval'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/dsl.rb:20:in _load_from'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/dsl.rb:9:inblock in load'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/dsl.rb:8:in tap'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/dsl.rb:8:inload'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/configuration.rb:35:in load'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/cli.rb:545:inparse_options'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/lib/puma/cli.rb:190:in run'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/gems/puma-2.11.3/bin/puma:10:in'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/bin/puma:23:in load'\nDEBUG [6ef5b3ad]        from /PATH_TO_MY_APP/shared/bundle/ruby/2.3.0/bin/puma:23:in'\nIn simply added this in puma.rb:\nbefore_fork do\n  # configuration here\nend\nAny idea??\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/886.\n. Where is it documented that performance l dynos have 12 threads?\n\nOn Thu, 7 Mar 2019, 10:21 Marco Colli, notifications@github.com wrote:\n\nThis is the result that I get with performance L for a simple html page\n(without db):\n\nhttps://ldr.io/2VITU8t (4 workers, max ~900req/s)\nhttps://ldr.io/2UpRqvA (8 workers, max ~1500req/s)\nhttps://ldr.io/2Uofpet (12 workers, max ~1500req/s)\n\nI can see an improvement from 4 workers to 8 but then it stops. So the max\nthat I can get is 1500req/s.\nBasically:\n\nrunning 20 instances of 1x standard dynos (tot $500) produces\n   ~6000req/s\nrunning 1 instance of performance L dynos (tot $500) produces\n   ~1500req/s\n\nIt's either Heroku pricing that is wrong or Puma that scales better\nhorizontally than vertically.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1734#issuecomment-470448490, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAFuJ44XmdCgisH7qnpJtrQ9s3IIh_qPks5vUNn9gaJpZM4baZ-j\n.\n. \n",
    "rosenfeld": "@evanphx in my tests sending USR2 to a MRI puma process really didn't miss any request but it does with a JRuby puma process until the application finishes restarting. I'm not preloading the application. Any ideas why it doesn't behave the same way in JRuby?\n. I figured out what the problem is:\nhttps://github.com/rails/rails/issues/16878\nShouldn't Puma wrap the res_body.close call in some timeout-like block?\n. This is tricky. Nginx has a default of 60s but this is only for the first byte sent. After that an streamed response could take any time.\nOn the other side, by the point you call close the application has already finished processing the request but was unable to deliver the full content yet. This could happen for big streams and slow clients. The proper timeout is very dependent on the application and usage profile. It's more important to me that it's configurable than that we have a good default.\nFor my application I'm experiencing with 120s but the throughput for this particular application is low. Maybe the default could be 60s.\nAlso, when a timeout happen it would be useful to either point to some url with more information on the issue and suggestions to fix it or to recommend directly to try increasing the proxy buffer size if one is used. Unless you are able to detect the connection is lost in some reliable way. If this was the case we shouldn't even call close maybe. \n. I'm curious. Why is close called by Puma? Is this part of Rack specs? \n. Hi @lfalcao, while this is certainly possible, the main reason why I'm requesting this feature is not that I want a better way to restart Puma. I'd really prefer to avoid restarting the application just to clear up some cache if possible. This way I can have better granularity. There are parts of the application that depend on specific tables from the database. When I change one of those tables that rarely change, I'd like to be able to only clear the related cache rather than all of them, for example...\n. That's really a good idea. I haven't thought about it before. Thanks :)\n. Thanks! Any reasons for not simply rescue Exception? \n. I see, it makes sense. Even when no memory is available I believe it's a valid attempt trying to better handle the response. Thanks. \n. :+1:  My Puma process got stuck today once again (it only happens in development mode so far, so I suspect it's related to some auto-reloading code I have) and it would be very helpful if I could get a dump for all threads. Unfortunately I can't get this backtrace by attaching gdb to the process because MRI doesn't allow me to inspect Ruby backtraces for threads not own by gdb or something like that. I'm planning to implementing this by using some custom code around the 'listen' gem by watching some file, but I agree with @mperham that a signal supported by Puma would be best as it would allow me to inspect any Puma process without any custom instrumentation set by me previously... I was about to create the same request when I saw @mperham have already created one.\n. Hi @alexdowad, thanks for the reply and sorry for not answering to that before but it has been a very atypical week around here with lots of urgent issues to work on lately. Although it's not finished yet I could find some time to respond to this issue.\nFirst of all, unfortunately, no auto-reloading is set in production for performance issues, so it wouldn't work to update that file when the issue is present as any changes wouldn't take effect until I restart the app but restarting the app would make the issue go away anyway. So, I'm guessing your suggestion is to  perform some changes before starting the application, which can be a bit tricky since our deploy is automated, but I think I can make some changes to this file in the Docker container that is used to run the app. I'll try to figure out some way to let you know for sure that it's gone through this branch although I suspect it is because the message I was getting was exactly like that.\nSince I no longer have a running application in that state, I'll have to wait for it to happen again before I can answer your questions.\nWith regards to the middleware stack it's a tricky question though. This is a Roda app, which makes use of the multi_run plugin:\nhttp://roda.jeremyevans.net/rdoc/classes/Roda/RodaPlugins/MultiRun.html\nThere's a main app that is composed by several other Rack/Roda apps, each one with their own set of middlewares set up, including quite some custom middlewares specifically suited for our own app. So, depending on the route, a different set of middlewares would be in place. For the static assets route, this is served directly by the main app. The main app has the following middlewares included in this specific order:\n\nBugsnag::Rack (https://github.com/bugsnag/bugsnag-ruby/blob/master/lib/bugsnag/integrations/rack.rb)\nProtectSourceMaps (custom middleware, will include the source below)\nRack::Static (see params below)\nRack::Session::Cookie\nThe remaining ones below are custom ones, but I think they are irrelevant as they don't get run for the /assets path:\nDbSession\nRequestId\nRequestLogger\n\nHere's the source for ProtectSourceMaps:\n```ruby\nfrozen-string-literal: true\nhide source-maps from non-authorized users\nrequire 'rack'\nrequire_relative 'db_session'\nmodule Apps\n  module Middleware\n    # warning: when using a CDN, this should require some special handling for .map files, telling\n    # the CDN that it shouldn't cache the request and always forward it to the server\n    class ProtectSourceMaps\n      def initialize(app)\n        @app = app\n      end\n  def call(env)\n    path = env['PATH_INFO']\n    if (match = path.start_with?('/assets') && path.end_with?('.map'))\n      return [ 403, {}, [] ] unless allow? env\n    end\n    status, headers, body = @app.call env\n    if match\n      headers = Rack::Utils::HeaderHash.new(headers)\n      headers['cache-control'] = 'private, max-age=31536000'\n    end\n    [status, headers, body]\n  end\n\n  ALLOWED_IPS = [ '104.196.245.109', '104.196.254.247' ] # bugsnag servers:\n  # https://docs.bugsnag.com/platforms/browsers/source-maps/\n  def allow?(env)\n    ip = env['HTTP_X_FORWARDED_FOR']&.split(',', 2)&.first\n    return true if ALLOWED_IPS.include? ip\n    session = DbSession::SessionHash.new Rack::Request.new env\n    session[:roles]&.any?{|r| r['role'] == 'admin' }\n  end\nend\n\nend\nend\n```\nHere's how Rack::Static is used:\nruby\nuse Rack::Static, urls: ['/assets', '/favicon.ico'], root: AppSettings.assets_path,\n  header_rules: [\n    [:all, {\n      'cache-control' => 'public, max-age=31536000',\n      'access-control-allow-origin' => '*',\n    }]\n  ]\nSo, maybe, for the time being, pehaps it would be better to simply copy the entire source of rack/file into my project and make the edits and use it instead of the one provided by the Rack gem. Then, once I have all details I can let you know and move back to the original implementation.\nI'll try to find some time this week to perform those changes.\nThanks for the heads up.. Hi @alexdowad, that's right, only in production and only a few times. Mind you that only about a dozen users seem to use the application in a given month and use it quite often. However, the requests to static resources are pretty rare since they're usually in those users' cache after the first visit after a new deployment, which probably explains why sometimes I notice and fix the issue even before our clients realize the problem.\nOur monitoring scrips perform some actions in the app every minute and, to save some time in the script execution, we run a headless Chrome instance with cache enabled which means our monitoring scripts currently don't detect this issue as the static resources would be usually in cache, so I'm not alerted when that happens in production. I should probably either disable caching in the scripts or add a new check to specifically make sure I can fetch any static resource anyway.\nOur app runs through a systemd service which basically runs Puma through a Docker container. When that issue happens, I noticed that running \"systemctl reload app-name\" would fix the issue and it basically runs \"docker kill -s USR1 container-name\", which I believe is what \"pumactl restart\" would do as well. That means it wouldn't help because as soon as I send the signal to Puma the issue would be fixed and I wouldn't be able to debug anything.\nI can try adding a local check in my development machine to see if I can reproduce the issue, but I'm not that confident that I'll be able to :)\nIdeally it would be great if I could somehow add remote breakpoints when adding some special params to the assets requests so that I could investigate what's happening when the issue happens. We use blue-green deployment, so the next time the issue arises I could perform a new deploy to the green version, for example, leaving the blue version as is so that I could debug it without interrupting the production services. The main problem is to understand how I can inspect the issue while the app is running without restarting the app, since restarting the app would immediately fix the issue, and I'm not talking about stopping and restarting the container, but by sending the USR1 signal to Puma.\nSo far I've instrumented the app to use a copy of 'rack/file' and 'rack/static' with some extra headers and logs to the original 'rack/file' so that it might give me a hint on why it's returning 404 sometimes. I intend to deploy this new version later today or tomorrow since it seems our client will be demonstrating our product today so I want to avoid any bugs caused by that change during the demonstration.\nAlso, I'm sorry but I sent you the middleware stack of another app, rather than the one experiencing this issue. For this one, there's only a single middleware in place before Rack::Static, which is a custom one:\n```ruby\nfrozen-string-literal: true\nmodule Apps\n  module Middleware\n    class DefaultCharset\n      def initialize(app)\n        @app = app\n      end\n  def call(env)\n    status, headers, body = result = @app.call(env)\n    h, v = headers.find{|k, v| k =~ /\\Acontent-type\\z/i }\n    if h && v !~ /charset/\n      headers[h] = \"#{v}; charset=UTF-8\"\n    end\n    result\n  end\nend\n\nend\nend\n```\nThen Rack::Static was used like this:\nruby\nuse Rack::Static, urls: ['/assets', '/favicon.ico'], root: 'public'. I've just added this to my crontab, I'll let you know if I'll be ever notified about it:\n* * * * * /bin/bash -c \"curl -f -s http://127.0.0.1:3000/favicon.ico || notify-send 'static assets returning 404'\". Hi @alexch, I could finally get to that scenario I described in the issue in our production environment.\nI've instrumented file.rb to something like this:\n```ruby\n        is_file = is_readable = system_call_error = nil\n        available = begin\n          #::File.file?(path) && ::File.readable?(path)\n          (is_file = ::File.file?(path)) && (is_readable = ::File.readable?(path))\n        rescue SystemCallError => e\n          msg = [e.message, e.backtrace.join(\"\\n\")].join(\"\\n\\n\")\n          puts msg\n          MatterhornSettings.instance.logger.error msg\n          system_call_error = e.message\n          false\n        end\n    if available\n      serving(request, path)\n    else\n      headers = { # extra debugging info\n        'x-path' => path,\n        'x-is-file' => is_file.to_s,\n        'x-is-readable' => is_readable.to_s,\n        'x-system-call-error' => system_call_error.to_s,\n      }\n      fail(404, \"File not found: #{path_info}\", headers)\n    end\n\n```\nThen the exact same request is sometimes successful and other times it returns \"File not found: /assets/xyz.min.js\".\nWhen I inspect the HTTP headers I see that x-is-file is set to false.\nThe path seems to be correct as I get the x-path header as public/assets/xyz.min.js.\nThe application is served with a few Puma workers in the same Docker container so I suspect some of the workers are working just fine while others are returning false for ::File.file?(path)).\nMaybe I should also send the x-pwd header to include Dir.pwd and see if the current directory is somehow changed by the application at some point, what do you think?\nMaybe I should replace root: 'public' to something like root: File.expand_path('public')''. I guess this could explain the issue, right? What do you think?\nThanks in advance!. I guess I should suggest the Rack team to either raise an error when a relative path is passed to Rack::Static or to resolve it to a full path during the initialization of the middleware, do you think that would make sense?. By the way, my crontab entry wasn't really working, I had to change it to set DISPLAY and XAUTHORITY env vars in order to actually get the notifications.. I've added that to the instrumentation code, but I suspect I won't be able to confirm because, if I'm right, I won't experience this bug anymore now that I'm passing the full path to the root argument.\nI'll try to find some time to submit a patch to Rack next week that will expand the path upon the initialization of the middleware.\nI do have this Dir.chdir block in one place as I mentioned in the Rack issue, but it should get back to the original directory so, unless there's a Ruby bug preventing that from happen in some circumnstances, that wouldn't explain the bug. Let me further explain it. We use a blue-green deployment strategy. When the bug happened I've issued a new deployment to the green variation (assuming we were running the application in the blue variation). After I switch it to the green variation no more requests are dispatched to the blue variation, however, I still get the intermitent 404 responses when requesting those assets in the blue variation and I'm pretty sure no other requests are being sent to blue when green is the active one, so I'm pretty sure that if pwd had changed for one of the workers it remained changed, without rolling back to the original directory. I couldn't find any usage of Dir.chdir without the block being provided.\nAs I said in the other issue, my code is currently not thread-safe, and I intend to fix it next week, but I think Ruby core should make sure the original pwd is restored even if concurrent calls to Dir.chdir{} take place. Anyway, it's my mistake and I'll fix it next week hopefully.\nThanks :). Indeed, I've sent a patch to Rack, which was merged a while ago and I forgot to close the issue here, sorry.. ",
    "welearnednothing": "I've never seen this work with JRuby, either. All incoming requests after the USR2 signal is sent immediately fail until the restart is complete.\n. ",
    "jeremyhaile": "I don't understand. So JRuby is one of the recommended Ruby platforms to run puma, but there is no way to do a hot restart?  \nHas anyone found a workaround for this? Are there other recommended rails servers that do support hot restart on JRuby?\n. @evanphx I guess I'm just wondering if there was a workaround or what people were doing to address that, since JRuby is a common platform for puma. Or if there were any plans to support some type of hot restart on JRuby.\nIt looks like we'll have to switch to passenger if I can't find any other options. Even the open-source passenger supports \"hot restart\" (in puma's terminology).  You need Enterprise passenger for phased restarts.  But before I did that I wanted to see if anyone had found a workaround with puma. (other than rolling our own proxy/multiport solution)\n. @evanphx ok - good to hear it does support hot restart. It wasn't working for me on JRuby 9.0.1.0 on Mac OS X Capitan. Thanks for responding so quickly.\n. Puma is recommended to run on jruby, but doesn't have hot restarts? This is pretty much a deal killer for using puma on jruby production apps. Has there been any update to this?\n. @evanphx Why not? Isn't that something that should be supported?\n. The problem I'm having is that if you start it as puma, then the logging doesn't operate like rails server But if you start it as rails server puma then it doesn't load our configuration, which includes reading environment variables and on_worker_boot callbacks. It seems like the inability to load the configuration file makes rails server puma unusable for many projects. \nIs that true? If it is true, perhaps we should update the README to reflect this.\n. ",
    "thapakazi": "long time no see. any update on this one ?\n@evanphx Is it possible, you could explain your methods of doing hot restarting PUMA with JRUBY || we have a something easy to work with added by now... . ",
    "joneslee85": "I close to submit a another fixed version\n. @pfleidi I do bump into the same issue, even with ruby 2.0, I ended up using monit to manage the process for me, but it is good that you raise this up\n. @pfleidi make sure you run rackup (if you use rails) to see if your app could start correctly, I have just fixed one issue in which -d failed to run due to the fact config.ru is causing problems\n. @evanphx it does start correctly without -d\n. @sabcio I think we should give some error results should the app fail to start, thanks for pointing it out\n. Duplicated of #253\n. @kakekake89 run rackup and see if it starts correctly first\n. @kakekake89 the control server seems very broken at the time, I do also have to clean up socket files after restart. Anyway, please read the blog post (http://ruby-journal.com/how-to-setup-rails-app-with-puma-and-nginx/). I highly recommend use the PID management way outlined in that blog post.\n. @evanphx I'd like to give one feedback on this commit, the pumactl start does not pick up the configuration in the statefile, instead pumactl overrides the existing state file.\n. @pelcasandra as far as I understand, pumactl does not pick up state file correctly when starting. :( \n. @pelcasandra use the reliable PID way, I have snippet here https://gist.github.com/joneslee85/5844933\n. @pelcasandra we do not use SysV initd, instead we use monit to manage processes, give it a try\n. Never mind, this is my mistake, I forgot to specify unix://, I close it now\n. @evanphx IMHO, we should encourage people to use pumactl instead of puma. Maybe removing puma completely?\n. this should be extracted to a separate gem\n. @radar i know, this ticket is for extracting the cap task outside, I still prefer this gem hosted under /puma\n. Sure, I'll make a PR\nOn Thu, Feb 20, 2014 at 6:03 AM, Stas Su\u0219cov notifications@github.comwrote:\n\n@modology https://github.com/modology @joneslee85https://github.com/joneslee85still interested in making the PR?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/420#issuecomment-35534993\n.\n. @mfilej I verify this issue. This issue only happen with single mode.\n. Good find. I can verify this occurs on latest 1.7.1\n. :+1: from me\n. Last version 2.8.0 introduced this, for this commit I believe it will be in the next version\u2014\nSent from Mailbox for iPhone\n\nOn Sat, Mar 8, 2014 at 11:17 PM, Abdelkader Boudih\nnotifications@github.com wrote:\n\n@evanphx from which version will feature be implemented ?\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/pull/493#issuecomment-37096217\n. \n",
    "afeistenauer": "Just found out that I can get the newest version from rubygems instead of github and that works.\n. @evanphx: Still don't see it. You changed History.txt and lib/puma/const.rb, puma.gemspec still has the commit message of Bump to V2.0.0.b6 as I see it.\n. ",
    "tamird": "Bit of a hop-on: are you guys able to run puma with SSL at all?\nI followed heroku's guide and generated a self-signed certificate. Then in my basic rails scaffold app I run puma -b 'ssl://127.0.0.1:9292?key=server.key&cert=server.crt'. Going to that address in chrome spins for a while and finally dies with net::ERR_INCOMPLETE_CHUNKED_ENCODING.\nAnyone else experiencing this?\n. Similarly curl'ing fails:\nbash\n$ curl -k https://127.0.0.1:9292\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:--  0:00:20 --:--:--     0\ncurl: (18) transfer closed with outstanding read data remaining\nBy the way, this works with puma 1.6.3 (in ruby 2.0.0).\n. womp womp\nhttp://jira.codehaus.org/browse/JRUBY-6874?focusedCommentId=332498&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-332498\nhttps://github.com/jruby/jruby/commit/06d62433b8ff54f1a90ebab0ff5c0fb80ce30826\n. :+1: \n. This is also broken on RBX.\n. @evanphx repro steps here: https://github.com/puma/puma/issues/223#issuecomment-16563769\n. ",
    "cure": "Yep, confirmed, Puma + SSL is broken in Puma 2.0.1 (ruby 2.0.0). I see the same thing.\n. ",
    "brblck": "Looks like @headius just committed a patch for this issue that's scheduled to ship with JRuby 1.7.5.\nhttp://jira.codehaus.org/browse/JRUBY-6874\nhttps://github.com/jruby/jruby/commit/a4397921ee4512cd1d4d3ce5c0e78ef19cadd859\nTest your little hearts out!\n. ",
    "nmccready": "Why is this CLOSED?\n. This issue happens on Jruby-1.7.4 as well.\n. Is there a specific issue for this yet or a specific issue naming convention that should be defined? The logs that I have are posted on issue #360 . I'll see if I can get a deeper trace.\n. This is all I have it repeats indefinitely for each client. \n2013-09-10 17:11:25 -0400: Read error: #<Errno::EPIPE: Broken pipe - Broken pipe>\norg/jruby/RubyIO.java:1294:in `syswrite'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:762:in `fast_write'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:589:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:470:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:343:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:339:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:242:in `run'\norg/jruby/RubyProc.java:255:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/thread_pool.rb:92:in `spawn_thread'\n2013-09-10 17:11:25 -0400: Rack app error: #<ThreadError: Mutex relocking by same thread>\norg/jruby/ext/thread/Mutex.java:90:in `lock'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-1.4.5/lib/rack/lock.rb:14:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/actionpack-3.2.14/lib/action_dispatch/middleware/static.rb:63:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in `forward'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:in `fetch'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in `lookup'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:in `call!'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/railties-3.2.14/lib/rails/engine.rb:484:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/railties-3.2.14/lib/rails/application.rb:231:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/newrelic_rpm-3.5.8.72/lib/new_relic/rack/developer_mode.rb:24:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-1.4.5/lib/rack/deflater.rb:13:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/configuration.rb:68:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:472:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:470:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:343:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:339:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:242:in `run'\norg/jruby/RubyProc.java:255:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/thread_pool.rb:92:in `spawn_thread'\n2013-09-10 17:11:25 -0400: Read error: #<Errno::EPIPE: Broken pipe - Broken pipe>\norg/jruby/RubyIO.java:1294:in `syswrite'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:762:in `fast_write'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:589:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:470:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:343:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:339:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:242:in `run'\norg/jruby/RubyProc.java:255:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/thread_pool.rb:92:in `spawn_thread'\n2013-09-10 17:11:25 -0400: Read error: #<Errno::EPIPE: Broken pipe - Broken pipe>\norg/jruby/RubyIO.java:1294:in `syswrite'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:762:in `fast_write'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:589:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:470:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:343:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:339:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:242:in `run'\norg/jruby/RubyProc.java:255:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/thread_pool.rb:92:in `spawn_thread'\n. I just double checked to see if multiple instances of puma may be an issue. However locusio still DOS'es a single instance of puma after hitting it around 300-350RPS. Also the errors are the same.\n. Pull / Fix https://github.com/puma/puma/pull/369\n. Ditto same issue.\n. If you really need SSL and puma right now put nginx in front of it. Use\nUnix sockets to communicate and have nginx do the SSL.\nOn Sep 3, 2013 4:09 PM, \"Jan Zimmek\" notifications@github.com wrote:\n\nSame problem here. 2.5.1 fails on 1.9.3p194. Same Key/Cert works totally\nfine on 1.6.3\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/350#issuecomment-23742895\n.\n. +1\n. Appears to happen from tons of these after a client has disconnected. \n\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/thread_pool.rb:92:in `spawn_thread'\n2013-09-06 16:28:01 -0400: Read error: #<Errno::EPIPE: Broken pipe - Broken pipe>\norg/jruby/RubyIO.java:1294:in `syswrite'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:762:in `fast_write'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:589:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:470:in `handle_request'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:343:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:339:in `process_client'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/server.rb:242:in `run'\norg/jruby/RubyProc.java:255:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/puma-2.5.1-java/lib/puma/thread_pool.rb:92:in `spawn_thread'\n2013-09-06 16:37:21 -0400: Rack app error: #<ThreadError: Mutex relocking by same thread>\norg/jruby/ext/thread/Mutex.java:90:in `lock'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-1.4.5/lib/rack/lock.rb:14:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/actionpack-3.2.14/lib/action_dispatch/middleware/static.rb:63:in `call'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in `forward'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:in `fetch'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in `lookup'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:in `call!'\n/home/nem/.rvm/gems/jruby-1.7.4/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:in `call'\n. wrong prject.. I mean here!\nhttps://github.com/puma/puma/issues/274\nThis error is hanging my puma processes. Any idea where a patch may be?\n. This error appears to not happen as long as the output is redirected to the null device.\nScratch that it still is occurring.\n. Fix here https://github.com/puma/puma/pull/369\n. :+1: \n. I will take a look for fun.\n. Any version of Ruby that seem to be causing more problems than others? Is this with MRI or JRuby?\n. Last time the main issue was line 780-781, my guess is we're not catching all the exceptions and false is not returned. Therefore the calling thread hangs.\nWould it be so bad to just add a generic , Exception to \nrescue  Errno::EPIPE, SystemCallError, IOError, Exception??\n          return false\n. When this used to happen to me regularly. It would consistently happen on load testing when all the tests were finished and the test client disconnected. The load test environment I was using was http://locust.io/\n. Side note, I get great speed results (rps) when running two instances . My guess is the upstart job is not finding the pid for the first puma instance and it is starting a second. On the second instance the PID is found. .. just a guess. \n. This looks to be a deadlock in spawn_thread within thread_pool.rb .  It is that or an issue with JRuby.\n. Well then.. that narrows it :) \n. Ok so looking at the code it appears that fast_write is not handling the Exception Errno::EPIPE: Broken pipe, and is only looking for (Errno::EAGAIN, Errno::EWOULDBLOCK) . This is where the exception bubbles up all the way to run. Here is where it is using ThreadPool. Within the mutex itself it does not appear to handle exceptions. Therefore since it fails it appears that the mutex.synchronize holds. Also on rescue I see this within fast_write:\nrescue Errno::EAGAIN, Errno::EWOULDBLOCK\n          IO.select(nil, [io], nil, 1)\n          retry\n        end\nWhich it appears that we retry on every fail, this would also deadlock the system, no? Should there be a retry limit? If it is Errno::EPIPE: Broken pipe should we just fail out and exit out of the thread? Also should the same be done if the exception does not match any of these and catch a generic Exception at the very least.\nJust ideas.\n. I am trying to test my theory / fix and trying to build this sucker is a royal PINA. How do I satisfy the puma_http11 not found problems? Especially when I am directing my application to test the fix to a git repository.\n. IE I am having problems building with #34\n. NVM I got it building and it looks like my theories are correct. It is working with my fixes.\nTo build:\nFirst set $JAVA_HOME\ngem install hoe\nrake compile:puma_http11\nThis was the hint, https://gist.github.com/evanphx/1401543\nThis should be added to the Readme and or Wiki.\n. I will submit a pull shortly.\n. So in ruby speak.. are u saying the rescue should be\nrescue Errno::EAGAIN, Errno::EWOULDBLOCK #keep ????\n          IO.select(nil, [io], nil, 1)\n          retry\n rescue Errno::EPIPE #ditch this?? this is what I added to fix the issue at hand\n          return false\n rescue SystemCallError, IOError #this instead\n   return false\n  end\n. Is there a way to patch 2.5.1 so we can get this fix via gems? or to release 2.5.2 sooner via a patch to 2.5.1?\n. AWESOME! thanks\n. @evanphx How should we plan to use this with a version from gem? Or should there be another branch that is considered stable? IE a branch with this fix only added to 2.5.1?\n. 2.6.0 released\n. Ditto\n. I am not sure if this is entirely puma, I switched to thin as workaround and still see the same error.\n. https://github.com/puma/puma/issues/223\n. ",
    "greghuc": "@nmccready I believe this was fixed by https://github.com/puma/puma/pull/530, which has been merged into master.\nSo I assume it'll be in the next Puma (2.8.3?) release.\n@dmarcotte @evanphx thanks for your hard work - looking forward to https on JRuby puma :-)\n. ",
    "shoutsid": "+1, \nUsing Rails4rc-1, jRuby 1.7.4, Puma 2.0.1\nI use capistrano to restart my server after a deploy. I had to not use -d due to using jRuby and deamonize is not compatiable. so changed I the script accordingly.\nWhen doing an update deploy, will cause the server to lock. \nI have to delete my current/tmp/sockets/puma.sock file and do the cap puma:start command manually before it will work\n. ",
    "slavajacobson": "I am getting the same problem here... please help! \n. ```\nChange to match your CPU core count\nworkers 2\nMin and Max threads per worker\nthreads 1, 6\napp_dir = File.expand_path(\"../../../..\", FILE)\nshared_dir = \"#{app_dir}/shared\"\nDefault to production\nrails_env = ENV['RAILS_ENV'] || \"production\"\nenvironment rails_env\nSet up socket location\nbind \"unix://home/deployer/apps/loco-engine/shared/tmp/sockets/loco-engine-puma.sock\"\n{}\"unix://#{shared_dir}/sockets/puma.sock\"\nLogging\nstdout_redirect \"#{shared_dir}/log/puma.stdout.log\", \"#{shared_dir}/log/puma.stderr.log\", true\nSet master PID and state locations\npidfile \"/home/deployer/apps/loco-engine/shared/tmp/pids/puma.pid\"\nstate_path \"/home/deployer/apps/loco-engine/shared/tmp/pids/puma.state\"\nactivate_control_app\non_worker_boot do\n  require \"active_record\"\n  ActiveRecord::Base.connection.disconnect! rescue ActiveRecord::ConnectionNotEstablished\n  ActiveRecord::Base.establish_connection(YAML.load_file(\"#{app_dir}/config/database.yml\")[rails_env])\nend\n```\n. https://github.com/puma/puma/tree/master/tools/jungle/upstart\n. ",
    "lalitlogical": "I have also facing the same issue as below.\nNo such file or directory - connect(2) for \"/tmp/puma-status-1438845213892-27028\"\nI have set up production server on EC2 and using Nginx and puma for my Rails app. I am using capistrano 3 for deployment. It has worked fine for initial deployment. Puma start successfully, but when I want to restart the puma server, I am getting such error.\nPlease help!\n. Hi @ponny, Can you please help me to resolve this issue?\nI am able to run 'cap production deploy:initial' command successfully. But when I want to redeploy something, puma does not restart and gave this error.\n. @godalphul, content of /config/puma.rb and /config/deploy.rb file look like as below\n/config/puma.rb\n```\nworkers 1\nthreads 4, 16\napp_dir = \"/var/deploy/myapp/current\"\nshared_dir = \"/var/deploy/myapp/shared\"\nport        ENV['PORT']     || 3000\nrails_env = ENV['RAILS_ENV'] || \"production\"\nenvironment rails_env\nbind \"unix://#{shared_dir}/tmp/sockets/puma.sock\"\nstdout_redirect \"#{shared_dir}/log/puma.stdout.log\", \"#{shared_dir}/log/puma.stderr.log\", true\npidfile \"#{app_dir}/tmp/pids/puma.pid\"\nstate_path \"#{app_dir}/tmp/pids/puma.state\"\nactivate_control_app\non_worker_boot do\n  require \"active_record\"\n  ActiveRecord::Base.connection.disconnect! rescue ActiveRecord::ConnectionNotEstablished\n  ActiveRecord::Base.establish_connection(YAML.load_file(\"#{app_dir}/config/database.yml\")[rails_env])\nend\n```\n/config/deploy.rb\n```\nset :puma_threads,    [4, 16]\nset :puma_workers,    1\nset :pty,             true\nset :use_sudo,        false\nset :puma_bind,       \"unix://#{shared_path}/tmp/sockets/puma.sock\"\nset :puma_state,      \"#{shared_path}/tmp/pids/puma.state\"\nset :puma_pid,        \"#{shared_path}/tmp/pids/puma.pid\"\nset :puma_access_log, \"#{release_path}/log/puma.error.log\"\nset :puma_error_log,  \"#{release_path}/log/puma.access.log\"\nset :puma_preload_app, true\nset :puma_worker_timeout, nil\nset :puma_init_active_record, true\nnamespace :puma do\n  desc 'Create Directories for Puma Pids and Socket'\n  task :make_dirs do\n    on roles(:app) do\n      execute \"mkdir #{shared_path}/tmp/sockets -p\"\n      execute \"mkdir #{shared_path}/tmp/pids -p\"\n    end\n  end\nbefore :start, :make_dirs\nend\nnamespace :deploy do\n  desc \"Make sure local git is in sync with remote.\"\n  task :check_revision do\n    on roles(:app) do\n      unless git rev-parse HEAD == git rev-parse origin/capistrano\n        puts \"WARNING: HEAD is not the same as origin/capistrano\"\n        puts \"Run git push to sync changes.\"\n        exit\n      end\n    end\n  end\ndesc 'Initial Deploy'\n  task :initial do\n    on roles(:app) do\n      before 'deploy:restart', 'puma:start'\n      invoke 'deploy'\n    end\n  end\ndesc 'Restart application'\n  task :restart do\n    on roles(:app), in: :sequence, wait: 5 do\n      invoke 'puma:restart'\n    end\n  end\nbefore :starting,     :check_revision\n  after  :finishing,    :compile_assets\n  after  :finishing,    :cleanup\n  after  :finishing,    :restart\nend\n```\nI have also used the below command to generate template for Puma and Nginx as below.\nrails g capistrano:nginx_puma:config\nI have run below commands to deploy my rails to EC2 instance (with Ubuntu)\ncap production deploy:check\ncap production puma:config\ncap production puma:nginx_config\ncap production deploy:initial\nNow, I want to deploy some changes with below code.\ncap production deploy\nBut I am getting the error as below.\n```\n(Backtrace restricted to imported tasks)\ncap aborted!\nSSHKit::Runner::ExecuteError: Exception while executing as ubuntu@54.175.134.149: bundle exit status: 1\nbundle stdout: No such file or directory - connect(2) for \"/tmp/puma-status-1439451994589-14316\"\nbundle stderr: Nothing written\nSSHKit::Command::Failed: bundle exit status: 1\nbundle stdout: No such file or directory - connect(2) for \"/tmp/puma-status-1439451994589-14316\"\nbundle stderr: Nothing written\nTasks: TOP => deploy:restart\n(See full trace by running task with --trace)\n```\n. @h0lyalg0rithm, Are you running the below commands as in sequence?\ncap production deploy:check\ncap production puma:config\nIf you need nginx config also\ncap production puma:nginx_config\nFirst time deploy\ncap production deploy:initial\nAfter first time deploy\ncap production deploy\nIf yes, then check your app directories paths for same. \nFor some thime(when you are not able to setup it properly) you just need to clear your root folder's (i.e. '/home/dev/apps/dev') files, folder and run again above commands in same sequence.\n. @skinofstars, I have posted same issue as above at https://github.com/seuros/capistrano-puma/issues/120. Please take a look into this.\nBut for now, I am using pro version of phusion passenger (with thread functionality).\n. https://github.com/seuros/capistrano-puma/issues/120\n. Thanks. \nActually I want to say that when I insert @ which later convert into % causes the issue.\nJust for confirmation, If I searched for @gmail.com or y@g, lalit@, etc, above commit work?\n. ",
    "godalphul": "@lalitlogical what does your /config/puma.rb and /config/deploy.rb file look like\n. ",
    "h0lyalg0rithm": "@lalitlogical I have the same configuration as yours.\nI got the following error.bundle stdout: No such file or directory - connect(2) for \"/home/dev/apps/dev/shared/tmp/sockets/dev-sock.sock\"\nbundle stderr: Nothing written\n. I tried the above.I even deleted the apps directory.\nI set the puma bind to the default as well as /var/run/puma.sock \nNone of these work.\nMy current project is running puma 2.12.2\nI had project running puma 2.11.3 with same config and it works.\nThey both run the same version of capistrano3-puma\n. ",
    "skinofstars": "Did this ever get resolved? Hitting the same issue. The curious thing to me is the attempted connection to /tmp/puma-status-1444..., whereas pid/state/sock files locations are explicitly declared. \n@lalitlogical how did you solve this?\n. ",
    "aleks": "Same problem here. Any results?\nUpdate: I've got this fixed by the help of this comment.\ncapistrano-puma ignored my puma.rb in shared/puma.rb and used config/puma.rb instead. I had to copy the contents of shared/puma.rb to config/puma.rb to make it work.\n. ",
    "aconfee": "Hello,\nDid anyone figure out what the root cause of this is? I'm not using capistrano and seeing this issue. Does puma.rb config need to be in a specific folder? Or puma.state? \nI can see that my puma.state file is referencing unix:///tmp/puma-status-1487214123928-21430 as the control_url, but I can't find this file, I assume it doesn't exist on my machine. Is something supposed to be creating it? \nAny help is very much appreciated! . @hackedunit Thanks for your quick response! This is helpful, but I still have a couple of questions. (Thanks for your patience, I'm really new to RoR and Linux). \nWith respect to provided example:\n1) /usr/local/bin/puma does not exist in my directory, although Puma is installed and working. What is /usr/local/bin/puma? Might this be installed somewhere else on my machine or did I miss an install step?\n2) Do I need an ExecStop action as well?\nBefore seeing your response, I tried going down a different path creating my .service file ( https://gist.github.com/aconfee/294e9ba8f9b5da8f93a8b14282925680 ) and ran into the following issue. Not to derail your current example, but could you shed any light onto this? http://stackoverflow.com/questions/42264408/cannot-run-puma-state-stop-missing-tmp-puma-status-xxxxxx.\nAs of now, I haven't been able to get this thing to run with rbenv, rvm, or puma. I'm sure I'm missing one tiny detail on all accounts. Thanks for your help! \n. @hackedunit \nPerfect! That's extremely useful to know. Thanks for hanging in with me. \nMy Puma server is now kicking off with that start command, but I'm receiving an error: \nERROR: No application configured, nothing to run\nI don't get this error if I run the entire start command from a terminal in my app directory, but the error appears when starting in systemd, or any other outer directory. This surprises me since my working directory is defined in puma.service (I copy/pasted my working dir and cd'd to it just to make sure no typos). Am I possibly not defining my app directory somewhere else?\nI started digging into Puma's source code, and it looks like it's failing on:\ndef load_and_bind\n      unless @launcher.config.app_configured?\n        error \"No application configured, nothing to run\"\n        exit 1\nend\napp_configured seems to check for the existence of a Puma config file or Rack config file (but maybe I'm mistaken) \nI can't seem to figure out what's wrong with my configuration. Do you see anything blatantly wrong or missing from my configs below?\nconfig.ru\nhttps://gist.github.com/aconfee/899ce7e7a4c720d634159fe844ea396b\npuma.rb\nhttps://gist.github.com/aconfee/85cef926b7462bb9c0ce7ba0e53f1d97\npuma.service\nhttps://gist.github.com/aconfee/8ad3fb3a96dad045bdff60a5ad8823ea\nI want to thank you one more time. I can't tell you how helpful this is.. @hackedunit \nThanks! That was an issue, and I also had to specify my 'directory' in puma.config. It looks like systemd is successfully starting puma now :) (Although, there are still a few unrelated kinks: https://github.com/puma/puma/issues/1218) \nFor anyone else who might read this in the future, this was extremely helpful: https://github.com/puma/puma/blob/master/examples/config.rb. \nThank you for all of your help!. ",
    "Rolando-Barbella": "@aconfee Did you manage to fix this?. I did: \npkill -f rpush\nstart-nameoftheapp\nIt work for now. ",
    "adamthedeveloper": "To get around this temporarily, I changed my Gemfile:\ngem 'puma', :git => \"git://github.com/puma/puma.git\"\nBut - alas - the capistrano deployment won't work because it will complain that the \"-d\" flag is invalid. \nSee issue https://github.com/puma/puma/issues/232\n. if you want daemon mode as well as the capistrano file, adjust your Gemfile:\nruby\ngem 'puma', '2.0.0.b7'\n. I figured this out. If you want daemon mode, you need to do:\nruby\ngem 'puma', \"2.0.0.b7\" # or greater\n. ",
    "shawnl": "I'm going to need another example in order to figure out how to use this...\nwith all the required files, lets say a simple sinatra app.\nThe provided .service unit doesn't work for me, specifically the ExecStart line.\n. ",
    "niedhui": "hi @tophe , seems the example/config.rb file is too old.\nenvironment 'production'\nthis should work. no =, string value\n. ",
    "samqiu": "+ 1\n. ",
    "jgarber": "Thanks! Works great now!\n. ",
    "acds": "With JRuby running in 2.0 compatibility mode, running rails 3.2.11. \n$ ruby -v\njruby 1.7.11 (2.0.0p195) 2014-02-24 86339bb on Java HotSpot(TM) 64-Bit Server VM 1.6.0_65-b14-462-11M4609 [darwin-x86_64]\nI still seem to get the C Extension install issue when installing the gem via git\nruby\ngem 'puma', :git => 'https://github.com/puma/puma.git'\nI get this error:\n```\nGem::Ext::BuildError: ERROR: Failed to build gem native extension.\n/Users/<user>/.rvm/rubies/jruby-1.7.11/bin/jruby extconf.rb\n\nNotImplementedError: C extension support is not enabled. Pass -Xcext.enabled=true to JRuby or set JRUBY_OPTS.\n(root) at /Users//.rvm/rubies/jruby-1.7.11/lib/ruby/shared/mkmf.rb:8\n  require at org/jruby/RubyKernel.java:1085\n   (root) at /Users//.rvm/rubies/jruby-1.7.11/lib/ruby/shared/rubygems/core_ext/kernel_require.rb:1\n   (root) at extconf.rb:1\nextconf failed, uncaught signal 1\nGem files will remain installed in /Users//.rvm/gems/jruby-1.7.11@rails-prelaunch-signup/bundler/gems/puma-de59a88c864c for inspection.\nResults logged to /Users//.rvm/gems/jruby-1.7.11@rails-prelaunch-signup/bundler/gems/extensions/universal-java-1.6/1.9/puma-de59a88c864c/gem_make.out\n```\n. I was trying to eliminate the\nError in reactor loop escaped (Java::JavaNioChannels::CancelledKeyException)\nerror, and found this https://github.com/puma/puma/issues/352 recommending the git approach. That is when I ran into this, but 20/20 hindsight see its not related, and assume this is a regular error when you Ctrl-C out of the server. \nOK dropping the git specification, but assume that this will not be a problem in the future if one wants to experiment.\nThanks!\n. ",
    "narutosanjiv": "@davidcelis  There is jruby version of puma without c extension. Starting from Jruby 1.7, jruby has drop support for c extension. \n. ",
    "ali-bugdayci": "Hi,\nAs i said in the comment the body is the same as the curl command data:\ncurl -XPOST http://localhost:9400/logstash/uygulama_silindi -d '{\"@source\":\"file://ubuntu/home/bor/sunucu/mdm/private/logstash_deneme/ANKARA.ANKAYA.hacettepe.3333.353921050771765.log.30\",\"@tags\":[],\"@fields\":{\"logdate\":[\"04-21 18:22:50.005\"],\"LogLeveL\":[\"W\"],\"package\":[\"Mdm\"],\"il\":[\"ANKARA\"],\"ilce\":[\"ANKAYA\"],\"okul\":[\"hacettepe\"],\"tckimlik\":[3333],\"imei\":[353921050771765],\"uygulama_silindi\":\"com.deneme \",\"package_formated\":[\"com_deneme \"]},\"@timestamp\":\"2013-04-21T15:22:50.005Z\",\"@source_host\":\"ubuntu\",\"@source_path\":\"/home/bor/sunucu/mdm/private/logstash_deneme/ANKARA.ANKAYA.hacettepe.3333.353921050771765.log.30\",\"@message\":\"04-21 18:22:50.005 W/Mdm ( 2067): uygulama_silindi=com.deneme \",\"@type\":\"logcat\"}'\nAs you said so, Json parser gives the error and listing the body which cut 2 characters at the end:\nJSON::ParserError (757: unexpected token at '{\"@source\":\"file://bor-asus4/home/bor/sunucu/mdm/private/uploads/device_error_log/AFYON.\u015eUHUT.ilkokul.2222.353241050241267.log.6_1366718319\",\"@tags\":[],\"@fields\":{\"LogLeveL\":[\"I\"],\"package\":[\"Mdm\"],\"il\":[\"AFYON\"],\"ilce\":[\"\u015eUHUT\"],\"okul\":[\"ilkokul\"],\"tckimlik\":[2222],\"imei\":[353241050241267],\"uygulama_silindi\":\"com.boryazilim.android.mdmtestclient\",\"package_formated\":[\"com_boryazilim_android_mdmtestclient\"]},\"@timestamp\":\"2013-04-23T09:06:49.615Z\",\"@message\":\"04-23 12:06:49.615 I/Mdm ( 2570): uygulama_silindi=com.boryazilim.android.mdmtestclient\",\"@type\":\"logcat'):\nso is it a restriction with reading the whole body into a string ? I suppose using StringIO.read and processing inside the block would give the same result?\n. I used the Puma 2.0.0.b7\nSince I got this error, I switched to Unicorn. Will try testing it with newer puma releases and let you know\n. ",
    "vovimayhem": "Ran into this, also...\nHow's the restart forking implemented with FFI??? (I'm kind of a newbie with threading & jruby)\n. Just trying to give some potential clues.\nI don't know if this is related, but i can't get pass 150 concurrent connections when binding to unix sockets, IT doesn't matter how quick or slow the ruby app is... \nNginx keeps reporting \"Resource temporarily unavailable\" on the 151-th concurrent connection.\nThis is happening to me on a CentOS server, with jRuby.\nHowever, running the same setup on MacOS X (my dev machine) it Gets over 2500 concurrent connections.\nI Hope this helps, I'm pretty stuck right now...\n. ",
    "kugaevsky": "Guys, something wrong with this PR?\n. Oh, I didn't know about it. Thanks for explanation.\n. Yeah, closing it.\n. ",
    "lephyrius": "Im also seeing this!\nUsing Ruby 2.0 and Rails 3.2.13 on Debian 6.\n. For me it is only from restart and stop not stats that gives bad response.\nThe command: \"puma\" works.(to start the server)\nstats doesn't print out anything and not \"bad response\" when running.\n. :+1: I also get this.\n. I dunno if it helps. But I noticed this as well.\nI followed these instructions but I used this Gemfile instead:\n``` ruby\nsource 'https://rubygems.org'\nruby '2.3.1', engine: 'jruby', engine_version: '9.1.5.0'\ngem 'rails', '4.2.7.1'\ngem 'sass-rails', '~> 5.0'\ngem 'uglifier', '>= 1.3.0'\ngem 'coffee-rails', '~> 4.1.0'\ngem 'jquery-rails'\ngem 'turbolinks'\ngem 'jbuilder', '~> 2.0'\ngem 'sdoc', '~> 0.4.0', group: :doc\ngem 'puma'\ngem 'activerecord-jdbcpostgresql-adapter', require: false\ngroup :production do\n    gem 'rails_12factor'\nend\n```\nHere is the log:\n2016-10-11T07:44:29.769553+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.54MB sample#memory_rss=360.48MB sample#memory_cache=0.20MB sample#memory_swap=0.86MB sample#memory_pgpgin=193872pages sample#memory_pgpgout=106650pages sample#memory_quota=512.00MB\n2016-10-11T07:44:49.824181+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.17 sample#load_avg_15m=1.37\n2016-10-11T07:44:49.824257+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.54MB sample#memory_rss=360.48MB sample#memory_cache=0.20MB sample#memory_swap=0.86MB sample#memory_pgpgin=194870pages sample#memory_pgpgout=107648pages sample#memory_quota=512.00MB\n2016-10-11T07:45:10.628343+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.16 sample#load_avg_15m=1.34\n2016-10-11T07:45:10.628414+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.54MB sample#memory_rss=360.48MB sample#memory_cache=0.20MB sample#memory_swap=0.86MB sample#memory_pgpgin=195503pages sample#memory_pgpgout=108281pages sample#memory_quota=512.00MB\n2016-10-11T07:45:30.150945+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.15 sample#load_avg_15m=1.31\n2016-10-11T07:45:30.151103+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.54MB sample#memory_rss=360.48MB sample#memory_cache=0.20MB sample#memory_swap=0.86MB sample#memory_pgpgin=195651pages sample#memory_pgpgout=108429pages sample#memory_quota=512.00MB\n2016-10-11T07:45:49.948651+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.14 sample#load_avg_15m=1.28\n2016-10-11T07:45:49.948651+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.77MB sample#memory_rss=360.48MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=195753pages sample#memory_pgpgout=108473pages sample#memory_quota=512.00MB\n2016-10-11T07:46:08.592752+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.13 sample#load_avg_15m=1.25\n2016-10-11T07:46:08.592846+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.77MB sample#memory_rss=360.48MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=195753pages sample#memory_pgpgout=108473pages sample#memory_quota=512.00MB\n2016-10-11T07:46:29.074225+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.12 sample#load_avg_15m=1.23\n2016-10-11T07:46:29.074225+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.78MB sample#memory_rss=360.49MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=195755pages sample#memory_pgpgout=108473pages sample#memory_quota=512.00MB\n2016-10-11T07:46:49.659795+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.29 sample#load_avg_5m=0.18 sample#load_avg_15m=1.22\n2016-10-11T07:46:49.659909+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.78MB sample#memory_rss=360.49MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=196881pages sample#memory_pgpgout=109599pages sample#memory_quota=512.00MB\n2016-10-11T07:47:09.189253+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.21 sample#load_avg_5m=0.17 sample#load_avg_15m=1.20\n2016-10-11T07:47:09.189253+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.79MB sample#memory_rss=360.49MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=196882pages sample#memory_pgpgout=109599pages sample#memory_quota=512.00MB\n2016-10-11T07:47:29.624636+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.15 sample#load_avg_5m=0.16 sample#load_avg_15m=1.17\n2016-10-11T07:47:29.624770+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.79MB sample#memory_rss=360.50MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=197991pages sample#memory_pgpgout=110707pages sample#memory_quota=512.00MB\n2016-10-11T07:47:50.756658+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.10 sample#load_avg_5m=0.15 sample#load_avg_15m=1.14\n2016-10-11T07:47:50.756745+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.81MB sample#memory_rss=360.52MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=198014pages sample#memory_pgpgout=110725pages sample#memory_quota=512.00MB\n2016-10-11T07:48:09.975309+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.08 sample#load_avg_5m=0.14 sample#load_avg_15m=1.12\n2016-10-11T07:48:09.975359+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.95MB sample#memory_rss=360.66MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=198051pages sample#memory_pgpgout=110726pages sample#memory_quota=512.00MB\n2016-10-11T07:48:28.884269+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.06 sample#load_avg_5m=0.13 sample#load_avg_15m=1.09\n2016-10-11T07:48:28.884342+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.95MB sample#memory_rss=360.66MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=198051pages sample#memory_pgpgout=110726pages sample#memory_quota=512.00MB\n2016-10-11T07:48:49.406683+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.04 sample#load_avg_5m=0.12 sample#load_avg_15m=1.07\n2016-10-11T07:48:49.406808+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.95MB sample#memory_rss=360.66MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=198051pages sample#memory_pgpgout=110726pages sample#memory_quota=512.00MB\n2016-10-11T07:49:09.200609+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.03 sample#load_avg_5m=0.11 sample#load_avg_15m=1.05\n2016-10-11T07:49:09.200662+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.95MB sample#memory_rss=360.66MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=198051pages sample#memory_pgpgout=110726pages sample#memory_quota=512.00MB\n2016-10-11T07:49:29.065597+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.02 sample#load_avg_5m=0.11 sample#load_avg_15m=1.02\n2016-10-11T07:49:29.065804+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.95MB sample#memory_rss=360.66MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=198051pages sample#memory_pgpgout=110726pages sample#memory_quota=512.00MB\n2016-10-11T07:49:49.578417+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.01 sample#load_avg_5m=0.10 sample#load_avg_15m=1.00\n2016-10-11T07:49:49.578512+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.96MB sample#memory_rss=360.67MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=199903pages sample#memory_pgpgout=112574pages sample#memory_quota=512.00MB\n2016-10-11T07:50:09.507955+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.01 sample#load_avg_5m=0.09 sample#load_avg_15m=0.98\n2016-10-11T07:50:09.508014+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.96MB sample#memory_rss=360.67MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=200902pages sample#memory_pgpgout=113573pages sample#memory_quota=512.00MB\n2016-10-11T07:50:29.129918+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.01 sample#load_avg_5m=0.09 sample#load_avg_15m=0.96\n2016-10-11T07:50:29.129999+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.96MB sample#memory_rss=360.67MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202212pages sample#memory_pgpgout=114883pages sample#memory_quota=512.00MB\n2016-10-11T07:50:50.411750+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.01 sample#load_avg_5m=0.08 sample#load_avg_15m=0.93\n2016-10-11T07:50:50.412262+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.96MB sample#memory_rss=360.67MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202242pages sample#memory_pgpgout=114913pages sample#memory_quota=512.00MB\n2016-10-11T07:51:09.263292+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.08 sample#load_avg_15m=0.92\n2016-10-11T07:51:09.263372+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.96MB sample#memory_rss=360.67MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202242pages sample#memory_pgpgout=114913pages sample#memory_quota=512.00MB\n2016-10-11T07:51:29.351524+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.07 sample#load_avg_15m=0.90\n2016-10-11T07:51:29.351576+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.96MB sample#memory_rss=360.67MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202242pages sample#memory_pgpgout=114913pages sample#memory_quota=512.00MB\n2016-10-11T07:51:49.426565+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.07 sample#load_avg_15m=0.88\n2016-10-11T07:51:49.426674+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:52:09.152514+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.06 sample#load_avg_15m=0.86\n2016-10-11T07:52:09.152567+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:52:28.966231+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.06 sample#load_avg_15m=0.84\n2016-10-11T07:52:28.966365+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:52:49.245269+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.05 sample#load_avg_15m=0.82\n2016-10-11T07:52:49.245324+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:53:08.853746+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.05 sample#load_avg_15m=0.80\n2016-10-11T07:53:08.853746+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:53:28.806532+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.05 sample#load_avg_15m=0.78\n2016-10-11T07:53:28.806608+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:53:49.036866+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.04 sample#load_avg_15m=0.77\n2016-10-11T07:53:49.036970+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=361.98MB sample#memory_rss=360.69MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202265pages sample#memory_pgpgout=114931pages sample#memory_quota=512.00MB\n2016-10-11T07:54:09.127486+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.04 sample#load_avg_15m=0.75\n2016-10-11T07:54:09.127588+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.00MB sample#memory_rss=360.71MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=202648pages sample#memory_pgpgout=115309pages sample#memory_quota=512.00MB\n2016-10-11T07:54:29.870491+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.04 sample#load_avg_15m=0.73\n2016-10-11T07:54:29.870583+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.00MB sample#memory_rss=360.71MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204441pages sample#memory_pgpgout=117102pages sample#memory_quota=512.00MB\n2016-10-11T07:54:49.620217+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.04 sample#load_avg_15m=0.72\n2016-10-11T07:54:49.620417+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.00MB sample#memory_rss=360.71MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204441pages sample#memory_pgpgout=117102pages sample#memory_quota=512.00MB\n2016-10-11T07:55:09.770897+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.03 sample#load_avg_15m=0.70\n2016-10-11T07:55:09.770983+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.00MB sample#memory_rss=360.71MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204523pages sample#memory_pgpgout=117184pages sample#memory_quota=512.00MB\n2016-10-11T07:55:29.531150+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.03 sample#load_avg_15m=0.69\n2016-10-11T07:55:29.531217+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.00MB sample#memory_rss=360.71MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204523pages sample#memory_pgpgout=117184pages sample#memory_quota=512.00MB\n2016-10-11T07:55:49.893951+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.03 sample#load_avg_15m=0.67\n2016-10-11T07:55:49.894082+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.01MB sample#memory_rss=360.71MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204525pages sample#memory_pgpgout=117185pages sample#memory_quota=512.00MB\n2016-10-11T07:56:09.650640+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.03 sample#load_avg_15m=0.66\n2016-10-11T07:56:09.650724+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.02MB sample#memory_rss=360.73MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204547pages sample#memory_pgpgout=117203pages sample#memory_quota=512.00MB\n2016-10-11T07:56:30.254665+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.03 sample#load_avg_15m=0.64\n2016-10-11T07:56:30.254762+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.02MB sample#memory_rss=360.73MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204547pages sample#memory_pgpgout=117203pages sample#memory_quota=512.00MB\n2016-10-11T07:56:48.850281+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.02 sample#load_avg_15m=0.63\n2016-10-11T07:56:48.850357+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.03MB sample#memory_rss=360.73MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204548pages sample#memory_pgpgout=117203pages sample#memory_quota=512.00MB\n2016-10-11T07:57:08.835429+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.00 sample#load_avg_5m=0.02 sample#load_avg_15m=0.61\n2016-10-11T07:57:08.835482+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.03MB sample#memory_rss=360.73MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204548pages sample#memory_pgpgout=117203pages sample#memory_quota=512.00MB\n2016-10-11T07:57:29.343623+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.29 sample#load_avg_5m=0.09 sample#load_avg_15m=0.62\n2016-10-11T07:57:29.343738+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.03MB sample#memory_rss=360.73MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204548pages sample#memory_pgpgout=117203pages sample#memory_quota=512.00MB\n2016-10-11T07:57:48.939922+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.21 sample#load_avg_5m=0.08 sample#load_avg_15m=0.61\n2016-10-11T07:57:48.940024+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.03MB sample#memory_rss=360.73MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204548pages sample#memory_pgpgout=117203pages sample#memory_quota=512.00MB\n2016-10-11T07:58:09.362342+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.15 sample#load_avg_5m=0.08 sample#load_avg_15m=0.60\n2016-10-11T07:58:09.362391+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.05MB sample#memory_rss=360.75MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204572pages sample#memory_pgpgout=117222pages sample#memory_quota=512.00MB\n2016-10-11T07:58:30.066368+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.11 sample#load_avg_5m=0.06 sample#load_avg_15m=0.04\n2016-10-11T07:58:30.066470+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=362.05MB sample#memory_rss=360.75MB sample#memory_cache=0.42MB sample#memory_swap=0.88MB sample#memory_pgpgin=204573pages sample#memory_pgpgout=117223pages sample#memory_quota=512.00MB\n2016-10-11T07:58:48.925643+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.08 sample#load_avg_5m=0.06 sample#load_avg_15m=0.04\n2016-10-11T07:58:48.925754+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=372.02MB sample#memory_rss=370.65MB sample#memory_cache=0.56MB sample#memory_swap=0.81MB sample#memory_pgpgin=206669pages sample#memory_pgpgout=117260pages sample#memory_quota=512.00MB\n2016-10-11T07:59:09.219354+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.05 sample#load_avg_5m=0.05 sample#load_avg_15m=0.04\n2016-10-11T07:59:09.219427+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=372.02MB sample#memory_rss=370.65MB sample#memory_cache=0.56MB sample#memory_swap=0.81MB sample#memory_pgpgin=208341pages sample#memory_pgpgout=118932pages sample#memory_quota=512.00MB\n2016-10-11T07:59:29.028619+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.04 sample#load_avg_5m=0.05 sample#load_avg_15m=0.04\n2016-10-11T07:59:29.028691+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=372.02MB sample#memory_rss=370.65MB sample#memory_cache=0.56MB sample#memory_swap=0.81MB sample#memory_pgpgin=208341pages sample#memory_pgpgout=118932pages sample#memory_quota=512.00MB\n2016-10-11T07:59:49.632499+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#load_avg_1m=0.03 sample#load_avg_5m=0.05 sample#load_avg_15m=0.04\n2016-10-11T07:59:49.632499+00:00 heroku[web.1]: source=web.1 dyno=heroku.57768658.b9f48709-d075-4276-84b4-6ece6c0b3a83 sample#memory_total=372.02MB sample#memory_rss=370.65MB sample#memory_cache=0.56MB sample#memory_swap=0.81MB sample#memory_pgpgin=208341pages sample#memory_pgpgout=118932pages sample#memory_quota=512.00MB\n2016-10-11T07:59:57.656626+00:00 heroku[web.1]: Idling\n2016-10-11T07:59:57.657322+00:00 heroku[web.1]: State changed from up to down\n2016-10-11T08:00:01.813526+00:00 heroku[web.1]: Stopping all processes with SIGTERM\n2016-10-11T08:00:03.144058+00:00 app[web.1]: - Gracefully stopping, waiting for requests to finish\n2016-10-11T08:00:03.225204+00:00 app[web.1]: === puma shutdown: 2016-10-11 08:00:03 +0000 ===\n2016-10-11T08:00:03.225277+00:00 app[web.1]: - Goodbye!\n2016-10-11T08:00:03.225400+00:00 app[web.1]: Exiting\n2016-10-11T08:00:04.044624+00:00 heroku[web.1]: Process exited with status 0\n. @schneems It has never done that at least not until it has started to swap. \nI thought that puma is kept inside the java memory space. Or does it use any C bindings?\n. ",
    "amencarini": "@lephyrius thanks for the feedback, and for reminding me to add Ruby and server info :)\n. @evanphx Capistrano executes this:\nbundle exec pumactl -S /var/www/project/shared/sockets/puma.state restart\nFrom what I can see, when I use this to restart, pumactl.sock gets removed, then I get the error, the puma process keeps working and when the whole application becomes available again, pumactl.sock reappears. And yes, I consistently get this, it's not spot. The server is a micro AWS instance: not sure if performance can play a role in this.\n. I've noticed two different behaviours on two different servers: on the faster server we get the 500 error 50% of the times, on the slower server we get it every time.\n. @evanphx I'm pretty sure it's unrelated to this, but I've used the master to see if how it went and a new problem arose, that was tackled in pull request #271. I'm using that patch and Capistrano deployment is now alright, at last :) Thanks!\n. @Arie thanks for the patch, I'm using your fork pending merge :)\n. ",
    "olistik": "Same error experienced with the same setup described by @amencarini \n. Here's a proof of concept that uses wait to receive a notification of the completion of the stop process: https://gist.github.com/olistik/fe93d26e29c3a1417ca462d415610b4e\nWould it be reasonable to rely on some sort of resource to exist (or to be removed) when the process stops?\nOne option could be to pass a flag/option that enables the removal of the PID file when the process stops.\nOr maybe a the state file could be updated accordingly.\nLooking forward to your thoughts about this.. I'm asking for Puma to ease its own process management. Puma itself already addresses some issues related to this very same scope. --state and --pid are an example of features that regards process management and could be implemented on their own.\nLet me know if you still feel that I'm still outside of the desired scope of this project. :-). ",
    "netmute": "Same error here. It doesn\"t always happen, though. Maybe 50% of the time. The server does not correctly restart after throwing that error, I have to manually start puma again.\n. ",
    "Arie": "I get the same thing. Capistrano 2.15, ruby 1.9.3-p392, puma 2.0.1, rails 3.2.13.\n. I found the backtrace the server shows when this 500 happens for me. Need to do some more investigating to wrap my head around what happens in that file.\nPuma caught this error: stream closed\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/server.rb:594:in `write'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/server.rb:594:in `<<'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/server.rb:594:in `begin_restart'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/app/status.rb:34:in `call'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/server.rb:364:in `handle_request'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/server.rb:243:in `process_client'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/server.rb:142:in `block in run'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/thread_pool.rb:92:in `call'\n/usr/local/rvm/gems/ruby-1.9.3-p392@foo/gems/puma-2.0.1/lib/puma/thread_pool.rb:92:in `block in    spawn_thread'\nundefined method `split' for nil:NilClass\n. The control server doesn't get started in cluster mode.\n. Same (ofc), I've pointed my Gemfile to the git repo to work around it for now.\n. ",
    "jockee": "Same here. Capistrano 2.15.4. Rub 2.0.0dev. Puma 2.0.1. Rails 3.2.13.\n. ",
    "macfanatic": "Same problem with the following setup:\ncapistrano 2.15.4\npuma 2.0.1\nrails 4.0.0.beta1\nruby 2.0.0p0\n. Has anyone found an easy workaround to this problem yet? For now I have to remove the tmp files for puma & kill the process itself after doing a ps aux | grep puma. I'm not a linux guru, so I might be making that part hard on myself, but I'd still like to use puma with capistrano and this is making it almost impossible.\n. ",
    "MJLang": "Im getting the same issue - Using Ubuntu 12.10, Ruby 2.0.0, Rails 4.0.0.rc1,  Puma at master and Capistrano 2.15.4\n. ",
    "rudijs": "I'm also seeing this on Ruby 1.9.3\nruby -v\nruby 1.9.3p392 (2013-02-22 revision 39386) [i686-linux]\nrails -v\nRails 3.2.13\n/srv/blog/current$ RAILS_ENV=production bundle exec puma -e production -d -b unix:///tmp/blog.sock  -S /tmp/blog.state --control 'unix:///tmp/blog_pumactl.sock'                        \nPuma 2.0.1 starting...\n- Min threads: 0, max threads: 16\n- Environment: production\n- Listening on unix:///tmp/blog.sock\n/srv/blog/current$ bundle exec pumactl -S /tmp/blog.state restart\nBad response from server: 500\n/srv/blog/current$ bundle exec pumactl -S /tmp/blog.state stop\nundefined method `split' for nil:NilClass\nI'm using rvm so when I try without \"bundle exec\" I also get same response.\nI haven't used it with Capistrano yet so it's likely nothing to do with Capistrano.\nReally hoping this bug gets squashed soon I'm turning into a real puma fan.\n. ",
    "zires": "I got same error message!\n. ",
    "tomasmuller": "Same error message and machine setup as @oolyme .\n. Oh, forgot to mention that. I'm also using Capistrano.\n. ",
    "thetron": "I too am getting this issue, also with the same setup as @tomasmuller and @oolyme. However, I am using capistrano. \n. ",
    "iamroody": "ruby 2.0 rails Rails 3.2.13 , same messages\n. When I change back to puma 2.0.0, this won't happen, everything works well.\n. ",
    "doabit": "ruby 2.0.0-p195, padrino and Ubuntu 12.04 , same error,\n. ",
    "bobbrez": "Same issue on Ruby 2.0.0-p0, Rails 4.0.0.rc1 Ubuntu 13.04 on EC3 Small.\n. ",
    "rebelwarrior": "Trying to deploy a Sinatra app, I believe I'm getting a similar error. Ruby 1.9.3p429 Sinatra 1.4.2 Puma 2.0.1\n. ",
    "ryanbooker": "I still get this error with the latest version of puma (2.5.0), which contains both of the patches mentioned here.\npuma:restart still can't find the .sock. \nI'm using bog standard with no config file by requiring 'puma/capistrano' in my deploy.rb.\npuma 2.5.0, capistrano 2.15.5, rails 3.2.14\n. I believe I'm seeing the same thing with 2.5.1.\n. ",
    "waynehoover": "I'm also getting this error with puma 2.5.1, capistrano 2.15.5, rails 4.0.0\nI'm not using a config file, just requiring 'puma/capistrano' in deploy.rb\nSometimes it works sometimes it doesn't.\n. Oh, this looks like its an issue with 2.3.0. https://github.com/puma/puma/issues/307\n. I'm also getting this error. \nRuby 2.1.1 \nRails 4.1.0rc1\nPuma 2.8.1\nEDIT:\nIn case anyone else comes across this problem, it was a Rails 4.1.0 issue for me:\n2014-03-14 08:09:45 +0000: Rack app error: #<RuntimeError: Missing 'secret_key_base' for 'production' environment, set this value in 'config/secrets.yml'> \nI just corrected the secret_key_base and it was fine. Would have been better if puma displayed a better error though.\n. ",
    "sydneyitguy": "+1 still getting the same issue with puma 2.5.1, capistrano 2.15.5, rails 4.0.0, ruby 2.0\nIt works about 20% on linode 1GB plan, 80% 500 error. I can't even stop the server after freezing, so I should use \"kill -9\" to force kill everything and start puma again.\n. ",
    "sandelius": "I'm having the same problem as well!\n. Same problem with 2.1.0p0 here.\n. @evanphx 2.7.1\n. What's the status on this one?\n. ",
    "softwaregravy": "Okay, I can do that\nFor reference, so far, I had been following http://stackoverflow.com/questions/13365940/how-do-i-get-puma-to-start-automatically-when-i-run-rails-server-like-thi \nsee the answer by simon woker\n. ",
    "tnarik": "And as well for Ruby 1.9.3-p392.\n. ",
    "RobertAudi": "SSL still hangs for me, at least in development (on a local machine) with a self-certified SSL certificate...\n. ",
    "schinery": "SSL is also still hanging for me using v2.4.0 and Ruby 1.9.3-p194.\nThe server starts fine...\npuma -b 'ssl://127.0.0.1:9292?key=config/certs/server.key&cert=config/certs/server.crt'\nPuma starting in single mode...\n* Version 2.4.0, codename: Crunchy Munchy Lunchy\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on ssl://127.0.0.1:9292?key=config/certs/server.key&cert=config/certs/server.crt\nUse Ctrl-C to stop\nbut any request I make to the server just hangs.\n. ",
    "wijet": "I have similar problem when using puma with haproxy (1.4.23, on 1.4.15 everything works fine). Haproxy heartbeats make puma leaks sockets. \nNetstat shows that connections are in CLOSE_WAIT state and with time puma uses all available sockets and stops responding to request.\nI've tested haproxy 1.4.23 with thin server and it works fine, so this seems to be puma problem.\n@evanphx I was not able to reproduce this issue with sample ruby client code, but I can provide haproxy configuration (basic one), which reproduces this problem in puma.\n. Here is test configuration https://gist.github.com/wijet/5643212\nWhat I noticed is that puma stops responding after threads pool runs out. If you set it to 200 for example, you will wait longer till you encounter the problem. With standard 16 threads it takes less than a minute, but even after that point, numbers of file descriptors in /proc/PUMA_PID/fd keeps rising.  \nIf you use homebrew, here is formula for haproxy 1.4.23 https://gist.github.com/wijet/d3e691f76da6c8783b3b/raw/770ed3cef9ab93151da757408fb28a7d8044018b/haproxy.rb\n. @evanphx Thank you!\n. ",
    "pfleidi": "@joneslee85 I'm not using rails but grape, but rackup seems to work flawlessly:\nbash\nbundle exec rackup\nPuma 2.0.1 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://0.0.0.0:9292\nHere's my config.ru\n``` ruby\nrequire 'rubygems'\nrequire 'bundler'\nrequire 'bundler/setup'\nroot = ::File.dirname(FILE)\nrequire ::File.join( root, 'app' )\nrun RootAPI.new\n```\n. ",
    "sabcio": "I have a similar issue when using --daemon option and there is a bug in application (so puma won't actually start) will generate the output (without printing out the backtrace with error):\nPuma 2.0.1 starting...\n* Min threads: 4, max threads: 16\n* Environment: production\n* Listening on tcp://0.0.0.0:2501\nExit status is 0 but puma is not running.\nBacktrace with error won't be even logged to a file when stdout_redirect is set in a config file.\nI would not say this is an expected behaviour. \n. @joneslee85 could you give some feedback on this matter? will this be fixed in the future release? \nThis prevents me from writing a init.d script that fails if puma won't start. \n. ",
    "yecine06": "Hi i'm having the same problem, puma does work in demon mode and i don't have any clue why\nEverything work correctly i don't have errors but at the end puma server is not running.\nHave anyone found out why ?\nThanks\n. same problem for me\n. ",
    "yourabi": "Additionally if workers is not set to 0 pumactl -S puma.state stop will close out the control_app socket but leave master and worker processes alive and well.\nEdit: Actually it seems when workers > 0 the control_app socket isn't created - is this expected?\n. ",
    "marcelow": "Just ran in a similar issue. We had a load spike yesterday and the app servers we had became overloaded. Adding more servers solved the load issue, but today I noticed that 'hot restarts' weren't working - even after issuing the USR1 command Puma wasn't reloading. After a hard stop / restart the issue went away - i.e. we can perform hot restarts again. I'm using Rails 3.2.13 and Puma 2.0.1 . \nPS: I noticed it because even after uploading code with Capistrano and issuing a hot restart command and waiting a couple of minutes some new assets weren't showing up.\n. @evanphx sorry for taking so long to respond, thinks have been quite hectic at work! The new release does seem to have solved the issue, although we have not experienced any overloads like last week. I'll keep you posted in case this happens again.\nAnswering your question, I don't recall seeing anything out of the ordinary when the command was issued. I only noticed the servers were not reloading hours later and the Rails logs showed nothing out of the ordinary.\n. ",
    "sugitak": "Here's a small application using puma which leaks its socket on restart.  Please make a try with ruby 1.9.3 or younger.\nhttps://github.com/sugitak/puma_leak\nIt will show you the status of the open sockets.  GET /reload will make puma reloaded.  Try some times and you'll see the sockets remaining and increasing.\n. This fix should remedy #193 too; setting :close_others on Kernel#exec will close file descriptors as well as sockets.\nBy the way, I found that the description I wrote was wrong:\n\nThis is because ruby 2.0 Kernel#exec has its option :close_others implicitly set to true.\n\nRuby 2.0 closes the file descriptors on exec, that's true, but the reason was not precise.\nIt's because ruby 2.0 creates file descriptors with close-on-exec defaulted to true, and so they close.\n. ",
    "thoughtafter": "I've been bitten by this as well. This is a small patch so hopefully it gets merged soon.\n. ",
    "reiz": "Yeah. I moved away from puma, because it was hard to make it run on the server. Using now again unicorn. \n. Thx. \n. ",
    "alexreynier": "Sorry - but... Any news on getting the init.d script working for redhat type OS?\n. ",
    "vsername": "Please do.\n. @joneslee85 what command is that? Puma starts fine, just not the control server.\nI ended up using vanilla Unix commands for now:\nkill \"cat /home/myapp/tmp/puma.pid\" && rm /home/myapp/tmp/puma.sock (for stopping)\nkill \"cat /home/myapp/tmp/puma.pid\" && rm /home/myapp/tmp/puma.sock && /home/myapp/.rvm/bin/bootup_puma --config /home/myapp/config/puma.rb (for restarting)\nIf anybody knows how to get pumactl working so I can have a cleaner syntax in my startup script, that'd be great.\n. Great article @joneslee85, it's what got me into Puma in the first place. Keep it up.\n. ",
    "shingara": "I understand. \nThanks for the explain and the idea to do it on a rack-middleware.\n. ",
    "oggy": "Seeing this also. On my Mac OS 10.8 system, for instance, with JRuby 1.7.3 (installed via \"rvm install jruby\"), and 2.0.1.\nUsing the simplest possible app:\n$ cat app.ru \nrun ->(env) { [200, {}, ['']] }\nRunning puma the simplest possible way:\n$ puma app.ru\nI slam the app with curl:\n$ while true; do curl -I localhost:9292; done\nAnd restart:\n$ kill -s SIGUSR2 (pid)\nAnd curl fails during the restart.\nThis does not happen with MRI 1.9.3-p429.\n. ",
    "pmorton": "Maybe @headius would have some input. Quite honestly I am not sure why this would be closed with such little engagement on the issue. \n. ",
    "likethesky": "Any update on the new docs (a wiki page link perhaps) or what the issue is?  Do you recommend using a [EDIT: TCP control server] --path-- rather than a socket when doing hot restarts on JRuby-Puma?  I'm using Puma 2.4.0 and JRuby 1.7.4  [UPDATE: Thanks @headius (below), I may wait for UNIXServer.for_fd to get fixed then.\nStill, if you have any docs explaining this issue @evanphx that'd be nice to reference, for additional clarity.  I'm happy to edit and submit a pull request on the main Puma README once I understand the issues, if that'd help.]\n. ",
    "PetrKaleta": "This is not issue of Puma, Heroku free instances are automatically sleeping after some time of inactivity...\n. Are you experiencing same issue with 8:8 configuration?\nBtw: sorry for my previous response, I misunderstood your previous question.\n. ",
    "ukolovda": "I have same problem too. It occured under high traffic.\n. Hello, I do not try it more, but server has enough RAM (dedicated server).\n. ",
    "tkoenig": "I had the same problem on heroku.\nReverting to puma 2.0.1 fixed this currently for me.\n. It happened to me with puma 2.1.1. I'm gonna try the latest version soon and report back if I still face this issue.\n. ",
    "phlipper": "I am seeing the same issue as @kenips on MRI 2.0.0-p247 and puma 2.5.1 whether I restart via pumactl or manually sending a kill -s USR1 to the pid using either unix or tcp sockets.\ntotal       used       free     shared    buffers     cached\nMem:          3750       1695       2054          0        245       1025\n-/+ buffers/cache:        424       3326\nSwap:            0          0          0\nEdit: The same issue occurs with either 1 or 2 workers, and any thread count.\n. ",
    "tevanoff": "i was having the exact same issue after upgrading to puma 2.0. i had a few free minutes so i just did a little digging.\nthe 1.6 implementation looks to be calling the code that loads up the app before the binding to the port, while the 2.0 version seems to bind first. \nit looks like it could be an easy fix so i'll try it out and issue a PR if it works. \n. Ok, thanks! Being unfamiliar with the codebase, I thought it could be a naive implementation.\nBinding to the port early is a deal breaker for running on heroku since it'll route requests to the dyno before the app is loaded. \nDo you think that the configuration variable is the way to go to handle that, or is there a better way to go about fixing this?\nThanks!\n. ",
    "codepunkt": "install webrick\nbash\ngem install webrick\nstart webrick in the public folder of my project\nbash\ncd public\nruby -rwebrick -e 'WEBrick::HTTPServer.new(:Port=>8000,:DocumentRoot=>\".\").start'\nrequest file from directory with a dot\nbash\nwget localhost:8000/components/handlebars.js/dist/handlebars.js\nHTTP request sent, awaiting response... 200 OK\nrequest same file from puma serving the same directory on port 9292\nbash\nwget localhost:9292/components/handlebars.js/dist/handlebars.js\nHTTP request sent, awaiting response... 404 Not Found\nrequest file from directory without a dot\nbash\nwget localhost:8000/components/handlebarsjs/dist/handlebars.js\nHTTP request sent, awaiting response... 200 OK\nrequest same file from puma serving the same directory on port 9292\nbash\nwget localhost:9292/components/handlebarsjs/dist/handlebars.js\nHTTP request sent, awaiting response... 200 OK\n. Copied my project into the public folder of a new blank rails app - started webrick with rails server - works!\nChanged the server to puma by adding gem \"puma\" to the Gemfile and adding\nruby\nrequire 'rack/handler'\nRack::Handler::WEBrick = Rack::Handler.get(:puma)\nto script/rails - works aswell!\nSeems puma is doing what it is supposed to do and i had problems with my config.ru setup.\nThanks for your time! :)\n. ",
    "titanous": "Related: rack/rack#574.\n. ",
    "joeellis": "@egru Did you end up reaching a solution to this?\n. Yeah, I just saw your issue on rails/rails (https://github.com/rails/rails/issues/10989) and am responding there\nOn Wednesday, June 26, 2013 at 10:49 PM, Eric Gruber wrote:\n\n@joeellis (https://github.com/joeellis) No, not yet. It's pretty annoying.\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/puma/puma/issues/281#issuecomment-20095782).\n. \n",
    "egru": "@joeellis No, not yet. It's pretty annoying.\n. ",
    "j16r": "Thanks, if you've happened across this issue in search of a solution for the above you may find this StackOverflow thread useful \n. ",
    "pelcasandra": "@joneslee85 So, how to load pumactl start with the already created /var/run/my_app.state?\nEverytime I run bundle exec pumactl -S /var/run/site.state start Is ignoring the site.state and therefore not loading the appropiate config. \n. @joneslee85 Okay, so what is the best way to launch it on boot?\n. Thanks, it is working now but If I try to launch the server from anwhere outside the rails with sh /var/www/site/current/script/puma.sh start I get Could not locate Gemfile. I'm trying to add that line to /etc/rc.local. Any idea how to configure it to launch at boot?\n. @joneslee85 Or should I move puma.sh to /etc/init.d ? \n. ```\nif defined?(Puma)\n  workers Integer(ENV[\"PUMA_WORKERS\"] || 6)\n  threads Integer(ENV[\"PUMA_THREADS_MIN\"] || 8), Integer(ENV[\"PUMA_THREADS\"] || 32)\n  environment (ENV[\"RACK_ENV\"] || \"production\")\n  daemonize true\n  pidfile \"/var/run/site_a.pid\"\n  state_path \"/var/run/site_a.state\"\n  bind 'unix:///var/run/site_a.sock'\n  stdout_redirect \"/var/log/puma.site_a.access.log\", \"/var/log/puma.site_a.error.log\", true\n  preload_app!\non_worker_boot do\n    ActiveRecord::Base.connection_pool.disconnect!\nActiveSupport.on_load(:active_record) do\n  config = Rails.application.config.database_configuration[Rails.env]\n  config['reaping_frequency'] = ENV['DB_REAP_FREQ'] || 10 # seconds\n  config['pool']              = ENV['DB_POOL'] || 35\n  ActiveRecord::Base.establish_connection(config)\nend\n\nend\nend\n```\n. Yes, with this:\nrequire ::File.expand_path('../config/environment',  __FILE__)\nuse Rack::Deflater\nrun Site_a::Application\n. I get a bunch of errors and then it loads.\nSee here\nhttps://gist.github.com/pelcasandra/6640309\n. Exactly.\n. ",
    "timuckun": "Is there any documentation anywhere describing how to set this up?\nIdeally I would like to set up multiple apps, each app is running in a fork and is threaded inside of that fork.\nAlso is there a mailing list where one can ask general questions?\n. @evanphx Yes I think it would be awesome if one puma could manage multiple apps. Maybe I am getting the cluster and the forest mixed up?\n. Actually the writeup contradicts itself. First it says puma is threaded only then it admits it's both multi process and threaded. But they do have some other points about the management of multiple apps.\nI do think management of multiple apps is a massive disadvantage of ruby frameworks. Each app is completely separate and has it's own bundle and it's own gems. Each app has to be monitored separately. Each app takes up a ton of RAM.\nProjects like Passenger and Torquebox are taking a more wholistic approach for those of us that are tasked with deploying and maintaining dozens of apps. Although they are big improvements I for one don't think they go far enough.  Ideally I would like to have just one gemfile, just one container that all my apps run in where they can share code, models, libraries, cache, database pool, etc.   If there is an upgrade required for a gem I would like to be able to update that one gemfile and deploy all my apps again in a safe and sane matter instead of checking out a dozen apps and deploying each one separately.    I know something like this may be difficult but I also know it's not impossible. Zope is a container like that and so are the many of the multi tenant CMS systems out there. \nJust my two cents.\n. ",
    "cpuguy83": "I get no details from the process. In cluster mode it just outputs that a new worker has booted up. So the master process doesn't die, just the worker.\n\u200bIn the Rails logs it just has the normal request logging but ends before completing the action.\nPerhaps hooking gdb up to one of the worker processed may help get more info?\u00a0\nBrian Goff\nNetwork Administrator\nEnVu, LLC\n813-863-1362\nOn Thu, Jun 27, 2013 at 11:50 PM, Evan Phoenix notifications@github.com\nwrote:\n\nI need details about the crash. Is there a backtrace? Does the process segfault?\nReply to this email directly or view it on GitHub:\nhttps://github.com/puma/puma/issues/294#issuecomment-20169094\n. [25953] - Worker 26339 booted, phase: 0\n[25953] - Worker 26344 booted, phase: 0\n2013-07-07 14:57:18 -0400: Read error\n    Broken pipe (Errno::EPIPE)\n\nBacktrace:\n               IO(UNIXSocket)#syswrite at kernel/common/io.rb:1192\n               Puma::Server#fast_write at /home/deployer/apps/emp/shared/bundle/rbx/1.9/gems/puma-2.3.1/lib/puma/server.rb:635\n           Puma::Server#handle_request at /home/deployer/apps/emp/shared/bundle/rbx/1.9/gems/puma-2.3.1/lib/puma/server.rb:484\n           Puma::Server#process_client at /home/deployer/apps/emp/shared/bundle/rbx/1.9/gems/puma-2.3.1/lib/puma/server.rb:248\n               { } in Puma::Server#run at /home/deployer/apps/emp/shared/bundle/rbx/1.9/gems/puma-2.3.1/lib/puma/server.rb:147\n                             Proc#call at kernel/bootstrap/proc.rb:22\n  { } in Puma::ThreadPool#spawn_thread at /home/deployer/apps/emp/shared/bundle/rbx/1.9/gems/puma-2.3.1/lib/puma/thread_pool.rb:92\n                             Proc#call at kernel/bootstrap/proc.rb:22\n                        Thread#run at kernel/bootstrap/thread19.rb:39\n. Some debug info\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\nprocess 17704 is executing new program: /usr/bin/env\nUnable to read JIT descriptor from remote memory!\n. ",
    "adammeghji": "I'm seeing some bizarre problems with wkhtmltopdf + puma after I issue a phased-restart:\nPDFKit::NoExecutableError: No wkhtmltopdf executable found at >> Please install wkhtmltopdf - https://github.com/jdpace/PDFKit/wiki/Installing-WKHTMLTOPDF\nFWIW, we're using pdfkit, which calls out to wkhtmltopdf.\nIf I do a hard shutdown + startup, it clears itself up, but the next phased restart seems to reintroduce this problem.  I'm totally perplexed, but it's causing us some grief here.  Any ideas?\nUPDATE: in general, pdfkit with wkhtmltopdf-binary was causing me various problems after puma phased restarts .. I ended up ditching it entirely, and am using wkhtmltopdf-0.9.9 via https://code.google.com/p/wkhtmltopdf/downloads/list.  This seems much more reliable than the gem, so if anyone else is experiencing grief here, I'd suggest using a static binary instead of the binary gem, and moving on.\n. ",
    "washu": "This happens with or without preload_app. a very easy example is do a simple ls when in clustered mode and it will kill the worker  and under rubinius it loops restart forever\n. it only happens under rbx, i ran it under mri and it doesnt do the reboot cycle thing\n. I just created a new gem call daemon_signals, require it after puma in your gemfile and you will get a nice SIGCONT after daemonization is called.\n. on_worker_boot is not fired for daemonization. for clustering where you have multi-process you above statement is very true, but for single mode where you have real threads re: rubinius or jruby preload doesn't do anything ill adjust the title appropriately \n. ",
    "Flink": "Ok too bad :( Thanks anyway!\n. ",
    "perplexes": "Ran into this and thought I would add more to what Evan said after some research.\nUnicorn also has a \"phased restart\" called \"upgrade\", which actually spawns a new master process, then hands over all FDs. See https://github.com/defunkt/unicorn/blob/master/lib/unicorn/http_server.rb#L425-454, this is the common \"fork-exec\" pattern.\nBundler cannot reload, it can only be loaded once per process.\nThe order of operations is something like (assuming no preloading and cluster mode):\n- puma is executed, probably as a Bundler binstub or through bundle exec\n- Bundler loads and runs Puma\n- Puma master process runs\n- Puma forks workers\n  - Your app loads in the worker process\nSo unless there is a somewhat major rearchitecting of Puma to follow a similar fork-exec pattern as Unicorn with master process replacement, Puma can't pick up Gemfile changes with phased restart; you need to issue a full restart. This can be less painful with a load balancer so you can take pumas out of rotation for restart.\n. ",
    "such": "I tried adding:\nENV[\"BUNDLE_GEMFILE\"] = File.join('myapp', 'current', \"Gemfile\")\nto my puma configuration file and it works for me.\n. ",
    "maxigs": "Had the same issue.\nDefining the ENV[\"BUNDLE_GEMFILE\"], much like @such in the puma-config seems to have fixed it (no crashes since).\n. ",
    "alxgsv": "Added to my config/puma/production.rb:\nENV[\"BUNDLE_GEMFILE\"] = \"/path/to/rails/root/Gemfile\"\nDoes not work for me: Gemfile reloads on hot restart, but not on phased restart. \nNow it's really easy to turn my production off by picking wrong restart strategy :-(\n. @evanphx oh thanks! This is what I need. But it's not working for me. Somehow when I use prune_bundler workers start to use system gems that conflict with Gemfile gem versions and puma crashes even after start.\nHere are logs and config: https://gist.github.com/LongMan/10706837\nThis command sent to Kernel.exec in Puma::Cli#run on start:\n/home/myproject/.rvm/rubies/ruby-2.1.1/bin/ruby -I /srv/myproject/www/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/../../extensions/x86_64-linux/2.1.0/puma-2.8.2 -I /srv/myproject/www/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/lib /srv/myproject/www/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/bin/puma-wild rack:1.4.5 -e staging -C ./config/puma/staging.rb\nOfftop:\nAlso, I think I found one not-so-important typo: https://github.com/puma/puma/commit/84b7b6a6743832285266d6f4dad93aee2d291153#commitcomment-6012210\n. I believe it's because of RVM. But unfortunately I can't fix it by myself. I can provide any information about this problem.\n. I am keeping all the releases, but problem persists\n. ",
    "boostbob": "sometimes, it works. because your  :keep_releases settings, if release has been removed, Gemfile not found.\nSINATRA_ROOT = \"/var/www/updating_server\"\nENV[\"BUNDLE_GEMFILE\"] = File.join(SINATRA_ROOT, 'current', \"Gemfile\")\nthanks.\n. @LongMan  did you try the way which posted by @such ?\n. ",
    "xiaoronglv": "I have the same problem...\n. @itsNikolay \nwhich restart method are you using\uff0c normal restart or phased restart\uff1f\nprune bundler only works in phased restart and do not preload app\n. We're seeing the same exact thing on 2.9.2 still. If we remove prune_bundler, things work fine. \nenvironment\n1. ruby 2.2.0\n2. puma 2.9.2\nthat's my project\nhttps://github.com/xiaoronglv/searchapp\nbash\ncd /home/webapp/www/searchapp/current && ( RACK_ENV=production ~/.rvm/bin/rvm 2.2.0 do bundle exec puma -C /home/webapp/www/searchapp/shared/puma.rb --daemon )\n* Pruning Bundler environment\n/home/webapp/.rvm/rubies/ruby-2.2.0/lib/ruby/site_ruby/2.2.0/rubygems/dependency.rb:315:in `to_specs': Could not find 'rack' (= 1.6.4) among 15 total gem(s) (Gem::LoadError)\nChecked in 'GEM_PATH=/home/webapp/.rvm/gems/ruby-2.2.0:/home/webapp/.rvm/gems/ruby-2.2.0@global', execute `gem env` for more information\n    from /home/webapp/.rvm/rubies/ruby-2.2.0/lib/ruby/site_ruby/2.2.0/rubygems/dependency.rb:324:in `to_spec'\n    from /home/webapp/.rvm/rubies/ruby-2.2.0/lib/ruby/site_ruby/2.2.0/rubygems/core_ext/kernel_gem.rb:64:in `gem'\n    from /home/webapp/www/searchapp/shared/bundle/ruby/2.2.0/gems/puma-2.9.2/bin/puma-wild:20:in `block in <main>'\n    from /home/webapp/www/searchapp/shared/bundle/ruby/2.2.0/gems/puma-2.9.2/bin/puma-wild:18:in `each'\n    from /home/webapp/www/searchapp/shared/bundle/ruby/2.2.0/gems/puma-2.9.2/bin/puma-wild:18:in `<main>'\n``` ruby\n!/usr/bin/env puma\ndirectory '/home/webapp/www/searchapp/current'\nrackup \"/home/webapp/www/searchapp/current/config.ru\"\nenvironment 'production'\npidfile \"/home/webapp/www/searchapp/shared/tmp/pids/puma.pid\"\nstate_path \"/home/webapp/www/searchapp/shared/tmp/pids/puma.state\"\nstdout_redirect '/home/webapp/www/searchapp/shared/log/puma_access.log', '/home/webapp/www/searchapp/shared/log/puma_error.log', true\nthreads 4,8\nbind 'unix:///home/webapp/www/searchapp/shared/tmp/sockets/puma.sock'\nworkers 1\nprune_bundler\non_restart do\n  puts 'Refreshing Gemfile'\n  ENV[\"BUNDLE_GEMFILE\"] = \"/home/webapp/www/searchapp/current/Gemfile\"\nend\n```\n. ",
    "AleksandrLeontev": "I have the same problem...\n. ",
    "itsNikolay": "I'd been using prune_bundler in many projects, but still has the same issue ^_^\npuma.rb\n``` ruby\n!/usr/bin/env puma\ndirectory '/home/deploy_user/apps/app/current'\nrackup \"/home/deploy_user/apps/app/current/config.ru\"\nenvironment 'production'\npidfile \"/home/deploy_user/apps/app/shared/tmp/pids/puma.pid\"\nstate_path \"/home/deploy_user/apps/app/shared/tmp/pids/puma.state\"\nstdout_redirect '/home/deploy_user/apps/app/shared/log/puma_access.log', '/home/deploy_user/apps/app/shared/log/puma_error.log', true\nthreads 0,16\nbind 'unix:///home/deploy_user/apps/app/shared/tmp/sockets/puma.sock'\nworkers 0\nprune_bundler\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\n```\n. @evanphx the issue is still there:\nsh\nPuma starting in cluster mode...\n* Version 3.6.0 (ruby 2.3.1-p112), codename: Sleepy Sunday Serenity\n* Min threads: 1, max threads: 6\n* Environment: production\n* Process workers: 1\n* Phased restart available\nERROR: Please specify the SSL ca via 'ca='\nexited with code 1\nwith ssl_bind:\nruby\nssl_bind '0.0.0.0', '9292', {\n  key:  \"#{app_dir}/config/certs/server.key\",\n  cert: \"#{app_dir}/config/certs/server.crt\"\n}\nBut there's NO the issue with ugly bind:\nruby\nbind \"ssl://0.0.0.0:9292\\\n?key=#{app_dir}/config/certs/server.key\\\n&cert=#{app_dir}/config/certs/server.crt\"\n. ",
    "araslanov-e": "@bnorton ENV[\"BUNDLE_GEMFILE\"] helped me\n. bundle update capistrano3-puma helped me B)\n. ",
    "todd": "We're also seeing the same issue as @mrbrdo when performing a phased restart. Running Puma 2.11.2, deploying with Capistrano. prune_bundler is present in our Puma config. Hot restart works as advertised.\n. Right - that's the issue. After we upgraded to 2.16.0, Puma stopped picking up the new release directory Capistrano symlinks to /path/to/app/current. This was all working as expected previously.\n. It's worth pointing out that we were using Puma 2.12 previously. Rolling back to 2.15 exhibits the same unexpected behavior. We've rolled back to 2.14 for now as that version doesn't seem to be affected.\n. I managed to track down the relevant commit, PR, and #770 where this behavior was discussed and changed. I can verify that using the directory DSL method fixed this problem for us. Moving forward, is the plan only to support setting the release directory with this method or should the CLI option be preserved?\n. @evanphx Oops, misremembered about setting it via the CLI. Ignore me on that front. I can take a stab at updating the docs if you'd like.\n. Will do. I'm going to close this issue.\n. :zap: :heart: \n. ",
    "marktran": "If you guys are using Capistrano, make sure the Ruby version defined in your config/deploy.rb (e.g. set :rbenv_ruby, '2.2.2') is the same as the version in your .ruby-version file.\nI ran into an issue the other week where Capistrano was running bundle install on a different version of Ruby and my app was booting up in another version.\n. ",
    "davidbalbert": "There still doesn't seem to be a way to do what's described here. Are there any plans to support a before_fork hook? /cc @catsby \n. ",
    "nandosola": "Cheers! Have a good weekend!\n. ",
    "nengxu": "Nginx talks to Puma via Linux socket. which is set up in the command:\n-b unix://#{@sockets}/#{@application}.sock\nI am testing in firewalled environment. All requests are under control, and no request is coming. Still, the puma process just hangs there while its socket has been killed by pumactl stop.\n. I've tried Puma 2.3.2. The problem I reported still exist. For your info: ruby-2.0.0-p247, Centos 6.2, Linux 2.6.32-358.11.1.el6.x86_64 #1 SMP\n. Actually, it is getting worse. Both the puma process and the puma socket still hang there, but not respond to request. Only the pumactl socket is removed, when running \"pumactl stop\".\nSo we cannot even start puma anymore, because:\nThere is already a server bound to: x.x.sock\nWe have to manually kill the process and remove the remaining puma socket.\nCould you please don't close this issue so quickly? Thanks.\n. Result of pumactl -S /.../xxx.state stop\nCommand stop sent success\nPuma config:\npuma -t 8:32 -e alpha -d -b unix:///.../xxx.sock -S /.../xxx.state --control unix:///.../pumactl.sock\nSo the output of starting puma:\nPuma starting in single mode...\nMRI version: ruby-2.0.0-p247\nBTW, I've run some benchmark with our application. Nginx + puma is about 10 times faster than Apache + passenger 3. Thank you for this good gem!\n. I tried to start puma without -d.\n\"pumactl restart\" seems working, though the puma process remains the same pid.\n\"pumactl stop\" . The puma process shutdown. The pumactl socket closed. But the puma socket remains there.\nSo it is the opposite to what I reported at beginning of this issue.\n. Update, when running with -d, after \"pumactl stop\", both the puma process and the puma socket remain. But puma is dead. Only the pumactl socket was removed.\n. I tried also the cluster mode with 3 workers.\n\"pumactl restart\" seems working, the puma process and the 3 workers all changed pid.\n\"pumactl stop\", the puma process and 3 workers are killed, the pumactl.sock is closed. But the puma socket still remains.\n. Tested against Puma 2.4.0\nFor single mode, \"pumactl restart\" worked. \"pumactl stop\" just closed pumactl.sock, left both the puma process and its socket running.\n. It is worse in cluster mode. I set up 3 workers:\n4070     1  0 14:12 ?        Sl     0:00 ruby /opt/rubies/ruby-2.0.0-p247/bin/puma -C config/puma.rb\n4078  4070  0 14:12 ?        Sl     0:00  \\_ puma: cluster worker: 4070\n4084  4070  0 14:12 ?        Sl     0:00  \\_ puma: cluster worker: 4070\n4090  4070  0 14:12 ?        Sl     0:00  \\_ puma: cluster worker: 4070\nAfter run \"pumactl restart\", failed the same:  just closed pumactl.sock, left both the puma process and its socket running. And this puma process is dead, cannot process any request.\n4220     1  0 14:13 ?        Sl     0:00 /opt/rubies/ruby-2.0.0-p247/bin/ruby /opt/rubies/ruby-2.0.0-p247/bin/puma -C config/puma.rb\nAnd the 3 workers are gone.\n.  In cluster mode, \"pumactl stop\" failed the same as \"pumactl restart\". So 2.4.0 seems getting worse as whole.\n. I will report late the result of Passenger 3.0.19 vs. Puma 2.3.2, in terms of speed and stability. Just warn you first, Puma seems to have problem of memory leak, and not as stable as Passenger.\nWe will be in heaven if Passenger guys can work together with Puma guys ;-)\n. Just tried with Puma 2.5.1 (Ubuntu 13.04 / Ruby 2.0.0p247).\nThe stop still left the application socket, thought the puma process was killed.\n. Retried. For Puma 2.5.1, 'pumactl stop' still leave the #{application}.sock. However, I can start puma again with the remaining  #{application}.sock, as well as run 'pumactl restart'. So the remaining sock issue is not blocking any more.\nThe same for Puma 2.6.0. \nGreat! So this is no longer a bug for me. Thanks.\nDon't know other people's mileage though. So still leave this issue open.\n. Yes, I already put \"preload_app!\" in puma config. However, even in not-preloading situation, RAM should not go up over time, because the load is almost constant. RAM should remain constant when maximum workers are all loaded. So there may be memory leak with Puma.\n. Any progress on this issue? Thanks.\n. Due to changes in projects, I have not used Puma for quite a while. Now I have the chance to pick up a Rails server for my current project. Considering the recent progress in Passenger 5, this memory leak issue, which I happened to open, may lead me towards Passenger 5 more than Puma.\nSo I would like to see some progress in this issue. Thanks for all involved.\n. @schneems \n1) Not all people in this thread are using PWK. Please think out of the box of PWK.\n2) How to explain when switch Puma to another application server, say, Passenger, then the memory usage pattern is so different, with the exactly same Rails application?\n. @schneems Please note that performance problems like memory leak often cannot be reproduced in trivial demo applications. The problem often occurs with real data, real application, real deployment, real user interaction. Unfortunately, it is often not legal / practical to share the real scenarios with outside.\nThough in my case, the Rails application was a simple restful API one. So it is highly probable that Puma did intervene.\nI agree with you that it makes the devs hard to investigate without repro. Thanks for your efforts.\n. @evanphx Your reason for closing this issue sounds not reasonable. Please reconsider the comment of @nacengineer, quoted below:\nAs I see it, there is definitely an issue with puma under ruby 2.1+ as far as I can tell it involves some sort of conflict between ruby 2.1's new GC settings and pumas worker threads. But that's just one developers educated guess not backed by any science.\nNot sure how to fix it going forward, which admittedly sucks and doesn't help the discussion but I am willing to run whatever tests might help. I still have two VMs running on puma that exhibit this slowdown, one of which I can change the ruby version back to 2.1+ and trigger the error. However I will eventually switch them to passenger, for this and a few other reasons, and become an indifferent observer of puma. \n. @evanphx Thanks for you guys' efforts!\n. ",
    "mapopa": "confirmed the socket is still there and one puma is left after each stop and the start cycle \n. puma stop is successfull , should i send a strace ? \n. ",
    "willcosgrove": "I'm having the same issue, with the same setup as @nengxu.  Running MRI: ruby-2.0.0-p247.  Puma is not running in clustered mode. 0:16 threads.  I can help answer any questions you have about my setup.\n. ",
    "synth": "+1; pumactl restart works for me, but stopping does not.  I'm running MRI ruby 2.0.0-p247; puma 2.3.2.  I end up having to kill process and clean up socket and state files manually.\n. ",
    "mattsoutherden": "I'm also seeing this behaviour on Puma 2.4.0 (Ruby 2.0.0p195).\nThe stuck bindings only seem to happen when using unix sockets. TCP sockets can start/stop/restart without issue.\n. ",
    "chetan": "I'm seeing this with Puma 2.5.1 also. The stop command is \"successful\" and the control socket (unix) is deleted but the server doesn't quit. \n. In my case I think the problem was running eventmachine alongside puma which was causing it to hang while stopping. I ended up writing my own start/stop script which gave me much better control of the process. \n. ",
    "lonre": "The same here.\nBundler 1.3.5\nRuby 2.0.0 (2013-06-27 patchlevel 247) [x86_64-linux]\nRubygems 2.1.3\npuma (2.6.0)\n. No, just normal website.\n. same problem here, runs ok with daemonize set to false\ncheckout config and output here: https://gist.github.com/lonre/7798987\n. @evanphx \njust a clean rails(Rails 4.0.2) app can reproduce this.\n1. rails new mysite\n2. add puma to Gemfile\n3. add puma.rb (gist above) under config dir\n4. bundle exec puma -C config/puma.rb\n. ",
    "kinaan-khan-confiz": "I am facing same problem. \"pumactl stop\" says \"Command stop sent success\" but puma stops responding to the requests . I have to manually kill the process.\nI am using apache on front-end and has configured reverse proxy using tcp socket. I am not giving any control url in configuration file too. I have tried using \n\"pumactl -p PID stop\" \n\"pumactl -P PID-FILE stop\"\n\"pumactl -S STATE-FILE stop\" \nBut all in vain. I have tried it on puma 2.6 and puma 2.4 both have same issue . \nI am using ruby 1.9.3p429.\n. Any updates on this issue ? As I would like to look into it. So need a green signal from the community specially the author of the gem.\n. ",
    "Zhomart": "cap puma:stop stops all workers, but /tmp/myapp.sock still remains.\npuma (2.6.0)\nruby 2.0.0p247 (2013-06-27 revision 41674) [x86_64-linux]\n. ",
    "alexandru-calinoiu": "same issue as @Zhomart \npuma (2.6.0)\nruby 2.0.0p247 (2013-06-27 revision 41674) [Ubuntu 12.04 x64]\n. After some more experimenting it turns out that if I have at least 1 worker it pumactl will stop the puma process\n. +1\n. I've managed to limit this behavior (1 in 15 - 20 deploys) by binding to tcp. Is not yet gone but at least is a little more  manageable.\n. Yap, they do seem very similar, but I am running MRI 2.0.0-p247.\n. Yes it does (forgot to mention this in the initial post)\nOn Wed, Dec 4, 2013 at 5:57 PM, Evan Phoenix notifications@github.comwrote:\n\nDoes it run ok if you remove the daemon flag?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/422#issuecomment-29822997\n.\n\n\nCalinoiu Alexandru Nicolae\nco-founder agilefreaks\nskype: calinoiu.alexandru\nlinkedin: http://www.linkedin.com/in/alexandrucalinoiu\n. I am using ruby 2.0 with rails 4.0\nI have a config file with the following:\nstdout_redirect \"/var/www/myapp_#{rails_env}/shared/log/puma_stdout.log\", \"/var/www/myapp_#{rails_env}/shared/log/puma_stderr.log\"\nBut nothing is added there when starting with -d\n. Thank you for your quick responses\nI will push a demo app and post a link yo the repo here\nOn Dec 4, 2013 6:36 PM, \"Evan Phoenix\" notifications@github.com wrote:\n\nCan you please gist the config file and the exact way you're invoking puma\n(since I don't see the config file specified, I assume you simplified it\nbut I need to see the exact thing).\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/422#issuecomment-29826693\n.\n. Thank you,\n. Well you can bundle all the mini apps in single site with diffrent end points. Something like:\n\nmini1.someapp.com\nmini2.somepp.com\nmikey.someapp.com\nThis is an example on how to do that in a rails app http://railscasts.com/episodes/221-subdomains-in-rails-3\n. Thanks @sorentwo, really usefull stuff\n. I get the following warning in a rack app only app (not using rails) \nDEBUG[c8381554]     [23243] ! # - /usr/local/rvm/rubies/ruby-2.1.5/lib/ruby/2.1.0/net/http.rb:920:in connect'\nDEBUG[c8381554]     \nDEBUG[c8381554]     [23243] ! #<Thread:0x0000000504b488 sleep> - /usr/local/rvm/rubies/ruby-2.1.5/lib/ruby/2.1.0/timeout.rb:84:insleep'\n. ",
    "nigelramsay": "as @balauru suggested, I also found adding this line made the cap puma:stop command work:\nworkers 1\nwith puma (2.6.0) and MRI (2.0.0p247) on Ubuntu 12.04 x64\n. Yes, we have a server_sent_events_controller that subscribes to a Redis queue and sends messages down the pipe to the browser. \n. ",
    "quentindemetz": "My puma is behind a firewall and not receiving any requests. But pumactl won't stop it unless I use the workers 1 workaround.\n. I'm encountering this issue as well. Has anybody managed to mitigate it?\n. ",
    "railsme": "yep, my production is down =) the same thing\n. need wait for updating in rubygems =) github repo is OK\n. ",
    "DavidRagone": "I'm experiencing this issue as well. \n. ",
    "toroidal-code": "I'm getting the same issue\n. ",
    "rpocklin": "You can serve them out by mapping the paths directly into Nginx / Apache or use config.serve_static_assets = true which worked for me.\n. ",
    "hansottowirtz": "In Rails 5, you can use RAILS_SERVE_STATIC_FILES=true rails s -e production.\nThe key in production.rb is now config.public_file_server.enabled.\n. ",
    "reejosamuel": "Like to point out, its better to use nginx or sorts to serve static assets rather than make rails do it. Since everything to points to say it is a overhead on rails to serve assets. Also worth mentioning nginx etc manages cache and is meant for this job.\n. ",
    "invalidusrname": "Thanks for this! +1\n. I'm seeing this as well. I've tried setting ENV['BUNDLE_GEM'], but after the release gets cleaned up by capistrano, puma exits.\nHere's my app running and exiting after 2 capistrano deploys\n$ bundle exec puma -q -e qa -b 'tcp://0.0.0.0:8026' -S /var/apps/my-app/shared/sockets/puma.state --control 'unix:///var/apps/my-app/shared/sockets/pumactl.sock'\nPuma starting in single mode...\n* Version 2.3.2, codename: Delicious Thin Mints\n* Min threads: 0, max threads: 16\n* Environment: qa\n* Listening on tcp://0.0.0.0:8026\n* Starting control server on unix:///var/apps/my-app/shared/sockets/pumactl.sock\nUse Ctrl-C to stop\n* Restarting...\nPuma starting in single mode...\n* Version 2.3.2, codename: Delicious Thin Mints\n* Min threads: 0, max threads: 16\n* Environment: qa\n* Inherited tcp://0.0.0.0:8026\n* Starting control server on unix:///var/apps/my-app/shared/sockets/pumactl.sock\nUse Ctrl-C to stop\n* Restarting...\n/var/apps/my-app/releases/20130805200422/Gemfile not found\nI've also tried adding this to my config/puma.rb file with no luck:\n```\n directory \"<%= current_path %>\"\n worker_directory \"<%= current_path %>\"\nENV[\"BUNDLE_GEMFILE\"] = \"<%= current_path %>/Gemfile\"\non_restart do\n   puts 'On restart...'\n   ENV[\"BUNDLE_GEMFILE\"] = \"<%= current_path %>/Gemfile\"\n end\n```\n. ",
    "ys": "When setting a full path like \"#{current_path}/config/puma.rb\" for the puma_config_file, it fails locally due to file not found.\n. Sure, but the problem is that current_path is the distant one not local one...\ncap puma:restart                                                                                                                                                         [ puma + ]\n  * 2013-07-19 15:38:15 executing `puma:restart'\n/Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/bundler/gems/puma-228edd90e052/lib/puma/configuration.rb:153:in `read': No such file or directory - /home/frontback/current/config/puma.rb (Errno::ENOENT)\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/bundler/gems/puma-228edd90e052/lib/puma/configuration.rb:153:in `_load_from'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/bundler/gems/puma-228edd90e052/lib/puma/configuration.rb:33:in `load'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/bundler/gems/puma-228edd90e052/lib/puma/capistrano.rb:62:in `configuration'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/bundler/gems/puma-228edd90e052/lib/puma/capistrano.rb:55:in `state_path'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/configuration/namespaces.rb:191:in `method_missing'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/bundler/gems/puma-228edd90e052/lib/puma/capistrano.rb:30:in `block (3 levels) in <top (required)>'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/configuration/execution.rb:138:in `instance_eval'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/configuration/execution.rb:138:in `invoke_task_directly'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/configuration/callbacks.rb:25:in `invoke_task_directly_with_callbacks'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/configuration/execution.rb:89:in `execute_task'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/configuration/execution.rb:101:in `find_and_execute_task'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/cli/execute.rb:46:in `block in execute_requested_actions'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/cli/execute.rb:45:in `each'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/cli/execute.rb:45:in `execute_requested_actions'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/cli/help.rb:19:in `execute_requested_actions_with_help'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/cli/execute.rb:34:in `execute!'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/lib/capistrano/cli/execute.rb:14:in `execute'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/lib/ruby/gems/2.0.0/gems/capistrano-2.15.4/bin/cap:4:in `<top (required)>'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/bin/cap:23:in `load'\n    from /Users/yannick/.rbenv/versions/2.0.0-p195/bin/cap:23:in `<main>'\n. This is what I have done finally.\nJust pointed out that if an absolute path is given, it fails.\nI think I will provide some kind of PR soon about that\n. ",
    "GoalSmashers": "@ys - why it fails locally? Could you include cap's output?\n. ",
    "jakubpawlowicz": "@ys Please correct me if I'm wrong but shouldn't you use path relative to your root directory?\nset :puma_config_file, 'config/puma.rb'\n. :+1: Raising an exception if config file cannot be found locally should be enough.\n. ",
    "lmarburger": "Great minds think alike! I just fixed the same typo in #324.\n. I'm not sure what happened. The rbx-19mode build seemed to stall and I don't have the ability to restart it.\n\n. ",
    "chrismytton": "@lmarburger Ha! What are the chances! Closing in favour of #324 \n. ",
    "bugaiov": "Sorry, did not think about that. Now it is 1.8 syntax compatible. Thanks.\n. ",
    "metasync": "I encountered the same issue. Similar issue occurred in Unicorn as well. The reason is Puma still remembers the very first release location (after restart) for the Gemfile and after Capistrano does its clean up on older releases, restart will fail for sure. I used the same solution in Unicorn to resolve this one in Puma. You can put the following line to your Puma configuration file (say, puma.rb):\nENV[\"BUNDLE_GMEFILE\"] = \"/path/to/your/Gemfile\"\nFor Rails project, most likely this line will be:\nENV[\"BUNDLE_GMEFILE\"] = \"<%= current_path %>/Gemfile\"\nBy doing so, I can restart without any such failures. \nHope this helps!\n. ",
    "chewi": "I've just switched from Unicorn to Puma and hope to look into this soon. Since I use this configuration in different environments, I didn't want to hardcode the absolute path to \"current\" so I solved it in Unicorn like this.\n``` ruby\nif config_file\n  self.config_file = File.expand_path(config_file)\n  working_directory dir = File.dirname(File.dirname(config_file))\nelse\n  dir = \".\"\nend\nbefore_exec do |server|\n  ENV[\"BUNDLE_GEMFILE\"] = File.join(dir, \"Gemfile\")\nend\n```\nAssuming you start Unicorn with -C path/to/current/config/unicorn.conf.rb, File.expand_path will turn this into an absolute path if it isn't already but will do so without resolving the symlink. You can then set the working directory to /absolute/path/to/current and also set BUNDLE_GEMFILE to /absolute/path/to/current/Gemfile in the before_exec hook. This works well for Unicorn but I need to double-check the same will work for Puma. @invalidusrname's comment suggests it won't.\n. This is something I'd like to have but I'm not desperately needing it right now. If it were to be done, I'd like to have the option for Puma not to dechunk the request in the middle because otherwise you'd lose some of the benefit. I'd be putting this in front of Grape, which might handle this better than Rails. I used to fetch large CSV files over SFTP and load them on the fly, sometimes even decompressing them and decrypting them in the process. It would be nice to be able to do the same over HTTP.\n. #712, as promised!\n. Also happens with USR2.\nI think I see how too. Puma fires up the new instance with a bunch of -I options pointing to the few dependencies it needs. One of these is rack. I need to look closer to understand why it doesn't pick up the newer version, although the newer version may not be the one your application is about to use. It's also not clear whether this is a known weakness in the mechanism. It's not documented, at least.\nWait, I've got an idea\u2026\n. Hmm it's harder than I thought. I was trying to get it to do a bundle exec inside prune_bundler  before re-execing yet again without Bundler like it currently does. I don't understand why it's so complicated though. If you allow the Bundler environment in Puma itself, why can't the restart simply be something like this?\nruby\nBundler.with_clean_env do\n  Kernel.exec(Gem.ruby, '-S', 'bundle', 'exec', *argv)\nend\nI quickly hacked this in and it seems to work here but maybe I'm missing the bigger picture.\n. I'm back from work now so can't test this in a hurry but try modifying the rack version in Gemfile.lock by hand instead of locking the Gemfile to a specific version. That's more representative of what's happening in production. \n. Indeed, I just tried to reproduce that way at home and failed. I tried with a pristine Rails app and that crashed like before. So Rails must be a factor.\nActually that makes sense. hello.ru does not load Bundler by itself so all that's happening is that Puma is stripping Bundler out and it subsequently never gets used. Rails fires up Bundler in boot.rb.\nBingo, if you add require 'bundler/setup' to the start of hello.ru then that triggers it.\n. I should add that the existing solution is broken for basic applications like hello.ru that do not explicitly load Bundler by themselves. Puma strips out Bundler and only loads dependencies required for itself so any additional dependencies required by the application would be missing. My simpler solution, suggested above, would fix this problem, as well as negating the need for any of the existing prune_bundler code, including puma-wild. Is there a downside?\n. Rack 1.5.5 is out now and I'm apprehensive about this tripping us (and others!) up in production again. I've made a suggestion but it's down to you to decide on the best solution.\n. @evanphx Well that was unexpected. Are you sure this is the right way to go? It does sidestep the issue as Puma has no other dependencies but it's not inconceivable that you may want to add one later. Removing Rack clearly also came at the cost of expanding the code base a little. Was there a problem with my suggestion?\n. @evanphx I thought that between Bundler.with_clean_env and Kernel.exec, you'd get a totally clean slate but I trust your judgement.\n. @evanphx Shouldn't the Gemfile include puma and therefore rack?\n. That's fair enough and I really don't mind but I thought I should follow this up because my approach would ultimately be less code (e.g. no more puma-wild) and would keep the door open for other dependencies in future. This could have happened with any gem, it's just that rack was the only dependency.\n. Sorry, bad habit!\n. Happy new year! Our next audit is coming up. What's the current feeling about this?. Me too. But @amichal, queue_requests is enabled by default and disabling it worked for me. Is that what you meant?\n. I mostly saw this while doing large database imports, which I think would exhibit the same behaviour as sleep.\n. Perhaps this is #1107?\n. I'll do that tomorrow. The error appears in Puma's output, probably from this line.\n. Sorry for not coming back to this. I was reminded of it recently but it's way down on my todo list at work.. Yes.. We've suddenly started seeing this on one of our applications, thankfully only on the staging server so far. I can't figure out what it is for the life of me. We're running two other Ruby applications on there. They're running Ruby 2.3 while the broken one is running 2.4 and we're using RVM to manage this. I've tried changing Bundler version, Puma version, RubyGems version, and updating other gems but no dice. Just as described, if I unset GEM_PATH or disable prune_bundler then it works but I shouldn't have to and that bothers me. We've been running with this setup for years.\nUpdate: I just tried copying the application to another directory. First I verified that it was still broken under Ruby 2.4 and then I reinstalled the gems under 2.3. Now it works. Huh.. I reinstalled 2.4.4 but still the same. I then tried 2.4.3 and it works. What the hell.. My issue is almost certainly bundler/bundler#6465 because if I replace the RVM global gemset symlink with a bind mount then it works.. Well doing any one of those three things (GEM_PATH, prune_bundler, or bind mount) makes the issue go away. I can only guess that unsetting GEM_PATH makes it work because instead of finding Bundler via /usr/local/rvm/gems/ruby-2.4.4@global (a symlink), it finds it via /usr/local/rvm/rubies/ruby-2.4.4/lib/ruby/gems/2.4.0 (what the symlink points to), which is already visible to that Ruby installation anyway.. In a somewhat desperate attempt for better performance, we put this into production on Monday and it's been working great.. @schneems, does using nginx mitigate that?. ",
    "ismaelga": "I've been able to fix this just by requiring require 'puma/minissl' outside of the class. I understand why it is now inside, to just get required when needed but is it really necessary?\nAny ideas on how to fix this?\nThx\n. great! Thanks!\n. ",
    "gonchs": "I am getting the same issue in 2.14.0\npuma -b 'ssl://127.0.0.1:3000?key=/Users/sergey/.ssh/server.key&cert=/Users/sergey/.ssh/server.crt'\nPuma starting in single mode...\n* Version 2.14.0 (ruby 2.2.3-p173), codename: Fuchsia Friday\n* Min threads: 0, max threads: 16\n* Environment: development\n/Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/activesupport-4.2.2/lib/active_support/dependencies.rb:274:in `require': cannot load such file -- puma/minissl (LoadError)\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/activesupport-4.2.2/lib/active_support/dependencies.rb:274:in `block in require'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/activesupport-4.2.2/lib/active_support/dependencies.rb:240:in `load_dependency'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/activesupport-4.2.2/lib/active_support/dependencies.rb:274:in `require'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/lib/puma/binder.rb:134:in `block in parse'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/lib/puma/binder.rb:84:in `each'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/lib/puma/binder.rb:84:in `parse'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/lib/puma/runner.rb:119:in `load_and_bind'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/lib/puma/single.rb:79:in `run'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/lib/puma/cli.rb:215:in `run'\n        from /Users/sergey/.rbenv/versions/2.2.3/lib/ruby/gems/2.2.0/gems/puma-2.14.0/bin/puma:10:in `<top (required)>'\n        from /Users/sergey/.rbenv/versions/2.2.3/bin/puma:23:in `load'\n        from /Users/sergey/.rbenv/versions/2.2.3/bin/puma:23:in `<main>'\n. ",
    "SamSaffron": "thanks, yeah the hijack protocol is weird, personally I think using Thin style throw :async after hijacking feels cleaner cause all sorts of weird can happen unwinding a big middleware stack. (etags, caching and so on) \n. honestly I don't remember off the top of my head, raise ulimits is the main one\n. In our farm we notice 2 or 3 thins just go into a \"hanged\" state once every few days. The way we detect it is by having haproxy do a http health check.\nIdeally I would like to have something like this:\n```\nping each worker every 30 seconds, check memory usage is below 700megs, check endpoint responds with 200\nif it fails, restart the worker\nworker_health_check :endpoint => 'http://somesite.com/my_health_check', :interval => 30, :max_memory_usage => 1024 * 1024 * 700\n```\n. @evanphx another thing worth looking at for context is https://github.com/kzk/unicorn-worker-killer\n. Puma support Rack Hijack. \nMessageBus and Action Cable build on this. Meaning the request hits puma, and then the hijack callback is made and it \"frees\" up the slot giving you back a raw socket to do what you will with. In message buses case it has a timerthread that manages 100s or thousands of these. \nThis SSE approach you are demonstrating is not using hijack so you will have trouble scaling it. Now ... personally I see zero point in using SSE.\nhttp://caniuse.com/#feat=eventsource\nYou can not use SSE in IE etc but can use XHR2 http://caniuse.com/#feat=xhr2 which gives you the ability to take advantage of chunked encoding in XHR requests (and stream data through an XHR - which is what message bus does) \nAdditionally if you want anything to be robust you got to support fallbacks (for example message bus has non chunked encoding and non long polling fallbacks) \nBottom line is avoid ActionController::Live, always use rack hijack or libraries building on hijack if you want to scale. \n. this is an improvement but still not ideal, I guess in a super ideal world you would patch the c binding for postgres as well to always double check if the socket is still good prior to running queries which would allow you to abort mid request. \nAlso since a middleware crawl can easily be a 10ms affair does it not makes sense to do one more check in the last middleware before handing the req to rails? \nI wonder if rack protocol should allow exposing this as well. > You'll be able to assert on puma.socket which is already in the environment. \nThe issue with that is that its not part of the rack spec, instead something like\nsocket.alive? being a proc that can be called by the app would keep the socket safe and could be available across the ecosystem\n\nand once it's in Rack-land, the user shouldn't have to worry about this anymore\n\nSort of, in the natural course of events a big pile of SQL statements get executed making a monkey patch to \"exec_async\" which double checks if the connection to client is even there prior to running sql a pretty appealing patch. That would be the only patch I would really consider. downside is that a proc gets called a bunch of extra times. \n. > I have a hard time parsing this, sorry. Can you elaborate further on what you mean by this? Are you saying this patch will not even be considered, or are you suggesting an alternative patch? What does SQL have to do with this?\nIt's @evanphx call what to do with this patch I am not on this project. \nAll I am doing is suggesting that \n\n\nA feature request be made to rack to add socket.alive? proc which enables the application to handle this if it so desires \n\n\nA proposed patch that the applications could make to save up on some work with a minimal monkey patch to either rails or whatever sql adapter they are using, which would allow you to catch this condition mid-request\n\n\n. What we do at Discourse with Unicorn/NGINX is this:\nhttps://github.com/discourse/prometheus_exporter/blob/master/lib/prometheus_exporter/middleware.rb#L70-L76\nWe set HTTP_X_REQUEST_START header and then have a metric of how long reqs are waiting between NGINX and unicorn. \nThis allows us quite extensive amount of excellent alerting ... we also use raindrops https://bogomips.org/raindrops/ which give us slightly cruder metrics (N Queued) \nWe use this kind of information for amazingly awesome features like: https://github.com/discourse/discourse/blob/master/lib/middleware/anonymous_cache.rb#L122-L131\nWe will force clients into an anonymous mode (and to use the blazing fast anonymous cache) if stuff starts queueing on a single route for logged in users. EG: someone posted ... OH NO OUTAGE AT EVE ONLINE on reddit and suddenly a herd of logged in users flood us. \nI don't know if you require NGINX in your architecture or not, but fronting stuff with it simplifies this so so much. \n. > I hope Discourse is threadsafe... I'm using it configured with a lot of Puma threads...\nYes Discourse is very threadsafe it has been so since day 1. . ",
    "wedgemartin": "I'm running puma 2.6.0, and since the patch, the response code is always 'HIJACKED'.  I've been unable to figure out a way to change it to anything from that without hacking the gem itself, which I'd like to avoid.  The problem is that when I run puma in production mode, it always reports a 500 from this call, even though it succeeds.  When run in development mode, it shows that the response code is actually HIJACKED, but I don't want to get into issues with NewRelic reporting high error rates just because it thinks  the response code is bad ( though I've not tested this with NewRelic yet, but I assume it will be the case )\n. :Looking at your patch it looks like this may only effect the log entry... hopefully.\n. The problem could also be that I'm a jackass.   I was looking at the production.log file when running in development mode... Had I been looking at the dev log, I would've seen the actual error, which has nothing to do with this.  Interesting though, that puma seems to ignore the RAILS_ENV and write to the production.log for the hijack output regardless.  Likely because the environment of my lambda'd process does not have RAILS_ENV set and it's defaulting to production or something along those lines.\n. ",
    "supagroova": "@SamSaffron we've recently authored a gem that provided this kind of async processing in Grape API (supporting the Rack Hijack API or AsyncCallback were applicable) called StuartApp/grape-async. We're about to jump into doing perf testing and I was wondering if you could provide some more details on the changes you made to your linux setup to achieve the performance you mentioned above?\n. Ok thanks, we\u2019ll share any further tweaks we find that work well here in the future.\n\nOn 03 Feb 2016, at 09:15, Sam notifications@github.com wrote:\nhonestly I don't remember off the top of my head, raise ulimits is the main one\n\u2014\nReply to this email directly or view it on GitHub https://github.com/puma/puma/issues/332#issuecomment-179081831.\n. \n",
    "srgpqt": "Thanks for the quick fix!\n. ",
    "renatosnrg": "I'm having this same issue: NewRelic is not reporting data when running Puma in non-clustered mode with daemonize. If I enable the clustered mode it works fine.\nI'm using the latest versions of both newrelic_rpm (3.6.6.147) and puma (2.4.0) gems. I also have an open ticket with NewRelic support. This issue thread finally explain what is happening :)\nThanks!\n. ",
    "HDLP9": "Hi,\nStill no fix for non-clustered mode? With clustered mode it works like a charm but it would be great to have a solution for this.\nThanks!\n. Hi @jasonrclark!\nThanks for all the work. I've just updated to 3.7.1.180 version, added restart_thread_in_child: true to newrelic.yml and deployed the app. (Gemfile with gem 'newrelic_rpm')\nIn newrelic the app is now active (green) but no data is being sent. When checking the HTML I see that newrelic js tags are missing.\nDo I need to make any more configuration?\nThis is really anoying beacause for a small, but important app, I need to have 2 ruby processes (more memory usage) to be able to send data to newrelic. \nMany thanks!\n. Hi @benweint,\nsorry for not responding but you know...holidays!!!!\nI'm using:\n- Puma 2.7.1\n- Rails 4.0.2 with turbolink enabled\n- ruby-2.0.0-p247\nSo the problem must be the ruby version. For me I don\u00b4t think that you need to work around this issue, not worth the trouble. \nI\u00b4ll try to update to the new stable version 2.1 and let you know. \n. ",
    "benweint": "Hi @Helio-Pereira - thanks for trying this out! There shouldn't be any additional configuration that's needed.\nIt's possible that you're just hitting a bug in our implementation (as Jason alluded to, the restart_thread_in_children feature is still considered experimental), or there might be an unrelated reason that your app isn't sending data in this scenario.\nIt's difficult to say what the issue is remotely without more information, specifically:\n1. The contents of your newrelic_agent.log after a restart of Puma (extending for a few minutes after the restart, preferably). This will be most useful if you're able to set log_level: debug in your newrelic.yml file in order to increase the verbosity of the log.\n2. The exact command you're using to start puma\n3. The version of Puma you're using, and a brief description of your app (is it a Rails app, Sinatra, something else?)\nIf you don't want to post the log contents publicly, feel free to follow up via a support ticket (mention my name and it'll get routed to the right place) or with me directly.\nEdit: original comment had an incorrect name for the configuration key. The correct name is restart_thread_in_children, not restart_thread_in_child.\n. @Helio-Pereira we just uncovered an issue that may be causing the restart_thread_in_child functionality to not work, depending on what version of Ruby you're on.\nIf you're on MRI < 2.0.0-p353, you're likely hitting https://bugs.ruby-lang.org/issues/8616\nIf you're on Rubinius 2.x, you're likely hitting https://github.com/rubinius/rubinius/issues/2861\nIn short: we rely on Thread#alive? to return an accurate value in order for our automatic restarting to work, and those two bugs cause it not to. We can probably figure out how to work around them, since a large number of folks are likely on versions of MRI / Rubinius that are affected.\n. @jeroenj well this is embarrassing - the setting is actually called restart_thread_in_children, not restart_thread_in_child. The CHANGELOG and Jason and I's comments above are incorrect (the setting is declared in the source here). Mind giving it a try with the correctly-named setting?\nSorry for the mix-up! I'll get the CHANGELOG corrected.\n. @jeroenj Glad to hear it, thanks for following up!\n. Hey folks - I know this issue's closed, but I wanted to give you a brief update. We've just released a new version of the newrelic_rpm gem (3.8.0) that should fix this issue.\nIn the latest version of the gem, we will automatically restart our background thread when we notice that a web transaction is processed and the PID we last started the thread in differs from the current PID.\nThis is effectively the same as the restart_thread_in_children setting mentioned above switching from default off to default on.\nIf you're still having trouble getting Puma (in any mode) working with newrelic_rpm, please let us know via a support ticket or in our forum and we'll be happy to help you out.\n. ",
    "jeroenj": "Hi @benweint,\nI've also tried this config but it doesn't seem to work either:\ncode\n[01/02/14 09:57:08 +0100 server (5772)] INFO : Starting the New Relic agent in \"production\" environment.\n[01/02/14 09:57:08 +0100 server (5772)] INFO : To prevent agent startup add a NEWRELIC_ENABLE=false environment variable or modify the \"production\" section of your newrelic.yml.\n[01/02/14 09:57:08 +0100 server (5772)] INFO : Reading configuration from config/newrelic.yml\n[01/02/14 09:57:08 +0100 server (5772)] INFO : Enabling the Request Sampler.\n[01/02/14 09:57:08 +0100 server (5772)] INFO : Environment: production\n[01/02/14 09:57:08 +0100 server (5772)] INFO : Dispatcher: puma\n[01/02/14 09:57:08 +0100 server (5772)] INFO : Application: trouw\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing ActiveRecord instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing Net instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing deferred Rack instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing Puma cluster mode support\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing Rails 3 Controller instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing Rails 3.1/3.2 view instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Installing Rails3 Error instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Finished instrumentation\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Doing deferred dependency-detection before Rack startup\n[01/02/14 09:57:09 +0100 server (5772)] INFO : Starting Agent shutdown\n[01/02/14 09:57:09 +0100 server (5790)] INFO : Starting Agent shutdown\nI don't have any special configuration set for puma and restart_thread_in_child: true is set. I'm using rails 3.2.15 on ruby 2.1.0. If you need anything else, please let me know. :)\n. We all make mistakes, right?! :)\nAnyway, that seems to fix it. My application is now reporting to New Relic. :thumbsup: \n. ",
    "jloper3": "Bump? I'm seeing this intermittently on Sinatra, so maybe its rack related?\n. ",
    "donamk": "@luislavena Thank you so much. Successfully installed Puma.\n. ",
    "lozandier": "It worked for me; @luislavena is awesome as always with these sort of problems; Is there like a list of gems that must go through this? So far, I've come across pg, sqlite and the puma gem that needs this sort of configuration...\n. @luislavena: I see;  thank you for the clarification.  For some reason, I had to follow your old instructions regarding pg for it to actually successfully connect to my PostgreSQL 9.3 x64 installation. \nHave had to manually rewrite Bundler's habit of changing the pg declaration to x86 instead of 64bit (as well as forking a build of eventmachine to successfully build as a gem manually to work with Ruby 2.0 x64). That may have been the actual problem.\nI was told to update to Bundler 1.4.rc1 (a version that understands x64 gems), but  that version of Bundler wants me  to insists have Nokogiri x64 no matter what.... Strange thing is before that (all of last evening and morning at least), I didn't need  to have the x64 version of Nokogiri to run the Rails 4 app I was developing w/ the 64 bit version of Ruby 2.0.\nNote: I'ved use Ruby 32-bit all this time and only decided to give Ruby 2.0 x64 bit a try again after I heard contributors fixed the atomic gem and bcrypt bug that prevented Rails 4 from being able to be used conveniently.\n. ",
    "ccoenen": "@luislavena you just saved me a ton of time right there! :+1: Thank you very much!\n. ",
    "gihdz": "Following your instructions @luislavena, it got me into this:\ngem install puma --platform=ruby -- --with-opt-dir=C:/Knapsack/x86-windows\nBuilding native extensions with: '--with-opt-dir=C:/Knapsack/x86-windows'\nThis could take a while...\nERROR:  Error installing puma:\n        ERROR: Failed to build gem native extension.\nC:/row/Ruby21/bin/ruby.exe -r ./siteconf20150202-6560-i9u6l7.rb extconf.rb --with-opt-dir=C:/Knapsack/x86-windows\nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\ncreating Makefile\nmake \"DESTDIR=\" clean\nprocess_begin: CreateProcess(NULL, rm -f, ...) failed.\nmake (e=2): The system cannot find the file specified.\nmake: [clean-static] Error 2 (ignored)\nprocess_begin: CreateProcess(NULL, rm -f puma_http11.so .o *.bak mkmf.log puma\nhttp11-i386-mingw32.def ._.time, ...) failed.\nmake (e=2): The system cannot find the file specified.\nmake: [clean] Error 2 (ignored)\nmake \"DESTDIR=\"\ngenerating puma_http11-i386-mingw32.def\nThe system cannot find the path specified.\nmake: *** [puma_http11-i386-mingw32.def] Error 1\nmake failed, exit code 2\nGem files will remain installed in C:/row/Ruby21/lib/ruby/gems/2.1.0/gems/puma-2\n.11.0 for inspection.\nResults logged to C:/row/Ruby21/lib/ruby/gems/2.1.0/extensions/x86-mingw32/2.1.0\n/puma-2.11.0/gem_make.out\nAny way i can solve this??\nRegards\n. @luislavena it worked! Just re-installed ruby 2.1.5 x86, rubygems 2.3.0 and everything worked fine! Thanks!\nRegards.\n. ",
    "Bogeyduffer": "This is great! I spent the last two day trying to get this to install. With your help it worked in just a few minutes. Thanks!  I wish I'd found this two days ago.\n. ",
    "ghiculescu": "I'm seeing similar results, except I'm getting 40 new strings every 15 seconds. Here's a quick sample: https://gist.github.com/ghiculescu/56dc083def5792c4e5d4\n. ",
    "JamesHarker": "I think I'm having a similar issue on 2.7.1. Memory seems to rise before plateauing out.\nI've also tried running Puma in clustered mode (using preload_app!) and again memory started to slowly rise but didn't have a chance to plateau as I ran out of memory. Whilst still in clustered mode, I changed the number of workers down to 1 but again, memory kept climbing before I ran out.\nDoes enabling clustered mode use more memory? I would have thought that running clustered mode with 1 worker would be the same as not running clustered mode? In both cases the memory starts at the level and then rises, but clustered mode doesn't seem to stop.\n. @evanphx Yes, after thinking about it a bit more I think we may just need to play about with our GC settings as the default for MRI 2.1 may be a bit high. Just thought it was a bit strange that memory keeps climbing with 1 worker in clustered mode where as it stops climbing when not using clustered mode.\nWe haven't used Unicorn, only ever Puma :heart:\n. ",
    "fesplugas": "Using the insanely simple rack app we see the problem reproduced. We can see that without any request to the server the memory is growing.\n20:30:39 web.1  | [97156] PumaWorkerKiller: Consuming 35.20703125 mb with master and 4 workers\n20:30:40 web.1  | [97156] PumaWorkerKiller: Consuming 35.2265625 mb with master and 4 workers\n20:30:41 web.1  | [97156] PumaWorkerKiller: Consuming 35.2265625 mb with master and 4 workers\n20:30:42 web.1  | [97156] PumaWorkerKiller: Consuming 35.23046875 mb with master and 4 workers\n20:30:43 web.1  | [97156] PumaWorkerKiller: Consuming 35.25390625 mb with master and 4 workers\n20:30:44 web.1  | [97156] PumaWorkerKiller: Consuming 35.25390625 mb with master and 4 workers\n20:30:45 web.1  | [97156] PumaWorkerKiller: Consuming 35.2578125 mb with master and 4 workers\n20:30:46 web.1  | [97156] PumaWorkerKiller: Consuming 35.26171875 mb with master and 4 workers\n20:30:47 web.1  | [97156] PumaWorkerKiller: Consuming 35.265625 mb with master and 4 workers\n20:30:48 web.1  | [97156] PumaWorkerKiller: Consuming 35.30078125 mb with master and 4 workers\n20:30:49 web.1  | [97156] PumaWorkerKiller: Consuming 35.3046875 mb with master and 4 workers\nIn our productions apps we are using PumaWorkerKiller to fix the issue.\nHere's a link to the application which reproduces the error https://github.com/fesplugas/puma-memory-leak\nEdit: Added link to the app which reproduces the error.\n. A trivial rack app with the memory leak was provided a few months ago.\nhttps://github.com/puma/puma/issues/342#issuecomment-60655325\n. ",
    "Kagetsuki": "This issue needs to be re-opened. I just confirmed the constant memory growth with a vanilla app and https://github.com/schneems/derailed_benchmarks . I simply did a rails new, added derailed benchmarks, and ran derailed benchmarks with puma and a 5 million query duty cycle. The memory never stopped growing. With webrick it climbed a bit but then remained at 121MB (only did 1 million cycles but it stablized at 121MB very early on). The rate of growth is also greater - it reached ~190MB at about 1 million cycles with Puma.\nTest env is Ruby 2.1.4 with Rails 4.1.7.\n. @evanphx Puma is version 2.9.2. I'll test with 2.0 now, but seeing as to how this issue did not appear in webrick I'm suspecting Puma. Of course it could be an issue with Puma and Ruby 2.1, so hopefully this test will help us determine that. I'll re-post when the test is done (it takes a very long time).\n. I'm only an hour into testing but it looks like it stabilized. So to run it down:\n- Ruby 2.1.4 + webrick = stabilizes, no leaks\n- Ruby 2.1.4 + puma = does not stabilize, leaks\n- Ruby 2.0.0 + puma = stabilizes (probably), no leaks (probably)\nMy conclusion here being that Puma leaks on 2.1.4. Not that 2.1.4 leaks, not that Puma leaks, but that Puma leaks when run under 2.1.4.\nI think we need someone else to run my tests though. Anyone up? Shall I explain the process in more detail?\n. @schneems Like I said above I just did \"rails new vanilla\", then added derailed_benchmarks as per the instructions.\nI'm assuming the patch @bashcoder mentioned isn't in the gem yet? I'll go ahead and add github master to my Gemfile for puma and run the test again. Then I'll try @schneems suggestion and see what results I get. I'm guessing @schneems gets no memory leak when he runs that test?\n. OK, I'm still getting a memory leak with github master.\n@schneems I can't seem to get derailed_benchmarks to run with a simple rack app. I get \nLoadError: cannot load such file -- application\nfrom line 22 of lib/derailed_benchmarks/tasks.rb .\nMore than that, can you confirm to me that you do not see this leak with a plain Rails setup and 2.1.4? I'm testing on my own here and getting these results but that doesn't mean there isn't something else in my environment that is causing these issues. I think it would be more helpful if someone else could run these tests once and report their results than me just running more tests. The fact is I am seeing an observable leak in Rails with puma and 2.1.4 - seeing if that leak is present with Rack could help narrow the field to observe but it doesn't change the fact that I am getting these results. If someone else does not get these results then we can close this issue for now and I can try and figure out what it is that's wrong (the fact that I'm on cedar-14 and using an Ubuntu 14 machine coupled with the the new memory allocation models in both Ruby 2.1 and Ubuntu 14 for example...).\n. My app isn't spawning more threads than the thread limit but I do have a background process which spawns threads (and cleans them up properly). I can confirm that I am not seeing memory increase due to a thread increase and I am not seeing an increase in threads that correlates to anything that looks like a memory leak. Furthermore the memory leak I tracked with derailed_benchmarks had no additional gems/libraries, no database models, and was doing nothing but rendering the default static view and yet I was seeing a leak with Puma.\nIn other-words @bashcoder is dealing with an entirely different issue which by his own admission is due to external sources. This does not change the fact that others, including myself, have observed and diagnosed a memory leak which only occurs with Puma. That is not to say however that it is not thread related - that possibility does exist (EG: spawn new instance with mutex + mutex locks instance in memory after dereference kind of scenario?), but at the moment I can not say I personally have this suspicion.\n. @hughevans I suspected logging from the start because memory leak rate does seem consistent with strings continually being lost. Maybe someone should set up a test app which just puts out tons of logs at a high rate and see if there is an observable spike in leaking. I'm super busy at the moment, any takers?\n. @tozz I never came up with anything so I'm just using puma killer. Worst case scenario for our app a user gets a timeout on a search. I'm curious about the setup of the people who say it doesn't leak... anyone?\n. @evanphx I really appreciate you bro but:  \n\nThis is totally unexpected and quite strange  \n\nSeveral of us have kind of been pointing this out for quite a while now. See above...  \nThe HUGE problem in all this is that Ruby makes it very very difficult to manage memory or find memory leaks. There are patches and hacks etc. etc. etc. yes but there really isn't a valgrind - and until there is I bet this problem will be close to impossible to fix. Personally I'm done with dealing with this because I'm sure that to fix it will take waaay too much effort. Puma worker killer works - and if you have some issue with long sessions that can't be interrupted you probably needed to refactor something anyway.  \n@schneems chose the right image to sum up the situation (from derailed benchmarks):\n\n. @evanphx You certainly didn't seem flippant - I was trying to say we appreciate your work on the issue but honestly I don't expect much progress because it's so hard to diagnose. I think our time would be better spent setting up some real way to analyze/manage memory in Ruby sophisticated enough that we can have a valgrind like tool and then using that to figure out what's going on. Maybe with static typing will come the basis for a setup like that?\n@tozz I did crazy memory stat runs, some spanning multiple days using a variety of tools and a patched Ruby and collected extended logs and the best I could come out with was it looked like short strings were getting stuck in some zombie heap, perhaps related to logging/back end output. But honestly I never figured out if it was rack or puma or Ruby 2.1 or 2.2 or 2.0 or what. As you'll note with the stats you collected everything is just a vague number - if one section were to increase you could kind of get an idea of which hay-stack to start looking in, but we're still talking about a very well hidden needle and we've got these abstracted dynamic-memory mittens on which make it really hard to dig around.\n. @tozz Just so you know what we're dealing with: I did embedded system dev in C/C++ and ASM and I've had to build my own memory managers and ran extremely low-level memory checks. I'm familiar with tools that can find a leak in an embedded app on a remote target. I used to simulate race conditions and broken mutexes on multi-threaded applications and was capable of finding memory issues. I've read core dumps and taken apart binary back-traces. And after all that I couldn't for the life of me get any idea what the f* is going on with memory in a one line rack app.  \n@evanphx Good luck my friend!\n. @tozz I think he's referring to setting environment variables to trigger GC timing and allocation limits.\nOlder article and I'm sure some of this changed in 2.2 but this should give you an idea:\nhttp://samsaffron.com/archive/2014/04/08/ruby-2-1-garbage-collection-ready-for-production\nI've played with these as well and my results were mixed - but no leaks seemed to get fixed from this\n. @evanphx \n\nWhat my current test suggests is that ab doesn't track the data for the results very efficiently.\n\nThat, sir, is an understatement. Also, in your tests are you disabling auto GC? It occurs to me that if there is a leak having auto GC enabled during the test will produce some very confusing numbers. Disable auto GC, enter loop, manually call GC each cycle, track results.\n. @evanphx What system are you running again? Thus far I've seen it on Ubuntu x64 with rvm, heroku cedar 14 with stock 2.1.x and 2.2.0, and OSX with rvm. I'm gathering Debian 7 x64 is also seeing the issue. So that gives us deb based distros and OSX seeing this with rvm; leaving RedHat/Fedora (and Windows but that's not really worth mentioning here) that I don't think we're sure about and I guess rbenv or manually compiled ruby. Perhaps if you could tell us what system you are running we could set up machines ourselves and see if we can't produce it on that environment.\n. @evanphx Check above:\n``` ruby\nrequire 'rack'\nrequire 'rack/server'\nProc.new {|env|  [200, {}, ['Hello World']] }\n```\nRun that with derailed benchmarks a few thousand times.  \nLet me just note though that I have NOT tried this with rbenv and the only person in this discussion other than you who has noted they are using rbenv seems to have a leaking app and has not provided results from a simple test like the one above. Seeing as to how we're seeing leaks with rvm and with manually compiled Ruby we can't discount the possibility rbenv has some extra flags set or something which is making it a little more aggressive or efficient with memory. Perhaps if you could fire up a VM and throw rvm on it and if someone else could fire up an rbenv install and run the same test we may be able to determine if it has something to due with that.  \nPersonally I have 0 experience with rbenv so I think it would be wise if someone who knew what they were doing did that instead of me.  \nAlso, you're running puma release or a dev branch?\n. @leifcr we're starting to see that some environments do not see memory growth and some do. Please provide your environment details.\n. This whole bug report thread is starting to depress me. I'm going to unsubscribe for now - PM or hit me up on something if I'm needed.\n. @ko1 The memory leak is gradual over time, it takes at least a few hours to really notice it.\n. Combining @ponny and @rhymes comments: No memory increase when using one worker with multiple threads or multiple workers with single threads.\n\u203bI'm not confirming either of these, just stating. Perhaps someone can test?\n. ",
    "bashcoder": "I deleted my previous comment regarding my memory leak issue, because it seems that a big part of my thread-growth (and therefore memory growth) problem was just patched in #594.\n. @hughevans - Have you checked to see if your thread count per process is growing over time? The reason I ask is that this looks a lot like my graphs, which turned out to be mostly an application issue. Part of my problem was that threads were overstaying their welcome due to database libraries that weren't thread-safe.\nPuma was convinced that it was managing 12 active threads, but I would sometimes see 100 threads build up per worker process. These are problems that never showed up in non-threading app servers such as Passenger and Unicorn.\nSo Puma thought the threads were done, but they weren't -- apparently because they were sharing and holding on to connections to resources that weren't meant to be shared. Not saying it isn't a Puma issue -- might be. But it means one thing if the garbage isn't being collected properly, and it means another thing if there is a steady growth in threads under each worker process.\n. It appears that this patch helped me out a lot - thanks @hassox!\nI am migrating from passenger enterprise to puma, and I have a Sinatra app that memoizes several datastore connections using Thread.current[] keys in singletons. I observed a steady growth in the thread count per cluster worker (far beyond the configured puma limits), and memory usage growth over time. My various datastores were also showing an increase in connections over time.\nI'm still having a few threading issues to track down, and I'm sure I'm not properly handling a few things. But this PR has solved several of my issues.\n. ",
    "pkmishra": "+1 I am also seeing same issue with Ruby 2.1.2\n. ",
    "hughevans": "MRI 2.1.3, Rails 4.1.7 and Comfortable Mexican Sofa:\n\nDoing manual restarts when we receive CloudWatch alerts.\nExactly the same app running with Unicorn does not leak.\nEdit: Puma 2.9.2\nEdit 2: I\u2019ve now fully caught up with what is above. When we were running Unicorn it could have been restarting processes for us. Looking into puma_worker_killer now.\n. I\u2019m not sure if this is useful or not, but when we rotate the logs and send a HUP signal, the memory usage goes up:\n\n. ",
    "tozz": "I noticed this today too and went deep into my app trying to figure out what was leaking, then I made a super simple test and it leaks in puma while thin keeps it consistent.\n``` ruby\nclass App\n  def self.call(env)\n    [200, {}, []]\n  end\nend\nrun App\n```\nRuby 2.2.0 Puma 2.11.1 (default startup settings, with the exception of -q to make sure no strings are generated for the log output).\n. It just seems like a massive anti pattern to me :/\nI tested on a fresh Debian box, I only installed Ruby, Rack, Thin and Puma to be able to compare.\nYeah, I was spending hours looking at my own app, just to realize Puma leaks no matter what. Would love to hear from someone who can say with certainty that Puma version x and Ruby version y doesn't leak at all.\n. 100-500KiB per 5000 requests. I'm looking at Real Memory Size and Private Memory Size in OSX (RSS under Debian), which isn't necessarily the best counters I know, but they still shouldn't increase that much. Using rbtrace I force the GC to run between passes but it doesn't reclaim everything (or even much of it).\nI would love to be proven wrong here though :)\n. Here's my data (puma -t 0:16 -q)\nI've attached GC.stats, I don't know if it will help or not, figured it's better to have more data than less.\nRSS initially (no requests) 17624\nInitial GC.stat (no requests)\njson\n{\n  \"count\": 13,\n  \"heap_allocated_pages\": 132,\n  \"heap_sorted_length\": 133,\n  \"heap_allocatable_pages\": 0,\n  \"heap_available_slots\": 53799,\n  \"heap_live_slots\": 53200,\n  \"heap_free_slots\": 599,\n  \"heap_final_slots\": 0,\n  \"heap_marked_slots\": 28171,\n  \"heap_swept_slots\": 24848,\n  \"heap_eden_pages\": 132,\n  \"heap_tomb_pages\": 0,\n  \"total_allocated_pages\": 132,\n  \"total_freed_pages\": 0,\n  \"total_allocated_objects\": 223144,\n  \"total_freed_objects\": 169944,\n  \"malloc_increase_bytes\": 4054720,\n  \"malloc_increase_bytes_limit\": 16777216,\n  \"minor_gc_count\": 9,\n  \"major_gc_count\": 4,\n  \"remembered_wb_unprotected_objects\": 458,\n  \"remembered_wb_unprotected_objects_limit\": 916,\n  \"old_objects\": 26951,\n  \"old_objects_limit\": 53906,\n  \"oldmalloc_increase_bytes\": 7700768,\n  \"oldmalloc_increase_bytes_limit\": 16777216\n}\nRSS after 20k requests 27216\nGC.stat after 20k requests\njson\n{\n  \"count\": 48,\n  \"heap_allocated_pages\": 132,\n  \"heap_sorted_length\": 133,\n  \"heap_allocatable_pages\": 0,\n  \"heap_available_slots\": 53799,\n  \"heap_live_slots\": 52425,\n  \"heap_free_slots\": 1374,\n  \"heap_final_slots\": 0,\n  \"heap_marked_slots\": 30438,\n  \"heap_swept_slots\": 1392,\n  \"heap_eden_pages\": 132,\n  \"heap_tomb_pages\": 0,\n  \"total_allocated_pages\": 132,\n  \"total_freed_pages\": 0,\n  \"total_allocated_objects\": 983385,\n  \"total_freed_objects\": 930960,\n  \"malloc_increase_bytes\": 44384,\n  \"malloc_increase_bytes_limit\": 16777216,\n  \"minor_gc_count\": 44,\n  \"major_gc_count\": 4,\n  \"remembered_wb_unprotected_objects\": 576,\n  \"remembered_wb_unprotected_objects_limit\": 916,\n  \"old_objects\": 29474,\n  \"old_objects_limit\": 53906,\n  \"oldmalloc_increase_bytes\": 10906032,\n  \"oldmalloc_increase_bytes_limit\": 16777216\n}\nOn my way to 40k requests the server hangs for a while (Apache bench stalls), but it eventually recovers. It's not the GC doing a major sweep, the count is the same.\nRSS at 60k requests 36320\nAnother hang on the way to 70k, I think it might be the number of requests coming in way too fast, it's a scenario that would never happen in real life (99% requests completed within 0ms).\nRSS at 100k requests 38728\nGC.stat after 100k requests\njson\n{\n  \"count\": 205,\n  \"heap_allocated_pages\": 132,\n  \"heap_sorted_length\": 133,\n  \"heap_allocatable_pages\": 0,\n  \"heap_available_slots\": 53799,\n  \"heap_live_slots\": 52602,\n  \"heap_free_slots\": 1197,\n  \"heap_final_slots\": 0,\n  \"heap_marked_slots\": 31263,\n  \"heap_swept_slots\": 2425,\n  \"heap_eden_pages\": 132,\n  \"heap_tomb_pages\": 0,\n  \"total_allocated_pages\": 132,\n  \"total_freed_pages\": 0,\n  \"total_allocated_objects\": 4404442,\n  \"total_freed_objects\": 4351840,\n  \"malloc_increase_bytes\": 125904,\n  \"malloc_increase_bytes_limit\": 16777216,\n  \"minor_gc_count\": 201,\n  \"major_gc_count\": 4,\n  \"remembered_wb_unprotected_objects\": 698,\n  \"remembered_wb_unprotected_objects_limit\": 916,\n  \"old_objects\": 30107,\n  \"old_objects_limit\": 53906,\n  \"oldmalloc_increase_bytes\": 10698480,\n  \"oldmalloc_increase_bytes_limit\": 16777216\n}\nIt strikes me that no major GC run has been done, so lets do that and see what happens.\nRSS after GC.start 38748\nGC.stat\njson\n{\n  \"count\": 206,\n  \"heap_allocated_pages\": 132,\n  \"heap_sorted_length\": 133,\n  \"heap_allocatable_pages\": 0,\n  \"heap_available_slots\": 53799,\n  \"heap_live_slots\": 30219,\n  \"heap_free_slots\": 23580,\n  \"heap_final_slots\": 0,\n  \"heap_marked_slots\": 30182,\n  \"heap_swept_slots\": 23616,\n  \"heap_eden_pages\": 132,\n  \"heap_tomb_pages\": 0,\n  \"total_allocated_pages\": 132,\n  \"total_freed_pages\": 0,\n  \"total_allocated_objects\": 4404567,\n  \"total_freed_objects\": 4374348,\n  \"malloc_increase_bytes\": 3536,\n  \"malloc_increase_bytes_limit\": 16777216,\n  \"minor_gc_count\": 201,\n  \"major_gc_count\": 5,\n  \"remembered_wb_unprotected_objects\": 545,\n  \"remembered_wb_unprotected_objects_limit\": 1090,\n  \"old_objects\": 29247,\n  \"old_objects_limit\": 58494,\n  \"oldmalloc_increase_bytes\": 3920,\n  \"oldmalloc_increase_bytes_limit\": 16777216\n}\nRerunning it results in the same kind of results and I haven't seen it plateau ever, RSS keeps going up indefinitely (within my testing, ~500k requests).\n. I agree @Kagetsuki, hard to pinpoint anything and it's also something I'm very unfamiliar with in general.\n. @JoshMcKin: What Ruby version are you running? Want to share what GC tuning you've done?\n. @evanphx Debian 7 x64, but the long redundant post above was on OSX. I can't even finish the test you ran, ab quits with apr_pollset_poll: The timeout specified has expired (70007)\nTotal of 16353 requests completed which is probably just a system configuration issue on my end.\nThe reports from the app is identical for every 5k request though:\n============ 10931611\nFREE: 27509 => 27537 (-28)\nT_STRING: 15597 => 15570 (27)\nT_HASH: 109 => 108 (1)\nDoes anyone know if the ruby-build plugin for rbenv has its own customizations when compiling?\n. If you're referring to me I showed my test app at https://github.com/puma/puma/issues/342#issuecomment-75843728\n. @evanphx I have a Debian installation at Digital Ocean that exhibits the behavior, it's brand new, nothing sensitive, I can send you the details if you want to check it out yourself, just tell me where to send it :)\n. @evanphx To set up the environment I went with https://www.digitalocean.com/community/tutorials/how-to-install-ruby-on-rails-with-rbenv-on-debian-7-wheezy (for rbenv and plugins, nothing Rails related). Usually I do things differently, but I figured a documented way would be better for reference. There's an issue with it though, you also need to install libffi-dev otherwise rbenv installation will fail.\nThen I did everything as described earlier (puma -q and ps to grep for RSS), just using ab to push requests, after 500k it still hasn't stopped growing. I know people say ps is not a good way of looking at memory consumption, but I haven't seen any other recommendations either.\n. Thanks for the quick fix, as for how common it is I have no idea, I've been doing it like this for many years with thin and puma and didn't stumble upon any issue until 2.12.2.\n. ",
    "davidelbe": "I am also seeing this memory increase on an app running Puma 2.11, Ruby 2.2.0. Unicorn or Thin does not have the same problem. At first I thought it was an issue with my app, but when other servers are stable in memory consumption I guess it can be Puma.\n. @evanphx Slowly, and not with every request but it builds up to over 1 GB of memory after a couple of days in production. Using Thin/Unicorn it grows to about 200 MB (90 MB directly after startup) and stays there. I am running ruby 2.2.0p0 (2014-12-25 revision 49005) [x86_64-linux] via rbenv. Puma is running a Rails app (4.2) behind nginx and connects to a mysql database if that matters, but given the simple example above it looks like it is more general than that. How can I help with more useful data?\nI am currently using monit to restart the process when it grows too large in memory.\n. ",
    "JoshMcKin": "I have more than a couple Rails 3.2 applications puma in production that experience don't experience unchecked memory growth. All are running 2.1.5 with and most have some GC tuning to ensure memory is managed. Prior to running puma I ran async-rails apps on thin which would suffer memory issues if the fiber pool was too large, although that was a couple years ago now.\nComparing  evented (thin), process (unicorn) and  thread severs for debugging a threaded server is probably not a fair comparison. Your \"thread-safe\" code is only tested in puma. \n. Ruby 2.1.5\nRails 3.2\nThere are plenty of GC settings to tool with but RUBY_GC_MALLOC_LIMIT_MAX\nRUBY_GC_OLDMALLOC_LIMIT_MAX are the only 2 I have found that stop unchecked growth for our applications which usually set at 9000000, although we ticker with most of the available settings.\nI found http://tmm1.net/ruby21-rgengc/ to be helpful. \nI've also heard/read, but cannot remember where, MRI does not return memory to the system after GC, it keeps it, like JVM, for re-allocation.\n. Use an analytics service that can monitor GC like NewRelic. \nAnything can be tested if enough effort is put forth by the victims in this issue.\n. @bufordtaylor you have proven your app uses more memory when running with more than 1 thread, not that puma has a memory leak.\nYour app probably uses dozens if not hundreds of gems, all of which must be checked for thread safety and memory leaks before assessing blame. \n. I was able to work around the issue by using a POST route but it would be nice to keep controllers a bit drier by having the option to increase maximum query string.\n. Its a feature that I believe Thin supported, if its not something Puma wants to do,  thats fine. Just thought it would be nice.\n. I should clarify this GET was to an INDEX action in a Rails controller. The index action accepts parameters for a search. I have yet to work on a Rails project where at least 1 controller index action doesn't accept extra parameters. In this case, one of the input parameters could be an array of user ids. A client wanted to perform a search that included all of their users with the exception of 1, they have 1500 users and as you can image the resulting form post query string was quite long. Our work around to use a POST request to a \"search\" action that is really an alias of index, while effective in providing immediate relief for the client, does not follow our implementation of a standard REST API.\nInitially we were not sure what was happening. When the client would submit the form the app was just rendering a blank screen. I tried tailing the Heroku logs (where the app is deployed) and nothing would show in logs when the form was submitted, New Relic missed the error as well. It wasn't until I started the app locally in production with dev logging that I was able see the error.\n. Could be rack time out is killing the request before it releases the lock on the table. I'd try removing it.\nhttps://github.com/heroku/rack-timeout#timing-out-inherently-unsafe\n. @vincentwoo, ask Heroku if you have a \"noisy neighbor\". We saw something similar on standard dynos a few months ago. Heroku's support explained another users application, sharing server public resources with our app, was eating all the public resources. The issue i have described only occurred on a single dyno and after restart the issue would disappear. \nIf you don't have something now, get NewRelic or a similar service and enable their  \"availability monitoring\" which alert you of a down time. Then the next time you have an event, open a ticket with Heroku with their highest priority.\nI've never experienced issues using NewRelic described by @guillaumebesse but if you are concerned there are other app monitoring services available.\n. More info:\nRemoved Ruby 2.0 and reinstalled but still got a compile error. \nCan install Puma 2.14.0 on Ruby 2.0\n. Thanks for the quick response. Yeah its time to update this system.\n. ",
    "pencilcheck": "This is still happening, on the latest puma.\n. Phased restart still doesn't work even with @jcoleman's solution. Running puma 2.9.1.\nBut with hot restart, it seemed to work without setting the env variable.\n. hot restart stops loading the new context in puma 2.9.2\n. What I mean by context is the line \"Gemfile in context: /path/to/Gemfile\" in puma.log\n```\nthreads 0, 16\nworkers 2\nbind \"unix:///var/run/wwm.sock\"\ndaemonize false\nactivate_control_app 'unix:///var/run/pumactl.sock'\nstdout_redirect 'log/puma.log', 'log/puma_error.log', true\npidfile 'tmp/pids/puma.pid'\nstate_path 'tmp/pids/puma.state'\nprune_bundler\n``\n. Doesn't work, sending USR1 for phased restart will still load the old path, but if sent USR2 it will restart with new path\n. @maljub01 I'm using ruby 2.1.1, puma 2.9.0\n. This doesn't work for me somehow using mina\n. This is the command I used, and it works all the timekill -s USR2 `\nBut USR1 always starts new process with the old path and thus it is essentially not working at all.\nI'm running puma 2.9.0 and ruby 2.1.1\n. ",
    "kartikluke": "@leifcr What environment are you using?\n. ",
    "ericraio": "I am having this issue too :( \n. I just compiled with jemalloc, deployed to production and it's still leaking\n. ",
    "nacengineer": "I'm not sure if this helps or confuses the issue but I think it might be related to running puma on a Virtual Machine instance. In my case I'm running on vSphere 5.0.0 and this issue seems to coincide with when the VM starts to swap out for more memory. However with this issue, the ballooned memory doesn't self correct as it should normally and the whole VM starts to crawl. Even ssh'ing into the machine is slowed. The only thing that fixes the balloon is a system restart. Note the app isn't even under much load when this happens. \nThis is not an issue for the same application running with passenger 5.0.7 and apache. I've actually begun moving the apps back to passenger because of this issue. Also binding to a socket seems to slow down the length it takes for this to occur a bit, but that may just be a coincidental observation. \nSimple rack application\nOn one setup I've got Ubuntu 14.04 and a simple geminthebox setup utilizing puma 2.11.2 on rubinius 2.5.2 via rvm. The rails app is proxied via nginx. This is the result when I add puma-worker-killer. \n\nAs you can see on the left part of the graph before restart the memory balloon never corrects.\nrails application\nHowever on a more complex rails application. The worker threshold has to be much lower to catch it quicker and avoid ballooning. Because if it gets too far into swap vSphere doesn't appear to correct the balloon. I haven't dialed this in yet 0.80 is probably too low also this machine was given way more memory than it needs during troubleshooting. \nThis machine is Ubuntu 14.04,  rbenv, ruby 2.2.2, puma 2.11.2, nginx as forward proxy.\n\nNote as of this writing the machine has not ballooned and its been an over an hour, previously it would take ~10-15 minutes to occur.\nancillary observations\nInterestingly enough with puma-worker-killer installed the rubinius instance lets the balloon return to normal eventually. This was not the case previously.\n\n@evanphx If you need more information, just let me know. The geminthebox app can be experimented with as its quasi-production. \nUPDATE After quite a few hours (overnight) memory has begun to balloon again. :(\n. FWIW @hackeron, i switched back to ruby 2.0.0 and the problem resolved. Granted more of an immediate solution, but still a solution none-the-less\n. @larryzhao you can try the gem mentioned earlier in the thread puma worker killer or tune down your max/min threads \nAlso ensure your Ruby is actaually downgraded. I know I switched to using rbenv from rvm and had a .rvmrc file that was still forcing ruby-2.2.2 in production. The puma specific memory leak shouldn't occur under 2.0 so you may have a leak from something else. Good luck!\n. @larryzhao don't forget there is a constant (RUBY_VERSION) that you can access to see what version of Ruby you're running if you want to throw a debug statement in your server view to check or view in rails console\n. I agree that  this thread has devolved into little tangible results for the devs but this is what I know from my situation. My goal isn't to add to the noise but simply present my specific breakdown of the issue. \n- On our vSphere 5 server (no heroku involved), under an dedicated 1 Ubuntu VM (14.04) per 1 rails application ANY of my Rails apps when run under puma and ruby 2.1+ cause the VM to become extremely slow and unresponsive. \n- The vSphere memory manager increases the memory allotment (on the VM) and starts to bloat VM memory slowing everything on the VM to a crawl even ssh'ing into the VM is slow\n- Restarting the VM clears up the vSphere memory bloat for a time but \n- eventually the VM will do the same thing all over again. \n- If I roll back to ruby 2.0.0, the only change I make on the VM, all of the above described issues on the VM resolve.\nThis is even without Puma Worker Killer being used as this was happening before I found out about that gem. I'm fairly confident that these results can be duplicated by just about anyone as my apps are fairly simple rails apps and not doing anything special.  \nTo handle the issue of mod-passenger running ruby 2.2.1+ working fine in the same setup and no memory bloat on the VM. IMO this can probably be attributed to mod-passenger being mostly optimized C code with a little ruby sprinkled into it. So there is probably little correlation to this issue. It could very well exist but whatever mod-passenger is doing fixes it. \nAs I see it, there is definitely an issue with puma under ruby 2.1+ as far as I can tell it involves some sort of conflict between ruby 2.1's new GC settings and pumas worker threads. But that's just one developers educated guess not backed by any science.\nNot sure how to fix it going forward, which admittedly sucks and doesn't help the discussion but I am willing to run whatever tests might help. I still have two VMs running on puma that exhibit this slowdown, one of which I can change the ruby version back to 2.1+ and trigger the error. However I will eventually switch them to passenger, for this and a few other reasons, and become an indifferent observer of puma. \n. I think further posting to this thread is a moot point. This issue IMO is directly related to the ruby 2.1 GC changes as puma, in my experience, doesn't exhibit this issue under 2.0.0. \nHowever since 2.0.0 support officially ends in February and a fix for this issue is unlikely to magically appear in a month and a half, if it were my application I'd explore other options, e.g. passenger, thin, unicorn, or tune puma down to a single thread. \n. For those that were asking my puma server exhibiting this seems to be fine in 2.3.0 now, so AFAIC this is a ruby 2.1/2.2 issue. \n. ",
    "xanview": "Any solution for this? :(\n. @nacengineer Thank you!! - I've downgraded from Ruby 2.2.2 to Ruby 2.0 and memory use dropped significantly and does not appear to leak. Even something like \"Eye\" (https://github.com/kostya/eye) - I saw a 50% decrease in memory and it stopped leaking.\nWith Ruby 2.2.2, \"eye\" memory increased from 4% to 5.5% (of 4GB) within an hour. With Ruby 2.0, it does not budge from 2.0%. It seems Ruby 2.2 uses double the memory + leaks more over time :(\n. @BenV can you provide details on how to switch to jemalloc? - Is this done on the OS level, or on the Ruby level? - Any links?\n. The issue is the leak is happening with all recent versions of Ruby. In fact any Ruby version released in the last 2 years it seems. If this was happening with only older versions it would make more sense to close the issue.\n. I've experienced the same not just with Puma, but with Ruby in general. Going from 2.0 to 2.2.3 doubles memory use :( - I had to revert back to 2.0 on many servers.\n. Any solution for this?\n. +1 any workaround?\n. ",
    "larryzhao": "@nacengineer I also switched back to ruby 2.0.0, but the problem still exists.\n. @nacengineer Thanks a lot for the response.\nYes, I am using puma-worker-killer, but it seems it's not working and I also tried https://github.com/tmikoss/puma-worker-killer-example, it's also not working on my laptop locally.\nThanks a lot for alerting me about that, I am using rvm and my .rvmrc is pointing to 2.2.1 currently. I thought it won't be a problem. I will try it later.\n. @nacengineer Yes, I confirmed that I am using 2.0.0, changed .rvmrc, checked in rails console with the constant, and I uninstalled other ruby installations on the server, but the memory is still there. \npuma-worker-killer still not working, I debugged it and found in pwk, puma cluster is not running... \nAnd I also decreased thread numbers, it takes quite a long time to eats up all memory but still it eats all.\n. @evanphx Yes, I think it works. But I really don't know why it doesn't work with my case. I've tried it with a fresh Rails project, but it runs into the same problem in daemon mode, in non-daemon mode, everything looks fine. \nI am also discussing it with the author of puma-worker-killer in another thread: https://github.com/schneems/puma_worker_killer/issues/15\n. @evanphx It's OK. Current puma works fine in my production deployment. I haven't had a close look into this with current puma yet. I will reopen if I find it's still there.\n. @nateberkopec I've upgraded to 3.10.0\uff0cjust now i try restart puma server and the hang occurs again.\nHere's something from strace the cluster process:\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(15, \"!\", 1)                       = 1\nsched_yield()                           = 0\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5731, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5733, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5735, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5737, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5739, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5741, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nsched_yield()                           = 0\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5743, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5745, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5747, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5749, NULL) = 0\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(15, \"!\", 1)                       = 1\nsched_yield()                           = 0\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5751, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5753, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nfutex(0xd7f0f0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5755, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f12c, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0xd7f128, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nfutex(0xd7f0f4, FUTEX_WAKE_OP_PRIVATE, 1, 1, 0xd7f0f0, {FUTEX_OP_SET, 0, FUTEX_OP_CMP_GT, 1}) = 1\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 1\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5759, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0xd7f0f4, FUTEX_WAIT_PRIVATE, 5761, NULL) = 0\nfutex(0xd7f0c0, FUTEX_WAKE_PRIVATE, 1)  = 0\nwrite(15, \"!\", 1)                       = 1\nwait4(-1, 0x7ffccf316220, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=31443, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(15, \"!\", 1)                       = 1\nwait4(-1,. Here you could see that what child process & master process is doing with strace:\n$ ps -ef | grep puma\ndeployer  6149  6071  0 17:45 pts/0    00:00:00 grep --color=auto puma\ndeployer 18807     1  0 14:20 ?        00:00:05 puma 3.10.0 (tcp://0.0.0.0:2801)\ndeployer 28828 18807 54 16:12 ?        00:50:08 puma: cluster worker 4: 18807\ndeployer 29601 18807 71 16:21 ?        00:59:45 puma: cluster worker 3: 18807\ndeployer 30936 18807 80 16:38 ?        00:53:12 puma: cluster worker 6: 18807\n$ sudo strace -p 30936\nProcess 30936 attached\nfutex(0x1c1081c, FUTEX_WAIT_PRIVATE, 15, NULL^CProcess 30936 detached\n <detached ...>\n$ sudo strace -p 29601\nProcess 29601 attached\nfutex(0x1c1081c, FUTEX_WAIT_PRIVATE, 13, NULL^CProcess 29601 detached\n <detached ...>\n$ sudo strace -p 28828\nProcess 28828 attached\nfutex(0x1c1081c, FUTEX_WAIT_PRIVATE, 29, NULL^CProcess 28828 detached\n <detached ...>\n$ sudo strace -p 18807\nProcess 18807 attached\nwait4(-1, 0x7ffeb64075e0, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=18807, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(16, \"!\", 1)                       = 1\nwait4(-1, 0x7ffeb64075e0, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=18807, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0x1c100f4, FUTEX_WAIT_PRIVATE, 489, NULL) = 0\nfutex(0x1c100c0, FUTEX_WAKE_PRIVATE, 1) = 0\nwrite(16, \"!\", 1)                       = 1\nwait4(-1, 0x7ffeb64075e0, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=18807, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0x1c100f4, FUTEX_WAIT_PRIVATE, 491, NULL) = 0\nfutex(0x1c100c0, FUTEX_WAKE_PRIVATE, 1) = 0\nwrite(16, \"!\", 1)                       = 1\nwait4(-1, 0x7ffeb64075e0, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=18807, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0x1c100c0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x1c100f0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x1c100f4, FUTEX_WAIT_PRIVATE, 493, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0x1c100c0, FUTEX_WAKE_PRIVATE, 1) = 0\nwrite(16, \"!\", 1)                       = 1\nwait4(-1, 0x7ffeb64075e0, 0, NULL)      = ? ERESTARTSYS (To be restarted if SA_RESTART is set)\n--- SIGVTALRM {si_signo=SIGVTALRM, si_code=SI_TKILL, si_pid=18807, si_uid=1001} ---\nrt_sigreturn()                          = -1 EINTR (Interrupted system call)\nwrite(6, \"!\", 1)                        = 1\nfutex(0x1c100c0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x1c100f0, FUTEX_WAKE_PRIVATE, 1) = 1\nfutex(0x1c100f4, FUTEX_WAIT_PRIVATE, 495, NULL) = -1 EAGAIN (Resource temporarily unavailable)\nfutex(0x1c100c0, FUTEX_WAKE_PRIVATE, 1) = 0\nwrite(16, \"!\", 1)                       = 1\nwait4(-1, ^CProcess 18807 detached\n <detached ...>\nRuby: 2.3.4\nPuma: 3.10.0\nRails: 5.0.5. @poc7667 no, we are still using puma. I didn't find out the exact problem or have a solution. \nBut it seems that it's related to resource cleaning up. Like, if you have a slow query which causing a db connection won't end up, it may occur. I think...\nSo after some tuning up, it's getting better.. ",
    "BenV": "We were experiencing similar issues on CentOS 6 with Ruby 2.1.2, and switching to jemalloc for our allocator seems to have eliminated the problem. Fingers crossed!\n. ",
    "foi": "@hackeron compile with parameter  --with-jemalloc\nfor rbenv http://groguelon.fr/post/106221222318/how-to-install-ruby-2-2-0-with-jemalloc-support\nfor rvm http://www.reddit.com/r/ruby/comments/3221x2/installing_using_jemalloc/\n. evanphx, I just checked with the latest versions (windows 8.1 update 1 x64, ruby-2.0.0-p481-x86, puma-2.9.0, redmine-2.5.2, Google Chrome 35.0.1916.153) and have not noticed any problems. Everything works very well. Thanks for update!\n. Hello! I have same trouble. I tried various rubies, pumas (2.7.1, 2.8.2, latest). We are using redmine app with puma with https unsigned certificate. When i trying to open redmine app from home (redmine server in long distance from me) it's hangs (in home i \"slow client\", poor quality 4G with a lot of packetloss and latency), not every time, but 25% of https initial requests hangs puma, puma 'https' get unacessible even from localhost. Recently i find Rack::Timeout gem and scenario for cope with problem https://devcenter.heroku.com/articles/request-timeout. I hope it will help me. Otherwise, i will be here again )\nWe switched to nginx as reverse proxy instead of only puma. Set puma only http, nginx set https. Quite stable.\n. ",
    "ko1": "@evanphx Thank you for reporting it.\nBut it is too many messages. Which comment should I check?\nDo you have reproducible case?\nDoes it have a cap per threads? It means that if you made n threads, then limit to n * X MB? Or unlimited?\nruby\ni = 0\nloop{\n  Thread.new(i+=1){|ti|\n  }\n  p [i, Thread.list.group_by{|t| t.status}.map{|s, ts| [s, ts.size]}] if (i%10_000) == 0\n}\nThis program consumes system memory. Is it similar problem?\nThanks,\nKoichi\n. @evanphx Thank you.\nI try puma with the following config.ru and access with ab (ab -n 10000000 -c 100 http://localhost:9292/).\n- Ubuntu 64bit\n- ruby 2.3.0dev (2015-08-16 trunk 51564) [x86_64-linux]\n``` ruby\nThread.new{\n  loop{\n    puts File.read('/proc/self/statm')\n    sleep 1\n  }\n}\nclass App\n  def self.call(env)\n    [200, {}, []]\n  end\nend\nrun App\n```\nAnd I can't observe memory growth in 1000 seconds.\nhttp://www.atdot.net/fp_store/f.04gatn/file.copipa-temp-image.png\nAny other conditions? Or 1000 seconds == 15min is short?\n. > I don't believe there is an idempotent way to check GC status in CRuby.\nCould you try https://github.com/ko1/gc_tracer ?\n. @joanblake \nThank you for your trying.\nLet us clarify: Your simple application grows memory usage permanently (reproduce this thread's issue)?  If so, I will debug with it.\n\nold_objects, heap_marked_slots\n\nIf these counters do not stop increase, it will be a object leak.\n\ntotal_allocated_objects, total_freed_objects \n\nThese counters are \"total allocated/freed objects counts from the beginning of the interpreter\". So that increasing permanently is normal. total_allocated_objects - total_freed_objects = current living objects.\n. ",
    "TheKidCoder": "I'll just drop my two cents here as I am still having major leak issues. The app is stable for a good period of time. It takes about 6-8 hours of requests before the app gets from 1gig of ram (on boot) to over 4gigs. This is puma latest & 2.2.2.\nI know I'm not providing any new information. Just wanted to add that it takes a few hours for my app to start building up ram utilization.\n. I was having a similar issue, even with the expect daemon stanza. I simply removed daemonizing from Puma and the upstart script and no longer have any issues... which leads to me to ask... are there any reasons to daemonize while using upstart?\nP.S. Probably worth mentioning that I am running on Jruby, so I suspect that could be the cause of my daemon issues with upstart.\n. I completely agree that Puma on Jruby should have SSL working, however, I wonder why you can't just terminate SSL at Nginx and proxy to puma?\n. Gonna close this in favor of https://github.com/jruby/jruby/issues/3673\n. What is the URL of the preview when you open it in chrome? This might be a interface binding issue.\n. This looks like an issue with capistrano / your env config. \nWhen puma gets the signal to restart, it is still using the old release path. releases/20160202205923/Gemfile\nYour new path is: /releases/20160204180200\nPuma is restarting with the old app code in its working directory.\n. If you have an initializer that was created by rails g mongoid:config I would imagine you can move whatever is within that initializer into the on_worker_boot block. \nIf you don't have an initializer, it looks like Mongoid.load!(\"path/to/your/mongoid.yml\", :production) inside the on_worker_boot block will work.\n. Likely you accidentally started a server in a terminal and killed the terminal without killing the server. You can do something like this: kill $(cat /mnt/zapstore.com/shopifyapp/zap/tmp/pids/server.pid) Which will kill the server process.. Looks like you're up and running. If you're using this on a remote server, however, you'll need to bind to the right eth interface. I would consider reading a tutorial on this: https://www.digitalocean.com/community/tutorials/deploying-a-rails-app-on-ubuntu-14-04-with-capistrano-nginx-and-puma. ",
    "dirkdk": "Our installs on Heroku with Ruby 2.2.2, Rails 4.2.2 and Puma 2.14 show a very slow memory leak. In 16 hours we can go from 320 to 390mb given 2 workers with max 5 threads. Our app is simple and we don't muck around with memory, so my first guess is that it is not our code. Will do further research of course\n. Well we ran out of memory on a standard 1x dyno, that wasn't great. Yes 320 is 30 minutes after a fresh reboot, right after the reboot it was 260mb.\nMy guess is that the standard 2 workers with 5 threads is too much for a 512 mb dyno, and that any increase in memory usage (e.g. ActiveRecord statement caching) will hit the ceiling pretty soon\n. ",
    "rhymes": "Not sure if it helps but I don't use multiple threads in our production environment (Rails 4.2.4, Puma 2.14 with 2 workers on 2 Heroku standard 2x dynos) and I haven't noticed any leakages. I use only 1 thread and 2 workers. \n. @ostinelli In the meantime I solved using rack-timeout and this gem https://github.com/keyme/rack-timeout-puma/\n. @nateberkopec @ticky unfortunately it seems the issue is still present even with Ruby 2.4.3\nI just encountered it with Ruby 2.4.3, macOS High Sierra 10.13.2 and puma 3.11.0:\n\n[10996] Puma starting in cluster mode...\n[10996] * Version 3.11.0 (ruby 2.4.3-p205), codename: Love Song\n[10996] * Min threads: 1, max threads: 1\n[10996] * Environment: development\n[10996] * Process workers: 2\n[10996] * Preloading application\n[10996] * Listening on tcp://0.0.0.0:5000\n[10996] Use Ctrl-C to stop\n[10996] - Worker 0 (pid: 11016) booted, phase: 0\n[10996] - Worker 1 (pid: 11017) booted, phase: 0\nI, [2017-12-15T19:27:32.407998 #11016]  INFO -- : REQUEST: method=GET path=/api/v1/... user_agent=curl/7.57.0 time=2017-12-15T18:27:32Z ip=127.0.0.1 host=localhost params={}\nobjc[11016]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called.\nobjc[11016]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.\n[10996] - Worker 0 (pid: 11060) booted, phase: 0\n\nI've isolated it, in my code, to these three lines of code which crash the process with our without preloading:\nruby\nFaraday.new(url: url) do |faraday|\n  faraday.adapter :patron\nend\n@ticky's patch did the trick\n. @webark yeah the issue is because of the usage of forking in OSX and how it changed in High Sierra, try @ticky's patch here: https://github.com/puma/puma/issues/1421#issue-260744683. @ticky amazing! thanks for the help and support!. ",
    "tomas": "The following sinatra example leaks on Puma 2.15.3 both under Ruby 2.1.0 and 2.2.0, on OSX 10.7.5 (yeah I know).\n```\nconfig.ru\nrequire 'bundler/setup'\nrequire 'sinatra'\nmodule Test\nclass App < Sinatra::Base\nget '/' do\n  \"Hello request: #{Time.now}\"\nend\n\nend\nend\nmap '/' do\n  run Test::App\nend\n```\nRunning with default settings and then doing: \nwhile sleep 0.1; do curl http://localhost:9292; echo; done\nMakes puma increase its mem usage roughly about 100KB per second. \nI hope this helps.\n. Nope, unicorn's mem usage increases a bit after boot but then stops. The point is that it's not some other library's fault -- from here it looks like it's definitely puma.\n. @evanphx I ran the test because I noticed that an app running over puma on one of my servers -- Debian 7, 4.1.5 x64 kernel -- had spiked to 300MB, when it normally shouldn't take more than 40-50MB. So I don't really think this is an OSX-specific thing.\nMy \"measuring tool\" was totally unscientific, by the way:\nwhile sleep 1; do ps aux | grep puma | grep -v grep; echo \"--\"; done\nThe reason I used curl every 0.1 secs for testing is because that is a much more realistic scenario of what happens with this app -- just a few hundred users, not a zillion requests per second. The unicorn memory level I analyzed is from the worker process, by the way. Running the same sinatra example with a single unicorn worker.\nI ended up switching (back to) unicorn for that app and I can see that the memory usage remains stable now. It's a pity though because I really like puma and would love to keep on using it.\n. @sebbarg Which other gems did you update? It'd help to know as my stack doesn't include Rails.\n@evanphx The same sleep / ps aux trick I did with puma. Besides htop or rtop, that's what I usually use on my servers anyway. :)\n. Good to know @evanphx, thanks.\n.  @Martin288 Did you manage to solve the issue one way or another? It's 2018 and I'm still running into this... :). @x-yuri I ended up solving the issue by hardcoding the restart_command path on my puma.rb config file. This way the restart task doesn't need to rely on Bundler for figuring out the location of the original command (which no longer exists after a few deploys).\n```\nroot = '/path/to/your/app'\nenvironment ENV['RACK_ENV'] || 'production'\nrackup root + '/current/config.ru'\npidfile root + '/shared/pids/puma.pid'\nbind 'unix://' + root + '/shared/sockets/puma.sock'\nrestart_command root + '/current/bin/puma'\nworkers 128\nprune_bundler\n```\n. Ha! The number was actually 12. I just thought 128 would look cooler. ;). ",
    "dsandber": "Eh, kind of hard to tease out what's happening there.  Could you provide a simpler test case? ;-)\nSeriously though, does it not leak with another web server like unicorn?\n. ",
    "sebbarg": "Hi there,\nJust a quick input - I have been having the same memory problems on Heroku running different combinations of workers and threads. \nMy app peaks at around 140 RPM, so not really busy. Within 24 hrs, my 512 MB dynos went from around 280 MB to game-over. \nUsing Puma Worker Killer solved the immediate problem. I did A LOT of tests to provoke the same problem locally, but I could not. \nI also upgraded Ruby from 2.0 to 2.2.3., but that only made things worse (I didn't try tweaking the GC params though).\nThen the other day I removed all the version restraints from my Gemfile, did a bundle update, closed my eyes and pushed to Heroku. And whaddayaknow - my app grew from around 280 MB to around 370 MB where it's been sitting for 24hrs! Yay!\nSo basically I upgraded from rails 4.0.13 to 4.2.5 (and of course a few other gems as well) and - apparently - that did the trick, memory usage seems stable.\n. Hi - just a brief follow up to my previous post.\nI wanted to go from ruby 2.0.0 to the latest stable 2.2.3 version. Still being on the latest version of all gems.\nOn Heroku I switch from running two 1x dynos (512 RAM limit each) to running one 2x dyno (1024 RAM limit and more cpu time).\nA baseline test with Ruby 2.0.0 running 4 workers with 6 threads each, flattened out the avg rss curve around 560 MB (after almost 8 hours).\nUpgrading to Ruby 2.2.3, but now running 2 workers instead of 4, again flattened avg rss around 560 MB. \nSo, I had to HALF the number of workers to get the same memory usage! But it didn't grow beyond those 560 MB and my response time was somewhere around 20% faster now (excellent!)\nThe bottom line here, is that I went from an apparent memory leak using Puma, to a memory stable and a faster site just by upgrading the various gems and tweaking worker and thread count. Running the same version of Puma all along. \n. @tomas - bundle list says:\nGems included by the bundle:\n- acl9 (2.1.1)\n- actionmailer (4.2.5)\n- actionpack (4.2.5)\n- actionview (4.2.5)\n- activejob (4.2.5)\n- activemodel (4.2.5)\n- activerecord (4.2.5)\n- activesupport (4.2.5)\n- annotate (2.6.10)\n- arel (6.0.3)\n- awesome_print (1.6.1)\n- binding_of_caller (0.7.2)\n- builder (3.2.2)\n- bundler (1.10.6)\n- callsite (0.0.11)\n- coffee-rails (4.1.0)\n- coffee-script (2.4.1)\n- coffee-script-source (1.10.0)\n- debug_inspector (0.0.2)\n- delayed_job (4.1.1)\n- delayed_job_active_record (4.1.0)\n- delayed_task (0.2.0)\n- erubis (2.7.0)\n- exception_notification (4.1.1)\n- execjs (2.6.0)\n- factory_girl (4.5.0)\n- factory_girl_rails (4.5.0)\n- git-version-bump (0.15.1)\n- globalid (0.3.6)\n- i18n (0.7.0)\n- jquery-rails (4.0.5)\n- json (1.8.3)\n- loofah (2.0.3)\n- mail (2.6.3)\n- meta_request (0.3.4)\n- mime-types (2.99)\n- mini_portile2 (2.0.0)\n- minitest (5.8.3)\n- newrelic_rpm (3.14.0.305)\n- nokogiri (1.6.7)\n- pg (0.18.4)\n- pony (1.11)\n- puma (2.15.3)\n- rack (1.6.4)\n- rack-contrib (1.4.0)\n- rack-test (0.6.3)\n- rack-timeout (0.3.2)\n- rails (4.2.5)\n- rails-deprecated_sanitizer (1.0.3)\n- rails-dom-testing (1.0.7)\n- rails-html-sanitizer (1.0.2)\n- rails_12factor (0.0.3)\n- rails_serve_static_assets (0.0.4)\n- rails_stdout_logging (0.0.4)\n- railties (4.2.5)\n- rake (10.4.2)\n- responders (2.1.0)\n- sass (3.4.19)\n- sass-rails (5.0.4)\n- sprockets (3.4.1)\n- sprockets-rails (2.3.3)\n- thor (0.19.1)\n- thread_safe (0.3.5)\n- tilt (2.0.1)\n- tzinfo (1.2.2)\n- uglifier (2.7.2)\n- web-console (2.2.1)\n. ",
    "Ricardonacif": "Did anyone see any specific improvement when upgrading to ruby 2.2.3?\n. Thanks @bbozo. Looks like it is an issue with GC on Ruby 2.0+ https://evilmartians.com/chronicles/ruby-2_2-oom \n. @mkilling what version of Rack-timeout are you using?\n. ",
    "bbozo": "@Ricardonacif no :-/ we rolled back ruby 2.2.3 to 2.0.0 (Heroku), this fixed the problem for us.\nIn our case, the server handling the API requests is working stable on 2.2.3 while the server handling web requests would go to pieces very very quickly even though the API server is under literally 10x the load on same kind of Heroku VM and suffering no memory problems at all.\nAPI server runs puma on 2.2.3 ruby with 3 processes with 4 threads,\nWeb server ran puma on 2.2.3 ruby with 2 processes with 4 threads - pretty significant memory leak - double footprint in 8-10h, 4 restarts a day, now working happily under ruby 2.0.0 with no memory issues at all\n. WOW :D Amazing news :D Thank you :D\n. Threading in C ruby is considered to be a bit of a corner case, and puma + heroku were possibly the first to introduce it into production on a large scale (as a brilliant way of dealing with the IO/memory bound in C ruby apps <3), so I would expect hickups in hindsight,\nso:\n1. no, it's probably not a problem IN Puma\n2. but yes, it's probably a problem (practically) only WITH Puma :D\nand the bug from https://bugs.ruby-lang.org/issues/11692#note-8 is possibly not the only one as @schneems noted, so it'll be great to see how the server behaves once ruby 2.3.5 has been released on Heroku\nAs for reproducibility, it's difficult in this case because:\n1. you need to run a certain version of C ruby\n2. you need to run Puma in threaded mode\n3. you need to use a lib or piece of code which could do a quick double native SO in a thread (this would explain @sebbarg's \"fix\")\n4. point 3 needs to happen often enough so PumaWorkerKiller and others can't make the problem \"go away\"\nDoes anybody know of a way to test if the native GC is turned off?\n. I'd like to give this a go if it's not overly complicated, or plan B switch to Trinidad and cross fingers and I hate Tomcat with equal passion with which I love puma <3 Not being able to upload from Android is a kind of a big issue for us, argh\n@evanphx can you maybe point me in a good direction how to tackle this?\n. Indeed :) https://github.com/defunkt/unicorn/commit/81026ea66279695206ea53287427c05281662572#diff-35c8f85f334e5bc83c3623ac6e9f4f94\nI'll take a look, maybe we can beg for help some of the guys from unicorn too ^_^\n. Hi :) for the moment, it seems this is the only solution: http://atnan.com/blog/2008/08/08/transfer-encoding-chunked-or-chunky-http/\nWe \"have\" a workaround for our use case with a free tier Amazon EC2 VM to fix the requests - it's not implemented yet mind you, so I can not guarantee 100%, our plan is to make uploads go directly to S3 in the future - which is really the better option for us anyway\nI'm interested in playing with this, I just need to get some slack of my work to be able to tackle it and learn some ragel :)\n. The thing with Puma is that it's the only mainstream project that encourages the use of threading in MRI Ruby (well, anyway, Heroku encourages).\nThis is why we sometimes see statements from people working on Puma about how people think that Puma has various kinds of issues, while the problem is elsewhere, and it is, and it affects only Puma :P\n\"We\" have discovered and fixed in the past some very freaky and nasty Ruby GC issues on heavy duty use of threads in Ruby MRI with some freaky corner cases (remember http://blog.skylight.io/hunting-for-leaks-in-ruby/ & #342) and who is to say this is not the last of such freaky issues that people attribute to Puma but are actually in Ruby?\nTry disabling threading for a while, see how it goes, and let us know, maybe the rabbit lies there\n. ",
    "mkilling": "Has anyone checked whether this can be fixed by using Ruby 2.3 yet?\n. I can confirm that the issue is still present with Ruby 2.3. This is our memory usage in production (2 professional-2x dynos, 1 worker each, 5 threads each):\n\nUsing unicorn, we could run 2 workers per dyno without running into memory issues\n. Just to throw another data point in here: It seems that for me, the issue grows worse the more Exceptions occur in our app server. After enabling Rack::Timeout, memory consumption was growing much faster than before. Still not sure if this is a Puma issue though, it is however very likely related to threading.\n. @Ricardonacif 0.3.2. Could there be anything wrong with it?\n. ",
    "bmjoan": "@mkilling  Did tested with minimal rack application and it unfortunately still leaks. Even this hopeful bugfix has no effect.\n. @ko1 I'm not quite sure what each of gc_tracer's log entries exactly means, but for simplest puma/rack application it shows constantly increasing old_objects, heap_marked_slots, total_allocated_objects, total_freed_objects (but always lesser then allocated), remembered_wb_unprotected_objects. For the following simplest app:\n``` ruby\nrequire 'rack/gc_tracer'\nuse Rack::GCTracerMiddleware, view_page_path: '/gc_tracer', filename: 'rackmini.log'\nrun proc {|env|  [200, {}, ['Hello World']] }\n```\nRun with 2 puma workers and default 0-16 threads pool:\nbundle exec puma -e production -t 0:16 -w 2\nBtw. in the same bundler environment the same rackup app but run with thin in threaded mode with 20 threads and memory usage stops growing after a few secs warmup and never exceeds the limit even after several days. Same load, tested with wrk tool. Not saying this prooves anything, but at least it should be taken into a consideration.\n. ",
    "bufordtaylor": "Like most of you, I experienced mem issues with puma and ruby 2.3.x. And I found the root cause to be threading within puma.\nWhat I saw:\n\nI needed restarts roughly every hour. I set Puma MAX_THREADS to 1 in order to fix it. Now I am running 4 puma workers on a standard 2x heroku dyno and have no memory issues..\n\n. ",
    "SuperMasterBlasterLaser": "I want to add to this thread how I encountered this problem. Maybe this will help you to reproduce it.\nMy puma server was always silently crashing at certain amount of time, Mostly 6 a.m. I needed to restart it manually. Every time when I look at my VPS dashboard where CPU time usage has shown, it shows me that it constantly been rising and then falls down at certain level(at time where Puma is crashed). My VPS hoster shows only CPU time usage.\nMy Rails App consists of these gems: Puma as Server, Mongoid as ODM, Grape for API and RailsAdmin for Dashboard.\nThe main action happens between Grape and Mongoid. When you define API via grape, you will write request names and params names as symbols. My Grape API had about 50 methods which most of them are POST methods. Each of these methods has also from 3 to 9 params. I have read somewhere that 2.1.* and 2.2.* versions of ruby has problems on symbols in garbage collector. \nEach Grape method will make Mongoid calls to fetch data from database. In some of these methods I have used multiple collections in order to fetch correct data(Think about trying to make something like JOIN Queries in MongoDB).\nWhen I started to make requests to these methods, CPU time usage starts to rise upwards constantly. Even when I stopped making requests, it still keeps at high position. Each time when I call HTTP requests to Grape methods, my server will allocate new chunk of memory instead of releasing old ones.\nI think the way to reproduce this problem is to create Rails App with Puma and Grape. Add a lot of POST methods with a lot of params. Each method will also make database queries. Then start to spam app with requests after a certain period of time.\n. @kriansa  @sheharyarn  This crash still happens on ruby 2.2.2\nThis is what happes to my vps:\n\nIt crashes at specific amount of time.\nI needed to restart them manually\n. ",
    "mrhead": "I see that I am not the only one who is stuck with 2.0.0 on Heroku. With 2.2 or 2.3 we need much more memory (almost twice as much as with 2.0). \nI am not saying that it has anything to do with puma. I just found this thread and saw that other people have similar issues. I will post here if I find any solution.\n. ",
    "prashantham": "I have 2 4GB app servers running puma. Both of which have the same issue. Memory keeps on increasing steadily. I have restart to restart the server after it reaches 90% usually in around 6hrs. I have another 8GB system which uses the same code base. This is our job server hence does not run puma. Memory usage is around 25%. Seems like some issue with puma. \nI will check if worker/thread changes might fix it.\n. Setting the workers to 1 reduced memory consumption. Steady at around 25%.\n. I have similar issue on a server which does not have any background workers. Using monit is the only solution?\n. ",
    "jrafanie": "I haven't seen MALLOC_ARENA_MAX mentioned before but we have seen similar memory issues with ruby + threads on glibc 2.10+.\nThe generational GC of ruby 2.1 seems to make these thread pools consuming memory worse.\nIn many environments, we have set this ENV variable to 2 or 1 and this drops memory growth considerably.\nGood luck!\n. Note:  RE: MALLOC_ARENA_MAX, my symptoms were: ruby 2.2 using a highly threaded app with a steadily increasing RES memory while the GC.stat[:heap_available_slots] was NOT growing at a similar rate, indicating something in the ruby process other than the ruby heap was consuming memory.\n. @nateberkopec No problem, I'm glad to share what I've found and I agree, a setting of 1 or at most 2 seems reasonable for our applications.\nWhile the MALLOC_ARENA_MAX setting (or the lack of a configured values) with glibc 2.10+ may account for some of the problems people are reporting here, there may still other memory issues specific to puma/ruby/etc.  If you're using glibc 2.10+, though, you must configure this value to 1 or 2 before you even try to chase down a puma issue.  The growth in memory with an unconfigured MALLOC_ARENA_MAX clouds all other diagnostics you can provide.\nI don't want to hijack this thread so I'll try not to talk more about this setting... but there were some good basic explanations, in depth information, and comical opinionated developer explanations I found when this was a problem I was facing.\n. @jjb I don't think activerecord's reaper does what you are expecting.  It reaps connections reserved for threads back into the connection pool for general use.  Unless you're doing absolutely nothing, I've found that the database.yml pool size is the number of db connections you'll see in pool.  ActiveRecord eagerly passes out connections until the pool size is reached and keeps those connections alive even if they aren't reserved by threads for a long time.  \nI would not expect spinning down the number of puma threads to decrease the number of connections in your pool unless you also change the pool size in database.yml.  I also don't know if you can dynamically lower the size of the connection pool and see those connections be disconnected.\n. @jjb if it's helpful, you could take something like this for Rails 5 to log the checkout/checkin of connections from the pool.  I believe one or two of the attributes of the pool are different in Rails 4, maybe num_waiting_in_queue, I don't remember.  If your logger logs Process id and thread id, you can easily see when the pool grows, connections are checked out and checked back in.  I don't know if the repear has callbacks you can hook for logging.  I don't know that it's puma at all.  \nOne pain point is that any rails rack request to puma needs to checkout a connection as part of the query cache middleware and because puma uses threads, you have thread contention on the pool so you have to make sure you don't have threads constantly in waiting_in_queue as per my link above for logging the pool information.  To do this, you have to have pool size greater than the number of puma threads handling rack requests.  This was a big pain in development mode where my rails server is processing requests for assets.  \ud83d\ude22 \nGood luck!\n. Are you saying the number of connections to the database exceeds the connection pool size?\nCould you share what information you're finding on the system during a \"normal\" spun up state vs spun down? Number of threads in puma, number of connections from that process as reported by the database, connection pool size, number reserved, number waiting, etc. \n. @walidvb Can you try 3.5.0, released today?\n9b1de100044a45a810927c1da0ca3a4dea413890 is supposed to fix #782 and is included in 3.5.0\n. @walidvb 3.5.2 was shipped yesterday with https://github.com/puma/puma/commit/84fdc1bb14b72856a3dac40db1a43a560435706b, I'm not sure if this helps.\nIf you want to bind to different address, maybe bind (-b) is what you want?\n. Did you try binding to 0.0.0.0 via the rails s -b option? I think that's right. rails s --help to double check. It might bind to 127.0.0.1 by default. \n. Note, 0.0.0.0 is not very secure and will raise flags in government security audits. \n. A firewall could also be dropping requests on the rails server machine. \n. Sorry, I can't tell if MALLOC_ARENA_MAX was configured here.  It's worthwhile checking even though it looks like puma has only 16 threads.  Here's more info. \n. @nateberkopec very interesting summary of your findings.  Thanks for digging into it.\nCan you confirm if you're using the defaults for min_threads (0), max_threads(16), and workers (0) (clustered mode disabled)?\ntldr; let's clarify fork or not...because....\nNote, I ask because clustered mode uses fork and ruby's GC 2.1+ fork is not very fork friendly since shared pages are slowly copied to private pages.  I've never really documented what we've found but basically any writes to the shared pages, even freeing a single memory location on the page, causes a copy on write of the whole page.  We've found that shared memory as reported by /proc//smaps is mostly shared (60-80%) just after fork but gradually drops to (10-30%) shared within several minutes.\n\nTuning any number of MALLOC env variables, such MALLOC_ARENA_MAX or even MALLOC_MMAP_THRESHOLD etc has no effect.\n\nAre you using glibc 2.10+?  How many threads are in your puma process?\nIn my experience, if you're using more than a  and MALLOC_ARENA_MAX isn't set, resident/virtual memory will rise significantly with NO increase in the heap_available_slots as reported by GC.stat.\n. Note, this and the other puma memory issues might look like some of us are complaining but honestly, this is really interesting... I'm sure with more people :eyes: who can provide their details/symptoms and expertise, we'll figure this out.  \ud83d\udc4f \n@nateberkopec in your comparison, did you use thin in threaded mode or use the default of \"non-threaded\"?\nhttps://github.com/macournoyer/thin/blob/a7d1174f47a4491a15b505407c0501cdc8d8d12c/lib/thin/backends/base.rb#L55\n. > I don't believe the glibc version matters because this behavior can be shown with glibc malloc or jemalloc.\nGood point @nateberkopec.  This is clearly different than other reported issues where the arena max setting mattered.\n\nIf I run GC.start every 5 seconds, causing a major GC, heap_live_slots does not increase under Puma. So there are no managed object leaks. Also, when running major GC every 5 seconds, I can almost completely negate all memory growth.\n\nSo, I've had this gut feeling that ruby's GC is allocation based and how this could cause us to not do a full GC for a long time if we time it \"wrong\".   If you do lots of allocations, grow the heap size and continue to allocate objects, everything is fine because the full GCs will triggered on allocations and will happen somewhat regularly.  But, if you have ups and downs where full GCs don't occur for a long time during the downs, you might accumulate lots of live slots that could be GC'd but aren't.\n. @mwpastore Sorry, I didn't visit your links... Did you share your puma config (specifically the min and max value for the threads)?  Also, what is the rails connection pool configuration?  It's very interesting.. @mwpastore is it easy to run the benchmark against a different adapter such as PostgreSQL?  I'm curious if you'd see the exact same behavior... I guess I would assume so...\nI don't have any answers but am curious to know what's happening here.. ",
    "odiszapc": "Right now I'm switching between Ruby 2.2.2 and 2.3.0 trying to detect difference in how puma consumes memory.\nIs it ok to monitor memory consumption with htop RES field?\n. I don't see memory leaks on 2.3.0 versus 2.2.2.\nI'm using htop RES column to compare results.\nruby was installed using\nrbenv install 2.3.0\n. ",
    "achiinto": "I have three applications, one extremely simple, are all having this memory issues on 2.2.2. I have upgraded one of it to 2.3.0 and the problem solved. Thanks @nacengineer and @odiszapc \n. ",
    "lylex": "I use ruby 2.3.0p0 (2015-12-25 revision 53290) [x86_64-linux], this issue reproduced.\nOur server is about 32GB mem, and puma consumes about 2GB.\nI think it seems can be reproduced this way: in a request, generate a huge array, repeat again and again. Then the memory will constringe to a high memory consumption level.\n. ",
    "lserman": "Ah yes, I forgot about this issue - I just needed to create the sockets directory and everything deployed, but for some reason it did include my CSS/JS files. The application.css and application.js files (with the cache digest on the end of each) do not actually exist.\n. I am used to Passenger which sets up Apache to do the static file serving apparently, so that's why that wasn't working. Thanks for taking a look.\n. ",
    "apantsiop": "It does the same on Puma 2.3.2 and jRuby 1.7.4\n. Updated to puma 2.5.1 and it works fine.\n. ",
    "chenghung": "jruby 1.7.4\npuma 2.5.1\npuma worked perfectly\nmaybe you could update puma to latest version and try again\n. I am seeking a fix or solution\nI expect that puma will be restarted but sometimes it is killed\n. the same issue when using jruby 1.7.4 and puma 2.7.1\n. the same issue with\npuma 2.8.2\njruby 1.7.4\npuma:restart always kill puma process\nI have to startup puma manually after deploy\n. the same issue when phase-restart puma (cluster-mode, worker = 3)\nruby 2.0.0-head\npuma 2.8.2\n. @seuros  Yes, I am using capistrano-2.15.5\n. @dkubb Yes, I specify state_path in puma config\n. puma 2.8.1 seems solve this problem\n. it's my problem\n. the same problem on puma-2.15.3 \nphased-restart do not reload new code after deploy\nI need to use restart instead of phased-restart.\n. ",
    "stevenharman": "I am seeing something similar on the latest Puma (2.5.1) on Ruby 2.0.0-p247. Requests seem to hang for a VERY long time. If I then stop the request in the browser I see that the markup and some images have loaded, but non of my Javascript nor CSS assets load. Requesting an asset directly seems to hang...\nEdit:\nAlso, when stopping the request via the browser, Puma spills the following to the logs\n127.0.0.1 - - [15/Aug/2013 16:47:47] \"GET / HTTP/1.1\" 200 - 2.2661\n. ",
    "benschrauwen": "I have a similar problem, but noticed that when I load the page in the Incognito mode of Chrome, it loads just fine. In a normal browser tab it just hangs.\nI am on 2.5.1\n. ",
    "jzimmek": "Same problem here. 2.5.1 fails on 1.9.3p194. Same Key/Cert works totally fine on 1.6.3\n. ",
    "wesbos": "Same problem here, any update? \n. Should note that if I delete all cookies, or open an igcognito tab as @benschrauwen  has mentioned, the first request works, but anything thereafter (js, css, images, partials) do not work. \n. Very fast without SSL.\nNo error reported, pretty sure I did see an inital request in the log\n. ",
    "thukim": "Yes, it keeps loading like forever, without any error printed out. However, if you reload the page, you will see the Broken pipe error as described above.\n. ",
    "toolmantim": "I've just been hit by this on a bigger app after upgrading to Puma 2. I put together the smallest possible example that demonstrates the problem: https://gist.github.com/toolmantim/88e2479b95af129cbb3c (using ruby 2.0.0p247 and puma 2.6.0)\nIf you hit https://localhost:7000/ with phantomjs, safari, chrome or firefox it just stalls, the rack app never even receives the request. If you do the same request with curl it works fine. And if you close the browser whilst it's hanging, Puma finally calls the rack app (but it's a bit too late at that point).\nIf you attach with gdb it shows the 4 puma threads are sleeping. Possibly it's waiting for a select call.\nI tried to compare curl & browser requests, tracing them through with gdb & byebug, but couldn't track it down.\nCan anyone else confirm that this simple example stalls for them too?\n. ",
    "ahacking": "I'm getting the same issue, on puma 2.6.0 with ruby 1.9.3p194\nI am unsure what caused it, it may have been caused by suspending my development laptop but I can't be sure.\n. ",
    "shipstar": "I just had the same thing happen in production on Heroku on 2.6.0 with Ruby 2.0.0.\nThere were 8 fairly simple requests that timed out after 30s, then the above error repeated over and over.\nLoad was not particularly high -- just one client issuing a handful of AJAX requests.\nI'm looking into it now -- will update if I find anything interesting.\n. ",
    "smidwap": "+1 Also getting this issue on Heroku with 2.6.0\n. ",
    "siong1987": "+1 also getting this issue on Heroku.\n. ",
    "goodwill": "Got issue today as well with heroku + puma 2.6.0 + ruby 2.0.0p353 (2013-11-22 revision 43784) [x86_64-linux]\n. Its an error that wont appear right after server start, so without putting this to production I don't think I could find out if it has problem. Would you have idea on when would a stable release with this patch could be included?\nOn 3 Dec, 2013, at 11:53 pm, Phillip Calvin notifications@github.com wrote:\n\nI'm seeing this error with Puma 2.6.0 on Heroku, running ruby 2.0.0p353 (2013-11-22 revision 43784) [x86_64-linux].\nHowever, the error appears to be fixed on master, I think as a result of 74ea16d.\n@goodwill, @smidwap, @siong1987, and @shipstar, can you verify things are working if you change the dependency in your Gemfile to the following?\ngem 'puma', :git => 'https://github.com/puma/puma.git'\nSince the build is failing I don't recommend running this in production, but it would be good to know the bug is gone.\n\u2014\nReply to this email directly or view it on GitHub.\n. FYI I end up solved this issue- but not really like fix it. What I came across is we need to pass an i18n translated value inside json in rails, and since we have some translation missing, we have some extremely long missing translation text message inside the json, which triggers this behaviour. We fixed the translation and its automatically solved for us. So its either special character combination or too long json will trigger this.\n\nOn 25 May, 2014, at 8:56 am, barelyknown notifications@github.com wrote:\n\nFor what it's worth, I'm seeing the same problem. Identical requests will sometimes work, but once in a while (1/1000 requests maybe), puma seems to be truncating the JSON.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "pnc": "I'm seeing this error with Puma 2.6.0 on Heroku, running ruby 2.0.0p353 (2013-11-22 revision 43784) [x86_64-linux]. \nHowever, the error appears to be fixed on master, I think as a result of https://github.com/puma/puma/commit/74ea16df7f2a3e5211b5325cb148d84cd65799d0.\n@goodwill, @smidwap, @siong1987, and @shipstar, can you verify things are working if you change the dependency in your Gemfile to the following?\ngem 'puma', :git => 'https://github.com/puma/puma.git'\nSince the build is failing I don't recommend running this in production, but it would be good to know the bug is gone.\n. @evanphx Are you planning a 2.6.1 release sometime soon for any other reason, or would you consider doing one if I created a branch that just addressed this bug?\n@goodwill I looked at the Travis output and it appears the build is only failing on Rubinius, and that's as a result of running it on a more specific version. Nevertheless, I'll let a maintainer comment on whether master is in a state that's safe to try on a production system.\n. ",
    "sriedel": "Just ran into a similar situation (using a templating system to render config files due to a heterogenous server environment, so puma_config_file and puma_state are both set, but the indicated files only exist on the servers, not locally).\n. @pawel2105: I submitted a PR that fixes this issue and it got merged into puma 2.16.0. Can you see if the issue continues after you update your puma to a more recent version?. It could happen if the signals are fired in quick succession (and maybe even by some automated process that wants to scale out by say 5 workers). If the signal handler isn't fast enough and the process is busy, signals can get dropped by the kernel, or worse, the signal handler can get interrupted by the same signal being sent again (thus we have a race condition in the signal handler). \nThe more I think about it, it may actually be better to rearchitect the entire signal handling block to use a self pipe (q.v. https://www.sitepoint.com/the-self-pipe-trick-explained/) in order to minimize the race condition potential (that already exists with the worker counter increment/decrement code as it stands). But that may be too much change in one go. \nWould you rather have a second pull request introducing a self pipe, or should I try and rewrite everything at once?. As I understand it, if the signals come fast enough, they can \"queue up\" before an indirect handler such as this one has a chance to run. But you're right, maybe having a difference counter is a better approach. I'll have a look at it this week.. ",
    "tume": "imho, seems logical that NullIO object would also appear as nil. \nSituations where you would be encountering NullIO objects you are getting normal IO objects without puma that responds to more methods than NullIO. Responding to nil? would make that checking a bit easier.\n. ",
    "darkskiez": "I've been trying to do per-thread logging to different files, this also requires these hooks.\n. ",
    "bpardee": "I've included the console output for several runs.  Each attempt gets a little farther along until I get a successful render but some of the javascript is corrupted as I receive console errors.  This includes a webrick run which successfully comes all the way up with no console errors.\nLet me know if there is anything else I can do to help!\nhttps://gist.github.com/bpardee/6306122\n. For my use case it's fine to just quit and terminate the streams.  I'm not sure if that would affect other use cases.  Otherwise I would be happy if there was a hook such as on_before_restart that got called before waiting for the requests.\nMy config/puma.rb has the following:\n```\nTODO: This doesn't work, we don't get to here.  See https://github.com/puma/puma/issues/389\non_restart do\n  # Clean up the event-stream actions\n  PubSub.shutdown\nend\n```\n. PubSub is just a singleton that maintains Queue instances, 1 for each client that is listening on an event stream where the controller action is blocking on Queue#pop.  PubSub#shutdown just pushes a :shutdown message on each queue to end the controller action.\n. ",
    "emiliowl": "The problem is happening with these assets... they keep trying to download indefinitely. Since I abort the process through firebug and the pages get loaded successfully. \n(sorry about the bad english)\nhttp://localhost:5000/assets/dataTables/jquery.dataTables.bootstrap3.css?body=1\nhttp://localhost:5000/assets/dataTables/jquery.dataTables.responsive.css?body=1\nhttp://localhost:5000/assets/jquery-ui/flick.css?body=1\nhttp://localhost:5000/assets/send_offs.css?body=1\nhttp://localhost:5000/assets/system_users.css?body=1\nTks for your help.\nhttps://gist.github.com/emiliowl/320284e00e135d962519\n. Solve the problem by set \nRack::Timeout.timeout = 0\nand at least I found some relevant info at the log file, really hope it helps:\nPuma caught this error: Attempt to unlock a mutex which is locked by another thread (ThreadError)\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/rack-1.4.5/lib/rack/lock.rb:20:in unlock'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/rack-1.4.5/lib/rack/lock.rb:20:inensure in call'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/rack-1.4.5/lib/rack/lock.rb:21:in call'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/rack-timeout-0.2.0/lib/rack/timeout.rb:108:incall'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/railties-3.2.16/lib/rails/engine.rb:484:in call'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/railties-3.2.16/lib/rails/application.rb:231:incall'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/railties-3.2.16/lib/rails/railtie/configurable.rb:30:in method_missing'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/rack_patch.rb:13:incall'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/configuration.rb:82:in call'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/server.rb:507:inhandle_request'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/server.rb:375:in process_client'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/server.rb:262:inblock in run'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/thread_pool.rb:104:in `call'\n/home/murta/.rvm/gems/ruby-2.1.3@trans_terralheiro_dev/gems/puma-2.11.1/lib/puma/thread_pool.rb:104:in\n. ",
    "knopkodav": "The same problem again.\n$ ruby -v\nruby 2.0.0p353 (2013-11-22 revision 43784) [x86_64-linux]\n$ bundle exec puma -C ./config/puma.rb\n[32374] Puma starting in cluster mode...\n[32374] * Version 2.7.0, codename: Earl of Sandwich Partition\n[32374] * Min threads: 4, max threads: 16\n[32374] * Environment: production\n[32374] * Process workers: 2\n[32374] * Preloading application\n[32374] * Listening on unix:///srv/prj/shared/sockets/puma.sock\n[32374] * Daemonizing...\nNo process, no answer, and no log files appears.\nUPD: our config:\n```\n      daemonize\n      environment           'production'\n      directory             '/srv/prj/releases/20131205131955'\n      state_path            '/srv/prj/shared/sockets/puma.state'\n      bind                  'unix:///srv/prj/shared/sockets/puma.sock'\n      activate_control_app  'unix:///srv/prj/shared/sockets/pumactl.sock'\n      stdout_redirect       '/srv/prj/shared/log/puma.stdout', '/srv/prj/shared/log/puma.stderr', true\n  threads 4, 16\n  workers 2\n  preload_app!\n  on_worker_boot do\n    ActiveSupport.on_load(:active_record) do\n      ActiveRecord::Base.establish_connection\n    end\n  end\n\n```\n. ",
    "zekus": "I have something similar too\nit crashes and nothing gets written in the logs. I can see the error only if I do not demonize it\n. @evanphx it was a syntax error in the application code but the point is: shouldn't puma fail to start but write the reason in the log?\n. ",
    "jasonm23": "Reopen? I'm getting it with puma 2.7.1\n. Will do @evanphx \n. @evanphx - The only problem with this error is it's very hard to catch, I get it once every few days, maybe as infrequent as once a week. The only thing I have to go on is the Maytag(tm) error message, and the logs don't show anything unusual... Any ideas how I could trap it? \n. MRI 2.0.0-p247\nOn Sunday, December 29, 2013, nmccready wrote:\n\nAny version of Ruby that seem to be causing more problems than others? Is\nthis with MRI or JRuby?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/360#issuecomment-31309283\n.\n\n\njasonm23@gmail.com\nskype: jasonm23\nm: (aus) : +61 (0)409 307 977\ntwitter/instagram: @ocodo\nsoundcloud: http://soundcloud.com/ocodo\nmixcloud: http://mixcloud.com/ocodo\n. ",
    "nathany": "We have users seeing this Maytag error on Heroku using Puma 2.7.1 in non-clustered mode with -t 8:8 threads. It's sporadic enough that I don't have any details on it yet.\n. @rolandjitsu If it rarely happens then you can config it to display a custom error page. see https://github.com/puma/puma/pull/470 (that was the case for us, I don't know in your case).\n. @rolandjitsu Yup.\n. An update on this issue is here: https://github.com/rails/rails/issues/14031#issuecomment-37877887 along with a sample app to reproduce it: https://github.com/GetJobber/logging_example\nI still don't know exactly which gem the issue is in.\n. Thanks @arthurnn\n. ",
    "ericchen": "still got the error with following setup:\n puma 2.8.1\n Rails 4.1.0.rc1\n ruby-2.0.0-p195\n nginx 1.2.1\n debian 6\n. ",
    "camstuart": "Me Too:\npuma 2.8.1\nRails 3.2.16\nruby 2.1.0\ni am using rack timeout to help me catch this, in my I get:\n2014-03-13 14:52:59 +0000: Rack app error: #\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:20:in unlock'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:20:inensure in call'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:21:in call'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/static.rb:63:incall'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in forward'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:infetch'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in lookup'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:incall!'\n2014-03-13T14:52:59.604601+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:in call'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:inblock in call'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:91:in block in timeout'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:incall'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:in timeout'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:incall'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/engine.rb:484:in call'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/application.rb:231:incall'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/railtie/configurable.rb:30:in method_missing'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:incall'\n2014-03-13T14:52:59.604864+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/configuration.rb:71:in call'\n2014-03-13T14:52:59.605060+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:490:inhandle_request'\n2014-03-13T14:52:59.605060+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:361:in process_client'\n2014-03-13T14:52:59.605060+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:254:inblock in run'\n2014-03-13T14:52:59.605060+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:in call'\n2014-03-13T14:52:59.605060+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: 2014-03-13 14:52:59 +0000: Rack app error: #\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:20:in unlock'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:20:inensure in call'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:21:in call'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/static.rb:63:incall'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in forward'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:infetch'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in lookup'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:incall!'\n2014-03-13T14:52:59.605495+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:in call'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:inblock in call'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:91:in block in timeout'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:incall'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:in timeout'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:incall'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/engine.rb:484:in call'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/application.rb:231:incall'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/railtie/configurable.rb:30:in method_missing'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:incall'\n2014-03-13T14:52:59.605799+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/configuration.rb:71:in call'\n2014-03-13T14:52:59.606094+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:490:inhandle_request'\n2014-03-13T14:52:59.606094+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:361:in process_client'\n2014-03-13T14:52:59.606094+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:254:inblock in run'\n2014-03-13T14:52:59.606094+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:in call'\n2014-03-13T14:52:59.606094+00:00 app[web.1]: /app/vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n2014-03-13T14:53:03.764081+00:00 heroku[router]: at=info method=GET path=/ host=www.tennis-sessions.com request_id=a9da6eee-e839-4b78-b0f1-02095de42af3 fwd=\"50.112.95.211\" dyno=web.1 connect=1ms service=24972ms status=500 bytes=892\n2014-03-13T14:53:03.761269+00:00 app[web.1]: \n2014-03-13T14:53:03.761269+00:00 app[web.1]: Timeout::Error (execution expired):\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/mysql2-0.3.15/lib/mysql2/client.rb:67:in connect'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/mysql2-0.3.15/lib/mysql2/client.rb:67:ininitialize'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:252:in new'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:252:inconnect'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/mysql2_adapter.rb:78:in reconnect!'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract_adapter.rb:219:inverify!'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:333:in block in checkout_and_verify'\n2014-03-13T14:53:03.761269+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:403:in_run__4606127975412736677__checkout__4406215798330668413__callbacks'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in __run_callback'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:385:in_run_checkout_callbacks'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:81:in run_callbacks'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:332:incheckout_and_verify'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:253:in block (2 levels) in checkout'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:242:inloop'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:242:in block in checkout'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/monitor.rb:211:inmon_synchronize'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:239:in checkout'\n2014-03-13T14:53:03.761539+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:102:inblock in connection'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/monitor.rb:211:in mon_synchronize'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:101:inconnection'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:410:in retrieve_connection'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_specification.rb:171:inretrieve_connection'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_specification.rb:145:in connection'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/query_cache.rb:61:incall'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:in call'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/callbacks.rb:28:inblock in call'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in _run__3770337490518138642__call__4406215798330668413__callbacks'\n2014-03-13T14:53:03.761745+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in__run_callback'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:385:in _run_call_callbacks'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:81:inrun_callbacks'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/callbacks.rb:27:in call'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/remote_ip.rb:31:incall'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/debug_exceptions.rb:16:in call'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/show_exceptions.rb:56:incall'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:32:in call_app'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:16:inblock in call'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/tagged_logging.rb:22:in tagged'\n2014-03-13T14:53:03.762476+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:16:incall'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/request_id.rb:22:in call'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/methodoverride.rb:21:incall'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/runtime.rb:17:in call'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/cache/strategy/local_cache.rb:72:incall'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:15:in call'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/static.rb:63:incall'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in forward'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:infetch'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in lookup'\n2014-03-13T14:53:03.762688+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:incall!'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:in call'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:inblock in call'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:91:in block in timeout'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:incall'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:in timeout'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:incall'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/engine.rb:484:in call'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/application.rb:231:incall'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/railtie/configurable.rb:30:in method_missing'\n2014-03-13T14:53:03.763084+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:incall'\n2014-03-13T14:53:03.763266+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/configuration.rb:71:in call'\n2014-03-13T14:53:03.763266+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:490:inhandle_request'\n2014-03-13T14:53:03.763266+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:361:in process_client'\n2014-03-13T14:53:03.763266+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:254:inblock in run'\n2014-03-13T14:53:03.763266+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:in call'\n2014-03-13T14:53:03.763266+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n2014-03-13T14:53:03.763266+00:00 app[web.1]: \n2014-03-13T14:53:03.763266+00:00 app[web.1]: \n2014-03-13T14:51:18.147168+00:00 heroku[router]: at=info method=GET path=/coaches host=www.tennis-sessions.com request_id=3e6bbb2c-8637-4f17-9874-d018b93a65c8 fwd=\"123.3.107.144\" dyno=web.1 connect=2ms service=24008ms status=500 bytes=915\n2014-03-13T14:53:20.671923+00:00 app[web.1]: \n2014-03-13T14:53:20.671923+00:00 app[web.1]: ActionView::Template::Error (Timeout::Error: execution expired: SHOW TABLES LIKE 'clubs'):\n2014-03-13T14:53:20.671923+00:00 app[web.1]:     1: <%\n2014-03-13T14:53:20.671923+00:00 app[web.1]:     2:    length = 20\n2014-03-13T14:53:20.671923+00:00 app[web.1]:     3: \n2014-03-13T14:53:20.671923+00:00 app[web.1]:     4:    date_parts = international_date_format(date: session.date_time, time_zone: session.club.time_zone).split\n2014-03-13T14:53:20.671923+00:00 app[web.1]:     5:    date_day = \"#{date_parts[0]} #{date_parts[1]}\"\n2014-03-13T14:53:20.671923+00:00 app[web.1]:     6:    date_time = \"#{date_parts[2]} #{date_parts[3]}\"\n2014-03-13T14:53:20.671923+00:00 app[web.1]:     7: \n2014-03-13T14:53:20.671923+00:00 app[web.1]:   app/views/tennis_session_events/_session.html.erb:4:in _app_views_tennis_session_events__session_html_erb___738460052597427200_70069145605760'\n2014-03-13T14:53:20.672195+00:00 app[web.1]:   app/views/tennis_session_events/index.html.erb:15:in_app_views_tennis_session_events_index_html_erb___3047219591128678442_70069148775180'\n2014-03-13T14:53:20.672195+00:00 app[web.1]:   app/controllers/tennis_session_events_controller.rb:67:in `index'\n2014-03-13T14:53:20.672195+00:00 app[web.1]: \n2014-03-13T14:53:20.672195+00:00 app[web.1]: \n2014-03-13T14:53:30.388682+00:00 app[web.1]: [deprecated] I18n.enforce_available_locales will default to true in the future. If you really want to skip validation of your locale you can set I18n.enforce_available_locales = false to avoid this message.\nActiveRecord::ConnectionTimeoutError (could not obtain a database connection within 5 seconds (waited 5.001970755 seconds). The max pool size is currently 5; consider increasing it.):\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:258:in block (2 levels) in checkout'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:242:inloop'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:242:in block in checkout'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/monitor.rb:211:inmon_synchronize'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:239:in checkout'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:102:inblock in connection'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/monitor.rb:211:in mon_synchronize'\n2014-03-13T14:32:18.775568+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:101:inconnection'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:410:in retrieve_connection'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_specification.rb:171:inretrieve_connection'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_specification.rb:145:in connection'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/query_cache.rb:67:inrescue in call'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/query_cache.rb:61:in call'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:incall'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/callbacks.rb:28:in block in call'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in_run__2110533664656440604__call__3360993207254776806__callbacks'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in __run_callback'\n2014-03-13T14:32:18.775793+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:385:in_run_call_callbacks'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:81:in run_callbacks'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/callbacks.rb:27:incall'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/remote_ip.rb:31:in call'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/debug_exceptions.rb:16:incall'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/show_exceptions.rb:56:in call'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:32:incall_app'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:16:in block in call'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/tagged_logging.rb:22:intagged'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:16:in call'\n2014-03-13T14:32:18.775961+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/request_id.rb:22:incall'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/methodoverride.rb:21:in call'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/runtime.rb:17:incall'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/cache/strategy/local_cache.rb:72:in call'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:15:incall'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/static.rb:63:in call'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ru\nby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:inforward'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:in fetch'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:inlookup'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:in call!'\n2014-03-13T14:32:18.776516+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:incall'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:in block in call'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:91:inblock in timeout'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:in call'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:intimeout'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:in call'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/engine.rb:484:incall'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/application.rb:231:in call'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/railtie/configurable.rb:30:inmethod_missing'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:in call'\n2014-03-13T14:32:18.776692+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/configuration.rb:71:incall'\n2014-03-13T14:32:18.777104+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:490:in handle_request'\n2014-03-13T14:32:18.777104+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:361:inprocess_client'\n2014-03-13T14:32:18.777104+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:254:in block in run'\n2014-03-13T14:32:18.777104+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:incall'\n2014-03-13T14:32:18.777104+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:in block in spawn_thread'\n2014-03-13T14:32:18.777104+00:00 app[web.1]: \n2014-03-13T14:32:18.777104+00:00 app[web.1]: \n2014-03-13T14:32:28.786987+00:00 app[web.1]: \n2014-03-13T14:32:28.786987+00:00 app[web.1]: ActiveRecord::ConnectionTimeoutError (could not obtain a database connection within 5 seconds (waited 5.000110467 seconds). The max pool size is currently 5; consider increasing it.):\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:258:inblock (2 levels) in checkout'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:242:in loop'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:242:inblock in checkout'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/monitor.rb:211:in mon_synchronize'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:239:incheckout'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:102:in block in connection'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/monitor.rb:211:inmon_synchronize'\n2014-03-13T14:32:28.786987+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:101:in connection'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:410:inretrieve_connection'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_specification.rb:171:in retrieve_connection'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_specification.rb:145:inconnection'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/query_cache.rb:67:in rescue in call'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/query_cache.rb:61:incall'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:in call'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/callbacks.rb:28:inblock in call'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in _run__2110533664656440604__call__3360993207254776806__callbacks'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:405:in__run_callback'\n2014-03-13T14:32:28.787209+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:385:in _run_call_callbacks'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/callbacks.rb:81:inrun_callbacks'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/callbacks.rb:27:in call'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/remote_ip.rb:31:incall'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/debug_exceptions.rb:16:in call'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/show_exceptions.rb:56:incall'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:32:in call_app'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:16:inblock in call'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/tagged_logging.rb:22:in tagged'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/rack/logger.rb:16:incall'\n2014-03-13T14:32:28.787371+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/request_id.rb:22:in call'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/methodoverride.rb:21:incall'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/runtime.rb:17:in call'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activesupport-3.2.17/lib/active_support/cache/strategy/local_cache.rb:72:incall'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-1.4.5/lib/rack/lock.rb:15:in call'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/actionpack-3.2.17/lib/action_dispatch/middleware/static.rb:63:incall'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:136:in forward'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:245:infetch'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:185:in lookup'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:66:incall!'\n2014-03-13T14:32:28.787981+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-cache-1.2/lib/rack/cache/context.rb:51:in call'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:inblock in call'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:91:in block in timeout'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:incall'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/ruby-2.1.1/lib/ruby/2.1.0/timeout.rb:101:in timeout'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/rack-timeout-0.0.4/lib/rack/timeout.rb:16:incall'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/engine.rb:484:in call'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/application.rb:231:incall'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/railties-3.2.17/lib/rails/railtie/configurable.rb:30:in method_missing'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/activerecord-3.2.17/lib/active_record/connection_adapters/abstract/connection_pool.rb:479:incall'\n2014-03-13T14:32:28.788158+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/configuration.rb:71:in call'\n2014-03-13T14:32:28.788579+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:490:inhandle_request'\n2014-03-13T14:32:28.788579+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:361:in process_client'\n2014-03-13T14:32:28.788579+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/server.rb:254:inblock in run'\n2014-03-13T14:32:28.788579+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:in call'\n2014-03-13T14:32:28.788579+00:00 app[web.1]:   vendor/bundle/ruby/2.1.0/gems/puma-2.8.1/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n. ",
    "inspire22": "secret_key_base just sets whether it encrypts the session storage - it shouldn't cause anything this massive.\nI'm also having issues with ActiveRecord::ConnectionTimeoutError (could not obtain a database connection within 5.000 seconds (waited 5.000 seconds), though no maytag error, so perhaps it's not related.  Driving me nuts and causing downtime though.\n. How do I set this?  It doesn't seem to work in puma.rb config..\n. Took me awhile, but I discovered that you can do this with the state file, I assume there's a similar way to use the \"control app\" tcp based version:\nbundle exec pumactl -S /tmp/puma.state stats\nShows # in queue, etc.  Would be great if it kept track of request/second, average length, slowest page, fastest url, or other details like that.\n. I still get this a few times a day with activerecord/mysql.  Right now I have a cron job checking the logfile for \"unable to establish a database connection in 5.000\" and restarting the server, but that is not ideal...\nMin==max threads == # of db connections in pool\n. ",
    "Alamoz": "I'm experiencing this problem when using Nginx as a reverse proxy to a Puma/Rails 4.1.0 cluster via TCP sockets. The problem does not appear when running Nginx interfaced with Puma and Rails 4.1.0 using a Unix domain socket on a single machine.\n. See https://github.com/rails/rails/issues/14674 for a more detailed explanation. Try setting your production secret_key_base in config/secrets.yml using a string in the file, rather than referencing the <%= ENV[\"SECRET_KEY_BASE\"] %> environment variable, that solves the problem, though that method is considered insecure compared to setting it via the ENV variable. I'm only experiencing the problem when running a Puma/Rails 4.1.0 cluster interfaced with a reverse proxy via TCP sockets.\n. So when I revert back to my old /path/to/app/puma.rb config file, puma 2.7.1 will run from the command line, but not from the jungle scripts. The config file is:\n!/usr/bin/env puma\nrailsenv = 'production'\napplication_path = '/path/to/app'\nthreads 1, 16\ndaemonize true\nbind \"tcp://127.0.0.1:9292\"\nbind \"unix://#{application_path}/tmp/sockets/#{railsenv}.socket\"\npidfile \"#{application_path}/tmp/pids/puma-#{railsenv}.pid\"\ndirectory application_path\nenvironment railsenv\nstate_path \"#{application_path}/tmp/pids/puma-#{railsenv}.state\"\nstdout_redirect \"#{application_path}/log/puma-#{railsenv}.stdout.log\", \"#{application_path}/log/puma-#{railsenv}.stderr.log\"\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\nAnd /etc/puma.conf is:\n/path/to/app,root\nSo it must be something in the config file. I've changed it to the settings recommended on the jungle page, including trying the minimal config options specified there, and still puma won't run from the jungle scripts.\n. Looks like something in my production environment is breaking the jungle script. Will let you know when I figure it out. I run Linux and build my Ruby from source. I think the jungle script assumes an RVM environment.\n. OK, so the problem was the /bin directory for my Ruby installation wasn't in the /etc/init.d/puma script. So for anyone else experiencing this, just add the /bin path for your particular Ruby to the PATH definition in /etc/ilnit.d/puma (see https://github.com/puma/puma/tree/master/tools/jungle/init.d).\nVery nice, seems to run faster than from the command line. :smile: \n. Well, now it no longer works, just stopped working after I tried restarting a few times to test it. Pretty flaky. Maybe I can figure it out and improve the reliability. For now, I'll just create a one-line /etc/init.d/puma script to start it, so at least it will restart if the service bounces.\n. ",
    "alec-c4": "i have this bug with rails 4.1.1, mri ruby 2.1.2 and puma 2.8.2 (tested on 2.6.0 too - same result)\n. Guys, sorry - my mistake :) I've forgotten to update secret_key_base in secrets.yml. But - error wasn't informative :(\n. ",
    "rolandjitsu": "I have the same issue here. Is there a fix for it @nathany ?\n. @nathany, is:\nruby\nlowlevel_error_handler do\n  [302, {'Content-Type' => 'text', 'Location' => 'foo.html'}, ['302 found']]\nend\nsomething I can use in a puma.rb config file along with the other configuration like workers?\n. @bwolkerstorfer, I have actually tracked it down and it was not Puma. It was some of my code that in production behaved different. And it usually occurs when you get a 500 exception somewhere in your code, you should probably give it a closer look.\n. @bwolkerstorfer, perhaps some of the code or dependencies don't play well with Puma as it's multithreaded and you are also using JRuby thus using native threads?\n. ",
    "bwolkerstorfer": "We experience the same issue when posting binary data to our servers.\nJRuby 1.7.12\nRails 4.0.5\nPuma 2.8.2\nNginx 1.6\nWhen we are posting gzipped data with the headers:\nContent-Type: application/octet-stream\nContent-Encoding: gzip\nWe get the following stack trace written to STDERR:\n/var/projects/.rvm/rubies/jruby-1.7.12/lib/ruby/1.9/uri/common.rb:898:in `decode_www_form_component'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/utils.rb:42:in `unescape'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/utils.rb:94:in `parse_nested_query'\norg/jruby/RubyArray.java:2409:in `map'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/utils.rb:94:in `parse_nested_query'\norg/jruby/RubyArray.java:1613:in `each'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/utils.rb:93:in `parse_nested_query'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/request.rb:373:in `parse_query'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/request.rb:211:in `POST'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/methodoverride.rb:26:in `method_override'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/methodoverride.rb:14:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/runtime.rb:17:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/activesupport-4.0.5/lib/active_support/cache/strategy/local_cache.rb:83:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/rack-1.5.2/lib/rack/sendfile.rb:112:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/railties-4.0.5/lib/rails/engine.rb:511:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/railties-4.0.5/lib/rails/application.rb:97:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/puma-2.8.2-java/lib/puma/configuration.rb:71:in `call'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/puma-2.8.2-java/lib/puma/server.rb:490:in `handle_request'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/puma-2.8.2-java/lib/puma/server.rb:488:in `handle_request'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/puma-2.8.2-java/lib/puma/server.rb:361:in `process_client'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/puma-2.8.2-java/lib/puma/server.rb:357:in `process_client'\n/var/projects/services/platform-service/shared/bundle/jruby/1.9/gems/puma-2.8.2-java/lib/puma/server.rb:254:in `run'\n. @rolandjitsu I am not so sure about that as we startet getting this issue after we switched from trinidad to puma for testing purpose. We just changed the application server no other changes to the service or the apps have been made and it immediately started showing up.\n. @rolandjitsu We have been using a multithreaded environment with the JRuby/Trinidad as well. I will take a closer look but it seems that the content is just not passed through to the application, when the HTTP headers are set to\nContent-Type: application/octet-stream\nContent-Encoding: gzip\nFrom the stack trace you can see that rack is trying to parse the request with decode_www_form_component\n. ",
    "LeandroSNunes": "I solved the problem \"A really lowlevel plumbing error occured. Please contact your local Maytag(tm) repair man\" with the tips of @Alamoz , assigning the key config/secrets.yml for the environment.\nRails 4.2.0\nruby 2.2.0p0\npuma 2.10.2\nnginx 1.1.19\nthank you!\n. ",
    "velobuff": "We've had this issue when someone attempts to POST malformed JSON i.e. it contains unescaped quotes. \nIn our case, someone was using the Postman Chrome plugin to post JSON. Apparently the plugin doesn't escape quotes in JSON strings before it transfers it over the ether.\n. ",
    "jackxxu": "it seems to be working now, so perhaps I was using a older version. I will try to keep an eye on it in case it happens again. thanks!\n. ",
    "krmichelos": "Great; Thanks\n. I think what we are looking for is something to tell us if puma is currently available to server requests or is not.  running and backlog can tell us if it is processing requests or has some waiting to process.  We need to know if it is in a state where it is able to accept requests.  Like when stop is call async how do we tell when the server has stopped and is no longer available to accept requests. \n. @krisleech \nI wouldn't use nc as it not generally going to be available on Windows.  I have code like this:\nruby\ndef running?\n  begin\n    uri = URI(@url)\n    if uri.host == '0.0.0.0' then uri.host = '127.0.0.1' end\n    Net::HTTP.get_response(uri)\n  rescue Errno::ECONNREFUSED\n    false\n  else\n    true\n  end\nend\nBut it feels clunky and has more potential for not behaving as intended than just being able to ask the puma server if it is up.\n. ",
    "sethvargo": "@evanphx is there something quick we can do in the meantime before you have a chance to make those APIs public?\n. @evanphx https://github.com/jkeiser/chef-zero/blob/master/lib/chef_zero/server.rb#L92-L129 for example and https://github.com/sethvargo/community-zero/blob/master/lib/community_zero/server.rb#L99-L104\n. @evanphx if you start the server in the background, it may not start immediately and we need to wait until it's ready to proceed.\n. :heart: \n. ",
    "krisleech": "Could use use while ! nc -vz localhost 9000; do sleep 1; done to wait for the port to become open, or do you need to know it can actually serve HTTP?\n. @krmichelos Ah, I thought you where talking about having something external to Puma, not built-in.\n. ",
    "nchelluri": "Yes, that would be helpful for me. \nNarsa\nOn 2013-09-09, at 9:56 AM, Evan Phoenix notifications@github.com wrote:\n\nSo you'd expect puma to print something like \"Internal server error\" then?\n\u2014\nReply to this email directly or view it on GitHub.\n. Even better would be to see it in the access log (or stdout on dev). \n\nNarsa\nOn 2013-09-09, at 9:56 AM, Evan Phoenix notifications@github.com wrote:\n\nSo you'd expect puma to print something like \"Internal server error\" then?\n\u2014\nReply to this email directly or view it on GitHub.\n. Just to be clear, the issue I'm facing is that the exception message doesn't show up before terminating the server (by hitting ^C), not that it never shows up.\n\nStacktrace:\nshell\n== Sinatra/1.4.3 has taken the stage on 4567 for development with backup from Puma\n^C\n== Sinatra has ended his set (crowd applauds)\nException handling servers: Could not find matching strategy for :bad_provider. You may need to install an additional gem (such as omniauth-bad_provider). (LoadError)\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/omniauth-1.1.4/lib/omniauth/builder.rb:40:in `rescue in provider'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/omniauth-1.1.4/lib/omniauth/builder.rb:37:in `provider'\napp.rb:9:in `block in <main>'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:55:in `instance_eval'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:55:in `initialize'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/omniauth-1.1.4/lib/omniauth/builder.rb:8:in `initialize'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:86:in `new'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:86:in `block in use'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:134:in `[]'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:134:in `block in to_app'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:134:in `each'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:134:in `inject'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/rack-1.5.2/lib/rack/builder.rb:134:in `to_app'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/sinatra-1.4.3/lib/sinatra/base.rb:1435:in `new'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/sinatra-1.4.3/lib/sinatra/base.rb:1424:in `prototype'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/sinatra-1.4.3/lib/sinatra/base.rb:1449:in `block in call'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/sinatra-1.4.3/lib/sinatra/base.rb:1726:in `synchronize'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/sinatra-1.4.3/lib/sinatra/base.rb:1449:in `call'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/puma-2.5.1/lib/puma/server.rb:472:in `handle_request'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/puma-2.5.1/lib/puma/server.rb:343:in `process_client'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/puma-2.5.1/lib/puma/server.rb:242:in `block in run'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/puma-2.5.1/lib/puma/thread_pool.rb:92:in `call'\n/Users/nchelluri/.rvm/gems/ruby-1.9.3-p429/gems/puma-2.5.1/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\n. Hi @masterkain , I tried adding that line (in several different spots in case order mattered) and it did not change anything.\n. My points are the following:\n- Puma does not return a 500 or any type of error when the request fails - it simply gives an empty reply without anything (no HTTP headers, even) in it\n- The error is not seen in the server console (even with $stdout.sync = true set) until I hit ^C and end the execution of the server\n- Thin used to both give a 500/\"Internal Server Error\" response and display an error message in the server console. It now (as of version 1.6.0) does the former (the 500 response, which is good, IMO), but not the latter (which is bad, IMO - I think it would be helpful if both Puma and Thin displayed an error message).\nDoes this make sense?\n. :+1: \n. ",
    "daniel-g": "I get this problem when I daemonize the process. I wonder if there is something else.\n. Thank you @alakra , I will try it out.\n. Since I got this problem and re-run the processes, I haven't had the same issue. Either way, I will try the expect daemon command.\n. No, I am using Ruby 1.9.3. But I wonder if it was just an error in my app. It has not happened again.\n. ",
    "alakra": "If you are still trying to run this as a daemon, you will need to include the following in your /etc/init/puma.conf:\nexpect daemon\nThe manpage (reachable via man 5 init on Ubuntu) has this to say about it:\n```\n       expect daemon\n          Specifies that the job's main process is a daemon, and will fork twice\n          after being run.  init(8) will follow this daemonisation, and  will  wait  for\n          this to occur before running the job's post-start script or considering the \n          job to be running.\n      Without this stanza init(8) is unable to supervise daemon processes and\n      will believe them to have stopped as soon as they daemonise on startup.\n\n```\nEDIT:\n~~You may also need to try expect fork if the daemon only forks once.~~\n. Best of luck @daniel-g :)\n. ",
    "yanivyalda": "Had this issue too. We switched to tcp socket until gem update.\n. ",
    "mscoutermarsh": "@inspire22 I was able to set it by passing a query param to bind. Here's an example config that I tested on Heroku.\n``` ruby\nworkers Integer(ENV['PUMA_WORKERS'] || 3)\nthreads Integer(ENV['MIN_THREADS']  || 1), Integer(ENV['MAX_THREADS'] || 16)\npreload_app!\nrackup      DefaultRackup\nport = Integer(ENV['PORT'] || 3000)\nbacklog = Integer(ENV['PUMA_BACKLOG'] || 20)\nbind \"tcp://0.0.0.0:#{port}?backlog=#{backlog}\"\nenvironment ENV['RACK_ENV'] || 'development'\non_worker_boot do\n  # worker specific setup\n  ActiveSupport.on_load(:active_record) do\n    config = ActiveRecord::Base.configurations[Rails.env] ||\n                Rails.application.config.database_configuration[Rails.env]\n    config['pool'] = ENV['MAX_THREADS'] || 16\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\n```\n. ",
    "david-harkness": "A way to do this with nginx would be great\n. Perhaps something like this?\nsh -c \"cd $dir && /home/[myapp]/.rvm/bin/rvm default do bundle exec pumactl -F $config_file start\"\n. ",
    "jgilles": "Putting this on the wiki and/or website would make it more findable...\n. +1 for this\n. ",
    "jihaia": "I have the same issue. On OS X and am receiving the following;\nERROR: worker mode not supported on JRuby and Windows\n. ",
    "henry74": "Yes, I am running Jruby on Ubuntu. Shouldn't it read Jruby OR Windows if what you're saying is true? \n. ",
    "sauliusgrigaitis": "I have same problem here. I use monit, so as temporary fix monit removes sock file before starting puma.\n. ",
    "eljojo": "Hi, is it possible to either backport this change or release a new version?\nCouldn't manage to install puma from master.\n. ",
    "mitjok": "+1\n. ",
    "shigi": "I agree this code makes slow.\nBut this codes doesn't screw up any apps depends on proper #byteslice because Ruby 1.9.1 and 1.9.2\ndoesn't have String#byteslice, and in Ruby 1.9.3 or later it doesn't overwrite String#byteslice, so it makes\nno effects.\n. ",
    "andorov": "I don't get a stack trace with rails 4.0 even though puma says it's in development mode.  However running puma with  rails server does show a stack trace.\n. webrick shows the stack trace\n. ",
    "brixen": "This isn't related to Puma. Fixing the issue in rubysl-json.\n. Thank you, @evanphx!\n. ",
    "eddloschi": "+1\n. @evanphx definitely a nice idea\n. ",
    "augustoht": "+1\n. ",
    "aeciobf": "+1\n. ",
    "crhan": ":+1: \n. ",
    "molfar": "I think the code of https://github.com/puma/puma/blob/master/lib/puma/capistrano.rb is total incompatible with Capistrano 3. Who can rewrite it? \n. I notices, when I add custom pause (sleep) after startup command - it works.\nIt looks like when \"* Daemonizing...\" is printed, script immediately completes - and puma cant start.\nBut when I add a extra pause (3 sec) after this command - puma starts well.\nThis is the second way, in addition to \"--redirect-stdout or --redirect-stderr args\" i wrote earlier.. I have no config file. All config options pass through command shown above.. I have very similar issue. Errors:\nThere is already a server bound to: tmp/puma/puma.sock\nAddress already in use - bind(2) for \"0.0.0.0\" port 9295\nDepending on puma config (tcp or socket).\nAnd again, without socket (systemd service only) - everything works fine.\n  . @dekellum I didnt dive into this problem, just tried to follow instructions from puma's doc. I'm not linux administrator at all... I also asked my hosting support to solve this problem, they spent a while day long and could not resolve it. Thats why I decided to inform with issue. For now I gave up with this socket activation feature and use simple systemd service. So you could close this issue if you wish.. ",
    "BlakeWilliams": "+1 to ripping out the scripts into a separate gem.\n. ",
    "andrewhavens": "@evanphx I'm not sure how to gather information about this. Do you have any suggestions? I tried installing New Relic, but couldn't get it to work. Here's what their support team had to say:\n\nYour logs are showing that we often don't detect Puma as your dispatcher (although we do sometimes). I'm wondering if this has to do with you running Puma in daemonized mode sometimes, but it failing at other times. Some other customers who are using the daemonized mode with Puma have had success in getting data reporting by setting 'workers 1'. The problem with daemonized mode is daemonization forks the process, and there is no hook for us to use in order to re-start the worker thread after that fork. Setting 'workers 1' causes puma to use clustered mode, which makes such a hook available.\n\nI think part of the problem is that the Capistrano script does not specify any workers so there is nothing to restart a killed process. I tried creating a config/puma.rb file, but the Capistrano script ignores my config file. I mentioned that someone gave me the idea that I might be running out of memory during asset compilation. I'm running a my VPS at 512 MB. I'm not sure how to monitor my memory usage to see if that's true or not.\nBTW: I'm running the older version of Capistrano: 2.15.5, not v3 that was just released.\n. I'm working on resolving this again. As I mentioned before, the Capistrano script does not read my config file (config/puma.rb). It just uses some default settings. I was reading through the source and it looks like the script is expecting a config file in a different location than I expected (config/puma/production.rb). I haven't tried creating this file yet but I'm wondering what the default settings are that I should put in this config. I want to be able to isolate the change by adding a worker and see if that fixes the problem. Also, I think these issues are related/duplicates: #395, #401\n. I am not using any custom deploy tasks and am still (randomly) having the issue. My deploy.rb is as minimal as it gets, and I am only using the default settings (no Puma config file):\n``` ruby\nrequire \"puma/capistrano\"\nrequire \"bundler/capistrano\"\n...normal server/repo config omitted...\nset :user, :deploy\nset :deploy_via, :remote_cache\nset :use_sudo, false\nset :default_environment, {\n  'PATH' => \"/usr/local/rbenv/shims:/user/local/rbenv/bin:$PATH\"\n}\nnamespace :deploy do\n  task :symlink_config, roles: :app do\n    run \"ln -nfs #{shared_path}/config/database.yml #{release_path}/config/database.yml\"\n  end\n  after \"deploy:finalize_update\", \"deploy:symlink_config\"\nend\nafter \"deploy\", \"deploy:cleanup\" # keep only the last 5 releases\n```\nI recently installed New Relic server monitoring, so the next time Puma fails to restart, I can watch to see if it is an issue with running out of memory.\n. ",
    "alepore": "@sorentwo i have the same problem with my custom cap 3 tasks https://gist.github.com/alepore/8083925\n. what i've learned in my case:\ncalling pumactl stop removes the pid but not the state file (feature?)\ncalling a pumactl restart after the stop actually works, but the process name will be pumactl and not puma (?) and a following pumactl restart will kill the process.\nnot sure why this happens.\ni think i solved in my cap task by calling pumactl only if the pid (and state file) exists, otherwise call puma\n. puma --tag myapp config.ru will display this in ps:\n$ ps | grep puma\n62645 ttys001    0:00.51 puma 2.7.1 (tcp://0.0.0.0:9292) [myapp]\n. i moved the code inside CLI, please take a look\n. sure! updated commit\n. ",
    "bwidtmann": "please try in production environment. maybe it is a problem with assets pipeline and caching, which is handled differently by other servers (thin, webrick). As soon as I have enough time, I will report detailed errors.\n. yes, i did not precompile the assets and wanted to compile them on-the-fly with \"config.compile = true\" in production.rb. this works fine on thin and webricks but not on puma. But since on real production system you should always precompile first and set \"config.compile = false\" this is a mistake on our side. \nSo you can close this issue as designed. Thank you\n. ",
    "avaranovich": "@bpardee what is PubSub in your case? I am experiencing the same issue, but I additionally need to free redis connections.\nTrying to stop eventmachine by pitting EM.stop inside on_restart\ngives me\nconfig/puma.rb:11:in `stop': eventmachine not initialized: evma_stop_machine (RuntimeError)\n. @evanphx Is there an option to \"just quit and terminate the streams\"?\n. I see in the log the following line\n[17922] - Gracefully shutting down workers...\npid is indeed removed. socks are there.\nLooks like the workers are frozen and puma cannot completely shut down.\n. 2 workers X 32 threads  = 64 threads. This number I would expect to see in htop. Hovewer I see much bigger number of threads.\n. In fact, I realized that kill -s SIGTERM does not kill the puma instance. And when I do a restart on a new deploy, a new process is created, but the old one is not killed.\nThis might be related to https://github.com/puma/puma/issues/392 and https://github.com/puma/puma/issues/389\nActually I see many  CLOSE_WAIT  1339/ruby TCP connections after that. Puma not killing them for some reason. I use nginx as a reverse proxy.\n. I am closing this issue in favour of https://github.com/puma/puma/issues/389\n. ",
    "jrobeson": "@evanphx : are you saving this for 2.8.0 now?\n. @mdimas : #408 was merged to increase the max length to 2048 (as per HTTP RFC), so you can likely close this ticket now\n. ",
    "itmanager223": "Any word when 2.8.0 will be released? We are running 2.7.1 with this issue now.\n. ",
    "itkin": "+1\n. ",
    "chriserik": "+1\n. ",
    "huetsch": "I'm quite unfamiliar with the Puma codebase, but I've worked my way down here from https://github.com/rails/rails/issues/10989. I'm a user of ActionController::Live. I'm lacking some crucial pieces of information: for example, how the sockets you collect here in binder end up wired up to my IO stream in my live controller actions.\nUltimately what we need up in our live controllers is a hook (or some sort of error handler) to be called when a client disconnects. A common usage case is we have an open connection to a browser window where we're sending Server Sent Events and we need to do some clean-up and close our stream when the client leaves. \nWhen the browser window closes, it surely must be sending either a RST or FIN packet (haven't sniffed to find which). I believe you should be able to detect this when you're calling read() on the socket. Then somehow we need to get that information to the controller and kill the connection.\nThis probably doesn't handle all of the scenarios: Clients don't just get disconnected because they close the browser window - it's also possible the network between the server and client just went down suddenly. So I think more ideally we would be using Socket::SO_KEEPALIVE for sockets used by a live controller to handle half-closed connections. But perhaps that could get complicated to implement.\n. ",
    "j-manu": "@evanphx Is there a cleaner way to fix this than using halt? Will the timed terminate option be added?\n. On further testing, found a way. Use phased-restart with a worker_shutdown_timeout. First TERM gets sent to workers and after worker_shutdown_timeout KILL gets sent if the worker hasn't shutdown. \n. ",
    "bfallik": "I just generated a new one:\n$ rails new dummy\n$ rails generate controller dumb do\nwith dummy_controller.rb containing:\n  class DumbController < ApplicationController\n    def do\n      render :text => '{\"id\": 1}'\n    end\n  end\nand enabling config.threadsafe! in config/environments/production.rb.\nI launched the app with $ jruby -S bundle exec puma -p 80 -q -e production.\nThanks.\n. gatling\n. @bbrowning thanks for the input, i'll have to retry my test with keepalives disabled.\n. ",
    "bbrowning": "I just wanted to note that I was testing Puma so I can compare it against the future torquebox-lite replacement and I also saw poorer than expected performance on JRuby.\nWith HTTP keepalives disabled, Puma could serve my \"Hello World\" (literally just outputs that in a config.ru) Rack app at 10k requests / second. This 10k requests / second number is limited because that's how fast my OS can open and close socket connections.\nWith keepalives enabled, to workaround the OS limit on recycling TCP connections, Puma dropped to an even lower number of requests / second that seems to be almost exactly 25 * # of concurrent clients per second. In other words, with keepalives enabled on the client and 1 client I get 25 requests per second, with 10 clients I get 250, and with 16 clients I get 400.\nSo, make sure you didn't have keepalives enabled for the HTTP client until the performance drop there can be looked into. For comparison's sake, other JRuby servers can achieve over 70k requests per second on my hardware with this application when HTTP keepalives are enabled.\n. Here's a gist with the sample Rack app and simple benchmark showing keepalive performance problems - https://gist.github.com/bbrowning/7378241\n. @johnthethird Interesting - I tried with keepalives on and JRuby 1.7.4 (vs 1.7.6) just to rule out that difference and I get the same results. I also tried with an Oracle Java 8 JDK to try and rule out my OpenJDK and got about the same results. So, perhaps there's something specific about Fedora 19 or Linux compared to the Mac?\n. I've tried running ab from a separate Linux client / server and get the identical slowdown as running locally when keepalives are enabled. What I haven't yet tried is a Mac client. I've also tried wrk (https://github.com/wg/wrk) to try and rule out weirdness with ab but it's showing the same slowdown as ab, and as far as I can tell wrk has no way to disable keepalives.\nSubstantially increasing the number of Puma threads and clients (512 of each) does allow the keepalive throughput to match that of keepalives disabled, but running with keepalives disabled requires far fewer Puma threads. However, I still can't get Puma close to the performance of other JRuby servers with keepalives enabled.\n. ",
    "johnthethird": "@bbrowning Just as another data point, I ran your gist with keepalives ON and got 24K req/sec on a new MacBook Pro / Mavericks.\njruby 1.7.4 (1.9.3p392) 2013-05-16 2390d3b on Java HotSpot(TM) 64-Bit Server VM 1.7.0_45-b18 +indy [darwin-x86_64]\npuma (2.6.0 java)\n. @bbrowning Yea, Linux is way slower with keepalives. \nUbuntu 10.04\nLinux stark 2.6.32-41-server #91-Ubuntu SMP Wed Jun 13 11:58:56 UTC 2012 x86_64 GNU/Linux\njruby 1.7.3 (1.9.3p385) 2013-02-21 dac429b on Java HotSpot(TM) 64-Bit Server VM 1.7.0_17-b02 [linux-amd64]\nab -c 10 -n 25000  http://127.0.0.1:9292/\n2496 req/sec\nab -c 10 -n 25000 -k  http://127.0.0.1:9292/\n200 req/sec\n. If I run ab from my Mac, against the Linux/JRuby Puma server, then the req/sec make a lot more sense (2k req/sec for no keepalives, and 9K req/sec for keepalives).  So is it something about running ab from the same host that the server is running on? \n. I think this is a bundle exec issue, bundler is starting up (in a JVM) and then its starting puma (in another JVM). If you try bundle install --binstubs, and then bin/puma, that should work.\n. ",
    "brendandc": "This seems similar to an issue I just created, #394.\nI am curious, did you recently upgrade to JRuby 1.7.5 from 1.7.4? \nThat was when I started to experience my issue.\n. ",
    "kyrylo": "@evanphx, please, reconsider it :)\nhttps://github.com/rubinius/rubinius/commit/b535225c2c7d5583a8965f72da844c7210e441c1\n. No problem, I understand. The good thing is that Rubinius has become slightly better.\n. ",
    "gastonmorixe": "same here :+1: \njruby 1.7.11 \nmac osx mavericks\n. Thank you\n On Apr 11, 2015 3:47 AM, \"Evan Phoenix\" notifications@github.com wrote:\n\nYou have a header that is a Symbol rather than a String.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/682#issuecomment-91773638.\n. \n",
    "xhh": ":+1: \n. ",
    "zamith": "@evanphx Do you have a repo for them already?\n. @Juanmcuello Yes, it is duplicate.\nThanks.\n. ",
    "robertosanchez": "+1\n. ",
    "selvamani-subramani": "If the last worker is responding to a request, then I decrement a worker by passing TTOU signal, will it terminate? \n. ",
    "grddev": "Looks like the behaviour encountered in #398\n. ",
    "silasjmatson": "@grddev I was thinking that when I saw your PR, but wasn't sure.\n. ",
    "bkudria": "I'm still seeing this issue in master. I'm able to consistently reproduce with a certain request in my app. I'm running rbx-2.2.5, and starting puma like so: \nbash\npuma -t 1:1 -p 5000 -C/dev/null --debug -w 1\n(I pass -C to avoid loading my app's config file, since it's not needed to reproduce.)\nI get the following output:\n``\n[32050] Puma starting in cluster mode...\n[32050] * Version 2.7.1 (rbx 2.1.0), codename: Earl of Sandwich Partition\n[32050] * Min threads: 1, max threads: 1\n[32050] * Environment: development\n[32050] * Process workers: 1\n[32050] * Phased restart available\n[32050] * Listening on tcp://0.0.0.0:5000\n[32050] Use Ctrl-C to stop\n[32050] - Spawned worker: 32219\n[32050] - Worker 0 (pid: 32219) booted, phase: 0\nError in reactor loop escaped: Bad file descriptor - select(2) failed (Errno::EBADF)\nkernel/common/io.rb:938:inselect'\n/Users/bkudria/.rbenv/versions/rbx-2.2.5/gems/bundler/gems/puma-e9bc323a83b1/lib/puma/reactor.rb:28:in run_internal'\n/Users/bkudria/.rbenv/versions/rbx-2.2.5/gems/bundler/gems/puma-e9bc323a83b1/lib/puma/reactor.rb:126:inrun_in_thread'\nkernel/bootstrap/proc.rb:20:in call'\nkernel/bootstrap/thread.rb:391:inrun'\n2014-02-26 12:59:59 -0800: Listen loop error\nBad file descriptor - select(2) failed (Errno::EBADF)\n\nBacktrace:\nError in select: closed stream (IOError)\nkernel/common/io.rb:899:in select'\nkernel/bootstrap/array.rb:87:inmap'\nkernel/common/io.rb:897:in select'\n/Users/bkudria/.rbenv/versions/rbx-2.2.5/gems/bundler/gems/puma-e9bc323a83b1/lib/puma/reactor.rb:28:inrun_internal'\n/Users/bkudria/.rbenv/versions/rbx-2.2.5/gems/bundler/gems/puma-e9bc323a83b1/lib/puma/reactor.rb:126:in run_in_thread'\nkernel/bootstrap/proc.rb:20:incall'\nkernel/bootstrap/thread.rb:391:in `run'\n                    IO.select at kernel/common/io.rb:938\n  Puma::Server#handle_servers at /Users/bkudria/.rbenv/versions/rbx-2.2.5/gems/bundler/gems/puma-e9bc323a83b1/lib/puma/server.rb:288\n      { } in Puma::Server#run at /Users/bkudria/.rbenv/versions/rbx-2.2.5/gems/bundler/gems/puma-e9bc323a83b1/lib/puma/server.rb:273\n                    Proc#call at kernel/bootstrap/proc.rb:20\n               Thread#run at kernel/bootstrap/thread.rb:391\n2014-02-26 12:59:59 -0800: Listen loop error\nclosed stream (IOError)\n\nBacktrace:\n^C[32050] - Gracefully shutting down workers...\n[32050] - Goodbye!\n```\nIf I don't ^C, the \"Backtrace: \" stanza loops. If I omit the -w 1 flag, the request is processed without any errors.\nI suspect the problem is the one @grddev mentioned in the last paragraph of #398. Is there something I can do to gain insight into what the problem is?\nThe output of bundle list:\nGems included by the bundle:\n  * actionmailer (4.0.3)\n  * actionpack (4.0.3)\n  * active_model_serializers (0.8.1)\n  * activemodel (4.0.3)\n  * activerecord (4.0.3)\n  * activerecord-deprecated_finders (1.0.3)\n  * activeresource (4.0.0)\n  * activesupport (4.0.3)\n  * addressable (2.3.5)\n  * amb (0.0.5)\n  * ansi (1.4.3)\n  * arel (4.0.2)\n  * atomic (1.1.14)\n  * attr_encrypted (1.3.2)\n  * autoparse (0.3.3)\n  * autoscaler (0.8.0 02afbb6)\n  * awesome_print (1.2.0)\n  * aws-sdk (1.19.0)\n  * bcrypt-ruby (3.1.2)\n  * binding_of_caller (0.7.2)\n  * buftok (0.2.0)\n  * builder (3.1.4)\n  * bundler (1.5.3)\n  * carrierwave (0.9.0)\n  * carrierwave-aws (0.3.2)\n  * carrierwave-serializable (0.0.5)\n  * carrierwave_backgrounder (0.3.0)\n  * celluloid (0.16.0.pre 45482b1)\n  * chartkick (1.2.1)\n  * coderay (1.1.0)\n  * color (1.4.2)\n  * colorist (0.0.2)\n  * colorize (0.6.0)\n  * connection_pool (1.2.0)\n  * coolline (0.4.2)\n  * crack (0.4.1)\n  * custom_configuration (0.0.2)\n  * dalli (2.6.4)\n  * debug_inspector (0.0.2)\n  * descendants_tracker (0.0.3)\n  * diff-lcs (1.2.5)\n  * diffy (3.0.1)\n  * docile (1.1.2)\n  * dotenv (0.9.0)\n  * dotenv-rails (0.9.0)\n  * em-proxy (0.1.8)\n  * encryptor (1.3.0)\n  * equalizer (0.0.9)\n  * erubis (2.7.0)\n  * eventmachine (1.0.3)\n  * excon (0.31.0)\n  * extlib (0.9.16)\n  * fakeredis (0.4.3)\n  * faraday (0.8.8)\n  * faraday_middleware (0.9.0)\n  * ffi (1.9.3)\n  * forceps (0.6.4)\n  * foreman (0.63.0)\n  * gcm (0.0.2)\n  * google-api-client (0.6.4)\n  * grit (2.5.0)\n  * grocer (0.5.0)\n  * hashie (2.0.5)\n  * heroku-api (0.3.17)\n  * heroku-forward (0.4.0)\n  * heroku_san (4.3.2)\n  * hike (1.2.3)\n  * hirb (0.7.1)\n  * hominid (3.0.5)\n  * http (0.5.0)\n  * http_parser.rb (0.5.3)\n  * httparty (0.12.0)\n  * httpclient (2.3.4.1)\n  * i18n (0.6.9)\n  * instagram (0.10.0)\n  * jazz_hands (0.5.2 5e4b48f)\n  * jsmin (1.0.1)\n  * json (1.8.1)\n  * jwt (0.1.8)\n  * kgio (2.8.1)\n  * koala (1.7.0rc1)\n  * launchy (2.4.2)\n  * little-plugger (1.1.3)\n  * logging (1.6.2)\n  * mail (2.5.4)\n  * memcachier (0.0.2)\n  * memoizable (0.2.0)\n  * method_source (0.8.2)\n  * mime-types (1.25.1)\n  * mini_magick (3.7.0)\n  * minitest (4.7.5)\n  * minitest-reporters (0.14.23)\n  * mixpanel-ruby (1.2.0)\n  * mock_redis (0.10.0)\n  * modularity (2.0.0)\n  * multi_json (1.8.4)\n  * multi_xml (0.5.5)\n  * multipart-post (1.2.0)\n  * newrelic_rpm (3.7.1.182)\n  * nokogiri (1.5.11)\n  * oauth (0.4.7)\n  * pg (0.17.1)\n  * plist (3.1.0)\n  * polyglot (0.3.4)\n  * posix-spawn (0.3.8)\n  * postmark (1.0.1)\n  * postmark-rails (0.5.2)\n  * powerbar (1.0.11)\n  * pry (0.9.12.4)\n  * pry-doc (0.4.6)\n  * pry-git (0.2.3)\n  * pry-rails (0.3.2)\n  * pry-remote (0.1.7)\n  * pry-stack_explorer (0.4.9.1)\n  * psych (2.0.4)\n  * puma (2.7.1 e9bc323)\n  * pusher (0.12.0)\n  * racc (1.4.11)\n  * rack (1.5.2)\n  * rack-cache (1.2)\n  * rack-contrib (1.1.0)\n  * rack-cors (0.2.9)\n  * rack-protection (1.5.1)\n  * rack-proxy (0.5.9)\n  * rack-ssl-enforcer (0.2.6)\n  * rack-test (0.6.2)\n  * rails (4.0.3)\n  * rails-observers (0.1.2)\n  * rails_12factor (0.0.2)\n  * rails_serve_static_assets (0.0.2)\n  * rails_stdout_logging (0.0.3)\n  * railties (4.0.3)\n  * rake (10.1.1)\n  * recursive-open-struct (0.4.5)\n  * redis (3.0.7)\n  * redis-namespace (1.4.1)\n  * retries (0.0.5)\n  * rollout (2.0.0)\n  * rr (1.1.2)\n  * safe_yaml (0.9.7)\n  * sanitize (2.0.6)\n  * shopify_api (3.1.8)\n  * sidekiq (2.17.4)\n  * sidekiq-benchmark (0.3.3)\n  * sidekiq-status (0.3.2 cc96714)\n  * sidekiq-unique-jobs (2.7.0)\n  * signature (0.1.7)\n  * signet (0.4.5)\n  * simple_cacheable (1.5.1 483cb40)\n  * simple_oauth (0.2.0)\n  * simplecov (0.8.2)\n  * simplecov-html (0.8.0)\n  * sinatra (1.4.4)\n  * sinatra-assetpack (0.3.1)\n  * slop (3.4.7)\n  * spoon (0.0.4)\n  * sprockets (2.10.1)\n  * sprockets-rails (2.0.1)\n  * subexec (0.2.3)\n  * thor (0.18.1)\n  * thread_safe (0.1.3)\n  * tilt (1.4.1)\n  * timers (1.1.0)\n  * treetop (1.4.15)\n  * tubesock (0.2.2 1952e99)\n  * twilio-ruby (3.11.4)\n  * twitter (5.0.1)\n  * twitter-text (1.7.0)\n  * tzinfo (0.3.38)\n  * unf (0.1.3)\n  * unf_ext (0.0.6)\n  * uuidtools (2.1.4)\n  * webmock (1.14.0)\n  * websocket (1.1.2)\n  * websocket-native (1.0.0)\n  * yard (0.8.7.3)\n. ",
    "luizcarvalho": "@muratguzel you can solve your problem?\nI have same problem and add question with more details Here\n. ",
    "mdimas": "Thanks for looking at this so quickly.\nAccording to the RFC \"The HTTP protocol does not place any a priori limit on the length of a URI.\" It appears that IE enforces the shortest maximum at 2083 bytes, with other browsers supporting much higher limits. It seems like at least allowing 2083 would be nice, or maybe not enforcing anything at all since it isn't defined in the standard.\n. @evanphx any estimate on when a release with the higher maximum will be available?\n. ",
    "priyankc": "Made a fix to increase URI path length to 2048. Included tests to validate.\n. ",
    "davetron5000": "FWIW, I'm seeing this pretty frequently with Unicorn as well. celluloid/celluloid#405 is a discussion I started, but abandonded, because I thought it was just me.  Two more of my teammates just started getting this, too.\n. ",
    "aaronbartell": "I believe I am seeing the same issue when doing comparison tests between Thin and Puma using JMeter.  \nIs there additional info I can provide that would aid in debugging this?  I did a quick search of the code and noticed environment variable PUMA_DEBUG but it doesn't appear the usage of that would aid learning more about this issue. \n. ",
    "reidmorrison": "Thank you, running \"bin/puma\" instead of \"bundle exec puma\" made it work\n. All tests pass aside from one pre-existing test failure on JRuby, which is unrelated to this change.\n. To prevent a security vulnerability from being flagged, would it be safer to default to MiniSSL::VERIFY_PEER?\n. ",
    "deathbob": "Hrm, kind of.  We're doing some websocket stuff, but it had occured to us that might be related and we're pretty sure all the websockets are closed before we try the stop.  \nI found https://github.com/puma/puma/issues/306 \nWe did not have a workers line in our puma.conf, so I added workers 2 and that made the stop work.\nHowever, now restart does not work :)  Using capistrano the default cap <env> deploy kicks off a pumactl restart, which appears to complete successfully, but puma doesn't start back up.  I get a 502 gateway error and have to run a cap <env> deploy:start to get puma back up.  \nIt seems like pumactl should not report success for stop or restart when stop or restart are not actually successful.\n. > Well, it returns that way because it's a request to stop, it's not waiting for the server to stop, the same behavior as sending a signal.\nIMHO that is not so great.  Contrast that behavior with nginx.  Is there a flag or something we can use to force pumactl to wait until the actual goal we want (stopping or starting puma, not just sending it a love letter :smile:) is accomplished?   \n\nI really need to get access to an app that this is occurring in to figure it out I think, because I have not be able to replicate it at all.\nIs there any chance that I could get access to the app? I'd be happy to keep it private or even debug over ssh on one of your systems only. \n\nYeah I think we could get you access, is there a day and time next week that's good for you?  My email is on my profile https://github.com/deathbob feel free to contact me there.\n. Can we do Wednesday 2pm Eastern, 11am Pacific?\nThursday and Friday are open for me too.\n. Puma 2.9.0, ruby 2.0.0p195\nAlso tried with Puma 2.7.1, and master.  \n``` ruby\nenvironment 'production'\ndaemonize true\napp_path = '/home/deploy/toro/current'\ndirectory app_path\nstdout_redirect \"#{app_path}/log/puma.stdout.log\", \"#{app_path}/log/puma.stderr.log\"\nworkers 8\nthreads 1,2\nbind \"unix://#{app_path}/tmp/sockets/puma.sock\"\nstate_path  \"#{app_path}/tmp/pids/puma.state\"\npidfile     \"#{app_path}/tmp/pids/puma.pid\"\nactivate_control_app \"unix://#{app_path}/tmp/sockets/pumactl.sock\", { no_token: true }\n```\n. ",
    "tbuehlmann": "Good question, I'm also pondering. Though: Even YARV has native threads but is subject to the GIL. So native threads don't imply true parallelism directly.\n. Joining the party with a similar problem. With and without prune_bundler, executing bundle exec pumactl -S /home/deploy/application/alpha/shared/puma.state restart will kill Puma. When redirecting errors, this is what I get:\n/home/deploy/application/alpha/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/bin/puma-wild:8:in': undefined method split' for nil:NilClass (NoMethodError)\nUsing phased-restart seems to work.\nEdit: This was using Puma 2.8.2. When using Puma from master I get the same error message as @fred.\n. I would like to reopen this as I still get an error with prune_bundler. This is my puma.rb:\n``` ruby\nenv = 'alpha'\nuser = 'user'\nproject = 'project'\nenvironment env\ndaemonize\npidfile \"/home/#{user}/applications/#{project}/#{env}/shared/puma.pid\"\nstate_path \"/home/#{user}/applications/#{project}/#{env}/shared/puma.state\"\nworkers 4\nthreads 2,4\nbind \"unix:///home/#{user}/applications/#{project}/#{env}/shared/puma.sock\"\nprune_bundler\nstdout_redirect \"/home/#{user}/applications/#{project}/#{env}/shared/log/puma_stdout.log\", \"/home/#{user}/applications/#{project}/#{env}/shared/log/puma_stderr.log\", true\n```\nWhen I issue a restart command using pumactl the process dies because of this:\n/opt/rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/dependency.rb:298:in `to_specs': Could not find '-C' (>= 0) among 98 total gem(s) (Gem::LoadError)\n  from /opt/rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/dependency.rb:309:in `to_spec'\n  from /opt/rubies/ruby-2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_gem.rb:53:in `gem'\n  from /home/user/applications/project/alpha/shared/bundle/ruby/2.1.0/gems/puma-2.9.0/bin/puma-wild:10:in `block in <main>'\n  from /home/user/applications/project/alpha/shared/bundle/ruby/2.1.0/gems/puma-2.9.0/bin/puma-wild:8:in `each'\n  from /home/user/applications/project/alpha/shared/bundle/ruby/2.1.0/gems/puma-2.9.0/bin/puma-wild:8:in `<main>'\nThis might not get a reopen, will open a fresh issue. :)\n. Looks okay for me, too. :yellow_heart:\n. Thanks, I think this is solved by now. :)\n. Ah, didn't know that, thanks.\n. @dcorlett: Same here. We are on Heroku and our App constantly goes OOM after some time. We are now using PWK as well.\nIf I can help somehow, just tell me. :)\n. ",
    "allaire": "@evanph so that means that even with Ruby MRI 2.1.0, we should put the number of workers == to the nomber of cores?\nAlso, I know that the cluster mode is enabled when specifying the workers option. Currently, I set it to 1 since my VPS only has 1 core, do I get any benefits by doing so or workers need to be >= 2 to get all the advantages of the clustered mode?\nI would really appreciate some insight :+1: \nThank you!\n. @jjb But in clustered mode, I think can have the preload_app to have phased restart correct?\n. @jjb I think with cluster mode, you get the zero-downtime \"hot restart\", is this correct @evanphx ?\nI think it would be really beneficial to explain a bit more the various mechanisms and modes of puma (clustered, preload, restarting mechanisms hot-restart/phased, etc.) \n. @evanphx Thank you!\nAnd for hot-restart (a.k.a zero downtime), when is this available? (only in cluster mode? only in cluster mode with preload_app? only NOT in cluster mode?).\nFinally, I see that preload_app is beneficial for Copy on Write, but the README does not give much more information about it, should it always be enabled when running on Ruby 2.0 and in cluster mode? Is there any reason to not use cluster mode anyway?\n. Thanks for testing it @phemmer.\nIn my tests, when I reverted back to the old foreman upstart style (the long line one), it worked again, can you valid this on your side?\nI'm running the latest Version of puma as well.\ncc @evanphx\n. For now I got it to work by passing the rack_env env variable in the upstart script and use two different puma config file for each env, with the production one setting the directory option to current/ symlink.\nWhat I don't link about this is having to manually set the directory option, where in the past I didn't have to since it was using the current directory(cd one) automatically. Hard coded path everywhere is something I try to avoid :)\n. Maybe relevant ? https://github.com/puma/puma/issues/416\n. @evanphx Thanks for taking the time to answer back!\nI suppose that's the envvar in question? https://github.com/puma/puma/blob/edfa7a6db2d1d6a4bd7d09e5e2b6bbb73bdb99ab/lib/puma/cli.rb#L332\nAlso any idea why @phemmer code is not behaving correctly?\nFor anyone having the same problem, here's my current solution:\nin my foreman's .env file:\nPATH=/opt/rbenv/shims:/opt/rbenv/bin:/opt/rbenv/plugins/ruby_build/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\nRAILS_ENV=production\nRACK_ENV=production\nin the Procfile:\nweb: bundle exec puma -p 5010\nI also now use puma config per environment, here's config/puma/production.rb :\nthreads 1, 8\nworkers 4\npidfile 'tmp/pids/puma.pid'\nstate_path 'tmp/pids/puma.state'\nbind 'unix://tmp/sockets/puma.sock'\ndirectory '/home/user/apps/app/current'\nThe only negative thing I see, is that I have to hardcode the directory path generated by Capistrano, so it's a good idea to add a comment to not forget to change it in the puma config as well:\nset :deploy_to, \"/home/#{fetch(:user)}/apps/#{fetch(:application)}\" # Also edit directory config in config/puma/env_config.rb\nHere's my full puma capistrano task:\nhttps://gist.github.com/allaire/97221660f058551c0c5f\nSee anything I could do better @evanphx ?\nThanks!\n. @CameronNemo That's one option, but still required hardcoding the value (which is what I tried to avoid). I think it's cleaner to use the setting made for that (directory) instead of relying on some PWD magic :)\n. Also @evanphx, should we activate prune_bundler by default when we detect the presence of a Gemfile and when the user has cluster mode, but without preload_app (since prune_bundler isn't compatible)? Is there any case where prune_bundler is not wanted when the user use both Bundler and Cluster mode without preload_app?\n. Yeah I vote for this too.\nLast question about that: Is it sure that an error will happen if prune_bundler isn't set and we use phased restart with keep_releases 5 in Capistrano? I can't recall if this happened on our last project, but we often restarted the app without phased-restart!\n. @seuros makes sense, thx.\n. @evanphx Do you think that could be useful? Currently, I'm monitoring the totalmem of all processes, and if it goes let say > 800mo, I simply launch a phased-restart, and every worker restart one by one.\nThe downside of this (I think) is that I'm restarting every workers because of a guilty one that leaks more memory than others. But since it's a phased-restart, this isn't that critical I think... What do you think?\nWaiting for your input to see if we want to close this :)\n. @evanphx Having some problem with the directory config as well with phased-restart. Seems like the Gemfile in context was still pointing to the old one, where in the past (if I recall), it was using the directory option I had set (in my case /current).\nI'm using environment configs (config/puma/staging.rb, etc.). Here's my staging.rb file:\nthreads 4, 10\nworkers 2\npidfile \"tmp/pids/puma.pid\"\nstate_path \"tmp/pids/puma.state\"\nbind \"unix://tmp/sockets/puma.sock\"\ndirectory \"/home/user/apps/web/current\" # https://github.com/puma/puma/issues/539\nprune_bundler true\n. @evanphx Any update on this? In the meantime, I'll try to rollback to a puma version prior that config refactor if it fixes it.\n. @evanphx confirmed that rolling back to 2.12.3 fixed the phased restart issue in my case. Here the upstart log:\n[8899] - Starting phased worker restart, phase: 2\n[8899] + Changing to /home/user/apps/web/current\n[8899] - Stopping 10925 for phased upgrade...\n[8899] - TERM sent to 10925...\n[12934] + Gemfile in context: /home/user/apps/web/releases/20150903174757/Gemfile\n[8899] - Worker 0 (pid: 12934) booted, phase: 2\n[8899] - Stopping 10996 for phased upgrade...\n[8899] - TERM sent to 10996...\n[13003] + Gemfile in context: /home/user/apps/web/releases/20150903174757/Gemfile\n[8899] - Worker 1 (pid: 13003) booted, phase: 2\nCurrent is a syslink:\ncurrent -> /home/user/apps/web/releases/20150903174757\nWith v2.13.4:\nCurrent is a syslink:\ncurrent -> /home/user/apps/web/releases/20150903180652\n[17379] - Starting phased worker restart, phase: 1\n[17379] + Changing to /home/user/apps/web/releases/20150903180259\n[17379] - Stopping 17444 for phased upgrade...\n[17379] - TERM sent to 17444...\n[19372] + Gemfile in context: /home/user/apps/web/releases/20150903180259/Gemfile\n[17379] - Worker 0 (pid: 19372) booted, phase: 1\n[17379] - Stopping 17448 for phased upgrade...\n[17379] - TERM sent to 17448...\n[19449] + Gemfile in context: /home/user/apps/web/releases/20150903180259/Gemfile\n[17379] - Worker 1 (pid: 19449) booted, phase: 1\nAs you can see, Gemfile in context points to the previous current symlink\n. @jamesmeador Was this fixed by #793 ?\n. @grimm26 then your issue is not related to this one.\n. Rolling restarts work for us with puma (2.15.3) with Capistrano deploys\n. Here's mine:\nthreads 6, 12\nworkers 6\npidfile \"tmp/pids/puma.pid\"\nstate_path \"tmp/pids/puma.state\"\nbind \"unix://tmp/sockets/puma.sock\"\ndirectory \"/home/myproject/apps/web/current\" # https://github.com/puma/puma/issues/539\nprune_bundler true\nTry defining directory\n. @ifyouseewendy glad to hear it :)\n. @grimm26 did you try in the config file? There's also this solution: https://github.com/puma/puma/issues/539#issuecomment-44980533\n. @OUGHT Have a similar issue here: https://github.com/puma/puma/issues/770\nOur current solution is to pin to an older Puma version:\ngem \"puma\", \"~> 2.12.3\"\nBasically I think 2.13 broke this. Can you confirm this on your side? It might help @evanphx to pin this down.\nThanks!\n. Related? https://github.com/puma/puma/issues/770 & https://github.com/puma/puma/issues/776 \n. @evanphx went frenzy and merge a lot of stuff, cool.\nI see this got merged in, maybe it fixes this? https://github.com/puma/puma/commit/718c401c7b1334dc6b6563127062472eb9e58840\n. @evanphx Phased restart bug was ditched with your latest fix, I think this should cover this as well!\n. started seeing this al well by upgrading to 3.0.2, was also to 2.16. 0 @robdimarco @evanphx, here's my config:\nthreads 1, 2\nworkers 2\npidfile \"tmp/pids/puma.pid\"\nstate_path \"tmp/pids/puma.state\"\nbind \"unix://tmp/sockets/puma.sock\"\ndirectory \"/home/user/apps/project/current\"\nprune_bundler true\nThe PWD trick didn't do it. Reverting to 2.16.0 did the trick. I'm using upstart as well.\nIn the log, the Gemfile in context was the old releases/xxxxxx and the Changing to line was not /home/user/apps/project/current but the same old releases/xxxxxx\n. The weird thing is with 3.x, I see in the log Changing to /home/user/apps/app/releases/20160319210637 where with 2.x I see Changing to /home/user/apps/app/current.\nSomething really changed internally, and by looking at the code, I can't see where the worker_directory is different in 3.x\nI also tried to disable prune_bundler, but that didn't change anything.\n. Via upstart\n. @evanphx sure, here is the cap file: https://github.com/agendrix/capistrano-foundation/blob/master/lib/capistrano/tasks/puma.rake\nHere's the upstart script:\n```\nstart on starting app-web\nstop on stopping app-web\nrespawn\nCustom to App\nhttp://sorentwo.com/2014/12/08/self-healing-deployment.html\nrespawn limit 3 30\nreload signal USR1\nnormal exit 0 TERM\nenv PORT=5000\nenv PATH='/opt/rbenv/shims:/opt/rbenv/bin:/opt/rbenv/plugins/ruby_build/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games'\nenv LANG='en_CA.UTF-8'\nenv RAILS_ENV='staging'\nenv RACK_ENV='staging'\n...\nsetuid app\nchdir /home/user/apps/project/current\nexec bundle exec puma\n```\nNote that I'm masking user + project name.\n. Nice! Make sure to let me know what is it, curious :)\n. Thanks evan! What's the related commit?\nIs the phased restart testable?\n. @schneems Hi Richard, I think this commit broke the correct loading of puma environment configuration file.\nBefore 3.8.0, I was able to run RACK_ENV=production bundle exec puma start and it was loading automatically config/puma/production.rb.\nOn 3.8.0, it's not picking up the production.rb config file. You can test this by simply putting a raise inside the config/puma/production.rb file.\nMight be related? https://github.com/puma/puma/pull/1269. Yes 3.9.0 is still broken. I can open a new issue for sure !\n--\nMathieu Allaire\nLe 2 juin 2017 16:46 -0400, Richard Schneeman notifications@github.com, a \u00e9crit :\n\nI'm not sure. Can you try with 3.9.0 and if it's still broken open a new issue with an example app?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Experiencing this issue as well. I don't understand, going back to puma 3.6.0 fix the environment config not loading bug, but it seems that this version is also using a proc for @options[:environment], which got me thinking it might be something else?\n\nI think https://github.com/puma/puma/issues/1284 is similar\nIn my case, on 3.6.0, having exec bundle exec puma -p 5010 in my upstart scripts works and correctly load the right environment file (defined by RACK_ENV).. @schneems Yep!. @schneems Great catch, thanks for fixing! \ud83c\udf7b . @ViliusLuneckas I'll try to repro on 3.11, but we switched to a single config file, with some if/else around the RACK_ENV. We are experiencing the same issue.\n@nateberkopec you mention to use pumactl phased-restart but AFAIK, it used USR1 under the hood:\nhttps://github.com/puma/puma/blob/6d0efee913905c6b2b9046475cf5f18939ce6b1c/lib/puma/control_cli.rb#L204\nI'm using inspeqtor with systemd:\nExecReload=/bin/kill -USR1 $MAINPID. @dstokes Sorry to bother, but I'm running the same exact setup as you, Did you figure it out?\nThanks!. I found what the issue was, and it's not related to puma! Here's a recap:\nI was using https://github.com/zzet/ansible-rbenv-role, and recently switched to user-install mode. the role ship with the rbenv-vars plugin, and has a default vars file that sets the GEM_PATH env (https://github.com/zzet/ansible-rbenv-role/blob/master/files/vars). This causes bundler to not touch the gem path: https://github.com/bundler/bundler/blob/3f244c698d1db82d19e8cd660ae0c0f80960f8fe/spec/runtime/with_clean_env_spec.rb#L28\nThe solution is to make sure GEM_PATH doesn't get overwritten.\nThanks!. @evanphx If by any chances you know why it does that, I would love to know, since it's not 100% clear to me :). Hi @chrismo, thanks for the digging and taking the time to write this down, I really appreciate it.\n\nIf you'd rather keep all your bundles isolated in each project, you can use bundle install --path ./.bundle (or bundle install --path ./my_project_gems - path name doesn't matter). Using this will store your bundle config such that GEM_PATH will get overridden and set to your specific path.\n\nI already have BUNDLE_PATH: \"/home/project/apps/dashboard/shared/bundle\" in my .bundle/config that was set by capistrano-bundler. Bundler is not present in this PATH though (since it's not in my Gemfile, bundler is in my rbenv gems directory). So I suppose this is not related to my error, correct?\n\nTurning off Puma's prune_bundler setting appears to have worked because presumably it would be running already in a production environment where Bundler was invoked with the '--deployment' flag keeping the gems in 'vendor/bundle' \n\nThe error I was getting was because my project couldn't find the bundler gem (cannot load such file -- bundler/setup (LoadError)). I still struggle to see why with  prune_bundler false, my project was able to load the bundler gem correctly (considering that my GEM_PATH was not correctly set by rbenv-vars). Is it because I have append in my systemd init file my PATH, which includes my rbenv shims location, and prune-bundler doesn't have access to that PATH, and can't rely on it to find bundler?\nPATH=\"$HOME/.rbenv/shims:$HOME/.rbenv/bin:$HOME/.rbenv/plugins/ruby_build/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games\"\nThanks!. @chewi Thanks for the update. You said\n\nif I unset GEM_PATH or disable prune_bundler then it works.\n\nI'm curious how this could relate to the bundler symlink issue\n. ",
    "JuniorJoanis": "Hi evanphx \nWhen did you fix it? How did you, I have the same error.\nThanks\n. ",
    "Jinxy5": "Just so you know, to fix this I just needed to delete my .sock file, restart the server and it worked fine after that!\n. ",
    "toymachiner62": "I had to do the same as @Jinxy5. Had to delete my .sock file, had to kill the puma process manually, then restarted the puma process and it worked fine. The puma process wasn't stopping when I issued the stop command but I didn't notice.\n. This is still an issue. I've experienced this issue for 2 years now and I preiodically have to restart my puma server. I can replicate this in my local server using puma, and when I switch back to webrick, the issue is nonexistant.\n. Here's a sample project which demonstrates the issue. https://github.com/toymachiner62/devise-connection-failure\nHit an incorrect url and then try to hit a valid url and you should see a database connection error.\n. I've had my production app with as many as 50 and eventually all the mysql connections all get used up, it just takes longer. \n. Exactly the problem i've been having for years with my one app running on puma. Seems to be a puma related issue b/c for my production app, when i switch to webrick, i don't have the issue.\n. I can give you access to my production app if you'd like. You can really see it then.\n. That would be helpful. I actually added a puma config file with this yesterday as well and haven't had any crashes yet, but it hasn't been long enough either.\n``` ruby\npreload_app!\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\nbefore_fork do\n  ActiveRecord::Base.connection_pool.disconnect!\nend\n```\n. I've had the problem since rails 3.1 and now i'm on 4.1.8\n. Here's some questions i've asked over the years without any real luck. http://stackoverflow.com/search?q=user%3A222403+puma\n. Airbrake does not access the database, but i'm storing images in my database. Not sure if dragonfly directly touches the db or not. \nHowever I have had this error since before I had integrated dragonfly into my app.\n. I'm not explicitly using Rack::Timeout. I'm not familiar with it.\n. Looks like that fix was made in V3.0.6 and i'm on gem 'airbrake', '~> 4.1'\n. It seems that updating puma to 2.15.x resolves the issue. I have a reproducible test case on my dev app that I could trigger the error every time (even after upgrading to rails 4.2.x). Now that my puma has been upgraded my test case does not trigger the connection timeout. Thanks for the help on this guys.\n. Looks like my previous version of puma was 2.10 if that helps.\n. I'm not on puma 3.x but ever since upgrading to puma 2.15.x I've not had an issue ever. \n. ",
    "wtn": "I rebuilt my libraries and Rubinius and I am now able to run my application. There may have been an issue with the libraries I build Rubinius against.\n. ",
    "HoneyryderChuck": "Yes, I guess that's what he meant by \"This is probably why you suggest using the stdout_redirect option on the application config file\", which is not the issue being discussed.\n. I tend to think of this more of a puma \"issue\" rather than Sinatra. Even if I do this properly and apply a loop inside the stream helper call while writing to the socket (a valid solution advertized by ActionController::Live, for example), I still starve a thread meanwhile, which means, if I have 16 max threads and 16 clients connecting, I don't have enough more threads to continue serving requests. Is that right? If so, is this by design? If not, are there known workarounds? Apparently actioncable worked around it, right? \n. @evanphx \nhttps://gist.github.com/TiagoCardoso1983/591d48cfde04219f801fa7fd7966571c\nin order to \"libify\" this, whose concern is to abstract the SSE/Websocket part, sinatra's or puma? from my understanding, sinatra \"stream\" helper should support the rack socket hijacking API and pass it to an appropriate driver, right?\n. @evanphx Did you try my example? It actually does this using the reel helpers, with the benefit that the socket is held inside the celluloid io reactor (non-blocking event loop), thereby releasing the puma worker for further http work. \nSinatra helpers seem to be more focused in eventmachine (I haven't managed to make anything work without it). My question was more if puma should provide some helpers for this use case, as most of the examples around for SSE or long-polling block the puma workers, and this usually ends up bringing performance down or even DoS. \n. not my use-case. As I'm hijacking the socket, puma thread is being anyways freed, and a separate thread/event-loop is handling the SSE, I handle termination there. I don't think that a threaded server is a good solution for such long-lived connections such as SSE or websockets, for that matter. But thx for the info. \n. @evanphx , can we come back at this issue? I've seen you opened a PR to add nio4r, and this could be the solution to the problem. \nA short summary of what was discussed: puma allows support for chunked responses, but not streamed (SSE), although it could. Problem being, the current reactor is read-only, while the writes are performed on the thread pool workers. For proper streaming to be implemented, the reactor must handle reads and writes. Which will probably be handled, as there's a websocket-related PR. \nAfter the reactor is handled, Puma could allow other types of responses, which implement a well-defined API(?), and this could be better handled here.\nI really think that, if puma goes the way of supporting websockets, it could definitely support streaming and chunked responses in a more evented way. \n. https://gist.github.com/TiagoCardoso1983/591d48cfde04219f801fa7fd7966571c \nDone with sinatra, but the principle is the same, and it works with puma. \n. @nateberkopec my concern isn't memory, just correctness. Not only based on API-maintenance, but I also don't think that rack is the best place to keep http internals info, so I actually welcome that commit. As puma is the http server, it seems more correct that this info is declared there.\nHowever, my argument was making it more \"central\" to ruby. I was comparing with nodejs, where http.STATUS_CODES is available to anyone in the runtime. Ruby also has the benefit of having an http server implementation in the standard library (and that module with status codes seems abstracted enough), but it has been slowly ignored by everyone, so that updates to new http status codes seem to get more love in rack. \n. Yes, I just agreed with you. I wasn't for reverting the commit. I just meant, webrick is always there, available. One could easily just:\nruby\nrequire \"webrick/httpstatus\"\nputs WEBrick::HTTPStatus::StatusMessage\nwhich is my proposal of the ruby version of nodejs's http.STATUS_CODES.\n. My understanding was that #705 was caused by relying on rack. I don't know what rails does with such static variables in reloading, but it seems out of scope, as there could be workarounds. \nThe question was more whether it was agreed that having these protocol standard things copy-pasted in many projects is a pattern to maintain, or whether there is openness and motivation to write once/share everywhere. If I'm doing yet another http server, should I just copy paste this hash? I can, but do I have to? If so, where from? puma? rack? thin? What's the more standard/more complete? \nI can indeed start by taking it up with ruby core and maybe make my point across, but then I still have to convince maintainers to opt in. I can always start by asking their opinion on the matter :) \n. https://bugs.ruby-lang.org/issues/12935\n. > We can check if this constant is defined and use it instead of our own\nWouldn't it be best to just require the file in ruby 2.5 or higher and always use it, and keep the file as a fallback to previous rubies?\nhumm, isn't this going to break if the Net::HTTP constant isn't defined, thereby making net/http a dependency?. Sorry, understood \"check if constant is defined and\" as:\nruby\ndefined?(Net::HTTP::STATUS_CODES) || begin\n.... it is. Time to add coverage?. ups, my bad. time to add ruby version requirement in the gemspec?. @eprothro last time I checked (2/3 minor versions ago), puma performs both reads and writes from the socket in worker threads, so this is also work for them. Slow clients emitting the request bytes slowly will one way or another fill the waiting sockets buffer and increase pool contention as much as ready requests. \nIn most deployment scenarios in the wild this won't impact much, as most reverse proxy standard configs just buffer the whole request and dump it to the back-end in 1/2 packets, but in the exceptional cases, I would not see the benefit of the accept then queue approach.. Shouldn't hot restarts work with UNIX servers? At least since latest JRuby, UNIX Sockets can survive the fork+exec cycle. . Why does puma have their own \"minissl\", instead of using ruby's \"openssl\" library?. @skrobul I get that, minissl is just an OpenSSL binding. However, the ruby std library already provides one, and with a stable albeit clunky API, which allows you to do the things this PR aims at in a future-proof way. So my question was really: why does puma feel the need to support their own openssl binding which has no API parity with ruby-openssl and has no clear advantage besides arguably \"leaner\" (and therefore more limited, hence this PR)? There must be a reason, right?. Interesting bug. HTTP_VERSION shouldn't be used to keep the http version header (not required by spec, at least). However puma, unicorn and thin (at least) set it. And I bet that some middlewares rely on it.\nGood luck \ud83d\udc4d. > You can avoid allocating an array each time you call #select by passing a block to select. \nFrom https://github.com/socketry/nio4r/wiki/Selectors#selecting-io-objects-for-readiness , if it's a nice-to-have.. ",
    "batizhevsky": "+1\n. I thought about it but could not understand. Can you give a cue to solve this problem roots? Thank you\n. Yes, this is not puma bug\n. ",
    "cknoxrun": "I am having the same problem across different servers. Sometimes it doesn't restart, other times it seems that the process is stopped but never started again. This is on 2.7.1. \nEdit: Actually, this may be that I deployed between the update from 2.6 -> 2.7.1. I'll deploy a bunch more today so I'll see if the problem continues with 2.7.1.\n. @sbull thanks a ton for posting your fix!\n. ",
    "totallymike": "I've experienced something like this.  \nbundle exec pumactl -F config/puma.rb restart wherein my puma.rb contains the pidfile, working directory, binding, etc.\nIt reports \"Command restart sent success\", and then it just kills the puma workers and the master process ends.  Here's what the log says:\n[17058] - Gracefully shutting down workers...\n[17058] * Restarting...\nAvailable commands: halt, restart, phased-restart, start, stats, status, stop\n. ",
    "simsicon": "Same here, I have to stop then start puma server sometimes. I've noticed the PID actually changed when running restart or phased-restart, but still applying the old code. \n. @evanphx The development side is 2.0.0-p353 and the production is 2.0.0-p195, and puma version is 2.7.1, is that help? Thanks. \n. ",
    "gamafranco": "ruby 1.9.3p0 (2011-10-30 revision 33570) [x86_64-linux]\n. Yeah. I'm closing this one and let people create separate ones. This is so old it probably has a 1m long beard.... ",
    "paustin01": "I'm seeing the same thing.  Running \"pumactl -S /my_app/path/shared/pids/puma.state restart\" (or phased-restart) keeps the master process pid, and the 2 clustered workers restart with new pids, but the master and clustered workers continue to work out of the previous release path.\nI'm running puma (2.7.1) on ruby 1.9.3p448 on ubuntu.\nExcerpt from ps:\n$ sudo ps -ef |grep 29171\npuma     27146 29171  0 18:32 ?        00:00:02 puma: cluster worker: 29171\npuma     27150 29171  0 18:32 ?        00:00:06 puma: cluster worker: 29171\npuma     29171     1  0 Jan13 ?        00:05:11 /usr/local/ruby/1.9.3-p448/bin/ruby /my_app/path/current/bin/puma -C /my_app/path/shared/config/puma.rb\nExcerpt from lsof: (note the current working directory shown for the process is correct but gems are running out of the old path)\n```\n$ sudo lsof |grep releases\nruby      27146       puma  cwd       DIR              202,1      4096     401852 /my_app/path/releases/f732c7...\nruby      27146       puma  mem       REG              202,1     85394     279105 /my_app/path/releases/c81ea3.../vendor/bundle/ruby/1.9.1/gems/bcrypt-ruby-3.1.2/lib/bcrypt_ext.so\nruby      27146       puma  mem       REG              202,1   1013965     289051 /my_app/path/releases/c81ea3.../vendor/bundle/ruby/1.9.1/gems/eventmachine-1.0.3/lib/rubyeventmachine.so\n...\nruby      27150       puma  cwd       DIR              202,1      4096     401852 /my_app/path/releases/f732c7...\nruby      27150       puma  mem       REG              202,1     85394     279105 /my_app/path/releases/c81ea3.../vendor/bundle/ruby/1.9.1/gems/bcrypt-ruby-3.1.2/lib/bcrypt_ext.so\nruby      27150       puma  mem       REG              202,1   1013965     289051 /my_app/path/releases/c81ea3.../vendor/bundle/ruby/1.9.1/gems/eventmachine-1.0.3/lib/rubyeventmachine.so\n...\nruby      29171       puma  cwd       DIR              202,1      4096     401852 /my_app/path/releases/f732c7...\nruby      29171       puma  mem       REG              202,1     85394     279105 /my_app/path/releases/c81ea3.../vendor/bundle/ruby/1.9.1/gems/bcrypt-ruby-3.1.2/lib/bcrypt_ext.so\nruby      29171       puma  mem       REG              202,1   1013965     289051 /my_app/path/releases/c81ea3.../vendor/bundle/ruby/1.9.1/gems/eventmachine-1.0.3/lib/rubyeventmachine.so\n...\n```\nAny ideas??\n. Update:  We did an strace on the puma pid and watched it during a pumactl restart. We noticed that everything was running out of the correct path until the standard rails boot.rb file (/my_app/path/[new_release]/config/boot.rb) fired and loaded the Gemfile from the previous release's path. When looking at boot.rb it was clear that it if the environment variable BUNDLE_GEMFILE already exists, it would use that rather than re-evaluating and setting it. That env would have already been set in the puma session by /my_app/path/[old_release]/bin/puma (the binstub created by bundler) and rather than using \"/my_app/path/current/Gemfile\" it would have used the \".realpath\". In short, the final value of BUNDLE_GEMFILE would be '/my_app/path/[old_release]/Gemfile' rather than the desired '/my_app/path/current/Gemfile' and so all gemfiles would run out of that path.  Our solution is to set the BUNDLE_GEMFILE environment variable in our puma upstart script to \"/my_app/path/current/Gemfile\".\n. ",
    "cheapRoc": "I can confirm @paustin01's findings because my Puma process's proc directory enviro file holds the previous revision's BUNDLE_GEMFILE and not the new release directory which Puma is running from (as mentioned previously, observable via lsof).\ncat /proc/cat /your/puma.pid/enviro\n. ",
    "jcoleman": "Is there a reason this got closed? This is still very much an issue, and it's trivial to reproduce: all you need is any project running puma from within a symlinked directory (e.g., deployed by capistrano), install new gems in a new revision, and reload puma with SIGUSR2 with re-use the BUNDLE_GEMFILE env var in Rail's boot.rb thus not loading new gems.\n. @sorentwo I worked around this issue by adding BUNDLE_GEMFILE=/deploy_path/current/Gemfile to my init.d script for Puma. If you're using Capistrano, you could similarly add that env var to the commands it runs.\n. @evanphx I didn't realize that the prune_bundler option existed either. My test case isn't easily reproducible now though because I had to restart puma with the updated config.\n. My solution ended up not fixing it for me either. The prune_bundler option frankly just doesn't work.\n. ",
    "cjbottaro": "We're seeing this issue also using Puma 2.9.0 deployed via Chef (it uses the same current symlink strategy as Capistrano).  Puma is not using newly deployed code after being issued a SIGUSR2.  Doing a stop/start gets Puma to use the new code.\n. What's the status of this?  We are having issues with phased restarts and symlinked deploys also.\n. ",
    "mcfadden": "I'm seeing this same issue as well (phased restart doesn't load new code)\nRunning Puma 2.11.2  on Ruby 2.2.2\nWe're deploying with Capistrano.\nHere's my puma.rb:\n```\nworkers 2\nthreads 5, 5\nprune_bundler\nrackup      DefaultRackup\nenvironment production\ndirectory '/www/my-app/current'\nbind \"unix:///var/run/puma.sock\"\nstate_path \"/var/run/puma.state\"\npidfile \"/var/run/puma.pid\"\n```\nWe're issuing this command to start the phased restart:\nsudo pumactl --config-file /www/my-app/current/config/puma.rb phased-restart\nWhen we issue the phased-restart the worker processes do restart (new PIDs), but they don't get the new code.\nWhere should I look for more information to figure out what's happening?\n. After coming back to this on a fresh day, I'm really not sure what I was experiencing. At one point in time my restarts definitely weren't working properly (as in, the new code definitely wasn't being loaded). This could have easily been a misconfiguration on my part.\nAfter I noticed the problem I started testing by looking at the output from ps aux which included the directory it had loaded in the tag. Ex:\npuma: cluster worker 1: 918 [20150429172950]\nWhen I was deploying new code I could see that worker restart, but the content loaded (20150429172950) wasn't changing, so I assumed it still wasn't loading the new code. I have no idea what I did to fix it, but it's loading the new code properly (with the configuration in my previous comment).\nPerhaps I should open a new issue about updating the Process tag on the workers to match the new directory.\nAt the moment I'm not even sure where it's pulling the current release from since i'm running this out of the /www/my-app/current directory. \n. ",
    "Epigene": "To my surprise, I got this to work.\nUsing puma v2.15.3\n``` ruby\nin /config/puma.rb\napp = \"my_app\"\nroot = \"/home/deployer/apps/#{app}\"\nworkers  4\nthreads  1, 2\nrackup      DefaultRackup\nenvironment ENV['RAILS_ENV']\ndaemonize   true\ndirectory \"#{root}/current\"\npidfile \"#{root}/shared/tmp/pids/puma.pid\"\nstdout_redirect \"#{root}/shared/log/#{Time.now.to_i}_production.log\", \"#{root}/shared/log/#{Time.now.to_i}_production_errors.log\"\nbind \"unix:/tmp/puma.socket\"\nprune_bundler\n```\nIt is crucial to stop and start puma with these settings to have the master process pick up :directory and, possibly, :prune_bundler directives.\nThen you can bundle exec pumactl -p {pid_path} phased-restart\nNB, that it may take a few moments for workers to pick up the new code, and also ps aux will incorrectly output workers' directory as the master processes' original directory, when the new code will actually be taken from the new symlinked current folder.\n. I would like to see configuration moving in the shared/tmp/pids/puma.pid direction because storing the pid in a specific release's directory makes the pidfile vulnerable to being lost if last release identifier changes, such as on deploy, rollback, black magic etc.\nWhereas being stored in the shared folder it will always be easy to find.\n. Setting up correct puma and nginx configuration can be a headache, here is a gist to what we are running in production at manabalss.lv https://gist.github.com/Epigene/616d397cda078fb2d176\nThe location @puma { line in nginx.conf is key.\n. Found a solution following this blog entry\n. The issue was with #{environment} parts. Switched to static strings everywhere and got it to work.\n. This is still relevant on v3.11.2\nI am following along Rails' ActionCable standalone server setup and having a cable-booting binstub like so\n```rb\n!/bin/bash\nbundle exec puma -C cable/puma.rb cable/config.ru\n``\nignores the \"cable/config.ru\" and instead uses the defaultconfig.rufile in project root. This can be easily confirmed by renaming the defaultconfig.ru` and having puma fail at boot with:\nCable Puma initializing for 'development'\nPuma starting in single mode...\n* Version 3.11.2 (ruby 2.4.2-p198), codename: Love Song\n* Min threads: 1, max threads: 8\n* Environment: development\nERROR: No application configured, nothing to run. ",
    "treenewbee": "This issue seems to recur in 3.9.0 and 3.9.1. \n3.8.2 works totally fine. . ",
    "pavelvasev": "In our case [ruby 2.3.3p222]\npuma (3.10.0)\npuma_worker_killer (0.1.0)\nPuma reloads ok on SIGUSR2 and doesn't reload on SIGUSR1.\nIt feels like preload_app gets automatically turned on somehow.. ",
    "bf4": "This may be related, using prune_bundler and SIGUSR2 via foreman\nJust observed something interesting using puma's phased restarts, old code was running against new code in the same process.... I think it has to do with how long it takes for our app code to load, about 3-4 seconds, then boot, about  6-7 seconds\n```\n03/27/2017 11:17:19.000 -0500  \n  [32187] - Starting phased worker restart, phase: 4\n  [32187] + Changing to /srv/rails_server\n03/27/2017 11:17:20.000 -0500\n  [32187] - Stopping 22094 for phased upgrade...\n  [32187] - TERM sent to 22094...\n  [1292] + Gemfile in context: /srv/rails_server/Gemfile\ntimestamp=2017-03-27T16:17:23Z env=production frontend=www level=error thread_id=9400300 \n  got a  NoMethodError on a model that a had that method added in the new code\ntimestamp=2017-03-27T16:17:23Z env=production frontend=www level=info thread_id=9400300 \n  next request is the same controller and method, but works\n03/27/2017 11:17:33.000 -0500 \n[32187] - Worker 0 (pid: 1292) booted, phase: 4\ntimestamp=2017-03-27T16:17:33Z env=production frontend=www level=info thread_id=7420240  tag=rails message={\"app_info\":{\"pid\":1292,\"load_time\":\"4.446515 seconds\",\"boot_time\":\"7.682576 seconds\",\"config\":{\"mysql\":{\"pool\":64},\"pg\":{\"pool\":64},\"dm\":{\"pool\":64},\"puma\":{\"workers\":\"2\",\"threads\":{\"min\":\"64\",\"max\":\"64\"}},\"haproxy\":{\"max_puma_connections\":128},\"sidekiq\":{\"concurrency\":25,\"timeout\":8},\"machine\":{\"processor_count\":2,\"physical_processor_count\":1}}\n[32187] - Worker 0 (pid: 1292) booted, phase: 4\n[32187] - Stopping 22148 for phased upgrade...\n[32187] - TERM sent to 22148...\n03/27/2017 11:17:43.000 -0500 \n[32187] - Worker 1 (pid: 1397) booted, phase: 4\ntimestamp=2017-03-27T16:17:43Z env=production frontend=www level=info thread_id=7420240  tag=rails message={\"app_info\":{\"pid\":1397,\"load_time\":\"3.273302 seconds\",\"boot_time\":\"6.384623 seconds\",\"config\":{\"mysql\":{\"pool\":64},\"pg\":{\"pool\":64},\"dm\":{\"pool\":64},\"puma\":{\"workers\":\"2\",\"threads\":{\"min\":\"64\",\"max\":\"64\"}},\"haproxy\":{\"max_puma_connections\":128},\"sidekiq\":{\"concurrency\":25,\"timeout\":8},\"machine\":{\"processor_count\":2,\"physical_processor_count\":1}},\n[32187] - Worker 1 (pid: 1397) booted, phase: 4\n```\nIN our config/environment.rb tl;dr we\n```ruby\nrequire 'benchmark'\nrequire 'logger'\napp_stats = {pid: Process.pid}\ntime = Benchmark.realtime {\n  # Load the Rails application.\n  require File.expand_path('../application', FILE)\n}\napp_stats[:load_time] = \"#{format('%f', time)} seconds\"\ntime = Benchmark.realtime {\n  # Initialize the Rails application.\n  Rails.application.initialize!\n}\napp_stats[:boot_time] = \"#{format('%f', time)} seconds\"\nrequire File.expand_path('../../lib/app_formatter', FILE)\nlogger = Logger.new(STDOUT)\nlogger.formatter = AppFormatter.new\nlogger.info({app_info: app_stats}.to_json)\n```\nIn our config/puma.rb, we have, in part:\n```ruby\nworkers 2\nthreads 32, 32\nAWS ELB \"Idle timeout\" is set to 30s. In order to prevent ELB 500s from unexpected connection closes, ensure that the backend persistent connection timeout is greater than this idle timeout. https://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/config-idle-timeout.html\nworker_shutdown_timeout 32\nprune_bundler\n```\nIf I flatten out the timeline:\n03/27/2017 11:17:19.000 -0500 Starting phased worker restart\n~~ 11:17:22 app code is being loaded\n03/27/2017 11:17:23.000 -0500 got exception\n~~ 11:17:26 app starts booting\n03/27/2017 11:17:33.000 -0500  app booted, new worker booted, old worker sent TERM\nhttps://github.com/puma/puma/blob/master/DEPLOYMENT.md#restarting\n```\nUse phased-restart (SIGUSR1 or pumactl phased-restart). \nThis tells the master to kill off one worker at a time and restart them in your new code. \nThis minimizes downtime and staggers the restart nicely. \nWARNING This means that both your old code and your new code will be running concurrently. \nMost deployment solutions already cause that, but it's worth warning you about it again. \nBe careful with your migrations, etc!\n```. ",
    "x-yuri": "I experienced a similar issue with puma-3.8.2, where it fails to restart after adding more gems. But with puma >= 10.0.0 it works.\nhttps://gist.github.com/x-yuri/52024f512bed39a2a0bd4c6e82d04c9f\nP.S. Well, not that simple a script, but what did you expect?...\n. See my better explanation in the following post.\nI'm experiencing related issue right now. I'm not running cluster, and it fails in the other place. Since it tries to exec puma from removed release. Let us follow the stack up:\nlib/puma/launcher.rb: restart_args\nlib/puma/launcher.rb: generate_restart_data\nAt this point we realize, that puma gets /home/user/site/releases/42/... path from bundler ($0).\nlib/bundler/cli/exec.rb: kernel_load\nlib/bundler/cli/exec.rb: run\nlib/bundler.rb: which (ENV['PATH'] is wrong)\nlib/bundler/shared_helpers.rb: set_path\nlib/bundler.rb: bundle_path\nlib/bundler.rb: root\nlib/bundler/shared_helpers.rb: root\nlib/bundler/shared_helpers.rb: find_gemfile\nlib/bundler/shared_helpers.rb: find_file\nlib/bundler/shared_helpers.rb: search_up\nlib/bundler/shared_helpers.rb: pwd\nThat's basically it. And I don't see a good solution to the issue, do you? bundler is not obliged to use version of the path with a symlink. On the other hand,puma can totally expect that the dir exists while it's running.\nAs a workaround, we can try disable_exec_load (bundler), or restart_command (puma). Any other ideas?\n@Martin288 Does adding realpath really help?\n\nDoes not contain symlinks or useless dots, .. and ..\n\nhttps://ruby-doc.org/stdlib-2.4.2/libdoc/pathname/rdoc/Pathname.html#method-i-realpath\nYou need the version of the path with a symlink, don't you?. I just checked, disable_exec_load doesn't help (not surprisingly). restart_command does make it work, but it may lead to duplicating settings. But it as well might not, if you keep everything in config file. I made it work with just restart_command 'bundle exec puma'.\nThe other option is to not use restart (only stop + start), but it triggers some errors on stop, although seems to work.. > Did you manage to solve the issue one way or another? It's 2018 and I'm still running into this... :)\n@tomas, can you help us find a solution? You see, when you run bundle exec puma (what generally happens during development), bundler executes puma this way: ~/app/releases/1/vendor/bundle/ruby/2.5.0/bin/puma ARGS. Some number of deploys into the future, and ~/app/releases/1 gets removed. After which puma is supposed to restart itself. So it does so as bundler did (~/app/releases/1/... ARGS). If it were executed like this: ~/app/current/... ARGS, that would be a different story. But how can puma figure out which path you want it to use?\nbundler itself builds path to puma based on current directory. You could say, You're in ~/app/current when running bundle exec puma. What's the matter? But:\n```sh\n!/usr/bin/env bash\nset -eu\nrm -rf 1 2 3\nmkdir 1\nln -s 1 2\nln -s 2 3\ncd 3\nruby --version\nruby -e 'p Dir.pwd'\nruby -rpathname -e 'p Pathname.pwd'\n```\nOutput:\n```\nruby 2.5.1p57 (2018-03-29 revision 63029) [x86_64-linux]\n\"/home/yuri/_/1/1\"\n\n```\nTo give you more details, bundler first adds bundle_path/bin to PATH:\nBundler::CLI::Exec#run -> SharedHelpers.set_bundle_environment\nBundler::SharedHelpers.set_bundle_environment -> set_path\nBundler::SharedHelpers.set_path -> Bundler.bundle_path\nBundler.bundle_path -> Bundler.root\nBundler.root -> SharedHelpers.root\nBundler::SharedHelpers.root -> find_gemfile\nBundler::SharedHelpers.find_gemfile -> find_file\nBundler::SharedHelpers.find_file -> search_up\nBundler::SharedHelpers.search_up -> SharedHelpers.pwd\nBundler::SharedHelpers.pwd -> Pathname.pwd\nThen looks for executable in paths from the variable:\nBundler::CLI::Exec#run -> Bundler.which\nBundler.which -> ENV['PATH']\nAfter that, it load's (optimization) the executable:\nBundler::CLI::Exec#run -> kernel_load\nBundler::CLI::Exec#kernel_load -> file\nHere's the script that reproduces the issue I'm experiencing.\nIn case, you're experiencing another issue, please amend the script according to what you see.\n. 128 workers? O.o You're one slave-driver, aren't you? :)\nAnyways, the workaround is indeed a restart_command. But I'd rather suggest restart_command 'bundle exec puma' for being more readable. In case puma starts from current dir, not from releases/<n>.. @grosser I'd rather avoid cleaning env. By the way, it's deprecated for one reason or another. Additionally, it makes restart_command mandatory.\nAlso, I can confirm that PR by @Martin288 fixes it for cluster mode + prune_bundler. Since in this case puma restarts itself before starting workers. But unfortunately not for other cases. The solution might be to set instance variable on Puma::Launcher.initialize with Pathname($0).realpath.to_s. And use that instead of $0 down the road.\nUPD And at first glance the following changes fixes it for single mode, cluster mode, cluster mode + prune_bundler when puma is invoked with bundle exec puma:\n```diff\n--- restart-puma/bundle/ruby/2.5.0/gems/puma-3.11.4/lib/puma/launcher.rb.orig   2018-07-06 09:52:32.450769132 +0300\n+++ restart-puma/bundle/ruby/2.5.0/gems/puma-3.11.4/lib/puma/launcher.rb    2018-07-06 09:55:35.604106620 +0300\n@@ -8,6 +8,8 @@\nrequire 'puma/binder'\n+require 'pathname'\n+\n module Puma\n   # Puma::Launcher is the single entry point for starting a Puma server based on user\n   # configuration. It is responsible for taking user supplied arguments and resolving them\n@@ -47,6 +49,7 @@\n       @argv          = launcher_args[:argv] || []\n       @original_argv = @argv.dup\n       @config        = conf\n+      @arg0 = Pathname($0).realpath.to_s\n   @binder        = Binder.new(@events)\n   @binder.import_from_env\n\n@@ -354,10 +357,10 @@\n       # it the same, otherwise add -S on there because it was\n       # picked up in PATH.\n       #\n-      if File.exist?($0)\n-        arg0 = [Gem.ruby, $0]\n+      if File.exist?(@arg0)\n+        arg0 = [Gem.ruby, @arg0]\n       else\n-        arg0 = [Gem.ruby, \"-S\", $0]\n+        arg0 = [Gem.ruby, \"-S\", @arg0]\n       end\n   # Detect and reinject -Ilib from the command line, used for testing without bundler\n\n```\nAdditionally, I've discovered that prune_bundler makes puma from ~/.gem/ruby/2.5.1 start, not from vendor/bundle/ruby/2.5.0. Is that expected?. With the following puma config (prune_bundler and workers are from the issue):\nrb\nrequire 'puma/runner'\nPuma::Runner.prepend(Module.new do\n    def before_restart\n        ENV.replace(Bundler.clean_env)\n    super\n  end\nend)\nworkers 2\nprune_bundler\nIt (puma-3.12.0) fails to restart but for another reason:\nrb\n/home/user/app/current/config/puma/production.rb:19:in `before_restart': uninitialized constant #<Class:#<Puma::DSL:0x007fe32aeb7ef8>>::Bundler (NameError)\n    from /home/user/app/releases/11/vendor/bundle/ruby/2.3.0/gems/puma-3.12.0/lib/puma/launcher.rb:194:in `run'\n    from /home/user/app/releases/11/vendor/bundle/ruby/2.3.0/gems/puma-3.12.0/lib/puma/cli.rb:78:in `run'\n    from /home/user/app/releases/11/vendor/bundle/ruby/2.3.0/gems/puma-3.12.0/bin/puma-wild:31:in `<main>\nSo, I remove prune_bunder, and this error goes away, but we're back at square one. The issue is not resolved. I try it without workers 2, same result. The result is also identical with the following block:\nrb\non_restart do\n  ENV.replace(Bundler.clean_env)\nend\nSo, it must be safe to assume, that bundler/bundler#5700 has nothing to do with it. And if you look at the report closely enough, you'll see that it has nothing to do with environment variables.",
    "radar": "There's already a separate gem: https://github.com/xijo/capistrano-puma/\n. ",
    "stas": "Btw, puma/capistrano is out of date with the latest changes in capistrano v3.\n. capistrano-puma gem works great. Feel free to extract it for Capistrano v2 users.\n. @modology @joneslee85 still interested in making the PR?\n. ",
    "modology": "What's your thought? Should we extract it to a separate gem? If you are okay, I could submit PR\n. ",
    "onemanstartup": "same issue\n. ",
    "ramon": "+1\n. ",
    "pkieltyka": "It's daemonizing for me on Rails 4.0.2 and Ruby 2.0.0p247 using the config file, however, when trying it with JRuby I get: ! Error starting new process as daemon, exitting. Here's the config file: https://gist.github.com/pkieltyka/1b3ff1dac156a9d62fe0\nstarting it with: bundle exec pumactl -F /Users/peter/Dev/myapp/config/puma/production.rb start\n(JRuby 1.7.8)\n. Btw, I just tried bundle exec puma -C /Users/peter/Dev/myapp/config/puma/production.rb with the same config and it daemonized fine on JRuby. And \"pumactl -F x stop\" can still properly stop the daemonized process (maybe obvious, but thought to mention it..)\n. So I did more testing and I pin pointed it down to Rack::Lock middleware being added when in development mode. I tested the app in production mode, and the lock isn't there- and everything works as expected.. in parallel! I thought Rails was in thread-safe mode, but its just when the environment config 'cache_classes' and 'eager_load' are true. Pheww :) \n. ",
    "tiekuhn": "same problem here\n. ",
    "smlsml": "I just encountered the same problem, I feel like an older version of puma used to compile on windows. Trying to find that now.\nLooks like OpenSSL headers is probably the issue: https://github.com/puma/puma/issues/202\n. ",
    "ooer": "Thank you. I'm still very interested in the solution of this problem.\n. ",
    "mro": "may be fixed with 8c5fd4f0 but plz verify.\n. ",
    "kingpin2k": "reinstalled ruby\n. ",
    "n8agrin": "@evanphx the cert is in X509 format. I played around with this more and it has to do with the Java library currently in MiniSSL.java not liking self-signed certs unless they are packaged into a specific keystore format.\nCan you provide me with any more info about what the status of MiniSSL.java is in Puma? We're using an outdated version of Puma under JRuby in order to support SSL and I'd like to be able to update us. I'm doing my best to work through the SSL code in MiniSSL.java in order to help as much as I can, but any info you could provide me about the status of this code and what remains to be finished would be very helpful.\n. ",
    "dmarcotte": "Just sent https://github.com/puma/puma/pull/530 addressing this\n. @evanphx note the test failures here should be transient.  Check out a green run of these changes here: https://travis-ci.org/looker/puma/builds/60907494\n. Thanks @evanphx!  Feel free to let me know when any of the JRuby stuff needs some attention in the future, and I'd be glad to lend a hand.\n. ",
    "asavartsov": "Having same problem. Adding workers doesn't change anything for me.\nAlso, pumactl restart stops puma only if it was started as pumactl -F ... start. If I start puma like puma -C ... restarting with pumactl works fine.\n. I am using ruby 2.1.0p0 (2013-12-25 revision 44422) [x86_64-linux].\nAlso, I think the problem might be in calculating argv for restart method. I've added log argv right after https://github.com/puma/puma/blob/master/lib/puma/cli.rb#L427 and got\n$ bundle exec pumactl -F config/puma.rb restart\n* Restarting...\n/home/deploy/.rvm/rubies/ruby-2.1.0/bin/ruby\n/home/deploy/.rvm/gems/ruby-2.1.0/bin/pumactl\nCommand restart sent success\nAvailable commands: halt, restart, phased-restart, start, stats, status, stop\nSo at restart it just calls /home/deploy/.rvm/rubies/ruby-2.1.0/bin/ruby /home/deploy/.rvm/gems/ruby-2.1.0/bin/pumactl.\n. % rails new puma_app\n% cd puma_app\nAdd puma to gemfile\npuma_app % bundle install\nCreate tmp/pids directory and file config/puma.rb with following:\nthreads 1, 16\nenvironment 'development'\nbind 'tcp://0.0.0.0:3000'\npidfile 'tmp/pids/puma.pid'\nstate_path 'tmp/pids/puma.state'\npreload_app!\nDo\n```\npuma_app % pumactl -F config/puma.rb start &\n[1] 9056\npuma_app % Puma starting in single mode...\n Version 2.7.1, codename: Earl of Sandwich Partition\n Min threads: 1, max threads: 16\n Environment: development\n Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\npuma_app % pumactl -F config/puma.rb restart\nCommand restart sent success* Restarting...\npuma_app % Available commands: halt, restart, phased-restart, start, stats, status, stop\n[1]  + 9056 exit 1     pumactl -F config/puma.rb start\npuma_app %\n```\n. ",
    "mfilej": "2.1.0p0 here as well.\n. ",
    "igorfrenkel": "Confirmed, getting the same issue on ruby 2.0.0p353\n. ",
    "HiroProt": "Bump...same with jruby 1.7.5 here.\n. ",
    "zmoshansky": "Same on ruby 1.9.3p448 (2013-06-27 revision 41675) [x86_64-linux]\npuma version 2.7.1\n. ",
    "frconil": "Still happening on puma 2.8.1 with ruby 1.9.3p194 (2012-04-20 revision 35410) [x86_64-linux]\n. ",
    "7even": "Switching to a cluster mode with 2 workers and phased restarts seems to fix the problem.\n. ",
    "openface": "Confirmed problem on ruby 2.1.1p76 (2014-02-24 revision 45161) [x86_64-linux]\nwith Puma 2.8.1\n. Another scenario is that this is a nuance of htop and perhaps there is a better way to determine individual thread usage in Puma.\nI figured I'd ask anyway, thanks!\n. ",
    "dcoxall": "Having the same trouble with Puma 2.8.1. The puma-wild script passes a -q flag to the gem method which then fails.\n. ",
    "irobayna": "same problem on \nruby 2.1.1p76 (2014-02-24 revision 45161) [x86_64-linux]\nand \npuma version 2.8.2\n. After upgrading gem to 3.0.2 I get the same error.  \nNo configuration changes from previous versions 2.x.x -- something has changed which obviusly is taking some settings in config\\puma.rb in development mode which are only intended for production (at least on my case)\n. ```\n!/usr/bin/env puma\nrackup '/home/deployer/apps/xyzAPI/current/config.ru'\nrails_env = ENV['RAILS_ENV'] || 'production'\nbasedir = '/home/deployer/apps/xyzAPI'\ndirectory \"#{basedir}/current\"\nenvironment 'production'\ndaemonize true\nbind  \"unix://#{basedir}/shared/tmp/puma/puma.sock\"\npidfile \"#{basedir}/shared/tmp/pids/puma.pid\"\nstate_path \"#{basedir}/shared/tmp/pids/puma.state\"\nthreads 8,32\nworkers 3\n``\n. @evanphx the problem is when I run ondevelopment` mode\nSomething has changed from 2.16.0 to latest release (have tried 3.0.2 and 3.1.0)\nHere is my output after reluctantly disabling the directory variable in the config file:\n09:52:14 rails.1   | => Booting Puma\n09:52:14 rails.1   | => Rails 4.2.5.2 application starting in development on http://localhost:3000\n09:52:14 rails.1   | => Run `rails server -h` for more startup options\n09:52:14 rails.1   | => Ctrl-C to shutdown server\n09:52:15 rails.1   | [80022] Puma starting in cluster mode...\n09:52:15 rails.1   | [80022] * Version 3.1.0 (ruby 2.3.0-p0), codename: El Ni\u00f1o Winter Wonderland\n09:52:15 rails.1   | [80022] * Min threads: 8, max threads: 24\n09:52:15 rails.1   | [80022] * Environment: development\n09:52:15 rails.1   | [80022] * Process workers: 3\n09:52:15 rails.1   | [80022] * Preloading application\n09:52:15 rails.1   | [80022] * Listening on tcp://localhost:3000\n09:52:15 rails.1   | [80022] * Daemonizing...\n09:52:15 rails.1   | Exiting\n09:52:15 rails.1   | Exiting\n09:52:15 rails.1   | exited with code 0\n. @evanphx After spending some time looking at this, I found my configuration works just find on staging and production.  I have added a guard clause to exit if running on development\nIf my guard clause makes sense, you might want to consider updating the documentation to help other developers who might be using a puma config file and want to use the latest versions of puma.  \nBTW, I am running 3.1.0 now (development/staging/production) and thank you for all your hard work!\n. Same issue, looking at pumactl --help still shows -F as a valid option\npumactl (-p PID | -P pidfile | -S status_file | -C url -T token | -F config.rb) (halt|restart|phased-restart|start|stats|status|stop|reload-worker-directory)\n    -S, --state PATH                 Where the state file to use is\n    -Q, --quiet                      Not display messages\n    -P, --pidfile PATH               Pid file\n    -p, --pid PID                    Pid\n    -C, --control-url URL            The bind url to use for the control server\n    -T, --control-token TOKEN        The token to use as authentication for the control server\n    -F, --config-file PATH           Puma config script\n    -H, --help                       Show this message\n    -V, --version                    Show version\n. ",
    "StephanX": "I'm still experiencing this with a normal restart in single mode.  pumactl restart kills the process; running it again starts the process.\nruby 2.0.0p451\npuma (2.8.2) from git HEAD: \n  remote: git://github.com/puma/puma.git\n  revision: 5d189197fb8f476ad94bfdbbb63d133f684d0ee3\ntranscript of behavior here:\nhttps://gist.github.com/StephanX/5f8ac46e28a5e58fc917\n. ",
    "rahilsondhi": "I started using the edge (github master) version of puma and it solved my problem, thanks to #537 \n. ",
    "sudara": "There aren't enough emoji in the emojiverse to express my thanks. Will be testing shortly with monit!\n. One little wrinkle here. Lets say we are running 4 workers:\npuma: cluster worker 0: 14488                                           \n  puma: cluster worker 1: 14488                                           \n  puma: cluster worker 2: 14488                                           \n  puma: cluster worker 3: 14488\nIf I kill off worker #2 it ends up being replaced with a worker #4\npuma: cluster worker 0: 14488                                           \n  puma: cluster worker 1: 14488                                           \n  puma: cluster worker 3: 14488    \n  puma: cluster worker 4: 14488\nHowever, normally one would have monit config with named pids like puma_worker_0.pid and depend on there being a static set of workers, in this case named 0 through 3.\n```\ncheck process puma_worker_0\n  with pidfile /myapp/tmp/pids/puma_worker_0.pid\n  if totalmem is greater than 130 MB for 2 cycles then exec \"/etc/monit/scripts/puma kill_worker 0\"\ncheck process puma_worker_1\n  with pidfile /myapp/tmp/pids/puma_worker_1.pid\n  if totalmem is greater than 130 MB for 2 cycles then exec \"/etc/monit/scripts/puma kill_worker 1\"\ncheck process puma_worker_2\n  with pidfile /myapp/tmp/pids/puma_worker_2.pid\n  if totalmem is greater than 130 MB for 2 cycles then exec \"/etc/monit/scripts/puma kill_worker 2\"\n...\n```\nA fix for this situation would be have next_worker_index always return \"the next lowest index available between 0 and @options[:workers] like so:\ndef next_worker_index\n  all_positions =  0...@options[:workers]\n  occupied_positions = @workers.map { |w| w.index }\n  available_positions = all_positions.to_a - occupied_positions \n  available_positions.first\nend\n. ",
    "beckmx": "I just did like the example\npuma -b 'ssl://127.0.0.1:9292?key=path_to_key&cert=path_to_cert' but\nit looks like it didnt work\n2014/1/23 Evan Phoenix notifications@github.com\n\nHow did you activate it?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/442#issuecomment-33150058\n.\n. ahh I see, actually I put that in another production environment, I thought\nthe 127.0.0.1 was just internal, so I imagine I must write my own server IP\nwith the SSL port right?\n\n2014/1/23 Evan Phoenix notifications@github.com\n\nDidn't work how? What happens when you go to \"https://127.0.0.1:9292\" ?\nOn Thu, Jan 23, 2014 at 10:07 AM, beckmx notifications@github.com\nwrote:\n\nI just did like the example\npuma -b 'ssl://127.0.0.1:9292?key=path_to_key&cert=path_to_cert' but\nit looks like it didnt work\n2014/1/23 Evan Phoenix notifications@github.com\n\nHow did you activate it?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/puma/puma/issues/442#issuecomment-33150058>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/puma/puma/issues/442#issuecomment-33150953>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/442#issuecomment-33151044\n.\n. well I used this command\n\nrails server puma -b -p 80\n'ssl://199.XX.XX.XX:443?key=/usr/src//key_.key&cert=/usr/src/cert_.csr' but\nit seems it doesnt turn on\nand I also tested just\npuma -b -p 80\n'ssl://199.XX.XX.XX:443?key=/usr/src//key_.key&cert=/usr/src/cert_.csr'\n----> and it does run but it doesnt respond on https\nany ideas on this?\n2014/1/23 Andres Santos asb.studios@gmail.com\n\nahh I see, actually I put that in another production environment, I\nthought the 127.0.0.1 was just internal, so I imagine I must write my own\nserver IP with the SSL port right?\n2014/1/23 Evan Phoenix notifications@github.com\n\nDidn't work how? What happens when you go to \"https://127.0.0.1:9292\" ?\nOn Thu, Jan 23, 2014 at 10:07 AM, beckmx notifications@github.com\nwrote:\n\nI just did like the example\npuma -b 'ssl://127.0.0.1:9292?key=path_to_key&cert=path_to_cert' but\nit looks like it didnt work\n2014/1/23 Evan Phoenix notifications@github.com\n\nHow did you activate it?\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/puma/puma/issues/442#issuecomment-33150058>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/puma/puma/issues/442#issuecomment-33150953>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/442#issuecomment-33151044\n.\n. Well not exactly, what I see with my version 2.7.1 is that the broser\nspinner keeps loading and the console is not showing any data actually, I\nread that downgrading to 2.6 could help but havent tried\n\n\n2014-02-19 0:02 GMT-06:00 Richard Schneeman notifications@github.com:\n\nLooks like @beckmx https://github.com/beckmx got SSL working. Can we\nclose this issue?\n\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/442#issuecomment-35468954\n.\n. \n",
    "mrPsycho": "where i can see any logs? \n. and i belive, that plugin is not cause of this problem.\nunicorn work nice now \n. if it can't find now, why it could find earlier? \nyes, directory correct:\n\n[liot_work@liot config]$ which puma\n~/.rvm/gems/ruby-1.9.3-p448/bin/puma\n\nok. i'll try change config, thanks!\n. ",
    "aivils": "yep it stops working.\nThis works for me:\nbash\nRAILS_ENV=production bundle exec puma -C config/puma.rb config.ru\nAdding at end of line config.ru is very important. Seems puma pick up an empty argument and then said \"No application configured, nothing to run\".\n. ",
    "jeffrafter": "This is an old issue, but in case someone else gets here, you might run into this if your config/puma.rb references RAILS_ENV instead of RACK_ENV or if the PORT is missing. Double check your ENV. Also, if your config.ru is older it might not have the correct path. A good config.ru looks like:\n```ruby\nThis file is used by Rack-based servers to start the application.\nrequire_relative 'config/environment'\nrun Rails.application\n```. ",
    "le0pard": "Thanks, looks like it help :+1: \n. ",
    "envygeeks": "If this is a Rails problem, please do let me know and I'll move it there but for now I'll leave it here.\n. ",
    "troex": "Btw this error doesn't happen if I start in clustered mode without preload_app, and it doesn't happen in clustered mode with preload_app and proper on_worker_boot setting, but I got absolutely the same behavior and error as with preload_app without on_worker_boot hook.\n. Sorry for bothering, but how do I tell it to reconnect if on_worker_boot is not triggered in single mode?\n. ",
    "tmm1": "/cc https://github.com/tmm1/gctools/pull/1\n. ",
    "akshayrawat": "@evanphx : When using MRI Ruby 2.1.2 with Puma (say 1 worker with 8 threads), when is the GC run? Is it run by the parent worker process when all those threads become idle, or would it be run by the parent process as needed, even when those threads are busy processing requests?\nAnd how would this behaviour be different in Ruby 2.0 (without deferred GC).\nAlso asked here.\n. ",
    "ketan": "I noticed that puma uses a rack.after_reply rack environment variable to perform stuff after a request is served out. We're using puma in worker mode for some of our legacy apps - and this rack middleware (adopted from passenger) seems to do fine for us.\nWould the maintainers be interested in a pull request?\nBe warned - this is a bad idea if you're using puma in threaded mode.\n``` ruby\nconfig.ru\n...\nuse PumaOOBGC, 10, Rails.logger\n...\n```\n``` ruby\nclass PumaOOBGC\n  def initialize(app, frequency, logger=nil)\n    @app           = app\n    @frequency     = frequency\n    @logger        = logger\n    @request_count = 0\n    @mutex         = Mutex.new\n  end\ndef call(env)\n    status, header, body = @app.call(env)\nif ary = env['rack.after_reply']\n  ary << lambda {maybe_perform_gc}\nend\n[status, header, body]\n\nend\ndef maybe_perform_gc\n    @mutex.synchronize do\n      @request_count += 1\n      if @request_count == @frequency\n        @request_count = 0\n        t0 = Time.now\n        disabled = GC.enable\n        GC.start\n        GC.disable if disabled\n        @logger.debug \"[OOBGC] [#{Process.pid}] Finished GC in #{Time.now - t0} sec\" if @logger\n      end\n    end\n  end\nend\n```\n. ",
    "wjessop": "I'd be really interested in Puma OOB GC functionality. Much as I like threads I really don't see much shift towards thread safety in a lot of Ruby gems which makes most larger apps with dependencies pretty much only safe to run single threaded, so a non-threaded only implementation would still be helpful.. I have no doubt that Puma, Sidekiq etc. are threadsafe, it's going to be a really basic Rails app that relies on just the big names though.\nThe app I'm working on right now has 374 dependencies (direct and indirect). A relatively small personal project has 112. I picked the first five dependencies at random of that larger project (that wasn't a Rails or a Rails dep, or a test dep) and checked the repos for mention of thread safety. They don't mention it.\nI suspect (and I'd be interested to see real stats rather than my gut feelings) that most non-trivial applications will depend on a fairly large number of gems, very few of which will be either written with thread-safety in mind, or will have the thread-safety documented.\n\nNot sure if you saw but GitHub actually turned off their OOBGC and got something a 10% decrease in overall CPU utilization.\n\nI did see that just today, it was very interesting. The biggest thing I'm interested in there was the 25% reduction in response times. If that's really the case then it's a pretty big reason not to implement OOB GC, but I think I'd like more information, for instance, were they queueing requests behind the GC in a process rather than routing requests round it?\nIn terms of CPU though, we already trade CPU for user experience with when we gzip responses. There's a tradeoff there, and it's something we need to balance, but we're currently averaging about 14ms GC time in our responses (Ruby 2.3.4, yes we need to upgrade) so we'd be willing to trade at least some CPU time to drop that figure.\nI got @ketan's middleware working and I'll do some testing to see what the tradeoffs are in our environment and I'll try to report back.\nThanks for the response!. ",
    "wjordan": "I've added PR #1648 that implements an out_of_band hook that can be used for implementing out-of-band GC in Puma.\nOOBGC still provides my own production workload ~10% faster response times on Ruby 2.5 (see https://github.com/tmm1/gctools/issues/16#issuecomment-390381824 for some details/discussion). Whether OOBGC ends up being useful or not will depend a lot on workload, exact tuning of GC variables, and also the exact OOBGC algorithm used.. I'm currently testing Puma with a large, high-traffic Rails web-application, specifically for multi-threaded workers as a mitigation against cascading failures caused by I/O latency spikes. To compare performance on my production workload, I deployed a 'canary' frontend running Puma v3.12.0 configured with min 0, max 4 threads alongside a fleet of 'baseline' frontends running Unicorn. Both configurations fork 40 worker-processes with application preloaded and a connection-pool size of 5.\nI'm seeing ~2x higher average database-query latency, and significantly higher variation (std dev) using Puma compared to Unicorn, within metrics captured by New Relic:\nUnicorn:\n\nPuma:\n\nThis query latency results in a noticeably more spiky 99%ile for DB-query transactions overall:\nUnicorn:\n\nPuma:\n\nI suspect the results I'm seeing are related to the benchmark results discussed in this issue, however my use-case is a 'normal' production workload (mostly CPU-bound, servers kept under 50% CPU-utilization).. Sounds plausible that thread contention within a single worker process (made worse by Ruby's global VM lock, I assume) is behind the increased DB-query latency variation.\nCorrect me if I'm mistaken, I don't think the performance degradation in the multi-thread case is only a matter of traffic load being too high. Rather, it seems that requests don't get distributed across processes as evenly as in the single-thread case, causing some requests to encounter thread/GVL contention on an already-busy process while other processes on the server are sitting idle without any requests.\nGiven my own use-case, I'm inclined to think that this scenario is not at all pathological, and may indeed be \"quietly afflicting thousands of Ruby applications running under Puma.\" Further, I think the issue is something that can be corrected in Puma, but not necessarily easily.\nFor comparison, Passenger has implemented least-busy-process-first request load balancing to address this issue. A code-comment in Passenger's AcceptLoadBalancer implementation describes the issue and its solution:\n\nFurthermore, it can also be very easy for threads to become unbalanced. If a burst of clients connect to the server socket, then it is very likely that a single Server accepts all of those clients. This can result in situations where, for example, thread 1 has 40 clients and thread 2 has only 3.\nThe AcceptLoadBalancer solves this problem by being the sole entity that listens on the server socket. All client sockets that it accepts are distributed to all registered Server objects, in a round-robin manner.\n\nDoes this sound like a plausible description of the issue underlying the performance degradation here? If so, then one possibility of fixing would be to implement a similar load-balancer in Puma that accepts client connections and forwards them to individual worker threads in a least-busy, round-robin manner.\nThere might also be a short-cut possible for Puma to distribute request load more evenly by adding a bit of cross-process synchronization to the existing thread auto-scaling behavior. I'm going to investigate this alternative a bit and see if I can come up with anything tangible.. I put together a first attempt at partially addressing this issue in PR #1646.\nRather than adding extra communication/synchronization between the workers or proxying requests through the master, I found that non-idle workers can do a non-blocking poll on the socket for queued work as a form of indirect communication, keeping the PR relatively simple compared to other alternatives.\nThis seems to help balance load across processes slightly and gets much closer to single-thread performance in my tests, but the caveat of this simple implementation is that it's only effective when the average thread depth is <= 1. Balancing requests across higher thread-depths might result in diminishing returns anyway, due to the GVL.\n@mwpastore I'd be interested in hearing if this PR has any impact on the benchmarks you've been checking in this issue. In my local tests against roda-sequel-postgres I'm seeing some improvement.. > I don't think we can get metrics from the TCP socket as to how many requests are waiting.\nThought I'd jump in here and note that it is technically possible to get info on waiting requests in the TCP socket through the sock_diag interface (or indirectly via the ss utility, or reading /proc/net/tcp, etc). The raindrops Rack middleware provides stats on the total number of queued clients on each TCP listener. It collects its info from a C extension (linux_inet_diag.c).\nI've found that monitoring this metric closely provides a very good early-warning for potential cascading failures in a production system.. I tried and failed to reproduce the original issue as described. I ran a local load-test against a rails new project with a simple head :ok controller-action, running a local-development Puma server (bin/rails s), testing with wrk (wrk http://0.0.0.0:3000/test/test -d 60 -t 50 -c 50) on Ubuntu 18.04, Ruby 2.4.1, Puma 3.11.4, and Rails 5.1.6. RSS remained constant around ~64MB throughout the length of the load-test.\nWhat's missing is a minimal, complete and verifiable example that reliably reproduces the issue across a broader range of environments. Until more specific steps can be found, it could very well be any number of other issues, unrelated to Puma, that could be causing abnormal memory growth in specific applications/environments.. Here's a first pass at benchmarking this PR using the TechEmpower Ruby Sinatra Benchmarking Test mentioned in https://github.com/puma/puma/issues/1254#issue-216906287:\nI ran these tests on a server with 40 vCPUs, configured with 1, 5, or 5* (running this PR) threads, concurrency 40, running the 'query' test with a query-level of 20. Here is the command-line used:\n./tfb --test sinatra --concurrency-levels 40 --type query --query-levels 20\nResults:\nSummary\n\n1 thread: 3458.32 req/sec\n5 threads: 2125.11 req/sec (39% less than 1-thread)\n5* threads: 2993.21 req/sec (13% less than 1-thread, 41% more than 5-thread)\n\nFull data\n\n1 thread\n\n```\nsinatra: [10] Puma starting in cluster mode...\nsinatra: [10] * Version 3.12.0 (ruby 2.4.4-p296), codename: Llamas in Pajamas\nsinatra: [10] * Min threads: 1, max threads: 1\nsinatra: [10] * Environment: production\nsinatra: [10] * Process workers: 40\n\n Queries: 20 for query\n wrk -H 'Host: tfb-server' -H 'Accept: application/json,text/html;q=0.9,application/xhtml+xml;q=0.9,application/xml;q=0.8,*/*;q=0.7' -H 'Connection: keep-alive' --latency -d 15 -c 40 --timeout 8 -t 40 \"http://tfb-server:8080/queries?queries=20\"\n---------------------------------------------------------\nRunning 15s test @ http://tfb-server:8080/queries?queries=20\n  40 threads and 40 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    11.35ms    2.78ms  71.50ms   80.34%\n    Req/Sec    87.03      9.24   121.00     73.40%\n  Latency Distribution\n     50%   10.46ms\n     75%   11.77ms\n     90%   15.20ms\n     99%   20.50ms\n  52222 requests in 15.10s, 39.07MB read\n  Socket errors: connect 0, read 52220, write 0, timeout 0\nRequests/sec:   3458.32\nTransfer/sec:      2.59MB\nSTARTTIME 1536794976\nENDTIME 1536794991\nBenchmark results:\n{'results': [{'connect': 0,\n              'endTime': 1536794991,\n              'latencyAvg': '11.35ms',\n              'latencyMax': '71.50ms',\n              'latencyStdev': '2.78ms',\n              'read': 52220,\n              'startTime': 1536794976,\n              'timeout': 0,\n              'totalRequests': 52222,\n              'write': 0}]}\n```\n\n\n5 threads\n\n```\n Queries: 20 for query\n wrk -H 'Host: tfb-server' -H 'Accept: application/json,text/html;q=0.9,application/xhtml+xml;q=0.9,application/xml;q=0.8,*/*;q=0.7' -H 'Connection: keep-alive' --latency -d 15 -c 40 --timeout 8 -t 40 \"http://tfb-server:8080/queries?queries=20\"\n---------------------------------------------------------\nRunning 15s test @ http://tfb-server:8080/queries?queries=20\n  40 threads and 40 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    18.58ms    7.01ms  57.54ms   70.36%\n    Req/Sec    53.41      9.63    80.00     71.38%\n  Latency Distribution\n     50%   17.28ms\n     75%   22.53ms\n     90%   27.46ms\n     99%   41.22ms\n  32087 requests in 15.10s, 24.01MB read\n  Socket errors: connect 0, read 32087, write 0, timeout 0\nRequests/sec:   2125.11\nTransfer/sec:      1.59MB\nSTARTTIME 1536794526\nENDTIME 1536794541\nBenchmark results:\n{'results': [{'connect': 0,\n              'endTime': 1536794541,\n              'latencyAvg': '18.58ms',\n              'latencyMax': '57.54ms',\n              'latencyStdev': '7.01ms',\n              'read': 32087,\n              'startTime': 1536794526,\n              'timeout': 0,\n              'totalRequests': 32087,\n              'write': 0}]}\n```\n\n\n5* threads\n\n```\nsinatra: [9] Puma starting in cluster mode...\nsinatra: [9] * Version 3.12.0 (ruby 2.4.4-p296), codename: Llamas in Pajamas\nsinatra: [9] * Min threads: 5, max threads: 5\nsinatra: [9] * Environment: production\nsinatra: [9] * Process workers: 40\n\n Queries: 20 for query\n wrk -H 'Host: tfb-server' -H 'Accept: application/json,text/html;q=0.9,application/xhtml+xml;q=0.9,application/xml;q=0.8,*/*;q=0.7' -H 'Connection: keep-alive' --latency -d 15 -c 40 --timeout 8 -t 40 \"http://tfb-server:8080/queries?queries=20\"\n---------------------------------------------------------\nRunning 15s test @ http://tfb-server:8080/queries?queries=20\n  40 threads and 40 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency    13.17ms    3.97ms  50.42ms   74.66%\n    Req/Sec    75.13      9.54   111.00     72.93%\n  Latency Distribution\n     50%   12.14ms\n     75%   15.14ms\n     90%   18.62ms\n     99%   25.85ms\n  45199 requests in 15.10s, 33.82MB read\n  Socket errors: connect 0, read 45197, write 0, timeout 0\nRequests/sec:   2993.21\nTransfer/sec:      2.24MB\nSTARTTIME 1536794378\nENDTIME 1536794393\nBenchmark results:\n{'results': [{'connect': 0,\n              'endTime': 1536794393,\n              'latencyAvg': '13.17ms',\n              'latencyMax': '50.42ms',\n              'latencyStdev': '3.97ms',\n              'read': 45197,\n              'startTime': 1536794378,\n              'timeout': 0,\n              'totalRequests': 45199,\n              'write': 0}]}\n```\n\n. I've now tested this PR on my production workload (see https://github.com/puma/puma/issues/1254#issuecomment-420112799 for earlier results) and confirmed that this patch brings Puma's multithreaded performance significantly closer to single-threaded Unicorn in my tests.\n\nThinking through it, I wonder if the main issue here is that if all workers have at least one thread working and a client connects, it might take up to 1 second before any worker realizes there is a new client and goes off to do the accept. I could see that happening in the case of a well loaded server, with the workers sometimes happening to check only a few milliseconds after the client tries to connect and then random it taking a full second. People would see observer that externally as a random 1 second latency in requests, which would be pretty odd.\n\nYeah, it looks like the tradeoff is frequency of select-polling on the socket (and associated overhead of re-acquiring the GVL and thread-pool mutex) versus max queue-latency when all workers are busy (averaging to something like timeout / workers when requests take longer than timeout to respond). It's possible this timeout could be reduced to 100ms or even 10ms without significant overhead, maybe a bit more testing/benchmarking could determine the best default for this value.. > Overall seems like an good experiment. Have you tried running with this patch in production? Any numbers to share?\nI just ran a ~12-hour side-by-side comparison of two Puma servers running Ruby 2.5, with/without the OOBGC from tmm1/gctools#17 along with this PR. The difference on my current production workload is slight but still noticeable.\n\nAverage response duration over a 3-hour interval of moderate traffic was 53.31ms without oobgc, and 49.34 with.\nAverage total memory utilization after 12-hours of uptime was at 53.2% without oobgc, and 49.0% with.\n\nSo both response duration and memory utilization are slightly improved by about ~7-8% running out-of-band GC on my workload.. I'm less confident the logic is correct to begin with, only that it's unchanged by the algebraic refactoring.\nIn fact, during some local benchmarking I noticed some weird anomalies where busy_threads was logged as 2 after only a single request was added to the worker-process, so I do suspect there might be some concurrency edge-case not handled perfectly by the current logic.\nAnyway, it's certainly possible to leave the equation untouched by this PR if that makes it any less nerve-wracking. I figured making the 'busy threads' count more explicit in this method makes the logic easier to follow, but the change isn't strictly necessary.. ",
    "SleeplessByte": "Puma / Ruby 2.x 64 bit\nRails 4\nContent Type text/javascript\nERR_INCOMPLETE_CHUNKED_ENCODING\nOnly happens chrome\n(cc @StefanDorresteijn)\nMight be Chrome Related (See http://stackoverflow.com/questions/22219565/iis-chrome-failed-to-load-resource-neterr-incomplete-chunked-encoding)\n. Just had the same issue for html content. Generally, just make the output smaller and it will work. \nWindows 64bit, Locally hosted.\n. @evanphx I actually read a bug report on chrome issues the other day (sorry, lost it) and someone ran into this issue. Basically what happened was that the chunk was not ended with a newline character. Chrome is super strict on the matter and will only download the first chunk (waiting on the new line character, which never comes).\nThat said, for text/plain content that has it's own extension, simply register an alias and make sure yo uuse text/plain as a workaround.\n. ",
    "StefanDorresteijn": "Same environment and happens on requests with these specifics as well:\nContent Type application/json\nDoesn't happen on requests that are relatively small (40ish JSON objects) but does happen on larger requests (120ish JSON objects)\n. This issue actually disappeared for me when I started using Heroku.\n2014-06-26 2:34 GMT+02:00 Gopal Patel notifications@github.com:\n\nA question for everyone who's hit this: Are you using Heroku? Have you\nseen it at all in any other hosting environment?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/452#issuecomment-47175990.\n\n\nMet vriendelijke groet,\nStefan Dorresteijn\nRefresh SEO\n+31(0)621127193\nhttp://www.refreshseo.nl/\nCONFIDENTIALITY NOTICE: This message is intended only for the use of the\nindividual or entity to which it is addressed, and may contain\ninformation that is privileged, confidential and exempt from disclosure\nunder applicable law.\n. ",
    "barelyknown": "For what it's worth, I'm seeing the same problem. Identical requests will sometimes work, but once in a while (1/1000 requests maybe), puma seems to be truncating the JSON.\n. ",
    "westonplatter": "Cool. I'll come back for a HTTP 2.0 party after I finish contirbutions to a couple other repos.\n. @schneems, here's a couple off the top of my head, \n- https://github.com/igrigorik/http-2\n- https://github.com/molnarg/node-http2 (pulls in https://github.com/molnarg/node-http2-protocol)\n. @schneems a more complete list, https://github.com/http2/http2-spec/wiki/Implementations.\n. ",
    "dblooman": "Any updates on this?\n. ",
    "deepj": "@evanphx Any plans to add HTTP/2 support into Puma in near future? It'd be worth having the support in those days.\n. @evanphx It seems Puma will be working a bit better than now on JRuby 9k https://github.com/jruby/jruby/issues/2957 :) That was the reason why I made this PR.\n. +1\n. It seems the failure is not caused by my changes rather it comes from the last PR. \n. Hi,\nthis is already fixed in JRuby 9k head -> https://github.com/jruby/jruby/issues/2957\n. One of solutions is the following\nbash\nbrew install openssl\nbrew link --force openssl\nIf I remember it correctly OpenSSL (headers only???) has been removed in favour of Apple's own crypto library. Some solution how to fix this (as above) would be mentioned in README. I guess for some people this will be unpleasant surprise on El Captain.\n. @isaachall Yes, that is another solution. But I have couple of scripts (which are run on other machines with different OSs and settings) installing gem dependencies via Bundler. So this approach is not viable for me. So the solution is linking OpenSSL on my machine.\n. @evanphx :+1:\n. @evanphx Hello there, is it possible to merge this PR? Thank you! \n. Unfortunately, Puma supports only Ruby 2.1 and later now. See https://github.com/puma/puma/commit/b4a2fded22299dd8a11fd64ea88aeb3b33a0846e.\n. OK\n. @vadimskrotsky OpenSSL 1.1.0 is fully supported since Ruby 2.4 (see https://www.ruby-lang.org/en/news/2016/12/25/ruby-2-4-0-released/). So I guess the problem lies in that.. @nateberkopec Done. But it failed on jruby problem https://travis-ci.org/puma/puma/jobs/377248677. @nateberkopec I'd be in to test it against JIT option with compilation and testing on the Windows. Unfortunately, I don't get how to set it up for Appveyor since I've never used Appveyor.. ",
    "dmccown1500": "Hey all wanted to revisit this since it has not been talked about in a bit. Particularly I am wondering if it is readily possible to @tenderlove's implementation as a plugin or config option that would allow for the core puma to stay HTTP 1.1 and open up 2.0 support for those that want it. I will admit I do not have the knowledge or expertise for this as a PR, sorry!\n. @nateberkopec Thamnks for the fast response. I will take a look at the article and see what I can learn. :-)\n. @nateberkopec Great read. I think I am probably actually getting a lot of the benefits of HTTP/2.0 now since Cloudflare sits in front of my site. @evanphx makes sense and I appreciate the feedback.\n. ",
    "mcary": "When using a CDN or loadbalancer/web server like Apache, a Rack or Rails app can send \"Link\" headers with \"rel=preload\" to trigger server pushes.  That's one way that you can get much of the HTTP2 benefit without actually terminating HTTP2 in your app server.  But your app server has to know to send the \"Link\" headers.  So either you have to write code to set response.headers or maybe someday Rails will automatically emit them when your code calls asset helpers or something.\nHTTP2 Push with:\nCloudflare: https://blog.cloudflare.com/announcing-support-for-http-2-server-push-2/\nApache: https://httpd.apache.org/docs/2.4/mod/mod_http2.html#h2push\nIt might be cool if the application server (such as Rails) could keep its context from the base HTML request when rendering the assets, at least if they are customized per user or something.  But that seems like a relatively minor optimization for an uncommon scenario.  Terminating HTTP2 at the CDN or loadbalancer would not enable that sort of thing.\nAnd, as @nateberkopec suggests, HTTP2 could replace WebSockets functionality.  That also cannot be accomplished with termination at the CDN or loadbalancer.\n. ",
    "philsturgeon": "Curious if anything has changed in the last 13 months regarding this situation? It seems like Rack 2.0 did not add HTTP 2, or at least I don't see it in the CHANGELOG and cannot find anything about it on Google. \nWe're just waiting on Rack?. @pedrofurtado I've heard a lot of good things about Falcon.. ",
    "CrowdHailer": "I would also be curious about the progress on this. I have been building a solution for Elixir based on Rack and would like to see what solution Rack decided on. ",
    "pedrofurtado": "Is there any plans to add Http2?. Thanks for share, @philsturgeon :+1: . ",
    "terryliulj": "I have the same problem. \nruby 2.0.0p451 (2014-02-24) [i386-mingw32]\nrails 4.0.3\nPuma: 2.8.0 and 2.8.1\nChrome console show the error:\n   GET http://localhost:3000/    net::ERR_INCOMPLETE_CHUNKED_ENCODING \n. ",
    "forrest": "+1\n. +1\n. I found that I just couldn't get PUMA to run on WSL. There are some known issues with how PUMA wants sockets and I found some mention of an issue with how FORK(2) works (above my head but I got the drift). I had to switch my code to use webricks in development on windows and it started working again.. ",
    "Dfenniak": "+1\n. ",
    "arthurnn": ":+1: \n. My PR should allow this configuration using the :config_file option. Lemme know if it works fine for you.\n. :+1: \n. weird CI.\ntests pass just fine under ruby 1.9.3p484 in my machine..\n. Usually the more idiomatic way is to run the tests including the lib and test folders to the load path, so adding the test/ path to the require will be broken.\n. :heart:\nOn Friday, February 28, 2014, Evan Phoenix notifications@github.com wrote:\n\nMerged #471 https://github.com/puma/puma/pull/471.\n\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/pull/471\n.\n\n\nArthur Nogueira Neves\nFollow me @arthurnn http://www.twitter.com/arthurnn89\n. ",
    "manuelmeurer": "Just ran into this... @evanphx how can one see stats with pumactl if not with a pid or status file?\n. Sweet! :full_moon_with_face:\n. ",
    "yld": "You need to specify a control URL in your puma config file and on your command line.\n. > So if you don't have a control URL defined in your config, this command is useless?\nExactly.. ",
    "troyswanson": "So if you don't have a control URL defined in your config, this command is useless?. ",
    "momer": "Closing: I imagine setting backlog on nginx would resolve this; and, as Puma does not actually listen to the Unix socket, it has no responsibility to set a backlog.\n. URI#escape\nis the wrong command to use.\nShellwords#shellescape\nShould do fine. This isn't a Puma issue, this is your config issue.\nSee http://www.ruby-doc.org/stdlib-2.1.5/libdoc/shellwords/rdoc/Shellwords.html for more info.\n. ",
    "collimarco": "@momer  I have your exact same issue. Do you remember how you solved that error? Any tip would be greatly useful ;). @swordfish444 You need to increase the backlog for Puma: https://github.com/puma/puma/issues/370\nSee also: https://www.nginx.com/blog/tuning-nginx/. At the moment is an hour that I have disabled the LogStashLogger gem and Puma seems to work.\nMy guess (it's just a guess) is that if an exception is raised by the gem and propagates up to Puma, it will try to start the application again, thus creating the same error again and again until the socket limit is reached. However I think that Puma isn't managing that situation properly: Puma shouldn't create that many puma.sock (until it eventually reaches the limit).\n. > Sounds more like that gem is creating sockets and not cleaning them up.\nThe gem owner said that the bug was like this: when an exception raised inside the gem, the gem would catch it and retry without closing the socket (which thus was leaked). So based on what he said the exception did never propagate to Puma: the thread was stuck on a try / catch loop.\n\nDid you restart puma a bunch of times?\n\nNo, not with hot restarts or things like that: at most I could have restarted the whole service (but that would mean a different process/PID and thus lsof wouldn't have listed those sockets if they were opened by a previous Puma process).\n\nHow long was it running?\n\nIn that case it was running for less than an hour. \n\nhow many worker_connections did you configure in the nginx config?\n\nI have never changed that setting, so it should be the default (512).\nIf it's like TCP sockets where for each connection you open a new socket it can be happened as follows:\n- open the first socket and process the first request\n- an exception occurs inside the gem\n- the thread gets stuck\n- nginx has to open a new connection (and the previous one i still there...) \n- the new request is processed and a second thread get suck\n- ...\n- all 16 threads get suck\n- nginx still tries to open new connections for the incoming requests...\n- and thus Puma opens new sockets even if it hasn't any thread left to process them\nProbably the scenario is slightly more complex since I also have rack-timeout installed which at some point should have terminated the stuck requests (I don't know if it actually worked). However terminating the processing probably frees the thread but not the sockets (leaked by the LogStashLogger  gem).\nIt would be useful if Puma could free all sockets opened by a thread every time it finishes the processing of a request (I don't know if Ruby provides an easy way to detect the sockets opened by a given thread).\n. After upgrading to Puma 3.8.2 in development I am experiencing a similar issue. When multiple requests are triggered at the same time, some of them hangs forever and you need to stop and restart puma.\nIs there any way to debug this issue? . Puma is stuck again in development. CPU usage is low (Puma is not in top processes). Looks like gdb is not available on macOS by default, so I have used the sample utility. Here's a gist with the result.. Btw it looks like starvation and not a deadlock: I have noticed that after some minutes Puma has completed the request.. I have found this error after the request was completed:\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.2.2-p95), codename: Sassy Salamander\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on ssl://127.0.0.1:443?cert=/Users/collimarco/Sites/pushpad/config/cert/ssl-wildcard.crt&key=/Users/collimarco/Sites/pushpad/config/cert/ssl-wildcard.key&verify_mode=none\nUse Ctrl-C to stop\nError in reactor loop escaped: undefined method `closed?' for #<Puma::MiniSSL::Socket:0x007fd39967e138> (NoMethodError)\n(eval):2:in `closed?'\n/Users/collimarco/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/puma-3.8.2/lib/puma/reactor.rb:31:in `block in run_internal'\n/Users/collimarco/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/puma-3.8.2/lib/puma/reactor.rb:31:in `any?'\n/Users/collimarco/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/puma-3.8.2/lib/puma/reactor.rb:31:in `rescue in run_internal'\n/Users/collimarco/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/puma-3.8.2/lib/puma/reactor.rb:28:in `run_internal'\n/Users/collimarco/.rbenv/versions/2.2.2/lib/ruby/gems/2.2.0/gems/puma-3.8.2/lib/puma/reactor.rb:153:in `block in run_in_thread'. This is the result that I get with performance L for a simple html page (without db):\n\nhttps://ldr.io/2VITU8t (4 workers, max ~900req/s)\nhttps://ldr.io/2UpRqvA (8 workers, max ~1500req/s)\nhttps://ldr.io/2Uofpet (12 workers, max ~1500req/s)\n\nI can see an improvement from 4 workers to 8 but then it stops. So the max that I can get is 1500req/s. \nBasically:\n\nrunning 20 instances of 1x standard dynos (tot $500) produces ~6000req/s \nrunning 1 instance of performance L dynos (tot $500) produces ~1500req/s\n\nIt's either Heroku pricing that is wrong or Puma that scales better horizontally than vertically.. @nathansamson I was talking about Puma worker processes (e.g. 12 workers with 5 threads each). More about dyno types: https://devcenter.heroku.com/articles/dyno-types. ",
    "poc7667": "@momer how could you solve this?. Yes I test it in development mode, I got it. thanks for your answer\n. The problem is still on-going even I turned the earger loading -> true.\nHere's my puma config and running the puma on ubuntu 14.\nAny idea? or any alternative server could run a with nginx rever-proxy and rails\n    bind \"unix:///tmp/puma.app.sock\"\n    stdout_redirect \"/tmp/puma.stdout.app.log\", \"/tmp/puma.stderr.app.log\", true\n    app_dir = File.expand_path(\"../..\", __FILE__)\n    workers 2 # because my linode only has 2 CPU cores\n    threads 1,2\n    pidfile \"/tmp/puma.app.pid\"\n    #port        7617\n    rails_env = \"production\"\n    environment rails_env\n    activate_control_app\n\n    on_worker_boot do\n      require \"mongoid\"\n      Mongoid.load!(\"#{app_dir}/config/mongoid.yml\", [rails_env])\n    end. @batasrki  any update on this issue? I ran into the same problem for me.\n\nUsing the reverse proxy and puma listening on the socket as well. Ubuntu 14. thanks so much!\n. Any alternative solution? Or if you gave up Puma then which server do you use now?. @karpa13a  # -1 (11: Resource temporarily unavailable)\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http cleanup add: 00000000017779D8\n        2017/12/27 08:22:54 [debug] 5808#0: *7 get rr peer, try: 1\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http upstream connect: 0\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http upstream send request\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http finalize request: -4, \"/api/v1/query?\" a:1, c:2\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http request count:2 blk:0\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http run request: \"/api/v1/get_registered_email?token=fwfmfi\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream check client, write event:1, \"/api/v1/get_registered_email\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream recv(): -1 (11: Resource temporarily unavailable)\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream request: \"/api/v1/get_registered_email?token=fwfmfi\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream dummy handler\n        2017/12/27 08:22:54 [debug] 5808#0: *6 http run request: \"/api/v1/statistics?token=fwfmfi\"\n        2017/12/27 08:22:54 [debug] 5808#0: *6 http upstream check client, write event:1, \"/api/v1/statistics\"\n        2017/12/27 08:22:54 [debug] 5808#0: *6 http upstream recv(): -1 (11: Resource temporarily unavailable)\n        2017/12/27 08:22:54 [debug] 5808#0: *6 http upstream request: \"/api/v1/statistics?token=fwfmfi\"\n        2017/12/27 08:22:54 [debug] 5808#0: *6 http upstream dummy handler\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http run request: \"/api/v1/query?\"\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http upstream check client, write event:1, \"/api/v1/query\"\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http upstream recv(): -1 (11: Resource temporarily unavailable)\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http upstream request: \"/api/v1/query?\"\n        2017/12/27 08:22:54 [debug] 5808#0: *7 http upstream dummy handler\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream request: \"/api/v1/get_registered_email?token=fwfmfi\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream process header\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http proxy status 304 \"304 Not Modified\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http proxy header: \"X-Frame-Options: SAMEORIGIN\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http proxy header: \"X-XSS-Protection: 1; mode=block\".\n",
    "swordfish444": "@momer @collimarco @poc7667 Did any of you resolve this issue? I'm also facing this and any tips would be very much appreciated! . ",
    "cs": "Anyone?\n. ",
    "tecnobrat": "I am also seeing something similar.\nBasically it just stops responding.\nIf I strace a worker thread, this is what I see:\nselect(4, [3], NULL, NULL, {0, 34701})  = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nselect(4, [3], NULL, NULL, {0, 100000}) = 0 (Timeout)\nI'm going to pump up the maxfiles on this server and see if that helps.\n. I think I'm going to switch to thin for the time being.  I can't have my server crashing every day (or multiple times a day!)\n. ",
    "undeflife": "you are not alone...\ni get same nginx error log every day. but i don't use rufus scheduler. i switched to unicorn ,but it didn't make any difference.\n. ",
    "jer0m": "@undeflife do you mean that switching to unicorn didn't resolve the problem? \nI have the same issue here and plan to switch back to unicorn meanwhile the origin of the issue is found but if it didn't resolve your problem...\n. I think I found a workaround for this issue.\nI just create a pid for each separate worker and monitor them with monit. My monit just kill each worker when the memory goes too high. \nI have put a low limit on memory so my workers are killed almost every hour. The issue didn't appear again for the moment.\n. ",
    "davejlong": "It appears that if I leave off the --daemon arg, the PID file works as expected.\n. ",
    "JonasBorchelt": "I have the same problem with version 2.14.0: get no pid if I start the server daemonized. \n. ",
    "mdesantis": "Same problem here, puma 3.6.2. Does GitHub allow to squash the commits or I have necessarly to clone my fork and squash them? :doughnut: . I agree wtih you; still I believe that the daemonized configuration is not an evil alternative and it could be worth to be mentioned.. @dekellum sorry for the delay. I add WorkingDirectory to the alternative forking configuration, since using bundle exec is almost required. https://github.com/puma/puma/pull/1370. For me it's OK to close too, the alternative configuration is still working good in production. @dekellum thanks again for the review, you made my edit so much better. Happy 2018 to everyone! . +1 my english is poor :). ",
    "jbarwick": "Is this still an active issue as of May 2017?\nOk, I'm on Puma 3.9.1.  My issue is not that the PID file is missing.  it's there.\nMy Issue is that the PID file is written AFTER daemonization!!!\nThat means, there is no PID file after the process forks.  The PID file is only written AFTER puma.rb executes fully (it seams) after all workers threads have been started (min-threads).\nIf you are using ActiveRecord, and it attempts to connect to the database, or have many workers/threads, this can take a few hundred milliseconds.\nIf you are using a 'forking' configuration in your capistrano, init.d, upstart, or systemd scripts, then after puma daemonizes, the PID file will either not be there (until later) or will contain an old process ID (cuz the new one hasn't been written yet).\nThe best way to get around this is to use a non-forking configuration (e.g. don't rely on the PID file at all..don't...it's ok...don't worry about it...it will get written soon enough).\nI use both upstart and systemd\nI also use .rbenv\nIn upstart and system.d I have\nrespawn\nchdir /myhome/myapp\nenv RAILS_ENV=production\nexec /myhome/.rbenv/shims/bundle exec puma config.ru\nType=simple\nRestart=always\nWorkingDirectory=/myhome/myapp\nEnvironment=RAILS_ENV=production\nExecStart=/myhome/.rbenv/shims/bundle exec puma config.ru\nAnd because puma is not running in daemon mode, the upstart and systemd will feel that the process is indeed 'started'.  \nIf you want to terminate puma, either Ctl-C the process (kill the upstart/systemd service thread) or call:  \"pumactl -S statefile.state stop\"\nOh...but beware...this means that your /var/log/syslog is really really...really messy unless you stop all puma STDOUT or ensure that stdout is redirected to a file...\nSo redirect it,\nstdout_redirect \"#{shared_dir}/log/puma.stdout.log\", \"#{shared_dir}/log/puma.stderr.log\", true\n. I understand your response.\nHowever, it would be nice if the parent process waited for the deamon process to write its pie file before it terminated.\nI doesn\u2019t feel right to put \u201cExecStart=/bin/bash -c \u201cpuma -d;sleep 2\u201d in my systemd script. @dekellum is on the right track.  IMHO, there are challenges with the daemon mode:\nWe'll make puma start as a daemon.  And below PID is the process ID\nThe daemon startup process is this:\npuma -d\nprocess1 -> puma startup PID 1\nprocess2 -> daemon process PID 2 \nPuma PID 1 exits (leaves the daemon PID 2 running)\nworker1 -> puma PID 3\nworker2 -> puma PID 4\nPuma PID 2 outputs the file puma.pid\nProblem 1:  The PID file is written only after all workers startup.   And this can take a while.\nProblem 2:  The process PID 1 exists immediately after it starts the daemon process \nresult:  PID 1 would be the 'child process' of systemd.  It exists.  So, systemd would assume the process has exited and stopped.  It doesn't know that PID2 is the 'real' process. \nYou can add ExecStartPre= to delete any old pid file and ExecStartPost= to wait for the PID file to be written.  And set the PIDfile= parameter.  But systemd will complain:  \"The monitored process is not the child process.  You will probably not know when it exits.\"  It takes about 3 seconds to start all the workers.  So, adding the ExecStartPost to wait for the pid file will delay your startup.\nWell, it works anyway.  So, you could, theoretically, change the systemd service type to 'forking' and use the -d parameter.  But, beware, there is another problem.\nProblem 3:  phased-restart\nyou can do a pumactl -P pidfile.pid phased-restart and the documentation says it will drain the workers before it restarts each one.  Nice.  But, if you use daemon -d mode, there is a problem.\nThe problem is that if you use daemon mode, and do a phased-restart, the daemon restarts.  This will delete the pid file.  Systemd will assume the process has crashed, and restart the service.\nIf you do not use daemon mode and issue a phased-restart, the parent process does not exit and the workers restart and get new PIDs.  The parent process keeps its pid cuz it doesn't restart.  NICE. This is the way it should be.\nI have communicated to the author of puma (ticket in Github) regarding the issue of the Pid file write timing (i was trying to use daemon mode in systemd). But, I don't think he understood what I was asking.  Anyway, he closed the case.  And this was before I knew about the behaviour of phased-restart.\nMoral of the story:\nAs of Version 3.10.0 (ruby 2.4.1-p111), codename: Russell's Teapot, do not ever ever ever...I mean never use -d daemon mode.  Whether you use systemd or not.  Use some other daemonizer.\nSo, in sytsemd..use simple mode.  It might seem weird when your program theoretically supports a daemon mode, but it works quite well.  I have tested start/stop/reload/restart and the phased-restart works nicely.  In simple mode running puma interactively without the -d flag. (systemd, upstart, or wherever)\n. ",
    "feliperoveran": "@jbarwick Writing the pidfile after daemonization is the desired behavior, since the pid will be different after daemonizing.\nExplanation:\nWhen daemonizing we need to make sure two things never happen - defunct processes and that our process never has a controlling terminal.\nTo do so, we fork twice (calling setsid between both forks, along with setting the creation mask and unlinking stdin, stdout and stderr file descriptors). On the first time we make sure the ppid - parent pid is the init process and that child processes will always be wait'ed on (thus not creating defunct processes) while on the second time, we make sure our process is not a session leader, so it can't be assigned with a controlling terminal.\nAfter Ruby 1.9, which introduced Process.daemon, we no longer need to do the fork - setsid - fork process by hand since Ruby handles that for us.\nDigging a little deeper into Ruby's implementation of Process.daemon, you can see the mentioned double-forking behavior here and the spec testing that the PID will be different after daemonizing here.\nTaking a closer look to Puma's startup process, we can see here that it is calling Process.daemon, thus we will have a different PID after daemonizing. So, if we wrote the pidfile before such process, we would have a wrong PID being written to the file.\nFor more information about the double-forking, setsid and everything related to the daemozination process, check Stevens's Advanced Programming in the UNIX Environment - chapter 13. ",
    "augustosamame": "I have just setup Puma with systemd on Ubuntu 18.04. I also get this when running sudo systemctl status puma.service\nApr 28 21:06:37 ip-172-31-55-37 systemd[1]: Starting Puma HTTP Forking Server...\nApr 28 21:06:37 ip-172-31-55-37 rvm[7959]: Puma starting in single mode...\nApr 28 21:06:37 ip-172-31-55-37 rvm[7959]: * Version 3.11.4 (ruby 2.5.1-p57), codename: Love Song\nApr 28 21:06:37 ip-172-31-55-37 rvm[7959]: * Min threads: 0, max threads: 8\nApr 28 21:06:37 ip-172-31-55-37 rvm[7959]: * Environment: production\nApr 28 21:06:37 ip-172-31-55-37 rvm[7959]: * Daemonizing...\nApr 28 21:06:37 ip-172-31-55-37 systemd[1]: puma.service: Can't open PID file /home/deploy/rails_app/shared/tmp/pids/puma.pid (yet?) after start: No such file or directory\nApr 28 21:06:38 ip-172-31-55-37 systemd[1]: Started Puma HTTP Forking Server.\nBut everything seems to work fine. Puma starts properly on server reboot and I can start/stop/restart using the systemctl commands. Why would I need to put the sleep 2 in the systemd script to wait for the PID?\n. ",
    "rubencaro": "If you apply your patch then you can end up with a dead application. see:\n1. Update your application by updating the $APP_DIR/current symlink.\n2. pumactl phased-restart\n3. Your patch updates master process' path to code.\n4. Workers die because the new code is somewhat wrong.\n5. Master process try to spawn new worlers, but uses the same new code that is broken.\n6. Workers die again.\nThere should be a way for the master process to update its path to code only when workers really are alive and stable.\n. Maybe this should be responsability of some else, and then add a command to pumactl to update code's path, like pumactl path-to-code '/path/for/new/code'.\n. Take a look at a possible workaround in #478\n. +1 for this!\nAlso if puma could know if a phased-restart went ok, then it could only update this directory when restart goes ok.\n. Be aware that you may need to configure some other things too:\n-  rackup '/path/to/current/config.ru' on config/puma.rb to force puma to resolve current each time it invokes rackup file.\n-  BUNDLE_GEMFILE=/path/to/current/Gemfile on start script to force bundler to resolve current each time puma restarts.\n. @seuros If directory is reloaded automatically then you can end up with a dead app, as newly spawned workers could try to load broken code. Take a look at #468 and #469.\n. @seuros 3 quick points come to me:\n- You will come to see one day that you can try your code, test it, probe it, make simulations, make alpha releases, beta releases, release it out of balancers and give it a controlled load, or whatever strategy you learn with time. No matter what. There's no environment like production environment. Therefore you can limit the number and size of your code breakages with all this stuff, but they will eventually happen. I would like to be prepared for it. \n- It is hard to notice the error (it's not, read this request's description!) if the application still using the old code base, but that's only because the application is still up! I guess that no downtime is a good thing. So maybe we should learn how to diagnose our app's status by other means than simply watching it die. I feel like I'm going that way. I would love to see no downtime ever for my apps. Even for those rare after beta bugs.\n- This is just a harmless option added to pumactl. Simply do not use it if you do no want to. This is an easy way (for me at least) to workaround one possible situation I have seen with puma deploys using symlink strategy and phased-restart. \nI hope that helps you understand this pull request's motivation. Anyway, I'm open to suggestions about how to solve the problem in #468 and #469 .\nThanks.\n. @seuros It's ok. Let's solve things.\nMaybe we can provide puma with a way to be aware of the app's health, and so be able to automatically decide if it should change the worker's code base... It would then be able to notify when everything is not ok too. \nMaybe a block passed on configuration that gets run at some specific point after restart?\nNot sure about the whole restart process and where to put that control point. And then what kind of notifications could be sent. Should we wait for it to end? Should it be async and then be polled?\nAny ideas?\n. @seuros I'm sorry too then. I misunderstood your comments. Sorry.\nYour two scenarios sound perfect to me. It would be great to get that from puma. \nThe main problem is how we get to that, because puma itself doesn't know how to tell that a worker is not only booted, but healthy enough to be considered stable code for future workers to be spawned. \nIf we were able to pass a block of application-specific code to puma, then it could run it to get the health level of each worker. This way decide if it is good enough to consider it stable code. If it's not good enough, then throw something like New codebase did not pass health checks, using old one. Phased-restart failed.\nBy getting the health level of a worker I mean nothing fancy. Just some plain vital signs gathered from within the beast itself. Although once you are there, any kind of test code can be run. Either way, it's app-specific code, it's up to you.\nWhat do you think?\n. Nice!  We already have the callback!\nThen the only thing left to do is to put the health checking code in there, and then setup some way to tell puma if the new code is stable or not. Any way to do this? \n. @seuros I think you had at least some good point that put us closer to a nice solution.\nThe current on_worker_boot is useless for this. That I agree. It will run before the app is booted.\nAnd the proposed after_worker_boot on #483 is also somewhat useless for this, as runs in a different context. That I agree too. \nWe should find a way to tell puma that the code is good from inside our app. Maybe there is already a reference to the master process, or maybe we need to call the proposed pumactl reload-worker-directory directly from our app's code once it checks it's own health.\n@evanphx will know better, I'm ok?\n. +1 for that\n. @IDreamerI seems to be right. on_worker_boot runs after the fork, but before boot (that happens here https://github.com/puma/puma/blob/b90fc4fa05108f8e6a8266f69787cbe0efaaab42/lib/puma/runner.rb#L130) so he relabels it to before_worker_boot. It may be useful to have the after_worker_boot too.\n+1 for this\n. ",
    "brandonparsons": "Did you figure anything out here?\n. I've tried just doing a standard phased restart with directory and rackup specified in puma.rb. It seems to be working. How can I expose issues here?\n. ",
    "maljub01": "My pull request #469 fixes this issue. I've actually been using it in production for some time now.\n. @pencilcheck which versions of puma/ruby are you using?\nAlso, check #563 it might be related to your problem.\n. ",
    "dkubb": "@chenghung does your puma config specify the state_path?\nI had issues similar to this (and other issues in the tracker) until I started specifying it explicitly. I had incorrectly assumed puma would be able to figure it out by default.\n. ",
    "randoum": "Ok so is there a solution to get a full app log output when using puma in ssl with rails?\n. @standingwave I guess not, if @evanphx is silently ignoring this since 3 years than you can consider it dead.\n. ",
    "Zeneixe": "+1\n. ",
    "standingwave": "Any chance of this getting addressed? Just bit me today....\n. ",
    "lvela": "I found a good workaround. Add to config/environments/development.rb\nconfig.logger = ActiveSupport::TaggedLogging.new(ActiveSupport::Logger.new(STDOUT))\nFrom http://stackoverflow.com/a/39276701. ",
    "schnittchen": "Bonus points to avoid running over other people's setup:\nif $0[\"puma\"]\n  config.logger = ActiveSupport::TaggedLogging.new(ActiveSupport::Logger.new(STDOUT))\nend. ",
    "yegortimoshenko": "3 months have passed, and still no update. \nIt is just a hoe gem update, it won\u2019t do any harm.\n. This doesn't seem to solve the issue, because DLDFLAGS is generated in #create_makefile. Closing.\nIf you have the same issue, see https://github.com/rmagick/rmagick/issues/96 and https://github.com/jekyll/jekyll/issues/2125.\n. ",
    "vemv": "I second @jeremyhaile's question - why not? Isn't it something worthy of working on?\nIt's not attractive having to use an alien command (puma) instead of transparently integrating into rails server, as Thin and others do.\n. @evanphx nice, sounds fair enough. Thanks for the answer!\n. @edogawaconan probably you can assign a ::Logger::Formatter.new (Rails' production logger formatter) somewhere\n. @jrochkind in your Gemfile, wrap your puma declaration within a group: :production,  :statging do block. How is your reply related to the outdated comment I point out?\n. ",
    "uishaq": "I tried and append my intermediate certificate to the first one but it doesn't work. The same certificate works fine with NginX. Also, Chrome seems to have no problem with this certificate but Firefox gives a certificate low trust warning.\n. ",
    "eliduke": "Hey Evan.\nSo, unfortunately this seems to be an intermittent problem for me. As soon as I heard back from you, it seemed as though everything fixed itself and it ran perfectly fine for about a week or so. Then tonight I was working on something and it started happening again. I had only one Terminal window open and I had restarted the Terminal a couple times, still with the same issue. I tried running 'netstat -tlpn` but it returned \"netstat: n: unknown or uninstrumented protocol\". I searched around a bit on how to kill processes, and this is what I did.\neliduke-master: foreman start\n22:24:28 web.1  | started with pid 12000\n22:24:28 web.1  | Puma 2.6.0 starting...\n22:24:28 web.1  | * Min threads: 0, max threads: 16\n22:24:28 web.1  | * Environment: development\n22:24:28 web.1  | * Listening on tcp://localhost:5000\n22:24:28 web.1  | == Someone is already performing on port 5000!\n22:24:28 web.1  | Puma 2.6.0 starting...\n22:24:28 web.1  | * Min threads: 0, max threads: 16\n22:24:28 web.1  | * Environment: development\n22:24:28 web.1  | * Listening on tcp://localhost:5000\n22:24:28 web.1  | == Someone is already performing on port 5000!\n22:24:28 web.1  | exited with code 0\n22:24:28 system | sending SIGTERM to all processes\nSIGTERM received\neliduke-master: netstat -anp tcp | grep 5000\ntcp6       0      0  fe80::1%lo0.5000       *.*                    LISTEN     \ntcp6     326      0  ::1.5000               ::1.57463              CLOSE_WAIT \ntcp6       0      0  ::1.57463              ::1.5000               FIN_WAIT_2 \ntcp4       0      0  127.0.0.1.5000         *.*                    LISTEN     \ntcp6     326      0  ::1.5000               ::1.57461              CLOSE_WAIT \ntcp6       0      0  ::1.57461              ::1.5000               FIN_WAIT_2 \ntcp6     326      0  ::1.5000               ::1.57457              CLOSE_WAIT \ntcp6       0      0  ::1.57457              ::1.5000               FIN_WAIT_2 \ntcp6     326      0  ::1.5000               ::1.56717              CLOSE_WAIT \ntcp6       0      0  ::1.5000               *.*                    LISTEN     \ntcp6       0      0  ::1.5000               ::1.63090              CLOSE_WAIT \neliduke-master: lsof -i :5000\nCOMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\nruby     6393  eli   11u  IPv6 0x75a17371c633a79f      0t0  TCP localhost:commplex-main (LISTEN)\nruby     6393  eli   14u  IPv6 0x75a17371b630c8df      0t0  TCP localhost:commplex-main->localhost:63090 (CLOSE_WAIT)\nruby     6393  eli   15u  IPv6 0x75a17371c633969f      0t0  TCP localhost:commplex-main->localhost:57457 (CLOSE_WAIT)\nruby     6393  eli   16u  IPv6 0x75a17371b630de1f      0t0  TCP localhost:commplex-main->localhost:56717 (CLOSE_WAIT)\nruby     6393  eli   17u  IPv6 0x75a17371b630c49f      0t0  TCP localhost:commplex-main->localhost:57461 (CLOSE_WAIT)\nruby     6393  eli   18u  IPv6 0x75a17371c633a35f      0t0  TCP localhost:commplex-main->localhost:57463 (CLOSE_WAIT)\nruby    11760  eli   11u  IPv4 0x75a17371bd6fcbe7      0t0  TCP localhost:commplex-main (LISTEN)\nruby    11776  eli   11u  IPv6 0x75a17371c63389df      0t0  TCP localhost:commplex-main (LISTEN)\neliduke-master: kill -9 6393\neliduke-master: lsof -i :5000\nCOMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME\nruby    11760  eli   11u  IPv4 0x75a17371bd6fcbe7      0t0  TCP localhost:commplex-main (LISTEN)\nruby    11776  eli   11u  IPv6 0x75a17371c63389df      0t0  TCP localhost:commplex-main (LISTEN)\neliduke-master: kill -9 11760\neliduke-master: kill -9 11776\neliduke-master: lsof -i :5000\neliduke-master: foreman start\n22:26:37 web.1  | started with pid 12042\n22:26:38 web.1  | Puma 2.6.0 starting...\n22:26:38 web.1  | * Min threads: 0, max threads: 16\n22:26:38 web.1  | * Environment: development\n22:26:38 web.1  | * Listening on tcp://localhost:5000\n22:26:38 web.1  | == Sinatra/1.4.3 has taken the stage on 5000 for development with backup from Puma\nIt's running fine now. I just don't understand why sometimes it doesn't kill the process when I kill the server.\n. I might be dialing this in a bit, and it seems to be a problem with the way that I am canceling foreman with Ctrl + C. I had it running for about 30 minutes, canceled it, and got this error...\neliduke-master: foreman start\n07:18:02 web.1  | started with pid 1614\n07:18:06 web.1  | Puma 2.6.0 starting...\n07:18:06 web.1  | * Min threads: 0, max threads: 16\n07:18:06 web.1  | * Environment: development\n07:18:06 web.1  | * Listening on tcp://localhost:5000\n07:18:06 web.1  | == Sinatra/1.4.3 has taken the stage on 5000 for development with backup from Puma\n^CSIGINT received\n/usr/local/foreman/lib/foreman/engine.rb:226:in `synchronize': can't be called from trap context (ThreadError)\n    from /usr/local/foreman/lib/foreman/engine.rb:226:in `output_with_mutex'\n    from /usr/local/foreman/lib/foreman/engine.rb:232:in `system'\n    from /usr/local/foreman/lib/foreman/engine.rb:304:in `terminate_gracefully'\n    from /usr/local/foreman/lib/foreman/engine.rb:41:in `block in start'\n    from /usr/local/foreman/lib/foreman/engine.rb:289:in `call'\n    from /usr/local/foreman/lib/foreman/engine.rb:289:in `wait2'\n    from /usr/local/foreman/lib/foreman/engine.rb:289:in `watch_for_termination'\n    from /usr/local/foreman/lib/foreman/engine.rb:48:in `start'\n    from /usr/local/foreman/lib/foreman/cli.rb:40:in `start'\n    from /usr/local/foreman/vendor/gems/thor-0.16.0/lib/thor/task.rb:27:in `run'\n    from /usr/local/foreman/vendor/gems/thor-0.16.0/lib/thor/invocation.rb:120:in `invoke_task'\n    from /usr/local/foreman/vendor/gems/thor-0.16.0/lib/thor.rb:275:in `dispatch'\n    from /usr/local/foreman/vendor/gems/thor-0.16.0/lib/thor/base.rb:425:in `start'\n    from /usr/bin/foreman:15:in `<main>'\nI started foreman again and everything seemed fine but when I tried to load the page Sinatra threw an error saying that it couldn't find that route. I ran lsof -i :5000 again, killed the processes, started foreman again, and this time everything is fine.\nCould this be a problem with Foreman / Sinatra and the way in which I'm canceling Foreman with Ctrl + C?\n. @LaurMo I don't know if I ever fully solved it, but it wasn't happening every time. I haven't been using Foreman much lately, so I'm a little out of the loop on that. Are you having the same exact error that I was having? Maybe we could post this error as an issue with Foreman. Thoughts?\n. This still happens sometimes. Still can't figure out what conditions cause it though.. ",
    "LaurMo": "Foreman is still doing this error for me - how did ya'll solve it?\n. ^ That's what I do as well.  Hoping this could somehow get fixed either on the foreman or puma side.\n. ",
    "mfpiccolo": "This is happening for me when I control c to quit foreman.  I then have to use lsof -i :5000 and sudo kill -9 <pid> to kill the process if I want to run foreman again.\n. Procfile:\nweb: puma --config config/puma.rb\nworker1: QUEUE=1 bundle exec rake jobs:work\nworker2: QUEUE=2 bundle exec rake jobs:work\nworker3: QUEUE=3 bundle exec rake jobs:work\n. config/puma.rb\n```\nworkers Integer(ENV['PUMA_WORKERS'] || 1)\nthreads Integer(ENV['MIN_THREADS'] || 2),\n        Integer(ENV['MAX_THREADS'] || 2)\npreload_app!\nrackup DefaultRackup\nHeroku will set this when the web process boots up\nport ENV['PORT'] || 3000\nOn Heroku this will be set to 'production' by default\nenvironment ENV['RACK_ENV'] || 'development'\non_worker_boot do\n  ActiveRecord::Base.establish_connection\nend\n```\n. ",
    "gunosk129": "+1\n. ",
    "askl56": "Hmm seems like this issue is still occuring\n. ",
    "Dorian": "It happens so often I made a script for it:\n```\n!/usr/bin/env ruby\nport = ARGV.first || 3000\nJust to get sudo\nsystem(\"sudo echo kill-server-on #{port}\")\npid = sudo lsof -iTCP -sTCP:LISTEN -n -P | grep #{port} | awk '{ print $2 }' | head -n 1.strip\nputs \"PID: #{pid}\"\nkill -9 #{pid} unless pid.empty?\n``. @lightbe justkill-server-on 3000` for instance to kill the process that is listening on port 3000 (puma in this case)\nI'm gonna add my scripts to https://github.com/Dorian/bin at some point this week. ",
    "lightbe": "@Dorian I'm having a consistent problem with puma in one of my Rails apps on my Mac Mini server and in Heroku. I would like to implement your script. How/where are you executing this script?. ",
    "brendonrapp": "It happens so often. I think I'm going to start using Dorian's script, I'm tired of it.. ",
    "tmornini": "@evanphx No, just saw that post and thought of you. :-)\n. Nice!\n. ",
    "lukeasrodgers": "It's worth noting that Typhoeus, which uses libcurl, includes this header with PUT requests (and possibly also some POSTs) by default. Ruby applications that attempt to handle these requests by returning 100 responses (in, e.g., rack middleware) will fail to handle such requests, while those that ignore this header will experience a 1s delay while curl waits for the 100 response before sending the PUT body.\n. ",
    "MichaelSp": "By the way: unicorn has an implementation for this https://github.com/defunkt/unicorn/blob/ec8c095b9f6081ec1462db6c4392ab181bd6790e/lib/unicorn/http_server.rb#L567-586\n. ",
    "PaulL1": "OK, makes sense, they looked a bit unmaintained, but still they gave me 90% of what I needed to get it working, so it would be a pity to lose them entirely.\nAre you suggesting a parallel repo (but not the main repo), or suggesting that I create a repo?  My concern is that the scripts I have work for my setup (all debian based), but that's no guarantee they work on other configurations.  I don't have any machinery on other distros, nor probably enough familiarity with those distros to maintain it myself.  I'm certainly happy to create a github repo with my updated scripts that you could point to if that's what you are thinking, but I'm also concerned about losing history that might be applicable to other people's situations.\n. ",
    "billingb": "Startup logging:\n[6240] Puma starting in cluster mode...\n[6240] * Version 2.8.2 (ruby 1.9.3-p0), codename: Sir Edmund Percival Hillary\n[6240] * Min threads: 1, max threads: 1\n[6240] * Environment: development\n[6240] * Process workers: 5\n[6240] * Phased restart available\n[6240] * Listening on ssl://0.0.0.0:3000?key=/home/vagrant/tmp/cert/server.key&cert=/home/vagrant/tmp/cert/server.csr\n[6240] Use Ctrl-C to stop\n[6242] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[6250] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[6248] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[6246] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[6258] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[6240] - Worker 1 (pid: 6246) booted, phase: 0\n[6240] - Worker 2 (pid: 6248) booted, phase: 0\n[6240] - Worker 4 (pid: 6258) booted, phase: 0\n[6240] - Worker 0 (pid: 6242) booted, phase: 0\n[6240] - Worker 3 (pid: 6250) booted, phase: 0\n. Tried again with updated ruby to 1.9.3p286. Still failing the same way.\nconsole:\n[30194] Puma starting in cluster mode...\n[30194] * Version 2.8.2 (ruby 1.9.3-p286), codename: Sir Edmund Percival Hillary\n[30194] * Min threads: 1, max threads: 1\n[30194] * Environment: development\n[30194] * Process workers: 5\n[30194] * Phased restart available\n[30194] * Listening on ssl://0.0.0.0:3000?key=/home/vagrant/tmp/cert/server.key&cert=/home/vagrant/tmp/cert/server.csr\n[30194] Use Ctrl-C to stop\n[30196] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[30200] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[30202] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[30206] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[30212] + Gemfile in context: /home/vagrant/src/dataset-search-services/Gemfile\n[30194] - Worker 0 (pid: 30196) booted, phase: 0\n[30194] - Worker 2 (pid: 30202) booted, phase: 0\n[30194] - Worker 1 (pid: 30200) booted, phase: 0\n[30194] - Worker 3 (pid: 30206) booted, phase: 0\n[30194] - Worker 4 (pid: 30212) booted, phase: 0\n^C[30194] - Gracefully shutting down workers...\nException handling servers: OpenSSL error: error:00000001:lib(0):func(0):reason(1) - 1 (Puma::MiniSSL::SSLError)\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/minissl.rb:31:in read'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/minissl.rb:31:inengine_read_all'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/minissl.rb:46:in read_nonblock'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/client.rb:151:intry_to_finish'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/client.rb:221:in eagerly_finish'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:244:inblock in run'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in call'\n/usr/local/lib/ruby/gems/1.9.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n[30194] - Goodbye!\n. I don't have a good way to recreate this issue at this time. I'm going to go ahead and close this issue. I think that most likely the problem is the above mentioned CSR vs certificate mixup. Some better error messaging would help anyone else who has this problem.\n. ",
    "igraves": "Removed my previous comment.  I realized my issue was that I was trying to get puma to run with a certificate signing request instead of a signed certificate.  Previously I was self-signing and it worked fine, but when moving around some files to bring in a proper CA certificate, I mixed up my files.  The above behavior with MiniSSL happened to me given that I was using a CSR instead of a certificate.  Trying the same thing on nginx got me an error message describing the actual problem.  @billingb, perhaps try your configuration with nginx or a different web server to see if your situation is something similar?\n. ",
    "pedrocr": "I tried to work around this by using rails' ActionController::Live. I did:\n``` ruby\nGET /cameras/1/stream\ndef stream\n    if !@camera.port\n      # FIXME: return a \"waiting for camera image jpg\"\n    else\n      begin\n        response.headers['Content-Type'] = 'multipart/x-mixed-replace;boundary=SurelyJPEGDoesntIncludeThis'\n    logger.info \"Connecting to camera on port #{@camera.port}\"\n\n    Net::HTTP.start(\"127.0.0.1\", @camera.port) do |http|\n      req = Net::HTTP::Get.new \"/mjpeg\"\n\n      http.request req do |res|\n        res.read_body do |chunk|\n          response.stream.write chunk\n        end\n      end\n    end\n    #10.times {response.stream.write \"Hello!\"*500+\"\\n\"; sleep 1}\n  ensure\n    response.stream.close\n  end\nend\n\nend\n```\nThis is actually much nicer code. It integrates nicely with the rest of the resources and is in the correct location in the controller instead of stored away in an initializer. Problem is, it doesn't work properly. For some reason after sending one or two chunks the socket gets closed. It doesn't happen if I use the 10.times line instead. My guess is that perhaps the way I'm using Net::HTTP isn't thread safe and something else is trampling on it.\n. I instrumented ActionDispatch::Response::Buffer#close to figure out who was calling close on me. Here are the results\nSomeone called close and @closed is currently false\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/live.rb:57:in `close'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_dispatch/http/response.rb:233:in `close'\n/var/lib/gems/1.9.1/gems/rack-1.5.2/lib/rack/body_proxy.rb:16:in `close'\n/var/lib/gems/1.9.1/gems/rack-1.5.2/lib/rack/body_proxy.rb:16:in `close'\n/var/lib/gems/1.9.1/gems/rack-1.5.2/lib/rack/body_proxy.rb:16:in `close'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:640:in `ensure in handle_request'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:642:in `handle_request'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:361:in `process_client'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:254:in `block in run'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `call'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\nSomeone called close and @closed is currently true\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/live.rb:57:in `close'\n/home/pedrocr/Projects/camerasink/app/controllers/cameras_controller.rb:40:in `ensure in stream'\n/home/pedrocr/Projects/camerasink/app/controllers/cameras_controller.rb:40:in `stream'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/implicit_render.rb:4:in `send_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/base.rb:189:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/rendering.rb:10:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/callbacks.rb:18:in `block in process_action'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/callbacks.rb:413:in `_run__295069754__process_action__callbacks'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/callbacks.rb:80:in `run_callbacks'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/callbacks.rb:17:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/rescue.rb:29:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/instrumentation.rb:31:in `block in process_action'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/notifications.rb:159:in `block in instrument'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/notifications/instrumenter.rb:20:in `instrument'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/notifications.rb:159:in `instrument'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/instrumentation.rb:30:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/params_wrapper.rb:250:in `process_action'\n/var/lib/gems/1.9.1/gems/activerecord-4.0.4/lib/active_record/railties/controller_runtime.rb:18:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/base.rb:136:in `process'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/rendering.rb:44:in `process'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/live.rb:132:in `block in process'\nand if I use the 10.times line instead of the Net::HTTP stuff:\nSomeone called close and @closed is currently false\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/live.rb:57:in `close'\n/home/pedrocr/Projects/camerasink/app/controllers/cameras_controller.rb:38:in `stream'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/implicit_render.rb:4:in `send_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/base.rb:189:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/rendering.rb:10:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/callbacks.rb:18:in `block in process_action'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/callbacks.rb:413:in `_run__732982961__process_action__callbacks'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/callbacks.rb:80:in `run_callbacks'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/callbacks.rb:17:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/rescue.rb:29:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/instrumentation.rb:31:in `block in process_action'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/notifications.rb:159:in `block in instrument'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/notifications/instrumenter.rb:20:in `instrument'\n/var/lib/gems/1.9.1/gems/activesupport-4.0.4/lib/active_support/notifications.rb:159:in `instrument'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/instrumentation.rb:30:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/params_wrapper.rb:250:in `process_action'\n/var/lib/gems/1.9.1/gems/activerecord-4.0.4/lib/active_record/railties/controller_runtime.rb:18:in `process_action'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/base.rb:136:in `process'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/abstract_controller/rendering.rb:44:in `process'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/live.rb:132:in `block in process'\nSomeone called close and @closed is currently true\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_controller/metal/live.rb:57:in `close'\n/var/lib/gems/1.9.1/gems/actionpack-4.0.4/lib/action_dispatch/http/response.rb:233:in `close'\n/var/lib/gems/1.9.1/gems/rack-1.5.2/lib/rack/body_proxy.rb:16:in `close'\n/var/lib/gems/1.9.1/gems/rack-1.5.2/lib/rack/body_proxy.rb:16:in `close'\n/var/lib/gems/1.9.1/gems/rack-1.5.2/lib/rack/body_proxy.rb:16:in `close'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:640:in `handle_request'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:361:in `process_client'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/server.rb:254:in `block in run'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `call'\n/var/lib/gems/1.9.1/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\nSo it seems it's indeed puma that's closing the socket before the time is due. Should I open a separate bug report for this?\n. Adding \"sleep 1\" after \"response.stream.write chunk\" fixes this. So there seems to be some kind of race condition going on.\n. This seems to have something to do with the way I'm starting puma. When I use \"rails server\" instead of my own launcher script it seems to work. My script is quite trivial though:\n``` ruby\n!/usr/bin/env ruby\nif ARGV.size != 1\n  $stderr.puts \"Usage camerasink \"\n  exit 2\nend\nrequire 'fileutils'\nENV['RAILS_ENV'] ||= 'production'\nLoad the Rails application.\nrequire File.expand_path('../config/application', File.dirname(FILE))\nCamerasink::BASEDIR = ARGV[0]\nFileUtils.mkdir_p Camerasink::BASEDIR\napp = Camerasink::Application.initialize!\nRack::Server.new(:app => app, :Host => '127.0.0.1', :Port => '8080').start\n```\n. This other issue does indeed seem to be a bug. I can reproduce it just fine with the normal rails puma launching with \"rails server\". I've submitted bug #525 about this.\n. Any ideas what this could be? Would love to be able to debug this properly and get it working.\n. This seems to be a puma issue indeed. I've switched back to unicorn and it works fine.\n. ",
    "nonnenmacher": "argh, but understood.\ntoo bad because, I tried longer than necessary to make it work, with the\ngiven example file and I saw other issue around this same problem.\nPerhaps some issue, about not having puma in the Gemfile, but installed as\na standard gem\nand config.ru nor application aware that the puma cli runner is starting it\n(i.e no require 'puma' anywhere)\nso the load path, isn't set properly somehow.\nLaunching puma with cli args work, until the control status module, is\nrequired and then fail.\nOn Sat, Apr 19, 2014 at 10:28 PM, Evan Phoenix notifications@github.comwrote:\n\nClosed #524 https://github.com/puma/puma/issues/524.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/puma/puma/issues/524\n.\n. \n",
    "matthewd": "Could this be a manifestation of #743? It seems unlikely that read_body would yield an empty chunk, but the described symptoms (including sleep fixing it) otherwise sound plausible.\n. From memory, the AR connection pool reaper got better at reclaiming connections from dead threads in 4.2. That should at least address the problem when the DB connection pool is larger than the puma worker pool -- even if every worker thread (including idle ones) ends up holding a connection somehow.\n. @toymachiner62 are you able to try 4.2? Or attempt to backport https://github.com/rails/rails/pull/14360?\n. ",
    "cinic": "@chenghung what's problem?\n. ",
    "AhmedElSharkasy": "Switching from active_record store in sessions to redis_store solved the issue. \nDid not dig deeper to get the cause but i can assure there is an issue with sessions in active_record store\n. ",
    "jmagoon": "Agreed, adding that if puma is started with workers and anything in the startup process fails (typo in environment file, rackup file, etc.), the master process goes into a loop trying to open child processes and never succeeds.\n$ bundle exec puma -e development\n[4629] Puma starting in cluster mode...\n[4629] * Version 2.9.1 (ruby 2.1.2-p95), codename: Team High Five\n[4629] * Min threads: 0, max threads: 16\n[4629] * Environment: development\n[4629] * Process workers: 2\n[4629] * Phased restart available\n[4629] * Listening on tcp://127.0.0.1:9292\n[4629] Use Ctrl-C to stop\n/Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie/configuration.rb:95:in `method_missing': undefined method `c' for #<Rails::Application::Configuration:0x007fabec5c74f8> (NoMethodError)\nfrom /Users/jmagoon/rails/portal/config/environments/development.rb:3:in `block in <top (required)>'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie.rb:210:in `instance_eval'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie.rb:210:in `configure'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie.rb:182:in `configure'\n    from /Users/jmagoon/rails/portal/config/environments/development.rb:1:in `<top (required)>'\n...\n/Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie/configuration.rb:95:in `method_missing': undefined method `c' for #<Rails::Application::Configuration:0x007fabef85b248> (NoMethodError)\n    from /Users/jmagoon/rails/portal/config/environments/development.rb:3:in `block in <top (required)>'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie.rb:210:in `instance_eval'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie.rb:210:in `configure'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/bundler/gems/rails-3a00c6f54530/railties/lib/rails/railtie.rb:182:in `configure'\n    from /Users/jmagoon/rails/portal/config/environments/development.rb:1:in `<top (required)>'\nThat error continues indefinitely until manually killing the master process. When daemonized, both the error and the loop state are opaque to the end user.\n. It looks like pumactl isn't checking for a response from the Process.kill('SIGUSR2',pid). I don't know enough about the Process library, but I'm not sure if it can. For me, Process.kill('SIGUSR2',pid) returns 1 whether or not puma actually does a restart. Doing a system call %x(kill -s SIGUSR2 <pid>) always returns an exit status of 0, whether puma restarts or crashes.\nSuccess:\nirb(main):010:0> Process.kill('SIGUSR2',3922)\n=> 1\n[3922] - Gracefully shutting down workers...\n[3922] * Restarting...\n[3922] Puma starting in cluster mode...\n[3922] * Version 2.8.2 (ruby 2.1.2-p95), codename: Sir Edmund Percival Hillary\nFailure:\nirb(main):012:0> Process.kill('SIGUSR2',3922)\n=> 1\n[3922] - Gracefully shutting down workers...\n[3922] * Restarting...\n/Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/cli.rb:424:in `fileno': closed stream (IOError)\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/cli.rb:424:in `block in restart!'\n    from /Users/jmagoon/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/cli.rb:423:in `each'\nMy current hacky workaround is to try to restart, check if the PID is running, and if not, try a cold start. If that fails, then I can handle the error (neither Process nor a system call return any puma errors).\n. ",
    "GetContented": "Yeah I'm having an issue where it seems to be ignoring my command line arguments... I specify --threads 1:1 and it ignores this and any other command line arguments I specify and just rungs 1:16... Guh... how annoying.\nI'm trying to replicate a bug locally that I'm experiencing on my server environment and my hunch is it's related to concurrency, but I need to constrain the concurrency to 1 or 2 threads max, and one process, so I can try to replicate the erroring context.\n. Oh actually I realised that I just needed to modify the config file setup in my procfile. I would have thought command line arguments would have overridden that, though :(\n. ",
    "albertnetymk": "\nPuma uses some threads for other things (such as handling slow clients) and that is what you're seeing.\n\nAnywhere I can see more documentation on those extra threads?\n. Using puma on herkou, and newrelic reports that I have 11 threads totally; not sure if that's reasonable and wonder if it's possible to bring it down a bit due to the frequent Memory Quota Exceeded warning.\n. http://newrelic.com/ can do it.\n. ",
    "vizcay": "Being caught by this also, wasted almost an hour tweaking the thread count wondering what exactly was puma doing because it didn't matched the numbers. Well, does some admin mind if I send a PR to state this in the README for the next curious soul?. I've just been bitten by this, here is my config:\nRuby: 2.3.3p222\nPuma: 3.9.1\nRails: 5.0.1\nbundler: 1.15.13\nrbenv: 1.1.1-2g615f844\nLinux: Ubuntu 16.04. I can confirm that switching back to puma 3.8.1 solved the issue.. @prashantvithani yes it's looks like an exact duplicate, sorry.. Fixed typo @nateberkopec.. ",
    "AntelopeSalad": "False alarm. My restart command wasn't correct. I was calling to restart the service instead of just pointing to the puma binary. I also had to change to USR1 to get the phased restart which does work.\n. ",
    "jume-dev": "well, actually it's not about ajax :(\nIt's a normal request for a website, with some javascript on it. And some of that assets get served as plain text.\n. Sorry, for not reporting back in this issue. It seems like it has nothing to do with puma. I traced the problem back to the cms i was using.\nIf you are interested in the problem, maybe take a look at the issue there comfy/comfortable-mexican-sofa#461\nBut we didn't find a solution yet, disabling the internal js and css files form comfy works for now.\n. ",
    "indirect": "nevermind! sorry! I was returning a hash to Sinatra, and it was coercing it into an Array, and then feeding it to Puma to return as a response. :(\n. Nothing changed that would effect that code that I know of, but I guess something did. FWIW the backtrace doesn't include anything but Rubygems, so it seems likely that the pruned load path somehow doesn't include rack.\n. I just discovered that everything works perfectly with the line rackup DefaultRackup removed completely, since (I assume) Puma can find the default config.ru file without being explicitly told to look for it. Still a pretty weird regression, though. :)\n. I have no idea how the jar gets built; if you're using the gem from git on JRuby, you have to build the jar as part of the release process, right?\n. I would also strongly prefer that it be possible to build the jar from source if Bundler is installing from git\u2014is it possible to make that happen via the usual gem build process?\n. ",
    "MilesChatterji": "How did you solve this? I realize what is happening, I have tried a few .to_s possibilities but with on luck. I thought about writing some extra code to parse, but I thought maybe there is a simpler way, and im just thinking about it to hard. \nThanks,\n-Miles\n. ",
    "chulkilee": "It works in my case. Travis build failures are due to timeout - @vlmonk could you rebase and push it again? Probably the last patch for gemfile may be related to failure in travis..\n. Apparently there is a bug in this PR so the test app server fails to start (and CI gets timeout error). I'm going to rebase it after #672 is merged to master to minimize the code change of this PR.\n. For the record: workaround is passing PORT variable to bin/rails, which is picked up by puma through config/puma.rb, not through rack handler.. ",
    "vlmonk": "@chulkilee I rebased branch, but travis still can't build it\n. same problem with puma 2.9.0, ruby 2.0.0p481\npuma complete ignore pumactl restart and kill -USR2 <pumapid>\nmy config: \ndirectory \"/home/deploy/app/current\"\nenvironment \"production\"\ndaemonize true\npidfile  \"/home/deploy/app/shared/puma/puma.pid\"\nstate_path \"/home/deploy/app/shared/puma/puma.state\"\nthreads 0, 8\nbind \"unix:///home/deploy/app/shared/puma/puma.sock\"\nworkers 0\n. ",
    "memiux": "My goal here is to keep the \"Need a bit of security? Use SSL sockets!\" updated, I don't want to break anything nor set some requirements.\n. ",
    "CameronNemo": "Why not just change the Upstart job and add a line env PWD=/home/user/apps/app/current ? That would emulate the cd behaviour so that the working directory is gotten from an envvar instead of getting the (resolved) working directory.\n. ",
    "vimutter": "@sorentwo Yes, as usual, cause was not in puma, but in faye itself. Calls to close didn't close opened sockets, had to deal with that. Thanks for your attention and sorry for false alarm.\n. ",
    "dcrec1": "What happens otherwise?\n. Sorry, how do I resolve this problem? Im using a Rails application and when restarting I get the follow message:\n/usr/local/lib/ruby/gems/2.3.0/gems/bundler-1.13.6/lib/bundler/runtime.rb:40:inblock in setup': You have already activated rack 2.0.1, but your Gemfile requires rack 2.0.3. Prepending bundle exec to your command may solve this. (Gem::LoadError)`. ",
    "YorickPeterse": "To clarify, we use custom options to \"bootstrap\" the applications. In some cases these applications need datasets that are either proprietary or too large to embed in the Gem itself. In those cases a user can specify an option to have a file downloaded automatically and the path stored in an environment variable. This way the user can provide their own datasets if they want to without having to change any code.\n. ",
    "bsnape": "Sure! Whatever works for you.\nJust pushed the change. :+1: \n. @seuros please can this be merged in now?\n. Hey @nateless, sorry I got sidetracked by other things at work but I'm happy to get this moving again.\nI was a little confused by @evanphx 's reply as it looks like there is a single startup message:\nhttps://github.com/puma/puma/blob/b90fc4fa05108f8e6a8266f69787cbe0efaaab42/lib/puma/runner.rb#L95\nBut two different shutdown messages (both STDOUT and log):\nhttps://github.com/puma/puma/search?q=goodbye&type=Code\nShould those be reconciled or am I barking up the wrong tree?\nAs I said, happy to get this moving again as it will be useful for myself and others.\n. ",
    "nateless": "@bsnape, did you find the problem? I think our issue #554, can be related.\n. We had, however they were empty, only timestamps of puma starting.\n. @evanphx well, found that error. The problem was in tubesock gem with redis. Seems like they don't play well. Sorry about it.\n. ",
    "fred": "Im also having this issue in production. Looks like it's a serious issue.\nWhen I running pumactl command like this\nbundle exec pumactl -S /app/shared/tmp/pids/puma.state restart\n==> puma.stderr.log <==\n/home/ubuntu/.rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/dependency.rb:298:in `to_specs': Could not find '-C' (>= 0) among 339 total gem(s) (Gem::LoadError)\n    from /home/ubuntu/.rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/dependency.rb:309:in `to_spec'\n    from /home/ubuntu/.rbenv/versions/2.1.2/lib/ruby/2.1.0/rubygems/core_ext/kernel_gem.rb:53:in `gem'\n    from /home/ubuntu/www/hipflat/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/bin/puma-wild:10:in `block in <main>'\n    from /home/ubuntu/www/hipflat/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/bin/puma-wild:8:in `each'\n    from /home/ubuntu/www/hipflat/shared/bundle/ruby/2.1.0/gems/puma-2.8.2/bin/puma-wild:8:in `<main>'\nThen puma fails to load.\n. But when using phased-restart I dont see the Bundle error.\nbundle exec pumactl -S /app/shared/tmp/pids/puma.state phased-restart\n. thanks for the fix, working great now.\ncheers.\n. ",
    "bmorton": "We're seeing the same exact thing on 2.9.1 still.  If we remove prune_bundler, things work fine.  Tested on both Ruby 2.0.0-p481 and 2.1.4 with the same result as above.\n@tbuehlmann did you ever get a resolution on this?\n. We're using upstart to bring it up/down and sending USR2 to hot restart.  Here's the config for that: https://gist.github.com/bmorton/adb2488cca9a62db61c9\n. Oh sweet, thanks for the quick turnaround!  I'll give it a go and report back.\n. Looks like that fixes it... thanks again! :+1: \n. ",
    "richardrails": "update it to latest 2.12.3 and it works\n. ",
    "EMRJ": "Getting the same above, how to trace errors?\nPuma 2.9.0\nRails 4.1.0\n. @evanphx, There is no error in puma_error log only the timestamp is present, getting only timeout error - \"Net::ReadTimeout: Net::ReadTimeout\" \nI have used Puma server as API server, more number of requests will be made within a minute and i set min and max thread size as \"8:64\" and workers as \"2\" but still the server goes down without any notification(error) when i search(grep) for puma it shows Puma server is still running but not accessible\nI don't have any clue why the server goes down\n. ",
    "moofish32": "+1 here - nearly identical behavior.  The major difference is our architecture directly uses Scala/Akka libraries to communicate to another system.  Puma 2.7.1 is stable and works Puma 2.8.2 and Puma 2.9.0 have the silent crash.  Typically after being up 4 + hours with steady test client activity.  I don't have a public piece of code or example that I can separate under the current schedule (which I know makes this almost impossible to trouble shoot). Rails 4.0.10, 4.0.3, & 4.0.5 were used.\n. ",
    "wuboy0307": "+1 here. rails 4.1.8.\nI also use websocket-rails with standalone thin server. Is websocket an issue? @nateless also has websocket with tubesock.\n. ",
    "sheharyarn": "Same issue, nothing of importance in the logs. Using Ruby 2.2, Rails 4.2 and Puma 2.11.1. \n. @evanphx After a few hours of running. Here are some graphs from my Digital Ocean Droplet:\n\n\nIt crashes every few hours, then I get a notification from Uptime Robot and I start Puma again. There's nothing of importance in the Puma Logs (well, at least that's what I think). Puma Access log  has occasional IOErrors but I think it's related to the External Binary I'm using, and Puma Error log only shows timestamps of when the server starts.\nPuma.error.log:\n=== puma startup: 2015-04-07 00:12:08 -0400 ===\n* Listening on unix:///home/deploy/apps/kholopk/shared/tmp/sockets/kholopk-puma.sock\n=== puma startup: 2015-04-07 02:19:16 -0400 ===\n* Listening on unix:///home/deploy/apps/kholopk/shared/tmp/sockets/kholopk-puma.sock\n=== puma startup: 2015-04-07 04:15:19 -0400 ===\n* Listening on unix:///home/deploy/apps/kholopk/shared/tmp/sockets/kholopk-puma.sock\n=== puma startup: 2015-04-07 06:16:15 -0400 ===\n* Listening on unix:///home/deploy/apps/kholopk/shared/tmp/sockets/kholopk-puma.sock\nI even tried removing the PumaWorkerKiller Gem and downgrading Puma to 2.7.1 (like @moofish32 said), but nothing worked. I don't have a clue what to do.\nCould this be an issue with heavy user traffic? Because I have another droplet that was set up almost the exact same way, but it doesn't crash and it has almost 0 traffic, on the other hand this droplet gets around >1k Page Views a day. Both of these droplets were set up this way.\n. Okay, I've looked into it a bit more, it seems that the issue was with the Memory Leaks in Ruby 2.2.0 that have existed in Ruby 2.1.x as well. Around 10 hours ago, I downgraded to ruby-2.0.0-p598 and Puma hasn't crashed since, so I'm hoping that the issue has been resolved (Previously it was crashing every 2-3 hours).\nI'm going to keep an eye on it for another few days, and will report back. Thanks for your input!\nSources:\n- Stackoverflow: Does Ruby 2.2 have Memory Issues?\n- Ruby Bug Tracker: #10686 - Memory leaking from torture test of symbol GC\n- Official Ruby Blog: Ruby 2.2.1 Released\n. @evanphx @MisterFine1 I don't think I might be able to report much because I'm not much of an expert. I also don't think the issue is with any other gems. I also checked my /var/log/syslog, and the last entry in it was from a month ago when I first created my droplet.\nMy app uses Mongoid, but I'm not writing anything to the DB at the moment, because all my app does is display some links to Youtube Videos that I get from the Youtube API I'm consuming. I'm also calling an external program youtube_dl using the system method for a JSON dump for some video information that I do not get using the official API.\nThis is my Gemfile:\n``` ruby\nsource 'https://rubygems.org'\nruby '2.0.0' \ngem  'rails', '4.2.0'\ngroup :assets do\n    gem 'sass-rails',       '~> 5.0'\n    gem 'coffee-rails',     '~> 4.1.0'\n    gem 'uglifier',         '>= 1.3.0'\nend\ngroup :development do\n    gem 'spring'\n    gem 'byebug'\n    gem 'web-console',      '~> 2.0'\ngem 'capistrano',         require: false\ngem 'capistrano-rvm',     require: false\ngem 'capistrano-rails',   require: false\ngem 'capistrano-bundler', require: false\ngem 'capistrano3-puma',   require: false\n\nend\ngroup :production do\n    gem 'puma',                 '2.7.1'\n    # gem 'puma_worker_killer'\nend\ngem 'twitter-bootswatch-rails', '~> 3.3.2'\ngem 'twitter-bootswatch-rails-helpers'\ngem 'less-rails'\ngem 'jquery-rails'\ngem 'slim'\ngem 'execjs'\ngem 'turbolinks'\ngem 'therubyracer'\ngem 'rails_autolink'\ngem 'youtube_it'\ngem 'rest-client'\ngem 'nokogiri'\ngem 'awesome_print'\ngem 'quiet_assets'\ngem 'devise'\ngem 'mongoid'\ngem 'newrelic_rpm'\n```\n. @kriansa Not completely, but it helped. It's still a very small VPS (with only 512mb of RAM) and my app is memory-intensive. It's crashed only 2 times since downgrading. I've not tried Ruby 2.2.2 yet, but I've heard it fixes a lot of the memory issues.\n. ",
    "MisterFine1": "@sheharyarn I work with moofish32.  We've been trying to figure this out but don't have a reliable way to re-create it.  It sounds like you do.  Have you been able to look in system logs or other logs to look for out of memory, security or any other related events that may have occurred at the same time?  Can you identify any pattern of the type of request it's serving, database being persisted to, etc. for any application layer patterns?\nLastly, could you (or have you) tried doing detailed logging/tracing (TracePoint) to try and identify any common patterns or locations in the code that are being accessed prior to failure?\nSorry for the lengthy/open questions... we're just trying hard to look for anything that might help clear this up.  It's a serious problem for us.\n. @evanphx... I hope you are right that is something within our app.  We use jruby exclusively, and haven't seen any memory leaks (we monitor the jvm).  Our app does do some it's own internal threading, using Akka as the actor system.  \nI'm a little confused by your idea that this failure could be caused by a deadlock.  If it were, wouldn't the symptom be long delayed or never-ending requests?  We are simply seeing Puma disappear without any logging.  It just vanishes.  I'm not clear how that would be from a deadlock.\n. ",
    "kriansa": "Hey @sheharyarn, downgrading fixed it for you? Did you tried Ruby 2.2.2?\n. ",
    "raldred": "I'm also getting silent crashes on Heroku using Puma 2.14.0 with Ruby 2.2.2.\nMine is not consistent like yours, it's rather random infact.\nAlthough I'm at a loss as to how to debug due to lack of error logging when workers & threads die or become blocked. The only info I have is a ton of H12 errors. A restart of the app server usually resolves it for a while. NewRelic reveals nothing sinister either.\nI have 4 dynos each with 2 workers and 3 threads.\n. @nateberkopec any possibility this has been regressed in 3.7.0? . ",
    "shingonoide": "For this problem you have to donwgrade Rubygems to 2.3.0 version\ngem update --system 2.3.0\nI've used the openssl-x86 version even in x64 system to compile works.\n. ",
    "ariveira": "An important detail, this instability does not occur when the application is running under Rainbows\nmy configuration: puma 2.8.2, ruby 2.1.1p76, rails 2.3.11\n. Error during failsafe response: incompatible character encodings: ASCII-8BIT and UTF-8\n2014-07-18 11:33:08 -0300: Read error: #<NoMethodError: undefined method each' for nil:NilClass>\n/opt/objectdata/ruby-2.1.1/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/server.rb:557:inhandle_request'\n/opt/objectdata/ruby-2.1.1/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/server.rb:361:inprocess_client'\n/opt/objectdata/ruby-2.1.1/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/server.rb:254:inblock in run'\n/opt/objectdata/ruby-2.1.1/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:incall'\n/opt/objectdata/ruby-2.1.1/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n. Hi Evan !\nI suppose that in addition to force the encoding in the string must have a \"protection\" in the thread via begin / rescue, because from this thread simply stops working. If I have 4 threads serving requests one in four requests comes in white. I still have probemas of blank screens on Puma but could not simulate other errors, but happen in 100 customers in one or two per week.\nThis problem has to do with the log generated, as in the development environment does not have the error, and production to solve the problem you can use one of the options below:\nconfig / enviroments / production.rb\nconfig.logger Logger.new = ('/ dev / null')\nor\nrequire 'syslog / logger'\nconfig.logger = :: Syslog Logger.new\n. Evan... this occurs not only in the development environment into production\n. I managed to solve the problem in rails (2.3.11) creating a monkey patch\nmodule ActiveSupport\n  class BufferedLogger\n    def add(severity, message = nil, progname = nil, &block)\n      return if @level > severity\n      message = (message || (block && block.call) || progname).to_s\n      # If a newline is necessary then create a new message ending with a newline.\n      # Ensures that the original message is not mutated.\n      message = \"#{message}\\n\" unless message[-1] == ?\\n \n      # aqui\n      buffer << message.force_encoding('UTF-8')\n      auto_flush\n      message\n    end \n  end \nend\n. I forced encoding here:\nmessage.force_encoding('UTF-8'),\nBut if some other error log or io occur there will still be the problem reported\n. Hi Evan !!!\nIf I started puma with 4 threads, up when the page is accessed, one of four connections comes in white. If 4 connections for page problem is run puma stop to respond, only blank pages are now displaying.\nIn puma.server in method process_client, I disabled the lines containing\nrescue StandardError => e\nclient.write_500\n@events.unknown_error self, e, \"Read\"\nAn error screen indicating reset request is displayed, but the above explained instability does not occur.\nTanks for your help\nAlexandre Riveira\n. Tanks SalmonL\nI believe it is the fact that I'm still using rails 2.3.11\nError during failsafe response: incompatible character encodings: ASCII-8BIT and UTF-8\n2014-08-16 19:16:09 -0300: Read error: NoMethodError: undefined method each' for nil:NilClass /opt/objectdata/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/server.rb:557:inhandle_request'\n/opt/objectdata/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/server.rb:361:in process_client' /opt/objectdata/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/server.rb:254:inblock in run'\n/opt/objectdata/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:in call' /opt/objectdata/ruby-2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.8.2/lib/puma/thread_pool.rb:92:inblock in spawn_thread'\n. ",
    "SaimonL": "I am using Ruby 2.1.2 with Rails 4.1.5. Using nGinX 1.6 in Debain 7.6. puma version 2.9.0\n using Socket connection. \nSo far I am not get this error.\n\nhttp://production.linuxkde.com/sections/\ufffd\ufffd\ufffd?z=\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd@?\nhttp://production.linuxkde.com/sections\ufffd\ufffd\ufffd?z=\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd@?\n\nMaybe I did not understand the question.\n. I am not using any capistrano at the moment and the way I restart puma is to run this bash command from the application directory. Works every time, however if you did bundle update or any Gemfile changes and then puma instance may not restart successfully (rare).\n\n#!/bin/bash\nkill -s SIGUSR2 $(cat tmp/pids/puma.pid)\n\nI see you are doing \n\nworkers 8\n\nWhich I think it means you are doing cluster server which I have no experience in. You will have to wait for some one from Puma to reply.\n.  \nUsed to take down the server but would not start it back up. But now it works for me, puma gracefully restarts the rails server by killing old threads one at a a time and then starting new ones. I tested it in CentOS 7 (64-Bit) and in Debain 7.7 (64-Bit) and both works.\nUsing puma 2.9.2\n. ",
    "Leooo": "Did you manage to get rid of this problem? have the same issue since months, and I cannot get a clean SSL in development because of it.\n. Having the same issue, puma_worker_killer doesn't restart a worker until after a user disconnects. Did you find a way to solve this properly? Could interrupt the SSEs from the server, but don't want to interrupt all the SSEs, would like to shut only the connections created by the worker which is shutting down..\n. Tried to keep the ActionController::Live streaming inside my main app, and indeed I have the same issue: I tried to shut down all the SSEs for the worker I want to restart by fetching the index of the worker in the SSE loop:\nObjectSpace.each_object(Puma::Cluster::Worker).map { |obj| obj }.first.try(:index)\nBut this is not enough: I have to close ALL SSEs connections for ALL workers in my app before being able to restart a particular worker, even the connections  on other workers. Seems like a bug to me (from Puma cluster, not from ActionController::Live?) and that makes ActionController::Live with Puma not scalable.\n=> Feature request: be able to restart a worker even if some (SSEs) processes on other workers are running?\n. Hi @FlySnake, did you manage to isolate your bug? Have the same behavior with puma / postgres / streaming.\n. ok same here, using ActionController:Live extensively, haven't tried to use it without DB access but this would seem laborious in our case, waiting for ActionCable too. Thanks.\n. ",
    "chytreg": "Have the same issue any cure for that?\n. Thx for the tip, will check it.\n. I have exactly the same issue and cannot track down the cause either.. As for me and some others, it happens also during runtime as well. ref: https://github.com/puma/puma/issues/1502#issuecomment-362289871. ",
    "raskhadafi": "With this change I got this error\n```\n13889] + Gemfile in context: /services/private/apps/root/test/spoc/releases/20140723153744/Gemfile\n[13889] Worker directory /services/private/apps/root/test/spoc/releases/20140723153744\n[13893] + Gemfile in context: /services/private/apps/root/test/spoc/releases/20140723153744/Gemfile\n[13893] Worker directory /services/private/apps/root/test/spoc/releases/20140723153744\n[12601] - Worker 1 (pid: 13893) booted, phase: 3\n[12601] - Worker 0 (pid: 13889) booted, phase: 3\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35: [BUG] Segmentation fault at 0x00000000002d86\nruby 2.1.2p95 (2014-05-08 revision 45877) [x86_64-linux]\n-- Control frame information -----------------------------------------------\nc:0008 p:---- s:0033 e:000032 CFUNC  :new\nc:0007 p:0064 s:0030 e:000029 METHOD /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35 [FINISH]\nc:0006 p:---- s:0025 e:000024 CFUNC  :new\nc:0005 p:0060 s:0020 e:000019 BLOCK  /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:295 [FINISH]\nc:0004 p:---- s:0015 e:000014 CFUNC  :each\nc:0003 p:0066 s:0012 e:000011 METHOD /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:289\nc:0002 p:0007 s:0004 e:000003 BLOCK  /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:273 [FINISH]\nc:0001 p:---- s:0002 e:000001 TOP    [FINISH]\n-- Ruby level backtrace information ----------------------------------------\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:273:in block in run'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:289:inhandle_servers'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:289:in each'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35: /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:295:inblock in handle_servers'\n[BUG] /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:295:in new'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35:ininitialize'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35:in `new'\nSegmentation fault at 0x00000000002d86\nruby 2.1.2p95 (2014-05-08 revision 45877) [x86_64-linux]\n-- C level backtrace information -------------------------------------------\n-- Control frame information -----------------------------------------------\nc:0008 p:---- s:0033 e:000032 CFUNC  :new\nc:0007 p:0064 s:0030 e:000029 METHOD /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35 [FINISH]\nc:0006 p:---- s:0025 e:000024 CFUNC  :new\nc:0005 p:0060 s:0020 e:000019 BLOCK  /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:295 [FINISH]\nc:0004 p:---- s:0015 e:000014 CFUNC  :each\nc:0003 p:0066 s:0012 e:000011 METHOD /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:289\nc:0002 p:0007 s:0004 e:000003 BLOCK  /services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:273 [FINISH]\nc:0001 p:---- s:0002 e:000001 TOP    [FINISH]\n-- Ruby level backtrace information ----------------------------------------\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:273:in block in run'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:289:inhandle_servers'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:289:in each'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:295:inblock in handle_servers'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/server.rb:295:in new'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35:ininitialize'\n/services/private/apps/root/test/spoc/shared/bundle/ruby/2.1.0/bundler/gems/puma-50c31e9f288a/lib/puma/client.rb:35:in `new'\n-- C level backtrace information -------------------------------------------\n```\n. ",
    "sheltond": "I have the same requirement.\nI'm currently using https://github.com/mperham/girl_friday, and am likely to switch to using https://github.com/brandonhilkert/sucker_punch in the future, but they and other similar things all need the ability for worker process shutdown to be delayed until their background threads are finished.\nI'll have a look at implementing something.\n. Yes, sorry about that. Should be fixed now.\nOn Mon, Aug 4, 2014 at 5:11 PM, Evan Phoenix notifications@github.com\nwrote:\n\nIn lib/puma/thread_pool.rb:\n\n@@ -176,7 +176,9 @@ def shutdown\n# Use this instead of #each so that we don't stop in the middle\n   # of each and see a mutated object mid #each\n-      @workers.first.join until @workers.empty?\n-     if !@workers.empty?\n\nCan you fix the formatting of this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/pull/566/files#r15763791.\n. \n",
    "rhinon": "I'm having a similar issue with puma 2.9.0 and jruby-1.7.13\nPretty much exactly the same as what is described in https://github.com/puma/puma/issues/436\n. ",
    "urkle": "this is not working at all under RHEL 6 64-bit.. \npuma 2.12.1 on ruby 2.2.1.\nany time I do a pumactl restart, phased-restart, or use the kill -USR2 / kill -USR1, puma simply exits.  This is rather disappointing that a core piece functionality is severely broken in puma.\nWhen I start puma (non-daemonized) in one terminal (pumactl start) and restart it from another terminal (pumactl phased-restart) here is the output I get.\n[2856] Puma starting in cluster mode...\n[2856] * Version 2.12.1 (ruby 2.2.1-p85), codename: Plutonian Photo Shoot\n[2856] * Min threads: 5, max threads: 5\n[2856] * Environment: production\n[2856] * Process workers: 2\n[2856] * Preloading application\n[2856] * Listening on tcp://0.0.0.0:3000\n[2856] Use Ctrl-C to stop\n[2856] - Worker 0 (pid: 2863) booted, phase: 0\n[2856] - Worker 1 (pid: 2868) booted, phase: 0\n[2856] * phased-restart called but not available, restarting normally.\n[2856] - Gracefully shutting down workers...\n[2856] * Restarting...\nAvailable commands: halt, restart, phased-restart, start, stats, status, stop, reload-worker-directory\nwhy would it spit out \"available commands\" like that ???\n.. time passes..\n Well, it seems I found the issue..   if I start the server with puma instead of pumactl it restarts correctly!!  So it seems somewhere in the code the restart_command is being incorrectly calculated when using pumactl to start the app\n. I was running \npumactl start  and  pumactl phased-restart\nto start the server and now am changing it to  puma\nIt would be better to reimplement it in terms of exec'ing puma, as right now (from others on this bug) running pumactl restart will start puma if it's not running (though presumably via the wrong command)\nAs for my configuration\n```\nworkers Integer(ENV['WEB_CONCURRENCY'] || 2)\nthreads_count = Integer(ENV['MAX_THREADS'] || 5)\nthreads threads_count, threads_count\npreload_app!\ndaemonize\nrackup      DefaultRackup\nport        ENV['PORT'] || 3000\nenvironment ENV['RAILS_ENV'] || 'development'\nstate_path  './tmp/puma.state'\npidfile     './tmp/pids/puma.pid'\nactivate_control_app\n```\n. Any progress in getting this properly fixed?\n. Regardless of that, the issue still remains pumactl start sets the wrong process name so restarting does not function.\n. ",
    "isaiah": "@urkle In the readme it says:\n\nNote that preload_app can\u2019t be used with phased restart, since phased restart kills and restarts workers one-by-one, and preload_app is all about copying the code of master into the workers.\n\n@evanphx Maybe you should put that in a big banner :)\n. ",
    "tonytonyjan": "I encountered the same problem with the config below (no preload_app!):\nruby\nthreads 8, 32\nworkers 4\nbind 'tcp://0.0.0.0:3000'\ndaemonize\nruby 2.2.3p173\npuma version 2.14.0\n. ",
    "localhostdotdev": "For future Googlers like me, now it's:\n```\n!/bin/bash\nkill -s SIGUSR2 $(cat tmp/pids/server.pid)\n```\n(credit to @SaimonL). ",
    "ur5us": "I also do experience the same issue running in cluster mode but using Puma 2.9.2, Bundler 1.7.2 and Ruby 2.1.2. It happens randomly and as far as I can tell changing the Gemfile is not the main factor, in other words, even if the Gemfile(.lock) hasn't changed it still happens.\n/home/deploy/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler/definition.rb:22:in `build': /srv/www/.../releases/20140828024001/Gemfile not found (Bundler::GemfileNotFound)\n        from /home/deploy/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler.rb:154:in `definition'\n        from /home/deploy/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler.rb:117:in `setup'\n        from /home/deploy/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler/setup.rb:17:in `<top (required)>'\n        from /home/deploy/.rbenv/versions/2.1.2/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:54:in `require'\n        from /home/deploy/.rbenv/versions/2.1.2/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:54:in `require'\n. ",
    "drush": "@sheltond Do you have some sample code where you utilize this for background threads that start cleanly and close gracefully?  Just looking to make sure signal hooks are used if needed, etc.\n. ",
    "repeatedly": "Ping?\n. I tried git master and it seems to resolve the problem.\nIncreasing memory usage is too slow and decreases memory usage after GC ran.\nThanks for the fix!\n. Thx!\n. ",
    "Geokoumpa": "I decided using a separate server for SSEs. I used this gem http://mikeatlas.github.io/realtime-rails/ first, but the node server proposed here uses websockets, which didn't work for me due to infrastructure restrictions. So I made a similar express.js server with SSEs and kept using the same gem server side for authentication. Let me know if you need to use the SSE version.\n. ",
    "cjmarkham": "I have since realised I was using a 32bit install of Ruby. I have changed to 64bit but still get some errors and can't install puma.\n```\nC:/Ruby200/bin/ruby.exe extconf.rb --with-opt-dir=C:\\OpenSSL\ncreating Makefile\nmake \"DESTDIR=\"\ngenerating puma_http11-i386-mingw32.def\ncompiling http11_parser.c\nIn file included from c:/Ruby200/include/ruby-2.0.0/ruby.h:33:0,\n                 from http11_parser.h:10,\n                 from ext/http11/http11_parser.rl:5:\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:125:14: error: size of array 'ruby_check_sizeof_voidp' is negative\nIn file included from c:/Ruby200/include/ruby-2.0.0/ruby.h:33:0,\n                 from http11_parser.h:10,\n                 from ext/http11/http11_parser.rl:5:\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h: In function 'rb_float_value':\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:826:13: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h: In function 'rb_num2char_inline':\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1214:35: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1214:35: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1214:35: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1215:9: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1215:9: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1215:9: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h: In function 'rb_class_of':\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1515:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h: In function 'rb_type':\nc:/Ruby200/include/ruby-2.0.0/ruby/ruby.h:1532:12: warning: cast to pointer from integer of different size [-Wint-to-pointer-cast]\next/http11/http11_parser.rl: In function 'puma_parser_execute':\next/http11/http11_parser.rl:111:3: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\nmake: *** [http11_parser.o] Error 1\n```\n. ",
    "callmeed": "Do you also have an /etc/puma.conf file? This tripped me up too, but there are actually 2 puma.conf files: (1) the one you have in /etc/init and (2) another in /etc/ which should contain the path to your app (one on each line for multiple apps)\n. ",
    "finist": "I switched to runit, this is the best solution for upstart application ;)\n. ",
    "minasmart": "alright, but testing on production machines, when a phased-restart is issued, there is a 5 second delay between each kill/start cycle. Is there a way to configure that, or is it hard-coded somewhere else?\n. Awesome! Thank you!\n. ",
    "FartParty": "https://github.com/puma/puma/blob/master/docs/signals.md\nYou want SIGTERM.\n. Turns out this is a duplicate of #573, apologies.\n. ",
    "fields": "Thanks!\n. ",
    "igormoochnick": "It hangs at the end of the next line:\nPuma starting in single mode...\n. ",
    "paulepanter": "@igormoochnick, were you able to solve this problem? We do not have any problems with our Docker containers. But we use Supervisor to start it.\n. If you still have the problem, please share the Docker version you use and the Dockerfile so people can try to reproduce it.\n. Was there a commit implementing the proper checking? I couldn\u2019t spot it at first look.\n. Am Freitag, den 12.12.2014, 17:34 -0800 schrieb Evan Phoenix:\n\nWhy is passing the name of a config file via an environment variable\nbetter than directly to puma?\n\nSorry for being unclear. I do not want to pass the name of the\nconfiguration file.\nI try to figure out how to pass the run-time options for JRuby and the\nJVM. If I am not mistaken, this is currently only supported by using\nenvironment variables like JRUBY_OPTS and/or JAVA_OPTS.\nUnfortunately this currently causes some duplication effort as you have\nto configure the Jungle init scripts and the Capistrano scripts to pass\nthese.\nSo it\u2019d be great to be able to place these into Puma\u2019s configuration\nfile as the configuration file is used in both methods, that means\nJungle init scripts and Capistrano\n. Am Samstag, den 13.12.2014, 11:50 -0800 schrieb Evan Phoenix:\n\nOh, you want to set JAVA_OPTS from a puma config file?\n\nExactly.\n\nThere is a chicken and egg problem there, so that probably won't work.\n\nThat\u2019s what I feared. Is a wrapper a solution, that reads the config\nfile and then starts the \u201creal\u201d Puma process?\n. ",
    "yurgon": "why?\nalternative?\n. MMMM....\nI need: someapp1.com someapp2.com someapp3.com\n. ",
    "ppsshh": "For some reason, when running rake test for ruby-1.9.3, travis uses minitest-5.4.1 while it should use minitest-4.7.5 as it is specified in Gemfile. Travis bug?\n. Ok, all done!\nBTW, what do you think about this PR? I'm still unsure.\nOn one hand it may be not good to break previous behavior (because someone can rely on it), but on the other hand, using dash in place of filename is a standard way to tell that program should read it from STDIN.\n. Also, I've found that it makes pumactl restart unable to reuse configuration that have been received from STDIN. In other words this PR is not ready to merge yet.\nWill try to find a solution to this soon.\n. ",
    "renier": "@evanphx I'm not, but using in internet-facing servers. I know you can put things behind a web server like nginx, but would rather just be able to configure this in puma for dev/test purposes.\n. @evanphx I think I can work on this patch. Would you prefer for SSLv3 to be enabled or disabled by default in the puma server?\n. @evanphx \nThere is still a potential problem with this.\nIf you try to open an SSLv3 socket to test for the POODLE vulnerability:\nopenssl s_client -connect localhost:443 -ssl3\nYou will see that the connection hangs. It is not being terminated properly by the server.\nIn mini_ssl.c#engine_read, I think you need to accept more SSL_get_error() values to send the EOF to.\nc\n    e = SSL_get_error(conn->ssl, bytes);\n    if(e == SSL_ERROR_ZERO_RETURN || e == SSL_ERROR_SYSCALL || e == SSL_ERROR_SSL) {\n        rb_eof_error();\n    }\nIf I test this locally, the connection is closed as I would have expected when I try the test above.\n. Hi, @evanphx. Pinging on the above.\n. ",
    "chriskilding": "If you look on Rubygems you see that every version of puma has 2 builds uploaded, one with a c extension, the other with a java extension, apart from 2.9.2, which at the current time is missing its Java version. Hence it fails.\n2.9.2 October 30, 2014 (88.5 KB)\n2.9.1 September 5, 2014 (87.5 KB)\n2.9.1 September 5, 2014 java (113 KB)\n2.9.0 July 13, 2014 (87.5 KB)\n2.9.0 July 13, 2014 java (113 KB)\n. ",
    "StriderKeni": "This solve my problem. First put this in terminal\nbrew unlink openssl && brew link openssl --force\nand then, install puma: gem install puma\n. ",
    "beouk": "@StriderKeni worked for me. Thanks.\n. ",
    "kochatice": "Thank you @StriderKeni, your answer is worked for me.\n. ",
    "msanchez01": "Worked for me too, thanks!\n. ",
    "ambshar": "@StriderKeni it worked for me.  Thanks\n. ",
    "d1rtyvans": "@StriderKeni Thank you!!\n. ",
    "rjshenk": "@StriderKeni this worked for me! thanks man\n. ",
    "mohdsameer": "Thank you for the help man, you saved my time, after wasting hours on it figuiring what's wrong. ",
    "hassox": "It bit me with a small bug yesterday when using thread locals which is how I found it.\n. ",
    "pctj101": "Yeah as far as I could find, there is a rack middleware that is supposed to do \"something like that\" called \"use ActiveRecord::ConnectionAdapters::ConnectionManagement\". \nMaybe it wasn't so default in older rails, but in my rails 4 app, it's already in my rails middleware stack by default.  Of course, it doesn't seem to do the job \"completely\".\nI guess that's why it's so hard at first to decide where to fix the problem. For me it's so rare/intermittent that I'm not even sure how to simulate it for test/fix. (which is good... but bad)\n. @sorentwo\nrails (4.1.5)\npuma (2.8.1)\n. ",
    "tenderlove": "The PG issue could be this.  I'm not sure about the mysql version of the issue though.  What version of Rails is the against, and does it happen against master or the 4.2 beta versions?\n. @evanphx ya, it does.  I can't remember why off the top of my head, but I'd like it to not do that.\n. @eileencodes and I are working on pushing early hints support in to the Rack spec for version 2 (version 2 of the SPEC, not version 2 of the gem).  I'd like to try experimentally implementing this in Puma first, then push support to Rails, then upstream to the Rack SPEC.  IOW, I don't want this to be done in a vacuum.\nDoes this seem like an acceptable approach for the Puma team?  I can understand if you don't want to be used for experimentation, but I would like to get this in to production systems sooner rather than later.  \ud83d\ude0a\nAlso, we've verified this to work with Puma + h2o.. I think we're using Ruby 2.3+ specific features in the test in this PR.  If the overall concept seems OK, then we'll fix it up.. @nateberkopec sounds good. \ud83d\ude0a. /cc @rafaelfranca. Hey folks.  I've been talking to @matthewd about this feature.  He convinced me that the lambda should take a header hash just like the header hash from a rack response.  This would alleviate webservers from knowing how the Link header is formatted, and may allow us to send other headers (like X- headers) for debugging purposes.\nIOW, we should change the API to be something like this:\nenv[EARLY_HINTS].call(\u201cLink\u201d: [\u201c<...>; rel=preload; as=..\u201d, \u201c..\u201d])\nWDYT?. > If I'm getting that right, your change makes sense. But should anyone be responsible for omitting the prohibited headers? Puma's job or someone elses?\nI think if you wanted to omit them in Puma it would be fine.  However, I'd expect the proxy to complain or raise an error if it got those headers, so maybe Puma shouldn't care?  It would be nice for the user to get an exception closer to where the error is occurring, but I don't think it's necessary.. According to this article, setting OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES environment variable should work too.  It's really not clear how to fix this correctly (besides not lazily loading things I guess).. Agree.  @eileencodes and I were trying to move it out of here, but the lambda body needs to access the io object (client) and fast_write is a private method.  I'd love to move the lambda out of here, but I am not sure how.. Unsure.  h2o pushes the resource regardless.  I think this is something we need to iterate on. (IOW, I don't have an answer). This is so that app code can run without changes even if the server is being run without early hints support.  The example app that @eileencodes put in the PR description can be run whether early hints is enabled or not (it prevents the app code from writing a if env[EARLY_HINTS] conditional). I saw the error, and I think RuboCop needs to be fixed.  This is new syntax in Ruby 2.3.  But also, since this is new syntax in Ruby 2.3, and you probably want to run the Puma tests on older versions of Ruby anyway, we'll fix it. \ud83d\ude09. ",
    "oozzal": "@evanphx The issue still persists for me. I'm on jruby-9.1.2.0 and puma-3.4.0. What about you @toymachiner62 ?\n. ",
    "davidhq": "Nothing specific in log...\n```\nworkers Integer(ENV['PUMA_WORKERS'] || 2)\nthreads Integer(ENV['MIN_THREADS']  || 1), Integer(ENV['MAX_THREADS'] || 16)\npreload_app!\nrackup      DefaultRackup\nport        ENV['PORT']     || 3003\nenvironment ENV['RACK_ENV'] || 'production'\non_worker_boot do\n  # worker specific setup\n  ActiveSupport.on_load(:active_record) do\n    config = ActiveRecord::Base.configurations[Rails.env] ||\n                Rails.application.config.database_configuration[Rails.env]\n    config['pool'] = ENV['MAX_THREADS'] || 16\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\n```\n. Now it happened on the third project, but this one is configured a bit differenty... anyway the log might still be useful:\n=== puma startup: 2014-11-09 16:37:28 +0000 ===\n=== puma startup: 2014-11-09 16:37:28 +0000 ===\n[26197] * Starting control server on unix:///var/www/sc/shared/tmp/sockets/pumactl.sock\n[26197] - Worker 0 (pid: 26203) booted, phase: 0\n[26197] - Worker 1 (pid: 26205) booted, phase: 0\n[26197] * phased-restart called but not available, restarting normally.\n[26197] - Gracefully shutting down workers...\n[26197] * Restarting...\n/home/david/.rbenv/versions/2.1.2/bin/ruby: No such file or directory -- /var/www/sc/releases/56/vendor/bundle/ruby/2.1.0/bin/puma (LoadError)\nThe last line is not puma's problem, but my configuration but I think the root cause is of some interest: so why phased-restart wasn't available? I have to see if this is the same reason other deploys failed.. but so far it hasn't happened on other projects.\n. Now it happened again and indeed this is the problem: when phased restart is not possible for some reason, then other errors come, not sure why... but maybe if phased restart worked always, the other things wouldn't be important.\n=== puma startup: 2014-11-09 19:55:09 +0000 ===\n=== puma startup: 2014-11-09 19:55:09 +0000 ===\n[25957] * Starting control server on unix:///var/www/sc/shared/tmp/sockets/pumactl.sock\n[25957] - Worker 0 (pid: 25963) booted, phase: 0\n[25957] - Worker 1 (pid: 25972) booted, phase: 0\n[25957] * phased-restart called but not available, restarting normally.\n[25957] - Gracefully shutting down workers...\n[25957] * Restarting...\nException handling servers: undefined method `join' for nil:NilClass (NoMethodError)\n/home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.9.2/lib/puma/thread_pool.rb:179:in `shutdown'\n/home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.9.2/lib/puma/server.rb:746:in `graceful_shutdown'\n/home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.9.2/lib/puma/server.rb:312:in `handle_servers'\n/home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/puma-2.9.2/lib/puma/server.rb:273:in `block in run'\n/home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler/definition.rb:22:in `build': /var/www/sc/releases/63/Gemfile not found (Bundler::GemfileNotFound)\n  from /home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler.rb:154:in `definition'\n  from /home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler.rb:117:in `setup'\n  from /home/david/.rbenv/versions/2.1.2/lib/ruby/gems/2.1.0/gems/bundler-1.7.2/lib/bundler/setup.rb:17:in `<top (required)>'\n  from /home/david/.rbenv/versions/2.1.2/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:54:in `require'\n  from /home/david/.rbenv/versions/2.1.2/lib/ruby/site_ruby/2.1.0/rubygems/core_ext/kernel_require.rb:54:in `require'\nAs I said, not sure why Gemfile is not found, maybe it has to do with deployment steps... Gemfile is available now when I look but maybe in the process something happened...\n. Ugh... thank you! I feel smarter now and sorry for coming with this.... \n. @sorentwo now that I removed preload I have problems with these settings:\non_worker_boot do\n  # worker specific setup\n  ActiveSupport.on_load(:active_record) do\n    config = ActiveRecord::Base.configurations[Rails.env] ||\n                Rails.application.config.database_configuration[Rails.env]\n    config['pool'] = ENV['MAX_THREADS'] || 16\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\nHere:\nconfig/puma.rb:15:in `block in _load_from': uninitialized constant Puma::Configuration::DSL::ActiveSupport (NameError)\nDo I even need this and if I'd like to use it, how do I set this up without preloading the app?\n. you mean the entire block or just on_worker_boot statement? A tiny bit confused... sorry\n. OK. I don't remember where I got this code from but if I remove it, will I always have a big enough db connection pool?\n. Ok, I'll use this: https://devcenter.heroku.com/articles/concurrency-and-database-connections#threaded-servers\nthank you again\n. ",
    "eadonj": "Good call.  I had update xcode thru the app store.  But running xcode-select --install fixed it.\n. ",
    "p8": "Thanks!\n. This fixes an issue we had with stopping and restarting. \ud83d\udc4d . ",
    "andrerocker": ":+1: \n. ",
    "wendelllam": "The scenario is that we setup puma to listen port 9292 with HTTPS protocol, we tried https://localhost:9292 and everything alright. Then we tried http://localhost:9292 for exceptional case and error raise in backend which is also reasonable.\nAfter that we tried to shutdown the puma by cmd+C, and it prompted a message \"Gracefully stopping, waiting for requests to finish\" and waited forever, we finally have to kill -9 the puma process.\nWe fixed it in the forked branch by putting the read action inside a rescue block, and would you  please merge to master if nothing violate the design, thanks.\n. ",
    "jmettraux": "OP is posting in parallel at http://stackoverflow.com/questions/27094321/rufus-scheduler-3-0-3-not-working-with-puma-2-9-2-in-my-development-environment\n. ",
    "robertochingon": "Hi evenphx,\nI try to answer your questions:\n1. Are you using preload? If so, turn it off and I'll bet the problem goes away.\n   I\u00b4m sorry, I think I\u00b4m quite new to Ruby on Rails and Puma and I don\u00b4t know what you mean. How can I check this?\nI attach my Puma config file (I have just enabled logs):\n stdout_redirect 'log/puma.stdout.log', 'log/puma.stderr.log', true\n1. When/how are you calling Rufus::Scheduler? In an initializer?\n   Yes, config/initializers/task_scheduler.rb.\n2. Also, what does your unicorn config look like?\n   My unicorn.rb config file:\n# config/unicorn.rb\nif ENV[\"RAILS_ENV\"] == \"development\"\n   worker_processes 1\n   stderr_path \"/Users/Rober/Projects/yanpy/dev/yanpyapi/log/unicorn.stderr.log\"\n   stdout_path \"/Users/Rober/Projects/yanpy/dev/yanpyapi/log/unicorn.stdout.log\"\n   elsif ENV[\"RAILS_ENV\"] == \"test\"\n   worker_processes 1\n   stderr_path \"/home/ubuntu/env/test/www/yanpyapi-test/log/unicorn.stderr.log\"\n   stdout_path \"/home/ubuntu/env/test/www/yanpyapi-test/log/unicorn.stdout.log\"\n   listen 8081\n   else\n   #worker_processes 3\n   worker_processes 1\n   stderr_path \"/home/ubuntu/env/production/www/yanpyapi/log/unicorn.stderr.log\"\n   stdout_path \"/home/ubuntu/env/production/www/yanpyapi/log/unicorn.stdout.log\"\n   end\ntimeout 30\nPlease let me know if you need any further information.\nThank you so much for your support.\n. Everything is commented in my Puma config file (default file), but the line regarding the logs: stdout_redirect 'log/puma.stdout.log', 'log/puma.stderr.log', true\nI start it in command line with : bundle exec puma -p 8080 -e development  -S ~/puma -C config/puma/development.rb -d\n. MacBook-Pro-de-Roberto:scripts Rober$ ./start_puma.sh \nPuma starting in single mode...\n- Version 2.9.2 (ruby 2.1.2-p95), codename: Team High Five\n- Min threads: 0, max threads: 16\n- Environment: development\n- Listening on tcp://0.0.0.0:8080\n- Daemonizing...\nMacBook-Pro-de-Roberto:scripts Rober$\nThe content of start_puma.sh is what I copied above: bundle exec puma -p 8080 -e development -S ~/puma -C config/puma/development.rb -d\nIn puma.stdout.log and puma.stderr.log I just see:\n=== puma startup: 2014-11-24 19:04:09 +0100 ===\nNothing else.\n. Good news! \nWhat should I do to test it?\n. You are great :+1: \nThank you so much!!!!!!!\n. Looks like the problem was caused by a syntax error. See this for details: http://stackoverflow.com/questions/33750226/puma-2-10-1-not-restarting/33751028#33751028\n. Fixed: There was an error in rails (for some reason, maybe related to versions, I didn\u00b4t get in development). \nMoreover, I could not see the log because it was demonized, but I don\u00b4t know why is not written in the error.log.. ",
    "mastfish": "The new version is speeding towards production - I'll update once I start seeing data on response times.\nMeanwhile, thanks for the amazingly quick response!\n. Looking good: those changes seem to have fixed the issue:\n\n. ",
    "kwilczynski": "Paging @evanphx @Hates.\nThis is very simple change, allowing one to pass environment variable, for example (puma.conf):\n/home/vagrant,vagrant,/dev/null,/home/vagrant/puma1.log\n/home/vagrant,vagrant,/dev/null,/home/vagrant/puma2.log,FOO=123;BAR=456\nI think, @Hates is using it in production at the moment (it was made for him in the first place).\n. Thanks @evanphx :cake: :heart: \n. Hi @hoshinotsuyoshi, thank you fixing this!\nA side note - this whole script is almost crying out loud to be re-written and maintained, somewhat.. ",
    "s01ipsist": "It would probably also be really helpful to use the text 'Puma' in the message, like it does on if @leak_stack_on_error to help the sucker who sees the message.\n\"An unhandled low level error occurred. The puma application logs may have details.\\n\"\n. ",
    "bailsman": "I'm not sure, I think then the connection will stay in the kernel connection queue until a worker becomes nonbusy.\nWhat seems to happen in practice is that even with 1 thread and only 1 worker, calling a controller that does sleep 10, even sending 5 or so requests concurrent is fine, they all get handled eventually. (the 6th and 7th requests may hit client side connection timeouts when it's their turn after 60+s).\nInterestingly, in wireshark I sometimes see 500 internal server errors show up, but they don't show up in the puma console output, the rails logfile or my client at all. I wonder what causes these?\n. Evanphx, thank you for your comments.\nIn order to both queue requests and balance them across workers, I think you would need some kind of inter process communication mechanism to hand over clients to another process. On Linux, this might be possible with fd passing over unix domain socket and shared memory.\nHowever a very common setup is to have puma behind a reverse proxy like nginx, which would be able to handle slow clients and queue the requests before they reach puma. In that setup, there's not much harm in disabling slow client, HTTP keep-alive and request queueing in puma.\nCould this be a configuration option? If the user wants to balance requests across workers, say if they have a single threaded application, then they know that they need a reverse proxy in front of puma and they can enable the configuration option.\nI made an attempt to implement a configuration option here: https://github.com/puma/puma/pull/640 . It's too much to put in line, so I made a pull request. That doesn't mean I think it is ready to be merged. For one, I'm not very confident that I did the right thing, for another, I have no idea what the impact of these changes is on performance, and finally, I didn't implement anything for the case of JRuby and SSL socket because I didn't fully understand what was going on there.\nI'm running two non-threadsafe apps with this configuration option now, and while it seems to work, I would like your advice for a robust solution.\n. Could it be a message from bundler?\n. Out of curiosity, why are you running with queue_requests = false?\n. Are you using Rails.logger.silence? This became thread-safe in rails only very recently (4.2.6), and if you're using an older version, a race condition can permanently set the log level to ERROR in your application, even if you've configured it higher. Are errors still logged?\n. I think not_full.signal should be in the critical section that also changes spawned. It makes sense to signal there, because that also changes the condition in wait_until_not_full (it increases the right hand side by 1, so the check should run again).\nOtherwise, we signal, then leave critical section and only later, in 2nd critical section decrease spawned. Something could happen in between those sections.\ndiff --git a/lib/puma/thread_pool.rb b/lib/puma/thread_pool.rb\nindex 11ddcf7..14bc0cc 100644\n--- a/lib/puma/thread_pool.rb\n+++ b/lib/puma/thread_pool.rb\n@@ -111,6 +111,7 @@ module Puma\n         mutex.synchronize do\n           @spawned -= 1\n           @workers.delete th\n+          @not_full.signal\n         end\n       end\nCan you check if this change also fixes the bug?\n. The race is probably more rare now, but can't it still occur?\n1) work is added, 1 waiting\n2) wait_until_not_full is executed\n   @todo.size = 1\n   @waiting = 1\n   @max = 1\n   @spawned = 1\n   1 - 1 < 1 - 1 is false so we block\n3) trim is requested. \n4) worker thread wakes up, decrements @waiting, executes the work.\n5) trim is requested, so we set continue = false, signal not_full, then leave the mutex.synchronize\n6) main server thread wakes up in wait_until_not_full:\n   @todo.size = 0\n   @waiting = 0\n   @spawned = 1\n   @max = 1\n   0 - 0 < 1 - 1 is false so we block again\n7) worker thread wakes up, decrements spawn and disappears. not_full is not signaled again and we have the deadlock again.\n. Hi Thomas. Is it a private repository? Are you able to produce a minimal app that still leaks and share it publicly in this issue?\n. ",
    "ponchik": "I use gem 'puma' without git link\nThanks for your answer. I was able to run puma with\nbundle exec puma\n. Dear Evan\nThis is rather rbx issue and they are aware of it\nhttps://github.com/rubinius/rubinius/issues/2756\nPlease close this one\n. ",
    "wherewelive": "Thanks for the response Evan! Not really sure why that is happening either. I am not explicitly using minitest anywhere and the only gem that depends on it is ActiveSupport. Happy to ignore the warnings for now, but hate not knowing what is causing it :/\n. I was able to get the warnings to go away by changing my config.  I was using the heroku suggestions to use a config file where the database connections are set in on_worker_boot\nHeroku config/puma.rb\non_worker_boot do\n  # worker specific setup\n  ActiveSupport.on_load(:active_record) do\n    config = ActiveRecord::Base.configurations[Rails.env] ||\n                Rails.application.config.database_configuration[Rails.env]\n    config['pool'] = ENV['MAX_THREADS'] || 16\n    ActiveRecord::Base.establish_connection(config)\n  end\nend\nNow I am just using the Procfile for all settings and a database initializer to handle the connections...\nProcfile\nweb: bundle exec puma -t ${MIN_THREADS:-1}:${MAX_THREADS:-1} -w ${WORKERS:-2} -p $PORT -e ${RACK_ENV:-development}\nworker: bundle exec sidekiq\nclock: bundle exec clockwork config/clock.rb\nconfig/initializers/database_connections.rb\n```\nENV['REDISTOGO_URL'] ||= 'redis://127.0.0.1:6379'\nSidekiq.configure_server do |config|\n  config.redis = { url: ENV['REDISTOGO_URL'], namespace: 'wherewelive' }\n  config.redis { |conn| conn.flushdb }\n# Set the db pool size for heroku\n  ENV['DATABASE_URL'] += \"?pool=#{ENV['DB_POOL'] || Sidekiq.options[:concurrency]}\" if ENV['DATABASE_URL']\nRails.application.config.after_initialize do\n    ActiveRecord::Base.connection_pool.disconnect!\nActiveSupport.on_load(:active_record) do\n  config = Rails.application.config.database_configuration[Rails.env]\n  config['reaping_frequency'] = ENV['DB_REAP_FREQ'] || 10 # seconds\n  config['pool'] = ENV['DB_POOL'] || Sidekiq.options[:concurrency]\n  ActiveRecord::Base.establish_connection(config)\n\n  Rails.logger.info(\"Connection Pool size for Sidekiq Server is now: #{ActiveRecord::Base.connection.pool.instance_variable_get('@size')}\")\nend\n\nend\nend\nSidekiq.configure_client do |config|\n  config.redis = { url: ENV['REDISTOGO_URL'], namespace: 'wherewelive', size: 1 }\nRails.application.config.after_initialize do\n    ActiveRecord::Base.connection_pool.disconnect!\nActiveSupport.on_load(:active_record) do\n  config = Rails.application.config.database_configuration[Rails.env]\n  config['reaping_frequency'] = ENV['DB_REAP_FREQ'] || 10 # seconds\n  config['pool'] = ENV['DB_POOL'] || 10\n  ActiveRecord::Base.establish_connection(config)\n\n  # DB connection not available during slug compliation on Heroku\n  Rails.logger.info(\"Connection Pool size for web server is now: #{config['pool']}\")\nend\n\nend\nend\n```\nHope this helps!\n. ",
    "taf2": "I have the following in my app:\n[93484] ! WARNING: Detected 2 Thread(s) started in app boot:\n[93484] ! #<Thread:0x007fac3389b3e0 sleep> - /Users/taf2/.rvm/rubies/ruby-2.1.5/lib/ruby/2.1.0/net/http.rb:920:in `connect'\n[93484] ! #<Thread:0x007fac35469bd0 sleep> - /Users/taf2/.rvm/rubies/ruby-2.1.5/lib/ruby/2.1.0/timeout.rb:84:in `sleep'\nnot minitest - might be nice if there was a full stack trace noisy or not... makes finding the root cause easier...\n. i added this in config/initializer/z.rb\nThread.new do\n  puts \"hello\"\nend\nand i noticed my thread ran before listen... so before the detected 2 threads warning...\n. ",
    "monfresh": "I'm getting the same thing with New Relic and AppSignal:\n2015-02-02T02:47:49.519893+00:00 app[web.2]: [3] ! WARNING: Detected 2 Thread(s) started in app boot:\n2015-02-02T02:47:49.519941+00:00 app[web.2]: [3] ! #<Thread:0x007f5b515a44d0 sleep> - /app/vendor/bundle/ruby/2.1.0/gems/newrelic_rpm-3.9.6.257/lib/new_relic/agent/event_loop.rb:118:in `select'\n2015-02-02T02:47:49.522561+00:00 app[web.2]: [3] Use Ctrl-C to stop\n2015-02-02T02:47:49.519964+00:00 app[web.2]: [3] ! #<Thread:0x007f5b55bebf10 sleep> - /app/vendor/bundle/ruby/2.1.0/gems/appsignal-0.10.6/lib/appsignal/agent.rb:33:in `sleep'\nCould this be the cause of R12 errors when Heroku restarts the dyno?\n. Not sure, but all I know is that only started happening after I switched from Unicorn to Puma. Here is the commit: https://github.com/codeforamerica/ohana-web-search/commit/b89425e0a8e5be42756748958011f72021480c45\n. Thanks for the super quick fix! It works.\n. ",
    "lacco": "@monfresh NewRelic should just work in thread mode since version 3.8.0 of their gem, it should automatically restart thread in children. See also https://github.com/newrelic/rpm/blob/master/CHANGELOG (search for \"restart_thread_in_children\").\n. Quick note: Bug can be fixed by reverting back to rspec 3.6.0, see https://github.com/rspec/rspec-rails/issues/1887. ",
    "afeld": "Hmm, running into the same issue with newrelic_rpm v3.12.1.298 and puma v2.11.3...\n$ bundle exec puma\n[27978] Puma starting in cluster mode...\n[27978] * Version 2.11.3 (ruby 2.2.2-p95), codename: Intrepid Squirrel\n[27978] * Min threads: 5, max threads: 5\n[27978] * Environment: development\n[27978] * Process workers: 2\n[27978] * Preloading application\n[27978] * Listening on tcp://0.0.0.0:3000\n[27978] ! WARNING: Detected 2 Thread(s) started in app boot:\n[27978] ! #<Thread:0x007fc9dd1039f8@/Users/aidanfeldman/.rvm/gems/ruby-2.2.2/gems/newrelic_rpm-3.12.1.298/lib/new_relic/agent/threading/agent_thread.rb:12 sleep> - /Users/aidanfeldman/.rvm/rubies/ruby-2.2.2/lib/ruby/2.2.0/net/protocol.rb:155:in `select'\n[27978] ! #<Thread:0x007fc9dd324840@/Users/aidanfeldman/.rvm/rubies/ruby-2.2.2/lib/ruby/2.2.0/timeout.rb:80 sleep> - /Users/aidanfeldman/.rvm/rubies/ruby-2.2.2/lib/ruby/2.2.0/timeout.rb:82:in `sleep'\n[27978] Use Ctrl-C to stop\n[27978] - Worker 0 (pid: 27999) booted, phase: 0\n[27978] - Worker 1 (pid: 28000) booted, phase: 0\n@Tomohiro's workaround worked for me, but not sure why the issue is still coming up.\n. ",
    "ghprince": "Hmmm I have the same issue too... no warning on local dev setup but always shows up on Heroku\napp web.1 - - [3] Puma starting in cluster mode...\napp web.1 - - [3] * Version 2.12.2 (ruby 2.2.2-p95), codename: Plutonian Photo Shoot\napp web.1 - - [3] * Min threads: 5, max threads: 5\napp web.1 - - [3] * Environment: production\napp web.1 - - [3] * Process workers: 2\napp web.1 - - [3] * Preloading application\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : To prevent agent startup add a NEWRELIC_AGENT_ENABLED=false environment variable or modify the \"production\" section of your newrelic.yml.\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Starting the New Relic agent in \"production\" environment.\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Reading configuration from config/newrelic.yml (/app)\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Environment: production\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing Rails 3+ middleware instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing Rack::Builder middleware instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing Rails 4 Controller instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Dispatcher: Puma\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Application: \napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing Net instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing deferred Rack instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing Rails 4 view instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Installing Rails 4 Error instrumentation\napp web.1 - - ** [NewRelic][08/05/15 04:17:36 +0000 web.1 (3)] INFO : Finished instrumentation\napp web.1 - - [3] ! WARNING: Detected 2 Thread(s) started in app boot:\napp web.1 - - [3] - Worker 1 (pid: 22) booted, phase: 0\napp web.1 - - [3] * Listening on tcp://0.0.0.0:24407\napp web.1 - - [3] ! #<Thread:0x007f4c477ee240@/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.12.1.298/lib/new_relic/agent/threading/agent_thread.rb:12 sleep> - /app/vendor/ruby-2.2.2/lib/ruby/2.2.0/net/http.rb:879:in `initialize'\napp web.1 - - [3] ! #<Thread:0x007f4c47566298@/app/vendor/ruby-2.2.2/lib/ruby/2.2.0/timeout.rb:80 sleep> - /app/vendor/ruby-2.2.2/lib/ruby/2.2.0/timeout.rb:82:in `sleep'\napp web.1 - - [3] Use Ctrl-C to stop\napp web.1 - - [3] - Worker 0 (pid: 8) booted, phase: 0\n. @evanphx thx a lot for the explanation! just curious what would be impact of this? to me the app seems to be running ok and new relic reporting are also normal, is there anything might go wrong?\n. ",
    "thom-nic": "@bbozo wow interesting that you just commented on this.  I recently ran into issues with chunked Transfter-Encoding across a number of different Ruby HTTP servers (Puma being one of them.)  \nI want to reference this rails issue for context then make a suggestion...\nIf you follow the (somewhat confusing) precedent that Webrick/ Unicorn/ Rainbows set (see linked issue above) you'll be passing along a request that ActionDispatch::Request fails to handle correctly if the Content-Length header is still missing.\nSo I'd suggest taking one of the two approaches:\n1.  Parse & reassemble the chunked data, reassign env['rack.input'] to the decoded body, and add a Content-Length header.  I'd also remove the Transfer-Encoding header, at which point rack/rails can be oblivious to the fact that the request was ever chunked.\n2.  Simply read the full request body but don't decode it, and pass it along to rack.  This is more of a \"hands-off\" approach which IMO is still valid but requires the user to add rack middleware to actually decode the request.  Another example which I think validates this approach is this middleware for handling Content-Encoding.\nThat said, I want to say thanks for tackling this!  I was rather surprised when I figured out thin doesn't support this then I tried Puma which didn't work either :) \n. @bbozo if you want to try your hand at this, it looks like this is where you need to start.  Unfortunately I'm not familiar with Ragel so it's all a bit of a mystery to me.\n. Well that's certainly helpful.  I didn't realize the projects share a common code ancestor.  Unfortunately ragel still seems way too far over my head for me to contribute a patch in the near future.\n. ",
    "javanthropus": "Since Unicorn already does this, it may also be useful to check out how this is done in its fork of the parser: http://bogomips.org/unicorn.git/tree/ext/unicorn_http.\n. ",
    "fedesoria": "Is there any workaround for this?\n. ",
    "marianandre42": "+1\n. ",
    "sabamotto": "Thanks momer. I didn't know Shellwords#shellescape.\nBut it occurred an URI::InvalidURIError like first code result.\nURI#parse seems it cannot parse a shell-escaped uri.\n[1] pry(main)> URI.parse Shellwords.shellescape('unix://space char/test.sock')\nURI::InvalidURIError: bad URI(is not URI?): unix://space\\ char/test.sock\nfrom /Users/saba/.rbenv/versions/2.1.3/lib/ruby/2.1.0/uri/common.rb:176:in `split'\n. ",
    "rdpoor": "Hmm -- maybe my amateur diagnosis is all wrong: -Wimplicit-function-generation should have prevented mkmf from raising an error on line 13 of the listing above, so I suspect another problem.  Would it help if I posted the entire mkmf.log somewhere, and if so, where?\n. Closing this in favor of the more helpful #627.\n. Perhaps @lowjoel could chime in on this.  extconf.rb changed between v2.9.1 and v2.9.2.  I'm not enough of an extconf expert to know for certain, but given the error message I'm seeing, this seems like a likely suspect : https://github.com/puma/puma/commits/v2.9.2/ext/puma_http11/extconf.rb\n. That would be a 'no':\n```\n$ gem install puma --version 2.10.2\nBuilding native extensions.  This could take a while...\nERROR:  Error installing puma:\n    ERROR: Failed to build gem native extension.\n/Users/r/Developer/Chalcedony/usr/bin/ruby extconf.rb\n\nchecking for SSL_CTX_new() in -lssl... no\nchecking for SSL_CTX_new() in -lssleay32... no\n extconf.rb failed \nCould not create Makefile due to some reason, probably lack of necessary\nlibraries and/or headers.  Check the mkmf.log file for more details.  You may\nneed configuration options.\nProvided configuration options:\n    --with-opt-dir\n    --with-opt-include\n    --without-opt-include=${opt-dir}/include\n    --with-opt-lib\n    --without-opt-lib=${opt-dir}/lib\n    --with-make-prog\n    --without-make-prog\n    --srcdir=.\n    --curdir\n    --ruby=/Users/r/Developer/Chalcedony/usr/bin/ruby\n    --with-puma_http11-dir\n    --without-puma_http11-dir\n    --with-puma_http11-include\n    --without-puma_http11-include=${puma_http11-dir}/include\n    --with-puma_http11-lib\n    --without-puma_http11-lib=${puma_http11-dir}/lib\n    --with-ssllib\n    --without-ssllib\n    --with-ssleay32lib\n    --without-ssleay32lib\nextconf failed, exit code 1\nGem files will remain installed in /Users/home/sandbox/usr/lib/ruby/gems/2.1.0/gems/puma-2.10.2 for inspection.\nResults logged to /Users/home/sandbox/usr/lib/ruby/gems/2.1.0/extensions/x86_64-darwin-14/2.1.0/puma-2.10.2/gem_make.out\nChalcedony[~/Projects/heroku-sample/server-side-events]$ \n```\n. @lowjoel : I'm assuming you mean mkmf.log (since gen_make.out is posted above).  I've posted mkmf.log for v2.10.2 to https://gist.github.com/rdpoor/21ad7a587763e7200576\nIf I misunderstood your request, or if you want mkmf.log for v2.9.2 instead, let me know.  Thanks.\n- Rob\n. A tyro's question: I've forked the latest and made the changes @lowjoel suggested.  How do I create a gem and test it?\n. Worked.  I should point out that the extconf file I pulled was using ssleay32 and libeay32 (not ssleay and libeay as your diff showed).  Here's the diff:\n``` diff\ndiff --git a/ext/puma_http11/extconf.rb b/ext/puma_http11/extconf.rb\nindex e523aba..0a5296c 100644\n--- a/ext/puma_http11/extconf.rb\n+++ b/ext/puma_http11/extconf.rb\n@@ -2,8 +2,8 @@ require 'mkmf'\ndir_config(\"puma_http11\")\n-if %w'ssl ssleay32'.find {|ssl| have_library(ssl, 'SSL_CTX_new')} and\n-  %w'crypto libeay32'.find {|crypto| have_library(crypto, 'BIO_read')}\n-\n+if %w'crypto libeay'.find {|crypto| have_library(crypto, 'BIO_read')} and\n+    %w'ssl ssleay'.find {|ssl| have_library(ssl, 'SSL_CTX_new')}\n+\n   create_makefile(\"puma/puma_http11\")\n end\n```\nI could create a pull request, but I'm not sure how to resolve the ???eay32 vs the ???eay thing.  Let me know if/how I can help.\n- r\n. Okay - that also works.  I'll create a pull request.\n. @juanpaulo : \n\nAn alternative solution for this issue is xcode-select --install to \ninstall necessary libraries and/or headers.\n\nI don't believe that's a solution for this issue -- I'd already done xcode-select --install on my system and was still getting build errors.\n. ",
    "lowjoel": "There's gen_make.log. Could you post that?\nI won't be booting into Mac OS until at least next week...\n. @rdpoor yup, sorry, I replied while away from computer.\nMy analysis is that Mac OS has libssl.so and libcrypto.so needing to be linked together, with libssl depending on libcrypto; so maybe this might work:\n``` diff\ndir_config(\"puma_http11\")\n$defs.push \"-Wno-deprecated-declarations\"\n-if %w'ssl ssleay32'.find {|ssl| have_library(ssl, 'SSL_CTX_new')} and\n- %w'crypto libeay32'.find {|crypto| have_library(ssl, 'BIO_read')}\n+if %w'crypto libeay32'.find {|crypto| have_library(crypto, 'BIO_read')} and\n+ %w'ssl ssleay32'.find {|ssl| have_library(ssl, 'SSL_CTX_new')}\ncreate_makefile(\"puma/puma_http11\")\n end\n```\nAlso I've fixed an embarrassing copy-paste fail in there. I don't think that the conditional went to the second line.\nI've not tested this on any platforms at all since I'm not going to be at my computer for a few days.\n. @rdpoor cd into the working directory, then gem build puma.gemspec. You can then gem install puma.gem\n. it should be libeay32 and ssleay32, sorry. I've updated the diff above.\n. As stated, while this fixes OS X, I've not tested this on Linux nor Windows.\n. ",
    "androbtech": "Interesting.\nWe use puma as the default web server for Tokaido. We are about to release another version and I started getting this same error.\nI compiled v2.10.2 successfully last week (Ruby 2.1.5p-273 static build on OS X v10.8.5). Decided to recompile it again today for preparing our release but I can't. \nI'm investigating.\nI still have the *.bundle's from last week's compilation.\n. I tried before hacking mkmf.rb for plain curiosity and simply return true for have_library.\nInterestingly, I did see look ups for crypto. Will try this.\n. @lowjoel worked. Thanks. \nThis is odd, it worked last week. I tried with a different Ruby build so the problem could be in the way I built the latest Ruby.\n. ",
    "juanpaulo": "An alternative solution for this issue is xcode-select --install to install necessary\nlibraries and/or headers. Hope it helps.\n. @rdpoor: Oh. One of my coworkers had the exact same issue and it was fixed with xcode-select --install.\nThanks for the feedback.\n. @ct-clearhaus that's what we did for our CentOS distro too. yum install openssl-devel\n. @grantgeorge glad the xcode-select --install fix worked for you.\n. yum install openssl-devel\nDid the trick for us. Looks more of an issue with CentOS not having the required OpenSSL files (ec.h, etc.). Will try to look for other options around this, if any.\n. ",
    "ct-clearhaus": "I experienced 2.9.1-2.10.2 fail installation on a very \"thin\" Debian (official docker Debian with only minimal stuff installed). Installing libssl-dev makes it roll through the installation.\n. ",
    "dt1973": "I'm getting the same on OpenBSD 5.7 (ftp://ftp.openbsd.org/pub/OpenBSD/snapshots/amd64/install57.iso with pkg_add ruby-2.2.0).\n```\nGem::Ext::BuildError: ERROR: Failed to build gem native extension.\n/usr/local/bin/ruby22 -r ./siteconf20150121-3373-h3ghnc.rb extconf.rb\n\nchecking for SSL_CTX_new() in -lssl... no\nchecking for SSL_CTX_new() in -lssleay32... no\n extconf.rb failed \nCould not create Makefile due to some reason, probably lack of necessary libraries and/or headers.  Check the mkmf.log file for more details.  You may need configuration options.\nProvided configuration options:\n        --with-opt-dir\n        --without-opt-dir\n        --with-opt-include\n        --without-opt-include=${opt-dir}/include\n        --with-opt-lib\n        --without-opt-lib=${opt-dir}/lib\n        --with-make-prog\n        --without-make-prog\n        --srcdir=.\n        --curdir\n        --ruby=/usr/local/bin/$(RUBY_BASE_NAME)22\n        --with-puma_http11-dir\n        --without-puma_http11-dir\n        --with-puma_http11-include\n        --without-puma_http11-include=${puma_http11-dir}/include\n        --with-puma_http11-lib\n        --without-puma_http11-lib=${puma_http11-dir}/lib\n        --with-ssllib\n        --without-ssllib\n        --with-ssleay32lib\n        --without-ssleay32lib\nextconf failed, exit code 1\nGem files will remain installed in /tmp/bundler20150121-3373-ghhjze/puma-2.1.2/gems/puma-2.10.2 for inspection.\nResults logged to /tmp/bundler20150121-3373-ghhjze/puma-2.10.2/extensions/x8_64-openbsd/2.2/puma-2.10.2/gem_make.out\n```\n. Marvellous, it works!\n. @pvijayror it's impossible for us to know without showing us your config files.\n. Sounds plausible. Please give me some time to investigate.\nThank you!\n. You are most right. Seems to be something with the db - getting the same with Unicorn.\nThanks again!\n. Sorry -- only tested for delays after reboot, which are to be expected:\nhttp://stackoverflow.com/questions/12903575/why-postgresql-queries-are-slower-in-the-first-request-after-server-start-than-d\nI've yet to investigate the cause of delay after periods of inactivity.\n. You're right. I was querying some external APIs and forgot to send them into the background.\nHappy Easter!\n. Not sure that's it either -- still getting extreme lags after prolonged periods of inactivity.\nCame across http://stackoverflow.com/questions/680302/how-can-i-find-out-why-my-app-is-slow -- is Puma doing anything similar?\n\nIf it is just slow on the first time you load it is probably because of passenger killing the process due to inactivity. I don't remember all the details but I do recall reading people who used cron jobs to keep at least one process alive to avoid this lag that can occur with passenger needed to reload the environment.\n. Maybe I need something like PgBouncer (https://wiki.postgresql.org/wiki/PgBouncer)?\n\nCame across http://hans.io/blog/2014/02/19/postgresql_connection/index.html -- shouldn't this be a must for everyone who uses Rails with PostgreSQL?\n. ",
    "jimmycuadra": "Even with Puma 2.11.0, I continued to get this build failure on Ubuntu 12.04.5 until I installed libssl-dev.\n. ",
    "rafaltrojanowski": "rvm pkg install openssl did the trick\n. ",
    "grantgeorge": "Stumbled upon this recently. I already had openssl installed through brew. Fixed with:\nxcode-select --install\nand accepting the license.\nsudo xcodebuild -license\n. ",
    "ryanmjacobs": "Solved it. :smile:\nOn Arch Linux make sure that you have libffi installed.\n$ pacman -S libffi\n$ gem install puma\nBuilding native extensions.  This could take a while...\nSuccessfully installed puma-2.10.2\n1 gem installed\n. ",
    "joevandyk": "Thanks!\n. ",
    "riffraff": "Thanks for your reply, in the meantime I have solved horribly by using reflection to get ahold of the socket, but I could try to write a PR.\nI realize now that the variables in ENV refer to the accepted socket rather than the listening one (i.e.  TCPSocketrather than the TCPServer that I'd actually needed). \nI am not sure if I should expose something like ENV['puma_binder'] to match @binder in Puma::Server the way it seems to be done for ENV['puma_socket'], or if there is a way to get a handle of the current server which I'm currently missing, could you clarify what you'd think would be the puma way of doing it?\nAlso, should this be made available only if a specific config option is enabled or made available in general?\nThanks. \n. ok, but that brings me back to the issue that I couldn't find a globally\naccessible reference to the current running Puma::Server, or the\nPuma::Runner that contains it, nor a way to find \"who is running me\" in a\ngeneric rack app.\nThe only way in which I can get access to the current server (that I could\nfind) is in Rack::Handler::Puma which appears to be a singleton object with\nclass-level methods, but I'm not sure Rack::Handler::Puma.clean_for_fork\nmakes total sense.\nI can get a reference to the Server via ObjectSpace or register the runner\nwith a class-level list on initialize but I'm not sure it's a good idea.\nSorry If I'm making stupid comments, but I'm just getting familiar with the\ncode base, and thanks for your patience.\nOn Wed, Jan 21, 2015 at 6:57 PM, Evan Phoenix notifications@github.com\nwrote:\n\nI think it needs to be abstracted. So something like Puma.clean_for_fork\nor something that closes the sockets, would be best.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/638#issuecomment-70887256.\n\n\ntwitter: @riffraff\nblog (en, it): www.riffraff.info\nwork: circleme.com\n. ",
    "tmikoss": "+1\n. ",
    "theonegri": "@evanphx I have the following 2 lines in my deploy.rb:\nset :puma_preload_app, false\nset :puma_prune_bundler, true\nbut I'm still getting the error: Gemfile not found (Bundler::GemfileNotFound)\n. deploy.rb\n```\nlock '3.3.5'\nset :application, 'myapp'\nset :repo_url, 'git@github.com:theonegri/myapp.git'\nset :user, \"myuser\"\nset :default_stage, \"staging\"\nset :puma_threads,    [4, 16]\nset :puma_workers,    0\nset :puma_bind,       \"unix://#{shared_path}/tmp/sockets/staging.myapp-puma.sock\"\nset :puma_state,      \"#{shared_path}/tmp/pids/puma.state\"\nset :puma_pid,        \"#{shared_path}/tmp/pids/puma.pid\"\nset :puma_access_log, \"#{release_path}/log/puma.error.log\"\nset :puma_error_log,  \"#{release_path}/log/puma.access.log\"\nset :puma_preload_app, false\nset :puma_prune_bundler, true\nset :puma_worker_timeout, nil\nset :puma_init_active_record, true  # Change to true if using ActiveRecord\nset :bugsnag_api_key, \"a key\"\nset :linked_dirs,  %w{bin log tmp/pids tmp/cache tmp/sockets vendor/bundle public/system}\nnamespace :puma do\n  desc 'Create Directories for Puma Pids and Socket'\n  task :make_dirs do\n    on roles(:app) do\n      execute \"mkdir #{shared_path}/tmp/sockets -p\"\n      execute \"mkdir #{shared_path}/tmp/pids -p\"\n    end\n  end\nbefore :start, :make_dirs\nend\nnamespace :myuser do\ndesc 'Initial myuser'\n  task :initial do\n    on roles(:app) do\n      before 'deploy:restart', 'puma:start'\n      invoke 'deploy'\n    end\n  end\ndesc 'Restart application'\n  task :restart do\n    on roles(:app), in: :sequence, wait: 5 do\n      invoke 'puma:restart'\n    end\n  end\nafter  :finishing,    :compile_assets\n  after  :finishing,    :cleanup\n  after  :finishing,    :restart\nend\n```\nstaging.rb\n```\nrole :app, %w{myuser@myapp.io}\nrole :web, %w{myuser@myapp.io}\nrole :db,  %w{myuser@myapp.io}\nserver 'myapp.io', user: 'deploy', roles: %w{web app db}, my_property: :my_value\nset :branch, \"staging\"\nset :env, \"staging\"\nset :deploy_to, \"/var/www/staging\"\nset :stage, :staging\nset :puma_bind,       \"unix://#{shared_path}/tmp/sockets/staging.myapp-puma.sock\"\nset :puma_state,      \"#{shared_path}/tmp/pids/puma.state\"\nset :puma_pid,        \"#{shared_path}/tmp/pids/puma.pid\"\nset :puma_access_log, \"#{release_path}/log/puma.error.log\"\nset :puma_error_log,  \"#{release_path}/log/puma.access.log\"\nset :puma_preload_app, false\nset :puma_prune_bundler, true\nset :puma_worker_timeout, nil\nset :puma_init_active_record, true  # Change to true if using ActiveRecord\n```\nCapfile\n```\nLoad DSL and Setup Up Stages\nrequire 'capistrano/setup'\nIncludes default deployment tasks\nrequire 'capistrano/deploy'\nIncludes tasks from other gems included in your Gemfile\n\nFor documentation on these, see for example:\n\nhttps://github.com/capistrano/rvm\nhttps://github.com/capistrano/rbenv\nhttps://github.com/capistrano/chruby\nhttps://github.com/capistrano/bundler\nhttps://github.com/capistrano/rails\n\nrequire 'capistrano/rvm'\nrequire 'capistrano/bundler'\nrequire 'capistrano/rails/assets'\nrequire 'capistrano/rails/migrations'\nrequire 'capistrano/puma'\nrequire 'capistrano/sidekiq'\nrequire 'bugsnag/capistrano'\nrequire 'capistrano/sidekiq/monit' #to require monit tasks # Only for capistrano3\nLoads custom tasks from `lib/capistrano/tasks' if you have any defined.\nDir.glob('lib/capistrano/tasks/*.rake').each { |r| import r }\n```\n. @evanphx Added it after reading the instruction here: https://github.com/puma/puma#capistrano-deployment\n. ping @evanphx \n. ",
    "sime": "I'm in the same boat:\ndeploy.rb\nset :puma_preload_app, false\nset :puma_prune_bundler, true\nVery similar Capfile compared to @theonegri \nCapistrano output:\nshell\nDEBUG [34ce57fd] Command: cd /home/staging/site/current && ( PATH=~/.rbenv/shims:~/.rbenv/bin:$PATH RBENV_ROOT=~/.rbenv RBENV_VERSION=2.2.1 RACK_ENV=staging ~/.rbenv/bin/rbenv exec bundle exec pumactl -S /home/staging/site/shared/tmp/pids/puma.state restart )\nDEBUG [34ce57fd]        Command restart sent success\nINFO [34ce57fd] Finished in 1.099 seconds with exit status 0 (successful).\nPuma log:\n/home/staging/.rbenv/versions/2.2.1/lib/ruby/gems/2.2.0/gems/bundler-1.9.1/lib/bundler/definition.rb:22:in `build': /home/staging/site/releases/20150330195226/Gemfile not found (Bundler::GemfileNotFound)\n. So it seems my puma.rb file still had preload_app! in it. So I generated a new one. Though, unfortunately, the problem still persists:\n``` ruby\n!/usr/bin/env puma\ndirectory '/home/staging/site/current'\nrackup \"/home/staging/site/current/config.ru\"\nenvironment 'staging'\npidfile \"/home/staging/site/shared/tmp/pids/puma.pid\"\nstate_path \"/home/staging/site/shared/tmp/pids/puma.state\"\nstdout_redirect '/home/staging/site/shared/log/puma_access.log', '/home/staging/site/shared/log/puma_error.log', true\nthreads 0,16\nbind 'unix:///home/staging/site/shared/tmp/sockets/puma.sock'\nworkers 0\nprune_bundler\n```\n. I believe I have solved this in capistrano-puma: https://github.com/seuros/capistrano-puma/issues/109\nIn short you can refresh the Gemfile with on_restart\n. ",
    "ain": "I haven't reached a stable setting with jungle + RVM so that it'd run multiple sites on different ports in parallel. rvmsudo gets it running, but one process goes down shortly after launching. Hard to debug as well, I have no info on the errors. \nIs there a debug flag or something that could be used to scope?\n. :+1: on @kayakyakr suggestion for automating this part.\n. ",
    "mensfeld": "@evanphx  @ain  here you go: http://dev.mensfeld.pl/2014/02/puma-jungle-script-fully-working-with-rvm-and-pumactl/\n. ",
    "kayakyakr": "Can we add rvm & bundle-ready versions of the init.d and run-puma scripts to the repository and modify the capistrano scripts to be able to install these? I think that would be helpful.\n. ",
    "bpaquet": "I was searching a way to limit the number of request queued in puma.\nBacklog can be a solution, but I realized after the question that puma\nqueued requests internally. So changing backlog do nothing for limiting the\nnumber of concurrent request.\nThe 2.11 add a new option queue_request. If I disable request queuing,\nbacklog can be set to limit concurrency. But a load balancer like Haproxy\ncan do this before Puma. So, no real usage of backlog on unix socket :(\nBut I think disabling queue_request will increase latency. Correct ?\nAnyway, thx you for you answer\nOn Thu, Feb 5, 2015 at 10:17 PM, Evan Phoenix notifications@github.com\nwrote:\n\nThere currently is one because I didn't think it made a whole lot of\nsense, given it's always on the local machine and thusly behind another\nload balancer. Can you tell me why you'd want to set it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/645#issuecomment-73130559.\n. Because request queueing in the process, not in socket can have lot of side effects :(\n. Probably related to https://github.com/heroku/rack-timeout/issues/76\n. 2.11.1\n\nOn Tue, Apr 21, 2015 at 5:26 PM, Evan Phoenix notifications@github.com\nwrote:\n\n@bpaquet https://github.com/bpaquet Which version of puma are you using?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/691#issuecomment-94838497.\n. \n",
    "dmkl": "I have the same issue.\n. I get this error only when I render routes from models (via Rails.application.routes.url_helpers helper). This request returns an ICS-formatted text file. I tried everything I could imagine, but could not reproduce this error manually. I keeps showing up in production. Here is my full backtrace:\n``\nNoMethodError: undefined methodurl_options' for #\n1 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 271 in call\n2 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 222 in call\n3 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 334 in block (2 levels) in define_url_helper\n4 File \"/app/app/models/order.rb\" line 428 in to_ics\n5 File \"/app/app/controllers/cleaners_controller.rb\" line 158 in block (3 levels) in cal\n... 2 non-project frames\n6 File \"/app/vendor/bundle/ruby/2.2.0/gems/activerecord-4.2.0/lib/active_record/relation/delegation.rb\" line 46 in each\n7 File \"/app/vendor/bundle/ruby/2.2.0/gems/activerecord-4.2.0/lib/active_record/relation/delegation.rb\" line 46 in each\n8 File \"/app/app/controllers/cleaners_controller.rb\" line 158 in block (2 levels) in cal\n... 2 non-project frames\n9 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/mime_responds.rb\" line 216 in call\n10 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/mime_responds.rb\" line 216 in respond_to\n11 File \"/app/app/controllers/cleaners_controller.rb\" line 145 in cal\n12 File \"/app/app/controllers/cleaners_controller.rb\" line 141 in ical\n... 153 non-project frames\n13 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/implicit_render.rb\" line 4 in send_action\n14 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/abstract_controller/base.rb\" line 198 in process_action\n15 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/rendering.rb\" line 10 in process_action\n16 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/abstract_controller/callbacks.rb\" line 20 in block in process_action\n17 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 117 in call\n18 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 117 in call\n19 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in block in halting_and_conditional\n20 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in call\n21 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in block in halting_and_conditional\n22 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in call\n23 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in block in halting_and_conditional\n24 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in call\n25 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in block in halting_and_conditional\n26 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 234 in call\n27 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 234 in block in halting\n28 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n29 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n30 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n31 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n32 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in call\n33 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 151 in block in halting_and_conditional\n34 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n35 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n36 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n37 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n38 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 234 in call\n39 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 234 in block in halting\n40 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n41 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n42 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n43 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n44 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n45 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n46 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in call\n47 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 169 in block in halting\n48 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 92 in call\n49 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 92 in _run_callbacks\n50 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 734 in _run_process_action_callbacks\n51 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 81 in run_callbacks\n52 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/abstract_controller/callbacks.rb\" line 19 in process_action\n53 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/rescue.rb\" line 29 in process_action\n54 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/instrumentation.rb\" line 31 in block in process_action\n55 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/notifications.rb\" line 164 in block in instrument\n56 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/notifications/instrumenter.rb\" line 20 in instrument\n57 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/notifications.rb\" line 164 in instrument\n58 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/instrumentation.rb\" line 30 in process_action\n59 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/params_wrapper.rb\" line 250 in process_action\n60 File \"/app/vendor/bundle/ruby/2.2.0/gems/activerecord-4.2.0/lib/active_record/railties/controller_runtime.rb\" line 18 in process_action\n61 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/abstract_controller/base.rb\" line 137 in process\n62 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionview-4.2.0/lib/action_view/rendering.rb\" line 30 in process\n63 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal.rb\" line 195 in dispatch\n64 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal/rack_delegation.rb\" line 13 in dispatch\n65 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_controller/metal.rb\" line 236 in block in action\n66 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 73 in call\n67 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 73 in dispatch\n68 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 42 in serve\n69 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/journey/router.rb\" line 43 in block in serve\n70 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/journey/router.rb\" line 30 in each\n71 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/journey/router.rb\" line 30 in serve\n72 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 802 in call\n73 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n74 File \"/app/vendor/bundle/ruby/2.2.0/gems/omniauth-1.2.2/lib/omniauth/strategy.rb\" line 186 in call!\n75 File \"/app/vendor/bundle/ruby/2.2.0/gems/omniauth-1.2.2/lib/omniauth/strategy.rb\" line 164 in call\n76 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n77 File \"/app/vendor/bundle/ruby/2.2.0/gems/omniauth-1.2.2/lib/omniauth/builder.rb\" line 59 in call\n78 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n79 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/rack/agent_hooks.rb\" line 30 in traced_call\n80 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 55 in call\n81 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/rack/browser_monitoring.rb\" line 23 in traced_call\n82 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 55 in call\n83 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/etag.rb\" line 24 in call\n84 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n85 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/conditionalget.rb\" line 25 in call\n86 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n87 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/head.rb\" line 13 in call\n88 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n89 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/params_parser.rb\" line 27 in call\n90 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n91 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/flash.rb\" line 260 in call\n92 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n93 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/session/abstract/id.rb\" line 225 in context\n94 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/session/abstract/id.rb\" line 220 in call\n95 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n96 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/cookies.rb\" line 560 in call\n97 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n98 File \"/app/vendor/bundle/ruby/2.2.0/gems/activerecord-4.2.0/lib/active_record/query_cache.rb\" line 36 in call\n99 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n100 File \"/app/vendor/bundle/ruby/2.2.0/gems/activerecord-4.2.0/lib/active_record/connection_adapters/abstract/connection_pool.rb\" line 647 in call\n101 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n102 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/callbacks.rb\" line 29 in block in call\n103 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 88 in call\n104 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 88 in _run_callbacks\n105 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 734 in _run_call_callbacks\n106 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/callbacks.rb\" line 81 in run_callbacks\n107 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/callbacks.rb\" line 27 in call\n108 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n109 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/remote_ip.rb\" line 78 in call\n110 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n111 File \"/app/vendor/bundle/ruby/2.2.0/gems/rollbar-1.3.1/lib/rollbar/middleware/rails/rollbar.rb\" line 24 in block in call\n112 File \"/app/vendor/bundle/ruby/2.2.0/gems/rollbar-1.3.1/lib/rollbar.rb\" line 747 in scoped\n113 File \"/app/vendor/bundle/ruby/2.2.0/gems/rollbar-1.3.1/lib/rollbar/middleware/rails/rollbar.rb\" line 22 in call\n114 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n115 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/debug_exceptions.rb\" line 17 in call\n116 File \"/app/vendor/bundle/ruby/2.2.0/gems/rollbar-1.3.1/lib/rollbar/middleware/rails/show_exceptions.rb\" line 22 in call_with_rollbar\n117 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n118 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/show_exceptions.rb\" line 30 in call\n119 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n120 File \"/app/vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/rack/logger.rb\" line 38 in call_app\n121 File \"/app/vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/rack/logger.rb\" line 20 in block in call\n122 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/tagged_logging.rb\" line 68 in block in tagged\n123 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/tagged_logging.rb\" line 26 in tagged\n124 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/tagged_logging.rb\" line 68 in tagged\n125 File \"/app/vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/rack/logger.rb\" line 20 in call\n126 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n127 File \"/app/vendor/bundle/ruby/2.2.0/gems/request_store-1.1.0/lib/request_store/middleware.rb\" line 8 in call\n128 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n129 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/request_id.rb\" line 21 in call\n130 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n131 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/methodoverride.rb\" line 22 in call\n132 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n133 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/runtime.rb\" line 18 in call\n134 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n135 File \"/app/vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/cache/strategy/local_cache_middleware.rb\" line 28 in call\n136 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n137 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-rewrite-1.5.1/lib/rack/rewrite.rb\" line 24 in call\n138 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n139 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-cache-1.2/lib/rack/cache/context.rb\" line 136 in forward\n140 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-cache-1.2/lib/rack/cache/context.rb\" line 245 in fetch\n141 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-cache-1.2/lib/rack/cache/context.rb\" line 185 in lookup\n142 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-cache-1.2/lib/rack/cache/context.rb\" line 66 in call!\n143 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-cache-1.2/lib/rack/cache/context.rb\" line 51 in call\n144 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n145 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/static.rb\" line 113 in call\n146 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n147 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/sendfile.rb\" line 113 in call\n148 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n149 File \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/ssl.rb\" line 24 in call\n150 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n151 File \"/app/vendor/bundle/ruby/2.2.0/gems/utf8-cleaner-0.0.9/lib/utf8-cleaner/middleware.rb\" line 18 in call\n152 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n153 File \"/app/vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/engine.rb\" line 518 in call\n154 File \"/app/vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/application.rb\" line 164 in call\n155 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n156 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-reverse-proxy-0.4.4/lib/rack/reverse_proxy.rb\" line 16 in call\n157 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n158 File \"/app/vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/deflater.rb\" line 35 in call\n159 File \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\n160 File \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/configuration.rb\" line 74 in call\n161 File \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/server.rb\" line 492 in handle_request\n162 File \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/server.rb\" line 363 in process_client\n163 File \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/server.rb\" line 254 in block in run\n164 File \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/thread_pool.rb\" line 101 in call\n165 File \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/thread_pool.rb\" line 101 in block in spawn_thread\n``\n. @evanphx it'sRails.application.routes.url_helpers.order_url(self), which returns a regular/orders/:idurl. My suspect was ActionPack 4.2, but other people reported that they could see this error withPumaonly. I didn't try other servers. This error happens few times a day out of 1000+ same requests on my server. And doesn't happen with any controller- or view-originated calls to route helpers.\n. Thanks @evanphx, it's on my refactoring list, but callingRails.application.routes.url_helpers` shouldn't cause errors like this. Do you think it is related to Puma multithreading?\n. @evanphx MRI\n. @sbull thanks for your example. It reduced the number of occurrences almost to none for me, but the error still happens\n. ",
    "intinig": "It's rails 4.2. All the gems we're using declare they are thread-safe, but\nsomeone is obv. lying :)\nFor the time being, being unable to troubleshoot the issue we had to turn\nback to unicorn, but god I'd love to go with puma.\nOn Thu, Feb 5, 2015 at 10:16 PM, Evan Phoenix notifications@github.com\nwrote:\n\nThese are thread safety related issues with your app. Is this a rails app?\nWhat version?\n\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/647#issuecomment-73130460.\n. I'll try to gather some info, thanks for the help.\n. Thank you for the full investigation on the error (on my part, but I bet I can safely say it's on behalf of everyone :)).\n\nThis isn't even Puma related!\n. ",
    "pusewicz": "Yup, having same problems here.\n. ",
    "nicolaracco": "In my case the error is raised by devise in one of its views when it calls one of its 'magic' paths (e.g. new_confirmation_path(resource_name)).\nBacktrace:\nActionView::Template::Error: undefined method `url_options' for #<Module:0x007ffe03407410>\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 271 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 222 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/route_set.rb\" line 334 in block (2 levels) in define_url_helper\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/routing/routes_proxy.rb\" line 31 in new_user_confirmation_path\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/devise-3.4.1/lib/devise/controllers/url_helpers.rb\" line 52 in new_confirmation_path\nFile \"/app/app/views/devise/shared/_links.slim\", line 11 in _app_views_devise_shared__links_slim__4228686031403603823_70364466969860\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/actionview-4.2.0/lib/action_view/template.rb\" line 145 in block in render\n[...]\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/application.rb\" line 164 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb\" line 57 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/configuration.rb\" line 74 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/server.rb\" line 492 in handle_request\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/server.rb\" line 363 in process_client\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/server.rb\" line 254 in block in run\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/thread_pool.rb\" line 101 in call\nFile \"/app/vendor/bundle/ruby/2.2.0/gems/puma-2.10.2/lib/puma/thread_pool.rb\" line 101 in block in spawn_thread\ndevise's url_helpers module: https://github.com/plataformatec/devise/blob/master/lib/devise/controllers/url_helpers.rb#L52\nDo you think this is related to the Rails.application.routes.url_helpers issue you just explained ?\n. MRI here too\nIl 13/feb/2015 09:30 \"Filippos Vasilakis\" notifications@github.com ha\nscritto:\n\nAlso MRI here\nOn Feb 13, 2015 9:00 AM, \"Dmitry Klimensky\" notifications@github.com\nwrote:\n\n@evanphx https://github.com/evanphx MRI\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/647#issuecomment-74217777.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/647#issuecomment-74220522.\n. Full backtrace: https://gist.github.com/nicolaracco/8d0e83f14663d29ab022\n. \n",
    "vasilakisfil": "Same here, puma 2.11 and rails 4.2 and ruby 2.2.\nBacktrace:\nActionView::Template::Error (undefined method `url_options' for #<Module:0x007fc23e2a8f20>): \n    1: <% provide(:title, \"Login\") %> \n    2: <%# should move such routes in application's application controller or even better the whole partial %> \n    3: <div class=\"options\"> \n    4:   <%= link_to \"Sign in\", sign_in_link, class: \"active\" %> \n    5:   <% if Casauth::Settings.new_account_text %> \n    6:     <%= link_to(Casauth::Settings.new_account_text, account_creation_link, \"data-no-turbolink\" => true)  %> \n    7:   <% end %> \n  app/helpers/application_helper.rb:27:in `sign_in_link' \n  app/views/shared/_menu.html.erb:4:in `_app_views_shared__menu_html_erb___3576146891509294833_70236101630660' \nPassing 'lock' command to redis as is; blind passthrough has been deprecated and will be removed in redis-namespace 2.0 (at /app/vendor/bundle/ruby/2.2.0/gems/pdxos-0.1.10/lib/pdxos/token.rb:17:in `block in generate') \nActionController::RoutingError (No route matches [HEAD] \"/profile\"): \n  vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/debug_exceptions.rb:21:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/show_exceptions.rb:30:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/rack/logger.rb:38:in `call_app' \n  vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/rack/logger.rb:20:in `block in call' \n  vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/tagged_logging.rb:68:in `block in tagged' \n  vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/tagged_logging.rb:26:in `tagged' \n  vendor/bundle/ruby/2.2.0/gems/activesupport-4.2.0/lib/active_support/tagged_logging.rb:68:in `tagged' \n  vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/rack/logger.rb:20:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/request_id.rb:21:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/methodoverride.rb:22:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/runtime.rb:18:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/static.rb:113:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/rack-1.6.0/lib/rack/sendfile.rb:113:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/actionpack-4.2.0/lib/action_dispatch/middleware/ssl.rb:24:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/honeybadger-2.0.2/lib/honeybadger/rack/error_notifier.rb:33:in `block in call' \n  vendor/bundle/ruby/2.2.0/gems/honeybadger-2.0.2/lib/honeybadger/config.rb:181:in `with_request' \n  vendor/bundle/ruby/2.2.0/gems/honeybadger-2.0.2/lib/honeybadger/rack/error_notifier.rb:30:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/honeybadger-2.0.2/lib/honeybadger/rack/user_feedback.rb:29:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/honeybadger-2.0.2/lib/honeybadger/rack/user_informer.rb:19:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/skylight-0.6.0/lib/skylight/middleware.rb:56:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/engine.rb:518:in `call' \n  vendor/bundle/ruby/2.2.0/gems/railties-4.2.0/lib/rails/application.rb:164:in `call' \n  vendor/bundle/ruby/2.2.0/gems/newrelic_rpm-3.9.9.275/lib/new_relic/agent/instrumentation/middleware_tracing.rb:57:in `call' \n  vendor/bundle/ruby/2.2.0/gems/puma-2.11.0/lib/puma/configuration.rb:82:in `call' \n  vendor/bundle/ruby/2.2.0/gems/puma-2.11.0/lib/puma/server.rb:507:in `handle_request' \n  vendor/bundle/ruby/2.2.0/gems/puma-2.11.0/lib/puma/server.rb:375:in `process_client' \n  vendor/bundle/ruby/2.2.0/gems/puma-2.11.0/lib/puma/server.rb:262:in `block in run' \n  vendor/bundle/ruby/2.2.0/gems/puma-2.11.0/lib/puma/thread_pool.rb:104:in `call' \n  vendor/bundle/ruby/2.2.0/gems/puma-2.11.0/lib/puma/thread_pool.rb:104:in `block in spawn_thread'\nruby\n  def account_creation_link\n    @create_account_path = \"#{main_app.new_profile_path}#{build_service}\"\n  end\n. Also MRI here\nOn Feb 13, 2015 9:00 AM, \"Dmitry Klimensky\" notifications@github.com\nwrote:\n\n@evanphx https://github.com/evanphx MRI\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/puma/puma/issues/647#issuecomment-74217777.\n. \n",
    "PJK": "Same thing here, ruby 2.0.0p576 (2014-09-19 revision 47628) [x86_64-darwin14.0.0]\n. Sorry for the noise, my error actually originated from a Sidekiq worker:\nhttps://gist.github.com/PJK/fc95af896faff98ee79c\nGuessing from this, it's probably a Rails issue\n. ",
    "JamesEntar": "For what it's worth, I'm occasionally experiencing the same error since upgrading to ActionPack 4.2, but in a Sidekiq worker instead of Puma, so this might be a multithreading issue in ActionPack after all\n. ",
    "sbull": "I'm experiencing this as well on Rails 4.2.0, Ruby 2.2.0 (MRI), Puma 2.11.0, Heroku cedar-14.\nHere's a trace (from Sentry) on a fairly uninteresting view using a rails_admin route: https://gist.github.com/sbull/65c985c09e6c430af595\nI've got other parts of my app in which this is happening that don't involve Puma at all, but happen during a long-lived \"rails runner ...\" process. It also uses threads to handle events: https://gist.github.com/sbull/654304a00dbb1aa7b02e\nIf I can help any more, let me know.\n. Here's a test script I'm using to demonstrate the problem. I get errors pretty reliably.\n``` ruby\nvia \"rails runner\"\nnum_threads = (ARGV[0] || 10).to_i\nnum_iterations = (ARGV[1] || num_threads).to_i\nputs \"Threads: #{num_threads}, Iterations per thread: #{num_iterations}\"\nthreads = []\nnum_threads.times do |thread_num|\n  threads << Thread.new(thread_num) do |num|\n    Thread.current.thread_variable_set('num', num)\n    num_iterations.times do |i|\n      Thread.current.thread_variable_set('iter', i)\n      print num.to_s+Rails.application.routes.url_helpers.root_path\n    end\n  end\nend\nerrors = []\nthreads.each do |thr|\n  begin\n    thr.join\n    print \"\\nFinished thread #{thr.thread_variable_get('num')}.\\n\"\n  rescue => e\n    errors << \"\\nException in thread #{thr.thread_variable_get('num')}, iteration #{thr.thread_variable_get('iter')}: #{e.message}\\n\"+e.backtrace.map{|bt| \"\\t#{bt}\\n\" }.join\n  end\nend\nputs \"\\n-------\\nERRORS: #{errors.length}\"\nputs errors\n```\nGist: https://gist.github.com/sbull/3e878e98c4ee3c0c8430\n. My workaround, loaded in an initializer:\nruby\nmodule Routes\n  module UrlHelpers\n    include Rails.application.routes.url_helpers\n  end\n  extend UrlHelpers\n  def self.default_url_options\n    ActionMailer::Base.default_url_options\n  end\nend\nElsewhere, replace Rails.application.routes.url_helpers.<your_url_or_path>\nwith Routes.<your_url_or_path>.\n. That's most likely because semicolon is a reserved character (https://tools.ietf.org/html/rfc1738#section-3.3). It should probably be encoded as %3B to work (i.e. http://localhost:3000/attachments/download/301/attachment-with-%3B-semicolon.txt)\n. Well, that section of RFC 1738 I referenced says ; is \"reserved\", meaning it can't just be used at will in the path. Likewise, ? is reserved and can't be used in the path - it designates the separation between the path and searchpath. It also elaborates that / may be used \"to designate a hierarchical structure\", but doesn't say anything about how ; may be used - so the presumption is that it may not be used, the same that ? may not be used in the path.\n\"Reserved\" can mean that a defined use may be codified for it in the future, so you can't use it at present without going against the spec and taking the risk of being forward-incompatible. So spec-conforming services can't use it. Unless there's an updated spec you're referencing?. ",
    "krim": "How fix it?\n. ",
    "sescobb27": "The Bug: https://bugs.ruby-lang.org/issues/10871\nThe Fix: https://github.com/ruby/ruby/commit/d84f9b16946bca06ce0557ebe99152d7d445c9ec , it's going to be released in ruby 2.3, but (maybe) we are going to have it solved as a patch just for 2.1 and 2.2. I'm waiting for it.\n. ",
    "vincentwoo": "FYI for anyone watching this thread: this is apparently mitigated in MRI 2.2.4 but not fixed completely until 2.3.\n. Taking a page from the other issue with dead workers, I've added the following to my config.ru in the hopes of catching something next time:\n``` ruby\nif ENV['RACK_ENV'] == 'production'\n  module PumaThreadLogger\n    def initialize args\n      ret = super args\n      Thread.new do\n        while true\n          puts \"puma threads: #{@workers.select{|x| x.alive?}.size}/#{@workers.size} alive\"\n          sleep 10\n        end\n      end\n      ret\n    end\n  end\nmodule Puma\n    class ThreadPool\n      prepend PumaThreadLogger\n    end\n  end\nend\n```\n. I see no untoward log messages at the start of H12 periods, besides perhaps an exclusive lock message from postgres. I am running one worker, seven threads per 2x dyno.\n. Yeah, it might be correlated with these kinds of postgres issues:\n590720474347274240  2015-10-13T22:39:18 2015-10-13T22:39:18Z    102409703   CoderPad    54.161.169.179  Local0  Info    app/postgres.12931  [DATABASE] process 12931 still waiting for ExclusiveLock on tuple (18856,8) of relation 16435 of database 16385 after 1000.129 ms Detail: Process holding the lock: 12210. Wait queue: 12274, 12190, 12243, 12307, 12335, 12191, 12312, 12378, 12333, 31279, 21050, 2855, 11741, 4825, 31612, 21051, 21061, 12882, 21073, 12872, 12881, 12931. Query: UPDATE \"pads\" SET \"events\" = '....', \"updated_at\" = '2015-10-13 22:39:17.370246' WHERE \"pads\".\"id\" = 242792 \nI could see deadlocking on a pg lock causing puma to stop being able to accept new requests. Restarting the dynos fixed the issue. Is there some way for puma to catch situations like this? Is this exacerbated by running rack-timeout-puma?\n. @schneems do you think having this passage on the default heroku docs might need reconsidering? https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server#timeout\n. @schneems I'm not doing any locking manually, it's probably just happening in Rails underneath me.\n. @schneems I'll try but that might be hard for me to pull off. Do you have any recommended approaches for debugging whether we're locking up workers or not? I can try just curl hammering some endpoints locally and hoping.\n. This happened again today on one of my dynos. Logging indicates that all worker threads are alive, but a ton of H12 errors were going on. No undue stress on the dynos, low volume (far past peak for the day). No postgres errors (indeed, I turned off prepared statements), no rack-timeout. I have no idea how to reproduce or what to do to mitigate. My app's memory footprint is very stable and doesn't go over 50% of the dyno allotment.\nI'd really love some advice! This is a very scary thing to have happen in production.\n. @schneems When I was running rack-timeout, I was seeing even more problems, and certainly got no backtraces of value.\nI can't really afford to just turn on performance dynos and wait - this problem happens infrequently enough that I'd never really be sure (and also eat a hefty bill in the meantime). If you're willing to comp me the dyno time, I'm happy to give it a shot, though.\nYour points about Timeout's unsafeness are what I had in mind too. I WAS logging the number of alive threads in Puma, and it reported everything as healthy when I saw the issue, but I have no idea if that's information you can trust.\n. The time between incidents is something like a few weeks. I run 7 workers on two 2x dynos right now. Is that... too many? I basically only use half of the available RAM with that configuration.\n. @schneems Happened again to me just now. Filed a high priority request with heroku support but they are closed right now. Trying to see if it is \"noisy neighbor\" related. If you could lend your support, that'd be great too: https://help.heroku.com/tickets/315102\n. I think this might be related to issues like https://github.com/heroku/rack-timeout/issues/39 - I've got some code that uses httpclient + Timeout under the covers. Investigating more.\n. I think my issues were exacerbated by certain long running requests. I removed or optimized every endpoint that ran for more than a short number of milliseconds and haven't seen issues since. My intuition is that puma can get into a bad state with long requests piling up. We never figured out a root cause.\n. @taytay any chance you could add backlog depth to the stats endpoint as a PR into puma? I think this really should be in there fore debugging purposes. Would help a TON on Heroku.\ncc @schneems \n. Nevermind, just hit this on 3.6.2 too. Ugh.. would really love a new version cut with this fix! Puma is hard to use in dev with SSL currently.. Hi @evanphx, sorry to nag, but I really think it'd be great to get a new Puma version out with this fix.. Is there any chance we can get a release cut without #1344? This one seems quite serious in comparison.. Sorry for splitting the thread, but interested parties may want to check out https://github.com/puma/puma/issues/1502. This is probably related to https://github.com/puma/puma/issues/1483, since we also run SSL in dev.. If relevant, this is our puma config:\n```ruby\nworkers Integer(ENV['WEB_CONCURRENCY'] || 1) # 1 worker 5 threads in dev\nthreads_count = Integer(ENV['MAX_THREADS'] || 5)\nthreads threads_count, threads_count\npreload_app!\n_port = ENV['PORT'] || 8080\n_environment = ENV['RACK_ENV'] || 'development'\nif _environment == 'development' && File.exists?('/etc/ssl/localhost.ssl.key')\n  ssl_bind '0.0.0.0', _port, {\n    key: '/etc/ssl/localhost.ssl.key',\n    cert: '/etc/ssl/localhost.ssl.crt',\n    verify_mode: 'none'\n  }\nelse\n  port _port\nend\nrackup DefaultRackup\nenvironment _environment\non_worker_boot do\n  # Worker specific setup for Rails 4.1+\n  # See: https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server#on-worker-boot\n  ActiveRecord::Base.establish_connection\nend\nplugin :tmp_restart\n```. After a lot of print debugging, I believe the exception-throwing line is: https://github.com/puma/puma/blob/master/lib/puma/minissl.rb#L66\nruby\n@engine.inject(data)\nwhen data is nil. This seems to happen pretty reliably once I added a bunch of print debugging, suggesting the problem is timing-related. The exception happens after the server has hung for many seconds, suggesting that the root cause is deeper.\n  . This is about as far as I am liable to get on my own, I fear. Help is desperately appreciated!. Oh, in case this is related: I'm running OSX. It's possible that the issues outlined in https://blog.phusion.nl/2017/10/13/why-ruby-app-servers-break-on-macos-high-sierra-and-what-can-be-done-about-it/ matter.\nedit: the manual fix in the article didn't seem to do it. Here's the mentioned crash report file:\nawdd_2018-01-19-005328_Vincents-MacBook-Pro.awd.zip\n. My intuition is that the fix for read_nonblock crashing on nil was masking another problem. If read_nonblock returns nil, it appears we can call @parser.execute with a nil data payload. The docs on HttpParser_execute suggest this is a problem:\n/**\n * call-seq:\n *    parser.execute(req_hash, data, start) -> Integer\n *\n * Takes a Hash and a String of data, parses the String of data filling in the Hash\n * returning an Integer to indicate how much of the data has been read.  No matter\n * what the return value, you should call HttpParser#finished? and HttpParser#error?\n * to figure out if it's done parsing or there was an error.\n *\n * This function now throws an exception when there is a parsing error.  This makes\n * the logic for working with the parser much easier.  You can still test for an\n * error, but now you need to wrap the parser with an exception handling block.\n *\n * The third argument allows for parsing a partial request and then continuing\n * the parsing from that position.  It needs all of the original data as well\n * so you have to append to the data buffer as you read.\n */\nVALUE HttpParser_execute(VALUE self, VALUE req_hash, VALUE data, VALUE start). Adding a bit of logging seems to confirm:\ndata is nil, @buffer is nil\ncalling @parser.execute with {\"rack.version\"=>[1, 3], \"rack.errors\"=>#<IO:<STDERR>>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.11.1 Love Song\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\", \"HTTPS\"=>\"https\"}, nil, 0\n/Users/vwoo/.gem/ruby/2.5.0/gems/puma-3.11.1/lib/puma/client.rb:297: [BUG] Segmentation fault at 0x0000000000000008. Thanks!. ",
    "daqo": "Yes, the class is PointImpl. On my local production server (Not heroku) I'm using Passenger for production. Over there, the app works fine. If the problem was a thread unsafe code, shouldn't I have the same problem over there as well?\n. Right now I checked it with Unicorn too. (On Heroku)\nIt seems that Unicorn has the same problem with that piece of code. So I think the problem goes back to the way Heroku manages stuff; though, I'm not sure.\n. Yes, every single request. Lat and Lon method will get disappeared. \n. So basically, in my model I use:\nset_rgeo_factory_for_column(:current_location, \n                              RGeo::Geographic.spherical_factory(:srid => 4326))\nAnd then in my location creator I have the following code:\n```\nrequire_relative \"./coordinate_builder\"\nrequire 'rgeo'\nclass LocationCreator\n  DEFAULT = \"default\"\ndef self.create(lat_lng)\n    return nil if lat_lng == DEFAULT\nlng, lat = CoordinateBuilder.parse(lat_lng)\ngeographic_factory = RGeo::Geographic.spherical_factory\ngeographic_factory.point(lng, lat)\n\nend\nend\n```\n. Ok I found the problem Evan.\nIt had nothing to do with Puma. Apparently Rgeo::Geographic::SphericalPointImpl falls back to RGeo::Cartesian::PointImpl in my Javascript code. As a workaround, I changed my javascript calls from @hive.current_location.lat to @hive.current_location.y. Also for latitude, I did the same: @hive.current_location.lon to @hive.current_location.x. This fixed the problem for me.\nThank you so much for your help.\n. ",
    "NickLaMuro": "We ran into the same issue on our end.  For what it is worth, removing the prune_bundler option seemed to allow us to deploy again.\n. @evanphx I am looking into it a bit myself since I have a bit of free time today.  I will get back to about the Gemfile, but since it isn't \"my code to share\", I don't want to just copy-pasta the Gemfile in since it is using Gemfury for some of the sources.  I will work on a stripped down dummy app to recreate this via vagrant to hopefully help things out.\nIn regards to what I can share, we are actually using multiple versions of puma.  2.9.0, 2.9.1, and 2.10.0 are the ones that I just quickly checked on the about 10 different apps running puma as the app server.  The only changed that happened yesterday that would affect this is bundler, and that is because we have some deploy automation code in place that auto upgrades bundler for us.  The release that happened yesterday seems to have then caused this to fail in multiple places.\nI tracked down the failure in puma-wild to here for 2.9.1, and that seems to match the stacktrace that @dvrensk ran into for 2.11.0 (https://github.com/puma/puma/blob/e8feadd2b/bin/puma-wild#L20).  I have had trouble recreating it locally myself, and the closest I have gotten is by running the command passed to Kernel.exec with sudo -u <my_user_name>, which is similar to how we handle it in our init scripts.\nThis backs up the theory you seem to be having that it is an ENV issue change with bundler, so I will see what I can do to get you more information to prove/disprove that thought.\n. I just realized that because I am using .rvm, my tests locally are going to be fruitless when using sudo -u because it won't pull in the .bashrc env to get RVM into the load path :disappointed:.  I will probably get you some Vagrant config once I get that going.\n. @evanphx Yup, all our apps are installing gems using bundle install --path, so that seems like the likely culprit. \n. Also confirmed that applying that fix solves our issue!  Great work on this and sorry I wasn't more help.\n. @brian-kephart This looks to be a rails bug, and not a puma.  The correct port is being passed to puma, but Rails is displaying the line incorrectly when outputing \"=> Rails 5.1.2 application starting in development on http://localhost:3000\":\nhttps://github.com/rails/rails/blob/f8f3d70/railties/lib/rails/commands/server/server_command.rb#L69\nI think you probably should report the bug there.\n. cc @schneems . Fixes https://github.com/puma/puma/issues/1371. I will be adding tests as well, but wanted to get a proposal up for https://github.com/puma/puma/issues/1371 to get some eyes on it incase it was the wrong approach.. ",
    "SavithaK": "FWIW, we are seeing the same issue as well.\n. Our app uses puma with rails (MRI). With bundler v1.8, puma would continuously start and crash. Exception was the same as @dvrensk specified above. \nWe've redeployed without prune_bundler as a workaround. \nOur puma config looks like this:\nthreads 8,32\nworkers 16\nbind \"unix:///tmp/service.puma.sock\"\nenvironment \"production\"\npidfile \"/opt/service/current/tmp/pids/puma.pid\"\nactivate_control_app 'tcp://0.0.0.0:8081', { no_token: true }\ndirectory '/opt/service/current'\nprune_bundler\nstdout_redirect '/opt/service/current/log/puma.stdout.log',\n  '/opt/service/current/log/puma.stderr.log', true\n@git_sha = File.exists?('/opt/service/current/VERSION') &&\n  File.read('/opt/service/current/VERSION')[0,8]\ntag @git_sha\non_worker_boot do\n  puts \"[#{Process.pid}] Booting puma worker: git rev #{@git_sha}\" if @git_sha\nend\n. Also some version details:\nRails: 4.1.5\nRuby: 2.1.4\nPuma: 2.9.2\n. Thanks for looking at this @evanphx and sorry I wasn't able to help out much today. Yes, we do use bundle install --path. I have some time now, if you need any other info.\n. Upgraded one of our apps to 2.11.1 and seeing something odd. When using pumactl to start the app, I get:\nbundle exec pumactl -F config/puma/staging.rb start\n- Pruning Bundler environment\n  /opt/exceptions/shared/bundle/ruby/2.1.0/gems/puma-2.11.1/lib/puma/cli.rb:295:in parse_options': invalid option: -F (OptionParser::InvalidOption)\n  from /opt/exceptions/shared/bundle/ruby/2.1.0/gems/puma-2.11.1/lib/puma/cli.rb:453:inrun'\n  from /opt/exceptions/shared/bundle/ruby/2.1.0/gems/puma-2.11.1/bin/puma-wild:31:in `'\nI don't see this in the other apps that are running puma version 2.9.2. I'll dig around some more in case this is unrelated.\n. Actually the other apps have prune-bundler removed too. So that's another difference.\n. curl --silent --show-error http://localhost:8081/restart works, however\n. We aren't using any kind of persistent requests. This is a new problem that we haven't seen before. The last issue we had was with the bundler update to 1.8 that broke hot restart with prune_bundler. That got fixed with the update to puma 2.11.1. \nThe phased restart was just a one off attempt to see if the issue still happened. We are only using hot restart for our deployment.\n. @evanphx is there a way we can check what the worker processes are doing? Any worker level logs?\n. We stopped all traffic to the box and  the restart worked. It does look like we may have hanging requests that caused the workers to not stop. We'll investigate a bit more.\n. It does appear this was because of hung requests. \n. ",
    "dvrensk": "I've gisted Gemfile and the output of gem env and gem list here: https://gist.github.com/dvrensk/f28d4c5bcf91582b2b46\nThe only difference between the broken environment and what I have now is that I have run gem uni bundler, gem i bundler -v 1.7.12.\nThanks for your help!  Let me know if there is anything more I can do.  (If you want me to really recreate the problem by launching a new server running bundler 1.8.0 I can do that too, although not immediately.)\n. Yes, we are using --path (bundle install --gemfile /home/simba/app/releases/20150209113217/Gemfile --path /home/simba/app/shared/bundle --deployment --quiet --without development test to be specific).\nI'm launching a new server now to see if 2.11.1 solves the problem.  Thanks a lot!  And I'm sorry about the time zone delays.\n. ",
    "ball-hayden": "Apologies for bumping an old issue, but I'm seeing the following:\n[sparkseat@localtest current]$ puma -v\npuma version 2.11.2\n[sparkseat@localtest current]$ bundle exec pumactl -F config/puma.rb start\n* Pruning Bundler environment\n/var/www/sparkseat_api/shared/bundle/ruby/2.1.0/gems/puma-2.11.2/lib/puma/cli.rb:534:in `parse_options': invalid option: -F (OptionParser::InvalidOption)\n    from /var/www/sparkseat_api/shared/bundle/ruby/2.1.0/gems/puma-2.11.2/lib/puma/cli.rb:190:in `run'\n    from /var/www/sparkseat_api/shared/bundle/ruby/2.1.0/gems/puma-2.11.2/bin/puma-wild:31:in `<main>'\nWe are also installing gems using bundle install --path\n. I'm also having issues running dashing after upgrading to puma 2.12.2. I see the following in puma.err:\n/usr/local/rvm/rubies/ruby-2.2.2/lib/ruby/site_ruby/2.2.0/rubygems/core_ext/kernel_require.rb:54:in `require': cannot load such file -- dashing (LoadError)\n    from /usr/local/rvm/rubies/ruby-2.2.2/lib/ruby/site_ruby/2.2.0/rubygems/core_ext/kernel_require.rb:54:in `require'\n    from config.ru:1:in `block in <module:PumaRackCompat>'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:189:in `instance_eval'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:189:in `initialize'\n    from config.ru:in `new'\n    from config.ru:in `<module:PumaRackCompat>'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:183:in `eval'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:183:in `new_from_string'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:174:in `parse_file'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/configuration.rb:104:in `load_rackup'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/configuration.rb:71:in `app'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/runner.rb:123:in `app'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/runner.rb:130:in `start_server'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:215:in `worker'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in `block (2 levels) in spawn_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in `fork'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in `block in spawn_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:105:in `times'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:105:in `spawn_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:157:in `check_workers'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:423:in `run'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cli.rb:215:in `run'\n    from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/bin/puma-wild:31:in `<main>'\nIn case it is significant, I am bundling dashing from master (rather than the version released to rubygems).\nMy puma.rb:\n```\nDEPLOY_NAME=\"sparkseat_dash\"\ndirectory \"/var/www/#{DEPLOY_NAME}/current\"\nenvironment 'production'\ndaemonize false\npidfile    \"/var/www/#{DEPLOY_NAME}/shared/tmp/puma/pid\"\nstate_path \"/var/www/#{DEPLOY_NAME}/shared/tmp/puma/state\"\nthreads 0, 16\nbind \"unix:///var/www/#{DEPLOY_NAME}/shared/tmp/puma.sock\"\nstdout_redirect \"/var/www/#{DEPLOY_NAME}/shared/log/puma.log\", \"/var/www/#{DEPLOY_NAME}/shared/log/puma.err\", true\n=== Cluster mode ===\nworkers 1\nprune_bundler\n=== Puma control rack application ===\nactivate_control_app\n.\nconfig.ru\nrequire 'dashing'\nconfigure do\n  set :default_dashboard, 'default'\nhelpers do\n    def protected!\n     # Put any authentication code you want in here.\n     # This method is run before accessing any resource.\n    end\n  end\nend\nmap Sinatra::Application.assets_prefix do\n  run Sinatra::Application.sprockets\nend\nrun Sinatra::Application\n``\n. I've done a little more digging, and I get a different error if I explicitlyBundler.require`:\n```\nconfig.ru\nrequire 'rubygems'\nrequire 'bundler'\nBundler.require\nrequire 'dashing'\nconfigure do\n  set :default_dashboard, 'default'\nhelpers do\n    def protected!\n     # Put any authentication code you want in here.\n     # This method is run before accessing any resource.\n    end\n  end\nend\nmap Sinatra::Application.assets_prefix do\n  run Sinatra::Application.sprockets\nend\nrun Sinatra::Application\n```\nconfig.ru:8:in `block in <module:PumaRackCompat>': undefined method `configure' for #<Puma::Rack::Builder:0x00000001b39830> (NoMethodError)\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:189:in `instance_eval'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:189:in `initialize'\n        from config.ru:in `new'\n        from config.ru:in `<module:PumaRackCompat>'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:183:in `eval'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:183:in `new_from_string'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/rack/builder.rb:174:in `parse_file'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/configuration.rb:104:in `load_rackup'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/configuration.rb:71:in `app'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/runner.rb:123:in `app'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/runner.rb:130:in `start_server'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:215:in `worker'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in `block (2 levels) in spawn_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in `fork'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:109:in `block in spawn_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:105:in `times'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:105:in `spawn_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:157:in `check_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cluster.rb:423:in `run'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/lib/puma/cli.rb:215:in `run'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.12.2/bin/puma-wild:31:in `<main>'\n. @evanphx: I'm afraid I'm still seeing this issue with 2.13.4:\nconfig.ru:8:in `block in <main>': undefined method `configure' for #<Puma::Rack::Builder:0x00000000b677e0> (NoMethodError)\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/rack/builder.rb:184:in `instance_eval'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/rack/builder.rb:184:in `initialize'\n        from config.ru:in `new'\n        from config.ru:in `<main>'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/rack/builder.rb:170:in `eval'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/rack/builder.rb:170:in `new_from_string'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/rack/builder.rb:161:in `parse_file'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/configuration.rb:129:in `load_rackup'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/configuration.rb:96:in `app'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/runner.rb:123:in `app'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/runner.rb:130:in `start_server'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:215:in `worker'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:109:in `block (2 levels) in spawn_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:109:in `fork'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:109:in `block in spawn_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:105:in `times'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:105:in `spawn_workers'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cluster.rb:386:in `run'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/lib/puma/cli.rb:215:in `run'\n        from /var/www/sparkseat_dash/shared/bundle/ruby/2.2.0/gems/puma-2.13.4/bin/puma-wild:31:in `<main>'\n. A few trials shows that this regression was introduced in 3.0.0.\n2.16.0 is the last version that works.\n. @evanphx Thanks for fixing that.\nWould it be worth adding a test for these arguments?\n. ",
    "ianwalter": "That is the correct pid file. Just realized puma is ignoring puma.rb even though it's in the config directory. I was running rails s -b 0.0.0.0 originally, but changing it to puma -C config/puma.rb doesn't seem to fix the problem, although the config is no longer ignored. I'm running the restart commands inside of the container using docker exec. I'm sorry I forgot to mention that the pumactl commands return:\nNeither pid nor control url available\n. So the pid is 1 according to ps I guess since there's only one process running within the container. The pid file does contain 1. But I still get that message even when I pass 1 directly with -p\n. ",
    "david-pm": "For posterity, if you run into this error and preloading the app isn't your issue, make sure you are including ActiveRecord in your application.rb: require 'active_record/railtie'. ",
    "knovoselic": "@evanphx thanks. Any way to see what was wrong in the request?\n. Ok, thanks.\n. ",
    "beanieboi": "to restore the performance we just downgraded puma. (see screenshot)\n\nour config/puma.rb\n```\nworkers Integer(ENV['PUMA_WORKERS'] || 2)\nthreads Integer(ENV['PUMA_MIN_THREADS']  || 8), Integer(ENV['PUMA_MAX_THREADS'] || 12)\nrackup      DefaultRackup\nenvironment ENV['RACK_ENV'] || 'development'\nif dir = ENV[\"BOXEN_SOCKET_DIR\"]\n  bind \"unix://#{dir}/mothership\"\nelse\n  port ENV[\"PORT\"] || 3000\nend\n```\nthe actual values from Heroku:\nLoading production environment (Rails 4.1.9)\nirb(main):001:0> ENV['PUMA_WORKERS']\n=> \"5\"\nirb(main):002:0> ENV['PUMA_MIN_THREADS']\n=> \"8\"\nirb(main):003:0> ENV['PUMA_MAX_THREADS'] \n=> \"10\"\nirb(main):004:0> ENV['RACK_ENV']\n=> \"production\"\nirb(main):007:0> ENV[\"BOXEN_SOCKET_DIR\"]\n=> nil\nirb(main):008:0> ENV[\"PORT\"]\n=> \"47778\"\nand from the Procfile\nweb: bundle exec puma -C config/puma.rb\n. the official Heroku explanation is here: \nhttps://devcenter.heroku.com/articles/newrelic#interpreting-new-relic-request-queuing\n\nOn 07.03.2015, at 00:23, Evan Phoenix notifications@github.com wrote:\nHm. The request queue... What exactly is that measuring? /cc @schneems\n\u2014\nReply to this email directly or view it on GitHub.\n. i\u2019ll give it a shot.\nOn 08 Mar 2015, at 16:14, Evan Phoenix notifications@github.com wrote:\nIs it possible that the request time leaped up because the change was deployed on a dyno with a drifted clock then? Seems like just deploying it again might tell us.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/puma/puma/issues/661#issuecomment-77754682.\n. tl;dr; \nsorry for the false alert.\n\ni deployed the new puma version 30mins ago and there is no increase in the request queuing. it looks like there happened something in NewRelic/Heroku/Middleware which caused the time spent in the request queue to spike.\ni'll reopen in case something happens later today and looks related to that.\nthanks everybody for the help!\n. ",
    "dnishimura": "@beanieboi Were you able to verify this?\n. Thanks. I downgraded to version 2.10.2 from 2.11.1 and didn't see a difference as well.\n. ",
    "Vikaton": "o wow, thanks\n. ",
    "JuanitoFatas": "P.S. Could you please also add some metadata information of this gem in this page https://rubygems.org/gems/puma/edit :pray::\n\nHere are some I know:\n| URL | VALUE |\n| --- | --- |\n| Source Code URL | https://github.com/puma/puma |\n| Documentation URL | http://www.rubydoc.info/gems/puma |\n| Wiki URL | https://github.com/puma/puma/wiki |\n| Mailing List URL | N/A |\n| Bug Tracker URL | https://github.com/puma/puma/issues |\nThank you!\n. Thank you so much, Mr Phoenix!!!!!! :heart: :yellow_heart: :blue_heart: :green_heart: :purple_heart:\n. @evanphx Sorry I found bug tracker url is wrong:\n\"bug_tracker_uri\"=>\"https://github.com/github/puma/issues\"\nshould be \n\"https://github.com/puma/puma/issues\"\nMy apologies.\n. ",
    "frodsan": "@evanphx Hi, I upgrade puma to 2.13.0 and I'm having some problems in Heroku.\nIn version 2.12.3, the port is correctly binded.\n$ PORT=123132 bundle exec puma -C config/puma.rb\n[904] Puma starting in cluster mode...\n[904] * Version 2.12.3 (ruby 2.2.2-p95), codename: Plutonian Photo Shoot\n[904] * Min threads: 5, max threads: 5\n[904] * Environment: development\n[904] * Process workers: 2\n[904] * Preloading application\n[904] * Listening on tcp://0.0.0.0:123132\nIn version 2.13.0, it takes the default port even when I assign a different one.\n$ PORT=123132 bundle exec puma -C config/puma.rb\n[909] Puma starting in cluster mode...\n[909] * Version 2.13.0 (ruby 2.2.2-p95), codename: A Midsummer Code's Dream\n[909] * Min threads: 5, max threads: 5\n[909] * Environment: development\n[909] * Process workers: 2\n[909] * Preloading application\n[909] * Listening on tcp://0.0.0.0:9292 # <=======\nThis is my config file:\n``` ruby\nworkers_count = Integer(ENV[\"PUMA_WORKERS\"] || 2)\nthreads_count = Integer(ENV[\"PUMA_MAX_THREADS\"] || 5)\nworkers workers_count\nthreads threads_count, threads_count\nrackup      DefaultRackup\nport        ENV[\"PORT\"]\nenvironment ENV[\"RACK_ENV\"]\npreload_app!\n```\nThanks in advance!\nUPDATE: I found that cli option bind overrides config file bind option even when is empty. See https://github.com/puma/puma/commit/d2d1e5b430fffb98c33f7fd8798fba689715c300#commitcomment-12722808\n. Hi,\nI'm debugging https://github.com/puma/puma/issues/919. I think it's related to this change. Now that puma loads config/puma.rb by default, I'm having errors using my application code in hooks like before_fork or on_worker_boot. How do you make them work in Rails?\nThanks!\nPD: I made a simple application that shows the error: https://github.com/frodsan/shotgun-puma-bug.\nYou can check the config/puma.rb file. \n. I checked the issue again. It works if I use puma:\n$ puma\n[3933] Puma starting in cluster mode...\n[3933] * Version 3.0.2 (ruby 2.3.0-p0), codename: Plethora of Penguin Pinatas\n[3933] * Min threads: 5, max threads: 5\n[3933] * Environment: development\n[3933] * Process workers: 2\n[3933] * Preloading application\n[3933] * Listening on tcp://0.0.0.0:\nIt looks like the problem is in shotgun :-(\n. I'm seeing the same report in my logs. I'm going to try different versions of puma.\n. @mecampbellsoup https://github.com/puma/puma/commit/5fdf337790a62bdcb8a7db3379cba4007426ebec\n. I don't fully understand the question, but my changes are based on this commit https://github.com/puma/puma/commit/ba27787d0ddeec09330c1cd85d20f585a66b3b8e.\nWhen Rack was removed as a dependency, 100-199 status codes were added again:\nhttps://github.com/puma/puma/commit/537bc21593182cd9c4c0079a3936d05b1f91fe14#diff-2fedc25f1c4d43a510ad5e4f6a5c4ed3R82 but I don't think that is necessary when status < 200 is present.\nSTATUS_WITH_NO_ENTITY_BODY is only used on the server.rb code. I updated the request changed to only remove 100-199 status codes.\n. \ud83d\udc4d thanks!\n. This is the only test file where minitest's class is used. The other files use Test::Unit::TestCase. I think test/unit uses minitest behind the curtains. I also can change MiniTest::Unit::TestCase to Minitest::Test if you prefer. Would you like to update the test suite to use only minitest classes?\n. Rack 2.0 supports Ruby 2.2.2+. This breaks the build for Ruby 2.1.9 https://travis-ci.org/puma/puma/jobs/157756156. Any experience handling different Gemfiles in Travis?\n. Also, puma is not tested on Ruby 1.9.3 and 2.0.0: https://github.com/puma/puma/commit/b4a2fded22299dd8a11fd64ea88aeb3b33a0846e\n. This is the code now:\n``` ruby\nwhen 0\n  threads.each do |t|\n    t.raise ForceShutdown\n  end\nthreads.each do |t|\n    t.join SHUTDOWN_GRACE_TIME\n  end\nelse\n  timeout.times do\n    # ...\n  end\nthreads.each do |t|\n    t.raise ForceShutdown\n  end\nthreads.each do |t|\n    t.join SHUTDOWN_GRACE_TIME\n  end\nend\n```\nAnd this is the proposed change:\n`` ruby\nelse\n  # If timeout is0`, it will skip this part,\n  # and raise (force shutdown) immediately\n  timeout.times do\n    # ...\n  end\nthreads.each do |t|\n    t.raise ForceShutdown\n  end\nthreads.each do |t|\n    t.join SHUTDOWN_GRACE_TIME\n  end\nend\n```\nI think in both scenarios #raise is called, when timeout is equal or greater than 0.\n. Do you have a config/puma.rb file that you can post here?\n. I made some notes about that test here: https://github.com/puma/puma/commit/43ee6ddcacc1c5925a5e51059573dd80ccf10be0#commitcomment-20006908. There is one test file that doesn't pass in isolation:\n```\n$ ruby -Ilib -Itest test/test_binder.rb\nRun options: --seed 48927\nRunning:\nE* Listening on tcp://localhost:10001\n.\nFabulous run in 0.005208s, 384.0246 runs/s, 192.0123 assertions/s.\n1) Error:\nTestBinder#test_localhost_addresses_dont_alter_listeners_for_ssl_addresses:\nNoMethodError: undefined method check' for Puma::MiniSSL:Module\n    /Users/frodsan/Code/frodsan/puma/lib/puma/binder.rb:149:inblock in parse'\n    /Users/frodsan/Code/frodsan/puma/lib/puma/binder.rb:88:in each'\n    /Users/frodsan/Code/frodsan/puma/lib/puma/binder.rb:88:inparse'\n    test/test_binder.rb:27:in `test_localhost_addresses_dont_alter_listeners_for_ssl_addresses'\n2 runs, 1 assertions, 0 failures, 1 errors, 0 skips\n```\nI'm investigating why, maybe a require is missing.\nUpdate: fixed. \ud83d\udc4d  I hope not. @nateberkopec fixed. \ud83d\udc4d  That was fast \ud83d\ude02 . Oops ... fixed :). ",
    "dwilkie": "+1\nThis is causing elastic beanstalk to break as well. bind is ignored in the configuration file\n. ",
    "aslong": "@kcollignon did you happen to find a resolution to how to debug this? I'm running into a similar issue.\n. @kcollignon Thanks for the updated info! I did some more searching and found that you can specify 'worker_timeout' in your puma config to change from the default of 60 seconds. I'm working with an especially slow booting monolith and as soon as I bumped that to 2 minutes we stopped timing out the workers.\n. ",
    "kcollignon": "@aslong it turned out to be our server. We were trying to spin up 8 worker, and it was just too much for that particular M3 Medium instance on AWS. Once we scaled the worker count down to 3 it worked beautifully. Hope that helps!\n. Do you have Nginx configured correctly to the correct upstream server?\n. ",
    "kirs": ":+1: from the Capistrano team\n. ",
    "thiaguerd": "single rack\nbuttons, click in the button to execute command\nI am doing like this\n: rails new test_app\n: add \"gem 'puma'\" on Gemfile\n: rails g controller multiple index slow\n: change method slow to\n    def slow\n        sleep 5\n    end\n: export SECRET_KEY_BASE='MY_SECRET_KEY'\n: on prodiuction.rb\n    config.assets.compile = true\n: add on routes\n    post ':controller/:action'\n    get ':controller/:action'\n: index.html.erb \n    <% 20.times do %>\n        <%= button_to \"Go!\", {controller: \"multiple\", action: \"slow\"}, remote: true, :class => 'btn'  %> \n    <% end %>\n    <input type=\"button\" value=\"Go all!\" onclick=\"$('.btn').each(function(){$(this).click()})\" />\n: mv slow.html.erb slow.js.coffee\n: in slow.js.coffee\n    console.log \"done\"\nusing WEBrick: 1 at a time\nusing puma: 6 at a time\nwhat I need to execute all 20 or more request at a time?\n. ",
    "Telmo": "I am having similar issue but it stems from the fact that I need both regular and HTTPS traffic. Can puma handle this? So far based on the documentation it can do one or the other but not both?\nWhat I a currently doing is running Docker containers with a rails app controlled via puma and then an nginx container proxying the traffic to those containers. Unfortunately I must have both 80 and 443 up and running.\nThis is the error in questions, related to https traffic being redirected to puma http:\n2015/04/30 17:39:38 [error] 509#0: *56 SSL_do_handshake() failed (SSL: error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol) while SSL handshaking to upstream, client: 10.10.10.172, server: server.examplet.net, request: \"GET / HTTP/1.1\", upstream: \"https://10.10.0.53:80/\", host: \"server.examplet.net\"\nIs there a way of actually accomplishing this?\n. ",
    "olegantonyan": "@Leooo nope. I use god http://godrb.com/ as a dirty workaround which restarts puma when it crashes and for this project it's kind of OK.\nIt feels to me that the problem is somewhere with thread-safety, like using same db connection in different threads, but I couldn't find it. One suggestion is to avoid using database queries inside threads, but rather pass them data through queues. But I haven't tried this.\nHopefully, with upcoming ActionCable there will be no such problems.\n. https://github.com/olegantonyan/puma_systemd_bug here is a fresh new rails app with puma.service and puma.socket\n~/.bashrc\n```\nnodenv\nexport PATH=\"$HOME/.nodenv/bin:$PATH\"\neval \"$(nodenv init -)\"\nrbenv\nexport PATH=\"$HOME/.rbenv/bin:$PATH\"\neval \"$(rbenv init -)\"\nexport PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"\n. Works without `/bin/bash -lci` \npuma.service\n[Unit]\nDescription=Puma HTTP Server\nAfter=network.target\nRequires=puma.socket\n[Service]\nType=simple\nWorkingDirectory=/home/oleg/projects/github/puma_systemd_bug\nEnvironment=PUMA_DEBUG=1\nEnvironment=PATH=/home/oleg/.nodenv/shims/:/bin:/usr/bin\nExecStart=/home/oleg/.rbenv/shims/bundle exec puma --config config/puma.rb\nRestart=always\nUser=oleg\nGroup=users\n[Install]\nWantedBy=multi-user.target\n```\nthx @molfar . ",
    "utilityboy": "@L-Oto Unicorns do return this header as well.  I'm experiencing the same thing.\n. ",
    "syednasir": "We are having the same issue with Puma on Heroku.\n. ",
    "ostinelli": "\nSince our app is likely not thread safe, we've set WEB_CONCURRENCY to 1 and scaled the processes by setting WEB_CONCURRENCY to values ranging from 4 to 8. \n\n@plindelauf If your actually meant MAX_THREADS as your second entry here, I think you're seeing this upside down. \"Worker processes are isolated from one another at the OS level, therefore not needing to be thread safe\", while on the contrary \"Puma can serve each request in a thread from an internal thread pool\", and these needs to be thread safe (source).\nAnyhow, I'm experiencing the same one on Heroku. H12 when dynos are restarted. I'd be willing to offer details.\n. Thank you @rhymes, I'm already doing a custom implementation as described in this github issue, which is what that gem does anyway.\nThis does not solve my problem.\n. ",
    "fiftyandfifty": "sure - here is the puma request.env\n```\n{\n  \"success\":false,\n  \"error\":{\n    \"message\":\"No email address was provided, please include one.\",\n    \"messages\":[\n      \"No email address was provided, please include one.\"\n    ],\n    \"params\":{\n      \"rack.version\":[\n        1,\n        3\n      ],\n      \"rack.errors\":\"#\\u003cIO:0x007f40cbcb9c20\\u003e\",\n      \"rack.multithread\":true,\n      \"rack.multiprocess\":false,\n      \"rack.run_once\":false,\n      \"SCRIPT_NAME\":\"\",\n      \"CONTENT_TYPE\":\"text/plain\",\n      \"QUERY_STRING\":\"\",\n      \"SERVER_PROTOCOL\":\"HTTP/1.1\",\n      \"SERVER_SOFTWARE\":\"2.11.1\",\n      \"GATEWAY_INTERFACE\":\"CGI/1.2\",\n      \"REQUEST_METHOD\":\"POST\",\n      \"REQUEST_PATH\":\"/api/v1/accounts/131/process_without_auth\",\n      \"REQUEST_URI\":\"/api/v1/accounts/131/process_without_auth\",\n      \"HTTP_VERSION\":\"HTTP/1.1\",\n      \"HTTP_HOST\":\"www.example-staging.com\",\n      \"HTTP_CONNECTION\":\"close\",\n      \"HTTP_ACCEPT\":\"/\",\n      \"HTTP_ORIGIN\":\"https://www.example-staging.com\",\n      \"HTTP_ACCEPT_LANGUAGE\":\"en-US\",\n      \"HTTP_ACCEPT_ENCODING\":\"gzip, deflate\",\n      \"HTTP_USER_AGENT\":\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n      \"HTTP_CACHE_CONTROL\":\"no-cache\",\n      \"HTTP_X_REQUEST_ID\":\"ee6b81ee-c1d0-4df5-abad-168e6b6c2033\",\n      \"HTTP_X_FORWARDED_FOR\":\"70.166.93.205\",\n      \"HTTP_X_FORWARDED_PROTO\":\"https\",\n      \"HTTP_X_FORWARDED_PORT\":\"443\",\n      \"HTTP_VIA\":\"1.1 vegur\",\n      \"HTTP_CONNECT_TIME\":\"1\",\n      \"HTTP_X_REQUEST_START\":\"1429219388958\",\n      \"HTTP_TOTAL_ROUTE_TIME\":\"0\",\n      \"CONTENT_LENGTH\":\"383\",\n      \"SERVER_NAME\":\"www.example-staging.com\",\n      \"SERVER_PORT\":\"443\",\n      \"PATH_INFO\":\"/api/v1/accounts/131/process_without_auth\",\n      \"REMOTE_ADDR\":\"10.120.241.74\",\n      \"puma.socket\":\"#\\u003cTCPSocket:0x007f40d82948c8\\u003e\",\n      \"rack.hijack?\":true,\n      \"rack.hijack\":\"#\\u003cPuma::Client:0x007f40d82948a0\\u003e\",\n      \"rack.input\":\"#\\u003cStringIO:0x007f40d82a3c60\\u003e\",\n      \"rack.url_scheme\":\"http\",\n      \"rack.after_reply\":[\n  ],\n  \"puma.config\":\"#\\u003cPuma::Configuration:0x007f40cc3ad0b8\\u003e\",\n  \"newrelic.transaction_started\":true,\n  \"ORIGINAL_FULLPATH\":\"/api/v1/accounts/131/process_without_auth\",\n  \"ORIGINAL_SCRIPT_NAME\":\"\",\n  \"action_dispatch.routes\":\"#\\u003cActionDispatch::Routing::RouteSet:0x007f40d2421b38\\u003e\",\n  \"action_dispatch.parameter_filter\":[\n    \"password\",\n    \"credit_card\",\n    \"user\",\n    \"card_number\",\n    \"number\",\n    \"cvc\"\n  ],\n  \"action_dispatch.redirect_filter\":[\n\n  ],\n  \"action_dispatch.secret_token\":null,\n  \"action_dispatch.secret_key_base\":\"433ae41c69b7a5c8cc351752dd8da66d1468bd2d4b6fade7d96ac9ab410b9610912828c0008a0e3df41a4e92859136ade582e9a58708799a9ebb0e0730c2b592\",\n  \"action_dispatch.show_exceptions\":true,\n  \"action_dispatch.show_detailed_exceptions\":false,\n  \"action_dispatch.logger\":\"#\\u003cActiveSupport::Logger:0x007f40d2458598\\u003e\",\n  \"action_dispatch.backtrace_cleaner\":\"#\\u003cRails::BacktraceCleaner:0x007f40d62e7a48\\u003e\",\n  \"action_dispatch.key_generator\":\"#\\u003cActiveSupport::CachingKeyGenerator:0x007f40d0643648\\u003e\",\n  \"action_dispatch.http_auth_salt\":\"http authentication\",\n  \"action_dispatch.signed_cookie_salt\":\"signed cookie\",\n  \"action_dispatch.encrypted_cookie_salt\":\"encrypted cookie\",\n  \"action_dispatch.encrypted_signed_cookie_salt\":\"signed encrypted cookie\",\n  \"action_dispatch.cookies_serializer\":null,\n  \"action_dispatch.cookies_digest\":null,\n  \"ROUTES_69958191091100_SCRIPT_NAME\":\"\",\n  \"rack-timeout.info\":\"#\\u003cstruct Rack::Timeout::RequestDetails id=\\\"ee6b81ee-c1d0-4df5-abad-168e6b6c2033\\\", wait=0.38487488531311037, service=0.000655991, timeout=60, state=:active\\u003e\",\n  \"rack.cors\":\"#\\u003cRack::Cors::Result:0x007f40d82a1eb0\\u003e\",\n  \"rack-cache.cache_key\":\"Rack::Cache::Key\",\n  \"rack-cache.verbose\":true,\n  \"rack-cache.storage\":\"#\\u003cRack::Cache::Storage:0x007f40d1ee2730\\u003e\",\n  \"rack-cache.metastore\":\"#\\u003cDalli::Client:0x007f40d245b748\\u003e\",\n  \"rack-cache.entitystore\":\"#\\u003cDalli::Client:0x007f40d245b748\\u003e\",\n  \"rack-cache.default_ttl\":0,\n  \"rack-cache.ignore_headers\":[\n    \"Set-Cookie\"\n  ],\n  \"rack-cache.private_headers\":[\n    \"Authorization\",\n    \"Cookie\"\n  ],\n  \"rack-cache.allow_reload\":false,\n  \"rack-cache.allow_revalidate\":false,\n  \"rack-cache.use_native_ttl\":false,\n  \"action_dispatch.request_id\":\"ee6b81ee-c1d0-4df5-abad-168e6b6c2033\",\n  \"action_dispatch.remote_ip\":\"70.166.93.205\",\n  \"rack.session\":\"#\\u003cActionDispatch::Request::Session:0x007f40d82aefc0\\u003e\",\n  \"rack.session.options\":\"#\\u003cActionDispatch::Request::Session::Options:0x007f40d82aef70\\u003e\",\n  \"action_dispatch.request.content_type\":\"text/plain\",\n  \"warden\":\"Warden::Proxy:69958240662480 @config={:default_scope=\\u003e:default, :scope_defaults=\\u003e{}, :default_strategies=\\u003e{:_all=\\u003e[:api_token_strategy]}, :intercept_401=\\u003etrue, :failure_app=\\u003ePublicController}\",\n  \"action_dispatch.request.path_parameters\":{\n    \"controller\":\"api/v1/accounts\",\n    \"action\":\"process_without_auth\",\n    \"id\":\"131\"\n  },\n  \"action_controller.instance\":\"#\\u003cApi::V1::AccountsController:0x007f40d82ac450\\u003e\",\n  \"action_dispatch.request.request_parameters\":{\n\n  },\n  \"rack.request.query_string\":\"\",\n  \"rack.request.query_hash\":{\n\n  },\n  \"action_dispatch.request.query_parameters\":{\n\n  },\n  \"action_dispatch.request.parameters\":{\n    \"controller\":\"api/v1/accounts\",\n    \"action\":\"process_without_auth\",\n    \"id\":\"131\"\n  },\n  \"action_dispatch.request.accepts\":[\n    \"*/*\"\n  ],\n  \"action_dispatch.request.formats\":[\n    \"*/*\"\n  ]\n}\n\n}\n}\n```\nand here is thin\n```\n{\n  \"SERVER_SOFTWARE\"  =>\"thin 1.6.3 codename Protein Powder\",\n  \"SERVER_NAME\"  =>\"www.example.local\",\n  \"rack.input\"  =>#,\n  \"rack.version\"  =>  [\n    1,\n    0\n  ],\n  \"rack.errors\"  =>#>,\n  \"rack.multithread\"  =>false,\n  \"rack.multiprocess\"  =>false,\n  \"rack.run_once\"  =>false,\n  \"REQUEST_METHOD\"  =>\"POST\",\n  \"REQUEST_PATH\"  =>\"/api/v1/accounts/131/process_without_auth\",\n  \"PATH_INFO\"  =>\"/api/v1/accounts/131/process_without_auth\",\n  \"REQUEST_URI\"  =>\"/api/v1/accounts/131/process_without_auth\",\n  \"HTTP_VERSION\"  =>\"HTTP/1.1\",\n  \"HTTP_ACCEPT\"  =>\"/\",\n  \"HTTP_ORIGIN\"  =>\"http:  //www.example.local:3000  \", \"  HTTP_ACCEPT_LANGUAGE\"=>\"en-US\",\n  \"HTTP_ACCEPT_ENCODING\"  =>\"gzip,\n  deflate\",\n  \"HTTP_USER_AGENT\"  =>\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)\",\n  \"HTTP_HOST\"  =>\"www.example.local:3000  \", \"  HTTP_CONNECTION\"=>\"Keep-Alive\",\n  \"HTTP_CACHE_CONTROL\"  =>\"no-cache\",\n  \"CONTENT_LENGTH\"  =>\"383\",\n  \"GATEWAY_INTERFACE\"  =>\"CGI/1.2\",\n  \"SERVER_PORT\"  =>\"3000\",\n  \"QUERY_STRING\"  =>\"\",\n  \"SERVER_PROTOCOL\"  =>\"HTTP/1.1\",\n  \"rack.url_scheme\"  =>\"http\",\n  \"SCRIPT_NAME\"  =>\"\",\n  \"REMOTE_ADDR\"  =>\"127.0.0.1\",\n  \"async.callback\"  =>#,\n  \"async.close\"  =>#,\n  \"ORIGINAL_FULLPATH\"  =>\"/api/v1/accounts/131/process_without_auth\",\n  \"ORIGINAL_SCRIPT_NAME\"  =>\"\",\n  \"action_dispatch.routes\"  =>#,\n  \"action_dispatch.parameter_filter\"  =>  [\n:password,\n:credit_card,\n:user,\n:card_number,\n:number,\n:cvc\n  ],\n  \"action_dispatch.redirect_filter\"  =>  [\n],\n  \"action_dispatch.secret_token\"  =>nil,\n  \"action_dispatch.secret_key_base\"  =>\"433ae41c69b7a5c8cc351752dd8da66d1468bd2d4b6fade7d96ac9ab410b9610912828c0008a0e3df41a4e92859136ade582e9a58708799a9ebb0e0730c2b592\",\n  \"action_dispatch.show_exceptions\"  =>true,\n  \"action_dispatch.show_detailed_exceptions\"  =>true,\n  \"action_dispatch.logger\"  =>#,\n  @formatter=#,\n  @logdev=#,\n  @mutex=#>>>,\n  \"action_dispatch.backtrace_cleaner\"  =>#,\n    #,\n    #,\n    #\n  ],\n  @silencers=  [\n    #\n  ],\n  @root=\"/Users/bryanexample/Sites/example.com/example_app/\">,\n  \"action_dispatch.key_generator\"  =>#,\n  @cache_keys=#\"\\xE5\\xBF6\\xC5P\\xDD\\xC4\\x8C    [\n      \\xF0\\xDF\\v\\xED\\x95\\x8Dr\\x05\\xBE_i\\x94\\xAE0|\\xEE`\\xCA\\xD2\\xA7\\xB1 \\x99\\xC8\\xA6\\xE7\\x8D\\x93P\\x01\\xFD\\xFEIv\\xE9\\xDF\\xD6\\xB5K4\\xE9\\xE0%\\x88\\xBEt\\x16\\d\\x8F\\x9A\\x9A\\x8F\\xBFe\"\n    },\n    @default_proc=nil>>,\n    \"action_dispatch.http_auth_salt\"    =>\"http authentication\",\n    \"action_dispatch.signed_cookie_salt\"    =>\"signed cookie\",\n    \"action_dispatch.encrypted_cookie_salt\"    =>\"encrypted cookie\",\n    \"action_dispatch.encrypted_signed_cookie_salt\"    =>\"signed encrypted cookie\",\n    \"action_dispatch.cookies_serializer\"    =>nil,\n    \"action_dispatch.cookies_digest\"    =>nil,\n    \"ROUTES_70168739948220_SCRIPT_NAME\"    =>\"\",\n    \"rack.cors\"    =>#,\n    \"rack.request.form_hash\"    =>    {\n      \"campaign_id\"      =>\"0\",\n      \"fundraiser_id\"      =>\"0\",\n      \"dont_send_receipt_email\"      =>\"false\",\n      \"first_name\"      =>\"\",\n      \"last_name\"      =>\"\",\n      \"email\"      =>\"bryan@example.com\",\n      \"amount_in_cents\"      =>\"100\",\n      \"recurring\"      =>\"false\",\n      \"phone_number\"      =>\"\",\n      \"street_address\"      =>\"\",\n      \"street_address_2\"      =>\"\",\n      \"city\"      =>\"\",\n      \"state\"      =>\"\",\n      \"zip_code\"      =>\"\",\n      \"country\"      =>\"\",\n      \"comment\"      =>\"\",\n      \"on_behalf_of\"      =>\"\",\n      \"anonymous\"      =>\"false\",\n      \"dump\"      =>\"\",\n      \"referrer_id\"      =>\"\",\n      \"card\"      =>      {\n        \"number\"        =>\"4242 4242 4242 4242\",\n        \"exp_month\"        =>\"12\",\n        \"exp_year\"        =>\"14\",\n        \"cvc\"        =>\"123\"\n      }\n    },\n    \"rack.request.form_vars\"    =>\"campaign_id=0&fundraiser_id=0&dont_send_receipt_email=false&first_name=&last_name=&email=bryan%40example.com&amount_in_cents=100&recurring=false&phone_number=&street_address=&street_address_2=&city=&state=&zip_code=&country=&comment=&on_behalf_of=&anonymous=false&dump=&referrer_id=&card%5Bnumber%5D=4242+4242+4242+4242&card%5Bexp_month%5D=12&card%5Bexp_year%5D=14&card%5Bcvc%5D=123\",\n    \"rack.request.form_input\"    =>#,\n    \"action_dispatch.request_id\"    =>\"9c36c90f-a443-4e50-bd09-98a3c88ae9e8\",\n    \"action_dispatch.remote_ip\"    =>#,\n      #,\n      #,\n      #,\n      #,\n      #\n    ],\n    @ip=\"127.0.0.1\">,\n    \"rack.session\"    =>#,\n    \"rack.session.options\"    =>#:default,\n:scope_defaults=>      {\n  },\n\n:default_strategies=>      {\n:_all=>        [\n:api_token_strategy\n        ]\n      },\n:intercept_401=>true,\n:failure_app=>PublicController\n    },\n    @app=#>,\n    @cache_control=\"max-age=0,\n    private,\n    must-revalidate\",\n    @no_cache_control=\"no-cache\">>>,\n    @parsers=    {\n      #=>:json\n    }    >>,\n    @default_options=    {\n:path=>\"/\",\n:      domain=>:all,\n:expire_after=>nil,\n:secure=>false,\n:httponly=>true,\n:defer=>false,\n:renew=>false\n    },\n    @key=\"_example.com_session\",\n    @cookie_only=true>,\n    @env=    {\n      ...\n    },\n    @delegate=    {\n:path=>\"/\",\n:      domain=>:all,\n:expire_after=>nil,\n:secure=>false,\n:httponly=>true,\n:defer=>false,\n:renew=>false\n    }    >,\n    \"action_dispatch.request.content_type\"    =>nil,\n    \"warden\"    =>Warden::    Proxy:70168817508160    @config=    {\n:      default_scope=>:default,\n:scope_defaults=>      {\n  },\n\n:default_strategies=>      {\n:_all=>        [\n:api_token_strategy\n        ]\n      },\n:intercept_401=>true,\n:failure_app=>PublicController\n    },\n    \"action_dispatch.request.path_parameters\"    =>    {\n:controller=>\"api/v1/accounts\",\n:action=>\"process_without_auth\",\n:id=>\"131\"\n    },\n    \"action_controller.instance\"    =>#\"text/html\"\n    },\n    @_status=200,\n    @_request=#\"0\",\n      \"fundraiser_id\"      =>\"0\",\n      \"dont_send_receipt_email\"      =>\"false\",\n      \"first_name\"      =>\"\",\n      \"last_name\"      =>\"\",\n      \"email\"      =>\"bryan@example.com\",\n      \"amount_in_cents\"      =>\"100\",\n      \"recurring\"      =>\"false\",\n      \"phone_number\"      =>\"      [\n        FILTERED\n      ]      \", \"      street_address\"=>\"\",\n      \"street_address_2\"      =>\"\",\n      \"city\"      =>\"\",\n      \"state\"      =>\"\",\n      \"zip_code\"      =>\"\",\n      \"country\"      =>\"\",\n      \"comment\"      =>\"\",\n      \"on_behalf_of\"      =>\"\",\n      \"anonymous\"      =>\"false\",\n      \"dump\"      =>\"\",\n      \"referrer_id\"      =>\"\",\n      \"card\"      =>      {\n        \"number\"        =>\"        [\n          FILTERED\n        ]        \", \"        exp_month\"=>\"12\",\n        \"exp_year\"        =>\"14\",\n        \"cvc\"        =>\"        [\n          FILTERED\n        ]        \"}, \"        controller\"=>\"api/v1/accounts\",\n        \"action\"        =>\"process_without_auth\",\n        \"id\"        =>\"131\"\n      },\n      @filtered_env=nil,\n      @filtered_path=nil,\n      @protocol=nil,\n      @port=nil,\n      @method=nil,\n      @request_method=\"POST\",\n      @remote_ip=\"127.0.0.1\",\n      @original_fullpath=nil,\n      @fullpath=\"/api/v1/accounts/131/process_without_auth\",\n      @ip=nil,\n      @uuid=nil>,\n      @_response=#,\n      @stream=#,\n      @buf=      [\n  ],\n  @closed=false>,\n  @header=      {\n    \"X-Frame-Options\"        =>\"SAMEORIGIN\",\n    \"X-XSS-Protection\"        =>\"1; mode=block\",\n    \"X-Content-Type-Options\"        =>\"nosniff\"\n  },\n  @status=200,\n  @sending_file=false,\n  @blank=false,\n  @cv=#<MonitorMixin::      ConditionVariable:0      x007fa2e6ea3608 @monitor=#<ActionDispatch::      Response:0      x007fa2e6ea3770 ...>,\n  @cond=#<Thread::      ConditionVariable:0      x007fa2e6ea35e0>>,\n  @committed=false,\n  @sending=false,\n  @sent=false,\n  @content_type=nil,\n  @charset=nil,\n  @cache_control=      {\n\n  },\n  @etag=nil,\n  @request=#<ActionDispatch::      Request:0      x007fa2e6ea3798 @env=      {\n    ...\n  },\n  @filtered_parameters=      {\n    \"campaign_id\"        =>\"0\",\n    \"fundraiser_id\"        =>\"0\",\n    \"dont_send_receipt_email\"        =>\"false\",\n    \"first_name\"        =>\"\",\n    \"last_name\"        =>\"\",\n    \"email\"        =>\"bryan@example.com\",\n    \"amount_in_cents\"        =>\"100\",\n    \"recurring\"        =>\"false\",\n    \"phone_number\"        =>\"        [\n      FILTERED\n    ]        \", \"        street_address\"=>\"\",\n    \"street_address_2\"        =>\"\",\n    \"city\"        =>\"\",\n    \"state\"        =>\"\",\n    \"zip_code\"        =>\"\",\n    \"country\"        =>\"\",\n    \"comment\"        =>\"\",\n    \"on_behalf_of\"        =>\"\",\n    \"anonymous\"        =>\"false\",\n    \"dump\"        =>\"\",\n    \"referrer_id\"        =>\"\",\n    \"card\"        =>        {\n      \"number\"          =>\"          [\n        FILTERED\n      ]          \", \"          exp_month\"=>\"12\",\n      \"exp_year\"          =>\"14\",\n      \"cvc\"          =>\"          [\n        FILTERED\n      ]          \"}, \"          controller\"=>\"api/v1/accounts\",\n      \"action\"          =>\"process_without_auth\",\n      \"id\"          =>\"131\"\n    },\n    @filtered_env=nil,\n    @filtered_path=nil,\n    @protocol=nil,\n    @port=nil,\n    @method=nil,\n    @request_method=\"POST\",\n    @remote_ip=\"127.0.0.1\",\n    @original_fullpath=nil,\n    @fullpath=\"/api/v1/accounts/131/process_without_auth\",\n    @ip=nil,\n    @uuid=nil>>,\n    @_env=        {\n      ...\n    },\n    @_lookup_context=#<ActionView::        LookupContext:0        x007fa2e6ea3388 @details_key=nil,\n    @details=        {\n\n:locale=>          [\n:en\n          ],\n:formats=>          [\n:html,\n:text,\n:js,\n:css,\n:ics,\n:csv,\n:vcf,\n:png,\n:jpeg,\n:gif,\n:bmp,\n:tiff,\n:mpeg,\n:xml,\n:rss,\n:atom,\n:yaml,\n:multipart_form,\n:url_encoded_form,\n:json,\n:pdf,\n:zip\n          ],\n:variants=>          [\n      ],\n\n:handlers=>          [\n:erb,\n:builder,\n:raw,\n:ruby,\n:jsonify,\n:coffee,\n:haml\n          ]\n        },\n        @skip_default_locale=false,\n        @cache=true,\n        @prefixes=        [\n          \"api/v1/accounts\",\n          \"api/v1/base\"\n        ],\n        @rendered_format=nil,\n        @view_paths=#<ActionView::        PathSet:0        x007fa2e6ea3310 @paths=        [\n          #<ActionView::          OptimizedFileSystemResolver:0          x007fa2dda573c8 @pattern=\":          prefix/:action          {\n            .:locale,\n      }          {\n        .:formats,\n\n      }          {\n        +:variants,\n\n      }          {\n        .:handlers,\n\n      }          \", @cache=#<ActionView::Resolver::Cache:0x007fa2dda573a0 @data=#<ActionView::Resolver::Cache::SmallCache:0x007fa2dda57378 @backend={#<ActionView::LookupContext::DetailsKey:0x007fa2e80bfad0 @hash=-1549977740415523038>=>#<ActionView::Resolver::Cache::SmallCache:0x007fa2e80beba8 @backend={\"          show\"=>#<ActionView::          Resolver::          Cache::          SmallCache:0          x007fa2e80beae0 @backend=          {\n        \"js/v1/form_inners\"            =>#<ActionView::            Resolver::            Cache::            SmallCache:0            x007fa2e80be9f0 @backend=            {\n          false              =>#<ActionView::              Resolver::              Cache::              SmallCache:0              x007fa2e80be900 @backend=              {\n            [\n\n            ]                =>                [\n              app/views/js/v1/form_inners/show.haml\n            ]\n          },\n          @default_proc=nil>\n        },\n        @default_proc=#<Proc:0            x007fa2db418090@/Users/bryanexample/.rvm/gems/ruby-2.1.2@exampleapp.com/gems/actionview-4.2.0/lib/action_view/template/resolver.rb:46            (lambda)>>\n      },\n      @default_proc=#<Proc:0          x007fa2db418040@/Users/bryanexample/.rvm/gems/ruby-2.1.2@exampleapp.com/gems/actionview-4.2.0/lib/action_view/template/resolver.rb:47          (lambda)>>\n    },\n    @default_proc=#<Proc:0        x007fa2db418018@/Users/bryanexample/.rvm/gems/ruby-2.1.2@exampleapp.com/gems/actionview-4.2.0/lib/action_view/template/resolver.rb:48        (lambda)>>\n  },\n  @default_proc=#<Proc:0      x007fa2db418c48@/Users/bryanexample/.rvm/gems/ruby-2.1.2@exampleapp.com/gems/actionview-4.2.0/lib/action_view/template/resolver.rb:49      (lambda)>>>,\n  @path=\"/Users/bryanexample/Sites/example.com/example_app/app/views\">,\n  #<ActionView::      OptimizedFileSystemResolver:0      x007fa2dda4e390 @pattern=\":      prefix/:action      {\n    .:locale,\n\n  }      {\n    .:formats,\n\n  }      {\n    +:variants,\n\n  }      {\n    .:handlers,\n\n  }      \", @cache=#<ActionView::Resolver::Cache:0x007fa2dda4e368 @data=#<ActionView::Resolver::Cache::SmallCache:0x007fa2dda4e2a0 @backend={}, @default_proc=#<Proc:0x007fa2db418c48@/Users/bryanexample/.rvm/gems/ruby-2.1.2@exampleapp.com/gems/actionview-4.2.0/lib/action_view/template/resolver.rb:49 (lambda)>>>, @path=\"      /Users/bryanexample/.rvm/gems/ruby-2.1.2@exampleapp.com/gems/twitter-bootstrap-rails-3.2.0/app/views\">\n]    >>,\n@_action_name=\"process_without_auth\",\n@_response_body=nil,\n@current_account=nil,\n@_params=    {\n  \"campaign_id\"      =>\"0\",\n  \"fundraiser_id\"      =>\"0\",\n  \"dont_send_receipt_email\"      =>\"false\",\n  \"first_name\"      =>\"\",\n  \"last_name\"      =>\"\",\n  \"email\"      =>\"bryan@example.com\",\n  \"amount_in_cents\"      =>\"100\",\n  \"recurring\"      =>\"false\",\n  \"phone_number\"      =>\"\",\n  \"street_address\"      =>\"\",\n  \"street_address_2\"      =>\"\",\n  \"city\"      =>\"\",\n  \"state\"      =>\"\",\n  \"zip_code\"      =>\"\",\n  \"country\"      =>\"\",\n  \"comment\"      =>\"\",\n  \"on_behalf_of\"      =>\"\",\n  \"anonymous\"      =>\"false\",\n  \"dump\"      =>\"\",\n  \"referrer_id\"      =>\"\",\n  \"card\"      =>      {\n    \"number\"        =>\"4242 4242 4242 4242\",\n    \"exp_month\"        =>\"12\",\n    \"exp_year\"        =>\"14\",\n    \"cvc\"        =>\"123\"\n  },\n  \"controller\"      =>\"api/v1/accounts\",\n  \"action\"      =>\"process_without_auth\",\n  \"id\"      =>\"131\"\n}    >,\n\"action_dispatch.request.request_parameters\"    =>    {\n  \"campaign_id\"      =>\"0\",\n  \"fundraiser_id\"      =>\"0\",\n  \"dont_send_receipt_email\"      =>\"false\",\n  \"first_name\"      =>\"\",\n  \"last_name\"      =>\"\",\n  \"email\"      =>\"bryan@example.com\",\n  \"amount_in_cents\"      =>\"100\",\n  \"recurring\"      =>\"false\",\n  \"phone_number\"      =>\"\",\n  \"street_address\"      =>\"\",\n  \"street_address_2\"      =>\"\",\n  \"city\"      =>\"\",\n  \"state\"      =>\"\",\n  \"zip_code\"      =>\"\",\n  \"country\"      =>\"\",\n  \"comment\"      =>\"\",\n  \"on_behalf_of\"      =>\"\",\n  \"anonymous\"      =>\"false\",\n  \"dump\"      =>\"\",\n  \"referrer_id\"      =>\"\",\n  \"card\"      =>      {\n    \"number\"        =>\"4242 4242 4242 4242\",\n    \"exp_month\"        =>\"12\",\n    \"exp_year\"        =>\"14\",\n    \"cvc\"        =>\"123\"\n  }\n},\n\"rack.request.query_string\"    =>\"\",\n\"rack.request.query_hash\"    =>    {\n\n},\n\"action_dispatch.request.query_parameters\"    =>    {\n\n},\n\"action_dispatch.request.parameters\"    =>    {\n  \"campaign_id\"      =>\"0\",\n  \"fundraiser_id\"      =>\"0\",\n  \"dont_send_receipt_email\"      =>\"false\",\n  \"first_name\"      =>\"\",\n  \"last_name\"      =>\"\",\n  \"email\"      =>\"bryan@example.com\",\n  \"amount_in_cents\"      =>\"100\",\n  \"recurring\"      =>\"false\",\n  \"phone_number\"      =>\"\",\n  \"street_address\"      =>\"\",\n  \"street_address_2\"      =>\"\",\n  \"city\"      =>\"\",\n  \"state\"      =>\"\",\n  \"zip_code\"      =>\"\",\n  \"country\"      =>\"\",\n  \"comment\"      =>\"\",\n  \"on_behalf_of\"      =>\"\",\n  \"anonymous\"      =>\"false\",\n  \"dump\"      =>\"\",\n  \"referrer_id\"      =>\"\",\n  \"card\"      =>      {\n    \"number\"        =>\"4242 4242 4242 4242\",\n    \"exp_month\"        =>\"12\",\n    \"exp_year\"        =>\"14\",\n    \"cvc\"        =>\"123\"\n  },\n  \"controller\"      =>\"api/v1/accounts\",\n  \"action\"      =>\"process_without_auth\",\n  \"id\"      =>\"131\"\n},\n\"action_dispatch.request.accepts\"    =>    [\n  #<Mime::      Type:0      x007fa2e6ec9b00 @synonyms=      [\n\n  ],\n  @symbol=nil,\n  @string=\"*/*\">\n],\n\"action_dispatch.request.formats\"    =>    [\n  #<Mime::      Type:0      x007fa2e6ec9b00 @synonyms=      [\n\n  ],\n  @symbol=nil,\n  @string=\"*/*\">\n]\n\n}\n```\n. ",
    "shanaver": "@evanphx - dunno why the output is different, I generated them the same way, with the request.env hash.\nCan you think of a setting that would affect the POST body?\n. ",
    "fatroller": "Hi evan,\nI planning on deploying my app on Heroku - so will let you know how I go. But at the moment my dev environment is Windows... Without SSL, Puma works fine on windows...\nIs there a way to debug what really is happening? At the moment there are no errors in the logs and it seems that the browser is unable to connect to the application when SSL is configured... \n. ",
    "krzcho": "I need SSL on Windows and pulling my hair to make it work\nI would first need some guidelines how to compile puma with openssl\nI was trying with instructions from here https://github.com/hicknhack-software/rails-disco/wiki/Installing-puma-on-windows but I can't make compiler to see openssl libs :(\n. ok some hair left untouched after moving from ruby193 to 21 with fresh devkit\npuma with openssl is compiled and installed but i do have the same result:\npuma launches, starts to listen on specified port but fails to serve any request\nadditionally exactly after 2 mins of time after first request it stops listening on the port\n. ",
    "rob-murray": "Shortest PR ever.\n. ",
    "jefflab": "Yep, exactly. I just wasn't sure if this should be done in Puma or downstream (Rack?)\n. I haven't had a chance to get the request.headers for both. However, I'm pretty sure I remember the problem had something to do with this line of binder.rb\nhttps://github.com/puma/puma/blob/master/lib/puma/binder.rb#L21-L24\n```\nRack blows up if this is an empty string, and Rack::Lint\nblows up if it's nil. So 'text/plain' seems like the most\nsensible default value.\n\"CONTENT_TYPE\".freeze => \"text/plain\",\n```\nBecause the Content-Type is being set to \"text/plain\", the POST params are not being converted to a hash in Rails. I think the default for a POST should be application/x-www-form-urlencoded in Puma.\nMy suspicion is that Unicorn doesn't set the Content-Type (as you discovered). As a result, Rails chooses to interpret POST params has a hash.\nIf this isn't a sufficient clue, and you want me to get the request.headers, please reply and I'll try to get that for you.\n. Great!\n. ",
    "spk": "Is the build error normal? It seams it just stop?\n. There is a discussion here #454 \n. This might be because Debian sid is currently moving to libssl1.1 and mini_ssl only support libssl1.0 ABI\n. ",
    "luxx": "Can someone release the gem please?\n. ",
    "deees": "@evanphx any plans to release v2.11.4 ? :smile: \n. ",
    "denji": ":+1: \n. ",
    "ramonsnir": "Don't merge yet, we suspect a bug here. Will update.\n. Enabling and disabling this patched correlates very well to segmentation faults in lib/puma/server.rb. I'm closing this PR, but I think this feature still deserves a resolution.\n. ",
    "OleMchls": "I'm not completely sure what is killing the threads, from my educated guess, it's the fact that the exception is raised from another thread and thus bubbling up and killing the thread.\nAll the issues we've linked above and also our initial problem we've faced is that heroku recently switched their recommended web server from unicorn to puma (ref), but due to the fact that puma does not have a request timeout rack-timeout is recommended. And from what I've seen in our app as well in a bunch of issues all over different ruby repos (rails/puma/rack-timeout) is that this combination is not working pretty well, I'm not sure why this happens exactly. But it seems like the ActiveRecord connection pool is having a lot of trouble with that as we as the puma thread pool. From our investigations all the following reported problems are caused by a combination of rack-timeout not playing well with puma. \n- https://github.com/puma/puma/issues/691\n- https://github.com/puma/puma/issues/681\n- https://github.com/heroku/rack-timeout/issues/76\n- https://github.com/rails/rails/issues/1627#issuecomment-76172622\nMaybe pulling in @schneems and @kch might be a good idea.\n. @evanphx we are wondering what do you exactly mean with top level?\n. @kch I agree on the point that dead threads should be reaped, that is why we proposed this PR. On the other hand, could you maybe (in the rack-timeout repo) a bit more transparent on what kind a changes you are working and if you might need help? Happy to help :) Because of the issue mentioned we now switched back to unicorn.\n. ",
    "kch": "rack-timeout is kind of terrible and currently doesn't play well with puma, I'm working on a bunch of changes to it, which should remedy that.\nBut it seems to me threads could die for a number of valid reasons and puma should reap them when that happens.\n. ",
    "knightq": "+1 for merge and release\n. We'll switch back to unicorn too until this will be resolved: it's source of dangerous raising in request queueing during high request burst periods.\n. ",
    "sgranata82": "+1\n. ",
    "SebastianEdwards": "+1\n. ",
    "acrolink": "I had encountered the issue of request time-outs when I was testing my app using httperf tool (conns 500, rate 50, timeout 10). Puma failed badly,out of 500 requests: 181 status 200 OK and 319 time-out.\nAny new updates on this?\n. @emerleite .. If you don't mind trying a new framework I highy recommend Phoenix (upon Elixir) .. It is super fast compared to Ruby's Rails (5 or 6 times faster). As for Puma, I have kept using Unicorn for the meantime.\n. Thanks @evanphx \n. ",
    "emerleite": "@acrolink same here. Sometimes I have only one thread working :(\n. We're currently doing that, but the Rails app must be running for now :)\n. ",
    "ebeigarts": "Same issue here, locking puma and rack to specific versions as a workaround, so that bundle update doesn't try to update them.\ngem 'puma', '2.11.3', require: false\ngem 'rack', '1.5.3' # https://github.com/puma/puma/issues/705\n. ",
    "evmorov": "\nSorry, how do I resolve this problem?\n\nAdd require 'bundler/setup' to your config/puma.rb. ",
    "trekr5": "Hi Nathan,\nI've created two dashbaords both powered by puma servers on the same linux box.\nThese dashboards are stored in different locations on the same box and the start puma command for each dashboard run from these directories.\nThis is my config.ru file for both:-\nconfig.ru\nrequire 'dashing'\nconfigure do\nset :auth_token, 'YOUR_AUTH_TOKEN'\n  set :bind, '0.0.0.0'\n    helpers do\n    def protected!\n     # Put any authentication code you want in here.\n     # This method is run before accessing any resource.\n    end\n  end\nend\nmap Sinatra::Application.assets_prefix do\n  run Sinatra::Application.sprockets\nend\nand my command to start puma server 1 is:-\n\"bundle exec puma -d config.ru tcp://0.0.0.0:9292 --pidfile /var/run/puma.pid\" \nwith pid stored as /var/run/puma.id\nMy command to start puma server 2 is:\nbundle exec puma -d config.ru tcp://0.0.0.0:3030 --pidfile /var/run/puma-3030.pid\nwith the pid stored as /var/run/puma-3030.pid\n. The two dashboards are Sinatra framework running with a Puma server. Don't have any puma logs though..\nJust tried to see if I could generate puma logs when running a puma server but it seems as if nothing is written to the puma log file. Maybe because it is a rack app?\n. ",
    "julik": ":heart: \n. Sorry messed up a commit, will reopen\n. ",
    "nwshane": "This error occurs when I first stop and then start puma (as part of my deploy task). Does that imply to you that the puma stop command isn't ending the process in time for puma start to work? \nIf the command pumactl -F <config_path> stop is immediately followed by puma -q -d -e staging -C <config_path>, do they run asynchronously? Or will the stop command complete before puma begins?\n. @alfie-max As far as I remember, I think the issue might have been that instead of calling stop and then start separately, you need to run a restart command: https://github.com/puma/puma/blob/master/docs/restart.md. Shoot, is it possible to delete an issue? Created this one in the wrong repository by mistake.\n. ",
    "Rttearth": "I also have the same problem with @nwshane. It seems like puma.sock & puma.state is not cleaned when invoke \"restart\". Previously it was a bug, but @evanphx said it has been fixed after version 2.3.0. The condition is that I still have the problem when deploying. I doubt whether this cause from deploy.rb file. Still waiting for solutions.\n. ",
    "kannancet": "This issue occurs while deploying with capistrano. What could be the reason ? Any idea\n. ",
    "alfie-max": "no fixes?? Still facing same issue in 2018\nIt happens to me when stop & start called from mina script executed from codeship.\nBut when i deploy directly from my local machine everything works. @nwshane  i tried that, but then the server didn't even start. got a 502 at ngnix\nSeems like puma stop doesn't stop it in time. \ni'm using mina for deployment and calling puma:stop and puma:start\nthis is the log : \n-----> Using RVM environment \"ruby-2.4.1\"\n       Using /home/ubuntu/.rvm/gems/ruby-2.4.1\n-----> Stopping Puma...\n       Command stop sent success\n-----> Starting Puma...\n       [30986] Puma starting in cluster mode...\n       [30986] * Version 3.9.1 (ruby 2.4.1-p111), codename: Private Caller\n       [30986] * Min threads: 2, max threads: 2\n       [30986] * Environment: production\n       [30986] * Process workers: 1\n       [30986] * Phased restart available\n       [30986] * Listening on unix:///home/ubuntu/site_home/shared/tmp/sockets/puma.sock\n       bundler: failed to load command: puma (/home/ubuntu/site_home/releases/4/vendor/bundle/ruby/2.4.0/bin/puma)\n       RuntimeError: There is already a server bound to: /home/ubuntu/site_home/shared/tmp/sockets/puma.sock\nSo the stop command is sent successfully.. but it doesn't stop immediately.\nRight now i've to run the deploy script twice. or stop the puma manually and run deploy.. ",
    "milesmatthias": "Going to close this issue since I've since migrated from Nginx + Puma to Apache + Passenger and can't debug this situation. If someone else comes along with similar issues, feel free to re-open.\n. ",
    "jeremy": "10.11 removes /usr/include/openssl for good.\n. To make this sticky for all your Gemfiles:\nsh\nbundle config build.puma --with-opt-dir=/usr/local/opt/openssl\nConfirm:\nsh\n$ grep PUMA ~/.bundle/config \nBUNDLE_BUILD__PUMA: \"--with-opt-dir=/usr/local/opt/openssl\"\n. ",
    "ryanbrink": "~~Is there a simple workaround I can use for now?~~\nFigured it out. It's not in the 10.11 SDK but is in the 10.10 SDK. Appending the --with-opt-include option seems to work. For example:\ngem install puma -v '2.8.2' -- --with-opt-include=\"/Applications/Xcode-beta.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.10.sdk/usr/include/\"\n. ",
    "robertjpayne": "If you are installing home-brew openssl it's a lot easier:\n`gem install puma -- --with-opt-include=/usr/local/opt/openssl/include``\n@evanphx is there any way to specify a default opt-include path? Seems like you could state that openssl via homebrew is a requirement for installing it on OS X or the user must manually manage it.\n. ",
    "MxBird": "On El Capitan, @robertjpayne's answer works.  many gem could install success in this way\n. ",
    "betoharres": "@robertjpayne worked to me as well ^^ thanks!\n. ",
    "jonnyarnold": ":+1: for @jeremy's comment\n. ",
    "jeroenvisser101": "@jeremy, that's great!\n. ",
    "jomarquez": ":+1: thank you @jeremy and @robertjpayne\n. ",
    "vkhang55": ":+1: Jeremy\n. ",
    "kocodude": "awesome, thanks @jeremy and @robertjpayne \n. ",
    "chischaschos": "thank you @jeremy and @robertjpayne \n. ",
    "enderahmetyurt": "cheers @jeremy \n. ",
    "rwz": "True. Although, I was expecting explicit arguments to take precedence here.\n. yay. thanks!\n. ",
    "hbin": "Same issue. \nIt seems like something wrong with prune_bundler. The latest capistrano-puma switch puma_preload_app from true to flase https://github.com/seuros/capistrano-puma/commit/013bb17ce0c79bfd3e40bf9648ddb819d9f6e58b \nTry to set puma_preload_app to true, and it will works again.\n. @evanphx I pasted both configuration files that work and doesn't work below:\nWork\n```\n!/usr/bin/env puma\ndirectory '/home/deploy/apps/sample_app_production/current'\nrackup \"/home/deploy/apps/sample_app_production/current/config.ru\"\nenvironment 'production'\npidfile \"/home/deploy/apps/sample_app_production/shared/tmp/pids/puma.pid\"\nstate_path \"/home/deploy/apps/sample_app_production/shared/tmp/pids/puma.state\"\nstdout_redirect '/home/deploy/apps/sample_app_production/shared/log/puma_access.log', '/home/deploy/apps/sample_app_production/shared/log/puma_error.log', true\nthreads 0,16\nbind 'unix:///home/deploy/apps/sample_app_production/shared/tmp/sockets/puma.sock'\nworkers 2\npreload_app!\non_restart do\n  puts 'Refreshing Gemfile'\n  ENV[\"BUNDLE_GEMFILE\"] = \"/home/deploy/apps/sample_app_production/current/Gemfile\"\nend\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\n```\n\nDoesn't Work\n```\n!/usr/bin/env puma\ndirectory '/home/deploy/apps/sample_app_production/current'\nrackup \"/home/deploy/apps/sample_app_production/current/config.ru\"\nenvironment 'production'\npidfile \"/home/deploy/apps/sample_app_production/shared/tmp/pids/puma.pid\"\nstate_path \"/home/deploy/apps/sample_app_production/shared/tmp/pids/puma.state\"\nstdout_redirect '/home/deploy/apps/sample_app_production/shared/log/puma_access.log', '/home/deploy/apps/sample_app_production/shared/log/puma_error.log', true\nthreads 0,16\nbind 'unix:///home/deploy/apps/sample_app_production/shared/tmp/sockets/puma.sock'\nworkers 2\nprune_bundler\non_restart do\n  puts 'Refreshing Gemfile'\n  ENV[\"BUNDLE_GEMFILE\"] = \"/home/deploy/apps/sample_app_production/current/Gemfile\"\nend\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\n```\nThe error message from puma_error.log file\n==> log/puma_error.log <==\n/home/deploy/apps/sample_app_production/shared/puma.rb:30:in `block in _load_from': uninitialized constant Puma::DSL::ActiveSupport (NameError)\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:211:in `call'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:211:in `block in worker'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:211:in `each'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:211:in `worker'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:109:in `block (2 levels) in spawn_workers'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:109:in `fork'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:109:in `block in spawn_workers'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:105:in `times'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:105:in `spawn_workers'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:157:in `check_workers'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cluster.rb:421:in `run'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/lib/puma/cli.rb:216:in `run'\n    from /home/deploy/apps/sample_app_production/shared/bundle/ruby/2.2.0/gems/puma-2.11.3/bin/puma-wild:31:in `<main>'\nI deploy this app on Ubuntu 14.04 LTS running in the VirtualBox 4.3.28 supervised by Vagrant.\nHope these message could help you.\n. ",
    "kamok": "I got this issue after deploying my app using Puma for the very first time.\nHere's my config file: \n```\nif Rails.env.production?\n  #Can't use this on Windows...\n  workers Integer(ENV['WEB_CONCURRENCY'] || 2)\nend\nthreads_count = Integer(ENV['RAILS_MAX_THREADS'] || 5)\nthreads threads_count, threads_count\npreload_app!\nrackup      DefaultRackup\nport        ENV['PORT']     || 3000\nenvironment ENV['RACK_ENV'] || 'development'\non_worker_boot do\n  # Worker specific setup for Rails 4.1+\n  # See: https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server#on-worker-boot\n  ActiveRecord::Base.establish_connection\nend\n```\nHere are the logs:\n``\n2016-09-05T04:14:17.575779+00:00 heroku[web.1]: State changed from crashed to starting\n2016-09-05T04:14:20.107795+00:00 heroku[web.1]: Starting process with commandbundle exec puma -C config/puma.rb2016-09-05T04:14:22.691015+00:00 heroku[web.1]: Process exited with status 1\n2016-09-05T04:14:22.702562+00:00 heroku[web.1]: State changed from starting to crashed\n2016-09-05T04:14:22.621186+00:00 app[web.1]: config/puma.rb:1:in_load_from': uninitialized constant Puma::DSL::Rails (NameError)\n2016-09-05T04:14:22.621201+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/dsl.rb:26:in instance_eval'\n2016-09-05T04:14:22.621202+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/dsl.rb:26:in_load_from'\n2016-09-05T04:14:22.621203+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/dsl.rb:9:in load'\n2016-09-05T04:14:22.621204+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/configuration.rb:205:inblock in load'\n2016-09-05T04:14:22.621204+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/configuration.rb:202:in each'\n2016-09-05T04:14:22.621205+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/configuration.rb:202:inload'\n2016-09-05T04:14:22.621208+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/launcher.rb:62:in initialize'\n2016-09-05T04:14:22.621209+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/cli.rb:65:ininitialize'\n2016-09-05T04:14:22.621209+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/lib/puma/cli.rb:65:in new'\n2016-09-05T04:14:22.621210+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/bin/puma:8:innew'\n2016-09-05T04:14:22.621211+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/gems/puma-3.6.0/bin/puma:8:in <top (required)>'\n2016-09-05T04:14:22.621211+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/bin/puma:23:inload'\n2016-09-05T04:14:22.621212+00:00 app[web.1]:    from /app/vendor/bundle/ruby/2.2.0/bin/puma:23:in `'\n```\n. ",
    "kylebebak": "Hey Evan, thanks for the help! I'm using Puma to run a Sinatra app. Sinatra doesn't have its own logging capabilities as far as I know.\nI was hoping to use Puma's logging functionality because it covers all our needs, and I was having trouble getting CommonLogger to play nice when it was deployed in a docker container.\nIs there any way to use Puma's stdout_redirect in production mode?\n. Hey Evan, thanks for the info I'll try this and see if I can get it working in environments other than development.\n. @vadviktor , I haven't looked at this yet, I've been working on other stuff. As Evan noted above, Puma's logger use its own request logging in production.\nI think I'll end up defining an instance of Rack::CommonLogger in config.ru and use that\n. ",
    "vadviktor": "@kylebebak, have you got any luck getting it to work yet? Please share any working solution to this, I'm facing the very same problem :( Thank you very much!\n. ",
    "mwpastore": "Hi, sorry to butt in on an old issue, but Sinatra does indeed have its own logging, and it writes to stderr by default: \n\n:logging - log requests to STDERR\nWrites a single line to STDERR in Apache common log format when enabled. This setting is enabled by default in classic style apps and disabled by default in Sinatra::Base subclasses.\nInternally, the Rack::CommonLogger component is used to generate log messages.\n\nSo if you have a classic style app in production (or a modular app with enable :logging or set :logging, true) this explains the behavior you're seeing. \n. @Silex Take a look at this. What's your config.log_level (or Rails.logger.level) set to in production?\n. Right, as @evanphx explained above, in production Puma simply captures the stderr and stdout of the app and redirects it to the files you specify with stdout_redirect. I don't know enough about Rails\u2014I thought maybe setting the log_level to :info or :debug would do the trick.\n. @Silex I believe it's this statement.\n. It looks like @evanphx added a --log-requests (or log_requests) option in Puma 3.0 that can be used to enable request logging in production. If you're planning to upgrade soon, maybe you can give that a try! \n. @perlun Unfortunately, no, and even worse, I can't remember why I closed this issue. :. This may be related to #880?\n. Boom! Thanks @evanphx! Can't wait for 3.0. \n. Something simple to try:\napacheconf\nRequestHeader set X-Forwarded-Proto 'https' env=HTTPS\nRequestHeader set X-Forwarded-Proto 'http' env=!HTTPS\nI also found this StackOverflow question that suggests you may need to add early to the end of those lines. Try it with and without.\n. I just ran into this. Puma stops writing to the logs after they're rotated. The solution is to sighup the top-level Puma process after logrotate does its thing. This tells Puma to close and reopen the log files.\nI'm running Puma through Upstart, so I modified my puma config.rb to write out a pid file, and added these lines to /etc/logrotate.d/upstart:\nsharedscripts # only run the postrotate script once\n        postrotate\n                /bin/kill -HUP `cat /srv/rack/example.com/puma.pid 2>/dev/null` 2>/dev/null\n        endscript\nYour solution may not be exactly identical but hopefully this gives you a place to work from.\n. @grepruby:\n1. What operating system?\n2. How did you install God? gem install or yum or apt or something, e.g. apt-get install ruby-god?\n3. Are you redirecting stderr and stdout through puma (e.g. puma --redirect-stdout ..) or through god (e.g. w.log_cmd = ..)?\n. Try:\nsh\nsudo service puma start app=/home/deployer/apps/my_app/current\nor just:\nsh\nsudo start puma app=/home/deployer/apps/my_app/current\nIt's right there in the doc you linked. :smiley:\n. @thezed OK, anything in /var/log/upstart/puma-_home_deployer_apps_my_app_current.log (or similarly-named file)?\n. You should have a log for your application as well. Did you create an empty\n/etc/puma.conf file, and did you customize /etc/init/puma.conf per the\ninstructions?\n. In order for me or anyone else to help you, we're going to need more information. \n- What puma-specific settings did you put in your deploy.rb file (for capistrano-puma)?\n- Do you have a config/puma.rb file in your app repo?\n- Do you have a tmp/puma directory in your app repo, writeable by the deployer/user running puma?\n- Did you add pidfile, state_path, and activate_control_app statements to config/puma.rb?\n- Can you manually cd to an app and bundle exec puma? How are you running it if the upstart script isn't working?\n- ~~Are you using rbenv, chruby (with or without auto), rvm, or a system-wide Ruby?~~ Where is rvm installed? Did you modify /etc/puma.conf to reflect any differences cf. your system configuration? \n- What Ruby version are you using and how do you have your app configured to select that version? .ruby-version? .rvmrc? Did you make sure to install the bundler gem? \n. Nope, this appears to be time-related, and it affects 2.16 as well as 3.0. It must be due to another change I've made in the past week or two. Sorry for the false alarm! \n. @evanphx Ah, I thought I tried disabling keepalive in my HAproxy backend to work around this \"issue\", but I only disabled TCP keepalive, not HTTP keepalive mode. I added \"option http-server-close\" just for kicks; I'll let you know if Puma continues throwing this error even so. Thank you!\n. Sure enough, no errors since making the HAproxy configuration change, so keepalives are definitely to blame here.\n. There are a few ways to do it. If you're using rbenv, for example, you can use the rbenv-vars plugin and store your variables in a .rbenv-vars file in your application directory or any parent directory thereof. The upstart script that comes with Puma will automatically detect rbenv and load it before starting. I'm sure chruby and RVM have similar solutions. \nUpstart doesn't invoke the bashrc of the target user. It has nothing to do with Puma; it's just a bad default behavior for init systems. You can, however, manually source (dot in) a file with environment variables in your upstart script, e.g.:\nscript\n. /path/to/your/environment\nexec bundle exec puma -C config/puma.rb\nend script\nYou'll need to update your upstart script to source this file. (Sorry, my upstart is a little rusty after learning systemd, so the above may not be 100% correct.)\nYet a third option is to use something like dotenv and load the environment in your application. The hitch would be if you want to use these environment variables within your Puma configuration. It might be tricky to get the order of operations right. \n. @jrafanie Tests were run under Puma, Unicorn, and Passenger, with Sequel and ActiveRecord, against MySQL and PostgreSQL. All the configuration is available at the linked GitHub repository, and all the benchmark data is available at the linked Round 14 Preview 1.1 results. . @evanphx Sorry, it's somewhat inscrutable. Yes, that's correct.\n. @evanphx That's a pleasantly simple and elegant explanation of this issue, and hearing it from the master himself definitely eases my mind. \ud83d\ude47 I was starting to question my sanity, or at least question my understanding of how computers work! \nI will try to reproduce the anomaly using Siege instead of wrk and then fiddle with Siege's request staggering option to see if it \"solves\" the \"problem.\" If it does, maybe I can try convincing wg to add a similar feature to wrk, or failing that, try convincing TechEmpower to use a different benchmarking tool (which isn't likely since they rely pretty heavily on wrk's Lua feature). \nThank you! Is it alright if we leave this issue open while I keep poking at it? . @nateberkopec Running some quick benchmarks just to get a feel for it:\nIn a database test (query) with Sequel, Unicorn 5.3.0 (three workers, single-threaded) outperforms Puma 3.9.1 (three workers, 86 threads/worker) by up to 60% (e.g. 3,454 req/sec cf. 2,152 at a wrk concurrency of 256) on my test system. That thread count is definitely oversized for this machine; when TFB R15-preview is available we can see the results on larger hardware. However, this is consistent with the results I've been seeing, and the results published in TFB R14. Interestingly, single-threaded Puma outperforms Unicorn (cf. above at 4,117 req/sec).\nIn a non-database test (json), a similar pattern can be observed. Single-threaded Puma clocks in at 8,385 req/sec, Unicorn at 6,520, multi-threaded Puma at 5,856.. ",
    "Silex": "\nIs this in a rails app? One thing is that puma does not use it's own request logging in production mode, leaving it up to the app to provide that via either Rack::CommonLogger or as Rails does, it's own internal logging.\n\nIs there a way to tell puma to use its own request logging in production mode? If not, how do we get the same behavior using rails? code example would be great :wink: \nI'm using rails and this is a bit annoying because in development, puma.stdout.log looks like:\n172.18.0.1 - - [26/Feb/2016:11:20:41 +0100] \"GET /jquery-fbd3c47db8f7f0030f377a105ed8fc51.js?body=1 HTTP/1.0\" 200 294162 0.0082\n172.18.0.1 - - [26/Feb/2016:11:20:41 +0100] \"GET /bootstrap/modal-0f3e39885adbbe856a4c41ebc3ce9402.js?body=1 HTTP/1.0\" 200 9939 0.0052\n172.18.0.1 - - [26/Feb/2016:11:20:41 +0100] \"GET /bootstrap/popover-8c9186ed62118dbc62c6f97afd0ca840.js?body=1 HTTP/1.0\" 200 3164 0.0079\nBut in production that log is just empty.\nThe rails log production.log looks as expected but that log doens't contain the same information.\n. @mwpastore: it's set to :info. I remember testing with :debug too but it didn't change anything, tho it was a quick test so maybe I messed up.\n. > puma.stdout.log looks like:\n\nBut in production that log is just empty.\n\nI should be more clear here, to be exact that log looks like this:\n=== puma startup: 2016-02-25 09:47:08 +0100 ===\n* Listening on unix:///var/run/puma/puma.sock\nThat is, the same as in development but without the requests logs.\n. @mwpastore: can you show me where puma checks for the production environment and decides not to log? I mean in which file?\n. @mwpastore: thanks! now I know what to fiddle with :)\n. ",
    "DzmitryNikitsin": "is there any reason why you skip logs for jruby & damon mode ? https://github.com/puma/puma/blob/master/lib/puma/jruby_restart.rb#L55 \n. ",
    "pangloss": "+1\n. Thanks, that's all I needed to know!\n. ",
    "joeyfreund": "Thanks @evanphx, we'll probably wait for your PR to be merged and released.\n. ",
    "rayning0": "Thanks @evanphx! I just had this same problem in my Rails app. Just changed my web server to Puma. Got 'Unable to load application: NameError: uninitialized constant Rack::Cors'. Fixed it by adding:\nrequire 'rack'\nrequire 'rack/cors'\nto config.ru.\n. ",
    "Eric-Guo": "Really fast fix, confirm work, thanks.\n. ",
    "iparips": "I'm experiencing the same issue with Puma version 2.13.4 & Dashing version 1.3.4.\n. @pedroaxl: Thank you for your quick response. The workaround worked for me. Are there plans to add a more long term resolution for this issue in an upcoming release?\n. ",
    "pedroaxl": "@iparips As a temporary (and ugly) solution, it works if you call the methods using complete namespace:\n```\nSinatra::Application.configure do\n  Sinatra::Application.helpers do\n    def protected!\n      do_something!\n    end\n  end\nSinatra::Application.get '/hello_world' do\n    return \"hello world!\"\n  end\nend\n```\n. ",
    "petrblaho": "The similar problem on Fedora 22.\n```\n/usr/bin/ruby extconf.rb\nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\ncreating Makefile\nmake \"DESTDIR=\" clean\nrm -f \nrm -f puma_http11.so  .o  .bak mkmf.log .*.time\nmake \"DESTDIR=\"\ngcc -I. -I/usr/include -I/usr/include/ruby/backward -I/usr/include -I.   -fPIC -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -mtune=generic -fPIC -m64 -o io_buffer.o -c io_buffer.c\nio_buffer.c: In function \u2018buf_to_str\u2019:\nio_buffer.c:119:21: warning: pointer targets in passing argument 1 of \u2018rb_str_new\u2019 differ in signedness [-Wpointer-sign]\n   return rb_str_new(b->top, b->cur - b->top);\n                     ^\nIn file included from /usr/include/ruby/ruby.h:1694:0,\n                 from /usr/include/ruby.h:33,\n                 from io_buffer.c:2:\n/usr/include/ruby/intern.h:704:7: note: expected \u2018const char \u2019 but argument is of type \u2018uint8_t \u2019\n VALUE rb_str_new(const char, long);\n       ^\ngcc -I. -I/usr/include -I/usr/include/ruby/backward -I/usr/include -I.   -fPIC -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -mtune=generic -fPIC -m64 -o mini_ssl.o -c mini_ssl.c\nIn file included from mini_ssl.c:3:0:\n/usr/include/ruby/backward/rubyio.h:2:2: warning: #warning use \"ruby/io.h\" instead of \"rubyio.h\" [-Wcpp]\n #warning use \"ruby/io.h\" instead of \"rubyio.h\"\n  ^\nmini_ssl.c: In function \u2018raise_error\u2019:\nmini_ssl.c:236:3: error: format not a string literal and no format arguments [-Werror=format-security]\n   rb_raise(eError, msg);\n   ^\nmini_ssl.c: In function \u2018engine_read\u2019:\nmini_ssl.c:242:14: warning: unused variable \u2018n\u2019 [-Wunused-variable]\n   int bytes, n, error;\n              ^\nmini_ssl.c: In function \u2018engine_peercert\u2019:\nmini_ssl.c:341:28: warning: pointer targets in passing argument 1 of \u2018rb_str_new\u2019 differ in signedness [-Wpointer-sign]\n   rb_cert_buf = rb_str_new(buf, bytes);\n                            ^\nIn file included from /usr/include/ruby/ruby.h:1694:0,\n                 from /usr/include/ruby.h:33,\n                 from mini_ssl.c:2:\n/usr/include/ruby/intern.h:704:7: note: expected \u2018const char \u2019 but argument is of type \u2018unsigned char \u2019\n VALUE rb_str_new(const char, long);\n       ^\nmini_ssl.c: In function \u2018raise_error\u2019:\nmini_ssl.c:215:5: warning: ignoring return value of \u2018strerror_r\u2019, declared with attribute warn_unused_result [-Wunused-result]\n     strerror_r(err, buf, sizeof(buf));\n     ^\ncc1: some warnings being treated as errors\nMakefile:225: recipe for target 'mini_ssl.o' failed\nmake: *** [mini_ssl.o] Error 1\nmake failed, exit code 2\n```\nIt is probably due to strict compiler flags used in most of new linux distros.\n. Thanks for quick solution.\n. ",
    "shuiiiiiimu": "slove it....\nadd require 'puma' in config/puma.rb\n. ",
    "delwaterman": "I still get this issue with the require statement:\n```\n!/usr/bin/env puma\nrequire 'puma'\nputs 'Reading puma configuration'\nthreads ENV.fetch(\"PUMA_THREADS_MIN\", 4), ENV.fetch(\"PUMA_THREADS_MAX\", 16)\n```\n15:50 $ ./config/puma.rb\nReading puma configuration\nPuma starting in single mode...\n* Version 2.16.0 (ruby 2.3.0-p0), codename: Midwinter Nights Trance\n* Min threads: 4, max threads: 16\n* Environment: development\nReading puma configuration\n! Unable to load application: NoMethodError: undefined method `threads' for main:Object\n/Users/orion.delwaterman/code/greenhouse/mobile_notification_server/config/puma.rb:3:in `<top (required)>': undefined method `threads' for main:Object (NoMethodError)\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/rack-1.6.4/lib/rack/builder.rb:42:in `require'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/rack-1.6.4/lib/rack/builder.rb:42:in `parse_file'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/puma-2.16.0/lib/puma/configuration.rb:155:in `load_rackup'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/puma-2.16.0/lib/puma/configuration.rb:99:in `app'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/puma-2.16.0/lib/puma/runner.rb:114:in `load_and_bind'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/puma-2.16.0/lib/puma/single.rb:79:in `run'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/puma-2.16.0/lib/puma/cli.rb:214:in `run'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/gems/puma-2.16.0/bin/puma:10:in `<top (required)>'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/bin/puma:23:in `load'\n    from /Users/orion.delwaterman/.rvm/gems/ruby-2.3.0/bin/puma:23:in `<main>'\n. ",
    "kwngo": "@evanphx How exactly would you load a config file?\n. ",
    "leoskyrocker": "@kwngo What @delwaterman was doing was $ ./config/puma.rb instead of $ bundle exec puma -C config/puma.rb.. ",
    "NoSkillGuy": "@zhangsm @kwngo  bundle exec puma -C /data/app_name/shared/config/puma.rb and i am still getting the same issue. . ",
    "marcello-telles": "i'm having the same issue, even adding require 'puma'. @NoSkillGuy did you came up with any solutions for this?. ",
    "ashishcompwiz": "some kind of similar issue here\nashish@AshishPC:/mnt/d/Websites/Developing/BudgetSchoolErp3_Fedena_SchoolMaster/BudgetSchoolErp3$ bundle exec puma -C config/puma.rb\nPuma starting in single mode...\n* Version 2.16.0 (ruby 1.8.7-p376), codename: Midwinter Nights Trance\n* Min threads: 5, max threads: 5\n* Environment: development\n! Unable to load application: NoMethodError: undefined methodparse_file' for Rack::Builder:Class\nbundler: failed to load command: puma (/home/ashish/.rvm/gems/ruby-1.8.7-head/bin/puma)\nNoMethodError: undefined method parse_file' for Rack::Builder:Class\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/gems/puma-2.16.0/lib/puma/configuration.rb:155:inload_rackup'\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/gems/puma-2.16.0/lib/puma/configuration.rb:99:in app'\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/gems/puma-2.16.0/lib/puma/runner.rb:114:inload_and_bind'\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/gems/puma-2.16.0/lib/puma/single.rb:79:in run'\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/gems/puma-2.16.0/lib/puma/cli.rb:214:inrun'\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/gems/puma-2.16.0/bin/puma:10\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/bin/puma:19:in load'\n  /home/ashish/.rvm/gems/ruby-1.8.7-head/bin/puma:19. ",
    "W-Mills": "Is puma in your gemfile?:\ngroup :production do\n  gem \"puma\"\nend\n. ",
    "hab278": ":+1: Thanks! Definitely will try it out.\n. ",
    "lfalcao": "@rosenfeld, \nwhy not reload puma from the controller? :)\nsystem('bundle exec pumactl -P /path/to/pid/file.pid restart')\n. ",
    "GUI": "Thanks so much for the speedy fix, that's done the trick!\n. ",
    "deivid-rodriguez": "Thanks!! :heart_eyes: \n. Guys, this was fixed in https://github.com/puma/puma/pull/742. The warning should be gone in the next release, so you can close this!\n. @evanphx You mean you have to check in a different jar file every time you release? If so, why? Updating the generated jar file when necessary seems enough.\n. This should probably be moved to rubygems/rubygems then.\n. Thanks for considering by the way :smiley: \n. It's green now!. @MSP-Greg But did you notice that you were installing the 32 bit package during the 64 bit jobs, and viceversa?. @MSP-Greg I changed the code to swap the packages-. @junegunn I think you're trying to fix the same issue as me in #1567. However, I tried your patch and it didn't work for me (tests hang). Weird because I like it better than mine... :S. @nateberkopec I think these are alternative solutions to the same problem but won't play nice with each other. My fix was specifically checking for a nil return but with this PR that would change.. Failure looks unrelated, culprit could be #1487.. I rebased this PR since master is now green.. Rebased again now that master is green again.. New year, new rebase! :stuck_out_tongue_closed_eyes: . In particular, I was asked not only to remove all versions of puma from my system, but also capistrano3-puma. That seems like a pretty high barrier for running puma's tests.. Thanks! :). This PR fixes the Travis build, can we merge it?. Can you upgrade to Ruby 2.6.2 and try again?. Maybe we can change it to gem update --system 2.7.7 so that further updates of rubygems don't break the build?. Since the line is no longer run, we probably no longer need the comment either.. Not sure when 3.x will be released, but ok with that! Hopefully new releases won't break anything... :). ",
    "gregors": "seeing the same\n. ",
    "dqdinh": "Does anyone know where I can find when the next release will be out?\n. ",
    "brunoadacosta": "ipv6 sorry\n. ",
    "chloerei": ":+1: \n. ",
    "jlduran": "@rosswilson solved in 2.13.4\n. https://github.com/puma/puma/commit/459ab196369be9355432d43097e43a5e0761b8e9. The README.md also references --control. But that can be easily addressed in a separate commit.. ",
    "camallen": "Yeah I'm seeing something very similar, ~~1 worker~~ running via bundle exec rails s puma so default setup / threads, etc. What happens is after some time 1 or more threads gets stuck in a tight loop like what you listed above with high cpu usage. This resource contention ends up slowing all other threads on that node. It's been happening for a while on our api nodes and a restart of puma fixes it but they come back. \nHave you tried debugging the process / threads using gdb to get backtraces? This article is pretty useful. My process is running under docker on Ubuntu and I haven't been able to attach gdb to it unfortunately. \nI'm assuming it's our code / a gem but not ruling anything out yet.\n. Right ended up tracking my problem down . It was totally a userland loop that could not finish, nothing to do with Puma.\nOn a colleague's recommendation I added rack-timeout to the app to get a stack trace via the error reporting app. Normal traffic ended up triggering the stuck state pretty quickly and i got a useful stack trace to track down the stuck loop conditions. Fixed this and haven't experienced the error for a day now ... happy days.\n. ",
    "mwalsher": "I updated the puma gem to 2.12.3 and so far everything is running much smoother. I'll close this for now and reopen if the problem reoccurs.\n. ",
    "saizai": "Thanks :)\n. ",
    "abritinthebay": "Puma's limit is 10240, which isn't THAT long but it's waaaay longer than say: IE. If you're using that long of a query string you're going to run into issues cross-browser.\nHowever it should be configurable... ideally. Most are, and the spec actually says there is no limit at all which is.. you know, fun.\nFor reference:\n- MSIE - max total url length (inc path, etc) -  2,083 characters.\n- Chrome -  stops displaying after 64k characters, but can serve more than 100k characters. \n- Firefox - same as Chrome.\n- Safari - will not truncate and can handle up to 100k.\n- Apache - ~4,000 characters, after which Apache produces a \"413 Entity Too Large\" error.\n- IIS -  16,384 characters, but is configurable.\n- Perl HTTP::Daemon - 8,000 bytes. After which a 413 is issued.\n. Well yeah, GET adds to the url so that makes sense. If you're sending data (rather than getting it) you should be using POST anyhow...\n. ",
    "thanostollos": "I second to Josh, I'm facing a situation where I can't use POST, so it would be nice if query length can be configured over 10K.\n. ",
    "liamwhite": "\"I'd rather protect users out of the box\"\nThis is more akin to security theater than actual security.\nI recently switched over an app from Passenger Enterprise (which has left a bad taste in my mouth) to Puma, to try it out. We are now going to move to Unicorn because of this particular bug. My app doesn't exactly get enormous requests like these all the time, and it doesn't generate links that large on its own, but it has no trouble handling extremely large requests.\nI could fork puma and change the code in the C extension, but I'd really rather not do all that work if such a change would never be accepted by the team.. @nateberkopec It's that much more effort I have to put in for\n\nupdates to puma\nbugfixes\nreal security vulnerabilities\n\nif I have to maintain a fork.\nThis is a particular edge case where the normal GET query size would be one or two simple values in the query string, but has no theoretical limit, because not having a limit makes it a boon to users who desire to use search in interesting ways; in the context of my app, Elasticsearch doesn't really care if you're doing a filter on two terms (50ms) or two hundred thousand terms (350ms).. ",
    "duttski": "Yep apologies Puma-people, just this morning managed to get a similar issue with Rainbows too. \n. ",
    "keithpitt": "Thanks @evanphx! :heart_decoration: \n. @TiagoCardoso1983 oh great, thanks for the sample code! I'm starting to understand now. By using rack.hijack, it frees the connection from Puma allowing other code to keep the connection open and do with it what it wills.\nSo why doesn't Puma do that sort of thing out of the box? It seems like a good idea! (Sorry again if I'm asking silly questions)\n. @evanphx actually, ignore my comment on that. Reading back on everything it doesn't make sense. But thanks for reaching out for clarification!\nI think my confusion came from the threads setting. I was looking at how ActionCable and MessageBus work, and from what I saw it stored the connection, returned a response, and managed it in another thread. But I was like \"but threads is set to 2, how can they use more?! how are they circumventing the Puma setting\" I realise that was a bit silly. I think my comment was \"well, if I can just use rack.highjack and handled more concurrent requests, why have the threads option to begin with?\". So it sounds like threads is actually more like concurrent_requests for that worker?\n@SamSaffron thanks for going into more detail about how it all works! I was trying to rack (pardon the pun) my head around how you're able to \"free\" the thread from Puma. I was looking at SSE mostly because it seemed easy, but you're right, I should definitely look into the other standards.\n. (I closed the issue since it's not an \"issue\" anymore)\n. ",
    "rosswilson": "I'm getting the exact same error, with Ruby 2.2.2. I've got\nworkers 2\ndefined in my config file. Removing this gets rid of the error. I'm looking into the root cause now.\n. ",
    "jamesmeador": "After some digging, it appears that this is more of a problem with respecting the directory configuration.\nIt looks like puma \"prefers\" to use env['PWD'] but this is causing us issues.\nWe spawn Puma from a custom daemon that we use that listens to watches on a zookeeper node. We fire updates to the node, and whatever machines we have listening to that node will pick up the new configuration (a git sha and repo). \nWhen the daemon starts up, it clones the repo into a directory, say root/master-gitsha1. It changes into that directory, runs \"bundle\" to activate gems, as well as compile assets, etc.\nWhen the setup is done, it's time to symlink root/master-gitsha1 to root/current and launch puma for the first time. The daemon Dir.chdir's to root/current, and does \nBundler.with_clean_env { Process.spawn('bundle exec puma -w 3 -C ${HOME}/puma.rb') }\nNote that Dir.chdir in Ruby just changes to the target, thus anything spawned from Ruby means that ENV['PWD'] will be the target directory, not the symlink. In this case, the target directory is root/master-gitsha1.\nPuma takes off. CLI.new sets @cli_options[:worker_directory] to ENV['PWD'] because Dir.pwd and ENV['PWD'] match.\nCLI.run calls parse_options which then (via configuration.rb and dsl.rb) parses my config file, setting @options[:directory] and @options[:worker_directory] to root/current. Then, it merges cli_options in. This is what I think is a bug:\nNow, @options[:directory] is root/current and @options[:worker_directory] is root/master-gitsha1.\nwhen start_phased_restart for a cluster is called, the directory will always be changed to root/master-gitsha1.\nSeems unreasonable to me. Am I crazy? It seems silly to store a cli_option that the user didn't explicitly set. (and then override an attribute they did intentionally set).\n. @evanphx thoughts on this? This bug just caused some downtime for us.\n. ",
    "grimm26": "793 did not fix this for me and neither does using 2.12.3\n. I specify directory on the command line, but puma dereferences the 'current' symlink and sticks with that dereferenced directory after a USR1 and the 'current' symlink has changed.\n. ",
    "ifyouseewendy": "same issue, version 2.15.3\n. @allaire Thanks. I was using puma 2.15.3, with mina deploy (phased restart). Got the issue, losing the right directory when restarting. Here is my conf, anything wrong?\n``` ruby\nworkers Integer(ENV['WEB_CONCURRENCY'] || 4)\nthreads_count = Integer(ENV['MAX_THREADS'] || 1) # Not sure for thread safe\nthreads threads_count, threads_count\nrackup      DefaultRackup\nport        ENV['PORT']     || 3000\nenvironment ENV['RACK_ENV'] || 'development'\nprune_bundler\nstdout_redirect 'log/puma.stdout.log', 'log/puma.stderr.log', true\n```\n. @allaire :+1: it works, thanks very much\n. ",
    "scrummie02": "If this is done it works:\nhttps://groups.google.com/forum/#!topic/rubyinstaller/RkBqA6DsGl0\n. ",
    "dedles": "And in macs?\n. ",
    "vipulnsward": "Looks like still an issue on ruby-head https://travis-ci.org/puma/puma/jobs/121497343\nThis is also causing failure on rails builds with ruby-head.\n. This can now  be closed.\n. Related https://github.com/rails/rails/issues/24435#issuecomment-205875323\nrails server, does not support it. running puma directly will.\n. In the above case, when trying to read binds, its hitting https://github.com/puma/puma/blob/c4006b93fad6052a915bca2703cccedf6d4812b8/lib/puma/configuration.rb#L33\nThis is taking precision for user_options passed on from Rails(default 3000), since its the first entry created in https://github.com/puma/puma/blob/c4006b93fad6052a915bca2703cccedf6d4812b8/lib/puma/configuration.rb#L17-L18.\nChanging it to reverse_each, fixes this issue.\n. ",
    "snicky": "Is it caused by this setting in the puma.conf:\nsetuid snicky\nsetgid snicky\n? I'm running all the above commands as a root and not as snicky.\n. I went through this tutorial and everything seems working now: https://www.digitalocean.com/community/tutorials/how-to-deploy-a-rails-app-with-puma-and-nginx-on-ubuntu-14-04\n. ",
    "snow": "It passed all tests on my OSX but failed for one on travis, I'm looking into it.\n. The error on travis seens like something inside puma, or the build, not the script I modified. So I'ill just ignore it.\n. ",
    "mcb": ":+1:  Seeing this here as well. However if we remove the puma.sock as well, it works as it is expected. The puma.sock file does persist after stop.\n. ",
    "StragaSevera": "You mean remove socket file before or after the phased restart? =-)\n. ",
    "goofansu": "I'm using upstart right now.\n. ",
    "jxc876": "Still not working for us ... :(\n. ",
    "tawan": "Hi @wonderer007 Thank for the bug report. Do you still encounter the issue?\n. Hi, CodeTriage assigned this issue to me.\n@klemenkobetic Can you elaborate on how you connect to the database? Which client library did you use?\n. This seems to be the same issue as reported in #620 . @NZGlitch can you confirm?\n. ",
    "dgoradia": "Closing. I was using -p 5001 to start the app server but shouldn't have. All working after I removed that.\n. ",
    "ArturT": "I had this problem and I was looking for solution. I'm not sure what is exactly the reason of this behaviour but here are things I found out and solution to fix this in my app. \nWhen I start rails server then puma listens on localhost:3000\n$ rails server\n=> Booting Puma\n=> Rails 4.2.3 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\n=> Ctrl-C to shutdown server\nPuma 2.13.4 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://localhost:3000\nand http://127.0.01:3000 doesn't work.\nWhen I start puma explicitly without config file then it listens on  tcp://0.0.0.0:3000 and http://127.0.01:3000 does work.\nRead more https://github.com/puma/puma#configuration-file\n$ puma -C '-'\nPuma starting in single mode...\n* Version 2.13.4 (ruby 2.2.3-p173), codename: A Midsummer Code's Dream\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nWhen I created my custom config file then it works as well\nbundle exec puma -C config/puma.rb\n``` ruby\nconfig/puma.rb\nworkers Integer(ENV['WEB_CONCURRENCY'] || 2)\nthreads_count = Integer(ENV['MAX_THREADS'] || 5)\nthreads threads_count, threads_count\npreload_app!\nrackup      DefaultRackup\nport        ENV['PORT']     || 3000\nenvironment ENV['RACK_ENV'] || 'development'\non_worker_boot do\n  # Worker specific setup for Rails 4.1+\n  # See: https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server#on-worker-boot\n  ActiveRecord::Base.establish_connection\nend\n```\n. ",
    "Tonkpils": "From my understanding both rails and rack moved away from defaulting the listening host to 0.0.0.0 as it caused security implications. They then moved to listening on localhost. Starting the app with puma command actually does listening on 0.0.0.0 which pretty much listens on all network interfaces. \nOn other webservers, namely Unicorn and Webrick listening on localhost allows accessing the server from 127.0.0.1 as well as localhost. Puma doesn't seem to follow that same behavior though. \n. @evanphx I simply started puma with rails s and tried curling 127.0.0.1 which gives me connection refused. Once I curl localhost it works just fine.\n\n. Ruby 2.2.1 and OS X Yosemite.\n. So, it seems that accessing '[::1]:3000' works instead of 127.0.0.1. So correct me if I'm wrong, Puma is being asked to connect to localhost but Ruby or the OS translates that to IPv6 or IPv4 and puma then uses that to connect?\n. I'm assuming unicorn and webrick are connecting some different way than TCPServer. \nIn any case, I'll close this as a non-issue since this is not puma's issue.\n. @chadzink ::1 localhost simply means that accessing localhost will point to IPv6 address ::1. It's simply a name resolution.\n. ",
    "graudeejs": "Having this issue on FreeBSD as well\nCommenting out\n::1         localhost\nin /etc/hosts\nseams to fix this issue.\nI think it's Puma issue\n. ",
    "chadzink": "@evanphx: Having this same issue on OSX El Capitan, this definitely seems like a puma issue. The trick ^^^ worked, not sure what ::1 localhost means in the hosts file though.\n. ",
    "gryphon": "The same for me. Got this issue after El Capitan upgrade.\nSeems that Puma checks hostname of HTTP request to be equal to what it is binded to.\n. ",
    "whistlerbrk": "If you're visiting this thread you may be forgetting that you might now, by default, be using IPv6 networking. As a result your resolution of localhost turns to ::1 rather than 127.0.0.1. So any aliases you had on that line in your /etc/hosts need to be duplicated or moved to the ::1 line.\n. ",
    "himdel": "To clarify, it's still a puma bug if localhost can resolve to both 127.0.0.1 and ::1 - which it does on some systems.\n. @shawzt not sure.. I'd love to, but essentially what happens is TCPServer.new('localhost', port) - which makes me think the bug is not quite a puma bug but a TCPServer bug.\nIf you need a workaround and use puma only from rails, this works...\nrb\ndiff --git a/lib/puma/binder.rb b/lib/puma/binder.rb\nindex 6cb8909..2751484 100644\n--- a/lib/puma/binder.rb\n+++ b/lib/puma/binder.rb\n@@ -236,6 +236,12 @@ module Puma\n     # allow to accumulate before returning connection refused.\n     #\n     def add_tcp_listener(host, port, optimize_for_latency=true, backlog=1024)\n+      if host == 'localhost'\n+        add_tcp_listener('127.0.0.1', port, optimize_for_latency, backlog)\n+        add_tcp_listener('::1', port, optimize_for_latency, backlog)\n+        return\n+      end\n+\n       host = host[1..-2] if host and host[0..0] == '['\n       s = TCPServer.new(host, port)\n       if optimize_for_latency\n(but it's clearly not the right place to fix this).\n. ... and according to ext/socket/tcpserver.c, it is supposed to return the first successfully created socket for one of the addresses returned by getaddrinfo.\nSo I guess I'll leave it to somebody who's already touched puma before :)\n. @alecguintu if you look at it carefully, you'll see that it mentions both the file name and the line numbers..\n. @alecguintu probably the safest way to find the right file would be gem contents puma (should list all the files from the puma gem, with their whole actual paths)\n. ",
    "shawzt": "Hi @himdel , any plan to fix this?\n. ",
    "alecguintu": "+1 here. Just want to know how/where will I add those snippet code to @himdel? \n. Haha. I mean, where do I find the file binder.rb? I've checked on rails app, there is no puma folder under lib folder.\n. Thanks so much for this @himdel!\n. ",
    "insanux": "/.rvm/gems/ruby-2.2.4/gems/puma-3.2.0/lib/puma/binder.rb \n. ",
    "Eduzenet": "Thank you @himdel! now it works with 127.0.0.1 and ::1\n. ",
    "agrberg": "For those coming here who access their Rails server via an alias you can simply duplicate your alias as @whistlerbrk said. For example, if your alias is www.fakewebsite.com you'll want the following two lines in your /etc/hosts file:\ntext\n127.0.0.1    www.fakewebsite.com\n::1    www.fakewebsite.com\nLooking at what @graudeejs and @himdel said in this comment it seems like puma is matching the first valid address in your /etc/hosts file. If yours is like mine it'll start with:\ntext\n127.0.0.1 localhost\n::1    localhost\nso for IPv4 you get 127.0.0.1 and for IPv6 you get ::1. Removing the latter doesn't seem to force IPv4 for me as it may have for @graudeejs so I just added the second alias for ::1.\nI'll comment back if I figure out anything more or how this potentially effects production.\n. ",
    "tomascharad": "Hi guys,\nThis is a very important issue.\nI was trying to proxy some requests from webpack-dev-server and had a very hard time to figure that I should change localhost:3000 to [::1]:3000.\n. ",
    "RobinDaugherty": "I just want to mention why this is such an issue. Issues like this cause developers to turn off IPv6 on their machine rather than a more complicated workaround. This is bad for the community and the Internet at large. We need to encourage IPv6 adoption, and every \"minor\" issue like this is a blocker.\n. This bug has been fixed, so localhost should work correctly with or without IPv6. Using 0.0.0.0 makes the server available to everyone on every wireless network you connect to, and 127.0.0.1 does not work with IPv6. Please leave it the default or use localhost.\n@yohayg: your advice leaves people open to unintended public access. It would be best if you remove your comment.. @sfcgeorge glad to hear that localhost is working as intended. Do you know where that default value of 127.0.0.1 was coming from? Was it in your puma.rb or somewhere like that?. And in my case, the default is 0.0.0.0, running Rails 4.2 on High Sierra. I suspect it's a Rails default, but I haven't had time to dig deeper.. ",
    "yohayg": "I'm using: bundle exec rails server  Puma -b 0.0.0.0 -p 3000 which works just fine. ",
    "babinslava": "I'm using rackup -p 7000 -o 127.0.0.1. ",
    "sfcgeorge": "I had this same issue that I couldn't connect locally to Puma aka curl [::1]:3000 failed. Fixed it without disabling IPv6 by forcing Puma to listen on localhost rather than 127.0.0.1 which it was for some reason using by default.\nsh\nbundle exec rails s -b localhost -p 3000\nSo the above command works with IPv4 and IPv6.. @RobinDaugherty I don't wish to be the harbinger of reopened issues but I believe our app is just using the defaults. My machine is macOS High Sierra (experiencing the issue), whereas my colleague's machine running an older version of macOS is unaffected and seems to work fine. . ",
    "hellojere": "+1\n. Thanks, worked!\n. ",
    "isaachall": "Instead of force linking openssl, this command worked for me using El Capitan (10.11) and openssl installed (but not linked) via Homebrew:\nsh\n$ gem install puma -v 2.13.4 -- --with-opt-dir=/usr/local/opt/openssl\n. ",
    "gvt": "+1 this worked for me on El Capitan \ngem install puma -v 2.13.4 -- --with-opt-dir=/usr/local/opt/openssl\n. ",
    "carlosveucv": "+1.\nFor older versions gem install puma -v '2.1X.X' -- --with-opt-dir=/usr/local/opt/openssl works like a charm.\n. ",
    "vladiim": "+1\n. ",
    "oshchyhol": "assuming you've got openssl installed from homebrew (brew install openssl)\nbundle config build.puma --with-opt-dir=/usr/local/opt/openssl\n. ",
    "olivierlacan": "I would highly recommend against doing:\ngem install X --with-opt-dir\nThis will only fix the problem for the current Ruby version your using, and the current Puma version. Any subsequent install will still fail. It's much better to use @oshchygol's advice of configuring Bundler to learn how to build Puma's native extensions:\nbundle config build.puma --with-opt-dir=/usr/local/opt/openssl\nAgain, assuming you're using the openssl from Homebrew.\nFor the record your can find these build instructions in ~/.bundle/config or display them with bundle config.\n. @kenan-memis No, the \"development\" in the block after ENV.fetch is a fallback for the ENV#fetch method. Think of it as ENV[\"RAILS_ENV\"] || \"development\". The only way this gets set to \"development\" is if ENV[\"RAILS_ENV\"] is returning nil, which is possible but unlikely.. ",
    "vishaldeepak": "yes worked for me thanks \n. ",
    "llwang8": "This one works for me.  Thanks deepj,\ndeepj\nOne of solutions is the following\nbrew install openssl\nbrew link --force openssl\n. ",
    "cema-sp": "For me worked only this (brew install openssl - without linking):\ngem install puma -v '2.13.4' -- --with-opt-include=/usr/local/opt/openssl/include\nAnd bundler config:\nbundle config build.puma --with-opt-include=/usr/local/opt/openssl/include\n. ",
    "ninabreznik": "\ngem install puma -v 2.13.4 -- --with-opt-dir=/usr/local/opt/openssl \nbundle config build.puma --with-opt-include=/usr/local/opt/openssl/include\n\nworked on El Capitan. Thanks.\n. ",
    "voelzmo": "Shouldn't puma use pkgconfig, such that export PKG_CONFIG_PATH=$(brew --prefix openssl)/lib/pkgconfig works as well?\n. @evanphx I'd like to re-iterate my question in https://github.com/puma/puma/issues/783#issuecomment-190245578 given that there are still numerous users commenting on this: \n\nShouldn't puma use pkgconfig, such that export PKG_CONFIG_PATH=$(brew --prefix openssl)/lib/pkgconfig works as well?. \n",
    "Marcos2015": "I use: gem install puma -v 2.13.4 -- --with-opt-dir=/usr/local/opt/openssl\n          brew link --force openssl\nstill can't resolve the problem, I use EI Capitan, please help me !\n/Applications/Xcode-beta.app/Contents/Developer/usr/bin/make  all-recursive\nMaking all in .\n/bin/sh ./libtool --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./compat -I./include -I./include    -g -O2 -Wall -fno-strict-aliasing -Wno-deprecated-declarations -D_THREAD_SAFE  -MT bufferevent_openssl.lo -MD -MP -MF .deps/bufferevent_openssl.Tpo -c -o bufferevent_openssl.lo bufferevent_openssl.c\nlibtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./compat -I./include -I./include -g -O2 -Wall -fno-strict-aliasing -Wno-deprecated-declarations -D_THREAD_SAFE -MT bufferevent_openssl.lo -MD -MP -MF .deps/bufferevent_openssl.Tpo -c bufferevent_openssl.c  -fno-common -DPIC -o .libs/bufferevent_openssl.o\nbufferevent_openssl.c:60:10: fatal error: 'openssl/bio.h' file not found\ninclude \n     ^\n\n1 error generated.\nmake[2]: * [bufferevent_openssl.lo] Error 1\nmake[1]: * [all-recursive] Error 1\nmake: *** [all] Error 2\n. ",
    "rahuldavd": "I have downloaded openssl-0.9.8k_X64.zip and followed link https://github.com/hicknhack-software/rails-disco/wiki/Installing-puma-on-windows#install-the-puma-gem but on trying to install gem install puma -- --with-opt-dir=c:\\openssl but it is giving error:-193: %1 is not a valid Win32 application. - C:/Ruby22/lib/ruby/2.2.0/i386-mingw32/openssl.so\ni have followed each step but couldn't get to install Puma.\n. ",
    "patr1ck": "FYI \u2013 For users of macports, this can be solved similarly using the macports opt path:\ngem install puma -- --with-opt-include=/opt/local\nor with bundler config\nbundle config build.puma --with-opt-include=/opt/local\n. ",
    "traynorkevin": "+1 !  gem install puma -v 2.13.4 -- --with-opt-dir=/usr/local/opt/openssl\nWorked for me!!! I used version 3.4.0 though!. ",
    "albertoalmagro": "You also need to care about in which directory is puma being installed. i.e. If you wat Puma to be installed in your application's vendor/bundle directory you may have to run:\ngem install puma -v 2.13.4 -i ./vendor/bundle -- --with-opt-dir=/usr/local/opt/openssl\nNote: Puma version, vendor/bundle and openssl directories may be different in your case.. ",
    "gerard-morera": "@albertoalmagro solution works for me. Mac OS Sierra + using rbenv. ",
    "sonjarae": "gem install puma -v '3.0.0' -- --with-opt-dir=/usr/local/opt/openssl\nbundle install \n\ud83d\udcaa. ",
    "annzenkina": "bundle config build.puma --with-opt-include=/usr/local/opt/openssl/include/\nWorked for me on MacOS 10.12.6.. ",
    "dekellum": "See updated https://github.com/puma/puma#process-monitors\n. See also: https://github.com/puma/puma#process-monitors\n. You probably don't want to daemonize puma when using with systemd, or if you do, you'll need to inform systemd that its a forking server.  Check out sample config now on master at docs/systemd.md\n. Have you had a chance to try this under latest Puma 3.4.0? There were some fixes for socket activation that landed in 3.3.0.  Also latest jruby 1.7.25 or 9.x might help.   That said, I'm not sure how much socket activation has been tested for Puma on JRuby in particular.\n. Sorry if what I authored in https://github.com/puma/puma/blob/master/docs/systemd.md is short of jungle, but the config described, including the socket activation and thus systemd managed \"hot restart\" is working well for me in production.  There is a usage component to the doc that I don't think would fit in jungle alone, but people might not be finding the doc?\n@t27duck's unit is otherwise pretty similar.  I don't follow using SIGQUIT for ExecStop when the default SIGTERM is handled by puma as a graceful shutdown. Is QUIT for immediate shutdown?  Also not sure if rolling restart should be integrated by default in ExecReload (which would normally be for reloading config).  In general though I hope the recommended config is improved. \n. Oh, I might just be confused.  Previously I had thought part of /tools/jungle was some templates and generation of these other init-system scripts/config.  Looking again I see that isn't the case\u2014its just the individual files with comments and then an overview README.  What I added to /docs/systemd.md could certainly be adopted to this same format, e.g. /tools/jungle/systemd with puma.service, puma.socket and remainder of the documentation in a README.md.\nGiven that the existing doc now ranks first here:\nhttps://www.google.com/search?q=puma+systemd\nI suggest that /docs/systemd.md be made a symlink if github gracefully handles that, or a \"Moved to...\" markdown link.\nAlso note the Process Monitors section of toplevel README.md has a reference.\nOne other thing that might be found lacking in the puma.service as documented is lack of any particular support for RVM, rbenv, et al.   I don't use those in production myself, preferring a /usr/local/bin/ruby, as I don't like how these complicate this very scenario!   SystemD config isn't bash so its not as simple as a bash source. The Bundler binstub (prefering WorkDirectory/sbin to avoid conflicts with /bin) approach seems nicer than bundle exec --keep-file-descriptors.  Do Bundler's binstubs play nice with RVM, etc.?  Or does RVM, etc. provide its own binstub mechanism that could be referenced here?\n. In 17a81f2a (PR #1329) I added a link to /docs/systemd from the tools/jungle/README.md. Does that sufficiently satisfy the original issue expressed here: that people starting at /tools/jungle didn't know about /docs/systemd?  If so, close this?. /docs/systemd has had some recent improvements.  Without something more specific about what is missing (or a PR improving the doc) I'd suggest closing this.. @nateberkopec I'll give this a review. Regarding the wiki suggestion, I would like to point out that [search] for the current docs/systemd.md is working well and it would be unfortunate to break that. Also I think pull-request formality for docs is appropriate given the cost of any misinformation accumulated. \n[search]: https://www.google.com/search?q=puma+systemd. I'm a regular puma and systemd user, but not a capistrano3-puma user.  Some comments on this PR:\nGiven the style of config generation in capistrano3-puma, and its current support for init.d (and others?), it would seem that a more complete solution would be to push support for systemd upstream to that project?  Cc: @seuros  \n@mdesantis, the original contributor to ./docs/systemd alt. forking config, apparently had some luck using the default pumactl based stop/start/restart of capistrano3-puma, and as far I can tell, that is the most compelling reason to want to use the alt. forking config, and deal with its additional idiosyncrasies.   If as this PR suggests, @stevenchanin is able to make capistrano 3 use systemctl for stop/start/restart, then why not just use the Type=simple preferred configuration, with or without socket activation?\nOne important difference is that this is using/documenting systemd directive Restart=always where @mdesantis omitted this and the default is \"Restart=no\".  Restart=no makes a certain sense, if one is solely trying to get a systemd config to replace a legacy System V init.d config, for start on boot. As this PR points out, this wont have the advantage of systemd actively monitoring and restarting puma. In cluster mode however, the puma master also has the functionality to do that with its workers. (With legacy init.d, that was the only mechanism.)\nI suspect the PID file warning is likely a timing issue with puma using --daemon: systemd expects that upon exit from the ExecStart command, the PID file is written and readable.  That might not be the case for example if the PID file is written by a subsequent, forked process.\n@stevenchanin would you be willing to also cross review changes in #1329?  . Folks, I think we have already extracted everything useful from this thread into the docs. \n@nateberkopec: suggest closing. Happy New Year! . I think @stereobooster was correct, the HUP is from ssh session close, and Puma is not going to somehow subvert/work-around this feature of UNIX by doing something different with that signal.  Cc: @nateberkopec, suggest closing.. I would have liked to get feedback on this as well as my review of #1264, but this PR's changes just improves upon and clarifies the rationale for use of the various systemd config options.  Could you please merge this? . Note if this is merged (or #1183 is fixed as intended) that systemd service configs, as in /docs/systemd, should be updated to include:\n~~~ ini\nSuccessExitStatus=143\n~~~\n...to avoid normal shutdown/restart reporting warning and a failed state.  I'm assuming this isn't a systemd default because traditional, forked unix daemons only return an exit status during startup (before forking away).\n. I agree that your interpretation of the systemd docs is plausible and that the docs are not particularly clear. I've assumed that SuccessExitStatus=143 is necessary because I similarly had to do this with a java-based project. (JVM exits with 143 by SIGTERM) See this [serverfault answer].   Without SuccessExitStatus=143 on that project, I would get the following on a systemctl stop:\n~~~ \nJul 17 12:05:24 klein systemd[877]: Stopping Guppy (Fishwife) HTTP Server...\nJul 17 12:05:24 klein systemd[877]: guppy.service: Main process exited, code=exited, status=143/n/a\nJul 17 12:05:24 klein systemd[877]: Stopped Guppy (Fishwife) HTTP Server.\nJul 17 12:05:24 klein systemd[877]: guppy.service: Unit entered failed state.\nJul 17 12:05:24 klein systemd[877]: guppy.service: Failed with result 'exit-code'.\n~~~\nBut perhaps the JVM is doing something wrong, while your PR does it correctly? Next I've gone ahead and setup a direct test of your changes, using MRI ruby.  Here is the complete tree for this test application (branch 'puma'):\nhttps://github.com/dekellum/lionfish\nWith this setup if I use your exact PR branch, it fails on startup here:\n~~~\nJul 17 11:33:46 klein systemd[877]: Started Lionfish (Puma) HTTP Server.\nJul 17 11:33:46 klein puma[2025]: % Registered tcp:0.0.0.0:9238 for activation from LISTEN_FDS\nJul 17 11:33:46 klein puma[2025]: /home/david/.gem/ruby/2.2.0/bundler/gems/puma-fd38f5c8ce97/lib/puma/launcher.rb:171:in run': undefined methodinclude?' for nil:NilClass (\nJul 17 11:33:46 klein puma[2025]:         from /home/david/.gem/ruby/2.2.0/bundler/gems/puma-fd38f5c8ce97/lib/puma/cli.rb:77:in run'\nJul 17 11:33:46 klein puma[2025]:         from /home/david/.gem/ruby/2.2.0/bundler/gems/puma-fd38f5c8ce97/bin/puma:10:in'\nJul 17 11:33:46 klein puma[2025]:         from /home/david/src/lionfish/sbin/puma:17:in load'\nJul 17 11:33:46 klein puma[2025]:         from /home/david/src/lionfish/sbin/puma:17:in'\nJul 17 11:33:46 klein systemd[877]: lionfish.service: Main process exited, code=exited, status=1/FAILURE\nJul 17 11:33:46 klein systemd[877]: lionfish.service: Unit entered failed state.\nJul 17 11:33:46 klein systemd[877]: lionfish.service: Failed with result 'exit-code'.\nJul 17 11:33:47 klein systemd[877]: lionfish.service: Service hold-off time over, scheduling restart.\nJul 17 11:33:47 klein systemd[877]: Stopped Lionfish (Puma) HTTP Server.\nJul 17 11:33:47 klein systemd[877]: lionfish.service: Start request repeated too quickly.\nJul 17 11:33:47 klein systemd[877]: Failed to start Lionfish (Puma) HTTP Server.\nJul 17 11:33:47 klein systemd[877]: lionfish.service: Unit entered failed state.\nJul 17 11:33:47 klein systemd[877]: lionfish.service: Failed with result 'exit-code'.\n~~~\nIf I instead cherrypick fd38f5c onto a new testing branch rooted at the v3.9.1 tag, it starts correctly but I get this error on stop:\n~~~\nJul 17 11:47:20 klein puma[2487]: - Gracefully stopping, waiting for requests to finish\nJul 17 11:47:20 klein systemd[877]: Stopping Lionfish (Puma) HTTP Server...\nJul 17 11:47:20 klein puma[2487]: /home/david/src/puma/lib/puma/server.rb:392:in close': can't modify frozen IOError (RuntimeError)\nJul 17 11:47:20 klein puma[2487]:         from /home/david/src/puma/lib/puma/server.rb:392:inensure in handle_servers'\nJul 17 11:47:20 klein puma[2487]:         from /home/david/src/puma/lib/puma/server.rb:396:in handle_servers'\nJul 17 11:47:20 klein puma[2487]:         from /home/david/src/puma/lib/puma/server.rb:325:inblock in run'\nJul 17 11:47:20 klein systemd[877]: lionfish.service: Main process exited, code=exited, status=1/FAILURE\nJul 17 11:47:20 klein systemd[877]: Stopped Lionfish (Puma) HTTP Server.\nJul 17 11:47:20 klein systemd[877]: lionfish.service: Unit entered failed state.\nJul 17 11:47:20 klein systemd[877]: lionfish.service: Failed with result 'exit-code'.\n~~~\nSo in summary, I'm finding other problems on the master branch and/or your change that occur before I can even determine if the SuccessExitStatus=143 setting is necessary. Could you try and reproduce this yourself?  One more note in case it matters, my local version of MRI ruby is:\n~~~\nruby 2.2.7p470 (2017-03-28 revision 58194) [x86_64-linux]\n~~~\n[serverfault answer]: https://serverfault.com/questions/695849/services-remain-in-failed-state-after-stopped-with-systemctl/695863#695863. The puma/launcher.rb:171:in `run': undefined method `include?' issue is apparently #1344. I found workaround for this (853c01d) and now I'm able to confirm that your initial branch is working as you expected with sytstemd, no problems and without SuccessExitStatus=143 being required. I tested it both with and without clustered mode.\nAgain with the workaround, if I merge this PR to master it also works.\nMy cherry-picked variant based on v3.9.1 has that other error, but I don't think its relevant to this PR and current master.\nSo I guess the JVM has it wrong and you have it right, at least for MRI. Thanks @shayonj. Next I want to check this with jruby (on JVM).. OK this PR seems to also work fine under jruby with systemd (without socket activation), and SuccessExitStatus=143 is not required for any of these combinations.  I assume the difference is that the JVM's signal handler is calling exit(128+15) instead of kill(\"TERM\") on itself after removing the exit handler? Even on jruby installing the DEFAULT signal handler (presumably via JNI) is working as intended.. This PR was merged. I suggest you (@wodka) please create a new issue for this, including the text of the error.  I do believe the change of this PR has uncovered a bug, but the bug probably resides with how bundler's bundle exec is rescuing Puma's SignalException, logging it to stderr, and calling Kernel::abort (status 1). I am able to reproduce the error, outside of Heroku, with bundle exec. The issue is avoided when using a bundle binstubs puma wrapper.  I'm not sure if that is under user control and thus a viable workaround on Heroku.. Consider that puma offers HTTPS support and that is common when using that support to configure 2 ports: one HTTPS and one HTTP port.  In such a case:\n\nThere needs to be some means of distinguishing which socket is which on the puma side. (The current implementation does handle that.)\nConnection parameters like the ssl protocol, key and cert are used only on the puma side.\n\nWhen I last implemented improvements for socket activation, I did consider adding support in puma to read $LISTEN_FDNAMES as set by systemd and use that value as the complete configuration of the puma connection. This might satisfy your interest in consolidating the config in systemd socket file(s). However, there are some complications:\n\n[FileDescriptorName] oddly applies to all sockets in a single socket unit, so it appears to require one socket unit file per socket for this to work.\nThe ':' character is reserved so we'd need some replacement character like ';' for 'ssl;//', etc. and hope that ';' isn't used elsewhere.\n\nMy impression at the time was that this wasn't worth the additional complexity.\n[FileDescriptorName]: https://www.freedesktop.org/software/systemd/man/systemd.socket.html#FileDescriptorName=. @nateberkopec suggest closing this since there is no further comment and nothing actionable.  House cleaning. Happy New Year!. I extracted a minimum reproducible case outside of Puma and reported this as jruby/jruby#4715.  I suggest giving them a little time to respond. If this becomes a \"known limitation\" or not something that will be fixed soon, then I'll submit a PR here to document this, advising people not to configure socket activation when using Puma with jruby.. Fair enough, as there is no clear workaround that puma could implement here and no response on the upstream jruby/jruby#4715 issue.   If you are compiling release notes than please consider this more a Won't/Can't Fix than a Fix.. The change to /docs/systemd looks fine to me. Please merge, @nateberkopec ?. Looks like your system is using systemd, so follow instructions in docs/systemd, not the legacy init.d documentation. . Unless there are any objections or better ideas, I will attempt to open/link an issue and PR with bundler, to hopefully get the issue fixed there. Please reply here if anyone thinks my interpretation of this being a bundler issue is wrong.\ncc: @shayonj @schneems . My fix and test for bundler/bundler#6090 was merged to master and will presumably be released in the next bundler 1.16.x and possibly backported if they are still maintaining 1.15.x.  In the interim, linked issue also gives details on two possible workarounds: using a bundle binstub, or setting bundle config disable_exec_load true\nI would suggest that a puma committer can now close this issue.  This could also be noted in puma release notes as a known issue, referencing the bundler issue.\n. cc: @nateberkopec . Hi, I try to help with Puma and systemd but I only check for new issues periodically.\nCould you include your specific systemd socket and service config files? The extra indirection through bash -lci on ExecStart, or bundle exec without --keep-file-descriptors, are both opportunities to have the socket FDs closed before they reach puma.   If the bash is due to some RVM/rbenv/etc., they tend to have some provision for building at least a bundle command wrapper (with the right ruby), if not a working puma command wrapper (and right bundle and ruby) that makes this all one ruby process with no further execs, etc.. @molfar, your error message doesn't sound the same at all to me.  Can't help without seeing the configs. Likely better in a new issue.. Thanks for reporting this and the cc. I am also able to reproduce the issue with an external (systemd oriented) test harness, on branch and the versions in the Gemfile.lock shown:\nhttps://github.com/dekellum/lionfish/blob/puma-jruby/Gemfile.lock\nThe issue anecdotally seems more common on jruby-9.1.16, but at least with my test harness, the problem is intermittent: only sometimes do I see the same stack trace and the error code 1 (which in my case, gets logged as a failure in systemd.)\nI can reproduce the issue on jruby 9.1.14 and 9.1.12 as well (though it seems less frequent).  My test harness is with puma 3.10.0, so this isn't a very recent puma regression.  I'm not sure how or if I missed this issue in my prior testing around #1337.  I guess another possible change is JDK version? I see you are using Java 9. I'm now testing on a recent version of Open JDK 8:\ntext\nopenjdk version \"1.8.0_162\"\nOpenJDK Runtime Environment (build 1.8.0_162-b12)\nOpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)\nIf you want to pursue this, I would suggest attempting to reduce it to a simple non-puma test ruby script that exits by explicitly raised SignalException:\nruby\nraise SignalException, \"SIGTERM\"\nIf you can get that to fail in the same way on jruby, then you could report the issue there.. Sadly, proper exit code isn't tested on jruby, presumably due to lack of fork support. Couldn't this be done instead with a thread in the jruby case? cc: @shayonj \nhttps://github.com/puma/puma/blame/c87bbffdb037e379844fae76a775f15e6c273110/test/test_integration.rb#L242. ",
    "Preen": "What was the error here?\n. ",
    "shilpi230": "I am also getting the same error. Any fixes please?\n. ",
    "donv": "``` ruby\nsource 'https://rubygems.org'\ngem 'rails', '~>4.2.4'\nplatform :jruby do\n  gem 'activerecord-jdbcpostgresql-adapter'\n  gem 'rmagick4j'\n  gem 'therubyrhino'\nend\nplatform :ruby do\n  gem 'pg'\n  gem 'rmagick'\n  gem 'therubyracer'\nend\ngem 'bootstrap-datepicker-rails'\ngem 'bootstrap3-datetimepicker-rails'\ngem 'bootstrap-sass'\ngem 'coffee-rails'\ngem 'dynamic_form'\ngem 'gruff'\ngem 'jquery-rails'\ngem 'momentjs-rails'\ngem 'sass-rails'\ngem 'schema_plus'\ngem 'puma'\ngem 'uglifier'\ngem 'will_paginate'\ngroup :development do\n  gem 'capistrano'\n  gem 'capistrano-bundler'\n  gem 'capistrano-rails'\n  gem 'capistrano-rvm'\n  gem 'capistrano-scm-copy'\nend\ngroup :test do\n  gem 'minitest-reporters'\n  gem 'rubocop'\n  gem 'simplecov'\nend\n. ruby\nGEM\n  remote: https://rubygems.org/\n  specs:\n    actionmailer (4.2.4)\n      actionpack (= 4.2.4)\n      actionview (= 4.2.4)\n      activejob (= 4.2.4)\n      mail (~> 2.5, >= 2.5.4)\n      rails-dom-testing (~> 1.0, >= 1.0.5)\n    actionpack (4.2.4)\n      actionview (= 4.2.4)\n      activesupport (= 4.2.4)\n      rack (~> 1.6)\n      rack-test (~> 0.6.2)\n      rails-dom-testing (~> 1.0, >= 1.0.5)\n      rails-html-sanitizer (~> 1.0, >= 1.0.2)\n    actionview (4.2.4)\n      activesupport (= 4.2.4)\n      builder (~> 3.1)\n      erubis (~> 2.7.0)\n      rails-dom-testing (~> 1.0, >= 1.0.5)\n      rails-html-sanitizer (~> 1.0, >= 1.0.2)\n    activejob (4.2.4)\n      activesupport (= 4.2.4)\n      globalid (>= 0.3.0)\n    activemodel (4.2.4)\n      activesupport (= 4.2.4)\n      builder (~> 3.1)\n    activerecord (4.2.4)\n      activemodel (= 4.2.4)\n      activesupport (= 4.2.4)\n      arel (~> 6.0)\n    activerecord-jdbc-adapter (1.3.18)\n      activerecord (>= 2.2)\n    activerecord-jdbcpostgresql-adapter (1.3.18)\n      activerecord-jdbc-adapter (~> 1.3.18)\n      jdbc-postgres (>= 9.1)\n    activesupport (4.2.4)\n      i18n (~> 0.7)\n      json (~> 1.7, >= 1.7.7)\n      minitest (~> 5.1)\n      thread_safe (~> 0.3, >= 0.3.4)\n      tzinfo (~> 1.1)\n    ansi (1.5.0)\n    arel (6.0.3)\n    ast (2.1.0)\n    astrolabe (1.3.1)\n      parser (~> 2.2)\n    autoprefixer-rails (6.0.3)\n      execjs\n      json\n    bootstrap-datepicker-rails (1.4.0)\n      railties (>= 3.0)\n    bootstrap-sass (3.3.5.1)\n      autoprefixer-rails (>= 5.0.0.1)\n      sass (>= 3.3.0)\n    bootstrap3-datetimepicker-rails (4.15.35)\n      momentjs-rails (>= 2.8.1)\n    builder (3.2.2)\n    capistrano (3.4.0)\n      i18n\n      rake (>= 10.0.0)\n      sshkit (~> 1.3)\n    capistrano-bundler (1.1.4)\n      capistrano (~> 3.1)\n      sshkit (~> 1.2)\n    capistrano-rails (1.1.3)\n      capistrano (~> 3.1)\n      capistrano-bundler (~> 1.1)\n    capistrano-rvm (0.1.2)\n      capistrano (~> 3.0)\n      sshkit (~> 1.2)\n    capistrano-scm-copy (0.7.0)\n      capistrano (~> 3.0)\n    coffee-rails (4.1.0)\n      coffee-script (>= 2.2.0)\n      railties (>= 4.0.0, < 5.0)\n    coffee-script (2.4.1)\n      coffee-script-source\n      execjs\n    coffee-script-source (1.9.1.1)\n    colorize (0.7.7)\n    docile (1.1.5)\n    dynamic_form (1.1.4)\n    erubis (2.7.0)\n    execjs (2.6.0)\n    globalid (0.3.6)\n      activesupport (>= 4.1.0)\n    gruff (0.6.0)\n      rmagick (>= 2.13.4)\n    gruff (0.6.0-java)\n      rmagick4j (>= 0.3.9)\n    i18n (0.7.0)\n    its-it (1.1.1)\n    jdbc-postgres (9.4.1200)\n    jquery-rails (4.0.5)\n      rails-dom-testing (~> 1.0)\n      railties (>= 4.2.0)\n      thor (>= 0.14, < 2.0)\n    json (1.8.3)\n    json (1.8.3-java)\n    key_struct (0.4.2)\n    libv8 (3.16.14.11)\n    loofah (2.0.3)\n      nokogiri (>= 1.5.9)\n    mail (2.6.3)\n      mime-types (>= 1.16, < 3)\n    mime-types (2.6.2)\n    mini_portile (0.6.2)\n    minitest (5.8.0)\n    minitest-reporters (1.1.0)\n      ansi\n      builder\n      minitest (>= 5.0)\n      ruby-progressbar\n    modware (0.1.2)\n      its-it\n      key_struct (~> 0.4)\n    momentjs-rails (2.10.3)\n      railties (>= 3.1)\n    net-scp (1.2.1)\n      net-ssh (>= 2.6.5)\n    net-ssh (2.9.2)\n    nokogiri (1.6.6.2)\n      mini_portile (~> 0.6.0)\n    nokogiri (1.6.6.2-java)\n    parser (2.2.2.6)\n      ast (>= 1.1, < 3.0)\n    pg (0.18.3)\n    powerpack (0.1.1)\n    puma (2.14.0)\n    puma (2.14.0-java)\n    rack (1.6.4)\n    rack-test (0.6.3)\n      rack (>= 1.0)\n    rails (4.2.4)\n      actionmailer (= 4.2.4)\n      actionpack (= 4.2.4)\n      actionview (= 4.2.4)\n      activejob (= 4.2.4)\n      activemodel (= 4.2.4)\n      activerecord (= 4.2.4)\n      activesupport (= 4.2.4)\n      bundler (>= 1.3.0, < 2.0)\n      railties (= 4.2.4)\n      sprockets-rails\n    rails-deprecated_sanitizer (1.0.3)\n      activesupport (>= 4.2.0.alpha)\n    rails-dom-testing (1.0.7)\n      activesupport (>= 4.2.0.beta, < 5.0)\n      nokogiri (~> 1.6.0)\n      rails-deprecated_sanitizer (>= 1.0.1)\n    rails-html-sanitizer (1.0.2)\n      loofah (~> 2.0)\n    railties (4.2.4)\n      actionpack (= 4.2.4)\n      activesupport (= 4.2.4)\n      rake (>= 0.8.7)\n      thor (>= 0.18.1, < 2.0)\n    rainbow (2.0.0)\n    rake (10.4.2)\n    ref (2.0.0)\n    rmagick (2.15.4)\n    rmagick4j (0.3.9-java)\n    rubocop (0.34.1)\n      astrolabe (~> 1.3)\n      parser (>= 2.2.2.5, < 3.0)\n      powerpack (~> 0.1)\n      rainbow (>= 1.99.1, < 3.0)\n      ruby-progressbar (~> 1.4)\n    ruby-progressbar (1.7.5)\n    sass (3.4.18)\n    sass-rails (5.0.4)\n      railties (>= 4.0.0, < 5.0)\n      sass (~> 3.1)\n      sprockets (>= 2.8, < 4.0)\n      sprockets-rails (>= 2.0, < 4.0)\n      tilt (>= 1.1, < 3)\n    schema_auto_foreign_keys (0.1.0)\n      schema_plus_foreign_keys (~> 0.1)\n      schema_plus_indexes (~> 0.2)\n    schema_monkey (2.1.1)\n      activerecord (~> 4.2)\n      its-it\n      modware (~> 0.1)\n    schema_plus (2.0.0)\n      activerecord (~> 4.2)\n      schema_auto_foreign_keys (~> 0.1)\n      schema_monkey (~> 2.1)\n      schema_plus_columns (~> 0.1)\n      schema_plus_core (~> 0.4)\n      schema_plus_db_default (~> 0.1)\n      schema_plus_default_expr (~> 0.1)\n      schema_plus_enums (~> 0.1)\n      schema_plus_foreign_keys (~> 0.1)\n      schema_plus_indexes (~> 0.1, >= 0.1.3)\n      schema_plus_pg_indexes (~> 0.1, >= 0.1.3)\n      schema_plus_tables (~> 0.1)\n      schema_plus_views (~> 0.1)\n    schema_plus_columns (0.1.0)\n      activerecord (~> 4.2)\n      schema_plus_indexes (~> 0.1)\n    schema_plus_core (0.6.0)\n      activerecord (~> 4.2)\n      schema_monkey (~> 2.1)\n    schema_plus_db_default (0.1.0)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.2, >= 0.2.1)\n    schema_plus_default_expr (0.1.0)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.2, >= 0.2.1)\n    schema_plus_enums (0.1.0)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.2, >= 0.2.1)\n    schema_plus_foreign_keys (0.1.2)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.5)\n      valuable\n    schema_plus_indexes (0.2.0)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.1)\n    schema_plus_pg_indexes (0.1.5)\n      activerecord (~> 4.2)\n      schema_plus_indexes (~> 0.1, >= 0.1.3)\n    schema_plus_tables (0.1.0)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.2)\n    schema_plus_views (0.3.0)\n      activerecord (~> 4.2)\n      schema_plus_core (~> 0.1)\n    simplecov (0.10.0)\n      docile (~> 1.1.0)\n      json (~> 1.8)\n      simplecov-html (~> 0.10.0)\n    simplecov-html (0.10.0)\n    sprockets (3.3.4)\n      rack (~> 1.0)\n    sprockets-rails (2.3.3)\n      actionpack (>= 3.0)\n      activesupport (>= 3.0)\n      sprockets (>= 2.8, < 4.0)\n    sshkit (1.7.1)\n      colorize (>= 0.7.0)\n      net-scp (>= 1.1.2)\n      net-ssh (>= 2.8.0)\n    therubyracer (0.12.2)\n      libv8 (~> 3.16.14.0)\n      ref\n    therubyrhino (2.0.4)\n      therubyrhino_jar (>= 1.7.3)\n    therubyrhino_jar (1.7.6)\n    thor (0.19.1)\n    thread_safe (0.3.5)\n    thread_safe (0.3.5-java)\n    tilt (2.0.1)\n    tzinfo (1.2.2)\n      thread_safe (~> 0.1)\n    uglifier (2.7.2)\n      execjs (>= 0.3.0)\n      json (>= 1.8.0)\n    valuable (0.9.9)\n    will_paginate (3.0.7)\nPLATFORMS\n  java\n  ruby\nDEPENDENCIES\n  activerecord-jdbcpostgresql-adapter\n  bootstrap-datepicker-rails\n  bootstrap-sass\n  bootstrap3-datetimepicker-rails\n  capistrano\n  capistrano-bundler\n  capistrano-rails\n  capistrano-rvm\n  capistrano-scm-copy\n  coffee-rails\n  dynamic_form\n  gruff\n  jquery-rails\n  minitest-reporters\n  momentjs-rails\n  pg\n  puma\n  rails (~> 4.2.4)\n  rmagick\n  rmagick4j\n  rubocop\n  sass-rails\n  schema_plus\n  simplecov\n  therubyracer\n  therubyrhino\n  uglifier\n  will_paginate\nBUNDLED WITH\n   1.10.6\n```\n. Hi again!\nThe log was fine, my expectation of its location was wrong.  Using capistrano, I expected the log to be in <app_dir>/shared/log but it was still in <app_dir>/current/log.  When I found it, the log contained the relevant error message.  It turned out to be the switch from marshal to jsoncookie serializer.\nThank you for being my rubber ducks :smiley: \n. I can confirm the issue on our system, CentOS release 5.11 + jruby-9.0.5.0\n. Puma 2.16.0 is working for us.\n. @evanphx Thanks!  Looking forward to it.  Puma is great!\n. @evanphx Awesome!\n. I can confirm the fix OK :+1: in our production test environment.  Thanks!\nWill let you know about any JRuby issues :smile: \nWell, actually, I'll submit a trivial consmetic issue right away: https://github.com/puma/puma/issues/923\n. @evanphx BTW: the tests are failing for JRuby in travis:  https://travis-ci.org/puma/puma\n. Thanks!\n. ",
    "dunyakirkali": "+1\n. ",
    "eliank": "I'm encountering similar issues, the requests aren't sent to an idle worker but instead to a worker that is already handling a request. This wouldn't be an issue if the worker threads handled requests concurrently, but in conjunction with MRI it doesn't\n. ",
    "vterdunov": "Looks like Puma can't store a .sock file on a Virtual Box Shared Folder\nvia http://stackoverflow.com/questions/16388342/unicorn-fails-to-start-on-vagrant-box-due-to-errnoeperm\n. ",
    "perlun": "@mwpastore Did you find any reason for this difference?. I can confirm that the @rvictory-suggested hack \"works\" in the sense that the error I was previously getting went away:\n\nError reached top of thread-pool: undefined method `init?' for # (NoMethodError)\n\nIt would be great if someone could carry #1146 to completion.. However, I still get errors like this after each HTTPS request:\n2018-04-24 09:55:04 +0300: Read error: #<EOFError: EOFError>\n/Users/plundberg/.rvm/gems/jruby-9.1.17.0/gems/puma-3.11.3-java/lib/puma/client.rb:292:in `try_to_finish'\n/Users/plundberg/.rvm/gems/jruby-9.1.17.0/gems/puma-3.11.3-java/lib/puma/client.rb:106:in `reset'\n/Users/plundberg/.rvm/gems/jruby-9.1.17.0/gems/puma-3.11.3-java/lib/puma/server.rb:450:in `process_client'\n/Users/plundberg/.rvm/gems/jruby-9.1.17.0/gems/puma-3.11.3-java/lib/puma/server.rb:302:in `block in run'\n/Users/plundberg/.rvm/gems/jruby-9.1.17.0/gems/puma-3.11.3-java/lib/puma/thread_pool.rb:120:in `block in spawn_thread'. FWIW, I just happened to verify this. Our use case is slightly different: we have a magic URL that runs this code:\nruby\n    Process.kill('USR2', Process.pid)\nThis works fine on both MRI and JRuby. @zouqilin, could you try it out in your use case?. (The same behavior exists when restarting via USR1 btw)\nI experimented a bit and created a dummy config.ru file, and started puma like this instead:\nshell\n$ bundle exec puma -b tcp://127.0.0.1:9292\nThen I gave it a USR1 signal:\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.4.1-p111), codename: Private Caller\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://127.0.0.1:9292\nUse Ctrl-C to stop\n* phased-restart called but not available, restarting normally.\n* Restarting...\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.4.1-p111), codename: Private Caller\n* Min threads: 0, max threads: 16\n* Environment: development\n* Inherited tcp://127.0.0.1:9292\nUse Ctrl-C to stop\nAs can be seen, it used the right port here; it properly handled the USR1 signal.\nSo I am suspecting it is specifically when puma is started up via the Rack::Server.new approach that the options originally passed is not properly propagated through to the new process when it receives the restart signal, or something similar to this. @nateberkopec, any ideas on how I might help nailing it down further?. @stereobooster \n\nsmall example with reproduction based on Rack::Server.new?\n\nHere goes:\n```ruby\nconfig.ru\nclass HelloWorld\n  def call(env)\n    [200, {\"Content-Type\" => \"text/html\"}, [\"Hello World!\"]]\n  end\nend\nrun HelloWorld.new\nserver.rb\n!/usr/bin/env ruby\nrequire 'rack'\nserver = Rack::Server.new(\n  config: 'config.ru',\n  server: 'puma',\nPort: ARGV[0]\n)\nserver.start\n```\nHere is the output when running this. Note how it forgets the port number when restarted. I would guess this is because the ARGV values are not propagated to the new process on restart or something? If you have a workaround, I'm more than willing to try it!\nshell\n$ bundle exec ruby server.rb 12345\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.4.1-p111), codename: Private Caller\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://0.0.0.0:12345\nUse Ctrl-C to stop\n* Restarting...\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.4.1-p111), codename: Private Caller\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://0.0.0.0:9292\n* Closing unused inherited connection: tcp://0.0.0.0:12345\nUse Ctrl-C to stop\n. @stereobooster Any ideas?. Thanks @nateberkopec, sorry for the delay on my part. Our problem is that we do not only use the command line arguments for the port number, but we also have other things like --license-file and --apps-folder being passed in via command line arguments.\nWe can use ENV variables for all of this, but it feels a bit clumsy. Are you saying this is the expected behavior, the command line arguments are expected to be erased between restarts? Is this something that could be fixed, would you accept PRs that changes the behavior or do you consider this to be \"the way it should behave\"?. > Well, it does work when you use Puma::CLI, which is what is supported.\nAh, I see - thanks for making that super-clear! Keep up the great work here and elsewhere, @nateberkopec. \ud83d\udc4d . @nateberkopec Trying out this approach now, but I am struggling with getting it accepting my config.ru:\nruby\n    def start_server\n      config_path = File.join(UxFactory::ServerConfig.instance_path, 'server/config.ru')\n      cli = Puma::CLI.new([\"--port=12345 #{config_path}\"])\n      cli.run\n    end\n...but this is what it outputs:\nPuma starting in single mode...\n* Version 3.10.0 (ruby 2.4.2-p198), codename: Russell's Teapot\n* Min threads: 0, max threads: 16\n* Environment: development\nERROR: No application configured, nothing to run\nThe config.ru looks roughly like this, somewhat simplified:\n```ruby\nrackup = Rack::Builder.app do\n  use Rack::Chunked\nuse Rack::Deflater unless UxFactory::ServerConfig.disable_compression\n  run UxFactory::Server::SinatraApp.new\nend\nrun rackup\n```\nbundle exec puma full/path/config.ru gives other output (since not all the dependencies are loaded if I do it that way). Any obvious thing I'm doing wrong here?. Anyone?. Doh! Silly me. \ud83d\ude03 Thanks, will try that instead.. FWIW, the Puma::CLI.new approach did not work, since it will start the original binary with the Puma arguments on restart - which doesn't work, since these arguments are not what the original executable expected:\n$ bundle exec uxfactory --port 12345\nINFO  [2017-10-05 11:09:10.984] UxFactoryServer: Version 8.7.3 started (powered by Puma 3.10.0 and ruby 2.4.2)\nINFO  [2017-10-05 11:09:10.984] UxFactoryServer: Listening at http://localhost:12345.\nINFO  [2017-10-05 11:09:10.997] UxFactory::BackgroundWorkersManager: Started 1 background workers (ConsoleLogger).\nERROR: \"uxfactory start\" was called with arguments [\"--quiet\", \"/Volumes/extra/git/ecraft/ecraft.uxfactory.server-8.x/src/server/config.ru\"]\nUsage: \"uxfactory start [--port=12345] [--license-file=/my/license/file] [--apps_folder=~/git]\"\n~Will try the Puma::Launcher approach instead, which seems (at first glance) to work better (but involves a bit more plumbing.)~ Puma::Launcher worked fine, thanks for suggesting that.. > Shouldn't hot restarts work with UNIX servers? At least since latest JRuby, UNIX Sockets can survive the fork+exec cycle.\nI'd be really happy if they did - how can I enable it? (it doesn't seem to be enabled by default, if it is available)\nRolling restarts cannot work on JRuby, because of the lack of fork in JRuby. I tried disabling the code in Puma that performs the check and that's exactly the error I got:\nNotImplementedError: fork is not available on this platform\n                    fork at org/jruby/RubyKernel.java:1736\n  block in spawn_workers at /Users/plundberg/.rvm/gems/jruby-9.1.15.0/gems/puma-3.11.2-java/lib/puma/cluster.rb:126\n                   times at org/jruby/RubyFixnum.java:299\n           spawn_workers at /Users/plundberg/.rvm/gems/jruby-9.1.15.0/gems/puma-3.11.2-java/lib/puma/cluster.rb:122\n                     run at /Users/plundberg/.rvm/gems/jruby-9.1.15.0/gems/puma-3.11.2-java/lib/puma/cluster.rb:453\n                     run at /Users/plundberg/.rvm/gems/jruby-9.1.15.0/gems/puma-3.11.2-java/lib/puma/launcher.rb:183. Ping @HoneyryderChuck, any ideas?. > The issue anecdotally seems more common on jruby-9.1.16, but at least with my test harness, the problem is intermittent: only sometimes do I see the same stack trace and the error code 1 (which in my case, gets logged as a failure in systemd.)\nThat's actually exactly what I'm seeing as well - intermittent CI failures in my specs! Interesting. With the rackup use case mentioned above though, it fails every single time for me (I tested 5 times now, and the exit code is 1 every single time.)\n\nI can reproduce the issue on jruby 9.1.14 and 9.1.12 as well (though it seems less frequent). My test harness is with puma 3.10.0, so this isn't a very recent puma regression. I'm not sure how or if I missed this issue in my prior testing around #1337. I guess another possible change is JDK version? I see you are using Java 9. I'm now testing on a recent version of Open JDK 8:\n\nActually, as mentioned above, I did test it with both OpenJDK 8 and 9. The \"five times\" mentioned now is with Java 8.\n\nIf you want to pursue this, I would suggest attempting to reduce it to a simple non-puma test ruby script that exits by explicitly raised SignalException:\nruby\nraise SignalException, \"SIGTERM\"\nIf you can get that to fail in the same way on jruby, then you could report the issue there.\n\nInterestingly enough, it does indeed fail for me consistently. Even with a 1.7 JRuby, it does.\nIf you manage to add tests for it here, that's great - I think I'll file a JRuby issue about this as well, since it does indeed seem to be an upstream issue. Thanks for your feedback thus far!. @nateberkopec Thanks for asking. I think it might not be, since I've unfortunately still have seen intermittent CI failures. Will try to revisit this next week to see how it behaves w/ latest JRuby and keep you posted.. @BlackNightFury MiniSSL is not a concept in itself, more than the class name of Puma's SSL implementation which can be found here: https://github.com/puma/puma/blob/master/lib/puma/minissl.rb\nI don't have much experience with it myself, but is currently looking into start using Puma's SSL support for development & production usage. To the best of my knowledge \"it works\" (even though I've seen some exceptions & weird behavior; will file issues about them if I can reproduce it.)\nWhat are the specific things in \"OpenSSL\" (a C library used for implementing SSL/TLS support) that are missing in Puma? If you make a list and be very specific, I am sure you will get more feedback from the project maintainers. (I am not a maintainer myself.). @BlackNightFury \n\nSo we have been using OpenSSL and our project needs strong security.\n\nPlease define \"strong security\".\n\nWe planned to move from thin to puma because of concurrency issue but we also could see that Puma is using MiniSSL not OpenSSL.\nSo can you guarantee that MiniSSL is same as OpenSSL?\n\nI can most certainly guarantee that MiniSSL is not the same as OpenSSL. OpenSSL is a library which implements the SSL/TLS protocols. LibreSSL is another library which was forked from OpenSSL a few years ago.\nCan you guarantee that LibreSSL is the same as OpenSSL? Probably mostly yes, since they share the same ancestry, but there might still be subtle differences.\nCan you guarantee that Puma's MiniSSL implementation is the same as OpenSSL? No, it's a different implementation of its own.\nI guess what you're really asking is whether Puma's MiniSSL implementation is secure enough. To that, I cannot really answer - @evanphx or @nateberkopec are probably much better suited to answer that. (I would also be personally interested in the answer btw \ud83d\ude04). @BlackNightFury I think if you share some code example of what you are trying to achieve, it will be easier for me or others to help you. What are you doing more specifically with OpenSSL::X509::Store?\n(I am currently using Puma with SSL, it seems to work pretty fine - great on MRI, almost great on JRuby.). ",
    "robdimarco": "FWIW, we have been running this in production for the past 3 weeks with no issues.\n. After upgrading from 2.16.0 to 3.0.2, I am seeing this behavior again. We do have the directory set in the puma config file as per the instructions to #770 \n. I figure out how to work around our problem. We are using upstart and I had in there a\nchdir /mnt/deploy/app/current\nTo get the reload to work correctly, I had to explicitly set and export the present working directory env variable with\nenv PWD=/mnt/deploy/app/current\nexport PWD\nApparently upstart does not set the PWD when you use, which I learned reading #539\n. @allaire one other thing. I had to stop and start via upstart. When I just did a restart of the process, it did not pick up my change. Maybe that is it.\n. ",
    "unleashed": "This is not strictly needed but offers better ergonomics, specially when most configuration file parameters have equivalent arguments for the CLI.\nI have a use case in my workplace where I have one single puma config file with common parameters used in all environments. I fill in the rest of the parameters via command line arguments.\nDetermining some of the remaining parameters in some environments is cumbersome to do when puma is launching. In my case I derive some things such as the number of workers just doing some simple checks with the available hardware info exposed by the OS at launch time, and I just pass in -w n. However, the destination storage for logs is quite dynamic based on conditions not only tied to the environment (ie. whether puma is running inside a docker container, availability of network shares, etc).\nAdding the corresponding command line switches helps a lot in not maintaining extra config files and avoids hacking in some ugly text replacement before loading them with puma.\nOTOH I guess the options themselves are pretty verbose. I chose to maintain their original name, but would be happier if I could convey their meaning with far fewer characters or if a shortcut was agreed upon.\nEdit: dropping the \"redirect\" part may be way more palatable.\n. This used to happen to me, but it went away for me using --dir option in CLI. You should try the latest version and see if this is now fixed.\n. Just a heads up: this is a bug that causes high spikes in latency if you use Puma in non multithreaded applications.. At this phase clients are potentially sending another request on keepalive. The value of this timeout makes Puma (with threads=1) sit idle waiting on this potential next request from this specific client leaving everything else unattended. That is currently 200ms of idling per worker process. The reset call on the client object will return falsey and further IO will be processed later on just like any other IO. This code is there just to handle the fast keepalive case in which data is ready to be processed. This is arguably an optimization, but not a good one when you are limited in threads. In fact, one single client per worker process can DoS Puma without multithreading by sending keepalive requests with the right timing: right under 200ms for the next request is enough to keep it busy with the same client all the time.. ",
    "jchatel": "I have some too. I moved 2 weeks ago to Puma.\nI have multiple Heroku dynos (5 for web). And my config is as recommended on Heroku (2 Workers, 5 Threads). I don't have any time out because my requests don't take long, there is the occasional db taking a bit of time to serve the request but that's all.\nYet since I moved to puma I had a few occurrences of blocked dynos.\nheroku router - - at=error code=H12 desc=\"Request timeout\" method=GET path=\"[simple request here]\" host=[my domain] request_id=ae10b6f6-0028-4393-b99a-a104353c0f87 fwd=\"84.19.57.4\" dyno=web.1 connect=0ms service=30000ms status=503 bytes=0\nand hours later, web.1 still failing. May have been an underlying problem before when I was using webbrick (default) and I did not configured timeout in rack but that's not good. I don't see why any request would take so long to execute.\nI do have one lock on the database (with_lock do) to prevent multiple threads to update the same object list, but it only sets some flags (unlikely to time out, and the other web.X are all working on Heroku, so no deadlock).\nThinking of going back to webbrick :(\n. Yes I do.\n. Did that stop the H12 errors for you? I hardly use New Relic (except when I need it of course :/)\n. For me, I stopped using New Relic and used rack-timeout in production. No problem since.\n. Honestly, I wasn't using new relic, the things that interested me were badly priced (too high for my usage). I just removed it.\n. ",
    "guillaumebesse": "Do you use New Relic ? We had issues with it and puma. \n. we stop using New Relic after Puma migration... \n. We had a lot of H12, H13 and R12 errors. The outages were chaotic and restarting the web app (eventually several times) fixed temporary the problem. \nWe also had a hight level of postgres memory cache usage in our rails app :  it is \na bug in prepared statements in activerecord. We didn't try to re-introduce New Relic after all this difficulties.\n. ",
    "larchiu": "@vincentwoo / @schneems , were you able to solve this issue? We're getting this H12 problem happening at least once a week on a static page in Heroku with one of our two 1x dynos. :(  The issue does not stop until we do a dyno restart.\nWe have already reached out to Heroku support and they mentioned that it might be a dead Puma worker and couldn't figure out what the root cause is exactly. We're running New Relic and our average transaction times are 100ms or lower. This issue really baffles and concerns me. :(\n. @evanphx/all, thanks for your response. \n@evanphx, we currently have puma (2.15.3) installed now on Heroku.  Was there a particular fix in v2.16.0 that would potentially solve request timeout issues in Heroku?\n. We had the same recurring H12 errors  for v2.15.3. \nThe problem went away after the upgrade to  v2.16.\n. ",
    "benoittgt": "@jchatel We have some issues too. We use a lot newrelic. Did you contact them before removing the service? \n. @Baril27 Which version of puma and ruby are you using ?\n. Thanks @baril27 ! \n. Yep. We switch from ruby-2.2.2  to ruby-2.3.0. I will keep you updated about this in the next two weeks to see if the optimisation is confirmed. \nThanks !\n. @evanphx Seems still good. Thanks !\n. Hello @epinault \nSee : https://github.com/puma/puma/pull/1068\n\ud83c\udf89 \n. No repro so closing the issue.. @kyle776 I think we upgrade ruby, remove spring, clear bootsnap cache. :). Did you check https://github.com/schneems/derailed_benchmarks ? Also you can warp your calls into https://github.com/SamSaffron/memory_profiler\nBut I don't think puma is the root cause of your issue. Also you said \"and memory do not return to system.\", I think you should read : https://github.com/puma/puma/issues/1047 (especially big comments from Nate and Richard). Please @BlackNightFury this is an open source project. No one is pay to maintain this. \n\nSo please help me to fix this issue asap\n\nI don't think \"asap\" is needed if your situation is critical. :)\nAlso if you could provide a repository with a script to reproduce the issue it will help to look at the issue.\n. ",
    "Baril27": "I am getting this same issue after upgrading to puma on heroku. I am using rack-timeout and newrelic as well. Looking at the logs the H12's occur at random points and well below the 29 second cutoff in rack-timeout.\n. @benoittgt \nCurrently on:\nruby '2.3.0'\npuma '2.14.0'\nI am going to bump puma to 2.16.0 see how that goes first stay tuned!\n. So little update. After upgrading to puma v2.16.0 I have not seen the random timeouts for the last two days. Going to say for me its fixed.\n. ",
    "wcurtis": "Unfortunately we've been seeing the same problem with v2.16 in prod. Ruby 2.3.0.\n. ",
    "klemenkobetic": "similar problems:\nhttps://github.com/felixge/node-mysql/issues/821\n. Hi!\nWe used this gem and we are connecting to the database as written in gem documentation  :\nhttps://rubygems.org/gems/pg/versions/0.18.2\n. ",
    "ccastaneda13": "hi @klemenkobetic , ran into the same issue. Did you find a workaround for this or some health check script to auto restart the puma servers on the failover?. ",
    "grepruby": "Having same issue with my production app. When I start puma, everything works fine. Log appears in production.log however over the time, it doesnt log anything in log file. \n. @evanphx Rails log are blank. \n. I am not using upstart instead I am using god script to start/stop puma. Also we are not rotating log of god/puma , so not sure in our case what is causing issue. \nMy sidekiq workers keep writing in log file, regardless what puma does. So clear it has something with puma. \nPlease advice. \n. @evanphx \n1 Ubuntu 14\n2 Installed God through  gem. I am using rvm to manage ruby.\n3 No. Not redirecting stderr or stdout\nMy God script looks like below\n```\nRAILS_ROOT='/var/www/apps/wl-event/current'\nSHARED_ROOT='/var/www/apps/wl-event/shared'\nSTATE = \"#{SHARED_ROOT}/log/puma.state\"\nSOCK = \"unix://#{SHARED_ROOT}/tmp/puma.sock\"\nPID = \"#{SHARED_ROOT}/tmp/puma.pid\"\nGod.watch do |w|\n  w.name = 'wl-event'\n  w.dir = \"#{RAILS_ROOT}\"\n  w.log = \"#{SHARED_ROOT}/log/god.log\"\n  w.pid_file = File.join(SHARED_ROOT, \"/tmp/puma.pid\")\n  w.start = \"cd #{RAILS_ROOT}; bundle exec puma -b #{SOCK} -C #{RAILS_ROOT}/config/puma.rb --pidfile #{PID}\"\n  w.stop = \"cd #{RAILS_ROOT}; bundle exec pumactl -p cat #{w.pid_file} stop\"\nw.behavior(:clean_pid_file)\n  w.start_if do |start|\n    start.condition(:process_running) do |c|\n      c.interval = 5.seconds\n      c.running = false\n    end\n  end\n  w.keepalive\nend\n```\n. ",
    "micfan": "I have install puma-manager plugin by digital ocean guide one month ago, and it works well.\nHowever, if only the system wide cmd was packaged to apt-get or the same ones should be easy to install & config.\n. @jmacmullin Why this occurs in Mac machine? My Ruby version is RVM.ruby-2.2.1\n. $ rackup -h\nRack options:\n  -s, --server SERVER      serve using SERVER (thin/puma/webrick/mongrel)\n  -o, --host HOST          listen on HOST (default: localhost)\n...\n@Nowaker I think your are using an args -o which has been overrode by thethinserver. Of cause,pumado not offer this feature. Config your .socket file in config.rb https://github.com/puma/puma/blob/v2.16.0/examples/config.rb#L75\n. Yeah, we assume this is awell-recognized convention` on Linux. \nBut please check puma source code, you may find puma do not implement a rack/handler in the well-recognized convention.\nAnd you will find thin's implement with override select_backend() and def select_backend()\nAny PR?\n. - This is a question of Rails, not puma. If you play both server and client why not do server inner call? If servers are independent, you have to call each other using http-client.\n- The best way is redirect_to\n-  Read docs: http://guides.rubyonrails.org/action_controller_overview.html#flash-now\n- CLOSE it pls\n. ",
    "Taytay": "I've ended up writing this code and putting it in my config.ru. I got the base of the code from @vincentwoo here : https://github.com/puma/puma/issues/797#issuecomment-147892653\n``` ruby\nmodule PumaThreadLogger\ndef initialize args\n    ret = super args\n    Thread.new do\n      while true\n    # Every X seconds, write out what the state of this dyno is in a format that Librato understands.\n    sleep 5\n\n    thread_count = 0\n    backlog = 0\n    waiting = 0\n\n    # I don't do the logging or string stuff inside of the mutex. I want to get out of there as fast as possible\n    @mutex.synchronize {\n      thread_count = @workers.size\n      backlog = @todo.size\n      waiting = @waiting\n    }\n\n    # For some reason, even a single Puma server (not clustered) has two booted ThreadPools.\n    # One of them is empty, and the other is actually doing work\n    # The check above ignores the empty one\n    if (thread_count > 0)\n\n      # It might be cool if we knew the Puma worker index for this worker, but that didn't look easy to me.\n      # The good news: By using the PID we can differentiate two different workers on two different dynos with the same name\n      # (which might happen if one is shutting down and the other is starting)\n      source_name = \"#{Process.pid}\"\n\n      # If we have a dyno name, prepend it to the source to make it easier to group in the log output\n      dyno_name = ENV['DYNO']\n      if (dyno_name)\n        source_name=\"#{dyno_name}.\"+source_name\n      end\n      msg = \"source=#{source_name} \"\n\n      msg += \"sample#puma.backlog=#{backlog} sample#puma.active_connections=#{thread_count - waiting} sample#puma.total_threads=#{thread_count}\"\n      Rails.logger.info msg\n    end\n  end\nend\nret\n\nend\nend\nmodule Puma\n  class ThreadPool\n    prepend PumaThreadLogger\n  end\nend\n```\n. I think that this is due to a browser side limit, rather than a server side limit. My apologies for the noise!\n. ",
    "jeffblake": "Any chance of getting the control server to report the # of \"actively serving threads\"? It currently reports the count of spawned threads per worker, but not how many are serving a request.\n. ",
    "gucki": "@allaire I read those but I'm not sure as in my case puma seems to load the correct code (follow the new symlink), it just doesn't update the name of the process.\n. ",
    "Paxa": "This http://forum.rubyonrails.pl/t/centos-systemd-puma-service/9808 ?\n. ",
    "jmacmullin": "I get the same error when attempting to install the latest Puma on Mac OS X El Capitan\n. Well, similar:\nio_buffer.c:118:10: warning: passing 'uint8_t ' (aka 'unsigned char ') to parameter of type 'const char *' converts between pointers to integer types with different sign [-Wpointer-sign]\n. ",
    "nikita-barsukov": "238 looks related\n. ",
    "tomohiro": ":+1:\n. ",
    "kwugirl": "Oo thanks for taking care of it @evanphx!\n. ",
    "Nowaker": "I tried with rackup which didn't work either.\n% x rackup -s puma -E development -o ./test.socket config.ru\nPuma 2.15.3 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://./test.socket:9292\n/home/nowaker/.rvm/gems/ruby-2.2.4/gems/puma-2.15.3/lib/puma/binder.rb:234:in `initialize': getaddrinfo: Name or service not known (SocketError)\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/gems/puma-2.15.3/lib/puma/binder.rb:234:in `new'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/gems/puma-2.15.3/lib/puma/binder.rb:234:in `add_tcp_listener'\n        from (eval):2:in `add_tcp_listener'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/gems/puma-2.15.3/lib/rack/handler/puma.rb:33:in `run'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/gems/rack-1.6.4/lib/rack/server.rb:286:in `start'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/gems/rack-1.6.4/lib/rack/server.rb:147:in `start'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/gems/rack-1.6.4/bin/rackup:4:in `<top (required)>'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/bin/rackup:23:in `load'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/bin/rackup:23:in `<main>'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/bin/ruby_executable_hooks:15:in `eval'\n        from /home/nowaker/.rvm/gems/ruby-2.2.4/bin/ruby_executable_hooks:15:in `<main>'\nThe same thing works with Thin without any problem:\n% x rackup -s thin -E development -o ./test.socket config.ru\nThin web server (v1.6.4 codename Gob Bluth)\nMaximum connections set to 1024\nListening on ./test.socket, CTRL+C to stop\nThis seems to work with Puma without any problem:\n% cat config/puma.rb | grep bind\nbind \"unix://./puma.sock\"\n% x puma\n[8420] Puma starting in cluster mode...\n[8420] * Version 2.15.3 (ruby 2.2.4-p230), codename: Autumn Arbor Airbrush\n[8420] * Min threads: 8, max threads: 32\n[8420] * Environment: development\n[8420] * Process workers: 1\n[8420] * Preloading application\n[8420] * Listening on unix://./puma.sock\n[8420] Use Ctrl-C to stop\n[8420] - Worker 0 (pid: 8423) booted, phase:\n. The host can be a path to socket too, that's a well-recognized convention on Linux. Thin follows the convention. Example from PostgreSQL:\n-h, --host=HOSTNAME      database server host or socket directory (default: \"local socket\")\n. Thanks @evanphx!\n. ",
    "Edwardzyc": "What ruby implementation are you running?\nworker count should optimally be equal to the number of cores on your system(4)\nthread count should be 4-8\nIf anyone thinks otherwise - post a comment!\n. Number of workers should equal number of cores if RAM allows for it. \nDoes that make more sense? (At least on MRI)\n. Can you chart the data for a longer period of time? Your application might just have a high resident memory usage and it hasn't 'capped out'\n. ",
    "senott": "I'm sorry to disagree, but is working fine in production with Unicorn.\n. Ok, give me some time to prepare a virtual machine to make a new deploy changing the server because this is running in production now. Thanks for you time!\n. ",
    "Zapotek": "I also tried it with the PEMs used in testing and got the same results, any feedback on this would be appreciated.\nIf this feature is broken on Windows the platform constraints should be updated to reflect that.\n. ",
    "vais": "@evanphx this is still very much a problem on latest puma and latest Ruby that'a available on Windows (2.3.3). Puma builds on Windows without SSL by default, recompiled with SSL in the same way @Zapotek described, after which puma happily runs bound to SSL as if everything's fine, but the server does not respond when you try to hit it in the browser. Does anyone on the puma core have a Windows machine to look into this issue? If not, is there anything specific I can look into and report here?. ",
    "preetpalS": "@evanphx I also experienced this issue with Puma (puma (3.9.1)) with Ruby 2.4.1 (x64) on Windows. Using Puma (puma (3.9.1-java)) with JRuby 9.1.9.0, I did not encounter this problem (the app was able to serve pages over HTTPS).\n@Zapotek, @vais If your application runs on JRuby, then you can use Puma with SSL on Windows.\nThe following is my Puma config:\n```\nbind 'ssl://192.168.1.50:443?key=cert.key&cert=cert.crt&verify_mode=none'\nWorking JRuby HTTPS config\nbind 'ssl://192.168.1.50:443?keystore=cert.jks&keystore-pass=password&cert.key&cert=cert.crt&verify_mode=none'\n```\nTesting failing version (Puma 3.9.1 on Ruby 2.4.1 (x64)) with Curl (mingw64):\n```\n$ curl -vvv https://192.168.1.50/ -k\n STATE: INIT => CONNECT handle 0x600057470; line 1418 (connection #-5000)\n Added connection 0. The cache now contains 1 members\n   Trying 192.168.1.50...\n TCP_NODELAY set\n STATE: CONNECT => WAITCONNECT handle 0x600057470; line 1471 (connection #0)\n Connected to 192.168.1.50 (192.168.1.50) port 443 (#0)\n STATE: WAITCONNECT => SENDPROTOCONNECT handle 0x600057470; line 1588 (connection #0)\n Marked for [keep alive]: HTTP default\n ALPN, offering http/1.1\n Cipher selection: ALL:!EXPORT:!EXPORT40:!EXPORT56:!aNULL:!LOW:!RC4:@STRENGTH\n successfully set certificate verify locations:\n   CAfile: /usr/ssl/certs/ca-bundle.crt\n  CApath: none\n TLSv1.2 (OUT), TLS header, Certificate Status (22):\n TLSv1.2 (OUT), TLS handshake, Client hello (1):\n STATE: SENDPROTOCONNECT => PROTOCONNECT handle 0x600057470; line 1602 (connection #0)\n TLSv1.2 (IN), TLS handshake, Server hello (2):\n TLSv1.2 (IN), TLS handshake, Certificate (11):\n TLSv1.2 (IN), TLS handshake, Server key exchange (12):\n TLSv1.2 (IN), TLS handshake, Server finished (14):\n TLSv1.2 (OUT), TLS handshake, Client key exchange (16):\n TLSv1.2 (OUT), TLS change cipher, Client hello (1):\n TLSv1.2 (OUT), TLS handshake, Finished (20):\n TLSv1.2 (IN), TLS change cipher, Client hello (1):\n TLSv1.2 (IN), TLS handshake, Finished (20):\n SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384\n ALPN, server did not agree to a protocol\n Server certificate:\n  subject: C=XX; ST=XX; L=XX; O=XX; CN=XX\n  start date: Aug 12 04:49:02 2017 GMT\n  expire date: Aug 10 04:49:02 2027 GMT\n  subject: C=XX; ST=XX; L=XX; O=XX; CN=XX\n  SSL certificate verify result: self signed certificate (18), continuing anyway.\n* STATE: PROTOCONNECT => DO handle 0x600057470; line 1623 (connection #0)\n\nGET / HTTP/1.1\nHost: 192.168.1.50\nUser-Agent: curl/7.53.1\nAccept: /\n\nSTATE: DO => DO_DONE handle 0x600057470; line 1685 (connection #0)\nSTATE: DO_DONE => WAITPERFORM handle 0x600057470; line 1812 (connection #0)\nSTATE: WAITPERFORM => PERFORM handle 0x600057470; line 1822 (connection #0)\nSSL read: error:00000000:lib(0):func(0):reason(0), errno 104\nMarked for [closure]: Transfer returned error\nmulti_done\nClosing connection 0\nThe cache now contains 0 members\nExpire cleared\ncurl: (56) SSL read: error:00000000:lib(0):func(0):reason(0), errno 104\n```. \n\n",
    "RobotJiang": "When I add some codes in before_fork block, It will raise a error (allocator undefined for Proc) when I stop or restart puma server.\n. ",
    "joemiller": "I took a shot at implementing this here:  https://github.com/puma/puma/pull/833\n. The travis tests failed only on ruby 2.2 and it looks like a timeout issue, possibly a transient issue. Can someone help re-run the test? I couldn't find an option to do so as I am not too familiar with travis-ci\n. ping. Any feedback on this PR? Thanks!\n. ",
    "frankwong15": "This is breaking puma when using binder dsl because there is no way to set verify_mode. \n. ",
    "ndeto": "Is there any progress with this? I need to check which host is connecting first before i respond with the right SSL cert. ",
    "bigsur0": "I am seeing strangeness too, when I hit my Sinatra app at \"/\" I get prompted to download a file instead of Sinatra serving the page. No errors in any logs.\n. ",
    "Ch4s3": "Has anyone reported this over at Jruby?\n. ",
    "tpodom": "Looking at the source I think this is an error in documentation and naming. The underlying code registers an after_worker_fork callback which parallels before_worker_fork and it appears both of those callback happen in the master process.  It does not look like there is any support for the documented after_worker_boot.  I'm currently looking to see if there's any workaround because I'm trying to register Bunny consumers after Rails has loaded.\n. @evanphx I saw this was closed but I'm not sure why.  It seems like either after_worker_boot should be renamed to after_worker_fork, or it's documentation should be updated to make it clear that it's after a worker is forked in the master process, or it should be updated to work as described.\n. @evanphx thanks, that will be much more clear going forward\n. ",
    "maxkwallace": "@tpodom Thanks for the reply!\n. @evanphx Thanks! The updated documentation is super helpful. I noticed two tiny inconsistencies so I sent over a PR: https://github.com/puma/puma/pull/956\n. ",
    "niels-s": "I finally found out why my application was behaving the way it was. After trying to use a tcp connection and switching to Unicorn I start looking into other possible sources.\nThat's when I thought maybe my connection to Google Cloud SQL could be the problem. Once I read the faq of Cloud SQL, they mentioned that you have to tweak you Compute instances to ensure they keep open your DB connection. So I performed the next steps they recommend and that solved the problem for me, I added them just in case:\n```\nDisplay the current tcp_keepalive_time value.\n$ cat /proc/sys/net/ipv4/tcp_keepalive_time\nSet tcp_keepalive_time to 60 seconds and make it permanent across reboots.\n$ echo 'net.ipv4.tcp_keepalive_time = 60' | sudo tee -a /etc/sysctl.conf\nApply the change.\n$ sudo /sbin/sysctl --load=/etc/sysctl.conf\nDisplay the tcp_keepalive_time value to verify the change was applied.\n$ cat /proc/sys/net/ipv4/tcp_keepalive_time\n```\nsorry for the misunderstanding\n. ",
    "rilian": "thank you!. ",
    "robomc": "What if there is 32GB of RAM and 2 cores though. \nHeroku's advice suggests you should keep adding workers until you run out of RAM. Is that not the case? Should you stay at just two workers in that situation?\n. ",
    "charlesetc": "Have you tried something like this:\nsudo apt-get install libgmp3-dev\nIt's just a guess from cannot find -lgmp\n. ",
    "osheroff": "@evanphx any chance of a release with the latest?\n. I'm running with the gemfile pointed at dbb1932\n. so should we do something like not_full.signal before the worker exits?\n. our configuration is (load balancer -> local nginx -> puma), 20 workers and a single thread per worker.  Our workload consists of quite a lot of very short requests and a few very very long requests (1-20 seconds.)  My understanding is that if we run with queue_requests = true, a worker that's busy processing a 20 second request maybe accept() a smaller request and then get blocked waiting for it to finish\n. no, that was a mistake.  will revert.\n. fixed\n. ",
    "boxofrad": "My colleague pointed out that it may not be clear that the second require 'rack' is happening when we require 'sinatra' in our config.ru.\nWhile I could remove the require 'rack' from the config.ru in my example app, I cannot remove the require 'sinatra' from my real app.\n. Here's a very hacky workaround (in my puma.rb) that forces the bundler version into the $LOAD_PATH before the system-wide version:\nruby\nif defined?(Bundler)\n  prune_bundler\nelse\n  require 'bundler'\n  $LOAD_PATH.unshift File.join(Bundler.rubygems.find_name('rack').first.full_gem_path, 'lib')\nend\n. Thanks so much @evanphx, sorry about the misinformation!\n. Admittedly this is a dirty dirty solution, and seems to have broken the build on JRuby... So I'm very much open to other suggestions :smile: \n. Closing this in favour of #864\n. ",
    "BRMatt": "@evanphx If you run ./boot it should run puma in a way that reproduces the issue, is this not happening? (I work with Daniel) EDIT: From what I remember we were able to reproduce it using just that command. If that's not working we may need to try specifying the exact ruby/bundler versions.\n. > Oh, I see that the vendor dir requires me to be running ruby 2.1.0. Let me install that and try again.\nDoh! Sorry!\n. @evanphx thanks for merging and the speedy release! :heart: \n. ",
    "JerryGreen": "@evanphx Silly me, thank you! I solved it. The problem was there was no stdout and stderr files.\n. ",
    "pnomolos": "@evanphx I realize that I'm necroing/hijacking this thread, but is there any reason that Puma doesn't bail and spew an error from the main runner on what could be considered fatal errors?  Items such as \"unable to create pid file\", \"unable to create socket file\".  I rarely set up new puma apps so I'm bit by those every time (because I forget what the problem was last time it happened) and burn time trying to figure out what's going on.\n. ",
    "Tom-Tom": "Hi, same problem here\n@rda1902 : Can you tell me how you have launched ActionCable with Thin ?\n. @evanphx thanks for this information.\nOn my side, when i'm on localhost development, i need the SLL everywhere on my rails application.\nI can use Thin to run SSL, just for development. But need to rack ActionCable and cannot use the in-app ActionCable like rails s with Puma and with route mounting. And Thin is not the example in ActionCable documentation in Rails 5 actually.\nIt's sure in production i can easily get SSL with NGINX proxy_pass and launch Puma or Thin for ActionCable without SSL\nThe link of @rda1902 is usefull\nAnd if you want NGINX proxy_pass :\nhttp://stackoverflow.com/questions/32322652/configuring-nginx-to-proxy-thin-and-rails-actioncable\n. ",
    "carlzulauf": "Encountering this issue too. Would really like to use Puma + SSL + ActionCable but currently doesn't work. Websocket connections work fine if I turn off SSL.\n. My reason for direct SSL+Puma for action cable was due to an old version of apache that didn't support mod_proxy_wstunnel. I have since switched to a new server with nginx as a workaround. I noticed as well EM seemed to be attempting to manipulate the socket directly and I wasn't able to work around the issue either. I tried making MiniSSL::Socket provide the fileno response from it's @socket but there are deeper issues it seems.\n. ",
    "YurySolovyov": "Same here. I use puma + ssl + plain rack app running on jruby.\n\nI'd love to hear more about why people are running Puma + SSL for actioncable rather than running SSL in front of puma though.\n\nIt might sound weird, but I do this indeed to drop ssl proxy on front of the app. Nothing to do with actioncable though\n. does app itself continue running? I guess rack env is set to production?\n. ",
    "errm": "I wouldn't want that . . . thats the point I would like this useless configuration to at least say:\n\"Warning: This configuration of max_threads leaves puma without the ability to serve requests\"\nor to exit with a similar message.\nI just stayed up all night trying to work out why the puma process that looked like it was running fine, without any errors in the logs was not accepting any connections.  So its a silly thing really, but 3am me would love some sort of explicit warning why that was happening.\nI can open a PR if you think its a good idea @evanphx \n. What do you prefer a warning or an exit\n. ",
    "stereobooster": "\nNo, there is no way to do this\n\nIt depends on situation. If server already started processing request and client disconnected you can't do anything about it. But if request was in the que and client disconnected before server started to process response it is possible to detect this situation. Unicorn have this feature: check_client_connection. What they do is they use TCP_INFO in Linux/FreeBSD/OpenBSD or try to write partial header before starting worker (and get io error if connection closed). Original idea proposed by Shopify. Also worth to note that detection only works for TCP loopback or unix socket, which is ok, because everybody use it behind nginx.\nUPD: There is check for closed connections https://github.com/puma/puma/blob/master/lib/puma/server.rb#L123. But it works only for linux.\n. You need to use USR1 (or USR2) for restart.  HUP is for log rotation. I wonder if this is the problem with touch tmp/restart.txt here too https://github.com/puma/puma/issues/1060. Need to try with kill -USR2 pid. @herregroen maybe you having the same problem as in https://github.com/puma/puma/issues/1346\nTry to provide redirect_stdout in configuration if this not the case yet.\n. here is reproduction\ntest.rb\nruby\nloop do\n  Signal.trap(\"HUP\") do\n    puts \"HUP\"\n    exit 1\n  end\n  sleep 1\nend\n```bash\nvagrant ssh # or any other ssh to linux box\nnohup ruby test.rb &\nnohup: ignoring input and appending output to 'nohup.out'\nexit\ncat nohup.out\nHUP # nohup sends HUP signal when parent process (ssh session) closed\n```\nThis is documented, that HUP signal server as INT if no redirect_stdout provided. Not a bug. Not reproducible on linux (ubuntu)\nVagrantfile\nruby\nVagrant.configure(2) do |config|\n  config.vm.box = \"ubuntu/xenial64\"\nend\nvagrant ssh\nbundle exec puma --bind unix:///tmp/puma.sock\nPuma starting in single mode...\n* Version 3.7.0 (ruby 2.3.4-p301), codename: Snowy Sagebrush\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on unix:///tmp/puma.sock\nUse Ctrl-C to stop. please rebase, tests were fixed in master. I had something similar (spikes in latency), but in multi-threaded application. I deployed server with 8 workers instead of 2 (error in ansible recipe), my intention was to use number of workers equal to number of processors. And after this I started to notice 20-30 requests with timeouts (after 30s). But later when I noticed error in deployment script and lowered number of workers problem went away.. I would also like to see full integration test with symlink deployment and rolling restart. This what I'm struggling right now.\n\ntest that code reloads\ntest that gem reloads\ntest that env variables reloads\ntest that remove of previous release folder will not break server\n. Also note: this fix will work only for this kind of setup: when vendor folder is symlinked to shared/vendor. > HUP reopen log files defined in stdout_redirect configuration parameter. If there is no stdout_redirect option provided it will behave like INT.\nINT equivalent of sending Ctrl-C to cluster. Will attempt to finish then exit.\nhttps://github.com/puma/puma/blob/master/docs/signals.md\n\n\n\nI suppose mina uses HUP at some point.. master fixed, you can rebase. @nateberkopec do we have tests for those things?. For URLs, like /gc. @grosser by idea this is same as your code, but less hacky\n```ruby\nconfig/puma.rb\non_restart do\n  ENV.replace(Bundler.clean_env)\nend\n```. > However, I think this option could reduce some of the complexity in the Reactor and improve capacity.\nEven without this optimisation, it would be possible to start two pumas on the same port with SO_REUSEPORT.\nSome resources:\n- https://domsch.com/linux/lpc2010/Scaling_techniques_for_servers_with_high_connection%20rates.pdf\n- https://idea.popcount.org/2017-02-20-epoll-is-fundamentally-broken-12/. As per this issue https://github.com/puma/puma/issues/1034\n\nYes, You should start puma without bundle exec.\n\nBut I suppose @grosser is strongly recommend to use it with bundler. What is the proper way? And why?. Experiments with restart https://github.com/stereobooster/ruby-server-experiment/tree/master/puma-symlink. @grosser https://github.com/stereobooster/ruby-server-experiment/tree/master/puma-symlink-grosser\nUPD: @grosser why do you prefer USR2 over USR1 restart?\nPS simplified patch\nruby\nrequire 'bundler/setup'\non_restart do\n  ENV.replace(Bundler.clean_env)\nend. @glebtv \nbash\nruby -v\ngem -v\nbundle -v\nrails -v\nuname -a. Can you try to do testcase for this? Similar code changed https://github.com/puma/puma/pull/1219/files\nAlso rebase - tests were fixed in master. Please add test for this. It is supposed to integration test. By idea you should launch instance of puma in subshell and capture return status from it.\n```\nreturns pid of process\ndef start_puma()\n  fork do\n    exec \"bundle exec puma -C puma_config.rb\"\n  end\nend\ndef stop(pid)\n  Process.kill(\"TERM\", pid)\nend\n```\nneed to google on how to capture respond code in this scenario. Tests are failing. Does your branch based on latest master?. Steps to reproduce or example application would be helpful. What is the result for (in the same dir as bundle exec puma):\nbundle exec ruby -e 'puts ENV[\"RUBYOPT\"]'\n. bash\nruby -v\ngem -v\nbundle -v\nrails -v\npuma --version\nuname -a. This fix tests, which are broken in master \ud83d\udc4d . We can try https://www.appveyor.com/ for windows tests (travis supports MacOS).\nUPD: how about some tests for new code?. @nateberkopec but travis does not support windows. It is https://ci.appveyor.com/ that supports windows . @NickolasVashchenko tests failing (RuntimeError: can't modify frozen IOError). And before separate gem commit it was green\n@nateberkopec if we enabled windows, maybe we want to enable MacOS in travis too?. @NickolasVashchenko can we add this to Gemfile?\nruby\nif %w(2.2.7 2.3.4 2.4.1).include? RUBY_VERSION\n  gem \"stopgap_13632\", \"~> 1.0.0\", :platform => \"mri\"\nend. @NickolasVashchenko yeah latest commit fixed 2.4.1, but not 2.2.7. I wonder why\n. Isn't RuntimeError: can't modify frozen IOError should be fixed by this branch? It was all green before. I'm a bit far from underlying error, so please, tolerate my infinite questions . Awesome. Can't wait it to be merged. Well also would be nice to mention about stopgap and affected versions in readme and fix appveyor build, but this can go in other PR (this one hanging too long).. I found what is the \"problem\":\nruby\n        Signal.trap \"SIGHUP\" do\n          if @runner.redirected_io?\n            @runner.redirect_io\n          else\n            stop\n          end\n        end\nit is kind of counter-intuitive to do hup in one case and stop in other. Maybe hup, and warning message in other?. > HUP reopen log files defined in stdout_redirect configuration parameter\ndocs actually does not mention that without redirect_stdout option it will restart server. Related https://github.com/puma/puma/issues/1344\nUPD Need to do better investigation before merging this. Related https://github.com/puma/puma/issues/867. Lets see all cases:\n1. connection was dropped before it was processed by server. This is handled by Puma, but only for TCP socket and on Linux.\n2. connection was dropped while server processed request. What we can do about it:\n2.1. wait until the end of execution e.g. detect if socket closed right before write response\n2.1.1 write in log error for that case 499 in manner of nginx. But this will shadow original response (200 or 500 or any other).\n2.1.2 write in log response code as is. This is what Puma does now.\n2.1.3 write in log response code as is and add some additional note (client dropped connection)\n2.2. interrupt immediately. First of all not sure if it is possible in current implementation, because current way is to read full request, process, write response. There is no code which is monitor state of socket while server works. Let's assume it is possible.\n2.2.1. kill thread. This is not very friendly if your script doing some db writes or network connections\n2.2.2. raise exception. Not sure it is possible to raise exception in the thread of server. But it is potentially possible to use the same trick as in Timeout e.g. monitor connection in one thread (throw exception if client dropped connection) and run server in another, join those two threads. This solution will have same cons as Timeout e.g. it will not be able to interrupt long running IO operation.\nRelated: https://github.com/puma/puma/issues/1067\n. @nateberkopec this is not a bug, this is feature. Can you provide example application, where it is possible to reproduce error?. @rekha014 this seems to be log of nginx not Puma, am I right? If so can you post log of Puma. Not sure what you trying to achieve. You want to bind two puma servers to one socket in manner of unicorn :reuseport => true?. Well try puma standalone \nbundle exec puma -C $configPath --daemon\nif it will run only once it means that the problem (most likely) is not in the puma, but in the shell script.\n. > any ideas on how I might help nailing it down further?\nsmall example with reproduction based on Rack::Server.new?. sh\nstop: Unknown parameter: app\nstart: Unknown parameter: app\nsh\ninstance ${app} # this is parameter\nprovide app parameter\nsh\nsudo start puma app=PATH_TO_APP\nfrom https://github.com/puma/puma/tree/master/tools/jungle/upstart. Nice. Can you please rebase? Tests (at least Travis) were fixed in the master. @NickolasVashchenko you can add badge right in this PR. Thought this is already awesome enough. \n[![AppVeyor](https://img.shields.io/appveyor/ci/nateberkopec/puma.svg)](https://ci.appveyor.com/project/nateberkopec/puma). I suppose this PR shadows this one https://github.com/puma/puma/pull/1348/files. . Need more steps of reproduction. How exactly you send response headers?. What is inside config/puma2.rb?\n$ rackup -s puma config_sidekiq_web.ru\n.../config_sidekiq_web.ru:26:in `block in <main>': uninitialized constant Sinatra (NameError)\nbundle exec rackup ... ?. @nateberkopec do I get it right?\nFrom queue_requests docs\n# When set to true (the default), workers accept all requests\n# and queue them before passing them to the handlers.\n\nSo in my scenario I'm able to flood request queue and queue latency gets super high. This is why on vegeta plot time of response is jumping that much. While unicorn does not accept more items in queue than backlog specifies, and that is why it is able to keep queue latency stabile.\nUPD I tested queue_requests false it doesn't change the picture. But I still believe the problem is in how Puma processes queue\n . I got reports on GC with trashed gem - to make sure there is no issues due to GC pauses. Also looking into how to instrument Puma to report queue length and number of threads. Related https://github.com/basecamp/trashed/pull/12\n@danshultz thanks for the clue\n. As of queue delay. Here is how I measure it:\nnginx conf\nserver {\n  ...\n  proxy_set_header X-Request-Start \"${msec}\";\n  ...\n}\nRack middleware\n```ruby\nmodule Application\n  module Rack\n    class RequestLogger\n      HTTP_X_REQUEST_START = \"HTTP_X_REQUEST_START\"\n  def initialize(app)\n    @app = app\n  end\n\n  def call(env)\n     began_at = Time.now.to_f\n     if env[HTTP_X_REQUEST_START]\n       request_latency = ((began_at - env[HTTP_X_REQUEST_START].to_f) * 1000).round(3)\n     end\n     @app.call(env)\n  end\n\nend\nend\n```\nAs soon I will get other recipes will post it too \nUPD\nNot the best way to measure backlog\nstats.sh\nsh\ncurl http://127.0.0.1:9293/stats | jq \".worker_status[] |  [(.last_checkin | fromdate), (.last_status | .backlog)] | @csv\" | sed -e 's/^\"//' -e 's/\"$//' >> data.csv\nhttp.gnuplot:\nset datafile separator \",\"\nset terminal png size 900,400\nset title \"backlog\"\nset ylabel \"Backlog items\"\nset xlabel \"Time\"\nset xdata time\nset timefmt \"%s\"\nset format x \"%H:%M:%S\"\nset key left top\nset grid\nplot \"data.csv\" using 1:2 with lines lw 2 lt 3\n```sh\nchmod +x ./stats.sh \nwatch -n5 ./stats.sh\nlater\ngnuplot < http.gnuplot  > backlog.png\n```\n. Backlog vs response time\nat 75 rps\nNote: they have slightly different scale on time axes. If backlog is 0, response time is reasonable 10-300ms, but if backlog is not zero it jumps to 3s-25s.\n\n\nat 100 rps\nResponse is pretty bad, a lot of errors - backlog never gets to 0\n\n\n. But I want to be able to instruct Puma, to reject requests immediately if it is more than some number, like in Unicron I'm able to set backlog size to 1024, so delay in queue never gets higher than 2s. While in Puma that number can get upto 60s (after which connection will be reseted by nginx).. I did\nbind \"unix:///srv/app/shared/sockets/app.sock?backlog=1024\"\nbut it has no effect. At least it is not the same as in Unicorn. In Unicorn if I get higher than 70rps I start to get errors, but time in queue does not get higher than 2s. In Puma start to get errors and time in queue can jump up to 25s. If I get to 100rps time in queue jumps to 10s in average. So I created reproducible benchmarks https://github.com/stereobooster/puma-benchmarks. It is clearly shows how Puma queue is much slower than Unicorn (see graphs for latency).. > What version of puma are you using?\n3.10.0 https://github.com/stereobooster/puma-benchmarks/blob/master/benchmark/Gemfile.lock\n\nHow deep is your socket backlog?\n\n1024 https://github.com/stereobooster/puma-benchmarks/blob/master/benchmark/puma-config.rb\n\nAre your errors 502's from Nginx\n\nNeed to check. Do not remember\n\nAre you measuring response time at the nginx plane or the puma/app plane?\n\nI measure the time between request landed in nginx and it get to Rack app. https://github.com/stereobooster/puma-benchmarks/blob/master/benchmark/config.ru#L44\n. response time in \"vegeta\" graphs is total time from client point of view. Vegeta is a tool like wrk. https://github.com/tsenart/vegeta. Just make sure you use everywhere same values for server and same port, then restart Puma and nginx. I see you tried different combinations:\n[8534] * Listening on tcp://0.0.0.0:3000\n...\n[11972] * Listening on unix:///home/ypill/tcm/shared/sockets/puma.sock\nresponse: Failed to open TCP connection to localhost:9200 (Connection refused - connect(2) for \"localhost\" port 9200). All you need is carefully walk through all setup again.\nStop nginx and puma. Clean log files. Start puma, check if it works (in browser or via curl). Start nginx, check if it works.\n. Did you give it time to warm up? Give a try with vegeta. You can use nginx in front of local Puma TCP socket. I created reproducible benchmark using Vagrant, you can try it to see if you can reproduce issue https://github.com/stereobooster/puma-benchmarks. I actually measured passenger (community edition - which allows forks, but not threads) with overload. Every ruby server is awful over TCP (in my bench). Passenger over TCP is the worst. Puma is the best over unix socket (but not over TCP).\nPassenger (over TCP)\n\nPuma (over unix socket)\n\n. Try to find 300Gb of logs https://www.cyberciti.biz/faq/check-free-space/. No phased restart does not reload config file. Documented here https://github.com/puma/puma/blob/master/docs/signals.md#send-usr1. Well obviously this is a lock situation.\n1. you have 1-st request\n2. controller do subrequest to the same server (which is 2nd request)\n3. 1st request still hangs and 2-nd request comes in. 2-nd request will be placed in queue if there are no free workers\n4. this will stay in lock situation until subrequest timeouts (which is 1 min by default, unless you changed it)\nIt will be the same in any server. Strange that you can not Ctrl+C out of it. But nothing strange in lock situation - this is not a bug in Puma\nUPD as of Ctrl+C it will kick in after request finished (after timeout).\n```ruby\nrequire \"net/http\"\nrequire \"uri\"\nclass HomesController < ApplicationController\n  def show\n    uri = URI.parse(api_users_url)\n    http = Net::HTTP.new(uri.host, uri.port)\n    http.read_timeout = 5\n    http.open_timeout = 5\n    @body = http.request(Net::HTTP::Get.new(uri.request_uri)) # Typhoeus.get(api_users_url)\n  end\nend\n```\n``\nrails s\n=> Booting Puma\n=> Rails 5.1.4 application starting in development\n=> Runrails server -h` for more startup options\nPuma starting in single mode...\n Version 3.10.0 (ruby 2.2.6-p396), codename: Russell's Teapot\n Min threads: 1, max threads: 1\n Environment: development\n Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\nStarted GET \"/\" for 127.0.0.1 at 2017-10-17 15:50:12 +0200\nProcessing by HomesController#show as HTML\n^C                                             <----- Ctrl + C\nCompleted 500 Internal Server Error in 10422ms <----- But it will do nothing until timeout error\nNet::ReadTimeout (Net::ReadTimeout):\napp/controllers/homes_controller.rb:10:in `show'\nStarted GET \"/api/users\" for 127.0.0.1 at 2017-10-17 15:50:23 +0200\nProcessing by Api::UsersController#index as /\n  Rendering api/users/index.html.erb within layouts/application\n  Rendered api/users/index.html.erb within layouts/application (0.3ms)\nCompleted 200 OK in 294ms (Views: 274.0ms)\n\nGracefully stopping, waiting for requests to finish\n=== puma shutdown: 2017-10-17 15:50:24 +0200 ===\nGoodbye!\nExiting\n```\n\nAnd this happens because Puma waits for shutdown. If you change puma config\nruby\nworkers 1\nYou would see that puma response to  Ctr+C immediately.\n[64435] - Worker 0 (pid: 64448) booted, phase: 0\n^C[64435] - Gracefully shutting down workers...\n. If ruby 2.1 or less supported it should be listed in travis https://github.com/puma/puma/blob/master/.travis.yml . > If ruby 2.1 or less supported \nI meant versions less than ruby 2.1. Ruby 2.1 obviously supported. Any chance that this issue is behind https://github.com/puma/puma/issues/1405 ?. > Collect per-request threadpool wait time\nIf you use nginx you can set\nproxy_set_header X-Request-Start \"${msec}\";\nand then you can create Rack middleware which will read this value and calculate difference between this value and current time - this is time between request got to webserver and actually started to execute\nExample of Rack middleware\nUPD: also this one https://github.com/puma/puma/issues/1405#issuecomment-330806817. Do you have small reproducible example?. Yes sure. pipes = []. threads = []. r, w = IO.pipe. I suppose you want to skip it if purge_interrupt_queue undefined. this misleading: I suppose you want to guard against exact case (RuntimeError: can't modify frozen IOError) and not any error message containing IOError. |r, w|. Error:\nHttp11ParserTest#test_13632_workaround:\nNoMethodError: undefined method `purge_interrupt_queue' for #<Thread:0x0000000002ea02c0>\n    /home/travis/build/puma/puma/test/test_http11.rb:197:in `block (2 levels) in test_13632_workaround'. This reminds me old story: admin setup script to monitor server log, if there was fatal error in logs it restarted server. Later one user write to support complaining that every time one tries to login server was down. Username was \"femme fatale\". not sure I understand this situation. Is purge_interrupt_queue defined for any other ruby version? This test doesn't make sense if method undefined. Just to be sure: I'm proposing to check against exact error e.message == \"can't modify frozen IOError\". skip test for undefined Thread.current.purge_interrupt_queue or change to:\nruby\n Thread.current.purge_interrupt_queue if Thread.current.respond_to? :purge_interrupt_queue. technically speaking Thread.current.respond_to? :purge_interrupt_queue is defined only for %w(2.2.7 2.3.4 2.4.1), so this test should fail for all other ruby versions. What is the sense?. I finally got it. Disregard my previous comments. > with no real purpose\nI read the source code and my first thought was: why this strange way to detect IOError. Then I checked code above and saw rescue IOError. I got totally puzzled. I opened https://github.com/puma/puma/pull/1206, and only there I found that message reads as \"can't modify frozen IOError\". I would place comment here like this:\n\nWorkaround for https://bugs.ruby-lang.org/issues/13239. code lower in this file\n\nruby\nif Puma.jruby?\n  Signal.trap(\"INT\") do\nmove those two together. ruby\n@server.close\n@server = nil\n. ",
    "obromios": "Very good question.  It turns out that I was using an old url\nhttp://0.0.0:3000/rails/mailers\nwhen I should have been using\nhttp://localhost:3000/rails/mailers\nThe later works fine. Thank you for your help.\n. It looks like the issue is not due to Puma, but being caused by Cloudflare which is intercepting excessive length query strings.. ",
    "mfrank01": "When using a proxy server, i.e, Nginx, it automatically adds the REMOTE_ADDR field as the IP address of the host machine, and it is not the client's IP address. (Source: http://blog.stevesmind.net/2012/10/nginx-not-reading-real-client-ip-address/ )\nTypically, an HTTP header like X-Forwarded-For would be used to set the client's IP address.\nWithout this change, using nginx + puma will always result in 127.0.0.1 (or whichever interface to which nginx has been bound)  for a remote_ip method call to an instance of ActionDispatch::Request. At least, this has been my experience.\nIf I'm overlooking something, please let me know. I'd rather minimize code changes. \n. Sorry, I tried fixing the problem at the incorrect level. Puma is correctly passing headers along, it turns out my problem was with the RemoteIp middleware in rails. \nThey extend the custom_proxies configuration variable to include all private IP ranges if the configuration setting is present but not in the type of Array. (https://github.com/rails/rails/blob/master/actionpack/lib/action_dispatch/middleware/remote_ip.rb#L68)\nThanks for your time, I'll close this pull request for now. \n. ",
    "wallclockbuilder": ":+1: welcome\n. ",
    "dadah89": "I was able to reproduce (somewhat) consistently with 2.16.0. It happens when trim is executed at a bad time. In thread_pool.rb, with at least one thread waiting to pick up work and the todo queue being empty, this is what I see:\n- a call to << comes\n- next wait_until_not_full is executed (and it starts waiting on @not_full)\n- trim is executed (because @waiting is still > 0, @trim_requested gets incremented)\n- finally, the work thread wake up, decrements @waiting, execute the work (removing the last item from @todo), finishes it, an then hits the @trim_requested, exiting the loop before executing signaling @not_full, so wait_until_not_full never exits\nIn a configuration like threads 0,1, that means the worker not accepting more requests.\nWith more threads (any combination where min and max are not equal), I've seen some weird behavior where when the bug happened, some requests would keep waiting, and when I sent a new request, all of them would get responses at once. Still not sure what happens here (doesn't make much sense looking at thread_pool.rb), would need to debug more.\nMy test is pretty simple, I send between 1000 and 3000 requests (helps with slower requests, but even with a simple '{status:ok}' response I could reproduce), with the same configuration @osheroff described above, and adding some output when <<, wait_until_not_full and trim get executed. I should try building a test script or maybe a failing test... but hope this info helps.\n. oh missed the notification, will check\n. ",
    "tacocatcodes": "We are seeing this issue (or one extremely similar) as well. The socket stops responding after an indeterminate number of connections or amount of time. The only way we have found to mitigate is to restart the puma daemon processes. We have threads 2,6, workers 8, and are binding to a socket. We started seeing this issue after upgrading to Ruby 2.3.0 from Ruby 2.2.3 and Puma from 2.15.3 to 2.16.0. I hope this helps for reproduction.\n. ",
    "rishijain": "Hopefully its still not an an issue since its been 2 years now. But posting a possible solution for anyone else who stumbles to this problem.\nIf after restarting the puma server, it starts to print the logs. You most likely are missing the copytruncate option in the config. It would basically make a copy of the log file, truncate the existing log file.\nYou can read more about it here: http://excid3.com/blog/logrotate-rails-production-logs  and https://www.systutorials.com/docs/linux/man/5-logrotate.conf/\n. ",
    "deha23": "In my I access log I see that I have \"execution expired\" errors due to  a Website Scraping with Mechanize. Is it maybe the reason for my workers going to 100% CPU or do I see this because my workers ARE at 100% ?\nWhen I kill the worker processes everything works fine until they come up again.\n. ",
    "codebymikey": "This can also happen when one of your files have a syntax error. \nAlways check your puma.stderr.log file.. ",
    "ianks": "Can you print out a backtrace of all the threads on this process?\nThread.list.map(&:backtrace)\n. ",
    "simonmorley": "Of course, sods law dictates the issue stops happening as soon I raise an issue after months of troubleshooting.\nThanks for the command, I wasn't aware that could be run. Will do so again when it blocks.\nYesterday by chance I installed the rack timeout gem which started throwing connection timeout errors all over the shop - in particular with bunny + connection pool.\nI've now re-replaced bunny with AMQP and things are better but not perfect. We're working through the other timeouts now.\nIt feels a bit misplaced to have put this here since it's clearly not a direct issue with puma. However, it's the puma process that's blocking. It's just been hard to diagnose the issue.\nWill try with Thread.list and report back.\n. The following is from a running server with approx 800 CLOSE_WAIT connections 'stuck' as above. \n[[\"(irb):1:in `backtrace'\", \"(irb):1:in `map'\", \"(irb):1:in `irb_binding'\", \"/usr/local/lib/ruby/2.2.0/irb/workspace.rb:86:in `eval'\", \"/usr/local/lib/ruby/2.2.0/irb/workspace.rb:86:in `evaluate'\", \"/usr/local/lib/ruby/2.2.0/irb/context.rb:379:in `evaluate'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:489:in `block (2 levels) in eval_input'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:623:in `signal_status'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:486:in `block in eval_input'\", \"/usr/local/lib/ruby/2.2.0/irb/ruby-lex.rb:245:in `block (2 levels) in each_top_level_statement'\", \"/usr/local/lib/ruby/2.2.0/irb/ruby-lex.rb:231:in `loop'\", \"/usr/local/lib/ruby/2.2.0/irb/ruby-lex.rb:231:in `block in each_top_level_statement'\", \"/usr/local/lib/ruby/2.2.0/irb/ruby-lex.rb:230:in `catch'\", \"/usr/local/lib/ruby/2.2.0/irb/ruby-lex.rb:230:in `each_top_level_statement'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:485:in `eval_input'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:395:in `block in start'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:394:in `catch'\", \"/usr/local/lib/ruby/2.2.0/irb.rb:394:in `start'\", \"/usr/local/bundle/gems/railties-4.2.5.1/lib/rails/commands/console.rb:110:in `start'\", \"/usr/local/bundle/gems/railties-4.2.5.1/lib/rails/commands/console.rb:9:in `start'\", \"/usr/local/bundle/gems/railties-4.2.5.1/lib/rails/commands/commands_tasks.rb:68:in `console'\", \"/usr/local/bundle/gems/railties-4.2.5.1/lib/rails/commands/commands_tasks.rb:39:in `run_command!'\", \"/usr/local/bundle/gems/railties-4.2.5.1/lib/rails/commands.rb:17:in `<top (required)>'\", \"bin/rails:8:in `require'\", \"bin/rails:8:in `<main>'\"], [\"/usr/local/bundle/gems/eventmachine-1.0.9.1/lib/eventmachine.rb:193:in `run_machine'\", \"/usr/local/bundle/gems/eventmachine-1.0.9.1/lib/eventmachine.rb:193:in `run'\", \"/usr/local/bundle/gems/amqp-1.5.0/lib/amqp.rb:56:in `start'\", \"/myapp/config/initializers/amqp.rb:29:in `block in <top (required)>'\"]]\nAgain it looks like amqp is blocking everything. Line 29 refers to this:\nThread.new {\n  AMQP.start(connection_options.merge ssl_options) do |connection, open_ok|\nI'd previously swapped from AMQP and eventmachine to bunny but swapped back when we experienced timeouts there.\nI'm happy to move this elsewhere since puma blocking seems to be the symptom not the cause of this. Any advice appreciated however.\n. The root cause of this appears to be rabbitmq. \nI've been slowly working through end end-point, moving them to a separate set of pods. I've finally found the one (our second busiest) that's seemingly causing the issue.\nIt would appear that the connection to rabbit times out, even though I'm not seeing any errors on the rabbit servers. I thought rack timeout would kill the connections but maybe it's just leaving them in a state.\nAfter a few minutes, puma blocks with all the connections in CLOSE_WAIT state.\nLike I said, puma has taken the blame for something else. \n. ",
    "madsheep": "@simonmorley sorry for pinging you over an old issue - did you happen to identify the issue you had with rabbitmq? We have the exact same stack (puma / rabbitmq) and the exact same problem (CLOSE_WAIT connections). . ",
    "aotenko": "Did you know there is a Ruby issue handling CLOSE_WAIT sockets?\n\nstart a connection (plain TCPSocket.new(...))\nnow restart the other end\nwait for 60 seconds\nnow write to the connection from step 1.\n\nYou will see Ruby never throws, no matter how many times you repeat step 4, or what the size of the content is (well, unless you exceed TCP buffer size, and then you get blocked on write), and netstat -nap shows send queue growing for a CLOSE_WAIT socket.\nI can reproduce this always, ruby 2.2.0 for Ubuntu, at least for localhost connections.\nAlso, to preclude obvious suggestions and redirects to TCP implementation, I emphasise that things work as expected, if there is no wait for 60 seconds. I.e. if you skip step 3, and write, then after a couple of writes you get an exception.. ",
    "vincenzor": "Yes! That was my problem!\nSorry about that...\n. ",
    "subhash-shah": "@TheKidCoder : Thanks for your help.\n. ",
    "we138": "@evanphx thank you\n. ",
    "vanchi-zendesk": "cc @zendesk/rules\n. Just rebased against puma:master\n. The test failure is unrelated. Also, I think caused by this:\n```\n      @launcher.events.fire_on_booted!\n  begin\n    server.run.join\n\n```\nserver.run creates @thread_pool and the event is fired before that?\n. ",
    "thezed": "@mwpastore, tried them both, gives me this error:\nstart: Job failed to start\n. @mwpastore, I have /var/log/upstart/puma-manager.log. Inside of it:\nstart: Job failed to start\nstart: Job failed to start\n. @mwpastore, I have /etc/puma.conf and it's filled with paths to my apps. I was using https://github.com/seuros/capistrano-puma gem and it was generated by it.\nIn /etc/init/puma.conf I changed setuid and setgid to deployer. Nothing more I changed in the file.\n. @magedmakled I still did not fixed this. \nWhen I start Puma manually or with capistrano task cap production puma:start it runs ok. \nBut after server restart upstart script still fails, so I have to run all my apps manually.\n. ",
    "magedmakled": "@thezed, I'm getting the same error, how did you fix it? \nI'm using https://www.digitalocean.com/community/tutorials/how-to-deploy-a-rails-app-with-puma-and-nginx-on-ubuntu-14-04\n. @mwpastore when I ran bundle exec puma from the app I get\n[6318] Puma starting in cluster mode...\n[6318] * Version 3.2.0 (ruby 2.3.0-p0), codename: Spring Is A Heliocentric Viewpoint\n[6318] * Min threads: 1, max threads: 6\n[6318] * Environment: production\n[6318] * Process workers: 2\n[6318] * Phased restart available\n[6318] * Listening on unix:///home/deploy/myapp/shared/sockets/puma.sock\n[6318] Use Ctrl-C to stop\n. Worked for me as well.  Thanks @nateberkopec . ",
    "zzsnzmn": "I stopped puma before switching ruby versions and puma version.\nAnother shot in the dark--- could it be the config.ru we have?\n```\nThis file is used by Rack-based servers to start the application.\nrequire ::File.expand_path('../config/environment', FILE)\nrun Rails.application\n```\n. ",
    "corrupt952": ":+1: \n. Ah...\nYou might want to merge this, because phased-restart doesn't work.\n. @evanphx Thanks.\n. ",
    "sadiqmmm": "```\n!/usr/bin/env puma\nstart puma with:\nRAILS_ENV=production bundle exec puma -C ./config/puma.rb\napplication_path = '/home/appmboprod/public_html'\nrailsenv = 'production'\ndirectory application_path\nenvironment railsenv\ndaemonize true\npidfile \"#{application_path}/tmp/pids/puma-#{railsenv}.pid\"\nstate_path \"#{application_path}/tmp/pids/puma-#{railsenv}.state\"\nstdout_redirect\n{}\"#{application_path}/log/puma-#{railsenv}.stdout.log\",\n{}\"#{application_path}/log/puma-#{railsenv}.stderr.log\"\nthreads 0, 16\nworkers 2\npreload_app!\nbind \"unix://#{application_path}/tmp/sockets/#{railsenv}.socket\"\nbind 'tcp://0.0.0.0:3009'\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\n```\n. From the above puma.rb file script.\nRemoving the\n\n\"directory application_path\"\n\npuma is working correctly.\nProblem: \"directory\" option is pointing to the wrong Rails application path\nSolution: Point the correct path to the Rails application or just remove the \"directory\" option to test. \nNote: As the problem is solved. This ticket can be close.\n. ",
    "dirknilius": "Confirmed.\n. ",
    "klippx": "hear hear +1\nOur workaround is to use bundle exec puma -C config/puma.rb \n. I think it is due to having 2 workers and both trying to do the same task (which is deleting the file). One worker deletes the pid file, but the other worker throws. This hypothesis is backed by doing the following 3 quick tests:\n- If you start puma with only one worker and ctrl+c the error goes away.\n- If you start puma with two workers and ctrl+c the error is present (once).\n- If you start puma with three workers and ctrl+c the error appears twice...\nI agree with @jaredbeck that this would be nice to have a fix for.\n. @evanphx For a rails app the documentation says we should start it with the command rails s Puma. Rack::Server is not necessarily meant to be run concurrently in threads, isn't this is a responsibility that Puma pushes upon it? \nWhen Puma is \"Gracefully shutting down workers...\" it is actually doing so in parallell, right, so this becomes a concurrency issue/race condition because all threads see that the PID file exist and try to delete it:\nruby\n      def write_pid\n        ::File.open(options[:pid], ::File::CREAT | ::File::EXCL | ::File::WRONLY ){ |f| f.write(\"#{Process.pid}\") }\n        at_exit { ::File.delete(options[:pid]) if ::File.exist?(options[:pid]) }\n      rescue Errno::EEXIST\n        check_pid!\n        retry\n      end\n- rack-1.6.4/lib/rack/server.rb\nHow could Rack::Server protect itself from this particular failure, it already tries to do so? Wouldn't a better solution be that Puma shuts down workers one-by-one (ie even more Gracefully)?\nWould this issue not happen when starting puma with pumactl?\n. ",
    "Casara": "I had the following code in an initializer:\nruby\nPuma.cli_config.options.fetch(:max_threads)\nSo I did the following:\nruby\nPuma.cli_config.options[:max_threads]\nBut Anyway thanks!\n. @evanphx I'm reading the max threads to set ActiveRecord connection pool size: UPDATED - Setting ActiveRecord's connection pool size on Heroku with Puma or Sidekiq.\n. ",
    "jaredbeck": "\nThat error is actually inside Rack::Server itself, it's something I have no control over in this case because Puma is started via rails s.\n\nOK, so are you going to open an issue with that project, or do you want someone else to do it, or how do you want to proceed?\n. > > That error is actually inside Rack::Server itself, it's something I have no control over in this case because Puma is started via rails s.\n\nOK, so are you going to open an issue with that project, or do you want someone else to do it, or how do you want to proceed?\n\n@evanphx One month ping.  Please advise.\n. ",
    "oboxodo": "I'm having this same issue after upgrading from 2.15.x to 3.4.0. @evanphx is there a chance this is a regression since then?\nI'm rolling back the upgrade for now.\n. ",
    "sophiedeziel": "Just so you know, I opened a pull-request on Rack to solve that: https://github.com/rack/rack/pull/1080\n. ",
    "ma11hew28": "OK, I sure will. \ud83d\udc4d I'm glad to be of service. \ud83d\ude4f I appreciate you for implementing this request, especially so quickly. That's rare. I'm impressed & humbled. Thank you for your awesome contributions to the Ruby community. You are a true legend. :-)\n. The import method doesn't seem to work for Puma 3.8.2. Please advise.. In config/puma/development.rb. The first line is: import 'base', to import config/puma/base.rb.\nWhen I (after applying a quick fix for issue #1265) run RACK_ENV=development bundle exec puma --debug, I get the following error output:\nbundler: failed to load command: puma (/usr/local/bin/puma)\nNoMethodError: undefined method `import' for #<Puma::DSL:0x007fc2701bae60>\nDid you mean?  port\n  config/puma/development.rb:1:in `_load_from'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/dsl.rb:41:in `instance_eval'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/dsl.rb:41:in `_load_from'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/configuration.rb:204:in `block in load'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/configuration.rb:203:in `each'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/configuration.rb:203:in `load'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/launcher.rb:59:in `initialize'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/cli.rb:68:in `new'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/cli.rb:68:in `initialize'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/bin/puma:8:in `new'\n  /usr/local/lib/ruby/gems/2.4.0/gems/puma-3.8.2/bin/puma:8:in `<top (required)>'\n  /usr/local/bin/puma:22:in `load'\n  /usr/local/bin/puma:22:in `<top (required)>'\nI looked, and I don't see the import method defined in lib/puma/dsl.rb anymore.\n2: https://github.com/puma/puma/blob/master/lib/puma/dsl.rb. I just did gem update puma, but I still get this error. I guess this fix hasn't yet made it into the latest released version of the Puma Ruby gem: 3.6.0. So, I replaced the ssl_bind method on my Macbook Air with the one from master, and the server starts OK now.\n. OK. Sorry. Thank you. :\u2022). Hmm... \ud83e\udd14 When I run the command below, Puma seems to use its default configurations instead of the ones I've specified in config/puma/development.rb.\nbash\nRACK_ENV=development bundle exec puma --debug\nHow should I debug this?. Oh, nice! I found this handy method called p. \ud83e\udd13\nSo, I added p @options[:environment] right above lib/puma/configuration.rb:194 and then ran Puma again (with the command in my comment above). And, the output showed that @options[:environment] is a Proc. Huh?\nAhah! It's defined as a lambda literal at [lib/puma/configuration.rb:183][3].\nAdding .call to the end of @options[:environment] on line 194 seemed to fix this issue for me.\np to the rescue! \ud83c\udf89\n[3]: https://github.com/puma/puma/blob/master/lib/puma/configuration.rb#L183. I'm sorry. I don't know how to write a test for this, but when I tried it out, it worked for me.. ",
    "bgvo": "I solved this by installing OS updates and running `gem pristine ' as indicated by stdout.\n. ",
    "rodrigdav": "If I go back to puma version  2.12.2 which is what we were using before it starts working again.\n. ",
    "pglombardo": "Great on the quick fix.  I just hit this in my travis tests if an example is needed.\n. Ah ok that's understandable.  The gemspec should probably be updated to reflect that as well then.  Thanks!\n. ",
    "cdonadeo": "Hi Evan,\nSorry for the long delay - I am still here.  The controller just passes an AR relation to a Jbuilder view.  I hadn't really thought about it, but it could possibly make sense that the JSON is being built in some sort of incremental manner - I'm not very familiar with how Jbuilder goes about things under the hood.  Here's the controller in app/controllers/api/v1/services_controller.rb:\nclass Api::V1::ServicesController < ApplicationController\n  def problems\n    @services = Service.problems.includes(:host)\n  end\nend\nand here's the view in app/views/api/v1/services/problems.json.jbuilder:\njson.array!(@services) do |service|\n  json.extract! service, :id, :remote_id, :name, :backend_acknowledged, :acknowledged, :backend_downtimed, :downtimed, :check_interval, :retry_interval, :current_attempt, :max_check_attempts, :state, :last_state_change, :last_healthy, :last_check, :state_message, :notes_url, :notes, :host_id\n  json.host do\n    json.extract! service.host, :id, :remote_id, :name, :notes_url, :notes, :backend_id, :backend_acknowledged, :acknowledged, :backend_downtimed, :downtimed\n  end\nend\nAs far as I know this is pretty par for the course in a Rails app.  The output is something like this (only not formatted):\n[{\n    \"id\": 41413,\n    \"remote_id\": \"some_short_name\",\n    \"name\": \"A Longer Name\",\n    \"backend_acknowledged\": false,\n    \"acknowledged\": false,\n    \"backend_downtimed\": false,\n    \"downtimed\": false,\n    \"check_interval\": 60,\n    \"retry_interval\": 60,\n    \"current_attempt\": 1,\n    \"max_check_attempts\": 1,\n    \"state\": \"critical\",\n    \"last_state_change\": 1460250052,\n    \"last_healthy\": null,\n    \"last_check\": 1460413397,\n    \"state_message\": \"Usually a sentence or two\",\n    \"action_url\": \"http://example.net/some/path\",\n    \"notes_url\": \"http://example.net/some/other/path\",\n    \"notes\": \"Usually a sentence or two\",\n    \"host_id\": 4046,\n    \"host\": {\n        \"id\": 4046,\n        \"remote_id\": \"host.example.net\",\n        \"name\": null,\n        \"notes_url\": null,\n        \"notes\": null,\n        \"backend_id\": 1054,\n        \"backend_acknowledged\": false,\n        \"acknowledged\": false,\n        \"backend_downtimed\": false,\n        \"downtimed\": false\n    }\n}]\nThe above example shows roughly what we'd see when the relation yields 1 element.  When the query results are this short, I see no issues.  However, at the moment the live system ends up encoding an array with about 300 elements, which makes a much larger response and does exhibit the issue.\nAs a troubleshooting step, I rewrote my controller to use JSON.generate instead:\nclass Api::V1::ServicesController < ApplicationController\n  def problems\n    result = []\n    @services = Service.problems.includes(:host).each do |service|\n      result << service.to_hash\n    end\n    render json: JSON.generate(result)\n  end\nend\nService#to_hash does what it says - returns a simple hash representation of the AR object in question.  So, we're encoding nothing but a simple array of simple hashes - no magic involved.  I think this should make Rails just pass the already-encoded JSON - no templates or anything required.  It has the exact same issues with truncated output as my original implementation though.  My next step is to separate this logic from my application and move it to a new, clean Rails application and see if I can reproduce the issue there.  I'll let you know what I find.\n.  @evanphx I was actually able to reproduce this pretty easily with a fresh Rails app and very few changes.  I put the entire application in a repo so you can see:\nhttps://github.com/cdonadeo/puma_big_response_test\n. ",
    "dapicester": "The same for puma-3.1.0-java and jruby-1.7.19.\nI can confirm that removing the \u00f1 solves the problem.\n. @tbrisker Yes, that fixes the error. Thanks!\nI believe we should wait for the next Puma release to get the fix, right?\n. ",
    "tbrisker": "Please try https://github.com/puma/puma/pull/924, I believe it should fix this error.\n. ",
    "everplays": "Thanks for looking into this @evanphx. force_shutdown_after could make the situation better but will not solve it entirely.\nWhat it can improve:\nWe split the background jobs and try to keep them under the time that beanstalk waits for the server. For the most of the jobs, it is possible and force_shutdown_after solves the problem.\nThe problem which remains:\nImagine that we use forever for force_shutdown_after (which is the default value based on the code) and we do something that potentially could take 15 minutes to finish. After the initial SIGTERM, beanstalk will not wait 15 minutes but will issue a SIGKILL and terminates the process all together. In this scenario, force_shutdown_after will not solve the issue.\n. I absolutely agree with you. However, if we want to keep in the standard realm of elastic beanstalk, we can not run our own daemons. Therefore, using something like Sidekiq or DelayedJob is not possible.\n. @evanphx sorry for the delay. I was checking if it would solve the problem but unfortunately it doesn't. Still there is no way to determine if puma has received a SIGTERM in the middle of executing a job.\nIf you are willing to expose some kind of API to allow application get this information from puma, I would like to work on it.\n. @evanphx yes, that will work. We can stop whatever we are doing when the flag changes and let puma handle the signal by itself.\n(It will be comparable with the first code snippet that I put in the description)\n. @evanphx perfect. Thanks for taking care of it.\n. ",
    "wxianfeng": "this issue is not Puma problem\nat last i found active_model_serializers problem, am use not release code,  it's use class variables, not threadsafe !\ngem 'active_model_serializers', git: 'https://github.com/rails-api/active_model_serializers.git', ref: '2df8804'\nactive_model_serializers/lib/active_model/serializer.rb\nruby\ndef read_attribute_for_serialization(attr)\n  if self.class._serializer_instance_method_defined?(attr)\n    send(attr)\n  elsif self.class._fragmented\n    self.class._fragmented.read_attribute_for_serialization(attr)\n  else\n    object.read_attribute_for_serialization(attr)\n  end\nend\nchinese user , u can see here: https://ruby-china.org/topics/30188\n. ",
    "hsoerensen": "@robmathews There's a history file at https://github.com/puma/puma/blob/master/History.txt\n. ",
    "prathamesh-sonpatki": "This partially fixes #907. We will need to make some changes on Rails side also to pass restart command to Puma. I am working on it.\n@evanphx Please review.\n. @evanphx Done. Also added a test case.\n. I have also opened PR https://github.com/rails/rails/pull/24331 on Rails to pass the required restart_cmd to Puma.\n. Thanks @evanphx. Will you cut a new release so that Rails can depend on that version. The changes required to do on Rails side are already merged - https://github.com/rails/rails/issues/24331\n. I have opened https://github.com/puma/puma/pull/1120 to fix this issue.\n. @evanphx Can you help me for the ssl test. I have currently kept as pending because MiniSSL.check always throws error. Please guide me about how to make it pass.\n. @steakknife Awesome, thanks! I will incorporate it later today.\n. @steakknife I included your change for SSL test as well, thanks!\n. @evanphx Skipped test for JRuby but some other test failed. I guess it is unrelated to this change.\n. @nateberkopec rebased!. The restart support for Puma was added in Rails 5, so it won't work with Rails 4.2.8.. ",
    "huangxiangdan": "No, I'm not using puma as a proxy server. I run it direct on command \"rails server\", and then I request from my android app.\nSome android lib behave like this, using absolute url in the path. I'm using eventsource. And I checked HTTP 1.1 protocol , using absolute url is fine.\n. The query params are missing in the actual code.\n. ",
    "vandrijevik": "Thanks for confirming @evanphx, I suspected as much!\nWould you be open to making this behavior:\n\nPuma wants to wait until all the requests are done before shutting down\n\nconfigurable? Perhaps we could set a boolean like wait_on_threads_before_shutdown, or an integer like thread_wait_time_on_shutdown, which would let us get Rainbows!-like behavior, where a server shutdown doesn\u2019t wait on its threads (or waits up to a configured value).\nI think this would skirt the issue of Puma having to somehow detect that we expect long-lived requests (because we\u2019d be responsible for setting the setting), but it would allow us to use Puma in these scenarios also.\n. @evanphx I think that would work well! However, would it be possible to just kill the threads (thus unblocking the shutdown procedure which waits on them, and allowing it to continue) instead of calling exit!?\nI think that would cause less compatibility trouble (if someone has exit handlers they could still run, just the HTTP-serving threads would get killed abruptly), but I am not very familiar with Puma\u2019s code in this area, so sorry if this is a dumb question!\n. @evanphx Ah, good point! I was just wondering how other servers do this, and from this it seems that Rainbows! (which I\u2019ve been using to compare), does exit! just like you proposed above.\nSo that would be great after force_shutdown_after without any thread killing business! :)\n. Thank you @evanphx! :smile: \n. ",
    "micahlisonbee": "{:env=>nil}\n``` ruby\nworkers Integer(ENV['WEB_CONCURRENCY'] || 2)\nthreads_count = Integer(ENV['MAX_THREADS'] || 5)\nthreads threads_count, threads_count\npreload_app!\nrackup      DefaultRackup\np :env => ENV['port']\nport        ENV['PORT'] || 3001\nenvironment ENV['RACK_ENV'] || 'development'\non_worker_boot do\n  # Worker specific setup for Rails 4.1+\n  # See: https://devcenter.heroku.com/articles/deploying-rails-applications-with-the-puma-web-server#on-worker-boot\nend\n```\nIt seemed to be working for a while, is the port cached somewhere and it's not getting reset upon start/stop of server?\n. Heroku for production, but local development with rails s is where i'm having the port problems.\n. It looks like it defaults to 3000 with rails s regardless of what's in puma.rb. Looks like it reads the port correctly running heroku local, unless ENV['PORT'] conditional is defined. \n``` ruby\npuma.rb\nport        ENV['PORT'] || 3000\n```\nrails s\n[49861] * Listening on tcp://0.0.0.0:3000\nKill server.\nChange port to 3001:\n``` ruby\npuma.rb\nport        ENV['PORT'] || 3001\n```\nrails s\nListening on tcp://0.0.0.0:3000\nIt doesn't reflect the change to port 3001.\nRemove conditional and restart:\n``` ruby\npuma.rb\nport        3001\n```\nStill on 3000:\nListening on tcp://0.0.0.0:3000\nrun heroku local\nListening on tcp://0.0.0.0:3001\nIt works with heroku local.\nAdd back the conditional and it goes to 500\n``` ruby\npuma.rb\nport        ENV['PORT'] || 3001\n```\nListening on tcp://0.0.0.0:5000\nKill server and rails s again.\nBack on 3000:\nListening on tcp://0.0.0.0:3000\n. Ok thanks.\n. ",
    "omitter": "This is pretty confusing. I thought the implicit order when using rails server would be command-line options > environment variables > conf/puma.rb > puma default values. Maybe you can find a more straight forward solution? Is it possible to load conf/puma.rb earlier? Is there any way to make rails server bind to another ip/socket without using command-line options?. ",
    "confiks": "Alright, it was totally not obvious to me that Puma shouldn't be daemonized when running as, err, daemon from systemd. Especially since it only fails to work in the specific circumstances I described above, and just works otherwise. It does explain the exit code, as puma forks at that point. Thanks for the systemd example script too!\n. ",
    "richmolj": "Thanks for the response!\nYes, I can hit localhost:300 fine by hand. If I run haproxy in debug mode, you'll see the request come in, then see the response:\n00000000:rails.clicls[0008:000b]\n00000000:rails.closed[0008:000b]\nIf I start rails with webrick, you'll see the correct response as something like\n00000006:rails.srvrep[0008:000b]: HTTP/1.1 200 OK\n00000006:rails.srvhdr[0008:000b]: X-Frame-Options: SAMEORIGIN\n00000006:rails.srvhdr[0008:000b]: X-Xss-Protection: 1; mode=block\n00000006:rails.srvhdr[0008:000b]: X-Content-Type-Options: nosniff\n00000006:rails.srvhdr[0008:000b]: Content-Type: application/json; charset=utf-8\n00000006:rails.srvhdr[0008:000b]: Etag: W/\"9efea607cceada2d867cff074281c553\"\n00000006:rails.srvhdr[0008:000b]: Cache-Control: max-age=0, private, must-revalidate\n00000006:rails.srvhdr[0008:000b]: X-Request-Id: f5d94957-ce70-4f3b-9865-a92185038ec2\n00000006:rails.srvhdr[0008:000b]: X-Runtime: 0.058698\n00000006:rails.srvhdr[0008:000b]: Server: WEBrick/1.3.1 (Ruby/2.3.0/2015-12-25)\nI was thinking since puma starts on tcp://localhost:3000 and not http://localhost:3000. That might be the issue, but using haproxy's mode tcp seems to have a similar problem:\n00000000:http-farm.accept(0004)=0008 from [127.0.0.1:53529]\n00000000:rails.clicls[0008:0009]\n00000000:rails.closed[0008:0009]\n00000001:http-farm.accept(0004)=0008 from [127.0.0.1:53535]\n00000001:rails.clicls[0008:0009]\n00000001:rails.closed[0008:0009]\n. It actually looks specific to rails s. This will start puma and will fail with haproxy. HOWEVER, if I start puma using bin/puma -C config/puma.rb everything works ok.\n. Gave it a shot but no luck unfortunately\n. :+1: verified ::1:3000 fixes the issue\n. ",
    "freen": "@evanphx \nThis issue is happening to my team's application as well. When we send a phased-restart signal to the master puma process, there's a 50/50 chance that all requests interacting with the hard-drive will result in this error, e.g. getcwd, pwd, unlink_internal. The application more or less fails.\nFor instance, any transaction using RestClient is a valid test of whether this problem is occurring. since it invokes getcwdsomewhere under the hood. (We're making an API call in our health check to determine whether the bug is present or not.)\nWe're running:\n- Puma 3.1.0\n- Rails 5.0.0.beta3\n- Ruby 2.2.4\n- Ubuntu 14.04.3\nThis is an example stacktrace: https://gist.github.com/freen/0859baf8cb21859765c072fe0007069d\nThis is our puma.rb: https://gist.github.com/freen/cc970069488a2a5b672b95d10aea3f6d\n. Our current remediation step is to redeploy the application. The subsequent phased-restart typically resolves the issue (until the next botched deployment.)\n. @evanphx thanks!\nhttps://github.com/puma/puma/issues/952\n. I should also mention that we're managing the deployment using Capistrano, hence the shared / current folders in the stacktrace. We keep the five latest releases on disk, so it isn't a question of the process running in a release folder which has been deleted --- application releases which don't result in Error::ENOENT outages will consistently reflect the latest version deployed.\n. So far I'm seeing nothing apropos in the upstart logs, just standard restart details. I'm going to double check our error redirection and output settings and follow-up.\n. ",
    "jogaco": "Seems odd but this is the behaviour.\nsudo service puma-manager start => works fine\nsudo reboot => does not work\nwhereas if I change the name to puma_manager.conf, both scenarios work.\nAnd on several machines.\nI was moving my app to a new machine from a different provider and thus had the chance to try and fix this with the unused one. Tried many many things with no luck until leaving a bare-bones service, then changed its name et voil\u00e0: it worked\n. I meant sudo reboot\n. The service will not start automatically upon rebooting the system, with the current filename of puma-manager.conf.\nHowever, it will start fine if the file is renamed to puma_manager.conf\n. ",
    "ciaoben": "when it starts the server, it shows the right number of threads.\nI ll try to start it without thread to understand the minimun number that it uses and see if the behaviour is right with more thread! \n. ",
    "u007": "hi Evan,\nhow do i identify this?\ni realize when i set worker =0 and thread to [2,8],\nit works so much better now,\nbut after long idle, 2s load can take up to 8s load on browser.\nis this how rails refreshes all files on development?\nsorry forgot to mention, i'am using rails 5\n. hi,\nmy concern isn't just about worker,\nmy concern is why does my rails instance seems to lag when its long time idle...\nnow ive tested after several hours idle with thread[2,8], it took 21seconds to process a page.\nive hardcoded my dns ip on my etc hosts file, so it couldn't be dns issue...\n. for some reason, pid file is gone, but instance is still running but its not serving anymore ,\napache proxy seems to load forever.\nive to manually killed it in shell, and call puma start in daemon again\n. i think my server restarted using/etc/rc.local, so the pid is somehow started else where..\nand i tried deploying on a standalone server without cloud linux nor kernelcare,\nand it seems to be working fine even after long idle...\n. same issue here, ruby 2.3.1 puma 3.6.0, rails 5.0.0.\nit doesn't happen all the time, about 50% of the time\n. i realize its attempting to start another:\npuma 3.6.0 (tcp://127.0.0.1:3000,...)\nit does not seems to use existing pid , and it happen 50% of the time\n. @paz-choe does it mean that the old directory (original) started on gets deleted from capistrano, it will starts to fail? oh damn..\nanyone know  work around?\nor we should change\nbundle exec pumactl -P #{shared_path}/tmp/pids/puma.pid phased-restart \nto\npumactl -P #{shared_path}/tmp/pids/puma.pid phased-restart\n. even with restart, 50% of the time it wont start\n. with the pumactl command for restart nor phased-restart,\nit seems all instance are shutted down (not sure forcefully or gracefully), \nbut it does not spawn new instance\n. 1st, it seems to kill all workers at once, is this how phased-restart suppose to work?\n. got this\nSSHKit::Runner::ExecuteError: Exception while executing as wspine@live.wheelspine: bundle exit status: 1\nbundle stdout: No pid '1808' found\nbundle stderr: Nothing written\nwhy for no reason the pid does not exists, but process are still active on server\n. okay i found the problem,\ni didnt have remote assets precompile, therefore my Capfile was wrong,\nwas:\n```\nrequire 'capistrano/bundler'\nrequire 'capistrano/rails/assets'\nrequire 'capistrano/rails' # this is the biggest issue if i dont precompile on server\n```\nchanged to this works:\nrequire 'capistrano/bundler'\nrequire 'capistrano/rails/migrations'\n. ",
    "jf": "Ok, given that no actual code has changed, but only the text of a comment, I'm not expecting that this should fail. How do I request a restart of the checks?\n. Ok. Let me know if this is ok for you? I had to send @events the env as well (the alternative was to just specifically send env['REQUEST_METHOD'] and env['PATH_INFO']). I also collapsed the @stderr.puts calls into one, so it writes all the lines in one call.\n. > 3. The thread pops the connection, it's not ready yet, so it is added to Reactor and the thread finishes\nI dont think I can be of much help here, but if I may ask - what do you mean by connection is \"not ready yet\"? I'm reading https://github.com/puma/puma/blob/master/docs/architecture.md and I dont see how any connection picked up by a worker thread can be \"not ready\".. I'd prefer multiple calls too. Sorry for not elaborating; the only reason I did a single call is so that there wont be the possibility (?) of other output from other @stderr.puts calls being interleaved among these debug lines. Do you think this is a valid concern / makes sense?\n. Any comments, @evanphx ? Would appreciate your input on this.\n. ",
    "timabdulla": "hey, thanks! sorry for asking twice.\nwe use amazon's ELB as a load-balancer, which routes directly to our puma instances. the idle timeout on our ELB is 60 seconds, which controls both the time the ELB will wait for a response from the server before issuing a 504, and controls the amount of time which the ELB will keep the connection open if it's a persistent connection. if the persistent timeout is lower on the backend web server, the ELB will error out and issue a 504 if the connection is closed before the ELB expects it to be when a new client attempts to initiate a connection with the ELB (see this link under the heading \"Cause 2\" for more info: http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/ts-elb-error-message.html#ts-elb-errorcodes-http504). we would like to adjust our puma config to make the persistent timeout higher than 60 seconds. if that makes sense to you, we can prepare a PR over here to expose this setting.\n. ",
    "dollschasingmen": "hi -- where is this PR and was it merged? we are seeing similar issues with amazons ELB\n. ",
    "jorihardman": "@dollschasingmen The master branch now supports this. You can use persistent_timeout in your puma config to set the number of seconds.\n. Tests are failing due to a bundler error: An error occurred while installing rack (2.0.1), and Bundler cannot continue.\n. ",
    "subakva": "We also started seeing the same errors (\"Puma::ConnectionError: Connection error detected during read\") on Heroku when we upgraded to 3.4. The release notes say that version includes #894 to trigger the lowlevel_error_handler block more often.\nI assume this was happening quietly all along, and I think it's part of normal operation in that environment. If that's the case, I'd be interested in any advice on sorting the expected errors from the unexpected errors so that I can filter the noise out of our error notifications without potentially hiding real issues.\nI'd also be interested to hear if I'm totally wrong about this being \"normal\" in a Heroku legacy 1x dyno. \n. ",
    "kmayer": "Related to  #944.\n. @evanphx Sorry I took so long to make the pull request.\n. ",
    "dasomx77": "bump\n. ",
    "Eversilence": "Have the same issue with Puma. It is running on 3 servers, and after some time Puma starts to consume 100% of a CPU and become unresponsive as well. Only thing that helps is killing the pid and restarting.\nMemory consumption doesn't grow, New Relic doesn't show anything suspicious, and Puma logs are clean as well.\n. @equivalent, thanks for advice, but this didn't solve the issue. I have tried to update Puma ( i had exactly the same version you had ) and Puma still hangs.\n. @equivalent, I am pretty sure, that the problem is not with memory size, as I am running Puma on dedicated server with 16gb of RAM and Ubuntu 14.04. It is not in docker container. The only thing that I have noticed, is that it hang's up much more likely when there is a big number of request ( usually, when RPM becomes 500+ ). I am still trying to debug this issue, but so far I have no results.\nI have tried changing config of Puma ( add more/less threads and workers ) -- still no result.\nIf I will come up with something, I will let you guys know. \n. ",
    "equivalent": "had similar issue after doing some code and gem changes, suddenly puma took ages to load (like 10 minutes till worker respond)\nbumping puma version helped\n-    puma (2.15.3)\n+    puma (3.4.0)\n. @Eversilence   ...hmmm, this upgrade I've done was part of a Docker image change. One other thing I've done as a part of this change is that I've increased memory size for Docker container running Puma server, as it appeared to me that maybe Worker have not enough memory to worm up. But from the metrics you posted you should be fine. Maybe I had different issue than you had. \none last thing I would suggest is to decrees number of Puma workers to and Threads and see if that helps. If you already done that then sorry, I'm out of ideas \nOne last question:  Are you running you app directly on a server or in a Docker container ? if on Docker are you using docker-compose or maybe AWS Elastic Beanstalk Dockerrun.aws.json ? \n. ",
    "common-nighthawk": "@evanphx:\nI believe I am having the same problem.  Puma creates more threads than I ask for and the memory consumption continues to grow until all the processes need to be killed.\nI wrote up a full description on StackOverflow: http://stackoverflow.com/questions/36961700/puma-stops-running-for-rails-app-on-ec2-instance-with-nginx-using-capistrano-ca\nMy rails application is pretty simple--nothing that should keep a connection open.  Gemfile here: https://github.com/common-nighthawk/picturebookmylife-2.0/blob/master/Gemfile.  Puma config here: https://github.com/common-nighthawk/picturebookmylife-2.0/blob/master/config/deploy.rb.\nI don't know if this an 'Issue' or due to a lack of understanding on my part.  But I truly appreciate any help.\n. @evanphx Thank you so much for looking in this.  I really appreciate it.\nMy app is a standard CRUD app.  It has nothing fancy, including no long running connections.  I think it's pretty unlikely something in my rails app is causing a problem.  The entire project is available here: https://github.com/common-nighthawk/picturebookmylife-2.0\nI tried setting the threads to 1,1 and nothing changed.  Here is a screenshot of Puma running on 6 different processes after that update:\n\nAny ideas?  I'm confused because it feels like my app and configuration are pretty straightforward, so would love to get to the root problem.\n. @evanphx \nYou are exactly right.  Here is the output of ps ax: https://gist.github.com/common-nighthawk/48ba336df4ae06d7ace10e788cb85731\nWhen I run htop in \"tree\" mode, it's clear the six threads belong to the one worker.  But shouldn't threads 1,1 cap the maximum number of threads?\nI don't think there are any memory leaks in puma.  My memory problem is just that I'm getting more threads than I ask for.  (I am on ruby-2.2.1.)\nAs for replicating the issue--yes, this problem happens right from the start without any requests.  Here is my puma.rb file: https://gist.github.com/common-nighthawk/e910cb67d6cc4aeff8274657a734301a\n. @evanphx \nthank you so much!  i think its so cool the person that wrote puma helped me look into my issue.\neverything you said makes sense now.  ill summarize what i learned, should anyone else make it to this page with \"puma stops working\" problems.\n- if Puma stops working, make sure you have enough memory to handle to number of workers and threads that you specified.  each Puma process is pretty expensive.\n- if you set 0 workers, Puma will not run in cluster mode.  it is recommended to run MRI using cluster mode.\n- threads are set per cluster.  if you have 2 works and 0,8 threads that means you will have two works and each will have between 1 and 8 threads.\n- Puma uses processes in addition to the threads.  Puma has a PID for the parent process.  if you are using cluster mode, it has a PID to manager the clusters.  if you are using cluster mode, it also has a PID for each cluster.  then, there are a fixed number of PIDs to run other tasks (per cluster).  without cluster mode, there are 5 fixed PIDs.  with cluster mode, there are 7 fixed PIDs.\n- this is all to say--if you see more processes than you expect, this is why.  also--when you add a new worker you add a significant amount of expensive processes.  make sure you have the space.\ni have a small app, and things seem to be working nicely with 1 worker and min=1, max=4 threads.  having a max of 8 threads looks to be what kept killing puma for me.\n. Thanks to @bbozo for writing up an even more extensive answer on SO!: http://stackoverflow.com/questions/36961700/puma-stops-running-for-rails-app-on-ec2-instance-with-nginx-using-capistrano-ca/39835783\nHope this helps others going forward \ud83d\ude04.  Including it here for reference because this issue still seems to draw traffic from ppl hitting similar roadblocks.\n. @dthomaz\nIt's difficult to know much.  Evan seems prone to saying \"Your app is the primary unknown here\", which I like.\nOn the most basic level, threads are per worker.  How many workers do you have?  You set threads to be min 4, max 16.  But if you have 5 workers, those numbers are actually 5x.  Also--you'll probably want at least 1 worker so puma is running in cluster mode.\nNothing in your comment seems off.  It is possible you are running out of memory for valid reasons--resources consume memory.\nThere are two things to try: 1. reduce the number of workers and threads.  Try one worker and 1,4 threads maybe.  Your app will be slower if you have lots of concurrent users, but it should stop breaking.  2. buy a more powerful server.  You can always pay for more memory and processing power.  I would try 1, and if that works and you need to speed up the app, then go with 2.\nThis is the most basic answer.  If that doesn't help, you'll need to look into things like \"Are you using Actioncable or another solution that keeps connections open?\"  And it's always possible you're using a Gem with a memory leak.  But altering the puma setting is the more likely fix.\n. Pretty sure resque workers have nothing to do with puma workers.  I usually have a puma.rb file where I set up the dedicated webserver configuration.\nthreads 1,4\nworkers 1\nThe readme has a section dedicated to setting up the configuration: https://github.com/puma/puma#configuration\nAs far as reducing the number by one until it's stable: that backoff plan sounds good to me.\nLastly, I found running htop in tree mode (just hit t to toggle) to be helpful.  It allows you to see the parent puma process, its children (workers), and their children (threads).  Which was helpful for confirming numbers of workers, etc.\n. It looks like 0 workers are set by default.  There is a link to this file in the readme:\nhttps://github.com/puma/puma/blob/master/lib/puma/configuration.rb\nMaybe that's your problem?  It is recommended to run MRI using cluster mode.  (If you have 0 workers, you are not running cluster mode.)  I would try setting the workers to 1 in your puma.rb.\n. ",
    "fedeest": "I  have this same problem. I think this is an issue with Puma workers.\nI have set one worker for the app. After about 5 minutes of inactivity, the app hangs as explained here before and can only work again after restarting Puma.\nIf I set the workers to 2, after 5 minutes of inactivity the app works, BUT if I then wait another 5 minutes, the app hangs.\nBased on this I assume it must be something related to workers not being able to restart or something.\n. ",
    "machty": "This thread is very helpful.\nWhat are the implications on Puma / concurrency for apps that use ActionCable/SSE or otherwise keep connections open?\n. ",
    "dthomaz": "Hello, \nI have a service running puma 2.8.1 with setting threads 4, 16\nA process:\n[deploy@production ~]$ ps ax | grep 9199\n 9199 ?        Sl     6:16 puma 2.8.1 (unix:///data/apps/aquanetix/shared/sockets/puma.sock,unix:///data/apps/aquanetix/current/tmp/sockets/aquanetix-puma.sock)\n11262 pts/1    S+     0:00 grep --color=auto 9199\nkeeps growing in use of memory and also sometimes (once per day or once every 2 days) ends up blocking cpu (>100%) and users cannot access the web application (Gateway Time-Out 504).\nI need to restart puma to get users using the service again.\nMemory use goes from approx 2% to 30-40% in 24h and continues growing until the server is blocked.  At some point sometimes with 30% sometimes with >70% mem usage cpu usage peaks and stays high for long enough so users cannot use the service.\nFrom the above discussion, should I set threads to smaller values?\nThank you for your help.\n. @common-nighthawk \nThank you for the reply.\nI am not 100% sure of how many workers are 'setup'... If the workers are those setup with the line:\n```\nResque workers\nset :workers, { \"reporting\" => 3, \"periodic\" => 1 }\n```\nthen the total is 4.  Is this correct?.. or resque workers have nothing to do with puma workers? (apologies for the ignorance...)\nIf however this is correct then I could start by reducing total workers to 3, and if not enough to 2, etc... Correct?\nThanks again.\n. Hello again @common-nighthawk . My puma.rb file is the following:\n```\nrack_env = padrino_env = ENV['PADRINO_ENV'] || ENV['RACK_ENV'] || 'production'\nthreads 4,16\nbind  \"unix:///data/apps/aquanetix/current/tmp/sockets/aquanetix-puma.sock\"\npidfile \"/data/apps/aquanetix/current/tmp/puma-pid\"\nstate_path \"/data/apps/aquanetix/current/tmp/sockets/puma.state\"\nactivate_control_app\n```\nThe fact that no workers line appears means that by default there is only one? Or this will be defined somewhere else?\n. Will try that and feedback over this weekend on the progress of memory and cpu usage.  Thanks again @common-nighthawk \n. Hello again,\nI set workers to 2, kept threads 4,16 and my memory leak issue remains.\nFrom last night when I deployed the two worker processes have steadily increased in %mem use to values of 19% and 16%. No signs of these coming down, only up.\nIs it possible that the reason the memory increases continually is because we don't close sessions and with time, more users will just keep opening more sessions? Or puma would sort this itself?\n. ",
    "ArielAleksandrus": "Same thing is happening to me. I believe it happens when my machine goes to sleep. I leave it running and when it sleeps for no mouse/keyboard events, puma stops responding. When I log back, it is consuming 100% CPU and I must kill the process and then start again to make it work again. So far I'm testing keep my machine alive to see if this problem does not occur. So far, everything's ok...\nUpdate: problem still persisted... Puma ended up freezing. Same here. It works fine for like 10 minutes and then it suddenly freezes. Needs to restart server to make it work again. Someone please help!. @GabKlein this is an invalid config for me. Maybe because I'm using MySQL. Do you have any ideas?. ",
    "connorjacobsen": "Yes. I can dump the system log but there doesn't appear to anything useful there.\n. Just got it on 3.3 too.\n. 3.2 crashed it too. I have no idea what is going on.\n. ",
    "fvosburgh": "I'm also getting this issue using ruby version 2.1.10, rails version 4.1.0, and puma version 3.4.0. Downgrading to puma 2.14 fixed the issue as well. \n. ",
    "nolman": "Uninstalling all the previous versions of puma and reinstalling puma fixed this for me.\n. It ends up I am still seeing this on one of my machines, I will work on getting a core dump to dig into this more.\nHere is a kernel panic log:\nhttps://gist.github.com/nolman/6d9a64ba2cb880fd0d3cd1463efcf847\n. Hey @evanphx so I have upgraded 2 boxes to the latest version of el capitan (10.11.5) and no longer get the kernel panics, I believe you are correct that it is a kernel bug. We just updated puma to 3.4.0 in our main codebase this morning, was going to wait a bit longer before updating the issue to ensure upgrading OSX really fixed the issue. Lets assume that the OSX update fixed it, if I see the issue again will comment again.\n. ",
    "shekibobo": "Here's the example using puma as a rails server.\nIf I have a controller like this:\nruby\nclass ItemController < ActionController::Base\n  def show\n      def show\n        pp request.headers[\"HTTP_AUTHORIZATION\"]\n        pp ActionController::HttpAuthentication::Basic.decode_credentials(request)\n      end\n    end\n  end\nend\nHere's what I get if I send duplicate headers with encoded credentials:\n```\nuser@example.com:FourFourFourFour\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy\"\n\"user@example.com:FourFourFourFour\\x05\\xAB\\\"q\\xD5\\xCD\\x95\\xC9\\x01\\x95\\xE1\\x85\\xB5\\xC1\\xB1\\x94\\xB9\\x8D\\xBD\\xB4\\xE9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\"\nuser@example.com:FourFourFourFou\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=\"\n\"user@example.com:FourFourFourFou\"\nduplicate headers, user@example.com:FourFourFourFour and user@example.com:FourFourFourFou\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=\"\n\"user@example.com:FourFourFourFour\\x05\\xAB\\\"q\\xD5\\xCD\\x95\\xC9\\x01\\x95\\xE1\\x85\\xB5\\xC1\\xB1\\x94\\xB9\\x8D\\xBD\\xB4\\xE9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD4\"\nduplicate headers, user@example.com:FourFourFourFou and user@example.com:FourFourFourFour\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy\"\n\"user@example.com:FourFourFourFou\"\n```\nThe same examples using thin as a server:\n```\nuser@example.com:FourFourFourFour\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy\"\n\"user@example.com:FourFourFourFour\\x05\\xAB\\\"q\\xD5\\xCD\\x95\\xC9\\x01\\x95\\xE1\\x85\\xB5\\xC1\\xB1\\x94\\xB9\\x8D\\xBD\\xB4\\xE9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\"\nuser@example.com:FourFourFourFou\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=\"\n\"user@example.com:FourFourFourFou\"\nduplicate headers, user@example.com:FourFourFourFour and user@example.com:FourFourFourFou\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=\"\n\"user@example.com:FourFourFourFour\\x05\\xAB\\\"q\\xD5\\xCD\\x95\\xC9\\x01\\x95\\xE1\\x85\\xB5\\xC1\\xB1\\x94\\xB9\\x8D\\xBD\\xB4\\xE9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD5\\xC9\\x19\\xBD\\xD4\"\nduplicate headers, user@example.com:FourFourFourFou and user@example.com:FourFourFourFour\n\"Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3U=, Basic dXNlckBleGFtcGxlLmNvbTpGb3VyRm91ckZvdXJGb3Vy\"\n\"user@example.com:FourFourFourFou\"\n```\nAnd now that I've tested these examples, it looks like the issue is more likely with Rails than with Puma. I was pretty certain the examples worked when using thin before, but it was still unclear what the actual problem was. I'll move this issue to Rails unless you happen to see something that I'm missing.\nCode for handling duplicate headers is here for reference, but it looks like Puma is handling this as designed: https://github.com/puma/puma/blob/4ca81231c42828980dad26cfe21e47ba527d8be9/ext/puma_http11/puma_http11.c#L202-L212\n. ",
    "Baozi2": "thank  you\n. ",
    "jfeltesse-mdsol": "\nBy \"after a while\" do you mean the app is just sitting there or are you trying to restart it?\n\nThe app is in testing now so imagine about 5~10 users trying things out for an hour. We're not trying to restart it unless we have to (new code to deploy etc).\nThen SSH'ing into the instance shows the system as the log above.\nAlso, sending TERM to the individual workers does not work. It's KILL or nothing.\nWe have a super hacky quick n' dirty fix that goes like:\n``` ruby\non_worker_fork do\n  master = ObjectSpace.each_object(Puma::Cluster).first if defined?(Puma::Cluster)\n  workers = master&.instance_variable_get(:@workers)\n  dead_worker_indexes = workers ? ((0...@options[:workers]).to_a - workers.map(&:index)) : []\ndead_worker_indexes.each do |dead_worker_index|\n    list_puma_processes_cmd = \"ps -ef | grep -E 'puma: cluster worker [#{dead_worker_index}]' | grep -v grep\"\n    puts #{list_puma_processes_cmd}.split(\"\\n\")\n    pids = #{list_puma_processes_cmd} | awk '{print $2;}'.split(\"\\n\")\n    puts pids.inspect\n    pids.each do |pid|\n      puts \"sending KILL to puma cluster worker process #{pid}\"\n      puts kill -s KILL #{pid}\n    end\n  end\nend\n```\nIt seems that puma thinks a worker died and then tries to spawn a new one to replace it.\nLet me know if you need more information or if you want me to try something out.\n. Is there a way to get detailed info in the \"puma.log\"?\n(we set it by doing stdout_redirect 'log/puma.log', 'log/puma.log', true in the config file)\nRight now with our hack (mostly the same as above, more logging):\n``` ruby\non_worker_fork do\n    Rails.logger.info \"A new Puma worker has started\"\n    master = ObjectSpace.each_object(Puma::Cluster).first if defined?(Puma::Cluster)\n    workers = master&.instance_variable_get(:@workers)\n    dead_worker_indexes = workers ? ((0...@options[:workers]).to_a - workers.map(&:index)) : []\ndead_worker_indexes.each do |dead_worker_index|\n  list_puma_processes_cmd = \"ps -ef | grep -E 'puma: cluster worker [#{dead_worker_index}]' | grep -v grep\"\n  list_puma_processes_cmd_result = `#{list_puma_processes_cmd}`.split(\"\\n\")\n  Rails.logger.info \"Currently running processes info: #{list_puma_processes_cmd_result}\"\n  pids = `#{list_puma_processes_cmd} | awk '{print $2;}'`.split(\"\\n\")\n  Rails.logger.info \"Pids of currently running processes: #{pids.inspect}\"\n  pids.each do |pid|\n    Rails.logger.warn \"sending KILL to puma cluster worker process #{pid}\"\n    Rails.logger.warn `kill -s KILL #{pid}`\n  end\nend\n\nend\n```\nI get this kind of stuff:\npuma.log\n=== puma startup: 2016-07-26 07:43:06 +0000 ===\n[432911] - Worker 0 (pid: 433093) booted, phase: 0\n[432911] - Worker 1 (pid: 433121) booted, phase: 0\n[432911] - Worker 0 (pid: 434862) booted, phase: 0\n[432911] - Worker 1 (pid: 435737) booted, phase: 0\nrails' production.log\n```\n2016-07-26T07:43:06.952007Z: A new Puma worker has started\n2016-07-26T07:43:07.002751Z: Currently running processes info: []\n2016-07-26T07:43:07.025638Z: Pids of currently running processes: []\n2016-07-26T07:43:07.043337Z: Currently running processes info: []\n2016-07-26T07:43:07.065758Z: Pids of currently running processes: []\n2016-07-26T07:43:07.067256Z: A new Puma worker has started\n2016-07-26T07:43:07.138596Z: Currently running processes info: []\n2016-07-26T07:43:07.162656Z: Pids of currently running processes: []\n2016-07-26T07:57:22.880862Z: A new Puma worker has started\n2016-07-26T07:57:22.932615Z: Currently running processes info: [\"polybus  433093 432911  0 07:43 ?        00:00:04 puma: cluster worker 0: 432911 [2016-07-26_07-38-33] \"]\n2016-07-26T07:57:22.952576Z: Pids of currently running processes: [\"433093\"]\n2016-07-26T07:57:22.952628Z: sending KILL to puma cluster worker process 433093\n2016-07-26T08:04:08.034619Z: A new Puma worker has started\n2016-07-26T08:04:08.081598Z: Currently running processes info: [\"polybus  433121 432911  0 07:43 ?        00:00:03 puma: cluster worker 1: 432911 [2016-07-26_07-38-33] \"]\n2016-07-26T08:04:08.108456Z: Pids of currently running processes: [\"433121\"]\n2016-07-26T08:04:08.108499Z: sending KILL to puma cluster worker process 433121\n2016-07-26T08:11:12.402669Z: A new Puma worker has started\n2016-07-26T08:11:12.454752Z: Currently running processes info: [\"polybus  434862 432911  0 07:57 ?        00:00:03 puma: cluster worker 0: 432911 [2016-07-26_07-38-33] \"]\n2016-07-26T08:11:12.481565Z: Pids of currently running processes: [\"434862\"]\n2016-07-26T08:11:12.481612Z: sending KILL to puma cluster worker process 434862\n2016-07-26T08:20:56.201496Z: A new Puma worker has started\n2016-07-26T08:20:56.242872Z: Currently running processes info: [\"polybus  435737 432911  0 08:04 ?        00:00:04 puma: cluster worker 1: 432911 [2016-07-26_07-38-33] \"]\n2016-07-26T08:20:56.274206Z: Pids of currently running processes: [\"435737\"]\n2016-07-26T08:20:56.274274Z: sending KILL to puma cluster worker process 435737\n``\n. sure, let me try this and get back to you in a few days with the findings.\n. Nothing interesting appeared in the stdout or stderr logs but meanwhile we discovered that in the code there was a method being called frequently that would executeActiveRecord::Base.establish_connectionall the time. \nChanging this bit toActiveRecord::Base.connection_pool.with_connection do |c| ... end` made the problem disappear, at least for the past few days.\nClosing this for now, will reopen and ping back if the issue continues and we have more detailed info to provide.\nThanks!!\n. ",
    "shicholas": "looks like #955 does this and more?\n. ",
    "stmarier": "Hi,  I ran into this issue earlier this morning,  I took a look at https://github.com/puma/puma/blob/master/ext/puma_http11/mini_ssl.c#L12-L18 because the error message is in this file.\nIn my case, I didn't have any of these header files.  I had to install the openssl development tools, gem uninstall puma, and then gem install puma.  I don't really know a lot about ruby, or gems, so there might be a better way to trigger a 'recompilation' after this #ifdef evaluates to true.\nWith yum, I think the package name is \"openssl-devel\".  I'm not sure about apt or any other distros.  Hope this helps or puts you on the right track.\n. ",
    "promentus": "I have openSSL installed but after your suggestion to uninstall Puma, install openssl and reinstall Puma it works!\nThanks big time!\n. ",
    "fagiani": "To save some eventual struggle with openssl linking, I found here some useful command sequence:\nbrew install openssl\nln -s /usr/local/Cellar/openssl/1.0.2c/include/openssl /usr/local/include/openssl\ngem uninstall puma\ngem install puma\nHope it helps!\n. ",
    "djones": "gem install puma -v 'YOUR VERSION' -- --with-cppflags=-I/usr/local/opt/openssl/include --with-ldflags=-L/usr/local/opt/openssl/lib\nFixed it for me.\n. I just generally want to bump this issue. It completely blocks me from being able to use ActionCable. I tried Rails 5.1.0.rc1 and things remain broken there too. \nUsing Rails 5.0.0.1 as a fix is a poor option. That particular version of Rails has other issues that you would have to accept.. I know very little about websockets but I was able to figure some stuff out.\nIndeed if you implement #write_nonblock in minissl.rb it does get called from ActionCable.\nThe method signature should be something like\nruby\ndef write_nonblock(data, *args)\nI found this in the Passenger project: https://github.com/phusion/passenger/blob/stable-5.1/src/ruby_supportlib/phusion_passenger/utils/unseekable_socket.rb#L120-L124 and they recently added support for arguments.\nSomewhere in this method it should call @socket.write_nonblock enc, *args instead of @socket.write enc which is what the write (blocking) method calls.. @evanphx thanks for looking into this issue! Really appreciated!\nI actually tried out a similar idea to what you had and it doesn't fix the issue. I'd be interested to hear from others if #1274 fixes things for them.\nUsing the #1274 version of puma, I continue to see this cycle in my Rails log:\nStarted GET \"/cable\" for ::1 at 2017-04-13 19:11:35 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:35 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nFinished \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:41 -0700\nStarted GET \"/cable\" for ::1 at 2017-04-13 19:11:41 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:41 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nFinished \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:44 -0700\nStarted GET \"/cable\" for ::1 at 2017-04-13 19:11:45 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:45 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nFinished \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:50 -0700\nStarted GET \"/cable\" for ::1 at 2017-04-13 19:11:50 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:11:50 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nUpgrading the connection, finishing it, upgrading, finishing etc. Here's the view from the browser:\n\nSimply responding to write_nonblock calls doesn't seem to be enough.\nWhen I make the request with the exact same codebase over non-SSL instead, I get no warnings in the console of the browser and I see this in my Rails log\nStarted GET \"/cable\" for ::1 at 2017-04-13 19:14:52 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:14:52 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nReportsChannel is transmitting the subscription confirmation\nReportsChannel is streaming from reports\nIt does not repeatedly try to upgrade the connection to WebSocket. It just settles like this and it works as expected.\nAny ideas? What would cause the browser to think the connection wasn't actually upgraded and therefore make repeated requests? Or what would trigger the finishing of the connection?\nHappy to work with you on this one @evanphx . @evanphx confirming websockets not over SSL works fine.\nTrying out the latest change now.. Weird. I still have the same issue. I threw a binding.pry into write_nonblock(data, *_) and it definitely is being called.\nI'll create a barebones example Rails app just to be 100% certain it isn't something specific to my app. And that will give us something to test against too.\nHere's the result of putting binding.pry into the method and inspecting data each time.\n```ruby\nFrom: /Users/djones/Code/puma/lib/puma/minissl.rb @ line 91 Puma::MiniSSL::Socket#write_nonblock:\n89: def write_nonblock(data, *_)\n90:   binding.pry\n\n=> 91:   write data\n    92: end\n[1] pry(#)> data\n=> \"HTTP/1.1 101 Switching Protocols\\r\\n\" +\n\"Upgrade: websocket\\r\\n\" +\n\"Connection: Upgrade\\r\\n\" +\n\"Sec-WebSocket-Accept: tKPIqJ0uLj3sVkKF/l2maJZ+BRk=\\r\\n\" +\n\"Sec-WebSocket-Protocol: actioncable-v1-json\\r\\n\" +\n\"\\r\\n\"\n[2] pry(#)> exit\nFrom: /Users/djones/Code/puma/lib/puma/minissl.rb @ line 91 Puma::MiniSSL::Socket#write_nonblock:\n89: def write_nonblock(data, *_)\n90:   binding.pry\n\n=> 91:   write data\n    92: end\n[1] pry(#)> data\n=> \"\\x81\\x12{\\\"type\\\":\\\"welcome\\\"}\"\n[2] pry(#)> exit\nFrom: /Users/djones/Code/puma/lib/puma/minissl.rb @ line 91 Puma::MiniSSL::Socket#write_nonblock:\n89: def write_nonblock(data, *_)\n90:   binding.pry\n\n=> 91:   write data\n    92: end\n[1] pry(#)> data\n=> \"\\x81${\\\"type\\\":\\\"ping\\\",\\\"message\\\":1492139027}\"\n[2] pry(#)> exit\nFinished \"/cable/\" [WebSocket] for ::1 at 2017-04-13 20:03:50 -0700\nStarted GET \"/cable\" for ::1 at 2017-04-13 20:03:50 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 20:03:50 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nFrom: /Users/djones/Code/puma/lib/puma/minissl.rb @ line 91 Puma::MiniSSL::Socket#write_nonblock:\n89: def write_nonblock(data, *_)\n90:   binding.pry\n\n=> 91:   write data\n    92: end\n[1] pry(#)> data\n=> \"HTTP/1.1 101 Switching Protocols\\r\\n\" +\n\"Upgrade: websocket\\r\\n\" +\n\"Connection: Upgrade\\r\\n\" +\n\"Sec-WebSocket-Accept: zqqTAeqCtj9LSF0Hv2H4OIhD/LQ=\\r\\n\" +\n\"Sec-WebSocket-Protocol: actioncable-v1-json\\r\\n\" +\n\"\\r\\n\"\n[2] pry(#)> exit\nFrom: /Users/djones/Code/puma/lib/puma/minissl.rb @ line 91 Puma::MiniSSL::Socket#write_nonblock:\n89: def write_nonblock(data, *_)\n90:   binding.pry\n\n=> 91:   write data\n    92: end\n[1] pry(#)> data\n=> \"\\x81\\x12{\\\"type\\\":\\\"welcome\\\"}\"\n[2] pry(#)> exit\nFinished \"/cable/\" [WebSocket] for ::1 at 2017-04-13 20:03:56 -0700\n```\nIt seems to repeat a welcome message.. @evanphx any idea what causes the connection to finish?\nThat seems to be the core difference. Non-SSL opens the connection, upgrades it, and then my application specific code starts to appear, and that's the end. It stays open as expected.\nE.g.\nStarted GET \"/cable\" for ::1 at 2017-04-13 19:14:52 -0700\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2017-04-13 19:14:52 -0700\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\nReportsChannel is transmitting the subscription confirmation\nReportsChannel is streaming from reports\nBut over SSL it seems to open, upgrade, finish, repeat forever. Even after we know write_nonblock is being called.. @evanphx it works now! \ud83c\udf8a. ",
    "flah00": "If you want to fix it across the board, I think this will persist across all of your projects\nbundle config --global build.puma \\\n  --with-cppflags=-I/usr/local/opt/openssl/include \\\n  --with-ldflags=-L/usr/local/opt/openssl/lib\n. ",
    "NafaaBout": "On Ubuntu 16.04 I installed libssl-dev and it works fine. \nsudo apt install libssl-dev\ngem install puma\n. I got something similar with Rails 4.2.7.1 and Ruby 2.3.3.\n2017-01-23 17:46:09 +0100: Listen loop error: #<NoMethodError: undefined method `key' for true:TrueClass>\n/home/nafaa/.rvm/gems/ruby-2.3.3@bytestand/gems/puma-3.5.0/lib/puma/minissl.rb:168:in `server'\n/home/nafaa/.rvm/gems/ruby-2.3.3@bytestand/gems/puma-3.5.0/lib/puma/minissl.rb:168:in `accept_nonblock'\n/home/nafaa/.rvm/gems/ruby-2.3.3@bytestand/gems/puma-3.5.0/lib/puma/server.rb:329:in `block in handle_servers'\n/home/nafaa/.rvm/gems/ruby-2.3.3@bytestand/gems/puma-3.5.0/lib/puma/server.rb:324:in `each'\n/home/nafaa/.rvm/gems/ruby-2.3.3@bytestand/gems/puma-3.5.0/lib/puma/server.rb:324:in `handle_servers'\n/home/nafaa/.rvm/gems/ruby-2.3.3@bytestand/gems/puma-3.5.0/lib/puma/server.rb:297:in `block in run'. ",
    "gregoryrpitts": "In response to fagiani's comment about symlinking, make sure you aren't just copy/pasting the command and have the proper symlink. Example, mine was \nln -s /usr/local/Cellar/openssl/1.0.2p/include/openssl /usr/local/include/openssl. ",
    "evenluo": "@evanphx  yes, you are right. It's a load issue caused by a capital letter \ud83d\ude02 .\n. ",
    "vivek-nga": "Sorry, its the problem of 443 port not opened on server\n. ",
    "anshul": "@Botje I am also exploring some memory leaks that appear to come from puma.  Based on #342, it may be worth testing if you can replicate this with ruby 2.3.1...\n. ",
    "Botje": "Yes, I can also reproduce it with ruby 2.3.1(-p112).\n. ",
    "cleverlemming": "I had the same issue here. Assets stop loading and application hangs with no error messages. \nSolved in pull request #25240: Properly support reloading for Action Cable channels #25240\nCan close this issue (#979) I think.\n=> Booting Puma\n=> Rails 5.0.0.rc1 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.4.0 (ruby 2.3.0-p0), codename: Owl Bowl Brawl\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://localhost:3000\nUse Ctrl-C to stop\nStarted GET \"/cable\" for ::1 at 2016-06-13 17:43:23 -0600\n  ActiveRecord::SchemaMigration Load (0.5ms)  SELECT \"schema_migrations\".* FROM \"schema_migrations\"\nStarted GET \"/cable/\" [WebSocket] for ::1 at 2016-06-13 17:43:23 -0600\nSuccessfully upgraded to WebSocket (REQUEST_METHOD: GET, HTTP_CONNECTION: Upgrade, HTTP_UPGRADE: websocket)\n. ",
    "mostlydev": "I think this issue has returned in 3.10.  I just experienced exactly the same behaviour.  Downgrading to 3.9.1 resolved it.. ",
    "skrobul": "30 June 2018 Deadline for migrating away from TLSv1.0 and earlier is approaching very soon. Are there any plans of adding an option to at least disable certain versions?\nLooks like the cipher selection has been worked on in #1478, but is not ready/merged yet.. Sorry I forgot to comment, this has been rebased in e142b9f and it should be ready to merge. Tiago,\nHa! I was thinking the same, but after looking into code, the minissl is\njust a class name that is used to wrap SSL related functionality. Upon\ncompilation it still uses OpenSSL. See more details in\nhttps://github.com/puma/puma/issues/1561\nOn Tue, 28 Aug 2018, 19:14 Tiago, notifications@github.com wrote:\n\nWhy does puma have their own \"minissl\", instead of using ruby's \"openssl\"\nlibrary?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1562#issuecomment-416667824, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AALOqJ_57NOYJofy0-zASSw9USBjHwCCks5uVXqNgaJpZM4TYVuw\n.\n. \n",
    "t27duck": "I'm thinking this is more of an issue with Rails than it is with puma. Rails 5 introduced puma as the default web server for development mode. Perhaps since the gem is available Rails is loading it up regardless.\nRandom shot in the dark, have you tried running the server using bundle exec?\n. This is a (sanitized) system service file I use for a Rails app if you'd like an example to go off of.\n```\n[Unit]\nDescription=My App Puma Server\nRequires=redis.service postgresql-9.4.service\nWants=redis.service postgresql-9.4.service memcached.service\nAfter=redis.service postgresql-9.4.service\n[Service]\nType=simple\nUser=appuser\nPIDFile=/path/to/my/app-puma.pid\nWorkingDirectory=/path/to/my/app/current\nEnvironment=RAILS_ENV=production\nExecStart=/path/to/my/app/current/bin/bundle exec puma -e production -C ./config/puma.rb config.ru\nExecReload=/bin/kill -s USR1 $MAINPID\nExecStop=/bin/kill -s QUIT $MAINPID\nRestart=always\n[Install]\nWantedBy=multi-user.target\n``\n. I should also note that I havepidfile \"/path/to/my/app-puma.pid\"set in my Puma configuration file to get puma to create a pid file for systemd to monitor and use.\n. It technically doesn't, but it's handy to use to makeExecReloadandExecStop` easier to implement.\n. I suppose if you want the absolute minimum needed, this might do it:\n```\n[Unit]\nDescription=My App Puma Server\n[Service]\nType=simple\nUser=appuser\nWorkingDirectory=/path/to/my/app/current\nExecStart=/path/to/my/app/current/bin/bundle exec puma -e production -C ./config/puma.rb config.ru\nRestart=always\n[Install]\nWantedBy=multi-user.target\n```\nAlso, I just noticed that the docs directory in the puma repo has a section about systemd: https://github.com/puma/puma/blob/master/docs/systemd.md\nWould that be a better starting point for you?\n. @evanphx - Gotcha. I've come from the land of unicorn and was used to working with Pids, Socks, and signal sending so I based my systemd file off of that pattern since puma supported it. I personally prefer the pid method as it makes it easier to track the master process.\n. ",
    "alagos": "@t27duck nope, the same story\n\u279c bundle exec rails s\n=> Booting Puma\n=> Rails 5.0.0.rc1 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\n[24367] Puma starting in cluster mode...\n[24367] * Version 3.4.0 (ruby 2.3.1-p112), codename: Owl Bowl Brawl\n[24367] * Min threads: 5, max threads: 5\n[24367] * Environment: development\n[24367] * Process workers: 2\n[24367] * Preloading application\n[24367] * Listening on tcp://localhost:3000\n[24367] Use Ctrl-C to stop\n[24367] - Worker 0 (pid: 24381) booted, phase: 0\n[24367] - Worker 1 (pid: 24392) booted, phase: 0\nAt the end I gave up and I started to use puma locally\n. ",
    "1Rhino": "Same issue.\nRails (5.0.2).  puma (3.8.2).  (Puma so slow debugging with better_errors, binding_of_caller)\nA trick that worked for me.\nruby\nif ENV['RAILS_ENV'] == 'production'\n  gem 'puma', '~> 3.8'\nelse\n  gem 'thin'\nend. ",
    "kenan-memis": "the config/puma.rb file has a line:\n```\nSpecifies the environment that Puma will run in.\nenvironment ENV.fetch(\"RAILS_ENV\") { \"development\" }\n```\nSo, although you put it to production gem list, it fetches the dev environment. ",
    "netwire88": "I don't understand the instructions here. I am using earlier version of Puma + Heroku and already have apuma.rb file. Are we supposed to change it? What's the new puma-heroku lib? How do we even install it?\n. ",
    "biow0lf": "I have the same when I open https:// instead http://. E.g. https://localhost:3000/ instead http://localhost:3000/.\n. ",
    "seanvree": "is there any fix to this?\n\n2017-04-04 13:38:42 -0700: HTTP parse error, malformed request (): #\n2017-04-04 13:38:42 -0700: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.8.2 Sassy Salamander\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n-------------. is there any fix to this?\n2017-04-04 13:38:42 -0700: HTTP parse error, malformed request (): #\n2017-04-04 13:38:42 -0700: ENV: {\"rack.version\"=>[1, 3], \"rack.errors\"=>#>, \"rack.multithread\"=>true, \"rack.multiprocess\"=>false, \"rack.run_once\"=>false, \"SCRIPT_NAME\"=>\"\", \"QUERY_STRING\"=>\"\", \"SERVER_PROTOCOL\"=>\"HTTP/1.1\", \"SERVER_SOFTWARE\"=>\"puma 3.8.2 Sassy Salamander\", \"GATEWAY_INTERFACE\"=>\"CGI/1.2\"}\n. hey thanks @nateberkopec  - that's what I thought. . ",
    "dented": "I don't know if anyone else might have this issue, but mine was related to passing in the wrong address wss://localhost:3000/cablesecure instead of ws://localhost:3000/cableunsecure. ",
    "nicolasmlv": "my case : \nI tried to use https://github.com/schneems/derailed_benchmarks so I tried to launch my rails app in production mode, but in production mode I have the ssl force mode. Then I tried once to go to http://localhost:3000 on chrome (macos), then it redirected me to https. And now it always redirect http://localhost:3000 to https://localhost:3000\nand I don't want to clear all my browsing history \ud83d\ude22 so I use Firefox. ",
    "phantomwhale": "I'm seeing these errors constantly in production - running on port 80 (in ECS / Fargate behind an ALB - although doubt that should change anything)\nIt's been throwing these errors for a long time now (months, years maybe), but all the searches on the error seems to be around the localhost / local development issue. Is there a scenario where this might come up in production? And is there a way of unpacking what requests to puma are causing the error?. ",
    "Zloy": "@jjb I tried several times not stopping puma. The results are pretty same. Can you try this test?\n. ",
    "hakkiplaten": "True, I have:\nenvironment \"production\"\ndaemonize\nBut it seemed to ignore this in the past which to me was very convenient:\n% rails s\n=> Booting Puma\n=> Rails 4.2.4 application starting in development on http://0.0.0.0:3000\n=> Run `rails server -h` for more startup options\n=> Ctrl-C to shutdown server\nUnknown architecture (openbsd5.9) assuming one processor.\nPuma 2.11.3 starting...\n* Min threads: 0, max threads: 16\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nSo now I either have to permanently change config/puma.rb or put Puma inside :production in the Gemfile so development mode reverts to WEBrick?\n. ",
    "mathie": "OK, so I've now submitted a strikingly similar issue over at the Rails repo https://github.com/rails/rails/issues/25259 since I'm not sure who's best placed to deal with it!\n. Good call, there are a couple of exceptions: https://gist.github.com/mathie/28347310c600e0396568a79c61486099 Most are in select() somewhere in fsevent, but a couple are sleep()ing in listen.\nSorry, just to be a pain in the ass, this isn't exactly the same codebase as it was this morning -- I've been hacking on it all day -- and we're up to 25 mysterious threads now. \ud83d\ude04 \n. @evanphx I'm starting puma with bundle exec puma -C config/puma.rb -p ${PORT} (via foreman). The puma configuration is here: https://gist.github.com/mathie/9967bf0bf000b7fe27dbea94df734954 and it's just the stock configuration that's generated by Rails, except that I've uncommented workers, preload_app! and on_worker_boot as the comments indicate.\nIf I start it with rails s it doesn't show the warnings, but I suspect that's just because it's not honouring the configuration file at all? (Which strikes me as a bug in Rails, right?)\nThis whole conversation should probably be going over to rails/rails. It's perhaps worth noting, though, that code reloading definitely isn't working for me with workers enabled.\n. Scratch that thought: running rails s does honour the configuration file. But it doesn't show the warnings.\n. In either case, code reloading (in development) works OK in single mode, but not in clustered mode...\n. ",
    "lenage": "Brava!\n. ",
    "mirceal": "Would you consider taking a PR for this?\nThis looks trivial(-ish) if I'm not totally misunderstating what's going on here. Would need to:\n Add a cipher list as a field in the MiniSSL context\n Read and apply in c extension / java part. \n. Can we please reopen the issue?. PR is being prepared and should land shortly. May we get some feedback on https://github.com/puma/puma/pull/1476 and on the test failures?\nFrom the looks of it it does not seem to be related to the code that is introduced. How can we run the whole test suite locally before submitting the PR to ensure . any update on this?. missing \", FILE\" from end of line. ",
    "t-anjan": "Yes, I am using the mysql2 gem.\nI understand what you're getting at. You're saying that puma doesn't\nconnect directly to the DB at all. Neither does it maintain the connection\nwith the DB. It delegates that work to the mysql2 gem.\nSo, should I post this issue on the mysql2 github repo?\nI should probably try the same setup with a postgres DB and the pg gem.\n. I faced a very similar issue. More details here: https://github.com/bundler/bundler/issues/6667 .\nThis comment by @deivid-rodriguez helped me find the root cause of the issue, which is due to the usage of a deprecated bundler method in the launcher.rb of the Puma gem. \nMaintainers - Please let me know if I should create a PR with the alternate bundler method.. ",
    "brianmario": "@t-anjan I would be curious to see how the pg gem acts here.\nDo you have any of the read_timeout, write_timeout or connect_timeout mysql2 options specified when creating the connection? It may be worth playing with those.\n. ",
    "ThomasCelen": "Hi, I added you to a test repository. It's an older version where ruby isn't yet upgraded to 2.3.0 but besides that it's unfortunately also leaking...\n. hi @evanphx , have you been able to spot any issue yet?\nMany thanks for your help!\n. hi @evanphx , can you please share if you already looked into my issue? If not I will attempt Unicorn webserver as I'm really convinced it's Puma.. The only thing I'm doubtful of is that some library is loaded incorrectly and causes the memory leak.\nCheers,\nT\n. @codehotter and @evanphx  I added you both as collaborators to my test repository.. https://github.com/ThomasCelen/test_app/invitations\nCan you please help me find the issue? I've been tracking this for months and the only thing I can figure is that, unfortunately, it's a Puma issue in combination with Ruby.\n. @evanphx just running the app will already trigger the leak actually so it's not difficult to replicate..\n. @evanphx  Indeed, just \"rails s\" to boot the application, run localhost:3000 (don't think there are any big DB migrations that are needed\") and then the app automatically starts increasing RAM.. I've been tracking it with \"puma worker killer\", though I've never been able to trace the source. \n. @evanphx I'll run my test environment for a while without Puma_Auto_tune so that I can show you a screenshot of my memory usage. Thanks already for your checks.\n. \n@evanphx This is a screenshot from 24hrs ago. Please know that Puma_Worker_killer is still turned on here. During the time frame of the screenshot there was very little traffic on the app. At the end you can see that it goes out of memory. This was when I was trying to change some data in the admin.\n. ",
    "TomLiu": "@ThomasCelen have you found out what the problem is?\n. ",
    "ce07c3": "@ThomasCelen I'm seeing this (Puma 3.6.0, Ruby 2.3.1, Rails 5.0.0.1):\n\nPretty similar to your memory growth between 1AM and 4AM, occurring once an hour. What could this be? Using New Relic, I was able to see that it coincides with Ruby GC runs:\n\n@evanphx could this be related to a GC memory leak? My code doesn't run once an hour peridiocally...or maybe Rails does something? Or Puma? I'm out of suggestions, even \u2753 \n. @schneems the same growth happens on idle dynos. \nWhy do the large memory jumps cooincide with the GC spiking? I have no recurring tasks, no \"unusual calls\" once an hour either. \n. And most importantly, shouldn't the GC reduce memory? I am open for suggestions on what to do; profiling, benchmarking, ....\n. Thanks! Will read up on it.\n. Thanks a bunch, will read up on it.\n21 okt. 2016 kl. 17:11 skrev Richard Schneeman notifications@github.com:\nHere's a primer http://www.schneems.com/2015/05/11/how-ruby-uses-memory.html\nNo, GC should not reduce memory. Up to Ruby 2.3 it will not free aggressively, or not enough to matter for most apps. It also takes a larger memory overhead before it would start to reclaim memory. There is a GC setting for 2.4 that can be set, but it won't impact this use case.\nEven if the app isn't doing any thing there are still event loops that create objects and do things like set timeouts, wait for incoming requests, etc. Not just from puma but other places in your code or libraries can run loops in new threads.\nYour seeing memory grow and then level off. It's totally normal.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Same here.\n. Thanks, @evanphx. My min size of threads equals my max size of threads as per Heroku's recommendation, as do the number of connections. I guess all is fine and Ruby 2.3.1 just has an insatiable appetite for memory. \n. Same here.. ",
    "kjvarga": "Is this still needed under Rails 5.2?  It looks like Rails 5.2 automatically closes connections that were held in other processes.. ",
    "mattyb": "Thanks @evanphx, but adjusting timeouts doesn't affect this. This error occurs without going through the ELB.\n. I've reproduced the error with just a puma config file (no rack app) here: https://github.com/mattyb/puma-socket-test\n. Thanks for working on this! I no longer see the SSL errors popping up in the console and there are new messages in the verbose logs about connections being closed. Unfortunately, even with the reproduction code updated to the 46416cb commit, I still see failed requests from ab.\nConcurrency Level:      25\nTime taken for tests:   21.217 seconds\nComplete requests:      3000\nFailed requests:        21\n   (Connect: 0, Receive: 0, Length: 21, Exceptions: 0)\nIt's possible that this is a problem with ab's implementation, though I believe I'm using the latest version. I confirmed that there are no errors if I bind tcp without ssl. I assume the fact that all the errors are failures in \"Length\" is relevant.\nI could try the new commit on our real app in our staging environment and see if the ELB handles whatever error is happening more cleanly, but it seems like something may still be awry in puma.\n. ",
    "alexbrahastoll": "Thanks a lot for the clarification, @evanphx ! I was incorrectly assuming that MRI's GIL would prevent  serving requests in parallel even with multiple Puma processes. Good to know it is possible to scale by increasing the number of cores in the server (as long as enough memory is available for the new processes, as you explained) =)\n. ",
    "TengZhangAutoGrid": "@alexbrahastoll the reason why Puma is powerful is it's multi-threading feature. Multi-processing can be done by other servers like Unicorn. I think Puma is trying to solve the problem of scale by increasing the number of cores in the server by threading.. Hi @evanphx , I came here randomly. But, I feel you are the guy who can answer my questions. \nRecently, we started a new project using puma. According to our need, we have to make it support multi-tenancy. But, again, we have to make sure it's thread safe.\nHere's a old solution: http://railscasts.com/episodes/388-multitenancy-with-scopes\nBut, I think it's not thread safe because it's using class varible. (Correct me if I'm wrong.)\nHere's a gem called Apartment doing this: https://github.com/influitive/apartment\nBut, after some research on this, I still can't figure out if it's really thread safe.\nQuestion1. Is Apartment thread safe with puma?\nQuestion2. If it's not, is there any a good way or gem doing multi-tenancy, and compatible with Puma?. ",
    "gjastrab": "@jjb here is a Rails 4 version of what @jrafanie suggested above.\nThis was intermittently happening for us as well, and I realized that our connection pool settings in config/database.yml were quite close to the threshold, but even after bumping the pool size higher this is still happening for us.\nRelevant sections of our puma config is:\nworkers 2\nthreads 0,6\nprune_bundler\nSo we are in clustered mode without preloading so we can do rolling updates.\nI just added the connection pool logging yesterday and bumped up our threads to be threads 6,6 as suggested in this issue and it does seem to have helped so far.\nOther relevant version info for us:\n- Ruby 2.2.4\n- Rails 4.1.6\n- Puma 3.4.0\n- Sidekiq 3.4.1\nIn this staging environment our pool size was bumped up to 22 (from the previous value of 18) so that there is more overhead above the 2 * 6 + 3 (3 comes from us starting Sidekiq with a concurrency setting of 3).\nAlso I just want to confirm since we are not pre-loading our app we should not have to do any special on_worker_boot or before_fork blocks due to are using prune_bundler right?\nI had asked this in the puma gitter.im a few weeks back as well. @jjb when you notice the leaking happening would you see the workers logging ! Detected parent died, dying ?\nAt some point when this happens the workers would still be running and if we curl the stats from the local control app we would see something like this for the last_status of each worker:\n\"last_status\": { \"backlog\":124, \"running\":0 }\n                            ^\n                              \\ this number will continue to climb until QUIT'ing puma and starting back up\n. @jjb cool watching our server all day today too the issue seems to have been corrected by pinning the thread #s together as well. Still seems like there's a bug here though right?\nAh pool per worker makes sense I think I had meant to confirm that here at some point but forgot to come back to it. Will try turning those down and ensure everything is still smooth.\n. ",
    "sgringwe": "Thanks @evanphx . I had previously lightly tried that, but ended up getting lots of actual timeouts. I will try it again though, thanks for the response.\n. @evanphx FWIW, I did try that and the library did not work. It's unclear to me if that's because LDAP protocool does not work that way or another reason. For now, I'm searching for alternative solutions.\n. @evanphx I don't believe so, since in almost all cases there is only one authentication going on at a time. For now I'll close this and will re-open if I have further information.\n. ",
    "willnet": "\ud83d\ude03 \n. ",
    "brahamshakti": "What could be the reason for such error like if my nginx conf file is right, my puma.rb file is right then what else I can check. Means I randomly getting 502 bad gateway and above error. If this is the nginx error then what should I add in nginx file so that it can work correctly with puma. \nAny suggestion to resolve this issue?\n. Are you talking about ruby code so to be honest my ruby app is not showing any error but my method has so many iterations apart from 20 mysql queries, May be that could be the reason why puma could not handled those thousands request which is coming to nginx in time ramp of 20 seconds\nBut still, Is my configuration is not right or am I doing something wrong.\n. Are you talking about ruby code so to be honest my ruby app is not showing any error but my method has so many iterations apart from 20 mysql queries, May be that could be the reason why puma could not handled those thousands request which is coming to nginx in time ramp of 20 seconds\nBut still, Is my configuration is not right or am I doing something wrong.\n. ",
    "Fudoshiki": "@evanphx \n``` ruby\n----------------------------------------------------------------------------\nfrozen_string_literal: true\nNo license\nEncoding: utf-8\n----------------------------------------------------------------------------\nPuma can serve each request in a thread from an internal thread pool.\nThe \"threads\" method setting takes two numbers a minimum and maximum.\nAny libraries that use thread pools should be configured to match\nthe maximum value specified for Puma. Default is set to 5 threads for minimum\nand maximum, this matches the default thread size of Active Record.\n\nthreads_count = ENV.fetch('RAILS_MAX_THREADS') { 5 }.to_i\nthreads threads_count, threads_count * 2\nSpecifies the number of \"workers\" to boot in clustered mode.\nWorkers are forked webserver processes. If using threads and workers together\nthe concurrency of the application would be max \"threads\" * \"workers\".\nWorkers do not work on JRuby or Windows (both of which do not support\nprocesses).\n\nworkers ENV.fetch('WEB_CONCURRENCY') { 2 }\nAllow workers to reload bundler context when master process is issued\na USR1 signal. This allows proper reloading of gems while the master\nis preserved across a phased-restart. (incompatible with preload_app)\n(off by default)\nprune_bundler\nSpecifies the \"port\" that Puma will listen on to receive requests, default is 3000.\n\nport ENV.fetch('PORT') { 3000 } unless ENV['RAILS_ENV'] == 'production'\nSpecifies the \"environment\" that Puma will run in.\n\nenvironment ENV.fetch('RAILS_ENV') { 'development' }\nAllow puma to be restarted by \"rails restart\" command.\nplugin :tmp_restart\nStore the pid of the server in the file at \"path\".\n\npidfile 'tmp/pids/puma.pid'\nChange the default timeout of workers\n\nworker_timeout 30\nif ENV['RAILS_ENV'] == 'production'\n  # Use the \"preload_app!\" method when specifying a \"workers\" number.\n  # This directive tells Puma to first boot the application and load code\n  # before forking the application. This takes advantage of Copy On Write\n  # process behavior so workers use less memory. If you use this option\n  # you need to make sure to reconnect any threads in the \"on_worker_boot\"\n  # block.\n  #\n  # preload_app!\n# The code in the \"on_worker_boot\" will be called if you are using\n  # clustered mode by specifying a number of \"workers\". After each worker\n  # process is booted this block will be run, if you are using \"preload_app!\"\n  # option you will want to use this block to reconnect to any threads\n  # or connections that may have been created at application boot, Ruby\n  # cannot share connections between processes.\n  #\n  # on_worker_boot do\n  # end\n# Bind unix domain socket\n  bind ENV.fetch('PUMA_SOCK') { 'unix://tmp/sockets/puma.sock' } if ENV['RAILS_ENV'] == 'production'\n# Use \"path\" as the file to store the server info state. This is\n  # used by \"pumactl\" to query and control the server.\n  #\n  state_path ENV.fetch('SERVER_STATE') { 'tmp/pids/puma.state' }\n# === Puma control rack application ===\n# Start the puma control rack application on \"url\". This application can\n  # be communicated with to control the main server. Additionally, you can\n  # provide an authentication token, so all requests to the control server\n  # will need to include that token as a query parameter. This allows for\n  # simple authentication.\n  #\n  # Check out https://github.com/puma/puma/blob/master/lib/puma/app/status.rb\n  # to see what the app has available.\n  #\n  activate_control_app ENV.fetch('PUMACTL_SOCK') { 'unix://tmp/sockets/pumactl.sock' }\nend\n```\nReproduce:\n1) Create new Rails 5.0.0 app with current config, psych 2.1.0, RAILS_ENV=development\n2) rails s\n. ruby 2.3.1\nI reinstalled ruby, then gem update --system, gem update, bundle\npuma version 3.6.0, issue exist\n. 1) install rvm\n2) install ruby-2.3.1\n3) gem update --system\n4) gem update\n5) bundle\n6) rails s\n. @schneems https://github.com/puma/puma/blob/master/lib/puma/const.rb update version here\nruby\nPUMA_VERSION = VERSION = \"3.8.0\".freeze\n. ",
    "kleinjm": "Running gem pristine --all fixed it.\nhttps://bugs.ruby-lang.org/issues/11169\n. ",
    "marc": "Have you tried adding the Java parameters as JAVA_OPTS? That should work.\n. ",
    "walidvb": "@jrafanie apologies, should have mentioned that i am using 3.5.0 indeed. Was a coincidence the issue came the same day as the upgrade, but that happened on both 3.4 and 3.5.0\nMaybe what is missing is to listen on not only the localhost IPs, but also on the locally assigned IP?\n. @jrafanie that version doesn't help, unfortunately. \nIn all fairness, maybe this is out of the scope of this gem. However, why would a locally run server not listen to network calls? It's not an external address in the end, it is the address of the machine, no?\nIn my case, i'm using puma via rails, so i presume i'd need to config rails to bind the IP when starting puma.\n. ",
    "AKovtunov": "Downgrading to 3.4.0 works as charm\n. After that I was wondering the same too and cleared a bit my hosts file.\nI was working on a project with multi-service architecture and needed to bind some routes to 127.0.0.1 :) \nBut the error is still here, here is my basic hosts that were installed by ubuntu\n[1] pry(#<Puma::Binder>)> localhost_addresses\n=> [\"127.0.0.1\", \"127.0.0.1\"]\n[2] pry(#<Puma::Binder>)> TCPSocket.gethostbyname \"localhost\"\n=> [\"localhost\", [\"alexandr-Lenovo-IdeaPad-Y500\", \"localhost\", \"alexandr-Lenovo-IdeaPad-Y500.localhost\"], 2, \"127.0.0.1\", \"127.0.0.1\"]\n```\n\u2039ruby-2.3.1\u203a  \u2039master*\u203a $ cat /etc/hosts                                                                                                              1 \u21b5\n127.0.0.1   localhost\n127.0.1.1   alexandr-Lenovo-IdeaPad-Y500\n127.0.0.1 alexandr-Lenovo-IdeaPad-Y500.localhost alexandr-Lenovo-IdeaPad-Y500 localhost\nThe following lines are desirable for IPv6 capable hosts\n::1     ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n```\n. ",
    "schuetzm": "Please reopen, the bugfix in 84fdc1b does not work if the addresses for localhost are [\"127.0.0.1\", \"::1\"].\nTo reproduce, put the following lines into your /etc/hosts:\n127.0.0.1   localhost\n::1         localhost ipv6-localhost ipv6-loopback\n. @daya Which error are you getting exactly? \"Address already in use\", or \"Address not available\"? If it's the latter, it's likely a misconfiguration on your side, see #1062.. Yes, nothing else is listening on port 3000. But looking at the error again, it is EADDRNOTAVAIL, not EADDRINUSE. And indeed, I can't bind to any port on ::1 (even when I skip binding 127.0.0.1). As it turns out, my system configuration was somehow messed up: IPv6 was disabled, but there were IPv6 entries in /etc/hosts. Go figure... Sorry for the noise.\n. ",
    "daya": "@evanphx please reopen this issue @schuetzm steps are enough to reproduce the issue. @jeanlescure thanks that saved my day. ",
    "delner": "Getting this error in 3.6.2, while running in Rails & Docker via rails s:\n/usr/local/bundle/gems/puma-3.6.2/lib/puma/binder.rb:266:in `initialize': Cannot assign requested address - bind(2) for \"::1\" port 3000 (Errno::EADDRNOTAVAIL)\n    from /usr/local/bundle/gems/puma-3.6.2/lib/puma/binder.rb:266:in `new'\n    from /usr/local/bundle/gems/puma-3.6.2/lib/puma/binder.rb:266:in `add_tcp_listener'\n/etc/hosts:\n127.0.0.1   localhost\n::1 localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n172.20.0.5  4b1ea4eb06a8\nRunning with rails s -b 0.0.0.0 allows it to start.. @nateberkopec Just an FYI, that /etc/hosts is not a user specific configuration, thats the default from running Ubuntu in Docker. Anyone who uses that image in Docker (a ton of people) could possibly have the same problem.. If you'd like to reproduce the hosts file state via Docker, you can do that via docker run --rm ubuntu cat /etc/hosts. As far as Puma itself is concerned, I don't have a repro for you.. ",
    "gcaracausi": "You have to modify /etc/hosts and remove localhost from the ipv6 line.\nFrom ::1 localhost ip6-localhost ip6-loopback to ::1 ip6-localhost ip6-loopback. You have to modify /etc/hosts and remove localhost from the ipv6 line.\nFrom ::1 localhost ip6-localhost ip6-loopback to ::1 ip6-localhost ip6-loopback\n. ",
    "fera2k": "Thanks for the fix. Working fine!\n. ",
    "banister": "Thanks homie \n. ",
    "barbolo": "I'm using the Mechanize gem and I think I have a similar problem with leaking TCP Sockets.\nHave anyone evolved this issue?. ",
    "jeremyevans": "Roda rescues both StandardError and ScriptError in its error handling plugin, it might be a good idea for Puma to do something similar.\n. ",
    "matobinder": "This didn't seem to fix the issue. The change to the dsl.rb file is what is causing this issue.\nruby\nbind \"ssl://#{host}:#{port}?cert=#{opts[:cert]}&key=#{opts[:key]}&verify_mode=#{opts[:verify_mode] || 'peer'}\"\nWell I guess both changes in that pull request are making the problem\n. Sorry for delaying getting back at this. Puma wasn't the problem at all in this case, nor was the application access it, they had changed some networking on me . . I was too about to submit an issue on this, I'm stuck for a bit since I'm building apps in CloudFoundry and the ruby-buildpacks don't yet have the latest version of bundler.\nNote I was just looking at bundler source, not that it would probably fix having an older version of bundler, but it looks like Bundler.clean_env is deprecated\nhttps://github.com/bundler/bundler/blob/master/lib/bundler.rb#L257\n. Fixed it for me. I tried it with bundler 1.9.7 and bundler 1.15.1\nGoing against puma master (fec20f0f05a565a8f5f20e87d1280795ffee901c). ",
    "edwardmp": "@itsNikolay \nSeeing exactly the same issue here. No trouble with ugly syntax, but with ssl_bind the issue does occur.\n. I think I have experienced similar behavior. Puma seems to be in a deadlock and does not recover unless the full docker instance with puma in it is restarted. I do not have 100% CPU utilization though.\nI noticed that this issue started occurring after we updated from Puma 3.6 to 3.6.2.\nFor now I've rolled back to 3.6 and the issue disappeared. Logs did not seem to reveal anything out of the ordinary. The fact that it was caused in 3.6.1 or 3.6.2 might give you some pointers.\n@koenpunt which version of Puma are you using?. I'm still seeing this issue on Rails 5.0.2. Will open issue on Rails github.. @whitehat101 @djones \nYes that's the issue and for me too this makes ActionCable unusable in 5.0.1.\nI know nothing about sockets so my fixes were just to determine if that was the actual issue (e.g. rerouting write_nonblock to the normal write method)\nThe only thing that would need to be done is Puma's MiniSSL socket implemeting the write_nonblock method.. @evanphx @djones \nThis is great! I actually thought about something similar (stubbing those methods out) but thought that would be unacceptable.\n@djones actually they already know errors like these are masked (they are ignoring any errors; the actual error handling method has an empty body...) so my proposal was to simply log these messages: see https://github.com/rails/rails/issues/28362#issuecomment-285904683. @evanphx \nI'm working towards properly logging socket errors.\nHopefully I can submit this PR to the Rails team later today.\nEdit: Rails PR submitted: https://github.com/rails/rails/pull/28757. @evanphx my Rails PR for extra logging has been merged in. Can you merge this fix in and release a new Puma version in the coming week so I can finally use ActionCable again?. @evanphx bump. I'm not sure everyone is aware that ActionCable is completely broken in combination with Rails 5.0.1+ without this patch. ",
    "gitjake": "If you need a workaround for this issue manually set verify_mode: 'none' \nRuby\nssl_bind '0.0.0.0', '9292', {\n  key:  \"#{app_dir}/config/certs/server.key\",\n  cert: \"#{app_dir}/config/certs/server.crt\",\n  verify_mode: 'none'  \n}\n. ",
    "bdewater": "I'm almost done migrating a CentOS 6 server to a fresh installation of CentOS 7 w/ systemd. Regarding RVM/rbenv I've run in to the issue that systemd runs commands in pristine environment, e.g. shell hooks from RVM/rbenv do not 'just work'.\nA common workaround for this seems to be to spawn an interactive shell, e.g. ExecStart=/bin/bash -lc 'bundle exec puma' but that looks like a bit of a hack. A cleaner solution for RVM seems to be to generate a wrapper but I haven't tried it myself. Rbenv recommends setting PATH and using binstubs but that didn't work for me (journalctl reports '/usr/bin/env: bash: No such file or directory'). \nI did get it to work by using rbenv shims directly (example below is from a systemwide install so the directories are a bit different from default):\nWorkingDirectory=/var/www/my_rails_app/current\nExecStart=/usr/local/rbenv/shims/bundle exec puma -C config/puma.rb\nSyslogIdentifier=my_rails_app\nThe SyslogIdentifier bit ensures that journalctl output does not default to 'bundle'. Hope this helps!\n. I think you should use systemd as it is meant to be, like written here: Don\u2019t Daemonize your Daemons!. According to the systemd manpage on writing unit files:\n\nIf possible, do not use the Type=forking setting in service files. But if you do, make sure to set the PID file path using PIDFile=. See systemd.service(5) for details.\n\nIMO https://github.com/seuros/capistrano-puma should be updated to use systemd now that all major Linux distributions have switched to it. I've adapted its default templates and overridden a couple of rake tasks to  make the gem work with systemd. I'll open a PR there somewhere later this week with those changes.. The single test failure is unrelated to the change.. ",
    "macarthy": "@bdewater nice SyslogIdentifier=my_rails_app tip. Thanks!\n. ",
    "ndrsv": "Using rvm (system install, no gemsets, set default ruby) the default wrapper works. For example:\nExecStart=/usr/local/rvm/wrappers/default/bundle exec puma .... ",
    "daz": "What about multiple sites? Can anyone point me in the right direction?. ",
    "stiig": "@daz for multiple sites create systemd script for each site and it all. ",
    "Dombo": "@daz my recommendation if you were to have multiple sites would be to look at the concept of Instantiated Services in SystemD.. Hi, we've encountered a usecase for this inside one of our production applications - I notice that CI is failing on unrelated tests to this PR, has any of the maintainers had a chance to review the comment made by @daveallie on 07/12/2016.\nThanks . @nateberkopec sorry if I wasn't clear but yes it's the same use case as @daveallie's. ",
    "ankit-ricardokaka": "No problem with Puma. Instead of \n  w.start = \"RAILS_ENV=staging bundle exec puma -C #{RAILS_ROOT}/config/puma.rb\"\nif you write this,\n  w.start = \"cd #{RAILS_ROOT} && RAILS_ENV=staging bundle exec puma -C #{RAILS_ROOT}/config/puma.rb\" then it will start working.\n. ",
    "swrobel": "@evanphx will do, closing and will submit and alternate PR\n. So you don't see the value in having callbacks in single mode?. I'll respectfully disagree with your definition of \"clearly documented\" but that's sort of moot.\nMy use case is that I'm using the tmp_restart plugin and I want to display a notification using terminal-notifier when the restart completes.. I discovered that I'm able to get access to the callbacks by setting workers 1 which basically forces clustered mode. Seems like an arbitrary distinction to me, but I guess \ud83e\udd37\u200d\u2642\ufe0f, it works.. ",
    "danielnc": "Any way to check max number of workers/threads my OS supports or how to increase that number?\n. We had plenty of memory(16gb of ram and was still available to resources and we can upgrade that if necessary)\nFile descriptors we had over 80k available\nwhat is the most workers/threads you seen work on a production environment? do you know what was the config?\n. Just an update to whoever uses CentOS too\nhttp://wiki.brekeke.com/wiki/Increase-CentOS-6-Threads-or-Processes-Limit\nWe added unlimited threads and we were able to create 16 workers with 512 threads\n. ",
    "donce": "Hey @evanphx, @JeongHan-Choi.\nI am having the same issue - ruby 2.3.1, puma 3.6.0, rails 5.0.0.\nIt happens each time old directory gets deleted.\nI've checked $LOAD_PATH - there aren't any duplicated paths.\n. @JeongHan-Choi Thanks for help.\nFor phased-restart, I am using kill -USR1 without bundle exec.\nFor starting puma, I am using bundle exec puma ... - otherwise, it doesn't load gems. Do you mean I should start puma without bundle exec? How does it load gems then?\n. ",
    "paz-raon": "@donce \nif you use 'bundle exec' command, do not run puma on bundle context.\nbundle context is not available when old-directory is deleted.\n1. gem install puma\n2. pumactl -P puma.pid -F puma_production.rb phased-restart \n   (don't use bundle exec)\nI hope everything works out well.\n. @donce \nYes, You should start puma without bundle exec.\nto load gems, using 'prune_bundler' in puma's configuration.\n. ",
    "kainosnoema": "This just started happening to us as we're testing upgrading to Ruby 2.3.1. No other configuration or version changes. Haven't had a chance to dig into workarounds yet\u2014any tips on what's worked?. ",
    "hovancik": "Same issue here.  I use SIGUSR1 as well. Error happens on every 7th deploy. I keep 6 versions. \n/home/www/deploy/.rbenv/versions/2.3.1/lib/ruby/2.3.0/rubygems/stub_specification.rb:148:in `missing_extensions?': undefined method `missing_extensions?' for nil:NilClass (NoMethodError)\nruby 2.3.1\nrails 5.0.4\npuma 3.9.1\npuma.rb\n```\nthreads_count = ENV.fetch(\"RAILS_MAX_THREADS\") { 5 }.to_i\nthreads threads_count, threads_count\nworkers ENV.fetch(\"WEB_CONCURRENCY\") { 2 }\nenvironment ENV.fetch(\"RAILS_ENV\") { \"development\" }\nbind \"unix://#{ENV.fetch('APP_DIR')}/shared/tmp/sockets/puma.sock\"\npidfile \"#{ENV.fetch('APP_DIR')}/shared/tmp/pids/puma.pid\"\ndirectory \"#{ENV.fetch('APP_DIR')}/current\"\nactivate_control_app \"unix://#{ENV.fetch('APP_DIR')}/shared/tmp/sockets/pumactl.sock\", { auth_token: 'sddsfsd' }\nstdout_redirect \"log/puma.stdout.log\", \"log/puma.stderr.log\", true\nprune_bundler true\n```\n. so I have tried different versions of ruby and version 2.4.1 seems to work fine. \n```\nRUBY VERSION\n   ruby 2.4.1p111\nBUNDLED WITH\n   1.15.1\n```. ",
    "Frexuz": "2.4.1 seems to have fixed it for me as well.. I know, I know. Was hoping the clue was in the infinite restart :)\nClose and reopen if I find something new?. ",
    "cevans111": "More detail, after finding a similar issue from 2013 (#422) , I have tried the following startup commands with the same result:\n$ jruby -S pumactl -S puma.state start -d\n$ jruby -S bundle exec puma -S puma.state start -d\n$ bin/puma -S puma.state start -d\n. ",
    "giovdk21": "Hello,\nwhat do you mean? As far as I understand, that script is ran by root and it should be able to identify which user should run rbenv by looking at the /etc/puma.conf file, correct?\nAt the moment it is working that way when you start the service, but not when you stop it.\n. @evanphx hi,\nI tried the latest version of the script and still getting this; could you please check my previous message and clarify on that? Did I misunderstand how it's meant to be used?\nthanks. ",
    "spotscale": "I have the same issue.. ",
    "jaybloke": "Also have the same issue.. Experienced the same problem - simply updating to the latest version of bundler (1.15.1) fixed it for me.. ",
    "brodock": "@evanphx do you think you can support the rack.websocket effort?\n. ",
    "wwood": "Hi,\nI'm not a maintainer but the first one was already reported as https://github.com/puma/puma/issues/995, and I can confirm the second one: we observed it during the Guix build.\n. ",
    "wdrury": "I've had this happen too, several times over the last few days, running in development mode on an AWS instance running Ubuntu 14.04.4 where I simply started it with $ nohup rails server &\nAll I see in the logs, besides normal web activity, is what looks like an orderly shutdown, although I don't know what initiates it:\n\n\nGracefully stopping, waiting for requests to finish\n  === puma shutdown: 2016-08-16 00:35:59 +0000 ===\nGoodbye!\n  Exiting\n\n\nThe only change to the environment is a bundle update I did a few days ago.  I could revert my Gemfile and re-install to see if that causes the random exits to stop.\n. ",
    "CaptainStiggz": "@evanphx As far as I can tell, all I have is puma.access.log and puma.error.log. Do I need to configure puma to send stdout and stderr to one of these logs?\nI am using the gem 'mimemagic' https://github.com/minad/mimemagic which depends on the Ubuntu package 'libmagic' or 'libmagic-dev', I forget which one. Could be the bad extension. \n. I assume we're talking about puma.rb created with initial rails configuration? \n``` ruby\nPuma can serve each request in a thread from an internal thread pool.\nThe threads method setting takes two numbers a minimum and maximum.\nAny libraries that use thread pools should be configured to match\nthe maximum value specified for Puma. Default is set to 5 threads for minimum\nand maximum, this matches the default thread size of Active Record.\n\nthreads_count = ENV.fetch(\"RAILS_MAX_THREADS\") { 5 }.to_i\nthreads threads_count, threads_count\nSpecifies the port that Puma will listen on to receive requests, default is 3000.\n\nport        ENV.fetch(\"PORT\") { 3000 }\nSpecifies the environment that Puma will run in.\n\nenvironment ENV.fetch(\"RAILS_ENV\") { \"development\" }\nSpecifies the number of workers to boot in clustered mode.\nWorkers are forked webserver processes. If using threads and workers together\nthe concurrency of the application would be max threads * workers.\nWorkers do not work on JRuby or Windows (both of which do not support\nprocesses).\n\nworkers ENV.fetch(\"WEB_CONCURRENCY\") { 2 }\nUse the preload_app! method when specifying a workers number.\nThis directive tells Puma to first boot the application and load code\nbefore forking the application. This takes advantage of Copy On Write\nprocess behavior so workers use less memory. If you use this option\nyou need to make sure to reconnect any threads in the on_worker_boot\nblock.\n\npreload_app!\nThe code in the on_worker_boot will be called if you are using\nclustered mode by specifying a number of workers. After each worker\nprocess is booted this block will be run, if you are using preload_app!\noption you will want to use this block to reconnect to any threads\nor connections that may have been created at application boot, Ruby\ncannot share connections between processes.\n\non_worker_boot do\nActiveRecord::Base.establish_connection if defined?(ActiveRecord)\nend\nAllow puma to be restarted by rails restart command.\nplugin :tmp_restart\n```\n. @evanphx Sorry I forgot to respond. Good advice, thanks!\n. ",
    "Vipala": "I am facing the same issue. I increased the number of workers too. No luck can you please help me with this. ",
    "b1akely": "@evanphx yes I installed the latest version of puma-dev and received the error above\n. @evanphx \nMacBook-Pro:app-name name$ puma-dev -V\nVersion: v0.10 (go1.7)\nMacBook-Pro:app-name name$ ps\n  PID TTY           TIME CMD\n54326 ttys000    0:00.12 -bash\n69600 ttys000    0:09.51 puma 3.6.0 (tcp://localhost:3000) [app-name]  \n69603 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69604 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69605 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69606 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69607 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69608 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69609 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/con\n69610 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69611 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69612 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69613 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69614 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69615 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69616 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69618 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/db\n69619 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69620 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69621 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69622 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69623 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69624 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69625 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/app\n69626 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69627 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69628 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/.rvm/gems/ruby-2.3\n69629 ttys000    0:00.01 /Users/name/.rvm/gems/ruby-2.3.0/gems/rb-fsevent-0.9.7/bin/fsevent_watch --latency 0.1 /Users/name/Desktop/app-name/tes\n55531 ttys001    0:00.15 -bash\nMacBook-Pro:app-name name$\n. @evanphx sure:\nMacBook-Pro:User user$ ps ax | grep puma\n46155   ??  S      0:06.45 /usr/local/Cellar/puma-dev/0.10/bin/puma-dev -launchd -dir ~/.puma-dev -d dev -timeout 15m0s\n69600 s000  S+     0:16.69 puma 3.6.0 (tcp://localhost:3000) [appname]  \n99391 s003  S+     0:00.00 grep puma\nMacBook-Pro:User user$\n. ",
    "ericboehs": "@evanphx I have encountered this problem as well. Backtrace here.\nRails 5.0.0.1\npuma 3.6.0\nNo puma-dev\nmacOS Sierra\nHere's my ps ax | grep puma:\n3335 s002  S+     0:01.89 puma 3.6.0 (tcp://localhost:3000) [hats]  \n 3342 s002  S+     0:00.18 puma: cluster worker 0: 3335 [hats]\nThis seems independent from Rails though. If I start the server via puma -b tcp://localhost:3000 and then touch tmp/restart.txt it yields the same issue.\nIf I start it with puma -b tcp://127.0.0.1:3000, it works perfectly.\nMy guess is something to do with IPv6? I tried commenting out ::1 localhost in /etc/hosts with no luck. I didn't reboot or do anything to clear my DNS though.\nThis is far as my journey goes. \ud83d\ude04 I hope this info helps.\n. @evanphx I see you fixed this for puma-dev via https://github.com/puma/puma-dev/commit/49f2549c14fb5156255809b30c4273241dc3a231. Is there a solution for those of us who just use puma without puma-dev?\nThanks!\nEdit:\nRepro steps are as follows:\n\nrails new puma-crash\nSet workers to 2 in config/puma.rb (just uncomment line 24)\nrails s\nCtrl-C. I believe this is the same issue as https://github.com/puma/puma/issues/1046. I'm still experiencing this with Rails 5.1 and Puma 3.10.0.\n\nEdit: Hmm no, I think this is different. On previous versions of Puma/Rails I could specify workers 1 in my puma config and as long as I started it with rails server -b 127.0.0.1 I could restart using rails restart. This is no longer the case. If I simply specify that I want to use a worker it fails to restart, hanging on \"Gracefully shutting down workers...\". Setting workers 0 seems to fix it though.. I've found sending SIGQUIT (via Ctrl-\\) will kill the process.. ",
    "robinsingh-bw": "The leak in my app is much more evident:\n2016-08-14T14:28:43.284567+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=168.46MB sample#memory_rss=168.43MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=47343pages sample#memory_pgpgout=4218pages sample#memory_quota=512.00MB\n2016-08-14T14:29:03.445347+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=168.90MB sample#memory_rss=168.88MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72567pages sample#memory_pgpgout=29328pages sample#memory_quota=512.00MB\n2016-08-14T14:29:24.378890+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:29:24.378968+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=168.95MB sample#memory_rss=168.92MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72579pages sample#memory_pgpgout=29328pages sample#memory_quota=512.00MB\n2016-08-14T14:29:45.509970+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:29:45.510245+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.09MB sample#memory_rss=169.07MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72616pages sample#memory_pgpgout=29328pages sample#memory_quota=512.00MB\n2016-08-14T14:30:06.424100+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:30:06.424205+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.28MB sample#memory_rss=169.25MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72685pages sample#memory_pgpgout=29349pages sample#memory_quota=512.00MB\n2016-08-14T14:30:27.572912+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:30:27.573007+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.55MB sample#memory_rss=169.52MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72753pages sample#memory_pgpgout=29349pages sample#memory_quota=512.00MB\n2016-08-14T14:30:48.515228+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:30:48.515513+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.57MB sample#memory_rss=169.54MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72758pages sample#memory_pgpgout=29349pages sample#memory_quota=512.00MB\n2016-08-14T14:31:09.437338+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:31:09.437410+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.61MB sample#memory_rss=169.58MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72773pages sample#memory_pgpgout=29354pages sample#memory_quota=512.00MB\n2016-08-14T14:31:30.519127+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:31:30.519460+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.88MB sample#memory_rss=169.86MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72844pages sample#memory_pgpgout=29354pages sample#memory_quota=512.00MB\n2016-08-14T14:31:51.502919+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:31:51.503097+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=169.92MB sample#memory_rss=169.89MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72854pages sample#memory_pgpgout=29354pages sample#memory_quota=512.00MB\n2016-08-14T14:32:12.788326+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:32:12.788326+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=170.23MB sample#memory_rss=170.21MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72934pages sample#memory_pgpgout=29354pages sample#memory_quota=512.00MB\n2016-08-14T14:32:33.741972+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:32:33.742083+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=170.31MB sample#memory_rss=170.29MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=72967pages sample#memory_pgpgout=29367pages sample#memory_quota=512.00MB\n2016-08-14T14:32:54.631010+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:32:54.631073+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=170.54MB sample#memory_rss=170.52MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=73026pages sample#memory_pgpgout=29367pages sample#memory_quota=512.00MB\n2016-08-14T14:33:15.433935+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00\n2016-08-14T14:33:15.434042+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=170.62MB sample#memory_rss=170.59MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=73045pages sample#memory_pgpgout=29367pages sample#memory_quota=512.00MB\n2016-08-14T14:33:36.461219+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:33:36.461411+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=170.82MB sample#memory_rss=170.79MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=73097pages sample#memory_pgpgout=29367pages sample#memory_quota=512.00MB\n2016-08-14T14:33:57.767663+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:33:57.767801+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=171.04MB sample#memory_rss=171.01MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=73230pages sample#memory_pgpgout=29445pages sample#memory_quota=512.00MB\n2016-08-14T14:34:18.833845+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:34:18.833939+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=171.11MB sample#memory_rss=171.08MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=73248pages sample#memory_pgpgout=29445pages sample#memory_quota=512.00MB\n2016-08-14T14:34:39.339373+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:34:39.339452+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=171.39MB sample#memory_rss=171.36MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=84232pages sample#memory_pgpgout=40357pages sample#memory_quota=512.00MB\n2016-08-14T14:35:00.500665+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:35:00.500768+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=171.45MB sample#memory_rss=171.42MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=84248pages sample#memory_pgpgout=40357pages sample#memory_quota=512.00MB\n2016-08-14T14:35:21.464147+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:35:21.464217+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=171.64MB sample#memory_rss=171.61MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=84297pages sample#memory_pgpgout=40357pages sample#memory_quota=512.00MB\n2016-08-14T14:35:42.276521+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:35:42.276521+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=171.70MB sample#memory_rss=171.67MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=91876pages sample#memory_pgpgout=47922pages sample#memory_quota=512.00MB\n2016-08-14T14:36:03.521556+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:36:03.521663+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.00MB sample#memory_rss=171.97MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=91954pages sample#memory_pgpgout=47922pages sample#memory_quota=512.00MB\n2016-08-14T14:36:24.454603+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:36:24.454736+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.13MB sample#memory_rss=172.10MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=91987pages sample#memory_pgpgout=47922pages sample#memory_quota=512.00MB\n2016-08-14T14:36:45.613863+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:36:45.613863+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.20MB sample#memory_rss=172.17MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=92005pages sample#memory_pgpgout=47922pages sample#memory_quota=512.00MB\n2016-08-14T14:37:06.528348+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:37:06.528460+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.48MB sample#memory_rss=172.45MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=105845pages sample#memory_pgpgout=61690pages sample#memory_quota=512.00MB\n2016-08-14T14:37:27.785310+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:37:27.785464+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.55MB sample#memory_rss=172.52MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111444pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:37:48.725610+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.73MB sample#memory_rss=172.70MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111491pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:37:48.725385+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:38:10.470882+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:38:10.471080+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=172.82MB sample#memory_rss=172.79MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111513pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:38:30.519422+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:38:30.519528+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=173.08MB sample#memory_rss=173.05MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111581pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:38:51.339172+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:38:51.339305+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=173.13MB sample#memory_rss=173.10MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111593pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:39:12.503860+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=173.38MB sample#memory_rss=173.35MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111657pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:39:12.503755+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:39:33.575267+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:39:33.575371+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=173.74MB sample#memory_rss=173.71MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111749pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:39:54.499242+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:39:54.499318+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=173.77MB sample#memory_rss=173.74MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111756pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:40:15.518385+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:40:15.518510+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=173.82MB sample#memory_rss=173.79MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111770pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:40:36.496464+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:40:36.496749+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.07MB sample#memory_rss=174.05MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111835pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:40:57.371656+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:40:57.371721+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.14MB sample#memory_rss=174.11MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111852pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:41:18.639802+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:41:18.639934+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.36MB sample#memory_rss=174.34MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111909pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:41:39.485795+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:41:39.485932+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.44MB sample#memory_rss=174.41MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111929pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:42:00.524753+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:42:00.524907+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.59MB sample#memory_rss=174.57MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=111968pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:42:21.208558+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:42:21.208612+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.83MB sample#memory_rss=174.80MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=112028pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:42:42.424861+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:42:42.424961+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=174.89MB sample#memory_rss=174.87MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=112045pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:43:03.808435+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:43:03.808739+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=175.09MB sample#memory_rss=175.07MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=112096pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:43:24.567501+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:43:24.567671+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=175.14MB sample#memory_rss=175.11MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=112107pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:43:45.379209+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00\n2016-08-14T14:43:45.379318+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=175.27MB sample#memory_rss=175.24MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=112140pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n2016-08-14T14:44:06.228378+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#load_avg_1m=0.00 sample#load_avg_5m=0.00 sample#load_avg_15m=0.00\n2016-08-14T14:44:06.228546+00:00 heroku[web.1]: source=web.1 dyno=heroku.50110634.08176180-a5e9-4073-9941-93fe83ce6dcf sample#memory_total=175.57MB sample#memory_rss=175.55MB sample#memory_cache=0.03MB sample#memory_swap=0.00MB sample#memory_pgpgin=112219pages sample#memory_pgpgout=67272pages sample#memory_quota=512.00MB\n. @evanphx The test app on heroku does not receive any traffic at all, the usage just starts going up as soon as the server starts. \nAlso, the logs I pasted are in sequence, I did not cut anything out. It looks like some background thread might be leaking. The leak is much larger in my app where the memory usage is going up by megabytes (not KB like with the test app) even though I am making no web requests.\n. @evanphx Its a hobby instance not a free one. It restarts every 24 hours.\n. @Edwardzyc The test app just keeps going up until the instance is restarted, my app starts getting R14 out of memory.\n. I have taken some screenshots of the memory usage graph from heroku.\nAt the beginning usage looks like this: http://imgur.com/brKLOo4\nFirst log line:\n2016-08-29T23:24:26.780030+00:00 heroku web.1 - - source=web.1 dyno=heroku.55116999.055abfbe-7dc5-4b65-88c4-27f5de6383c5 sample#memory_total=62.27MB sample#memory_rss=62.27MB sample#memory_cache=0.00MB sample#memory_swap=0.00MB sample#memory_pgpgin=16159pages sample#memory_pgpgout=217pages sample#memory_quota=512.00MB\nAt the end the usage looks like this: http://imgur.com/M8KMHyQ\nLast log line:\n2016-08-30T11:58:25.453427+00:00 heroku web.1 - - source=web.1 dyno=heroku.55116999.055abfbe-7dc5-4b65-88c4-27f5de6383c5 sample#memory_total=68.87MB sample#memory_rss=66.10MB sample#memory_cache=0.63MB sample#memory_swap=2.14MB sample#memory_pgpgin=20634pages sample#memory_pgpgout=18882pages sample#memory_quota=512.00MB\nDuring the same time my production app consumed almost 50mb (although it appears the rss actually went down by 23mb and swap memory went up by 77mb, not sure if thats relevant). Heres the start and end screenshots for it:\nhttp://imgur.com/j0ekx6j\nhttp://imgur.com/1mRebPz\nNote that I did not make any web requests during the time to either of the apps.\n. @evanphx It just happens with the other app, smaller one just goes up ~10mb before heroku restarts the dyno.\n. Its the 24 hour rolling restart.\n. I posted some screenshots of it earlier: https://github.com/puma/puma/issues/1047#issuecomment-243418911\n. I see, thanks for looking into this. I just find it peculiar that the memory usage is spiking in my app even though there are 0 web requests, it only happens on heroku not on my local server. Anyway i resorted to a 1gb heroku instance (perhaps that's their intent?).\n. ",
    "berardpb": "I'm experiencing the same issue. \n. ",
    "dcorlett": "We have the same \"ever increasing memory\" issue with puma on Heroku, and have never been able to resolve it (since moving to puma 6 months ago).  I've spent quite a bit of time with derailed in my local dev environment, but have never been able to reproduce what we're seeing on Heroku.  The \"solution\" mentioned in various other threads is to run the pumaworkerkiller gem, but this is a hack and it would certainly be nice to put this issue to rest once and for all!  @robinsingh-bw thanks for reproducing it in isolation with a minimalist Heroku app.\n. I've watched this in a variety of conditions (all on our staging or production sites on Heroku), and I can say that my experience shows that it does NOT ever level out.  We eventually had to implement pumaworkerkiller or else we consistently see out of memory errors and swapping.  It is simply a matter of waiting long enough.\n. ",
    "Qwertie-": "Im having this issue too. Puma is serving no web requests but using 200MB of memory. It ends up getting killed by the kernel a few times a day\n. ",
    "Fryguy": "\nSo, I've had this gut feeling that ruby's GC is allocation based and how this could cause us to not do a full GC for a long time if we time it \"wrong\".\n\nThe GC is not aware of the underlying size of the Ruby objects, you can create a single Ruby object backed by 1GB of heap allocated memory and Ruby has no idea, so it won't kick off a GC.  It's Ruby's world, you only created one object in the Ruby heap, and there are plenty of slots leftover.\nSo, on that note, I'm wondering if an idling puma is creating a small enough number of Ruby objects to not trigger the GC, but that are backed by much larger heap allocated memory blocks.\n. Strings seem to work differently (I was over-generalizing :smile:) probably because Ruby is aware of the memory being allocated.  I'm more thinking of DataWrapStruct stuff in a C extension wrapping around a large amount of malloced memory...I saw the statement \"they make greater use of C-extensions and manage their memory outside of the Ruby VM\" up above and thats why this popped in my head.\n. ",
    "hakusaro": "I hate to bump a thread this old, but I believe this is still an issue. Heroku is a fairly big platform that many people use -- and their own help guides actually suggest using Puma as the preferred Ruby web server.\nDoes the hack of just running GC.start every 5 seconds on every worker actually work? While it's certainly a hack, it definitely beats memory consumption going out of control. I just spent several hours migrating a side project from to use Puma, only to crash straight into this issue. The memory use is fairly clear (switched from using Thin to Puma) -- and I understand that it's a super hacky solution, but if it works...it works?\n\nCoincidentally moved to Puma after reading this article by you, nateberkopec.. @nateberkopec while I'm certainly aware that it works it's a workaround, I'd say that puma_worker_killer's equally silly solution (or setting up a cronjob to run heroku restart every hour) are bad if not worse than just aggressively telling the gc to run. That's just my two cents, though. The activity on this issue probably disappeared when people realized that was a cheap and dirty fix to a fairly large problem.. @nateberkopec it doesn't fix the problem for me, but in the context of this thread, or at least the way I read your comment, it seemed to mitigate heap_free_slots growth when it would normally increase over time (if I've read that right).. I'd like to just point out that on Heroku-land, a few months ago with no changes to my code this magically stopped being an issue. I don't remember the exact date but after a scheduled dyno restart I've yet to experience uncontrolled growth ever again.. Hi there. Though this question is really old, you need to verify that you're actually referencing the WEB_CONCURRENCY environment variable when starting puma and allocating workers. If you're using rails, posting your puma.rb/puma config file would be great help.\nIf this issue isn't relevant anymore, though, please everyone know so the backlog can be cleaned up.. ",
    "WaKeMaTTa": "The truth is hard to find it, as History. :disappointed: \n. ",
    "kaihendry": "Struggling to understand how puma is supposed to be deployed. Via a .sock to nginx, similar to php-fpm?\n. ",
    "mieko": "Thanks, Evan.  I'll see what I can put together.\n. Thanks for checking it out, @nateberkopec.  I should be able run another pass over this PR and rebase over the next week or so.. ",
    "MikaelSmith": "This is a problem I'm also looking at. I may be able to help finish the C and Java work.\nI was originally looking at https://github.com/square/rails-auth, which gives a more complete authorization solution, but it requires that the certificate is passed through in the request header. Where do you fall on having multiple ways to handle authorization in the future?\nUpdate: figured out that puma stashes the cert in puma.peercert, which makes it pretty easy to use rails-auth to solve this and more general problems.. If anyone's interested in an example, here's an excerpt of my puma config\nrequire 'rails/auth/rack'\nwhitelist = ['certA', 'certB']\n...\nimpl = MySinatraApp.new\nacls = []\nwhitelist.each do |entry|\n  acls << {\n    'resources' => [{ 'method' => 'ALL', 'path' => '/.*' }],\n    'allow_x509_subject' => { 'cn' => entry }\n  }\nend\nacl = Rails::Auth::ACL.new(acls, matchers: { allow_x509_subject: Rails::Auth::X509::Matcher })\nimpl = Rails::Auth::ACL::Middleware.new(impl, acl: acl)\nimpl = Rails::Auth::X509::Middleware.new(impl,\n                                         ca_file: '/path/to/ca.pem',\n                                         cert_filters: { 'puma.peercert' => proc { |x| x } },\n                                         require_cert: true)\napp impl. I'd also like to be able to set both ca and verify_mode (when not using Java as well).. I'd love to see this fixed, although a more forward-ready implementation would be nice: the ability to list what TLS versions to allow would support disabling TLSv1.1 in the future.. ",
    "ad-johnson": "Subsequently found this is a duplicate.\n. Darn it! Closed the wrong one.  This is still an issue in my view.\n. Further investigation, I don't believe this is related to Puma after all.\n. ",
    "steakknife": "This worked:\n1. bin/bundle exec pumactl start -P tmp/pids/server.pid\n2. Wait for app to boot\n3. touch tmp/restart.txt\n4. App reboots just fine\nMaybe needs a minimal broken example in a gist/github and log output (per evanphx)?\nEnvironment Setup\n\nRails 5.0.0.1\nPuma 3.6.0\nRack 2.0.1\nRuby 2.3.1\n. @grosser I already fixed Rack in this PR in Nov 2016, but it's being held up, for reasons that are bikeshedding worthy and prolong delay for no good reason. Why? https://github.com/rack/rack/pull/1127. All good now. Close this issue.. TTIN doesn't seem to do anything\n\nTTOU appears to just kill a worker at random and then another one takes its place. It would make more sense for the expected worker count to also decrease.\n. Why did you close this issue? It's still broken on HEAD.\n. Ah that makes sense. Thanks for weekend FOSS hacking.\n. Seeing this also.\nUSR1, USR2 & tmp/restart.txt exhibiting this behavior.\n- ruby-2.3.1\n- rails (5.0.0.1)\n- puma (3.6.0)\n- rack (2.0.1)\n- bundler (1.13.6)\n. https://github.com/rails/rails/issues/24331\nhttps://github.com/rails/rails/issues/23910\n. ~~So it's not puma, not the tmp_restart plugin... it's fixed in rails dev, but needs to be merged into rails stable branches.~~ Almost\n. It's rails (via rack) trying to manage a pidfile, while puma does also (the same filetmp/pids/server.pid, another fragile choice). Rails is the one that setup the rack pidfile, which with puma, it shouldn't.\n- rack-2.0.1/lib/rack/server.rb:360 -> Rack::Server.write_pid  (which is set by Rails in config.ru)\n- puma-3.6.0/lib/puma/launcher.rb:126 -> Puma::Launcher#write_pid\nIdeally, puma and rack should always use different pid files, say, in order to not play favorites:\n- tmp/pids/puma.pid\n- tmp/pids/rails-server.pid \n. The easiest temporary fix is just run bin/bundle exec puma not bin/rails s.\n. And @prathamesh-sonpatki maybe still working on the non-trivial issue of reusing listeners so it works similarly as when run via puma.\n. There is yet another reproducible bug somewhere, which causes the rails server to deadlock. \n. @nateberkopec There's still an unaddressed deadlock in #1137. All known puma+rails restart issues addressed.. @prathamesh-sonpatki Check for a PR on your topic branch that might help the test.\n. @prathamesh-sonpatki Awesome! Let me know when ready to test, I'd be happy to be crash-test dummy in the rocket chair. \ud83d\ude80\ud83d\udcba :feelsgood: \ud83d\udc4d \n. This could also be done directly without donkey-punching the runtime.\n. (A minimal example as a public docker container might be ideal for repro.). Still an issue:\n=> Booting Puma\n=> Rails 5.1.1 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.4.1-p111), codename: Sassy Salamander\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\n* Restarting...\n=> Booting Puma\n=> Rails 5.1.1 application starting in development on http://localhost:3000\n=> Run `rails server -h` for more startup options. Still an issue on 3.9.0. Closing this since it's never getting fixed.. (travis failure unrelated to this PR)\n. bundler, Rails and springified\n. Ruby 2.3.1\n. Follow #1137 setup (basically rack and puma HEAD) and append to config/puma.rb\ntmp_dir = File.expand_path('../../tmp', __FILE__)\npidfile \"#{tmp_dir}/pids/puma.pid\"\nstate_path \"#{tmp_dir}/puma.state\"\nRun\nbin/bundle exec puma\nThen, in another terminal\nbin/bundle exec pumactl -S tmp/puma.state restart. ### Output\n```\nBuilding native extensions.  This could take a while...\nERROR:  Error installing puma:\n    ERROR: Failed to build gem native extension.\ncurrent directory: {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2/ext/puma_http11\n\n/opt/rubies/ruby-2.4.0-ossl1.1/bin/ruby -r ./siteconf20161226-57195-1pnp5eq.rb extconf.rb\nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\nchecking for openssl/bio.h... yes\ncreating Makefile\nTo see why this extension failed to compile, please check the mkmf.log which can be found here:\n{{~}}/.gem/ruby/2.4.0/extensions/x86_64-darwin-16/2.4.0/puma-3.6.2/mkmf.log\ncurrent directory: {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2/ext/puma_http11\nmake \"DESTDIR=\" clean\ncurrent directory: {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2/ext/puma_http11\nmake \"DESTDIR=\"\ncompiling http11_parser.c\next/puma_http11/http11_parser.rl:111:17: warning: comparison of integers of different signs: 'long' and 'unsigned long' [-Wsign-compare]\n  assert(pe - p == len - off && \"pointers aren't same distance\");\n         ~~~~~~ ^  ~~~~~~~~~\n/usr/include/assert.h:93:25: note: expanded from macro 'assert'\n    (builtin_expect(!(e), 0) ? __assert_rtn(__func, FILE, LINE, #e) : (void)0)\n                        ^\next/puma_http11/http11_parser.c:43:18: warning: unused variable 'puma_parser_en_main' [-Wunused-const-variable]\nstatic const int puma_parser_en_main = 1;\n                 ^\n2 warnings generated.\ncompiling io_buffer.c\ncompiling mini_ssl.c\nmini_ssl.c:90:5: error: incomplete definition of type 'struct dh_st'\n  dh->p = BN_bin2bn(dh1024_p, sizeof(dh1024_p), NULL);\n  ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:91:5: error: incomplete definition of type 'struct dh_st'\n  dh->g = BN_bin2bn(dh1024_g, sizeof(dh1024_g), NULL);\n  ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:93:10: error: incomplete definition of type 'struct dh_st'\n  if ((dh->p == NULL) || (dh->g == NULL)) {\n       ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:93:29: error: incomplete definition of type 'struct dh_st'\n  if ((dh->p == NULL) || (dh->g == NULL)) {\n                          ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:197:27: warning: 'DTLSv1_method' is deprecated [-Wdeprecated-declarations]\n  conn->ctx = SSL_CTX_new(DTLSv1_method());\n                          ^\n/usr/local/opt/openssl@1.1/include/openssl/ssl.h:1614:45: note: 'DTLSv1_method' has been explicitly marked deprecated here\nDEPRECATEDIN_1_1_0(__owur const SSL_METHOD DTLSv1_method(void)) / DTLSv1.0 /\n                                            ^\nmini_ssl.c:233:20: warning: implicit conversion loses integer precision: 'long' to 'int' [-Wshorten-64-to-32]\n  int verify_err = SSL_get_verify_result(ssl);\n      ~~~~~~~~~~   ^~~~~~~~~~~~~~~~~~~~~~~~~~\nmini_ssl.c:246:13: warning: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Wshorten-64-to-32]\n      err = ERR_get_error();\n          ~ ^~~~~~~~~~~~~~~\nmini_ssl.c:333:8: warning: unused variable 'buf' [-Wunused-variable]\n  char buf[512];\n       ^\nmini_ssl.c:332:11: warning: unused variable 'err' [-Wunused-variable]\n  int ok, err;\n          ^\nmini_ssl.c:350:8: warning: unused variable 'buf' [-Wunused-variable]\n  char buf[512];\n       ^\nmini_ssl.c:349:7: warning: unused variable 'ok' [-Wunused-variable]\n  int ok, err;\n      ^\nmini_ssl.c:349:11: warning: unused variable 'err' [-Wunused-variable]\n  int ok, err;\n          ^\n8 warnings and 4 errors generated.\nmake: ** [mini_ssl.o] Error 1\nmake failed, exit code 2\nGem files will remain installed in {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2 for inspection.\nResults logged to {{~}}/.gem/ruby/2.4.0/extensions/x86_64-darwin-16/2.4.0/puma-3.6.2/gem_make.out\n```\ngem_make.out\n```\ncurrent directory: {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2/ext/puma_http11\n/opt/rubies/ruby-2.4.0-ossl1.1/bin/ruby -r ./siteconf20161226-57195-1pnp5eq.rb extconf.rb\nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\nchecking for openssl/bio.h... yes\ncreating Makefile\nTo see why this extension failed to compile, please check the mkmf.log which can be found here:\n{{~}}/.gem/ruby/2.4.0/extensions/x86_64-darwin-16/2.4.0/puma-3.6.2/mkmf.log\ncurrent directory: {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2/ext/puma_http11\nmake \"DESTDIR=\" clean\ncurrent directory: {{~}}/.gem/ruby/2.4.0/gems/puma-3.6.2/ext/puma_http11\nmake \"DESTDIR=\"\ncompiling http11_parser.c\next/puma_http11/http11_parser.rl:111:17: warning: comparison of integers of different signs: 'long' and 'unsigned long' [-Wsign-compare]\n  assert(pe - p == len - off && \"pointers aren't same distance\");\n         ~~~~~~ ^  ~~~~~~~~~\n/usr/include/assert.h:93:25: note: expanded from macro 'assert'\n    (builtin_expect(!(e), 0) ? __assert_rtn(__func, FILE, LINE, #e) : (void)0)\n                        ^\next/puma_http11/http11_parser.c:43:18: warning: unused variable 'puma_parser_en_main' [-Wunused-const-variable]\nstatic const int puma_parser_en_main = 1;\n                 ^\n2 warnings generated.\ncompiling io_buffer.c\ncompiling mini_ssl.c\nmini_ssl.c:90:5: error: incomplete definition of type 'struct dh_st'\n  dh->p = BN_bin2bn(dh1024_p, sizeof(dh1024_p), NULL);\n  ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:91:5: error: incomplete definition of type 'struct dh_st'\n  dh->g = BN_bin2bn(dh1024_g, sizeof(dh1024_g), NULL);\n  ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:93:10: error: incomplete definition of type 'struct dh_st'\n  if ((dh->p == NULL) || (dh->g == NULL)) {\n       ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:93:29: error: incomplete definition of type 'struct dh_st'\n  if ((dh->p == NULL) || (dh->g == NULL)) {\n                          ~~^\n/usr/local/opt/openssl@1.1/include/openssl/ossl_typ.h:104:16: note: forward declaration of 'struct dh_st'\ntypedef struct dh_st DH;\n               ^\nmini_ssl.c:197:27: warning: 'DTLSv1_method' is deprecated [-Wdeprecated-declarations]\n  conn->ctx = SSL_CTX_new(DTLSv1_method());\n                          ^\n/usr/local/opt/openssl@1.1/include/openssl/ssl.h:1614:45: note: 'DTLSv1_method' has been explicitly marked deprecated here\nDEPRECATEDIN_1_1_0(__owur const SSL_METHOD DTLSv1_method(void)) / DTLSv1.0 /\n                                            ^\nmini_ssl.c:233:20: warning: implicit conversion loses integer precision: 'long' to 'int' [-Wshorten-64-to-32]\n  int verify_err = SSL_get_verify_result(ssl);\n      ~~~~~~~~~~   ^~~~~~~~~~~~~~~~~~~~~~~~~~\nmini_ssl.c:246:13: warning: implicit conversion loses integer precision: 'unsigned long' to 'int' [-Wshorten-64-to-32]\n      err = ERR_get_error();\n          ~ ^~~~~~~~~~~~~~~\nmini_ssl.c:333:8: warning: unused variable 'buf' [-Wunused-variable]\n  char buf[512];\n       ^\nmini_ssl.c:332:11: warning: unused variable 'err' [-Wunused-variable]\n  int ok, err;\n          ^\nmini_ssl.c:350:8: warning: unused variable 'buf' [-Wunused-variable]\n  char buf[512];\n       ^\nmini_ssl.c:349:7: warning: unused variable 'ok' [-Wunused-variable]\n  int ok, err;\n      ^\nmini_ssl.c:349:11: warning: unused variable 'err' [-Wunused-variable]\n  int ok, err;\n          ^\n8 warnings and 4 errors generated.\nmake: ** [mini_ssl.o] Error 1\nmake failed, exit code 2\n```\nmkmf.log\n```\nhave_library: checking for BIO_read() in -lcrypto... -------------------- yes\n\"clang -o conftest -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/x86_64-darwin16 -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/ruby/backward -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0 -I. -I/usr/local/opt/openssl@1.1/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/readline/include -I/usr/local/opt/libyaml/include -I/usr/local/opt/gdbm/include -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -D_DARWIN_UNLIMITED_SELECT -D_REENTRANT   -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wno-tautological-compare -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-self-assign -Wunused-variable -Wimplicit-int -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wshorten-64-to-32 -Wimplicit-function-declaration -Wdivision-by-zero -Wdeprecated-declarations -Wextra-tokens  -fno-common -pipe conftest.c  -L. -L/opt/rubies/ruby-2.4.0-ossl1.1/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib -L. -fstack-protector -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib     -lruby.2.4.0  -lpthread -lgmp -ldl -lobjc \"\nchecked program was:\n/ begin /\n1: #include \"ruby.h\"\n2: \n3: int main(int argc, char argv)\n4: {\n5:   return 0;\n6: }\n/ end /\n\"clang -o conftest -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/x86_64-darwin16 -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/ruby/backward -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0 -I. -I/usr/local/opt/openssl@1.1/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/readline/include -I/usr/local/opt/libyaml/include -I/usr/local/opt/gdbm/include -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -D_DARWIN_UNLIMITED_SELECT -D_REENTRANT   -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wno-tautological-compare -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-self-assign -Wunused-variable -Wimplicit-int -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wshorten-64-to-32 -Wimplicit-function-declaration -Wdivision-by-zero -Wdeprecated-declarations -Wextra-tokens  -fno-common -pipe conftest.c  -L. -L/opt/rubies/ruby-2.4.0-ossl1.1/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib -L. -fstack-protector -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib     -lruby.2.4.0 -lcrypto  -lpthread -lgmp -ldl -lobjc \"\nconftest.c:13:57: error: use of undeclared identifier 'BIO_read'\nint t(void) { void ((volatile p)()); p = (void (()()))BIO_read; return !p; }\n                                                        ^\n1 error generated.\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: int t(void) { void ((volatile p)()); p = (void (()()))BIO_read; return !p; }\n/ end /\n\"clang -o conftest -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/x86_64-darwin16 -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/ruby/backward -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0 -I. -I/usr/local/opt/openssl@1.1/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/readline/include -I/usr/local/opt/libyaml/include -I/usr/local/opt/gdbm/include -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -D_DARWIN_UNLIMITED_SELECT -D_REENTRANT   -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wno-tautological-compare -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-self-assign -Wunused-variable -Wimplicit-int -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wshorten-64-to-32 -Wimplicit-function-declaration -Wdivision-by-zero -Wdeprecated-declarations -Wextra-tokens  -fno-common -pipe conftest.c  -L. -L/opt/rubies/ruby-2.4.0-ossl1.1/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib -L. -fstack-protector -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib     -lruby.2.4.0 -lcrypto  -lpthread -lgmp -ldl -lobjc \"\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: extern void BIO_read();\n14: int t(void) { BIO_read(); return 0; }\n/ end /\n\nhave_library: checking for SSL_CTX_new() in -lssl... -------------------- yes\n\"clang -o conftest -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/x86_64-darwin16 -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/ruby/backward -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0 -I. -I/usr/local/opt/openssl@1.1/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/readline/include -I/usr/local/opt/libyaml/include -I/usr/local/opt/gdbm/include -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -D_DARWIN_UNLIMITED_SELECT -D_REENTRANT   -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wno-tautological-compare -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-self-assign -Wunused-variable -Wimplicit-int -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wshorten-64-to-32 -Wimplicit-function-declaration -Wdivision-by-zero -Wdeprecated-declarations -Wextra-tokens  -fno-common -pipe conftest.c  -L. -L/opt/rubies/ruby-2.4.0-ossl1.1/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib -L. -fstack-protector -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib    -lcrypto  -lruby.2.4.0 -lssl -lcrypto  -lpthread -lgmp -ldl -lobjc \"\nconftest.c:13:57: error: use of undeclared identifier 'SSL_CTX_new'\nint t(void) { void ((volatile p)()); p = (void (()()))SSL_CTX_new; return !p; }\n                                                        ^\n1 error generated.\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: int t(void) { void ((volatile p)()); p = (void (()()))SSL_CTX_new; return !p; }\n/ end /\n\"clang -o conftest -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/x86_64-darwin16 -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/ruby/backward -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0 -I. -I/usr/local/opt/openssl@1.1/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/readline/include -I/usr/local/opt/libyaml/include -I/usr/local/opt/gdbm/include -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -D_DARWIN_UNLIMITED_SELECT -D_REENTRANT   -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wno-tautological-compare -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-self-assign -Wunused-variable -Wimplicit-int -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wshorten-64-to-32 -Wimplicit-function-declaration -Wdivision-by-zero -Wdeprecated-declarations -Wextra-tokens  -fno-common -pipe conftest.c  -L. -L/opt/rubies/ruby-2.4.0-ossl1.1/lib -L/usr/local/opt/openssl@1.1/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib -L. -fstack-protector -L/usr/local/lib -L/usr/local/opt/openssl/lib -L/usr/local/opt/readline/lib -L/usr/local/opt/libyaml/lib -L/usr/local/opt/gdbm/lib    -lcrypto  -lruby.2.4.0 -lssl -lcrypto  -lpthread -lgmp -ldl -lobjc \"\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: extern void SSL_CTX_new();\n14: int t(void) { SSL_CTX_new(); return 0; }\n/ end /\n\nhave_header: checking for openssl/bio.h... -------------------- yes\n\"clang -E -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/x86_64-darwin16 -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0/ruby/backward -I/opt/rubies/ruby-2.4.0-ossl1.1/include/ruby-2.4.0 -I. -I/usr/local/opt/openssl@1.1/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/readline/include -I/usr/local/opt/libyaml/include -I/usr/local/opt/gdbm/include -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -D_DARWIN_UNLIMITED_SELECT -D_REENTRANT   -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wno-tautological-compare -Wno-parentheses-equality -Wno-constant-logical-operand -Wno-self-assign -Wunused-variable -Wimplicit-int -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wshorten-64-to-32 -Wimplicit-function-declaration -Wdivision-by-zero -Wdeprecated-declarations -Wextra-tokens  -fno-common -pipe  conftest.c -o conftest.i\"\nchecked program was:\n/ begin /\n1: #include \"ruby.h\"\n2: \n3: #include \n/ end /\n\n```. 3.8.2: tastes great and less filling! \ud83c\udf7b \n$ chruby-exec ruby-2.4.0-ossl1.1 -- gem install puma -N\nBuilding native extensions.  This could take a while...\nSuccessfully installed puma-3.8.2\n1 gem installed. LibreSSL is a different issue, don't conflate the two as others have done. Open a new issue because the OpenSSL one is solved.. Restarting by restart.txt when using rails s is broken. Don't use rails s, use pumactl.\nThere are rack and puma PRs that have been open over a year, but it still isnt fixed in any PR because there are more bugs.\nGiven the continued poor quality, abandonment and atrophy of vital Ruby gems, we're transitioning away from Ruby to other platforms which are much more elegant, debuggable and resilient.. ",
    "jdblack": "I saw this same thing as well.   I have attached a screenshot, as I can't seem to get markdown out of the way.    Its probably important to note that I'm using  a chef deploy resource that uses a symlink farm to handle multiple releases, but this may not be pertinent as rails was sufficiently satisfied to start up.   The symlink farm has the following appearance (which I manually pruned to remove extraneous directories and files) looks like this:\n\n\n. ",
    "grosser": "The issue seems to be some kind of miscommunication between rack and puma.\nBoth think they are responsible for the pid file ... and when the server restarts, rack finds that the pidfile is still there and the process is still running ... patch below ... I think the core issue is that puma should not mess with the pidfile at all and let rack handle it completely ... thoughts ?\n```Ruby\nMake rack not complain when puma restarts the server with the same pid\ncan be removed if rails s + kill -USR1  works\nhttps://github.com/rack/rack/blob/master/lib/rack/server.rb#L379-L393\nsee https://github.com/puma/puma/issues/1060 + https://github.com/rack/rack/issues/1159\n::Rack::Server.prepend(Module.new do\n  def pidfile_process_status\n    if ::File.exist?(options[:pid]) && ::File.read(options[:pid]).to_i == Process.pid\n      Rails.logger.info \" * PUMA in-place restart detected\"\n      :dead\n    else\n      super\n    end\n  end\nend)\n```. PR https://github.com/puma/puma/pull/1260. yes, update the title. @evanphx does this look sane to you ?. @schneems maybe ?. meh ... only 1 way to find out \ud83d\ude08 . the default behavior should be a sane one ... everything else can be an option .... \"hooks have people used to load or set env vars\"\nI think this does not matter since a restart means we do a new exec so\neverything will be loaded anew.\nOn Fri, Apr 7, 2017 at 11:26 AM, Richard Schneeman <notifications@github.com\n\nwrote:\nInteresting. I don't use the restart feature due to Heroku. So I have\nlittle operational opinions on it. This seems like reasonable behavior on\nthe surface. You're telling the server to go away and a new one to come up.\nMy only concerns oils be about why hooks have people used to load or set\nenv vars. If it's something that wouldn't get called after a restart then\nthis could break.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1260#issuecomment-292614983, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ_5snOw5JoUjnkJUxz0lrCUNyJj8ks5rtn_HgaJpZM4MwkYF\n.\n. I'll add a warn on the diff, should not be much ...\n\nOn Fri, Apr 7, 2017 at 11:41 AM, Richard Schneeman <notifications@github.com\n\nwrote:\nmaybe we could do a diff of the environment and output it as a\nwarning/debug-info?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1260#issuecomment-292619285, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZxrl3ZoOLEz4ZMZL_hCNvRZW9RSHks5rtoN0gaJpZM4MwkYF\n.\n. added ... is that what you had in mind ?\n\n```\n\n\nenv['XXX'] = '222'\n=> \"222\"\nrestore_environment env\nResoring process environment, updating:\nXXX from nil to \"222\"\n```. on the other hand I'd rather avoid this overhead/noise since no app does that when it restarts ... pumas restart is already very clean, it only has this bug because it uses exec ... if it was a spawner then this would not happen ... and I think it's reasonable to not expect it to happen ... . @schneems so you want it with warnings or without ?. sounds good, will work on tests now :)\n\n\nOn Mon, Apr 10, 2017 at 10:14 AM, Richard Schneeman \nnotifications@github.com wrote:\n\nIf you think without is the way to go, maybe leave a comment with that\ncode in this issue so we could add it back if lots of people are confused\nby the behavior.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1260#issuecomment-293016524, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ8UMKmmF2QHxXpVR0LFLsGhpqYSEks5rumNkgaJpZM4MwkYF\n.\n. Warning patch:\n```\n@@ -180,7 +180,7 @@ module Puma\n         graceful_stop\n       when :restart\n         log \"* Restarting...\"\n-        ENV.replace(env)\n+        restore_environment(env)\n         @runner.before_restart\n         restart!\n       when :exit\n@@ -204,6 +204,17 @@ module Puma\n\n private\n\n\ndef restore_environment(env)\na = ENV.to_a\nb = env.to_a\ndiff = (a - b) + (b - a)\nreturn if diff.empty?\n+\ndiff.map! { |k, _| \"#{k} from #{ENV[k].inspect} to #{env[k].inspect}\" }\nwarn \"Resoring process environment, updating:\\n#{diff.join(\"\\n\")}\"\nENV.replace(env)\nend\n+\n     def reload_worker_directory\n       @runner.reload_worker_directory if @runner.respond_to?(:reload_worker_directory)\n     end\n```. great ... master is broken :/. tests added ... extracted a few copy-paste parts into methods ... looks good ?. not fixing the other errors on master ...\nbut integration test runs green locally. @schneems mergeable ?. @schneems . easier to see if it succeeded or failed ?\n... I'd even prefer plain to pride since it's less distracting\n\nOn Mon, Apr 10, 2017 at 11:18 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn test/helper.rb\nhttps://github.com/puma/puma/pull/1266#discussion_r110726359:\n\nrequire \"net/http\"\n require \"timeout\"\n require \"minitest/autorun\"\n-require \"minitest/pride\"\n\nNo reason to remove IMO.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1266#pullrequestreview-31919593, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ0nvSq82-gmTAbzkJPSOml0jwL0iks5runJzgaJpZM4M5HDF\n.\n. it's not a test and rake test requires it ... which means it gets loaded\ntwice ... which is weird\n\nOn Mon, Apr 10, 2017 at 11:17 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn test/test_app_status.rb\nhttps://github.com/puma/puma/pull/1266#discussion_r110726173:\n\n@@ -1,4 +1,4 @@\n-require \"test_helper\"\n\nWhat's wrong with test_helper? That's a pretty common convention.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1266#pullrequestreview-31919384, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ1jxfzBo9RLzTjWXkJYtvK2jY3pUks5runI-gaJpZM4M5HDF\n.\n. let's kick it out and see what breaks then\n\nOn Mon, Apr 10, 2017 at 11:16 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn test/helper.rb\nhttps://github.com/puma/puma/pull/1266#discussion_r110726002:\n\n@@ -1,40 +1,32 @@\n # Copyright (c) 2011 Evan Phoenix\n # Copyright (c) 2005 Zed A. Shaw\n\n-begin\nI think this was added for a reason, I can't recall the issue.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1266#pullrequestreview-31919210, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ6Q_brs8BYBkxsyJhR5pd7-xTPFyks5runIUgaJpZM4M5HDF\n.\n. kk, will add that as comment\n\nOn Mon, Apr 10, 2017 at 11:27 AM, Nate Berkopec notifications@github.com\nwrote:\n\nRe: bundler rescue load-error, I mean I think that was added as a request\nby debian (?) or some other package manager team that wants to run the\ntests w/o bundler.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1266#issuecomment-293037033, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ27A9XSu2SbIbnI2p9yI7wjyz771ks5runSAgaJpZM4M5HDF\n.\n. @nateberkopec merge ?. so given these 2 options, which you would chose the fabulously failed test ?\n\n\n\n. extracted bundler changes to it's own PR. so before I rebase this ...\n - you still like pride more than rg ?\n - is moving to helper ok ? (atm test_helper is treated as a test which is weird and could lead to load order issues). new clean PR instead of rebasing this mess :D \nhttps://github.com/puma/puma/pull/1283. @nateberkopec good to go ?. @nateberkopec this looking good ?. The reason I changed that is to allow ruby test/test_foo.rb to work\nwithout seeing and having to debug \"cannot load foo test_helper\" error messages. ... that might not be an issue for senior devs, but it's not obvious that you have to add the magic -Itest incantation. release ?. I think this is good, but we are getting out of this restart handler mess ... see https://github.com/zendesk/samson/pull/1964\nthis patch worked but then it revealed that we could no longer restart since puma executable was also removed from path ... and I don't want to deal with this .... nvm ... we need to not lose requests ... so I guess this is still on :D. yep, I though we could get away with just hard restarts, but turns out we can't ... so still need this patch :). merge ?. now we only need green tests and a release :trollface: . ... this might make prune_bundler obsolete. seeing this error on master too:\n1) Failure:\nTestUserSuppliedOptionsIsEmpty#test_config_file_wins_over_port [/Users/mgrosser/Code/tools/puma/test/test_rack_handler.rb:121]:\nExpected: [\"tcp://0.0.0.0:6001\"]\n  Actual: [\"tcp://0.0.0.0:5001\"]. reverting the test changes fixes the issue ... so something went wrong there :/. found it ... https://github.com/puma/puma/pull/1304. FYI trying to fix this whole bundler mess by unsetting bundler ENV vars on restart ... would that solve your issue too ?\n```Ruby\nconfig/puma.rb\nPuma::Runner.prepend(Module.new do\n  def before_restart\n    ENV.replace(Bundler.clean_env)\n    super\n  end\nend)\n``. yes, using that in prod since ~1 year\nonly issue is that restarts might not know how to restart so you have to use--restart-cmd 'bundle exec puma'`. once https://github.com/bundler/bundler/issues/5700 is resolved puma will work like this ... but there will be solve fallout with puma forgetting how to restart ... so ideally we'd fix that up properly until then :). yeah either way is fine, this whole realpath mess goes away when deploying using docker anyway, so meh :). maybe wait for travis before merging :D \n2) Failure:\nTestIntegration#test_restart_restores_environment [/home/travis/build/puma/puma/test/test_integration.rb:254]:\nExpected # encoding: ASCII-8BIT\n\"Hello RAND 0.8985455927826598\" to not be equal to # encoding: ASCII-8BIT\n\"Hello RAND 0.8985455927826598\".. I'll look into that one next .... cannot fail before me since that's a test I added ;)\nOn Mon, May 29, 2017 at 6:04 PM, Nate Berkopec notifications@github.com\nwrote:\n\nI did! that was failing before you, but if you want to fix that too I\nwon't complain :D\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1304#issuecomment-304750875, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ5E9TeDDV42_9ER8xFbem1U-1uEWks5r-2sTgaJpZM4Npt8w\n.\n. here: https://github.com/puma/puma/pull/1305/files. hmm no I think this is even worse ... setting to \"\" will prevent things like Dotenv to load new env vars ... I'll have another look maybe just skip for now since it keeps the old behavior .... updated with a safer fixup ... better semi-broken then completely broken ...\notherwise we'd set \"\" everywhere and things sure would break ...\nalso once jruby is fixed it will just work .... basically it will behave as before without this fix ... so not a degradation/blocker ... and once jruby is fixed it will work just fine. tried reproducing it but without success ... so there must be something weird going on :/\n\nruby -e 'ENV[\"FOO\"] = \"BAR\"; ENV.replace({}); exec \"ruby -e \\\"puts ENV[%{FOO}]\\\"\"'. hmm I think this is because it restarts with an environment that does not have bundler\nso should tweak the restart command to include bundle exec ... I though I tried that ... for now might be able to fix it by setting --restart-command manually will take a look shortly. it works when running via rails s which hid the error ...\n@restart_argv is \"/rbenv/versions/2.3.3/bin/ruby\", \"ruby/2.3.0/bin/puma\", \"-p\", \"3000\", {:close_others=>true, 14=>14}] ... so no mention of bundler ... so it won't load bundler ... so it cannot find puma ... \nthe whole @restart_command is basically wrong and bundler used to override it by accident because it set a bunch of stuff in the ENV that was then inherited ... good times .... fixing here https://github.com/puma/puma/pull/1309. bundle exec puma did not work for me. the trick is to gem uninstall puma -a before doing it and having puma only installed via bundle install --path vendor/bundle. add gemspec to Gemfile and remove the dependencies like https://github.com/puma/puma/pull/1313 does then\ngem uninstall puma -a\nbundle exec puma test/rackup/hello.ru\nps -ef | grep puma\nkill -SIGUSR2 <puma-pid>\n-> kaboom. should be fixed in 3.9.1 too ... but it removes bundler reset functionality ... so the restart will have the old Gemfile loaded as 3.8 did ... will work on that in the meantime for next minor hopefully. @dguettler confirmed fixed ?. what bundler version are you on ?. nvm ... was able to reproduce ... weird ... will take another look. this would only happen if bundler has finally fixed the issue that bundle exec ruby -e 'puts Bundler::ORIGINAL_ENV' still included bundler variables ... that still prints all the vars locally but if I run bundle exec puma then Bundler::ORIGINAL_ENV seems to be clean .... https://github.com/puma/puma/pull/1317 should do the trick then ...\ntemp-fix would be to add --restart-cmd \"bundle exec puma\". FYI you'd be restarting with the same Gemfile/Load-paths so changing a gem (especially puma) would not work ... use https://github.com/zendesk/samson/blob/master/config/puma.rb#L15-L33 until https://github.com/bundler/bundler/pull/5701 is released.. Bundle.original_env needs a patch that is in bundler master to work as\nintended ... for the time being the patch I linked should work fine.\nOn Mon, Jun 26, 2017 at 10:10 PM, bughit notifications@github.com wrote:\n\n@grosser https://github.com/grosser\nFYI you'd be restarting with the same Gemfile/Load-paths\nThat does still seem to be the case, but why? BUNDLE_GEMFILE is no longer\npresent in the new puma process but the $LOAD_PATH is still wrong/old\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1308#issuecomment-311168032, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZyVBAJalKb6tbsFWjMEaHhbgxboAks5sIBApgaJpZM4NtbSQ\n.\n. idk why anyone would want to start a random version of puma ... so I'm doing bundle exec puma ... with the ENV reset it can restart with a new Gemfile and even a new puma version.. Good stuff :)\n\nUsing https://github.com/zendesk/samson/blob/master/config/puma.rb#L28-L33\nand everything works ... new gem/ENV on every reload ...\nOn Sun, Jul 2, 2017 at 7:54 PM, stereobooster notifications@github.com\nwrote:\n\nExperiments with restart https://github.com/stereobooster/ruby-server-\nexperiment/tree/master/puma-symlink\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1308#issuecomment-312506662, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZxfstvzwsYOsJzYVZTMzGPgQHEbwks5sJ9lSgaJpZM4NtbSQ\n.\n. ```\n\nGemfile\nsource 'https://rubygems.org'\ngem 'puma', git: 'https://github.com/grosser/puma.git', branch: 'grosser/fix'\nconfig.ru\nrun lambda { |env| [200, {'Content-Type'=>'text/plain'}, StringIO.new(\"Hello World!\\n\")] }\n```\nthis setup works fine for me ... \nhttps://github.com/puma/puma/pull/1385\nbundle exec puma\nkill -SIGUSR2 <PID>\n. Yeah, found this bug while trying to reproduce the restarting issue ... mind trying out https://github.com/grosser/puma-restart (using puma from master + my patch) and tell me if that works for you too ?. can you try with master to see if that fixes it ?\n@nateberkopec want to make a release to see if some of these get solved ?. obviously missing Teapot ;)\nI think it was not passing in -rbundler/setup so the new process tried to\nload puma without bundler and blew up\nOn Sun, Aug 13, 2017 at 4:36 AM, kyontan notifications@github.com wrote:\n\nIt seems fixed at a771ec3\nhttps://github.com/puma/puma/commit/a771ec3655f7ee7a0e0b620c1cadd0c1a1650aff\n!\nWhat's the reason of this issue?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1308#issuecomment-322036809, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ6TsSz9wdc5gMEr5eI2eHSxOwsFyks5sXt-2gaJpZM4NtbSQ\n.\n. @nateberkopec a bit of cleanup too ... hope you like it :D. nvm ... got to rework that ... we cannot run tests without bundler or they will randomly fail :/. getting better still need to iron out a few more kinks in the testing ... so many bugs in there that just worked by accident :D. in summary:\n - make tests simpler ... they use bundle exec anyway since we run with bundler\n - make bundle exec puma work from local repo and update out-of-sync gemspec\n - ensure no other puma versions are around so we actually fail as described in the issue\n - reinject bundler since we remove it from the ENV by using with_clean_env. extracted the basic logic into https://github.com/puma/puma/pull/1311 let's merge that first for a clean PR ... this here is getting messy. rebased with my 2 other PRs (gemspec and test cleanup) ... so should merge them first ... I'll try to get em both green ... hopefully just randomness. only change here is now the RUBYOPT re-injecting and using bundle exec puma to test ... but the injection is no longer needed since https://github.com/puma/puma/pull/1314 added all bundler variables back into the ENV ... so RUBYOPT is still present\n\ncould just keep it this way and release a patch version now ... but would mean Gemfile and bundler version will be stale during restarts (as in v38)\nalternatively re-implement .clean_env and whitelist RUBYOPT to make Gemfile/bundler changes possible. asking here to keep .clean_env around since idk how else to implement this feature without copy-pasting code from bundler https://github.com/bundler/bundler/pull/4702. that will surely take a while to resolve ... so maybe release the patch version now and we can improve the env reset in the next minor .... Did you find a bug that is fixed by using master ?\n. glad it's not me :D. updated ... realized I cannot remove bundler since then the gem would not be found (we already run the shellout with bundler since RUBYOPT is set) ... and switching to bundle exec does not work without the patch from https://github.com/puma/puma/pull/1309 ... so for now just some cleanup to make the other PRs cleaner. error seems to be:\n```\n  2) Error:\nTestIntegration#test_restart_closes_keepalive_sockets:\n: execution expired\norg/jruby/RubyIO.java:2233:in `gets'\n/home/travis/build/puma/puma/test/test_integration.rb:93:in `wait_for_server_to_boot'\n/home/travis/build/puma/puma/test/test_integration.rb:82:in `restart_server'\n/home/travis/build/puma/puma/test/test_integration.rb:56:in `restart_server_and_listen'\n/home/travis/build/puma/puma/test/test_integration.rb:208:in `test_restart_closes_keepalive_sockets'\n\n``\nalready restarted it twice ... reproduced it and turns out the-Ilib` re-injection did not work on jruby ... so another bug fixed :D. added more time to the timeout since it kept failing and on master the test took 59s ... on this branch it took 64 ... so I hope just randomness but now jruby has a bit more time ...\ntime ruby test/test_integration.rb -n 'TestIntegration#test_restart_closes_keepalive_sockets'. somehow TestThreadPool#test_trim_is_ignored_if_no_waiting_threads failed on ruby 2.1 now ... but should be unrealted .... 2.1 and jruby are green \ud83c\udf89 . https://github.com/puma/puma/pull/1314. not required.\nremoving all the things that are unnecessary\nOn Thu, Jun 1, 2017 at 5:09 PM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec requested changes on this pull request.\nIn puma.gemspec\nhttps://github.com/puma/puma/pull/1313#discussion_r119757583:\n\ns.email = [\"evan@phx.io\"]\n   s.executables = [\"puma\", \"pumactl\"]\n   s.extensions = [\"ext/puma_http11/extconf.rb\"]\n   s.files = git ls-files.split($/)\n   s.homepage = \"http://puma.io\"\n   s.license = \"BSD-3-Clause\"\n-  s.rdoc_options = [\"--main\", \"README.md\"]\n-  s.require_paths = [\"lib\"]\n\nisn't this a required attribute?\nIn puma.gemspec\nhttps://github.com/puma/puma/pull/1313#discussion_r119757593:\n\ns.email = [\"evan@phx.io\"]\n   s.executables = [\"puma\", \"pumactl\"]\n   s.extensions = [\"ext/puma_http11/extconf.rb\"]\n   s.files = git ls-files.split($/)\n   s.homepage = \"http://puma.io\"\n   s.license = \"BSD-3-Clause\"\n-  s.rdoc_options = [\"--main\", \"README.md\"]\n\nwhy remove?\nIn puma.gemspec\nhttps://github.com/puma/puma/pull/1313#discussion_r119757604:\n\ns.email = [\"evan@phx.io\"]\n   s.executables = [\"puma\", \"pumactl\"]\n   s.extensions = [\"ext/puma_http11/extconf.rb\"]\n   s.files = git ls-files.split($/)\n   s.homepage = \"http://puma.io\"\n   s.license = \"BSD-3-Clause\"\n-  s.rdoc_options = [\"--main\", \"README.md\"]\n-  s.require_paths = [\"lib\"]\n-  s.required_ruby_version = Gem::Requirement.new(\">= 1.9.3\")\n\nwhy remove?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1313#pullrequestreview-41667693, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ0DNseI0l9YQ-2bH4z87VzkARCAiks5r_1K0gaJpZM4Ntqrs\n.\n. please let me know which of the removed options is needed to run tests/use as git dependency ... or why else we'd need them would like to make this hacky gemspec as simple as possible. this is not the real gemspec, just a stub ... so nobody will use it for gem\ninstall\n\nOn Fri, Jun 2, 2017 at 5:40 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn puma.gemspec\nhttps://github.com/puma/puma/pull/1313#discussion_r119845984:\n\ns.email = [\"evan@phx.io\"]\n   s.executables = [\"puma\", \"pumactl\"]\n   s.extensions = [\"ext/puma_http11/extconf.rb\"]\n   s.files = git ls-files.split($/)\n   s.homepage = \"http://puma.io\"\n   s.license = \"BSD-3-Clause\"\n-  s.rdoc_options = [\"--main\", \"README.md\"]\n\nfrom what I can tell removing this would change the default output for gem\ninstall --document puma.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1313#discussion_r119845984, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZy1IcueHmY_siDQmROM9moS2d7Asks5sAALJgaJpZM4Ntqrs\n.\n. restored ruby requirement\n\nOn Fri, Jun 2, 2017 at 5:41 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn puma.gemspec\nhttps://github.com/puma/puma/pull/1313#discussion_r119846108:\n\ns.email = [\"evan@phx.io\"]\n   s.executables = [\"puma\", \"pumactl\"]\n   s.extensions = [\"ext/puma_http11/extconf.rb\"]\n   s.files = git ls-files.split($/)\n   s.homepage = \"http://puma.io\"\n   s.license = \"BSD-3-Clause\"\n-  s.rdoc_options = [\"--main\", \"README.md\"]\n-  s.require_paths = [\"lib\"]\n-  s.required_ruby_version = Gem::Requirement.new(\">= 1.9.3\")\n\nremoving this would remove the require ruby version check from bundler\nAFAICT, I don't want to do that.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1313#discussion_r119846108, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ46LnuHnfgzZCMwFVjywHxXQK9S6ks5sAALxgaJpZM4Ntqrs\n.\n. @nateberkopec good to go ?. @nateberkopec @matobinder @wangyuan99. we don't modify it, so no\n\nOn Fri, Jun 2, 2017 at 5:54 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn lib/puma/launcher.rb\nhttps://github.com/puma/puma/pull/1314#discussion_r119848497:\n\n@@ -163,7 +163,7 @@ def phased_restart\n\n # Run the server. This blocks until the server is stopped\n def run\n\n\nprevious_env = (defined?(Bundler) ? Bundler.clean_env : ENV.to_h)\nprevious_env = (defined?(Bundler) ? Bundler::ORIGINAL_ENV : ENV.to_h)\n\nAh I see, nvm. Should we be cloneing this like Bundler does though?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1314#discussion_r119848497, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ4GrfPKcF7O5Crz2PS2tbS_At1Uaks5sAAXygaJpZM4Nt2d8\n.\n. just being afraid that's not defined again on some version\n\nOn Fri, Jun 2, 2017 at 5:53 AM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn lib/puma/launcher.rb\nhttps://github.com/puma/puma/pull/1314#discussion_r119848253:\n\n@@ -163,7 +163,7 @@ def phased_restart\n\n # Run the server. This blocks until the server is stopped\n def run\n\n\nprevious_env = (defined?(Bundler) ? Bundler.clean_env : ENV.to_h)\nprevious_env = (defined?(Bundler) ? Bundler::ORIGINAL_ENV : ENV.to_h)\n\nWhy not .original_env?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1314#pullrequestreview-41766134, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZwuJGaG6LFJTsCCALDYJJBCI_0Hkks5sAAWygaJpZM4Nt2d8\n.\n. yeah, I am planing to turn this PR into a cleanup bundler ... just wanted to collect some proof first that we actually need to do that ... so playing with multiple gemfiles and seeing if not cleaning up all of bundler is good enough or if then load-paths etc stay around ... also still waiting for the bundler issue to either get resolved or clean_env to be restored. test takes 43s locally ... that seems a bit crazy ... will take a look why .... runs fast locally now .... still times out on travis :(. reproduction steps with bundler 1.15.0+\ngem uninstall puma -a\nbundle exec puma test/rackup/hello.ru\nps -ef | grep puma\nkill -SIGUSR2 <puma-pid>. idk what's going on in travis, but it's cancelling all builds .... I guess it cancels all builds when 1 fails ... which won't work here since we have undocumented  know failures .... found the ruby 2.1 bug ... the Gemfile had no gemspec directive :D \n... having a look at jruby next. green baby :D. only issue left is now the handling of non-standard Gemfile paths ... I\"ll address that in another PR. Don't know how ... Maybe could duplicate the required but that would not\nmatter ... maybe if you load puma from a gem and then bundlerBut after ...\nBut would not be defined at this point then\n\nOn Jun 7, 2017 10:32 AM, \"Nate Berkopec\" notifications@github.com wrote:\n\n@nateberkopec commented on this pull request.\nIn lib/puma/launcher.rb\nhttps://github.com/puma/puma/pull/1317#discussion_r120693205:\n\n@@ -163,7 +163,15 @@ def phased_restart\n\n # Run the server. This blocks until the server is stopped\n def run\n\n\nprevious_env = (defined?(Bundler) ? Bundler::ORIGINAL_ENV : ENV.to_h)\nprevious_env =\nif defined?(Bundler)\nenv = Bundler::ORIGINAL_ENV\n\nadd -rbundler/setup so we load from Gemfile when restarting\n\nenv[\"RUBYOPT\"] = (env[\"RUBYOPT\"].to_s.split(\" \") << \"-rbundler/setup\").uniq.compact.join(\" \")\n\nHmmm... are there scenarios where people's Rubyopt will get mangled by\nthis?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1317#pullrequestreview-42677751, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ9kZJziZd4O_7nX4Ah-tMfSDkwVLks5sBt6ZgaJpZM4NvG7z\n.\n. @nateberkopec good to go ?. isn't just appending more dangerous ... could lead to duplicates\nthis way it's added unless it's there ... could rewrite to be more ifs and\nless split/push/join style ... yeah the uniq could do something weird if\nthere are multiple -r in there ...\n\nOn Thu, Jun 8, 2017 at 6:48 PM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn lib/puma/launcher.rb\nhttps://github.com/puma/puma/pull/1317#discussion_r121034838:\n\n@@ -163,7 +163,15 @@ def phased_restart\n\n # Run the server. This blocks until the server is stopped\n def run\n\n\nprevious_env = (defined?(Bundler) ? Bundler::ORIGINAL_ENV : ENV.to_h)\nprevious_env =\nif defined?(Bundler)\nenv = Bundler::ORIGINAL_ENV\n\nadd -rbundler/setup so we load from Gemfile when restarting\n\nenv[\"RUBYOPT\"] = (env[\"RUBYOPT\"].to_s.split(\" \") << \"-rbundler/setup\").uniq.compact.join(\" \")\n\nThis still makes me nervous. Why aren't we just appending this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1317#discussion_r121034838, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZxrbqFtDj7wFyUTo9Zjm9y3yBMnyks5sCKSBgaJpZM4NvG7z\n.\n. updated to a safer approach\n\nOn Thu, Jun 8, 2017 at 7:48 PM, Michael Grosser michael@grosser.it wrote:\n\nisn't just appending more dangerous ... could lead to duplicates\nthis way it's added unless it's there ... could rewrite to be more ifs and\nless split/push/join style ... yeah the uniq could do something weird if\nthere are multiple -r in there ...\nOn Thu, Jun 8, 2017 at 6:48 PM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn lib/puma/launcher.rb\nhttps://github.com/puma/puma/pull/1317#discussion_r121034838:\n\n@@ -163,7 +163,15 @@ def phased_restart\n\n # Run the server. This blocks until the server is stopped\n def run\n\n\nprevious_env = (defined?(Bundler) ? Bundler::ORIGINAL_ENV : ENV.to_h)\nprevious_env =\nif defined?(Bundler)\nenv = Bundler::ORIGINAL_ENV\n\nadd -rbundler/setup so we load from Gemfile when restarting\n\nenv[\"RUBYOPT\"] = (env[\"RUBYOPT\"].to_s.split(\" \") << \"-rbundler/setup\").uniq.compact.join(\" \")\n\nThis still makes me nervous. Why aren't we just appending this?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1317#discussion_r121034838, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZxrbqFtDj7wFyUTo9Zjm9y3yBMnyks5sCKSBgaJpZM4NvG7z\n.\n\n\n. not really a bug ... but tests are ok ... want to try merging it ?. I was expecting it to fail since it's non-blocking ... will take a look at\nit\n\nOn Sat, Jun 3, 2017 at 4:16 PM, Nate Berkopec notifications@github.com\nwrote:\n\nNew failures on JRuby and 2.1. I can repro the 2.1 failure locally\nsometimes:\n[image: screen shot 2017-06-03 at 5 16 34 pm]\nhttps://cloud.githubusercontent.com/assets/845662/26757650/659c985e-4880-11e7-9218-b7ce3c8a82df.png\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1322#issuecomment-306007152, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZy2ijxLK2iCqMEG_8E4tZ_NaV0Ktks5sAelXgaJpZM4NvOYJ\n.\n. fixed up ... now reading until the body appears ... jruby expired a bunch so I bumped the timeout back to 120 for it ... test says it took 10s now ... so idk why it failed before ... but should be fine either way.. I think the issue was PERSISTENT_TIMEOUT = 20 basically the read waited until the session was ended so every connection took 20s. played more with it and also passes locally with 15 ... so that 20 just happens to  be the same value as the timeout, but is not correlated ... so the travis failures must also be random ... leaving the reafctoring since it's still more readable to me .... @nateberkopec . kinda ... was not sure what that did in addition to this \"just fix it\"\nchange ... so I made a new PR\n\nOn Sat, Aug 5, 2017 at 11:33 AM, stereobooster notifications@github.com\nwrote:\n\nI suppose this PR shadows this one https://github.com/puma/puma/\npull/1348/files.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1385#issuecomment-320462500, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ5yDNfH6zPaled5f2i8JrPfWcyu5ks5sVLV5gaJpZM4OuhO6\n.\n. added .dup for good measure ... might have produced weird errors if someone shells our from a puma process. it will kill the process (default behavior of SIGHUP) ... not sure if that's bad ... would certainly stop the server :D\n\nruby -e sleep\nkill -HUP 39052\nHangup: 1. that PR is about SIGINT not SIGHUP\nstop for SIGHUP was added in\nhttps://github.com/puma/puma/commit/eb6468d5585c01287b72a92e797773bcb123c1af\nto fix https://github.com/puma/puma/issues/911 which is about SIGTERM ...\nidk kinda confusing :D\nOn Tue, Mar 27, 2018 at 4:28 PM, Nate Berkopec notifications@github.com\nwrote:\n\n@nateberkopec commented on this pull request.\nIn lib/puma/launcher.rb\nhttps://github.com/puma/puma/pull/1527#discussion_r177602000:\n\n         @runner.redirect_io\n\n\nelse\n\n\nI think it was important to #1377 https://github.com/puma/puma/pull/1377\n?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/pull/1527#discussion_r177602000, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAAsZ5uFTg2cTdnNaQRUs3g9VwRbnUfFks5tissMgaJpZM4SjsLw\n.\n. #911 has reproduction steps, so if they still work then it's ok ?\nit's not really an urgent/important change, just 1-more-weirdness in the codebase \ud83e\udd37\u200d\u2642\ufe0f . yep :D. ... should both use gemspec and gemspec should not be generated by hoe .... fixed. these do not match what is in the Gemfile and used locally or travis ... useless anyway so just removing them .... if this fails then something is wrong ... better fail and fix instead of having weird versions. don't need any of that to work locally or via git dependency in bundler. idk ... making sure it restarts or so ... . the whole point of the soft restart is that the connection is not interrupted, so I don't think this made much sense .... yeah I think this is basically a blocking read with a 2s timeout that then raises a Errno::ECONNRESET if it's not done. I was basically thinking \"if you send SIGHUP to a server that does not have any reload mechanic then wtf are you doing\" ... but maybe it's valid usecase :(. \n",
    "ThomasDebek": "You have try  to reset your terminal. ",
    "michaelsauter": "I see the tests have failed, but it looks like some kind of timeout. Probably unrelated to the change here?\nAnything else I can do to get this merged?\n. Thanks for paging! We never had the issue described here, but I'd agree with @hoshinotsuyoshi that this needs fixing.. ",
    "SurinderDhillon": "Rails 5.0.0.1 server not working without IPv6. You need to enable IPv6 from /etc/sysctl.conf file(IPv6 is enabled by default). Remove or comment these lines if you have any.\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\nReboot the server to enable IPv6.\ntry\nrails server\n. ",
    "jeanlescure": "This is not only a Rails 5 IPV6 issue, having same exact problem with Rails 4+, issue has to do with having both 127.0.0.0 and ::1 pointing to localhost on /etc/hosts.\nI downgraded to Puma 3.4.0 and issue goes away, but thought I should drive by and let you know.\nCheers\n. ",
    "mecampbellsoup": "@evanphx could we get a link to the relevant pull request or commit to see what you changed? Trying to learn something today!\n. ",
    "janko": "The previous server implementation for the \"tus\" protocol used Goliath, which allows this by leveraging EventMachine. This is the only blocker in making my Rack implementation fully equivalent to the Goliath/EventMachine one.\n. The short term solution sounds perfect. Thanks, @evanphx!\n. That's weird, because Puma does unlink created tempfiles, even in case of an error.\nHowever, it seems there are two places where Puma creates tempfiles, in Puma::Server#read_body and in Puma::Client#setup_body, and only the one created by Puma::Client seems to get unlinked. I don't know when does Puma::Server#read_body get called.. ",
    "junaruga": "Thanks!\n. @evanphx okay. I got it. thanks.\n. Thanks!\n. Next week, let me explain more. Now I am in vacation  :)\n. @evanphx sorry for no update so far, because last week I was busy. But this week not busy. I'm gonna report this week definitely.\n. Hi @evanphx \nI found the way to reproduce above error easily.\n```\nmkmf.log is created from below command.\n$ ruby ext/puma_http11/extconf.rb\n$ grep error mkmf.log \nconftest.c:13:57: error: \u2018BIO_read\u2019 undeclared (first use in this function)\nconftest.c:13:57: error: \u2018SSL_CTX_new\u2019 undeclared (first use in this function)\n```\nCould you check this?\nThanks\n. @evanphx \nBecause, my above reproducing environment is different from first one.\nI am using OpenSSL on above environment.\nBut the mkmf.log message shows \"error: \u2018*\u2019 undeclared\"\n```\n$ ruby ext/puma_http11/extconf.rb \nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\nchecking for openssl/bio.h... yes\ncreating Makefile\n$ cat mkmf.log \nhave_library: checking for BIO_read() in -lcrypto... -------------------- yes\n\"gcc -o conftest -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/x86_64-linux -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/ruby/backward -I/usr/local/ruby-2.3.1/include/ruby-2.3.0 -Iext/puma_http11     -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration -Wdeprecated-declarations -Wno-packed-bitfield-compat -Wno-maybe-uninitialized  -fPIC conftest.c  -L. -L/usr/local/ruby-2.3.1/lib -Wl,-R/usr/local/ruby-2.3.1/lib -L. -fstack-protector -rdynamic -Wl,-export-dynamic     -Wl,-R/usr/local/ruby-2.3.1/lib -L/usr/local/ruby-2.3.1/lib -lruby  -lpthread -ldl -lcrypt -lm   -lc\"\nchecked program was:\n/ begin /\n1: #include \"ruby.h\"\n2: \n3: int main(int argc, char argv)\n4: {\n5:   return 0;\n6: }\n/ end /\n\"gcc -o conftest -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/x86_64-linux -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/ruby/backward -I/usr/local/ruby-2.3.1/include/ruby-2.3.0 -Iext/puma_http11     -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration -Wdeprecated-declarations -Wno-packed-bitfield-compat -Wno-maybe-uninitialized  -fPIC conftest.c  -L. -L/usr/local/ruby-2.3.1/lib -Wl,-R/usr/local/ruby-2.3.1/lib -L. -fstack-protector -rdynamic -Wl,-export-dynamic     -Wl,-R/usr/local/ruby-2.3.1/lib -L/usr/local/ruby-2.3.1/lib -lruby -lcrypto  -lpthread -ldl -lcrypt -lm   -lc\"\nconftest.c: In function \u2018t\u2019:\nconftest.c:13:57: error: \u2018BIO_read\u2019 undeclared (first use in this function)\n int t(void) { void ((volatile p)()); p = (void (()()))BIO_read; return !p; }\n                                                         ^\nconftest.c:13:57: note: each undeclared identifier is reported only once for each function it appears in\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: int t(void) { void ((volatile p)()); p = (void (()()))BIO_read; return !p; }\n/ end /\n\"gcc -o conftest -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/x86_64-linux -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/ruby/backward -I/usr/local/ruby-2.3.1/include/ruby-2.3.0 -Iext/puma_http11     -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration -Wdeprecated-declarations -Wno-packed-bitfield-compat -Wno-maybe-uninitialized  -fPIC conftest.c  -L. -L/usr/local/ruby-2.3.1/lib -Wl,-R/usr/local/ruby-2.3.1/lib -L. -fstack-protector -rdynamic -Wl,-export-dynamic     -Wl,-R/usr/local/ruby-2.3.1/lib -L/usr/local/ruby-2.3.1/lib -lruby -lcrypto  -lpthread -ldl -lcrypt -lm   -lc\"\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: extern void BIO_read();\n14: int t(void) { BIO_read(); return 0; }\n/ end /\n\nhave_library: checking for SSL_CTX_new() in -lssl... -------------------- yes\n\"gcc -o conftest -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/x86_64-linux -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/ruby/backward -I/usr/local/ruby-2.3.1/include/ruby-2.3.0 -Iext/puma_http11     -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration -Wdeprecated-declarations -Wno-packed-bitfield-compat -Wno-maybe-uninitialized  -fPIC conftest.c  -L. -L/usr/local/ruby-2.3.1/lib -Wl,-R/usr/local/ruby-2.3.1/lib -L. -fstack-protector -rdynamic -Wl,-export-dynamic    -lcrypto  -Wl,-R/usr/local/ruby-2.3.1/lib -L/usr/local/ruby-2.3.1/lib -lruby -lssl -lcrypto  -lpthread -ldl -lcrypt -lm   -lc\"\nconftest.c: In function \u2018t\u2019:\nconftest.c:13:57: error: \u2018SSL_CTX_new\u2019 undeclared (first use in this function)\n int t(void) { void ((volatile p)()); p = (void (()()))SSL_CTX_new; return !p; }\n                                                         ^\nconftest.c:13:57: note: each undeclared identifier is reported only once for each function it appears in\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: int t(void) { void ((volatile p)()); p = (void (()()))SSL_CTX_new; return !p; }\n/ end /\n\"gcc -o conftest -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/x86_64-linux -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/ruby/backward -I/usr/local/ruby-2.3.1/include/ruby-2.3.0 -Iext/puma_http11     -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration -Wdeprecated-declarations -Wno-packed-bitfield-compat -Wno-maybe-uninitialized  -fPIC conftest.c  -L. -L/usr/local/ruby-2.3.1/lib -Wl,-R/usr/local/ruby-2.3.1/lib -L. -fstack-protector -rdynamic -Wl,-export-dynamic    -lcrypto  -Wl,-R/usr/local/ruby-2.3.1/lib -L/usr/local/ruby-2.3.1/lib -lruby -lssl -lcrypto  -lpthread -ldl -lcrypt -lm   -lc\"\nchecked program was:\n/ begin /\n 1: #include \"ruby.h\"\n 2: \n 3: /top/\n 4: extern int t(void);\n 5: int main(int argc, char argv)\n 6: {\n 7:   if (argc > 1000000) {\n 8:     printf(\"%p\", &t);\n 9:   }\n10: \n11:   return 0;\n12: }\n13: extern void SSL_CTX_new();\n14: int t(void) { SSL_CTX_new(); return 0; }\n/ end /\n\nhave_header: checking for openssl/bio.h... -------------------- yes\n\"gcc -E -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/x86_64-linux -I/usr/local/ruby-2.3.1/include/ruby-2.3.0/ruby/backward -I/usr/local/ruby-2.3.1/include/ruby-2.3.0 -Iext/puma_http11     -O3 -fno-fast-math -ggdb3 -Wall -Wextra -Wno-unused-parameter -Wno-parentheses -Wno-long-long -Wno-missing-field-initializers -Wunused-variable -Wpointer-arith -Wwrite-strings -Wdeclaration-after-statement -Wimplicit-function-declaration -Wdeprecated-declarations -Wno-packed-bitfield-compat -Wno-maybe-uninitialized  -fPIC  conftest.c -o conftest.i\"\nchecked program was:\n/ begin /\n1: #include \"ruby.h\"\n2: \n3: #include \n/ end /\n\n```\n. @evanphx you haven't seen this message in your environment?\nOpenSSL version is 1.0.2h (openssl-devel-1.0.2h-1.fc23.x86_64)\nI am using Fedora 23.\nAnd it is the latest version of master branch.\n$ git log --pretty=oneline | head -1\n0b3626091e7ca7013312586a54a2f6c150f861b7 Provide write as <<. Fixes #1089\n$ gcc --version\ngcc (GCC) 5.3.1 20160406 (Red Hat 5.3.1-6)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n$ ruby -v\nruby 2.3.1p112 (2016-04-26 revision 54768) [x86_64-linux]\nBy the way $ bundle exec rake compile would succeed in spite of the mkmf.log message.\nAnd also all the test was passed except the issue https://github.com/puma/puma/issues/1044 .\n. > Wait wait, so does SSL support work or not?\nSorry that I confuse you.\nSSL support works. Only the mkmf.log's message is my concern.\nTests for SSL are passed.\n```\n$ bundle exec ruby -Ilib:. \\\n\n-r 'rubygems' -r 'minitest/autorun' \\\n  -e 'Dir.glob \"./test//test_ssl.rb\", &method(:require)' \\\n  -- -v\nLoaded suite -e\nStarted\nTestMiniSSL: \n  test_raises_with_invalid_cert_file:               .: (0.000252)\n  test_raises_with_invalid_key_file:                .: (0.000133)\nTestPumaServerSSL: \n  test_form_submit:                     .: (0.007461)\n  test_ssl_v3_rejection:                    .: (0.001483)\n  test_url_scheme_for_https:                    .: (0.007416)\n  test_very_large_return:                   .: (0.268683)\nTestPumaServerSSLClient: \n  test_verify_client_cert:                  .: (0.109976)\n  test_verify_fail_if_client_expired_cert:          .: (0.109023)\n  test_verify_fail_if_client_unknown_ca:            .: (0.108926)\n  test_verify_fail_if_no_client_cert:               .: (0.107421)\n\nFinished in 0.721859306 seconds.\n10 tests, 20 assertions, 0 failures, 0 errors, 0 pendings, 0 omissions, 0 notifications\n100% passed\n\n13.85 tests/s, 27.71 assertions/s\nRun options: --seed 61956\nRunning:\nFinished in 0.000964s, 0.0000 runs/s, 0.0000 assertions/s.\n0 runs, 0 assertions, 0 failures, 0 errors, 0 skips\n```\n. This also happens on Fedora rawhide.\nBuild puma with openssl-devel 1.0.2j: Success\nBuild puma with openssl-devel 1.1.0b: Failure\nSame error on nginx.\nhttps://trac.nginx.org/nginx/ticket/860#comment:6\n. @nateberkopec yeah I can understand your thoughts.\n@evanphx ok I will send the patch! Thanks for the idea!. @evanphx I update using begin - rescure instead of using environment variable.\nIt is easier for me not to use environment variable.\nHow?. As other possible solutions, I want to suggest if ENV[\"BUNDLER_VERSION\"] and if  defined?(Bundler).\n$ bundle exec ruby -e 'puts ENV[\"BUNDLER_VERSION\"] ? \"True\" : \"False\"'\nTrue\n$ ruby -e 'puts ENV[\"BUNDLER_VERSION\"] ? \"True\" : \"False\"'\nFalse\n```\n$ bundle exec ruby -e 'puts defined?(Bundler) ? \"True\" : \"False\"'\nTrue\n$ ruby -e 'puts defined?(Bundler) ? \"True\" : \"False\"'\nFalse\n```\n. On the Travis tests, only Ruby 2.3.3 failed with below Timeout::Error. However it is false positive isn't it?\nTestThreadPool#test_trim_is_ignored_if_no_waiting_threads = /home/travis/build/puma/puma/lib/puma/thread_pool.rb:268:in `join': execution expired (Timeout::Error)\n. > Thanks! No, we're not using the Hoe stuff anymore. There is an open issue for it's removal #1328.\nWelcome! OK. So, I sent another PR to fix the issue right now.\nhttps://github.com/puma/puma/pull/1395\n. I close this ticket because of below comment.\nhttps://bugs.ruby-lang.org/issues/5653#note-46\n\nOK, I withdraw the proposal. The autoload method will stay (for Ruby3.0).\n. \n",
    "adamdavis40208": "A little more info on my side -- I set up a new test on a copy of your master branch, with the following config:\nlog_requests\nstdout_redirect \"t4-stdout\", \"t4-stderr\", true\npidfile \"t4-pid\"\nbind \"tcp://0.0.0.0:10103\"\nrackup File.expand_path('../hello.ru', File.dirname(__FILE__))\ndaemonize false\nAnd wrote a test around \"200 OK should only be in stdout, not in stderr\" and everything is passing.\nI'm pretty stumped why spinning up a puma server from the base repo is acting so differently, but I will keep digging. \n. hey @evanphx, I'm going to close this issue. It truly does seem like a \"me/my app\" problem considering the tests still work. We're using a logger in sinatra that also uses Rack::CommonLogger. \nI will throw a pull request to get my test cases pulled in though, I didn't run into any that were testing the stderr log path for puma. \n. ",
    "kesha-antonov": "App - ios application on ReactNative.\nBackend - rails. I'm using websockets (ActionCable) and fetch/ajax.\n. Some more info:\nI have mount ActionCable.server => '/cable' in routes. I start only 1 server. Not 2 (for http requests and websockets)\n1) I change some files and if I see that server not responding I restart server\n2) If I reload app immediately and it sends 4 requests to server - server hangs.\n3) If I wait while app connects with websockets first and only then I reload app and it send 4 request - all ok. Requests served and websockets connected\nMaybe It depends of 1 server running for ActionCable + ActionController and code reloading.. In my app scenario is (as I remember):\n1) connect to puma with websockets\n2) send multiple requests\n3) everything is OK now\n4) change something in rails app\n5-a) try connect to puma again immediately -> stuck\nOR\n5-b) wait while websockets reconnect to puma via ActionCable -> send requests -> all OK\nMaybe it's related to ActionCable and code reloading of rails\nI did not debug further for now. ",
    "m1lt0n": "@kesha-antonov I'm facing the same issue with a rails-api (rails 5.0) application used as an API and probably the issue has to do with threading indeed. The problem never occurs when runing the application with 1 thread only. Also, the problem does occur only while restarting the server (either manually via rails restart or when code changes in the application and listen gem causes it to be restarted). So, if requests are done while restarting the server they may hang forever, but if I wait a few seconds while restarting and then make requests, everything works properly.\n. ",
    "pharmmd-rich": "@evanphx there is definitely something to this.  I'm having the exact same issue with a Rails application.  My Rails code is never hit, making it very hard to debug.  Advice?\nOnly when I hit CTRL-C do I get the following message:\nCompleted 401 Unauthorized in 85675ms (ActiveRecord: 0.3ms)\nPuma 3.7.1.  Rails 5.0.2.  No ActionCable.. Edit:  Nevermind, it's something else.  A gem possibly.  I just tried WEBrick and I have the same issue.. The way that I was triggering it is via a gem which makes a callback to the API part of the app.  If I have time I will try and create a barebones app + gem and see if I can reproduce it that way.. @stuartc I don't think so.  We aren't using anything like that on our end.. This should definitely be handled in a much better way, and the explanation does not explain what I am seeing.  I haven't had time to look into puma code much, but the requests we are doing shouldn't be hanging at all.  We used passenger and unicorn both without issue.  This only happens with puma.  \nNote that the number of workers at the time I had the issue was set to 5.  Are the workers not releasing after completing the request?  Is it possible there is a delay before they can be made available again?  1 worker should also not be handling all the requests from 1 user, is this the case with Puma?.  CSS files, JS files, etc. could have been downloading, but once that was done the rest of the requests should have been completed instead of hanging.\nMy time is limited unfortunately, I have 3 kids and have ongoing medical issues so I cannot help out much more here.  Hopefully everyone's input will provide some insight into what is actually happening and it can get fixed.\nWe ended up sticking with unicorn for now, and are considering a switch to passenger standalone in the future.. Actually, before I go, let me explain to you why this is happening in our case, then maybe this might help troubleshoot + fix the issue.\nWe have a set of applications, including an application that does user management, authentication, and authorization.  There is a Rails engine that handles a few authorization specific things, and a client gem that talks to the user management application.  We use the same engine and gem in every project, including the user management application.  This means that the user management application is actually talking to itself.  It's done that way to keep things simple and dry.  Every application behaves the same way.  The issue occurs when the user management application needs to use the client library gem (or maybe the engine, I cannot recall) tries to talk to itself.  If I recall correctly, this can happen when other workers were temporarily in use.  If this is the case, it may be a matter of monitoring other workers and shifting them to free workers as they free up.  I could be way out of line here.. ",
    "gertig": "@pharmmd-rich I'm experiencing a similar issue where my Rails code is never hit, ie no logs. Were you able to track down the culprit? Banging my head against a wall here.\nAfter a CTRL-C and restart I can get the app to receive one request then it hangs.\nPuma 3.8.2, Rails 5.0.2\nUPDATE: I think I may have figured out my issue had to do with Rails not autoloading some of my constants but not telling it wasn't doing so because it disliked how I was naming things.\nI have controllers in this structure controllers>api>v1>users_controller and before upgrading to Rails 5 I was using an inflection initializer to allow me to name my controllers API::V1::UsersController rather than Api::V1::UsersController. Purely an aesthetics thing. I decided to rename all those to Api::V1::UsersController and so far the issue seems to have stopped. Will update if I notice this is not an accurate fix.\nUPDATE I was wrong, that was only a temporary fix for some reason.. @kengreeff I'm experiencing the same issue with the exact same configuration except I'm on Puma 3.8.2 (I thought I had found the culprit earlier but I had not, see my previous comment).\n@nateberkopec any thoughts on how we can go about debugging this highly frustrating issue?. ",
    "kengreeff": "Rails 5.0.2\nPuma 3.7.1 (ruby 2.3.1-p112)\nMin threads: 5, max threads: 5\nEnvironment: development\nI'm also experiencing hanging in dev when firing off multiple javascript fetch requests at the same time. I can prevent the hangs by changing the RAILS_MAX_THREADS env variable from 5 to 1.\nIt doesn't happen all the time - a restart and page reload will sometimes work.\n@nateberkopec Hey Nate, how would I go about debugging this to try and provide more info?. ",
    "bviloria": "I'm facing the same issue. This is my environment:\nPuma starting in single mode...\n Version 3.8.2 (ruby 2.3.3-p222), codename: Sassy Salamander\n Min threads: 5, max threads: 5\n Environment: development\n Listening on tcp://localhost:3000\nStarted GET \"/api/v1/reason_types\" for ::1 at 2017-05-09 16:33:15 +0800\nStarted GET \"/api/v1/reasons\" for ::1 at 2017-05-09 16:33:15 +0800\nExiting\nTerminate batch job (Y/N)? y\nwhen i do multiple ajax/fetch call the server hangs. But its working fine if just call 1 at a time.\nStarted GET \"/api/v1/sites\" for ::1 at 2017-05-09 16:33:00 +0800\nProcessing by Api::V1::SitesController#index as JSON\n  Parameters: {\"site\"=>{}}\n  User Load (0.0ms)  SELECT  users.*........\nCompleted 200 OK\n. ",
    "ryanfields": "I know this isn't going to add much new info, and I haven't been able to reproduce this with a bare-bones app, but I appear to be having the same or a related problem.\nRails 5.0.1, Puma 3.9.1, ruby 2.3.1-p112\nWhen my web client loads and hits DOMContentLoaded, four requests are sent to my Rails server in rapid succession (as quickly as the JS interpreter allows).\nOn a fresh rails s with max threads > 1, one or none of these requests will complete.  The rest remain pending in the browser until I hit Ctrl-C to kill the server.\nOn a fresh rails s with max threads = 1, all requests will complete.\nIn the former scenario, I can mitigate by waiting until the first request completes before sending any further requests.  Once an initial request has completed, I can spam the server with as many simultaneous requests as I please and they all complete.  It's only after a fresh load of Puma does it hang on rapid-fire incoming requests.\n. ",
    "shssoichiro": "I am having the same issue as above, sending 4 requests simultaneously using Promise.all, also using 5 threads in Puma. The first request completes, and the others never complete. I get the following output from /rails/locks when adding DebugLocks to my middleware and experiencing the issue:\nThread 0 [0x3fde2918cd78 sleep]  No lock (yielded share)\n  Waiting in start_exclusive to \"load\"\n  may be pre-empted for: \"load\"\n  blocked by: 1\n\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/monitor.rb:111:in `sleep'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/monitor.rb:111:in `wait'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/monitor.rb:111:in `wait'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/monitor.rb:123:in `wait_while'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb:219:in `wait_for'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb:81:in `block (2 levels) in start_exclusive'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb:185:in `yield_shares'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb:80:in `block in start_exclusive'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/monitor.rb:214:in `mon_synchronize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb:75:in `start_exclusive'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/concurrency/share_lock.rb:147:in `exclusive'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies/interlock.rb:11:in `loading'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:36:in `load_interlock'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:357:in `require_or_load'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:510:in `load_missing_constant'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:202:in `const_missing'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/inflector/methods.rb:269:in `const_get'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/inflector/methods.rb:269:in `block in constantize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/inflector/methods.rb:267:in `each'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/inflector/methods.rb:267:in `inject'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/inflector/methods.rb:267:in `constantize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/inflector/methods.rb:312:in `safe_constantize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/core_ext/string/inflections.rb:77:in `safe_constantize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:147:in `_default_wrap_model'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:94:in `block in model'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/mutex_m.rb:74:in `synchronize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/mutex_m.rb:74:in `mu_synchronize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:94:in `model'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:121:in `name'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:259:in `_wrapper_key'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:286:in `_wrapper_enabled?'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:235:in `process_action'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activerecord-5.1.1/lib/active_record/railties/controller_runtime.rb:22:in `process_action'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/abstract_controller/base.rb:124:in `process'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal.rb:189:in `dispatch'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal.rb:253:in `dispatch'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/routing/route_set.rb:49:in `dispatch'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/routing/route_set.rb:31:in `serve'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/journey/router.rb:46:in `block in serve'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/journey/router.rb:33:in `each'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/journey/router.rb:33:in `serve'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/routing/route_set.rb:832:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bullet-5.5.1/lib/bullet/rack.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/etag.rb:25:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/conditional_get.rb:25:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/head.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activerecord-5.1.1/lib/active_record/migration.rb:556:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/callbacks.rb:26:in `block in call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/callbacks.rb:97:in `run_callbacks'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/callbacks.rb:24:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/executor.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/debug_exceptions.rb:59:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/show_exceptions.rb:31:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/rack/logger.rb:36:in `call_app'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/rack/logger.rb:24:in `block in call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/tagged_logging.rb:69:in `block in tagged'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/tagged_logging.rb:26:in `tagged'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/tagged_logging.rb:69:in `tagged'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/rack/logger.rb:24:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/remote_ip.rb:79:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/request_id.rb:25:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/runtime.rb:22:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/cache/strategy/local_cache_middleware.rb:27:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/executor.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/static.rb:125:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/sendfile.rb:111:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/debug_locks.rb:39:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/engine.rb:522:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/configuration.rb:224:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/server.rb:602:in `handle_request'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/server.rb:435:in `process_client'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/server.rb:299:in `block in run'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/thread_pool.rb:120:in `block in spawn_thread'\n\n\n---\n\n\nThread 1 [0x3fde2918cf1c sleep]  Sharing\n  blocking: 0\n\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/mutex_m.rb:74:in `synchronize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/2.4.0/mutex_m.rb:74:in `mu_synchronize'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:94:in `model'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:121:in `name'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:259:in `_wrapper_key'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:286:in `_wrapper_enabled?'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal/params_wrapper.rb:235:in `process_action'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activerecord-5.1.1/lib/active_record/railties/controller_runtime.rb:22:in `process_action'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/abstract_controller/base.rb:124:in `process'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal.rb:189:in `dispatch'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_controller/metal.rb:253:in `dispatch'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/routing/route_set.rb:49:in `dispatch'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/routing/route_set.rb:31:in `serve'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/journey/router.rb:46:in `block in serve'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/journey/router.rb:33:in `each'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/journey/router.rb:33:in `serve'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/routing/route_set.rb:832:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bullet-5.5.1/lib/bullet/rack.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/etag.rb:25:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/conditional_get.rb:25:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/head.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activerecord-5.1.1/lib/active_record/migration.rb:556:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/callbacks.rb:26:in `block in call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/callbacks.rb:97:in `run_callbacks'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/callbacks.rb:24:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/executor.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/debug_exceptions.rb:59:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/show_exceptions.rb:31:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/rack/logger.rb:36:in `call_app'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/rack/logger.rb:24:in `block in call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/tagged_logging.rb:69:in `block in tagged'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/tagged_logging.rb:26:in `tagged'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/tagged_logging.rb:69:in `tagged'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/rack/logger.rb:24:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/remote_ip.rb:79:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/request_id.rb:25:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/runtime.rb:22:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/cache/strategy/local_cache_middleware.rb:27:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/executor.rb:12:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/static.rb:125:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/sendfile.rb:111:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/actionpack-5.1.1/lib/action_dispatch/middleware/debug_locks.rb:39:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/engine.rb:522:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/configuration.rb:224:in `call'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/server.rb:602:in `handle_request'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/server.rb:435:in `process_client'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/server.rb:299:in `block in run'\n/Users/joshua.holmer/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/puma-3.9.1/lib/puma/thread_pool.rb:120:in `block in spawn_thread'.\n",
    "stuartc": "I'm also experiencing this issue - at least in development mode (can't reproduce in production).\nI'm wondering if suckerpunch or some other actor/thread based gem has something to do with this.. I can confirm this issue, if I run rails s or puma without bundle exec I get the error.\nHowever, curious; since rails runner 'puts ENV[\"RUBYOPT\"]' (without bundle exec) returns -rbundler/setup.. ",
    "sepastian": "I found the --tcp-mode paramter, which forces to run in TCP mode. I need the opposite of that.\n. No, I'm not using a config/puma.rb file.\nI just do rackup -s puma.\n. @evanphx, thanks for your reply, took me a while to test this.\nSo I tested with puma and thin; invoking both http://0.0.0.0:9292, http://127.0.0.1:9292 and http://locahost:9292; using both Firefox, Chrome and curl as a client.\nWhen using rackup -s puma --host 127.0.0.1, all clients timed out for all URLs. The console did not show any clients trying to connect.\nWhen using rackup -s thin --host 127.0.0.1 instead, Chrome and curl would connect and return as expected. Firefox would connect but returned a 404 error for some reason.\nFirefox returning a 404 from thin may point at a problem on my end. On the other hand, why does everything work fine in Chrome and curl using thin? What's the difference here between thin and puma?\n. Here's some information about my environment:\n$ uname -a\nLinux sebastian 4.6.0-1-amd64 #1 SMP Debian 4.6.4-1 (2016-07-18) x86_64 GNU/Linux\n$ ruby -v\nruby 2.3.0p0 (2015-12-25 revision 53290) [x86_64-linux]\n$ rbenv -v\nrbenv 1.0.0-21-g9fdce5d\n$ bundle show puma\n/home/sebastian/.rbenv/versions/2.3.0/lib/ruby/gems/2.3.0/gems/puma-3.6.0\n$ thin -v\nthin 1.7.0 codename Dunder Mifflin\n. No, neither did work with puma.\nHow can I verify whether of not this has to do with my network configuration?\nAs I said, the strange thing is that is works with thin...\n. That works, must be something in my app then!\nLast question: can I debug puma somehow? Can I hook into the request cycle?\nSorry for bothering you and thanks for you help!\n. Update.\nI have been digging deeper into this. The timeout occurs, if I define a rackup app as a class and require mongoid. It does not occur if I either don't require mongoid or define my app as a lambda.\n``` ruby\ncat Gemfile\nsource 'https://rubygems.org'\ngem 'mongoid'\ngem 'grape'\ngem 'roar'\ngem 'grape-roar'\ngem 'puma'\n```\nThis works:\n``` ruby\ncat config.ru\nrequire 'mongoid'\nrequire 'grape'\nrequire 'grape-roar'\nrun lambda { |env| [200, {\"Content-Type\" => \"text/plain\"}, [\"Hello World\"]]  }\n```\nAs does this:\n``` ruby\ncat config.ru\nrequire 'mongoid'\nrequire 'grape'\nrequire 'grape-roar'\nmodule GGAT\n  class API < Grape::API\n    get '/test' do\n      \"HALLO\"\n    end\n  end\nend\nrun GGAT::API\n```\nBut this does not (puma starts up but requests never show up on the console and, eventually, timeout):\n``` ruby\ncat config.ru\nrequire 'mongoid'\nrequire 'grape'\nrequire 'grape-roar'\nmodule GGAT\n  class API < Grape::API\n    get '/test' do\n      \"HALLO\"\n    end\n  end\nend\nrun GGAT::API\n```\nAlthough things start to fail as mongoid gets required, this has to do with puma, because the last version is working as expected in thin.\nCould this have to do with how/at which point puma is loading dependencies?\nI'm puzzled...\n. Found a solution. It made a stupid mistake, I didn't add rack to my bundle, but used Ruby's rackup command instead.\nFYI:\n```\nadd rack to Gemfile\ngem 'rack'\n```\n```\ngenerate binstubs for rack\n$ bundle binstubs rack\n$ bin/rackup # instead of just rackup\n```\nThanks for you help!\n. ",
    "terranisu": "@sriedel thanks a lot. after update to 2.16.0 (from 2.15.3) this problem is gone for me.. ",
    "shahkia": "@evanphx Thank you for your reply. \nActually, all parameters such as min_thread,workers and even before_fork which is a Proc  work well. The problem is exactly where I need to use something like stdout_redirect or preload! which can not be used like other (before_fork,...) .\nThe reason why I can't launch my app via puma, is that, my main app is containing another services(drb server, job runner service ,...) and each service is forked from main app. Also, only one of my services need to be served via Puma. Actually, I need main app process in each service(sub-processes). So, I only run ruby myapp.rb.\n. @evanphx Thank you so much\nPuma::Launcher and Puma::Configurtion did that:\nconf = Puma::Configuration.new do |c|\n  c.threads @conf.min_threads, @conf.max_threads\n  c.app @app\n  c.bind \"tcp://#{@conf.bind}:#{@conf.port}\"\n  c.bind \"unix:///#{MAIN.root_path}/tmp/puma-rest.sock\"\n  c.workers @conf.workers\n  c.environment MAIN.environment.to_s\n  c.stdout_redirect '/var/log/oopq/app.log','/var/log/oopq/app.log',false\n  c.quiet\n  c.before_fork do;end\n  c.on_worker_boot do;end\nend\nPuma::Launcher.new(conf).run\nThank you once again.\nthough, issue is closed, can I hide/redirect following outputs from puma?\n[12705] Puma starting in cluster mode...\n[12705] * Version 3.4.0 (ruby 2.3.0-p0), codename: Owl Bowl Brawl\n[12705] * Min threads: 16, max threads: 32\n[12705] * Environment: development\n[12705] * Process workers: 4\n[12705] * Phased restart available\n[12705] * Listening on tcp://127.0.0.1:9292\n[12705] Use Ctrl-C to stop\n. ",
    "SimonKaluza": "@headius Sure I will try with JRuby 9.1.5.0 tonight and leave it running overnight\nFWIW I can confirm that using a more conservative thread count in the puma.rb config did not help, the error had returned when I checked the server this morning.\n. No this error doesn't come up in MRI Ruby, I just included that as the Ruby version as extra information since that was the one that the JRuby version I was running conforms to.\nInterestingly enough my web application crashes even more quickly when using JRuby 9.1.5.0 due to a different error.  With JRuby 9.1.5.0 my application responds to a 2 or 3 requests just fine, but on one particular request that loaded fine in JRuby 9.0.1.0 (but is relatively heavy in ActiveRecord operations) we get the following stacktrace:\n2016-09-23 21:42:04 +0000: Listen loop error: java.lang.InternalError: BMH.reinvoke=Lambda(a0:L,a1:L)=>{\n    t2:L=BoundMethodHandle$Species_LL.argL1(a0:L);\n    t3:L=MethodHandle.reinvokerTarget(a0:L);\n    t4:L=MethodHandle.invokeBasic(t3:L,t2:L);t4:L}\njava.lang.invoke.MethodHandleStatics.newInternalError(MethodHandleStatics.java:97)\njava.lang.invoke.LambdaForm.compileToBytecode(LambdaForm.java:460)\njava.lang.invoke.LambdaForm.checkInvocationCounter(LambdaForm.java:634)\nwebroot.my_application.vendor.bundle.jruby.$2_dot_3_dot_0.gems.puma_minus_3_dot_6_dot_0_minus_java.lib.puma.server.RUBY$block$handle_servers$0(/webroot/my_application/vendor/bundle/jruby/2.3.0/gems/puma-3.6.0-java/lib/puma/server.rb:332)\norg.jruby.runtime.CompiledIRBlockBody.yieldDirect(CompiledIRBlockBody.java:156)\norg.jruby.runtime.MixedModeIRBlockBody.yieldDirect(MixedModeIRBlockBody.java:122)\norg.jruby.runtime.BlockBody.yield(BlockBody.java:108)\norg.jruby.runtime.Block.yield(Block.java:167)\norg.jruby.RubyArray.each(RubyArray.java:1734)\norg.jruby.RubyArray$INVOKER$i$0$0$each.call(RubyArray$INVOKER$i$0$0$each.gen)\norg.jruby.internal.runtime.methods.JavaMethod$JavaMethodZeroBlock.call(JavaMethod.java:497)\norg.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:77)\norg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:83)\norg.jruby.ir.instructions.CallBase.interpret(CallBase.java:428)\norg.jruby.ir.interpreter.InterpreterEngine.processCall(InterpreterEngine.java:356)\norg.jruby.ir.interpreter.StartupInterpreterEngine.interpret(StartupInterpreterEngine.java:73)\norg.jruby.ir.interpreter.InterpreterEngine.interpret(InterpreterEngine.java:78)\norg.jruby.internal.runtime.methods.MixedModeIRMethod.INTERPRET_METHOD(MixedModeIRMethod.java:144)\norg.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:130)\norg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:189)\norg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:318)\norg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:131)\norg.jruby.ir.interpreter.InterpreterEngine.processCall(InterpreterEngine.java:340)\norg.jruby.ir.interpreter.StartupInterpreterEngine.interpret(StartupInterpreterEngine.java:73)\norg.jruby.ir.interpreter.Interpreter.INTERPRET_BLOCK(Interpreter.java:132)\norg.jruby.runtime.MixedModeIRBlockBody.commonYieldPath(MixedModeIRBlockBody.java:148)\norg.jruby.runtime.IRBlockBody.call(IRBlockBody.java:66)\norg.jruby.runtime.Block.call(Block.java:126)\norg.jruby.RubyProc.call(RubyProc.java:324)\norg.jruby.RubyProc.call(RubyProc.java:249)\norg.jruby.internal.runtime.RubyRunnable.run(RubyRunnable.java:104)\njava.lang.Thread.run(Thread.java:745)\nSubsequent requests after this one will yield an exception like this:\nError reached top of thread-pool: identity=Lambda(a0:L,a1:L,a2:L)=>{\n    t3:L=Species_L.argL0(a0:L);\n    t4:L=ValueConversions.identity(t3:L);t4:L} (Java::JavaLang::InternalError)\nGoogling for errors like these leads me to StackOverflow posts that point to Out of Memory errors being a likely culprit, but increasing the JVM Heap size and adding full backtraces with JRUBY_OPTS=\"-J-Xmx2g -Xbacktrace.style=full\" does not seem to fix the problem.\n. Oracle JDK 7, specifically:\nroot@development-application-a:/usr/local/share/ruby-build# java -version\njava version \"1.7.0_75\"\nJava(TM) SE Runtime Environment (build 1.7.0_75-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 24.75-b04, mixed mode)\n. It is an admittedly complex application that is currently hosted via Unicorn on MRI in production.  We've been migrating to JRuby compatible gems and Puma to improve our concurrent request throughput.\nI will see if I can reproduce the conditions in a simpler application, and I will test a more recent JDK as well and report back\n. Interestingly enough, with the combination of the JDK update to 8 (1.8) and JRuby 9.1.5.0 the problem I experienced with JDK 7 and JRuby 9.1.5.0 immediately after launching went away.\nI will continue to monitor it and see how it performs overnight then report back.  Hopefully the original Listen loop error: #<SocketError: problem when accepting> I received after an extended time running with JRuby 9.0.1.0 and JDK 7 will go away as well.\n. ",
    "ckuwanoe": "I have a upstart script\n```\nstart on starting blocks-web\nstop on stopping blocks-web\nrespawn\nenv PORT=5000\nenv RAILS_ENV='production'\nenv PATH='/home/deployer/.rbenv/shims:/home/deployer/.rbenv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games'\nsetuid deployer\nchdir /home/deployer/apps/blocks/releases/353\nexec bundle exec puma -p 5000 -C config/puma.rb\n```\n. Did something change between 2.16 and 3.x with the startup of puma that would cause this?\n. Gotcha. Thanks so much for the help!\n. ",
    "pchaganti": "\ud83d\udc4d \n. \ud83d\udc4d . Hmm. Seeing the same thing. In our case, it seems to timeout for a bit, and then start responding again. The time varies between 1-5 min. This server not really under a lot of load either.\n. ",
    "fidalgo": "Any plans to get this?. Sorry @nateberkopec but can you point the commit or the PR?\nIf I start a Rails server rails s with:\n Puma starting in single mode...\n* Version 3.7.0 (ruby 2.3.3-p222), codename: Snowy Sagebrush\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nI still can't see the Backtrace, and by the commit messages after 3.7.0 I can't infer the commit on this.. @nateberkopec \nStaring a rails server with rails s\nIf I send the TTIN signal to the puma process: \nps uax | grep puma\nfidalgo   3703 18.6  1.3 1296600 110912 pts/3  Sl+  16:29   0:02 puma 3.7.0 (tcp://0.0.0.0:3000) [registadora]\nThe process continue to run:\nkill -TTIN 3703\n[fidalgo@localhost registadora]$ ps uax | grep puma\nfidalgo   3703  5.5  1.3 1296600 110912 pts/3  Tl   16:29   0:02 puma 3.7.0 (tcp://0.0.0.0:3000) [registadora]\nbut the rails s output console I have a message saying the process was stopped:\n``\nrails s\n=> Booting Puma\n=> Rails 5.0.1 application starting in development on http://localhost:3000\n=> Runrails server -h` for more startup options\nPuma starting in single mode...\n Version 3.7.0 (ruby 2.3.3-p222), codename: Snowy Sagebrush\n Min threads: 5, max threads: 5\n Environment: development\n Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\n[1]+  Stopped                 rails s\n```\nNote that puma is in single mode!\n. ",
    "gnazarkin": "@pawel2105 How'd you go about solving it?. ",
    "daveallie": "Upon further testing it looks like this won't worker for bundler gem installed from git as https://github.com/puma/puma/blob/master/bin/puma-wild#L20 calls gem 'gemname', 'gemversion' which looks for rubygem gems.\nMaybe the best way forward is to resolve needed gem paths before forking (at the top of config/puma.rb), and then adding them to the load path after fork?\n. Not running the extra dependencies through the gem call alleviates my previous concern, adding the path to $LOAD_PATH is enough.\n. prune_bundler runs before the before_fork block. It unloads all gems, except puma iirc, so that during your app boot, you can get bundler's context and load all the gems back in. I believe it is done this way so that individual workers hold the bundler context, allowing a rolling restart to reload all gems (and potentially use new ones if the gemfile has changed).\nBefore submitting it this way I attempted to load puma worker killer using bundler in the before_fork block, but to do it I needed to load bundler as well (as I was using a git ref for the gem). This meant that app workers were booted with a provided bundler context, meaning that rolling restarts didn't reload existing gems, unload old ones, or reload new ones.\nI could have copy-pasted the logic behind prune_bundler into the top of my before_fork but thought it better to loop it into the existing logic and make it available for anyone else in the same situation.. Any update on this PR?. @nateberkopec any chance of moving this PR forward? It's almost been a year and nothing has really happened.. Hi @nateberkopec or @schneems. Just thought I'd ping this again. Every time I want to use Puma master I need to rebase these changes on top.\npuma_worker_killer being required without the use of bundler means that it will fail to find puma if puma is being sourced through a github ref, meaning the app crashes on boot. Happy to work around this problem if you have another way, but at the moment it makes the most sense to me to allow user defined gems to be included required in the master process's memory.. ",
    "epinault": "thanks!\n. ",
    "amichal": "we had this too in an older version, queue_requests appears to fix it\n. If i change only the --bind \"ssl://0.0.0.0:3000?key=/var/www/test-app/shared/ssl-cert-snakeoil.key&cert=/var/www/test-app/shared/ssl-cert-snakeoil.pem\" to --bind \"tcp://0.0.0.0:3000\" AWS never reports 502 errors (where never means not with 10k sustained requests with the above siege line). Similarly if it use siege to hit puma on port 3000 directly i get no 502s. The problem is the interaction between AWS ALB and puma with SSL. ",
    "mitchenkod": "I have faced this issue too. The file uploading process, when a few relatively large files (~ 150 mb) are uploading concurrently puma server becomes completely unresponsive. This is the issue, I have closed it because of duplicating https://github.com/puma/puma/issues/1115.\n. Yes, seems like this it it\n. ",
    "ShootingStarr": "nvm, my mistake\n. ",
    "thefonso": "I'm sorry, I'm lost. what is the purpose of this file? Where does it go? I followed the conversation located here...https://github.com/puma/puma/issues/1211\nAnd I do not understand how this file fits into that previous conversation?. alternative usage of upstart does not work. stick with systemd solution as described by hackedunit. ",
    "montdidier": "Looks like it was a hung build? \n. Sorry for slow response. Will update as requested. . Ok. I've removed the Puma version bump and Puma::StateFile and added a test to cover the failure case. Anything else?. It was from capistrano-puma, so looking at the code it's calling pumactl -S .. -F restart which in lib/puma/control_cli.rb appears to be issuing: \nProcess.kill \"SIGUSR2\", @pid\nI discovered it running a very simple rails app and using rbenv. I can try to reproduce it with a simple rack app.. I couldn't reproduce the problem with a simple rack app, but my rbenv configuration in production is somewhat different to my test scenario. In the problem scenario, my rbenv has bundler installed into the user rbenv directory. The rest of the gems for the rails application are vendored in vendor/bundle. One can see in the log extract above the puma gem is being searched for in user gem area but not the vendor area when the puma gem is actually installed. \nI don't know exactly what prune_bundler is intended to do but it appears to reconfigure GEM_HOME correctly for this scenario. . ",
    "alexlance": "There is an issue with the AWS ELB, where if the ELB's idle timeout is larger than the instance's tcp keepalive timeout, then the ELB will sometimes report back 504s to the user client.\nIf you configure puma with this first_data_timeout setting to a value that is higher than the ELB's idle timeout, then you can circumvent that problem.\nThis patch exposes the first_data_timeout setting to the puma DSL. I have manually tested this patch and it works as intended.\n. Right. I don't know how to fix that.. So apparently if you add travis_wait in front of the bundle exec command in the .travis.yml file then you get 20 minutes of grace time instead of 10 minutes from travis ci. Would that help?. ",
    "zevarito": "Thinking a little more about this, what I really want is to be able within a Request to figure out which Puma Worker is serving it. after_work_fork will not work for what I am looking for.\nHere[0] is thread from Prometheus Ruby Client project that might clarify what and why I am looking for, besides that thread was initiated to make it work with Unicorn, Puma might present the same issue.\n[0] https://github.com/prometheus/client_ruby/issues/9\n. So, on_worker_boot seems to return worker index on block, I learn that from tests, it should be enough to set a process variable and use it.\n. ",
    "olleolleolle": "Here are some background reading links, for the interested learner:\n\nKernel in Ruby docs for 2.3.3 has information about what the close_others setting does in current Ruby.\nsystemd for Developers: Socket activation has narrative information about the Socket Activation feature.. @parhs I think you're right! I'm going to add a pen-PR.. The #init? is in the C ext but not in the Java one \n. Logged https://github.com/puma/puma/issues/1125 as its own issue.\n. The 2.2.5 build seems to have flaked - lost contact. I force-pushed this branch to make a new build.\n. Well, now the 2.3.1 failed.\n. @hapan Thanks for asking - I'm not planning on finishing this investigation by fixing the issue.. Heh, one day we'll all get tired of this and automate it once and for all. Until then, we can high-five at each patch release, like this: \u270b \n. @nateberkopec ~~This thing now makes that test pass \u2013 I don't know whether this breaks behavior.~~ Did the tests for that thing go away?\n\nDid the Java SSL code ever have the issues described in https://github.com/puma/puma/commit/46416cb49ed2f16614f019cee969bb8f5d0a6146 ?\n. CC @evanphx Hi! I'm a JRuby fan, and I flailed about with the MiniSSL.java code that you'd been editing, too. \nCan you tell if the implementation's now skipping anything useful in the Java implementation?\n. @evanphx I've not yet tried it against https://github.com/mattyb/puma-socket-test  - which I just noticed, so it'll be interesting.. Ah, here's the output I need to implement around:\nSSL handshake failed (1).\n140735159976016:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:/Library/Caches/com.apple.xbs/Sources/libressl/libressl-1.60.1/libressl/ssl/s23_clnt.c:565:\nSSL handshake failed (1).\n140735159976016:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:/Library/Caches/com.apple.xbs/Sources/libressl/libressl-1.60.1/libressl/ssl/s23_clnt.c:565:\nSSL handshake failed (1).\n140735159976016:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:/Library/Caches/com.apple.xbs/Sources/libressl/libressl-1.60.1/libressl/ssl/s23_clnt.c:565:\nSSL handshake failed (1).\n140735159976016:error:140770FC:SSL routines:SSL23_GET_SERVER_HELLO:unknown protocol:/Library/Caches/com.apple.xbs/Sources/libressl/libressl-1.60.1/libressl/ssl/s23_clnt.c:565:. @davidarnold Hello! Welcome to carry it! Status: stalled.\n(The current situation in master is NoMethodError on these new methods.)\nThis PR makes stupid changes, in order to start the conversation about what MiniSSL.java should do. It takes a bit of knowing to even see how that Java code does things, as compared to the C code. They're quite different.\nThe new stub methods are there because the main Ruby code minissl.rb gained a new API which wasn't there before, and which only has C extension support (but not for Java).\nTo prove the minimally extended MiniSSL.java does the right thing (or rather, that this change I proposed is quite wrong), I was linked to https://github.com/mattyb/puma-socket-test which is a system test for this thing. \n. @matobinder Can you edit your description text a little? Both of the cases that you contrast are labeled \"When using puma, and curl verbose I see it use [...]\".. @nateberkopec This Rack commit began using #eof?: https://github.com/rack/rack/commit/3f6aee5544582abff3911e61dbe3c01506c63fbd. Hi @senid231! I looked at this failure that someone else had run into when their apt installation lacked that function: Do you also need libgmp3-dev? See also https://github.com/puma/puma/issues/848\nhttp://stackoverflow.com/questions/30143180/puma-gem-failed-to-build-gem-native-extension/34537944#34537944. @nateberkopec For comparison to other gems. The sidekiq gem uses the TTIN signal to print threads' backtraces.\n\nCode\nWiki explanation. Is the use-case this awesome:\n\n\nHit Ctrl-T in your *BSD terminal to print thread backtraces\n\n(This is a super-neat feature.). (Also, could you rebase this on top of current master? The build is green on master.). This looks green and master builds green, are there any blockers to merging this change?. What's the relationship of this change to this change which closed that issue which this PR wants to fix?\nhttps://github.com/puma/puma/commit/0584a3345e9d7ecd84494abc6ba7971309b5a338. @nateberkopec Can you re-review this change, now that it's green?. @nateberkopec The failure is not on Travis, where I configured the change.\n\n\n```\nragel ext/puma_http11/http11_parser.rl -C -G2 -I ext/puma_http11 -o ext/puma_http11/http11_parser.c\nrake aborted!\nCould not build wrapper using Ragel (it failed or not installed?)\nC:/projects/puma/Rakefile:26:in `rescue in block in '\nC:/projects/puma/Rakefile:23:in `block in '\nCommand failed with status (127): [ragel ext/puma_http11/http11_parser.rl -C ...]\nC:/projects/puma/Rakefile:24:in `block in '\nTasks: TOP => default => test:all => test => compile => compile:x64-mingw32 => compile:puma_http11:x64-mingw32 => copy:puma_http11:x64-mingw32:2.2.6 => tmp/x64-mingw32/puma_http11/2.2.6/puma_http11.so => ext/puma_http11/http11_parser.c\n(See full trace by running task with --trace)\n```\n\nCould not build wrapper using ragel\n\nFailed AppVeyor build. Thank you for maintaining this!. A pedant's ~note~ question:\nCould it be reasonable to compile the C extensions using a compiler version which does not emit warnings? (Hint: no.)\nSketch of a Travis configuration to make this happen:\nyaml\naddons:\n  apt:\n    sources:\n      - ubuntu-toolchain-r-test\n    packages:\n      - g++-6\nenv:\n  global:\n    - CC=gcc-6\n    - CXX=g++-6\nExample warning:\n\nwarning: ISO C90 forbids mixed declarations and code [-Wdeclaration-after-statement]. @nateberkopec Hi Nate, I was looking at the compile output, and it's using an ancient version of GCC, which has C90 as standard. The emitted warnings are... old-style.. @nateberkopec I thought long about it, and wondered if we should be compiling with something v. old (proving that it works) OR if the right solution is to compile it with a newer version of GCC (avoiding compilation warnings about C90 code style).\n\n. (I'm sorry to make you have this low-value, pedantic conversation.). If usage is a guiding thing, then \"what Travis defaults to\" could be said to constitute usage. If that's so, then perhaps we ought to follow this old-style way of moving declarations to the top of the function (C90 recommendation)? \ud83c\udf17 Tricky.. Let's just not.. The test is quite clear.\nThe build:\n\nMRI failure looks like a flake failure\nJRuby failure could perhaps be a flaky test? Can we retry this rebased on master?. The issue is a current misbehavior in RubyGems + Bundler - a Bundler comes prepackaged with RubyGems. It can turn out that your installed bundle executable won't be found by Ruby.\n\nCurrent thing creating issues: The integration tests want to run the following shell command:\n```ruby\n  def server(argv)\n    base = \"#{Gem.ruby} -Ilib bin/puma\"\n    base.prepend(\"bundle exec \") if defined?(Bundler)\n    cmd = \"#{base} -b tcp://127.0.0.1:#{@tcp_port} #{argv}\"\n    @server = IO.popen(cmd, \"r\")\nwait_for_server_to_boot\n\n@server\n\nend\n```\nthis includes a bundle exec.\nI worked around this in some private code, today, by not shelling out to bundle exec, but that's not feasible here, I guess. TL;DR: The following won't help here.\nIn that case, I did two things: \n\nI had  before_install with redundant RubyGems upgrades:\n  ```yaml\n  before_install:\ngem update --system\ngem install bundler\n  ```\n\n\nMove out rvm elements to an explicit matrix description, to avoid shelling out to bundle exec inside the Ruby code (I moved the command that needed bundle exec in front of it out to the before_script step)\n  ```yaml\n  matrix:\n    include:\nrvm: 2.4.2\n    before_script:\nbundle exec rake db:create db:migrate\nbundle exec sequel postgresql://localhost/test_sessions -m migrations\n\n\nrvm: jruby-9.1.13.0\n    before_script:\nbundle exec rake db:create db:migrate\nbundle exec sequel jdbc:postgresql://localhost/test_sessions -m migrations\n  ```\n\n\n\n\n\n\nWorkarounds which include downgrading:\n\ntry installing a specific version of Bundler? Example: gem install bundler -v1.15.4\ntry installing a specific version of RubyGems? Example: gem update --system 2.6.14. Oh, 2.7.4 was just released!\n\nRelated: https://github.com/bbatsov/rubocop/commit/94174b10137095208ac292b0a7bc5e5471b37fb2. @nateberkopec Are you looking for a test for this bug fix?. @evanphx I apologize, this had been sitting - I'll turn these TODO items into a checkbox list and then do them.\nWhen visible and less obtuse, review becomes much easier.. The failing build is fixed by #1506 - which removes 2.1.. This looks like a good update to the test suite!. Hi, Florin, cool changes!\nI went and read the AppVeyor failed-build output.\nThis is the output message from the failed AppVeyor build: does it tell you anything?\n1) Error:\nTestPumaServer#test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk:\nErrno::ECONNABORTED: An established connection was aborted by the software in your host machine.\n    C:/projects/puma/test/test_puma_server.rb:686:in `read'\n    C:/projects/puma/test/test_puma_server.rb:686:in `test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk'\n\n\n```\nTestPumaServer#test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk = [MinitestRetry] retry 'test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk' count: 1,  msg: Errno::ECONNABORTED: An established connection was aborted by the software in your host machine.\n    C:/projects/puma/test/test_puma_server.rb:686:in `read'\n    C:/projects/puma/test/test_puma_server.rb:686:in `test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk'\n[MinitestRetry] retry 'test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk' count: 2,  msg: Errno::ECONNABORTED: An established connection was aborted by the software in your host machine.\n    C:/projects/puma/test/test_puma_server.rb:686:in `read'\n    C:/projects/puma/test/test_puma_server.rb:686:in `test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk'\n[MinitestRetry] retry 'test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk' count: 3,  msg: Errno::ECONNABORTED: An established connection was aborted by the software in your host machine.\n    C:/projects/puma/test/test_puma_server.rb:686:in `read'\n    C:/projects/puma/test/test_puma_server.rb:686:in `test_chunked_request_pause_between_cr_lf_after_size_of_second_chunk'\n2.05 s = E\n```\n\n\n. @florin555 Perhaps you know how to make a focused query to https://help.appveyor.com/discussions/problems ?. The reason is the shebang line: ruby -wc can't tell that this example is a Ruby script.\nIf we'd change ruby in the shebang line to puma, or remove the shebang line, the syntax check would output \"Syntax OK\".\n. The test failure is the same as previous versions on JRuby fails on:\nTestBinder#test_binder_parses_jruby_ssl_options = rake aborted!\n\nThese build warnings for the Java extension seem to remain the same, as well:\njavac -extdirs \"/usr/lib/jvm/java-8-oracle/jre/lib/ext:/usr/java/packages/lib/ext\" -target 1.5 -source 1.5 -Xlint:unchecked  -cp \"/usr/lib/jvm/java-8-oracle/jre/lib/resources.jar:/usr/lib/jvm/java-8-oracle/jre/lib/rt.jar:/usr/lib/jvm/java-8-oracle/jre/lib/sunrsasign.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jsse.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jce.jar:/usr/lib/jvm/java-8-oracle/jre/lib/charsets.jar:/usr/lib/jvm/java-8-oracle/jre/lib/jfr.jar:/usr/lib/jvm/java-8-oracle/jre/classes:/home/travis/.rvm/rubies/jruby-9.1.17.0/lib/jruby.jar:\" -d tmp/java/puma_http11 ext/puma_http11/PumaHttp11Service.java ext/puma_http11/org/jruby/puma/Http11.java ext/puma_http11/org/jruby/puma/Http11Parser.java ext/puma_http11/org/jruby/puma/MiniSSL.java\nPicked up _JAVA_OPTIONS: -Xmx2048m -Xms512m\nwarning: [options] bootstrap class path not set in conjunction with -source 1.5\nwarning: [options] source value 1.5 is obsolete and will be removed in a future release\nwarning: [options] target value 1.5 is obsolete and will be removed in a future release\nwarning: [options] To suppress warnings about obsolete options, use -Xlint:-options.\nNote: ext/puma_http11/org/jruby/puma/Http11.java uses or overrides a deprecated API.\nNote: Recompile with -Xlint:deprecation for details.\n4 warnings. Closed in favor of #1617 . So, this is a working fix to the issue, which was visualized in #1618 - can this be merged?. The new skip methods improve the legibility of the tests! The RuboCop update seems to have worked nicely.. \\o/ huzzah! @MSP-Greg thanks for that!. Can this be closed?. Thanks!. @MSP-Greg Oh, great, and now could we also add 2.6.0 in the CI matrix - in this PR?. Perhaps adding the 2.6.0 is worth its own PR.. Update: My findings were: there's a [45198] - Gracefully shutting down workers... where I stalled.. @enebo Would you be able to give this a review? \ud83d\udd0d \ud83d\udc41\u200d\ud83d\udde8 \ud83d\udd0e . @evanphx @schneems The tests pass (apart from a \"rubygems-update requires Ruby version >= 2.3.0.\" on Ruby 2.2) and code has been reviewed.\nIs this ready, now?. Oh, this duplicates the effort of #1693 (which was perhaps even more clarifying, but you added a great screenshot which explains the issue excellently).. This should build fine now, after MSP-Greg's commit.. Here's the current compare to master. Oh, this duplicates the effort of #1693. And #1688. . Ha, thanks for the mention, @MSP-Greg, I resolved the conflict with master, and this PR now has 0 changes and can be closed. <3 . Hi!\nThis text about how to define a systemd Service Unit file can be improved to avoid jargon.\n+# Use `cap <stage> puma:start -n` in order to spot the exact command,\n+# which could change for example is a Ruby version manager is used\nAfter reading a little about the systemd way of defining Services and Capistrano, I propose this different wording of the example text:  \n```\nTo learn which exact command is to be used to execute at \"ExecStart\" of this\nService, ask Capistrano: cap <stage> puma:start --dry-run. Your result\nmay differ from this example, for example if you use a Ruby version\nmanager. <WD> is short for \"your working directory\". Replace it with your\npath.\n```\n. Thanks for improving Puma!. Perhaps not use a regular expression, but a String instance method?\nruby\nelsif host && host.start_with?('ssl://'). Is there a specific reason not to use loop do ... end for this loop?. (It'd introduce a block, making data block-local.). jruby-9.1.17.0 is out!\nhttp://jruby.org/2018/04/23/jruby-9-1-17-0.html. Is this debugging which you want to keep in the test file?. suggestion\n  * Better support for detecting runtimes that support `fork` (#1630). suggestion\n        arg0[1, 0] = spec.full_require_paths.flat_map { |path| ['-I', path] }\nThis is great, with lots of clarity in the comment.\nMy suggestion was stylistic (Kernel#p is a method.). ",
    "parhs": "Are you sure that this works?! Its argv / args. I don't see where the variable is used.. @olleolleolle  Great! I run into this issue after adding prune_bundler into puma.rb configuration and doing a bundle exec pumactl -S .....  restart .\nHowever the error seems to occur the first time that prune_bundler is added. However I am not sure why prune_bundler is needed. ",
    "hiroara": "Gotcha! :). ",
    "elvoblin": "Thanks for explanation.\nI think issue may be closed.\n. ",
    "koenpunt": "Update; the problem even occurs when running 2 separate processes. Could it have to do with server limits? Because I've seen other issues mentioning that. And if that could be the case, how to track that down? \n. Creating a example app is difficult, because I haven't been able to manually reproduce it. But I might have found a pointer on what is causing it. Will get back when I know more.. ",
    "vk26": "I have faced with this problem. The reason was not in the puma, but in the rack. The problem manifested itself when was uploaded large file The problem was solved by upgrade rack ( I had version 2.0.1, updated to 2.0.4). After that, the problem no longer arose.\nLink to issue with rack\n. ",
    "hapan": "Hello, any plan to fix this issue? I am getting such error too.. ",
    "davidarnold": "@hapan Currently, this is just a cosmetic error and can be ignored.  The error is trapped and the socket is still closed correctly here. @rfischer20 There could be some overhead from throwing the exception, but I wouldn't expect it to make the server hang.  Maybe this message is being output at the same time something else is causing it to hang?\nCan you post which version you are running along with some logs?. @olleolleolle What's the status of this PR?  Are those test failures related to the new stub methods or a preexisting problem?\nI would like to take this forward in getting it merged.. @evanphx Are the changes from https://github.com/puma/puma/commit/46416cb49ed2f16614f019cee969bb8f5d0a6146 even relevant for JRuby?  It seems to me that in the JRuby case, the old behavior of just calling @socket.close is sufficient.\nJust asking the broader question of whether these methods need to be stubbed out at all, or rather the close behavior needs to be different if we are running on JRuby.. The code path in minissl.rb is also being implicated in https://github.com/puma/puma/issues/1214. Not sure why the CI tests failed only for 2.4.0 and succeeded for everything else.  This only changes a .java file.  JRuby passed, FWIW.\nEDIT: I spoke too soon.  It rebuilt and everything passed.. CI status keeps flip flopping.  Making note of last run, which was in fact successful: https://travis-ci.org/puma/puma/builds/213208778. @evanphx Would you mind giving this PR a look?  I believe it is all working and ready to go.. Submitted pull request https://github.com/puma/puma/pull/1248. PR merged. ",
    "rfischer20": "@davidarnold I am also receiving this error in jruby and it causes the server to hang for a period of time before completing the request. I plan to look more into it myself but just found this reference and it is impacting the app performance due to slow requests.. ",
    "rvictory": "A real hack, but for now this is what we're doing to work around the issue:\n```\nrequire 'puma/minissl'\nFix Puma SSL in JRuby\nif RUBY_ENGINE == 'jruby'\n  module Puma\n    module MiniSSL\n      class Socket\n        def should_drop_bytes?\n          false\n        end\n      end\n    end\n  end\nend\n```. ",
    "dawidof": "@pavan461 No. I haven't found for solution. You have another problem. I want to open https://localhost:3000, and in your case, as I have understood, http, but redirects to https. Read my answer in your opened issue, maybe it will help you.\n. @nateberkopec, Yeah sure, I cleared history and cookies, but I have another issue. I want to run rails via https on local machine, not via http.\n. You can try to clear browser cache(or history) and restart the browser. I've got Mac Os and chrome, in my case I should press button with 3 dots -> More Tools -> Clear Browsing History -> check 1. cookies and .., 2 cached images..\nHope it helps\n. ",
    "grammakov": "Another issue - you might be trying to launch your Rails app on localhost in production mode but have your config.force_ssl set to true.. ",
    "Juksefantomet": "i would just like to add a commentary on this issue - to confirm that other things can produce this specific error. in my case i had to reboot my router due to a port hangup. My port was blocked due to my router messing up.\nIn production mode on a live site - it was live for awhile and suddenly could not be reached - logged displayed above error. fixed with reboot of router.. ",
    "0xtobit": "If somehow (and I'm not sure how I did) you find a \\r\\n in your header, this malformed request can trigger as well. I was trying to use Insomnia to reach out to a graphql endpoint on rails and my Content-type application/json header somehow got mucked. Removing and retyping it fixed my issue.\nFor others with this issue, I switched to WEBrick which, in this case, gave me more verbose error reporting.\nHope this helps someone.. ",
    "chtitux": "@nateberkopec Should I add some lines to my patch? Or everything is fine for now?. ",
    "frazboyz": "@nateberkopec They are all different apps/websites. Not all the same.\n. @nateberkopec puma website advertises 78 megabytes of memory usage but yea 145 aint bad.\n. @evanphx I looked at the production.log and there were no errors.. ",
    "alexandrecardinal": "I have the same issue. I can live with the error for now but I'll post a small project that reproduces this issue, probably this week. The error is coming from Puma.. ",
    "YannHulot": "Hi there. I am kind of new to this so I'll try to be accurate. I am currently working on an application and I saw an error like this in the logs:  #\nAfter careful investigation it looks like it happens after a request hits a controller with a method rendering \"nothing\". As such: \ndef create\n  Service.link(params[:user_id], params[:id], params[:action_name], params[:important])\n  render nothing: true, status: :ok\nend\nI know this method is deprecated in Rails 5 as per:\nhttp://stackoverflow.com/questions/34688726/the-nothing-option-is-deprecated-and-will-be-removed-in-rails-5-1\nBut the app I am working on hasn't been upgraded to rails 5 yet and our users do not seem to be impacted by the error. Therefore, I am debating just leaving it be for now but would appreciate a solution if possible. . Left a comment there a little while ago, just wondering if anyone had any ideas on how to solve this problem?. ",
    "nicoe": "I might be mistaken but I doubt this is #783 because the error is not a missing file.\n. ",
    "scifisamurai": "For anyone else that finds this thread like I did (via google) due to getting a similar error trying to install puma 2.16.0 for ruby 2.3.0 on Arch Linux.  \nDoing the following worked to install puma using openssl-1.0 (after you install openssl-1.0 via pacman)\ngem install puma -v '2.16.0' -- --with-cppflags=-I/usr/include/openssl-1.0  --with-ldflags=-L/usr/lib/openssl-1.0\nReferences:\n1. https://github.com/puma/puma/issues/971 (shows the syntax for passing cppflags & ldfags switches to gem install )\n2. https://wiki.archlinux.org/index.php/RVM (shows the cppflags & ldflags appropriate for openssl 1 on Arch Linux). ",
    "kinduff": "Method above from @scifisamurai works to compile puma v3.2.0 for ruby 2.3.0 on ArchLinux. . ",
    "artkirienko": "Works fine for an old project after bundle update :+1: . ",
    "dannyfallon": "This was fixed in #1165 and I think it can be closed. Here's the output in my local env running Puma and sending TTOU followed by 2xTTIN:\n```\n[4464] - Worker 1 (pid: 4470) booted, phase: 0\n[4464] - Worker 2 (pid: 4474) booted, phase: 0\n[4464] - Worker 3 (pid: 4478) booted, phase: 0\n[4464] - Worker 0 (pid: 4466) booted, phase: 0\n\n\nSend TTOU\n[4464] - Worker 3 (pid: 4478) terminating\nSend TTIN (twice)\n[4464] - Worker 3 (pid: 4577) booted, phase: 0\n[4464] - Worker 4 (pid: 4583) booted, phase: 0\n```\n. /cc @nateberkopec @schneems @evanphx \n\n\nWe encountered this issue in development and it slowed down requests on a four-worker, single-threaded setup. Adding to the impact it even caused deadlocks when the request made a request to the same API endpoint and that internal request was picked up by the same worker \ud83d\ude2cWe monkeypatched it to the old behaviour and it's been working the past few months without issue. If you wanna chat about this one I'm at RailsConf again this year :+1:\n\nTo debug this, I turned into a puts debugger and put loads of lines around the 3 operations - #<<, #wait_until_not_full and the request thread's code in #spawn_thread. The debug stuff lives in a different branch over here\nUsing the test case from @ivanpesin in this comment  I got this when everything worked normally:\n```\n[96338] Puma starting in cluster mode...\n[96338] * Version 3.11.4 (ruby 2.4.3-p205), codename: Love Song\n[96338] * Min threads: 1, max threads: 1\n[96338] * Environment: development\n[96338] * Process workers: 2\n[96338] * Phased restart available\n[96338] * Listening on tcp://0.0.0.0:4567\n[96338] Use Ctrl-C to stop\n[96338] - Worker 0 (pid: 96352) booted, phase: 0\n[1523990354.618174000, PID: 96352, TID 70345126232300] Request thread has the mutex\n[1523990354.618414000, PID: 96352, TID 70345126232300] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n[96338] - Worker 1 (pid: 96353) booted, phase: 0\n[1523990354.619208000, PID: 96353, TID 70345126330160] Request thread has the mutex\n[1523990354.619330000, PID: 96353, TID 70345126330160] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n<... send requests ...> \n[1523990364.892947000, PID: 96353, TID 70345126329660] Main process has the mutex for adding work\n[1523990364.893070000, PID: 96353, TID 70345126329660] Added work, todo is now 1\n[1523990364.893170000, PID: 96353, TID 70345126329660] Signalled not_empty\n[1523990364.893613000, PID: 96353, TID 70345126329660] wait_until_not_full called\n[1523990364.893731000, PID: 96353, TID 70345126330160] Request thread is awake after not_empty signal received, has the mutex\n[1523990364.893830000, PID: 96353, TID 70345126330160] Waiting decremented, now: 0\n[1523990364.893905000, PID: 96353, TID 70345126330160] Work taken from todo. Todo is now 0. Releasing mutex\n[1523990364.894161000, PID: 96353, TID 70345126329660] Main process has the mutex for wait_until_not_full\n[1523990364.894275000, PID: 96353, TID 70345126329660] Todo is 0\n[1523990364.894370000, PID: 96353, TID 70345126329660] Full, waiting\n[1523990365.908539000, PID: 96352, TID 70345115057660] Main process has the mutex for adding work\n[1523990365.908680000, PID: 96352, TID 70345115057660] Added work, todo is now 1\n[1523990365.908825000, PID: 96352, TID 70345115057660] Signalled not_empty\n[1523990365.908971000, PID: 96352, TID 70345115057660] wait_until_not_full called\n[1523990365.909109000, PID: 96352, TID 70345126232300] Request thread is awake after not_empty signal received, has the mutex\n[1523990365.909216000, PID: 96352, TID 70345126232300] Waiting decremented, now: 0\n[1523990365.909311000, PID: 96352, TID 70345126232300] Work taken from todo. Todo is now 0. Releasing mutex\n[1523990365.909444000, PID: 96352, TID 70345115057660] Main process has the mutex for wait_until_not_full\n[1523990365.909668000, PID: 96352, TID 70345115057660] Todo is 0\n[1523990365.909769000, PID: 96352, TID 70345115057660] Full, waiting\n127.0.0.1 - - [17/Apr/2018:14:39:30 -0400] \"GET /sleep/5 HTTP/1.1\" 200 24 5.0109\n[96352] 127.0.0.1 - - [17/Apr/2018:14:39:30 -0400] \"GET /sleep/5 HTTP/1.1\" 200 24 5.0188\n[1523990370.928931000, PID: 96352, TID 70345126232300] Request thread has the mutex\n[1523990370.929086000, PID: 96352, TID 70345126232300] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n[1523990370.929289000, PID: 96352, TID 70345115057660] Main process is awake after not_full signal received, has the mutex\n[1523990370.929413000, PID: 96352, TID 70345115057660] Todo is 0\n[1523990370.929536000, PID: 96352, TID 70345115057660] Waiting count is 1, returning\n[1523990370.929738000, PID: 96352, TID 70345115057660] Main process has the mutex for adding work\n[1523990370.929847000, PID: 96352, TID 70345115057660] Added work, todo is now 1\n[1523990370.929973000, PID: 96352, TID 70345115057660] Signalled not_empty\n[1523990370.930132000, PID: 96352, TID 70345115057660] wait_until_not_full called\n[1523990370.930344000, PID: 96352, TID 70345115057660] Main process has the mutex for wait_until_not_full\n[1523990370.930512000, PID: 96352, TID 70345115057660] Todo is 1\n[1523990370.930659000, PID: 96352, TID 70345115057660] Waiting count is 1, returning\n[1523990370.930869000, PID: 96352, TID 70345126232300] Request thread is awake after not_empty signal received, has the mutex\n[1523990370.931026000, PID: 96352, TID 70345126232300] Waiting decremented, now: 0\n[1523990370.931181000, PID: 96352, TID 70345126232300] Work taken from todo. Todo is now 0. Releasing mutex\n127.0.0.1 - - [17/Apr/2018:14:39:32 -0400] \"GET /sleep/2 HTTP/1.1\" 200 24 2.0031\n[96352] 127.0.0.1 - - [17/Apr/2018:14:39:32 -0400] \"GET /sleep/2 HTTP/1.1\" 200 24 2.0038\n[1523990372.935631000, PID: 96352, TID 70345126232300] Request thread has the mutex\n[1523990372.935878000, PID: 96352, TID 70345126232300] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n127.0.0.1 - - [17/Apr/2018:14:39:34 -0400] \"GET /sleep/10 HTTP/1.1\" 200 25 10.0065\n[96353] 127.0.0.1 - - [17/Apr/2018:14:39:34 -0400] \"GET /sleep/10 HTTP/1.1\" 200 25 10.0146\n[1523990374.909421000, PID: 96353, TID 70345126330160] Request thread has the mutex\n[1523990374.909596000, PID: 96353, TID 70345126330160] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n[1523990374.909753000, PID: 96353, TID 70345126329660] Main process is awake after not_full signal received, has the mutex\n[1523990374.909898000, PID: 96353, TID 70345126329660] Todo is 0\n[1523990374.910038000, PID: 96353, TID 70345126329660] Waiting count is 1, returning\n```\nAnd then I ran the same producer of requests again and I encountered the bug. One of the worker processes took the first two requests and handed the third one to the second process \ud83e\udd14 Here's the different output:\n[1523990379.791620000, PID: 96353, TID 70345126329660] Main process has the mutex for adding work\n[1523990379.791774000, PID: 96353, TID 70345126329660] Added work, todo is now 1\n[1523990379.791902000, PID: 96353, TID 70345126329660] Signalled not_empty\n[1523990379.792004000, PID: 96353, TID 70345126329660] wait_until_not_full called\n[1523990379.792306000, PID: 96353, TID 70345126329660] Main process has the mutex for wait_until_not_full\n[1523990379.792582000, PID: 96353, TID 70345126329660] Todo is 1\n[1523990379.792886000, PID: 96353, TID 70345126329660] Waiting count is 1, returning\n[1523990379.793293000, PID: 96353, TID 70345126330160] Request thread is awake after not_empty signal received, has the mutex\n[1523990379.793583000, PID: 96353, TID 70345126330160] Waiting decremented, now: 0\n[1523990379.793861000, PID: 96353, TID 70345126330160] Work taken from todo. Todo is now 0. Releasing mutex\n[1523990380.806674000, PID: 96353, TID 70345126329660] Main process has the mutex for adding work\n[1523990380.806799000, PID: 96353, TID 70345126329660] Added work, todo is now 1\n[1523990380.806897000, PID: 96353, TID 70345126329660] Signalled not_empty\n[1523990380.806992000, PID: 96353, TID 70345126329660] wait_until_not_full called\n[1523990380.807089000, PID: 96353, TID 70345126329660] Main process has the mutex for wait_until_not_full\n[1523990380.807188000, PID: 96353, TID 70345126329660] Todo is 1\n[1523990380.807288000, PID: 96353, TID 70345126329660] Full, waiting\n[1523990381.815731000, PID: 96352, TID 70345115057660] Main process has the mutex for adding work\n[1523990381.815854000, PID: 96352, TID 70345115057660] Added work, todo is now 1\n[1523990381.815944000, PID: 96352, TID 70345115057660] Signalled not_empty\n[1523990381.816142000, PID: 96352, TID 70345115057660] wait_until_not_full called\n[1523990381.816246000, PID: 96352, TID 70345126232300] Request thread is awake after not_empty signal received, has the mutex\n[1523990381.816399000, PID: 96352, TID 70345126232300] Waiting decremented, now: 0\n[1523990381.816499000, PID: 96352, TID 70345126232300] Work taken from todo. Todo is now 0. Releasing mutex\n[1523990381.816655000, PID: 96352, TID 70345115057660] Main process has the mutex for wait_until_not_full\n[1523990381.817067000, PID: 96352, TID 70345115057660] Todo is 0\n[1523990381.817182000, PID: 96352, TID 70345115057660] Full, waiting\n127.0.0.1 - - [17/Apr/2018:14:39:43 -0400] \"GET /sleep/2 HTTP/1.1\" 200 24 2.0017\n[96352] 127.0.0.1 - - [17/Apr/2018:14:39:43 -0400] \"GET /sleep/2 HTTP/1.1\" 200 24 2.0023\n[1523990383.819630000, PID: 96352, TID 70345126232300] Request thread has the mutex\n[1523990383.819893000, PID: 96352, TID 70345126232300] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n[1523990383.820209000, PID: 96352, TID 70345115057660] Main process is awake after not_full signal received, has the mutex\n[1523990383.820488000, PID: 96352, TID 70345115057660] Todo is 0\n[1523990383.820707000, PID: 96352, TID 70345115057660] Waiting count is 1, returning\n127.0.0.1 - - [17/Apr/2018:14:39:49 -0400] \"GET /sleep/10 HTTP/1.1\" 200 25 10.0024\n[96353] 127.0.0.1 - - [17/Apr/2018:14:39:49 -0400] \"GET /sleep/10 HTTP/1.1\" 200 25 10.0029\n[1523990389.797607000, PID: 96353, TID 70345126330160] Request thread has the mutex\n[1523990389.797875000, PID: 96353, TID 70345126330160] Work taken from todo. Todo is now 0. Releasing mutex\n127.0.0.1 - - [17/Apr/2018:14:39:54 -0400] \"GET /sleep/5 HTTP/1.1\" 200 24 5.0010\n[96353] 127.0.0.1 - - [17/Apr/2018:14:39:54 -0400] \"GET /sleep/5 HTTP/1.1\" 200 24 5.0015\n[1523990394.800282000, PID: 96353, TID 70345126330160] Request thread has the mutex\n[1523990394.800630000, PID: 96353, TID 70345126330160] Waiting incremented, now: 1. Signalling not_full, releasing mutex on not_empty\n[1523990394.800954000, PID: 96353, TID 70345126329660] Main process is awake after not_full signal received, has the mutex\n[1523990394.801151000, PID: 96353, TID 70345126329660] Todo is 0\n[1523990394.801352000, PID: 96353, TID 70345126329660] Waiting count is 1, returning\nThe difference here is that we ran wait_until_not_full before the request thread woke up and got the mutex:\n[1523990379.791620000, PID: 96353, TID 70345126329660] Main process has the mutex for adding work\n[1523990379.791774000, PID: 96353, TID 70345126329660] Added work, todo is now 1\n[1523990379.791902000, PID: 96353, TID 70345126329660] Signalled not_empty\n[1523990379.792004000, PID: 96353, TID 70345126329660] wait_until_not_full called\n[1523990379.792306000, PID: 96353, TID 70345126329660] Main process has the mutex for wait_until_not_full\n[1523990379.792582000, PID: 96353, TID 70345126329660] Todo is 1\n[1523990379.792886000, PID: 96353, TID 70345126329660] Waiting count is 1, returning\n[1523990379.793293000, PID: 96353, TID 70345126330160] Request thread is awake after not_empty signal received, has the mutex\n. :wave: This should have a tiny effect on the backlog size, and mainly in a positive way - it should become more accurate. When a running process falls foul of this behaviour the backlog size will be > 0 despite all threads (one in this example) being busy processing work (@waiting = 0)\nWe don't want to take requests off the socket and put them into the thread pool unless we have capacity in the pool but because of the mutex sequencing in this race condition, caused by threads not waking up fast enough/before the server enters #wait_until_not_full this absolutely happens. \nSo in a single-threaded worker with a naive request shape (i.e. the request is ready to be served and doesn't get put into the reactor after being added to the pool) I expect that we'll never have @waiting > 0 and backlog > 0 which I believe was the goal of the original PR :+1:\n. > It looks like #1471 provides a test case for this behavior. Can you confirm the issue is gone with this patch?\nThe owner of that issue confirms this PR fixes it in the above comment :+1:\n\nAhh, so if there is a free thread to process requests, there shouldn't ever be a request waiting in the backlog. Makes sense.\n\nYep, and when the thread pool is full with no available threads the backlog builds on the socket not in the process.\n\n'm currently using backlog as a proxy for \"are we keeping up with the work\". It sounds like this won't impact that?\n\nJust for clarity here we're speaking about Puma's backlog metric, obtained via pumactl stats for example? If that's correct then right now this metric is possibly giving you bad data because the backlog can be > 0 while all threads are actually busy because the server process took too much work off the socket; this shouldn't happen anymore thanks to this PR.\nYou should continue to monitor the backlog stat because the server process can still end up with a backlog: for example if a the server takes requests off the socket that weren't fully ready they end up in the Reactor and then the request threads free up and take more work off the socket and are busy. Meanwhile, the reactor-based requests became ready and end up in the backlog - now you're not keeping up with the work. This is documented in #1453.\nAs a bit of a tangent on backlog monitoring I think if you want to know if you're keeping up with the work you also need to monitor the socket's backlog - if the socket backlog grows you're under capacity, not just when the threadpool(s) have backlogs.. Requests spinning in the Reactor until they're ready don't affect or inflate the threadpool's backlog. It's my understanding that when the pool thread tries to perform work on the client the client throws an exception which is caught and examined to see if the client needs to go into the Reactor at that point, freeing up the pool's thread to do work on other clients. \nAbsolutely agree with you on slow clients but I don't believe that is something that the original #1278 PR nor this one is trying to solve?\nUnless I'm mistaken this PR doesn't affect any of these client/slow-client interactions and I'm just trying to restore the thread pool's full capacity check to the one that was in Puma < 3.9.0 because, due to the way Puma puts request threads that are waiting for work to sleep, we can accidentally suck up more requests from the server than we can process after the thread(s) are woken up. \n. \ud83d\ude4c Thanks for not leaving this one in purgatory @schneems and for documenting all the gnarly bits you didn't understand while you got enough context - I wish I'd done it myself after my own voyage of discovery too \ud83d\ude04 . >Unfortunately I don't know how to dig deeper to figure out where the \"doubled\" version is coming from\nIt comes from here, set via this and lives in Ragel here and here\nAs part of the HTTP spec every client-provided header gets transformed to uppercase, has - replaces with _ and gets the prefix of HTTP_ added. In the case of a Version header that'll transform it to HTTP_VERSION.\nThe problem is we're already setting HTTP_VERSION to HTTP/1.1 in Puma (via the C/Java extensions) we fall into the RFC's behaviour for when you set the same header more than once: We add the client header value to create HTTP/1.1, HTTP/1.1. You could send a request with Version:Blabbedy and get HTTP_VERSION: HTTP/1.1, Blabbedy \ud83d\ude2c \nThough it's not in the Rack spec, this is also a problem in Rack itself (https://github.com/rack/rack/issues/970). We can fix Puma and we can open PRs on Thin/Unicorn too but I'd love some thoughts from the maintainers of Puma on it before I do. ",
    "andrew": "This change just caught me out too, had port ENV.fetch(\"PORT\") { 3000 } in config/puma.rb but using the following command in production:\nbundle exec puma -C config/puma.rb -p 5000\n\nWhen I deployed the upgrade to 3.6.1 today, it switched ports from 5000 to 3000, I'd guess because the -p option is no longer as important as the value in config/puma.rb but that's definitely a breaking change.\n\n. @nateberkopec sounds good \ud83d\udc4d . A similar thing happened back in November: https://github.com/puma/puma/issues/1154. ",
    "twiduch": "My use case is Ruby app, true. But it is Puma specific. I need to close all connections. \nTried to find any class method for that, but no luck.\nIf that is not implemented than maybe that feature should be added. Anybody using fork can face this problem.. ",
    "BigGillyStyle": "This is my first time in a long while posting an issue on an open source project, so let me know how I could be of more help.. I apologize.  After several hours of debugging this on my Ubuntu 14.04 box, I discovered that there were running Puma processes while I updated the Puma gem.  Once I stopped all those processes, updated Puma to 3.6.2, and then re-started them...there were no problems.. ",
    "sj26": "Oh, nice catch. What a typo!. Yes!!! Thank you!!!. Yesss, thank you!\nNow we can publish a \"Utilization\" metric for sensible autoscaling, at least in the N:N configuration. :100:. ",
    "zouqilin": "\n. is there any update? @nateberkopec . \n\njdk1.8.0_60\njruby-9.1.10.0(rvm)\npuma-3.8.2\ndebian(x86-64)\n@nateberkopec  failed again.. @perlun it did. but I found process id not changed.\n\n. if code got reloaded, maybe it is ok.. ",
    "kajisaap": "Confirming @zouqilin 's observation. \n\n\ncode seems to get reloaded as the proxy started throwing connect failed after sending USR2 signal. \n\n\nPID did not change.\n\n\n```shell\n:> cat .ruby-version\njruby-9.1.16.0\n:>puma --version\npuma version 3.7.1\n:>lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.3 LTS\nRelease:    16.04\nCodename:   xenial\n```\nIs this an expected behaviour? . ",
    "etrejos": "@steakknife Have you tried upgrading to 3.7.0?. ",
    "cybercode": "With LibreSSL, on alpine:3.5, puma 3.7+ compiles but fails when trying to call DH_set0_pqg\nNote that it will compile and run fine if DISABLE_SSL is set at build time (for obvious reasons)\nThe affected code (from mini_ssl.c)\n``` C\nif OPENSSL_VERSION_NUMBER < 0x10100005L\ndh->p = BN_bin2bn(dh1024_p, sizeof(dh1024_p), NULL);\n  dh->g = BN_bin2bn(dh1024_g, sizeof(dh1024_g), NULL);\nif ((dh->p == NULL) || (dh->g == NULL)) {\n    DH_free(dh);\n    return NULL;\n  }\nelse\nBIGNUM p, g;\n  p = BN_bin2bn(dh1024_p, sizeof(dh1024_p), NULL);\n  g = BN_bin2bn(dh1024_g, sizeof(dh1024_g), NULL);\nif (p == NULL || g == NULL || !DH_set0_pqg(dh, p, NULL, g)) {\n    DH_free(dh);\n    BN_free(p);\n    BN_free(g);\n    return NULL;\n  }\nendif\n```\nA possible solution:\nC\n6   #define LIBRESSL_VERSION_NUMBER 0x2050200fL\n7   #define LIBRESSL_VERSION_TEXT   \"LibreSSL 2.5.2\"\n8   \n9   /* These will never change */\n10  #define OPENSSL_VERSION_NUMBER  0x20000000L\n11  #define OPENSSL_VERSION_TEXT    LIBRESSL_VERSION_TEXT. ",
    "Martin288": "Meet same problem with OpenSSL 1.1.0e and Puma 3.8.0, using Ruby 2.4.0.\n```\n$ gem install puma -v 3.8.0\nBuilding native extensions.  This could take a while...\nERROR:  Error installing puma:\n        ERROR: Failed to build gem native extension.\ncurrent directory: /var/www/site/.rvm/gems/ruby-2.4.0/gems/puma-3.8.0/ext/puma_http11\n\n/var/www/site/.rvm/rubies/ruby-2.4.0/bin/ruby -r ./siteconf20170313-13337-26j4we.rb extconf.rb\nchecking for BIO_read() in -lcrypto... yes\nchecking for SSL_CTX_new() in -lssl... yes\nchecking for openssl/bio.h... yes\ncreating Makefile\ncurrent directory: /var/www/site/.rvm/gems/ruby-2.4.0/gems/puma-3.8.0/ext/puma_http11\nmake \"DESTDIR=\" clean\ncurrent directory: /var/www/site/.rvm/gems/ruby-2.4.0/gems/puma-3.8.0/ext/puma_http11\nmake \"DESTDIR=\"\ncompiling http11_parser.c\nIn file included from ext/puma_http11/http11_parser.rl:8:0:\next/puma_http11/http11_parser.rl: In function \u2018puma_parser_execute\u2019:\next/puma_http11/http11_parser.rl:111:17: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\next/puma_http11/http11_parser.rl: At top level:\ncc1: warning: unrecognized command line option \"-Wno-self-assign\" [enabled by default]\ncc1: warning: unrecognized command line option \"-Wno-constant-logical-operand\" [enabled by default]\ncc1: warning: unrecognized command line option \"-Wno-parentheses-equality\" [enabled by default]\ncc1: warning: unrecognized command line option \"-Wno-tautological-compare\" [enabled by default]\ncompiling io_buffer.c\ncompiling mini_ssl.c\nIn file included from mini_ssl.c:14:0:\n/usr/include/openssl/bio.h: In function \u2018DEPRECATEDIN_1_1_0\u2019:\n/usr/include/openssl/bio.h:671:1: error: expected declaration specifiers before \u2018DEPRECATEDIN_1_1_0\u2019\n DEPRECATEDIN_1_1_0(int BIO_get_port(const char str, unsigned short port_ptr))\n ^\n/usr/include/openssl/bio.h:678:2: error: expected declaration specifiers before \u2018;\u2019 token\n };\n  ^\nIn file included from /usr/include/openssl/asn1.h:24:0,\n                 from /usr/include/openssl/objects.h:916,\n                 from /usr/include/openssl/evp.h:27,\n                 from /usr/include/openssl/x509.h:23,\n...\n```\nI tried to use the following command, and it works! Thanks @cybercode \nsh\nDISABLE_SSL=true gem install puma -v 3.8.0\n. @x-yuri right, will try your way later.. @grosser thanks. but I don't know if there is any side effect that unsetting bundler ENV vars, have you tried it?. @grosser great! Will use it soon. But I still hope it can be solved from the puma repository, however, your solution is a good temporarily solution.. ",
    "irminsul": "Same here with alpine:3.5, MRI 2.3.3, OpenSSL 1.0.2k:\nLoadError: Error relocating /app/vendor/ruby/2.3.0/gems/puma-3.8.1/lib/puma/puma_http11.so: DH_set0_pqg: symbol not found - /app/vendor/ruby/2.3.0/gems/puma-3.8.1/lib/puma/puma_http11.so\n. ",
    "ayghor": "i can build but cant run: puma_http11.so: undefined symbol: DH_set0_pqg\nmy stuff:\n * puma (3.8.2)\n * libressl-2.4.5\n * ruby-2.4.1. see https://github.com/puma/puma/commit/42bec4600c51ab8a1c1ee5a0e1b738a4ffd82bf2, it was tagged v3.6.2 and was NOT merged on v3.7.0 for some reason:\n\nRevert \"fix access priorities of each level in LeveledOptions\"\n\nThis reverts commit 15f0847289d0815284f87eeea4625100b581c026.\n\nSee #1154, this commit unintentionally changed behavior regarding the\nimportance of command line options.\n\n\ncant test reverting teh reversion right now. v3.2.6 respect -p switch, v3.7.0 does not.. > @ayghor Do you mean 3.6.2? Since 3.2.6 doesn't exist.\n\nAnd with 3.6.2 it works fine.\n\nyes, 3.6.2. this was already fixed on v3.6.2 and not merged on v3.7.0 for some reason.. see #1200. oops sry for that <3. ",
    "karianne": "I have the same problem as OP.. ",
    "qmmp123": "I can install but can not run server symbol lookup error: /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.0/lib/puma/puma_http11.so: undefined symbol: OPENSSL_init_ssl. ",
    "Luclu7": "\nI can install but can not run server symbol lookup error: /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.0/lib/puma/puma_http11.so: undefined symbol: OPENSSL_init_ssl\n\nSame, but on a similar server, no problem, strange.. ",
    "denissellu": "installing 3.7.1 fixed the issue for me\ngem 'puma', '~> 3.7.1'\n. ",
    "rishka": "1285 Fixes this for me.",
    "warmwaffles": "I am still running into this issue.\nweb_1       | => Booting Puma\nweb_1       | => Rails 5.1.1 application starting in development on http://0.0.0.0:3000\nweb_1       | => Run `rails server -h` for more startup options\nweb_1       | Exiting\nweb_1       | /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require': Error relocating /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/puma_http11.so: DH_set0_pqg: symbol not found - /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/puma_http11.so (LoadError)\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `block in require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:258:in `load_dependency'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/server.rb:15:in `<top (required)>'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `block in require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:258:in `load_dependency'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/runner.rb:1:in `<top (required)>'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `block in require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:258:in `load_dependency'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/cluster.rb:1:in `<top (required)>'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `block in require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:258:in `load_dependency'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/puma/launcher.rb:4:in `<top (required)>'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `block in require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:258:in `load_dependency'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/activesupport-5.1.1/lib/active_support/dependencies.rb:292:in `require'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/rack/handler/puma.rb:14:in `config'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/puma-3.8.2/lib/rack/handler/puma.rb:56:in `run'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/rack-2.0.3/lib/rack/server.rb:297:in `start'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/commands/server/server_command.rb:44:in `start'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/commands/server/server_command.rb:131:in `block in perform'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/commands/server/server_command.rb:126:in `tap'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/commands/server/server_command.rb:126:in `perform'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/thor-0.19.4/lib/thor/command.rb:27:in `run'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/thor-0.19.4/lib/thor/invocation.rb:126:in `invoke_command'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/thor-0.19.4/lib/thor.rb:369:in `dispatch'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/command/base.rb:63:in `perform'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/command.rb:44:in `invoke'\nweb_1       |   from /usr/lib/ruby/gems/2.4.0/gems/railties-5.1.1/lib/rails/commands.rb:16:in `<top (required)>'\nweb_1       |   from bin/rails:4:in `require'\nweb_1       |   from bin/rails:4:in `<main>'\nBase docker image: https://github.com/simplecasual/ruby-docker/blob/master/Dockerfile\nApplication docker: https://github.com/simplecasual/ruby.fm/blob/master/Dockerfile\nI can not use openssl packages because postgres 9.6 depends on libressl.\nERROR: unsatisfiable constraints:\n  libressl-dev-2.5.4-r0:\n    conflicts:\n               openssl-dev-1.0.2k-r0[pc:libcrypto=2.5.4]\n               openssl-dev-1.0.2k-r0[pc:libssl=2.5.4]\n               openssl-dev-1.0.2k-r0[pc:openssl=2.5.4]\n    satisfies: world[libressl-dev]\n               postgresql-dev-9.6.2-r4[libressl-dev]\n  openssl-dev-1.0.2k-r0:\n    conflicts:\n               libressl-dev-2.5.4-r0[pc:libcrypto=1.0.2k]\n               libressl-dev-2.5.4-r0[pc:libssl=1.0.2k]\n               libressl-dev-2.5.4-r0[pc:openssl=1.0.2k]\n    satisfies: world[openssl-dev]\n. Sure, https://github.com/puma/puma/issues/1301. That certainly fixed it @romdim. Downgraded for now.. @nateberkopec which release was it fixed under? 3.9.0?. Can confirm that it indeed works. Thanks guys!. ",
    "brunowego": "Using bind instead of ssl_bind, works.\nrb\nssl_key = File.expand_path('../certificates/server.key', __FILE__)\nssl_cert = File.expand_path('../certificates/server.crt', __FILE__)\nbind \"ssl://127.0.0.1:3000?key=#{ssl_key}&cert=#{ssl_cert}\". Similar issue reported at https://github.com/rails/rails/issues/27625. The issue is in Rails 5.0.1, reported here https://github.com/rails/rails/issues/27421. I solved with downgrade to version of Rails 5.0.0.1.. ",
    "Scong": "I am seeing the same issues when trying to start to start puma for Capybara with SSL. With these attempts.\nRack::Handler::Puma.run(app, Host: host, Port: port, Threads: \"0:4\") do |server|\n      server.config.options.instance_variable_get(:@set)[1][:binds] = ['ssl://localhost:3002?key=./spec/support/server.key&cert=./spec/support/server.crt']\n   #Definitely not the right way to set a binding\n    end\nand\nPuma::Server.new(app).tap do |server|\n    context = Puma::MiniSSL::Context.new\n    context.key         = \"./spec/support/server.key\"\n    context.cert        = \"./spec/support/server.crt\"\n    context.verify_mode = Puma::MiniSSL::VERIFY_NONE\n    server.add_ssl_listener(Capybara.server_host, port, context)\n    #Maybe a more reasonable way\nend.run.join\nListen loop error: #<NoMethodError: undefined method `check' for true:TrueClass>\n/Users/scong/.rvm/gems/ruby-2.3.1/gems/puma-3.6.2/lib/puma/minissl.rb:209:in `accept_nonblock'\n/Users/scong/.rvm/gems/ruby-2.3.1/gems/puma-3.6.2/lib/puma/server.rb:332:in `block in handle_servers'\n/Users/scong/.rvm/gems/ruby-2.3.1/gems/puma-3.6.2/lib/puma/server.rb:327:in `each'\n/Users/scong/.rvm/gems/ruby-2.3.1/gems/puma-3.6.2/lib/puma/server.rb:327:in `handle_servers'\n/Users/scong/.rvm/gems/ruby-2.3.1/gems/puma-3.6.2/lib/puma/server.rb:300:in `block in run'. ",
    "shayonj": "Recently came across this, looks solvable, might require some tweaks to the suggested solution, to support graceful stop (will attempt at getting a PR).\nWhile at it, I also realized, there is no trapping of SIGINT, which means ctrl + c, doesn't do a graceful stop. Is that intentional?. I have proposed a PR for TERM events. I think INT is a little tricky, since we either ignore, swallow or rescue (via Interrupt which is what ruby throws for  ctrl + c) it in a number of places. To keep things simple, I have left it out the PR. I will try to take a deeper look into it and see if to what extent its solvable without issuing too much re-structure :). Happy to close the proposed PR, if we think it should all go out as a single PR.. @dekellum Thanks for the review!\nI believe both SIGINT and SIGTERM are already part of existing success exit handlers for SuccessExitStatus\n\nTakes a list of exit status definitions that, when returned by the main service process, will be considered successful termination, in addition to the normal successful exit code 0 and the signals SIGHUP, SIGINT, SIGTERM, and SIGPIPE. \n\nSource: https://www.freedesktop.org/software/systemd/man/systemd.service.html\nSo, for instance, if we were to run a systemd service, with Restart= with on-success, a TERM signal will be interpreted as a success and a proper restart would be conducted by the systemd service.\nExisting implementations still wont be affected, because the exit status being returned until this PR was 0, which is also considered success exit in this case.\nLet me know if I missed something :) and/or if you think it might be worth adding a note about success exit handlers in https://github.com/puma/puma/blob/master/docs/systemd.md ?. Thanks so much for such a thorough review @dekellum \ud83c\udf89 .\nYes, I have puma running using systemd config on my VPS, now that I am thinking, I should have perhaps shared the config here :). And yes MRI ruby.\nAnother quick way to test SuccessExitStatus requirements, would be to fire up a miniature systemd config, pointing to a bash script that is just sleep. You can kill that process with -15 and would see the on-success behavior accordingly.\nLastly, exit(143) is different than kill(\"TERM\"). exit more or less means a program terminated abnormally (and the status code is only for the terminal), where as kill allows to shutdown a process gracefully (if any).\n. TL;DR on the above: No additional changes to the existing implementation was introduced and TERM signals return the exit status code correctly, plus the graceful stop behavior on single mode.. @stereobooster do you mean tests for making sure the right exit status code is returned? I suppose, verifying the exit status of a server (or process) exit from within an existing process (test suite) could be tricky ($? would return the current process) and probably why there aren't existing specs/tests. \nI am going to attempt it and also look into any other additional way to expect/assert the trapping of Signal\n. @stereobooster yes I am going for integration tests. I was looking for a way to working with sub process. I have added some integration tests, they pass locally for me, but appears to be failing in TravisCI. Looks like re-raising/killing the process might be causing an exception to be raised. I will follow up on this soon. Also, the teardown isn't working as expected.. I have updated the integration specs (took a few retries :)).Sub-process killing was showing some weird/inconsistent behaviors in TravisCI's shell environment.\nIntegration tests now performs fork + exec of a controlled server and kills the respective pid accordingly.   . r? @schneems (I see you worked on the launcher migration) or  @nateberkopec || @stereobooster\n\ud83d\ude42 . TIL, we can re-raise the same signal exception instead of restoring signal and killing the process.\nUpdated \ud83d\ude42 . Even the proposed solution above could work, proposed trapping of SIGINT as part of setting up signals in Puma's Launcher class at: https://github.com/puma/puma/pull/1377. hi @nateberkopec, I can still reproduce this issue on master (3.10.0). this is solved in https://github.com/puma/puma/pull/1464. @nateberkopec @MSP-Greg is this good to close (and so the mentioned issue)? Looks the changes were rolled into a single PR. :). not at all @MSP-Greg. Thanks all!. Nice one @dekellum! I was able to replicate the issue using bundle exec puma and sending a TERM signal to the server. . fixed. ",
    "iGEL": "@nateberkopec @schneems I guess, this issue can be closed since #1337 was merged.. ",
    "vitobotta": "I have updated Rails to 5.0.1 but still the same. Very annoying :(\nAs said it's a new app but the only \"non default\" thing I have is a simple middleware for batch requests, see code below.\n```ruby\nclass BatchRequests\n  def initialize(app)\n    @app = app\n  end\ndef call(env)\n    if env[\"PATH_INFO\"] == \"/api/batch\"\n      request = ActionDispatch::Request.new(env.deep_dup)\n      responses = JSON.parse(request[:requests]).map do |override|\n        process_request(env.deep_dup, override)\n      end\n      [200, {\"Content-Type\" => \"application/json\"}, [{responses: responses}.to_json]]\n    else   \n      @app.call(env)\n    end\n  end\nprivate\ndef process_request(env, override)\n  path, query = override[\"url\"].split(\"?\")\n\n  env[\"REQUEST_METHOD\"] = override[\"method\"]\n  env[\"PATH_INFO\"] = path\n  env[\"QUERY_STRING\"] = query\n  env[\"rack.input\"] = StringIO.new(override[\"body\"].to_s)\n\n  status, headers, body = @app.call(env)\n\n  new_body = \"\"\n  body.each { |line| new_body << line }\n  body.close if body.respond_to?(:close)\n\n  headers['Content-Length'] = new_body.length.to_s\n\n  {status: status, headers: headers, body: new_body}\nend\n\nend\n```. Just tried with Passenger, and also with it I see no problems. So far it happens with Puma only.. ",
    "brateq": "Same problem here. Stage server hangs after couple minutes of idle time. Any ideas?\nEDIT: Update to 3.7.0 fix the problem.. ",
    "allcentury": "3.7.0 didn't fix it for me, I'm still running into issues.  Here's an example of rebooting the app but nothing is served on the GET requests:\n``\n\u25b6 be rails s\n/Users/anthonyross/.gem/ruby/2.4.0/gems/activesupport-5.0.1/lib/active_support/xml_mini.rb:51: warning: constant ::Fixnum is deprecated\n/Users/anthonyross/.gem/ruby/2.4.0/gems/activesupport-5.0.1/lib/active_support/xml_mini.rb:52: warning: constant ::Bignum is deprecated\n=> Booting Puma\n=> Rails 5.0.1 application starting in development on http://localhost:3000\n=> Runrails server -h` for more startup options\n/Users/anthonyross/.gem/ruby/2.4.0/gems/activesupport-5.0.1/lib/active_support/core_ext/numeric/conversions.rb:138: warning: constant ::Fixnum is deprecated\nPuma starting in single mode...\n Version 3.7.0 (ruby 2.4.0-p0), codename: Snowy Sagebrush\n Min threads: 5, max threads: 5\n Environment: development\n Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\nStarted GET \"/api/me\" for 127.0.0.1 at 2017-01-23 15:20:32 -0500\nStarted GET \"/api/transactions\" for 127.0.0.1 at 2017-01-23 15:20:32 -0500\nStarted GET \"/api/categories\" for 127.0.0.1 at 2017-01-23 15:20:33 -0500\nStarted GET \"/api/payees\" for 127.0.0.1 at 2017-01-23 15:20:33 -0500\n  ActiveRecord::SchemaMigration Load (0.5ms)  SELECT \"schema_migrations\". FROM \"schema_migrations\"\n  ActiveRecord::SchemaMigration Load (2.2ms)  SELECT \"schema_migrations\". FROM \"schema_migrations\"\n  ActiveRecord::SchemaMigration Load (2.3ms)  SELECT \"schema_migrations\". FROM \"schema_migrations\"\n  ActiveRecord::SchemaMigration Load (2.7ms)  SELECT \"schema_migrations\". FROM \"schema_migrations\"\nProcessing by Api::UsersController#me as JSON\n  Parameters: {\"user\"=>{}}\n```\n4 GET requests started but all hanging.  I never get a timeout and no backtrace when I cancel, I just see:\nCompleted   in 248554ms (ActiveRecord: 3.4ms)\nNothing weird in my gemfile AFAIK:\n```\nsource 'https://rubygems.org'\nruby '2.4.0'\nBundle edge Rails instead: gem 'rails', github: 'rails/rails'\ngem 'rails', '~> 5.0.1'\nUse postgresql as the database for Active Record\ngem 'pg', '~> 0.18'\nUse Puma as the app server\ngem 'puma', github: 'puma/puma', branch: '3.7.0'\ngem 'jbuilder', github: 'rails/jbuilder'\ngem 'knock', '~> 2.0'\nUse Redis adapter to run Action Cable in production\ngem 'redis', '~> 3.0'\nUse ActiveModel has_secure_password\ngem 'bcrypt', '~> 3.1.7'\nUse Capistrano for deployment\ngem 'capistrano-rails', group: :development\nUse Rack CORS for handling Cross-Origin Resource Sharing (CORS), making cross-origin AJAX possible\ngem 'rack-cors'\ngem 'money-rails', '~> 1.7'\ngroup :development, :test do\n  # Call 'byebug' anywhere in the code to stop execution and get a debugger console\n  gem 'byebug', platform: :mri\nend\ngroup :development do\n  gem 'listen', '~> 3.0.5'\n  # Spring speeds up development by keeping your application running in the background. Read more: https://github.com/rails/spring\n  gem 'spring'\n  gem 'spring-watcher-listen', '~> 2.0.0'\nend\nWindows does not include zoneinfo files, so bundle the tzinfo-data gem\ngem 'tzinfo-data', platforms: [:mingw, :mswin, :x64_mingw, :jruby]\n```\nNo pry's, no binding's or byebugs.  \nEDIT\nAfter digging into this for the last few hours, I found the same issue on rails https://github.com/rails/rails/issues/27455.  It has to do with Puma's concurrency on app boot which causes a deadlock on ruby 2.4.  \nThe quickfix is to set config.eager_load = true in your respective environment OR boot puma with only one thread.. @GabKlein if you put Puma w/ one thread do you still see the issue? . Appears to be the same as #1275 ?. ",
    "dteoh": "I started experiencing this problem after updating to macOS 10.12.4. I am running Rails 5.0.2 with Ruby 2.3.1p112. I was initially running puma 3.6.2 + mysql2 0.4.4. Updating to 3.8.2 did not help. Also upgraded mysql2 gem to 0.4.5 but it did not help. I am using the default puma config that ships with Rails.\nI tried debugging with lldb and dtruss, and can only see that a lot of threads are getting psynch_cvwait = -1 Err#316. Trying rb_backtrace was useless, either lldb gets stuck or I get:\n(lldb) expression (void) rb_backtrace()\n    from /Users/dteoh/.rbenv/versions/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.8.2/lib/puma/thread_pool.rb:236:in `block in start!'\n    from /Users/dteoh/.rbenv/versions/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.8.2/lib/puma/thread_pool.rb:236:in `sleep'\nMy only workaround for now is to use the multiprocess mode with single threading.. Okay, I finally got some hopefully useful backtrace from a crash when investigating through lldb. I removed the listen gem that shipped with Rails before doing this round of investigations, because that gem was causing me a lot of out of resource errors.\nI walked through each thread in the list, printing its backtrace:\nThis was the result for every thread except for thread number 12:\n(lldb) expression (void) rb_backtrace()\n    from /Users/dteoh/.rbenv/versions/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.8.2/lib/puma/plugin/tmp_restart.rb:19:in `block in start'\n    from /Users/dteoh/.rbenv/versions/2.3.1/lib/ruby/gems/2.3.0/gems/puma-3.8.2/lib/puma/plugin/tmp_restart.rb:19:in `sleep'\nThis is the thread list, after switching to thread 12:\n(lldb) thread list\nProcess 11725 stopped\n  thread #1: tid = 0x9b3de, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, queue = 'com.apple.main-thread'\n  thread #2: tid = 0x9b3df, 0x00007fffc3ccb19e libsystem_kernel.dylib`poll + 10, name = 'ruby-timer-thr'\n  thread #3: tid = 0x9b403, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, name = 'tmp_restart.rb*'\n  thread #4: tid = 0x9b404, 0x00007fffc3cc9d42 libsystem_kernel.dylib`__pthread_kill + 10, name = 'thread_pool.rb*', stop reason = signal SIGABRT\n  thread #5: tid = 0x9b405, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, name = 'thread_pool.rb*'\n  thread #6: tid = 0x9b406, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, name = 'thread_pool.rb*'\n  thread #7: tid = 0x9b407, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, name = 'thread_pool.rb*'\n  thread #8: tid = 0x9b408, 0x00007fffc3cc9c22 libsystem_kernel.dylib`__psynch_mutexwait + 10, name = 'thread_pool.rb*'\n  thread #9: tid = 0x9b409, 0x00007fffc3cc9eb6 libsystem_kernel.dylib`__select + 10, name = 'reactor.rb:151'\n  thread #10: tid = 0x9b40a, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, name = 'thread_pool.rb*'\n  thread #11: tid = 0x9b40b, 0x00007fffc3cc9bf2 libsystem_kernel.dylib`__psynch_cvwait + 10, name = 'thread_pool.rb*'\n* thread #12: tid = 0x9b40c, 0x00007fffc3cc9eb6 libsystem_kernel.dylib`__select + 10, name = 'server.rb:325'\nRunning rb_backtrace on thread 12, I get the output seen in this gist: https://gist.github.com/dteoh/500e8330c53700dbb5ad4e12f1af6af3\nI hope this will help in resolving this issue.\nEDIT: it made no difference whether the tmp_restart plugin is enabled or not.. ",
    "GabKlein": "Same here but I'm on rails 4.2.7 ruby 2.3.1. \nI'm deploying my whole stack on docker swarm :\n1) First connexion everything looks good.\n2) Wait 10 to 15 minutes\n3) Refresh\n4) My service do not respond\nThe log console shows nothing more than:\nStarted GET \"/v1/users/me\" for 11.11.2.43 at 2017-05-22 07:45:00 +0000\nOn the docker instance:\nA simple curl localhost:3000 do not respond\nNeither using telnet that at least connects then hangs\nI tried Puma 3.6.0, 3.8.2, and also 2.16 so I don't think it is Puma related and I'm about to try with unicorn and passenger.\nThis issue describes the most what I have but if you can point me out to an other I would be glad.\n EDIT \nExact same behavior with passenger.... Here the stack trace using Puma on the step 1:\nhttps://gist.github.com/GabKlein/4bc1271b65d03c6ce3f98f4b60cd2f09\nHere the stack trace using Puma on the step 4:\nhttps://gist.github.com/GabKlein/f08c03f0cb8dc6bff5ded564bafb8e96 . Ok, I found out what was going on and I hope that will help.\nFirst of all and in my case, it's not a Puma issue. It's related to the docker swarm environment were client and server are on the same cluster and connexion are passing through the overlay network. \nSwarm max long live connexion are 900 seconds. Ruby libpg using system keepalived connexion settings by default, idle connexion after 900 seconds were closed by Swarm without notification to the client that do not try to connect again.\nI fixed my issue by changing my client settings to never let connexions idle for more than 900. \nhere my database.yml\ndefault: &default\n  adapter: postgresql\n  encoding: unicode\n  pool: 10\n  timeout: 5000\n  variables:\n    tcp_keepalives_idle: 60\n    tcp_keepalives_interval: 60\n    tcp_keepalives_count: 100\nWhat helped me to understand: https://github.com/moby/moby/issues/31208. ",
    "ypadlyak": "It does!\nOn Sat, Jun 3, 2017, 19:53 Nate Berkopec notifications@github.com wrote:\n\nThis is probably Rails autoloading (rails/rails#27455\nhttps://github.com/rails/rails/issues/27455). Anyone in this thread\nexperiencing the issue, please report back if running Puma with one thread\nfixes the problem, or if you are experiencing this issue in production\n(i.e. config.eager_load = true).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1184#issuecomment-305987230, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAgoNquBS5Wvi-YHETBPFxjFk35L_9plks5sAY9sgaJpZM4LZfsU\n.\n. Yes, the same! But it is processing the request or two and stops again. I've now rebooted it and the log looks strange (these TERMs):\n[2419] - Starting phased worker restart, phase: 5\n[2419] + Changing to /home/ubuntu/tickets/current\n[2419] - Stopping 3229 for phased upgrade...\n[2419] - TERM sent to 3229...\n[2419] - TERM sent to 3229...\n[2419] - TERM sent to 3229...\n[2419] - TERM sent to 3229...\n[2419] - TERM sent to 3229...\n[2419] - TERM sent to 3229...\n[2419] - KILL sent to 3229...\n[4812] + Gemfile in context: //20170419101552/Gemfile\nI, [2017-04-19T10:19:18.119862 #4812]  INFO -- sentry:  [Raven] Raven 2.4.0 ready to catch errors\n[2419] - Worker 1 (pid: 4812) booted, phase: 5\n[2419] - Stopping 4356 for phased upgrade...\n[2419] - TERM sent to 4356...\n[4839] + Gemfile in context: //20170419101552/Gemfile\nI, [2017-04-19T10:19:27.204147 #4839]  INFO -- sentry:  [Raven] Raven 2.4.0 ready to catch errors\n[2419] - Worker 0 (pid: 4839) booted, phase: 5\n. I have now tried this suggestion: https://github.com/rails/rails/issues/27455#issuecomment-269262124 . I'll report if it happens again. \n",
    "simptive": "I'm using Rails 5.1.4 with Ruby 2.4.2. In my case, I found that Puma in development mode using same puma.rb which was modified for production actually.\nPutting the following block in condition fixed the issue:\n```\nif ENV.fetch(\"RAILS_ENV\") == 'production'\n  app_dir = File.expand_path(\"../..\", FILE)\nstdout_redirect \"#{app_dir}/log/puma.stdout.log\", \"#{app_dir}/log/puma.stderr.log\", true\non_worker_boot do\n    require \"active_record\"\n    ActiveRecord::Base.connection.disconnect! rescue ActiveRecord::ConnectionNotEstablished\n    ActiveRecord::Base.establish_connection(YAML.load_file(\"#{app_dir}/config/database.yml\")[rails_env])\n  end\nend\n```. ",
    "dottgonzo": "that's is crazy, rails is the only framework not working on my swarm! I've tried thin instead of puma (3.7), with same problems. I need to try in development mode but this is annoyng!!. ",
    "aquach": "Good question. It seems more correct for Puma to immediately reject to signify shutdown. I only added it as a configuration so that it wouldn't change existing behavior. Happy to make it the default too, if that's what you're interested in.. I'm unfamiliar with socket stuff, but would closing the listening socket cut off existing open connections that Puma is waiting for during the graceful shutdown period? If so, then that would break graceful shutdown.\nAlso, if I'm reading it right, https://github.com/puma/puma/blob/master/lib/puma/server.rb#L396 is the place where the sockets are closed. It's after the graceful shutdown on line 383.. ",
    "mainameiz": "+1 for rejecting new requests when shutting down.. > Rather than accepting and closing in a loop (which is pretty rude and can easily confuse load balancers), the right thing to do is have the listening socket closed. I thought that was already done though....\nAfter spending some time to investigate it, it seems to me that Binder#close is never called... There are two places where Binder#close is called - one and two, but before #close is called it checks @own_binder variable... and it seems to me that this variable is always false because:\n1) It is set to false in Server#inherit_binder\n2) Because all Server instances (for workers) are created only in one place, here\nhttps://github.com/puma/puma/blob/f7e1e58dcba1e19e470b6eb0c44c33a0722285f6/lib/puma/runner.rb#L162-L165\nand Server#inherit_binder always called for all this instances. > I think what I would ideally like is to first tell the socket to stop accepting new connections, then drain whatever is left in the queue. \nI am not sure that it is possible, but it is 100% possible in reverse order: 1) drain the queue 2) stop accepting new connections.\nI suppose that \u21161 is fast operation and gap between accepting TERM signal and stop accepting new requests is small.. Could you take a look at my fix https://github.com/puma/puma/pull/1685?. Is there any news?. @nateberkopec @evanphx do you have any comments about this issue and solution?. @schneems could you take a look?. ",
    "calvinxiao": "We are running puma on Kubernetes, here is the work we do to avoid this problem.\n\nadd readiness http probe in Kubernetes\nadd preStop hook in Kubernetes, commad is pkill --signal QUIT bundle && sleep 10\nadd get :readiness to return 200 or 500 depending global variable $app_ready\nset up :QUIT trap to update global variable $app_ready, so http call to :readiness will receive 500\n\nIn this way, Kubernetes will remove the not ready endpoints, stop sending traffic to the PODs that are about to close, one needs to make sure the sleep command is long enough for Kubernetes to fail the readiness probe.\nyou can apply check [health check(https://docs.nginx.com/nginx/admin-guide/load-balancer/http-health-check/) in nginx to avoid this problem.\n\nI would like to have before_server_close hook here to handle the readiness variable change, then I won't need the preStop hook.. ",
    "whitehat101": "I've reviewed the referenced rails issue, and this does appear to be from a quirk with Puma's minissl, so I thought I'd bump here instead of there. To use SSL+WebSockets I either need to lock rails to 5.0.0.1 or find someone other than Puma to do SSL termination, and I'm not excited about either. \nhttps://github.com/rails/rails/issues/28362#issuecomment-285903720\n\nMy take on how to fix this would be for Puma to conform to TCPSocket's API, and to implement the write_nonblock method.\n\nDo you agree that that is (part of) the long term solution?\nI see @edwardmp mentions trying things with write_nonblock, but I wasn't able to find a branch or code. Could you share those efforts, or call out the blocks of code that need to be massaged?\nI expect around: https://github.com/puma/puma/blob/0b3626091e7ca7013312586a54a2f6c150f861b7/lib/puma/minissl.rb#L57-L78\n. ",
    "risinglf": "@nateberkopec how is going with the new Puma release? AFAIK the 3.8.2 does not have that fix, right?\nthank you so much. ",
    "slehmann36": "Is there an update on this? I can't see this issue references in any release notes...\nAs others have mentioned, ActionCable is broken as of Rails v5.0.1 until this fix is released. Please release it at earliest convenience so everyone can start using ActionCable again... \nThanks. ",
    "pabloisrael": "Hi there, i'm having the same problem. Do you remember how did you fix it?. ",
    "manishval": "@nateberkopec I have started looking at this and it seems to be an issue with OptionParser parsing the :none value  being set for control_auth_token. OptionsParser is expecting a string instead of a symbol.\nI'm not sure what the intended behavior should be in terms of the no_token: true option and the control_auth_token. But it looks like we can set the value of control_auth_token to an empty string instead of :none and it will be taken care of in the runner.. Sure. I will do that in a bit.. Good idea. ",
    "plinss": "Had the same problem in a fresh install of stretch, solved with:\napt-get install libssl1.0-dev. ",
    "senid231": "i solve this by installing libssl1.0 from stable branch. ",
    "ilpianista": "This has been fixed by #1178.. ",
    "jonmchan": "Thanks for your help. I couldn't get nginx to properly queue like puma, ended up reverting and upgrading the virtualized hardware... . ",
    "faucct": "I can disable queue_requests and it decreases the time until puma becomes available again, but it is still more than 30 seconds. Are my calculations correct?. Please, see the example. The request I expect nginx to take does not have any delays in it.. Yes, I guess so. Thanks.. It seems to me that something is blocking those curl requests from running in parallel since they are not going from processes list simultaneously.. Wel, this is obvious that the single request will succeed. I think I have meant to put nginx proxy_connect_timeout and proxy_send_timeout to 5 seconds too. Then most of the curl requests completes at the same time. The nginx still cannot answer after 30 seconds though. I have updated the repo.. Okay, if I configure it to 1 worker, 1 process and the backlog of size 2 then it does not respond within 30 seconds, but it obviously responds in less than 1000 seconds, so good enough. I guess I am just bad at math. :(. ",
    "richpeck": "Confirmed issue with new puma gem. Removing options set in config pass correctly. +1 \n. ",
    "twalpole": "This is basically the same as issue #1199. This is the same as issue #1199 and #1200 - There is a workaround in the Capybara issue about it - https://github.com/teamcapybara/capybara/issues/1831. @ayghor It wasn't \"fixed\" on v3.6.2 - a fix for a different issue was reverted on 3.6.2 because while it fixed the previous issue it had created this new issue. So after reverting the previous issue still existed. This PR fixes both issues.. @nateberkopec  Thoughts?. This is caused by the same problem as issue #1199 #1200 and #1201 - Potential fix is #1203 . Same as issue #1199 #1200 #1201 #1203 #1204. LGTM. Nevermind - requiring 'rack/handler/puma' doesn't trigger a require of 'puma'  so Puma.stats_object= wasn't being defined.. @schneems Not really sure -- Capybara was just requiring 'rack/handler/puma' when setting up puma, which used to be fine, but with this change resulted in an error because Puma.stats_object= wasn't defined.  If Capybara requires 'puma' as well the error goes away.  Whether or not just requiring rack/handler/puma should be enough is up to you.. ",
    "michael-reeves": "Seeing the same thing trying to run port 3001.. ",
    "ivanovaleksey": "@ayghor Do you mean 3.6.2? Since 3.2.6 doesn't exist.\nAnd with 3.6.2 it works fine.. ",
    "roeintense": "Same problem. Downgraded to 3.6.2, everything fat and sassy again.. Not sure if this is the same issue that @frazboyz is having, but I see similar symptoms. Same, no output to my production log at all except that it shut down (polite 'Goodbye!'). \nI deploy using the background task with a nohup:\nnohup rails s -b 0.0.0.0 -e production &\nI had a colleague recommend daemonizing with the -d tag on rails s and it starts fine. Haven't tested for very long. Only thing I notice is that it ignores my config/puma.rb completely and doesn't fire workers. When I daemonize and force the config using the -c flag, I get:\n=> Booting Puma\n=> Rails 4.1.2 application starting in production on http://0.0.0.0:3000\n=> Run `rails server -h` for more startup options\n=> Notice: server is listening on all interfaces (0.0.0.0). Consider using 127.0.0.1 (--binding option)\n=> Ctrl-C to shutdown server\nExiting\n/var/www/jawa/config/puma.rb:4:in `<top (required)>': undefined method `workers' for main:Object (NoMethodError). Nevermind. It was dying because I wasn't demonizing properly. I figured it\nout, but I can't run rails s, I have to run pumactl -F.\nOn Feb 17, 2017 6:02 PM, \"Nate Berkopec\" notifications@github.com wrote:\n\n@roeintense https://github.com/roeintense Puma version?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/puma/puma/issues/1202#issuecomment-280799869, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGxKOyjIAdYgJDmBCtRfXrlR9Ndahknuks5rdjT9gaJpZM4LyAyx\n.\n. \n",
    "jaswinder97": "After updating the gem version and bundling: \ngem 'puma', '~> 3.6.2'\nif problem still persists,then try to kill the running puma process first:\nps aux | grep puma\n2598  2.4  3.5 1017520 141400 pts/16 Sl+  15:57   0:04 puma 3.7.0 (tcp://0.0.0.0:3000) [rails-app]\nand then restart the app again \njaswind+  2950 11.0  3.6 1020164 143996 pts/10 Sl+  16:02   0:04 puma 3.6.2 (tcp://0.0.0.0:3000) [rails_app_copy]                                                                                              \njaswind+  2976 18.1  3.6 1020880 144788 pts/16 Sl+  16:02   0:04 puma 3.6.2 (tcp://0.0.0.0:3001) [rails_app]\n. ",
    "beccasaurus": "Is there an ETA for 3.7.1 going out with this fix?  . ",
    "herregroen": "We're consistently experiencing this issue with any Puma version above 3.6.0.\nUnsure what's causing it but starting puma using nohup puma config.ru & with puma 3.5.2 and lower properly detaches the process. So once the ssh-session that initially started the process finishes puma stays active.\nDoing the same with puma 3.6.0 and higher keeps the process attached to the ssh-session that initially started it, so once you disconnect you also kill puma.\nI imagine the initial issue the server is started by some other process which stays alive for some random amount of time and upon ending takes down puma with it.. Having done some more research this seems to be directly caused #911. Undoing that commit fixes the issue entirely.. ",
    "bikramwp": "In Rails 5.0.1, this change looks like it causes the options in config/puma.rb to be ignored:\nScenario 1: config/puma.rb with custom bind option, running rails s command\nconfig/puma.rb contains:\nbind ENV.fetch(\"BIND\") { \"tcp://127.0.0.1:4444\" }\nIn Puma 3.7.0, that generated an options array of:\n[\n  { ... , :Port=>3000, :Host=>\"localhost\"  },\n  { ... , :binds=>[\"tcp://localhost:3000\"] },\n  { ... , :binds=>[\"tcp://127.0.0.1:4444\"] }\n]\nIn Puma 3.7.1, it generates:\n[\n  { ... , :binds=>[\"tcp://127.0.0.1:4444\"] },\n  { ... , :Port=>3000, :Host=>\"localhost\"  },\n  { ... , :binds=>[\"tcp://localhost:3000\"] }\n]\nScenario 2: config/puma.rb with custom bind option, running rails s -p 9999 command\nconfig/puma.rb: contains (same as Scenario 1):\nbind ENV.fetch(\"BIND\") { \"tcp://127.0.0.1:4444\" }\nIn Puma 3.7.0, that generated an options array of:\n[\n  { ... , :Port=>9999, :Host=>\"localhost\"  },\n  { ... , :binds=>[\"tcp://localhost:9999\"] },\n  { ... , :binds=>[\"tcp://127.0.0.1:4444\"] }\n]\nIn Puma 3.7.1, it generates:\n[\n  { ... , :binds=>[\"tcp://127.0.0.1:4444\"] },\n  { ... , :Port=>9999, :Host=>\"localhost\"  },\n  { ... , :binds=>[\"tcp://localhost:9999\"] }\n]\nSo, Rails' config (internal default or specified on rails s command) is always taking precedence\nand config/puma.rb settings don't take effect.\nI think this pull request is right in that rails s options should take priority over config/puma.rb,\nbut in the case where no rails s options are specified on the command line, config/puma.rb should\ntake precedence over Rails' internal defaults.\nUnfortunately, I am not sure what the proper solution would be: is Rails configuring Puma incorrectly\nor should Puma be working differently?\n. ",
    "wrburgess": "Temporary solution posted here: https://github.com/teamcapybara/capybara/issues/1831. ",
    "angeleah": "I suspected so but thought it would still be good to get it documented. Thank you!. ",
    "waltercool": "This version also makes problem with LibreSSL. 3.6.0 was fine.\nRuby 2.3.3\nLibreSSL 2.5.0 (and tested same problem with 2.5.1)\nAttaching compilation error.\ngem_make.txt\n. ",
    "nibygro": "@nateberkopec have the same issue, but with 3.10\n\n$ openssl version\nOpenSSL 1.1.0f  25 May 2017\n$ rails s\n=> Booting Puma\n=> Rails 5.1.3 application starting in development on http://localhost:3000\n=> Run rails server -h for more startup options\n/home/nikolai/.rvm/rubies/ruby-2.4.1/bin/ruby: symbol lookup error: /home/nikolai/.rvm/gems/ruby-2.4.1/gems/puma-3.10.0/lib/puma/puma_http11.so: undefined symbol: OPENSSL_init_ssl\n\nWhere I can read how to fix it? #1181 doesn't help. @nateberkopec you mean  libssl-dev ? Already installed:\n\n$ sudo apt-get install libssl-dev\nReading package lists... Done\nBuilding dependency tree     \nReading state information... Done\nlibssl-dev is already the newest version (1.0.2g-1ubuntu4.8).\n$ dpkg -s libssl-dev\nPackage: libssl-dev\nStatus: install ok installed\nPriority: optional\nSection: libdevel\nInstalled-Size: 7027\nMaintainer: Ubuntu Developers ubuntu-devel-discuss@lists.ubuntu.com\nArchitecture: amd64\nMulti-Arch: same\nSource: openssl\nVersion: 1.0.2g-1ubuntu4.8\nDepends: libssl1.0.0 (= 1.0.2g-1ubuntu4.8), zlib1g-dev\n...\n\nMaybe some symlinks are broken?. Nevermind. Fixed by recompiling openssl with ./configure CFLAGS=-fPIC CXXFLAGS=-fPIC\nThanx!. ",
    "NikolayRys": "Done :). I guess tests failed because of some erratic issue, which not related to this PR.. Okay, I think I can create some minimal implementation, that would produce an exception of such kind, give me a couple of days. Also, if you have any particular questions about the mechanism, you're welcome to ask me. . tl;dr: this error, which I've reported, happens when Puma::Server#stop is called from any rescue clause. And it's ruby's internal bug.\nHow it happens:\n- MRI has a special exception type which is thrown when one thread tries to access a file descriptor that is locked by some other thread: IOError: stream closed. \nDefinition: https://github.com/ruby/ruby/blob/trunk/thread.c#L4823 \nLocking: https://github.com/ruby/ruby/blob/trunk/thread.c#L1423\nScheduling interrupt when locked: https://github.com/ruby/ruby/blob/trunk/thread.c#L2191\n- It is exactly the error that the author of the line rescue IOError in the #stop method was trying to catch.\n\nThe problem is that it's an exception of a \"special type\" that is getting frozen(and even tainted) right after creation: https://github.com/ruby/ruby/blob/trunk/vm.c#L2078. I suppose that has been done because of security reasons and assumption that code, running in another thread, might have different SAFE level. \nAfter that it can not be handled as a regular exception anymore, as how ruby tries to prepare it before throwing it. Here is the actual offender: https://github.com/ruby/ruby/blob/trunk/eval.c#L511\nit's an attempt to assign the Exception#cause field that should contain link to another exception caught by current rescue clause. It simply can't do it because it's already frozen and the first exception gets discarded and substituted with what we saw earlier: RuntimeError: can't modify frozen IOError. \nIf it happens outside the rescue clause, the the original exception is thrown as expected, since there that \"cause exception\" would be nil and MRI does not try to assign it.\nTo sum up, it's 100% bug in ruby internals, that went unnoticed for a while because of not very often conditions when it happens. But I happened to have it failing for me in a production app with puma. I'm gonna to report it and maybe even try to submit a fix, but I'm not sure it will be accepted and backported soon, so puma in meanwhile actually may use this improvement.\nTo reproduce it: \nPull version of ruby where I've added one commit 5 sec sleep to imitate slow OS response or delay caused by ruby scheduler(so lock stays retained for a while): https://github.com/NickolasVashchenko/ruby/commits/special_exc_cause_bug\nCompile it and use it to run this script: \n```\nrd, wr = IO.pipe\nThread.new do\n  IO.select [rd]\n  wr.close\nend\n\nbegin\n  raise 'any exception'\nrescue\n  wr << 'A' \nend \n```. It leaves the thread in the bad state. So if it's somehow ended or killed, puma can keep up working.\nHowever, I've got a simpler and more direct solution for the workaround and gonna submit it a day or two.. Damn, rebasing the branch closed the the pull request. Is it possible to reopen it?. Thx for the attempt, but maybe someone of the maintainers could.. Okay, I'll create a new one then.. Recent MRI versions do not even have the have the \"stream closed\" error message for couple of months anymore, it's called now \"stream closed in another thread\". Please update Ruby to the most recent patch version, it most likely will resolve your issue.. this seems to be a bug I've described in #1302. It's a Ruby issue, I've reported it and you can monitor it being backported here: \nhttps://bugs.ruby-lang.org/issues/13632. submitted the initial version of fix: https://github.com/puma/puma/pull/1345. The hooks for the stopgap gem have not yet been added in a Puma release. They are only in master branch for now.. Please, either update to 2.4.2 or install the mentioned gem.. https://ruby-doc.org/core-2.2.0/Interrupt.html - this one can come virtually from anywhere, including IO.close.. Sure, why not. You're speaking about the failed tests on CI?. Ok, I'll check what could be done there :thumbsup:. Okay, clearly that's again a ruby bug with a complicated story. I'm still checking the details.. Here are preliminary results.\nIt has been introduced here: https://github.com/ruby/ruby/commit/bc855ef1391bb79ca3554f67ae2d3aad19178d94\nAs an attempt to fix this race condition, described in this ticket: \nhttps://bugs.ruby-lang.org/issues/13076\nThis race condition has been fixed by a busy waiting approach, so when a file descriptor is not released by some thread fast enough when this FD is closed in some other thread, then the \"stream closed\" exception gets repeatedly re-added in the thread internal interrupt queue. As you can see, it's being added in a cycle. \nIt leads to the case when it's possible for a thread to receive several hundred or even thousands of the interrupts of the same type during the looping. The main problem with it is that those exceptions being scheduled in bulk, but being dequeued one-by-one in very specific conditions. When a thread gets GVL back, on on any IO-related operation. Here you can see it happening:\nhttps://github.com/ruby/ruby/blob/trunk/thread.c#L2073\nBut besides that is obviously a poor solution from the performance point of view, it also introduces the bug we've seen on the Puma's CI. What is raised there is one of those excessively enqueued exceptions, that are being dequeued much-much later, on an IO operation performed in the affected thread. \nThis is easily verifiable by, doing, for example, an IO.sysopen method call, when any of \"stream closed in another thread\" exceptions are caught. After a couple of hundred calls to it, when the whole queue is processed, it starts working normally again, without raising an exception. \nI'm going to report this to the ruby-core team, but I'm still working on an example when it's consistently reproducible since again race conditions are involved. But that's not gonna be too hard. If you're interested how and when it's gonna be resolved, I can post the link here when I'm done.\nAnd here's what it probably means for Puma:\n1. That is/will be recognized as a Ruby bug.\n2. It will be fixed somehow before the 2.5.0 release.\n3. It does not affect any of the other Ruby versions that have ever been released.\n4. Thus, I don't think Puma needs a workaround for that because it's gonna be exceptionally ugly and short living.\nThoughts?. My bad, sorry. I see they backported it:  https://github.com/ruby/ruby/blob/ruby_2_4/thread.c#L2199\nIn meanwhile I managed to get the script for reproducing the bug.\nThey did it exactly for the list of rubies you mentioned: \"Backport changed from 2.2: REQUIRED, 2.3: REQUIRED, 2.4: DONE to 2.2: REQUIRED, 2.3: DONE, 2.4: DONE.\"\nTo sum it up, the guys downright have broken the interpreter :) There are a couple of ways how it can be fixed in puma until Ruby is okay again.\nAlso, what versions does Puma support? They also fixed the bug from https://github.com/puma/puma/pull/1206, but again, only starting from 2.2: \n https://bugs.ruby-lang.org/issues/13239\nSo I guess, if it is not expected to work with older MRIs, it can be reverted.\n. Yeah, I think, fix can be tolerably ugly, but it would involve a native extension.\nAlso, I see there's 2.1 in the CI build. If it's supported, then my fix from #1206 is still useful. . Reported it: https://bugs.ruby-lang.org/issues/13632. The guys fixed it blazingly fast \ud83d\ude04 And even already backported.\nhttps://github.com/ruby/ruby/commit/59fb92971fa8e461430c35307a6b9eb0572ed4c5\nPlease kick off the CI again to see the results again. Locally everything is fine for me for all those versions, and in my opinion, you're good to merge this PR.. I also expect this to fix the https://github.com/puma/puma/issues/1293.. Okay, sorry for misinterpreting that, it's not yet backported, but just in the trunk.\nAlso, regarding the trunk, the version in CI is quite old, so it does not have the right version as well:\n$ ruby --version\nruby 2.5.0dev (2017-05-26 trunk 58892) [x86_64-linux]. I see it fails with 2.3, since it has a bit different thread structure. Gonna be an easy fix.. Yeah, of course. I'll scope it to only affected versions when I'll be fixing it for 2.3.. The status quo: the bug has been introduced in 2.5.0, 2.4.1, 2.3.4 and 2.2.7, but has been only fixed for 2.5.0 so far. Thus my fix affects the other three and they are working now. \nI also gonna check it also for windows, and if it's okay there as well, we're good to merge it.\n@evanphx I could help you maintaining this code, monitor the status of backporting of the bug and its fixes in order to adjust and remove this workaround when the fix is backported everywhere. \nAlso I can help you with maintaining other C code.. I can add a test for it, but it's gonna be erratic since it's pretty hard to reproduce this issue in lab environment.. added a test to cover it. It mirrors the internal Ruby test for the same issue, but it's appropriate there.. Checked out appveyor, it does not have those 3 versions of ruby that are interesting. With other, older ones, the build works, however.. Also managed to test it with version 2.4.1 on windows. The windows-specific pieces of memory layout do not depend on ruby version, so from this point, I feel that we're good to merge it.. Spoke with @nateberkopec and we decided to move the C code in a separate gem, but leave hooks in the rescue IOError Puma blocks. \nI'll post an update here when ready.. Okay, the gem is created and ready for usage. Tested for majority ruby versions both for Unix and windows.\nhttps://rubygems.org/gems/stopgap_13632. So only the hooks in rescue block stay for now.\n@nateberkopec please, check the thing :). @stereobooster For this branch it's intended.. Updated, it became better. \nI can also setup appveyor for Puma. @nateberkopec do we need it here?. All the remaining failure messages seem unrelated to that Ruby bug.. Sure, no probs. \nI think it is maybe related to https://bugs.ruby-lang.org/issues/13239 and I'll look in it further in the next couple of days. But it's pretty safe to say that those issues are separate.. Okay, looks like it was a combination of both since they did not port the other one to 2.2.7. Don't forget to add the badge :). @stereobooster thanks :)\n@nateberkopec appveyor badges seem to use your project id or some token that I don't know:\nhttps://www.appveyor.com/docs/status-badges/. Merged your PR and released:\nhttps://rubygems.org/gems/stopgap_13632/versions/1.2.0. Sure.. No, I don't. The pending_interrupt? is not expected to be true for all rubies that are not addressed specifically. We'd better see immediately that it started to happen in this case. . Improved all other naming. Not quite. I intended to maintain resemblance here with rescue IOError clause by not adding excessive words. To be frank, any other RuntimeError with IOError in the message are extremely unlikely.. It's because the version of 2.5.0 on CI is very old and does not include the fix. Please, disregard. It's not the at the HEAD anymore there.. I'm calling a vanilla Ruby method, representing a syscall, so I don't not really expect them to be over-creative here :) However, I'm not insisting on keeping here a succint exception message, let's hear what other people say on that.. Got ya :) I just feel it'll be a bit uglier with no real purpose, actually. The << method of IO throws only IOErrors of the StandardError by it's API, so just therescue RuntimeError would be more than enough here. What we have there with e.message.include?('IOError') is already over-cautious and safe.. That would not be right, because of the reasons I've described above. However, I've improved a bit reporting there, to show a clear message in this case.. If the workaround method is not defined and the bug has happened, then the test is supposed to fail. It's its purpose.. Okay.. No-no, the 2.2.0-2.2.6, 2.3.0-2.3.3 and 2.4.0 are good and unaffected by the bug, I've checked that pretty carefully.. Yes, that's correct. The 3 mentioned versions with regards to the configuration and OS.. ",
    "respire": "looks like it can reproduced in production\nplease move to https://github.com/puma/puma/issues/1293 to track the progress\n\nok i just watch #1302 \nit's related to ruby bug 13632\nhttps://bugs.ruby-lang.org/issues/13632\nmaybe we can wait for upstream patching. but I'm not sure whether this issue can be reproduced in production. \n\nhello @nateberkopec @NickolasVashchenko \ni try to add mutex to guard the pipe, and after struggling fight (awww), it works now.\nhttps://travis-ci.org/respire/puma/builds/243074681\ni even run this build several times. looks like it's stable enough. \nthe commit fixed this issue:\nhttps://github.com/respire/puma/commit/4fe40c2904433a390cab0e12ddaf631286ea66d4\nnoted that this is not the final fix. i still need more time to analyze why it's working now.\nif you have some ideas, please reply!  thanks! :)\n. i believe it's related to the fix for this issue https://github.com/puma/puma/issues/1002\nthe commit https://github.com/puma/puma/commit/46416cb49ed2f16614f019cee969bb8f5d0a6146\ni'm still looking into that issue and trying to understand the code.\nanyone who gets hints are welcome. I submit a PR #1331 as one possible solution.\nIt works under my env: openssl 1.0.2k, osx Sierra, puma 3.6.1\nHowever, I'm not sure whether it's the correct way to fix this issue.. i try to write a no mutex version\nhttps://github.com/respire/puma/pull/3\nhttps://travis-ci.org/respire/puma/builds/243079095\nlooks like it works as well but not always stable ( I triggered 2.4.1 build several times until it turned green maybe due to some bad test cases)\nthe key point is check and close while rescue and retry\nadding mutex is similar to adding delay which makes conflicts not happening that often\nor just because it guards some bad test case\ni just wonder whether io.close is thread-safe while socket is force closed by another thread(like system) in ruby 2.x\n. close since it's not a good fix. @nateberkopec i used to. but do you know why master's tests keeps failing now? tests even hang in my laptop. actually, that makes me feel like i'm forking a wrong branch so i pick 3.6.2 to state the problem.. ",
    "tonobo": "Looks like an invalid response object, possibly a grape exception?. Its a grape bug! (Won't appear in 0.17.0). ",
    "marcmarcet": "+1\nI have exactly the same issue with:\nRuby 2.4.0p0\nRails 5.0.1\nUbuntu 16.04\nFallback to 3.6.2 solves the issue as well.\n. ",
    "noma4i": "Same here. Ubuntu 16.04\nbundle exec puma --daemon --bind unix://$PUMA_SOCKET --pidfile $PUMA_PID_FILE\"\nPuma is running and serving content while never creating socket file\nRollback to 3.6.2 solved the issue.. I believe thats kind of bug due to misconfiguration and eventually reproducible. So we can close that issue as no complains since late march 2017.. ",
    "tristanholl": ":+1:. ",
    "decoomanj": "I'm running a small Go-client which has ExpectContinueTimeout: 1 * time.Second, in its client. This causes a delay of 1s when doing posts to a Ruby service. I had to remove it for the Ruby services. So, yes, this should be fixed, I suppose.. ",
    "defsprite": "Excellent, this is great news! Thank you very very much! \ud83c\udf89  :1st_place_medal: \n. ",
    "hackedunit": "That is because Ubuntu 16.04 now uses systemd rather than Upstart (as described in that tutorial). So you will need to use systemd (there is an option to go back to using upstart though). \nI created a systemd service here - after following the example from here \nYou could download that gist and put it at /etc/systemd/system/puma.service\nwhich will let you use the following commands:\nsystemctl start puma\nsystemctl stop puma\nsystemctl status puma\nIf you want to start puma on boot:\nsystemctl enable puma. You could simply do which puma to see the complete path to your puma binary. \nI'm not sure you need an ExecStop as the stop command seems to work for me but using pumactl -S seems to be the recommended method to stop Puma server.. I think its a problem with defining the Rails env in your puma.rb file. You seem to have used the rails_env directive but haven't defined it here.\nHere is my puma.rb in case it helps you.. ",
    "adis-io": "http://codepany.com/blog/rails-5-puma-capistrano-nginx-jungle-upstart/ - alternative usage of upstart instead of systemd. Maybe this is related...\nhttps://github.com/puma/puma/pull/1252\nLook at this please, @nateberkopec . @schneems \nHi, could you please look at this?. @schneems \nFilling test for a bug.. ",
    "thadeu": "install upstart-sysv solve my problem. ",
    "markgarrigan": "In case anyone is searching for answers...\nThe only ExecStart that worked for me was this...\nExecStart=/bin/bash -lc 'puma -C /path/to/puma.rb. ",
    "MehmetKaplan": "For newbies (like me) simply edit your Gemfile.\nBy default(*) you should see something like:\n```Ruby\nUse Puma as the app server\ngem 'puma', '~> 3.0'\n```\nReplace the puma version as:\n```Ruby\nUse Puma as the app server\ngem 'puma', '~> 3.0'\ngem 'puma', '~> 3.7.1'\n```\nThen run following command under your rails application's root directory, for puma v3.7.1 to take effect:\nRails\nbundle install\nThis indicates;   3.7.1 <= Puma version < 4.0 is OK for your application. Beautiful! :-)\n(*) Of course that default value should change as rails evolve by time. So this post should be useful \"nowadays\".. ",
    "linediconsine": "I had the same problem with puma 3.7 \nSolved with above comment\nin Gemfile \n#gem 'puma', '~> 3.7' # commented\ngem 'puma', '~> 3.7.1'\nand then on console:\n$bundle install\nThanks\n. ",
    "tadas-s": "So I tried this with puma v3.6.1 + ssl + persistent_timeout 2 while watching netstat -nt | grep \":3000\":\n```\nEvery 0.3s: netstat -nt | grep \":3000\"                                                                       Fri Feb 17 17:05:37 2017\ntcp        1 262552 10.0.2.15:3000          10.0.2.2:61111          LAST_ACK\ntcp        0 275236 10.0.2.15:3000          10.0.2.2:61197          LAST_ACK\ntcp        0  11629 10.0.2.15:3000          10.0.2.2:61230          LAST_ACK\ntcp        0 189813 10.0.2.15:3000          10.0.2.2:61393          LAST_ACK\ntcp        1  96197 10.0.2.15:3000          10.0.2.2:61421          LAST_ACK\ntcp        0      0 127.0.0.1:3000          127.0.0.1:58170         ESTABLISHED\ntcp        0 224615 10.0.2.15:3000          10.0.2.2:65076          LAST_ACK\ntcp        0 275566 10.0.2.15:3000          10.0.2.2:65088          LAST_ACK\ntcp       31      0 127.0.0.1:58170         127.0.0.1:3000          ESTABLISHED\ntcp        1  94945 10.0.2.15:3000          10.0.2.2:59847          LAST_ACK\n```\nConnection stays in ESTABLISHED and won't time out.\nWith v3.6.0 connections stay ESTABLISHED for 2 seconds and then close:\n```\nEvery 0.3s: netstat -nt | grep \":3000\"                                                                       Fri Feb 17 17:08:18 2017\ntcp        1 262552 10.0.2.15:3000          10.0.2.2:61111          LAST_ACK\ntcp        0 275236 10.0.2.15:3000          10.0.2.2:61197          LAST_ACK\ntcp        0      0 127.0.0.1:3000          127.0.0.1:58172         FIN_WAIT2\ntcp        0  11629 10.0.2.15:3000          10.0.2.2:61230          LAST_ACK\ntcp        0 189813 10.0.2.15:3000          10.0.2.2:61393          LAST_ACK\ntcp        1      0 127.0.0.1:58172         127.0.0.1:3000          CLOSE_WAIT\ntcp        1  96197 10.0.2.15:3000          10.0.2.2:61421          LAST_ACK\ntcp        0 224615 10.0.2.15:3000          10.0.2.2:65076          LAST_ACK\ntcp        0 275566 10.0.2.15:3000          10.0.2.2:65088          LAST_ACK\ntcp        1  94945 10.0.2.15:3000          10.0.2.2:59847          LAST_ACK\n``. After some debugging I found that the [socket.sysread` @ minissl.rb:105](https://github.com/puma/puma/blob/v3.7.0/lib/puma/minissl.rb#L105) call blocks and causes this issue.. ",
    "fedebello": "Is this issue solved?\nI'm trying with version (3.11.3) and server hangs connections.\nIf issue is solved, could you please tell me how to fix this?\nThanks in advance!. ",
    "IngoAlbers": "Duplicate of #1212 - will be fixed in 3.7.1. ",
    "aasmith": "The easiest way to reproduce this problem on demand is to make a simple clone of the test_timeout_in_data_phase test against an app with a reduced timeout (note the change in timeout is not needed, but helps prove the issue quicker).\nAssuming a local instance of puma, running on port 80:\napp.ru:\n```ruby\nPuma::Const::FIRST_DATA_TIMEOUT = 0.2\napp = proc do |env|\n  [ 200, {'Content-Type' => 'text/plain'}, [\"a\"] ]\nend\nrun app\n```\nbreak.sh\nbash\nwhile true\ndo\n  curl -X POST -H \"Host: example.com\" -H \"Content-Type: text/plain\" -H \"User-Agent:\" -H \"Accept:\" -H \"Content-Length: 5\" 0:80\ndone\nStart puma with puma -w 1 -t 1:1 -e deployment -p 80 -v app.ru, and then run the shell script. The shell script will pause forever when it hits the bug.. I'm not sure if my suggestion is the correct fix, but I'd like to hear from others who can repro this and what the best solution might be.. Also see the discussion on commit 43ee6ddca for more context.. I just repro'd this using the 3.9.1 gem, using the ruby docker image.\nI have a small adjustment to the steps to repro this in docker (in case it's platform-specific?)\n```console\n$ docker run -it ruby bash\ngem install puma\n\npuma -w 1 -t 1:1 -e deployment -p 80 -d app.ru\n[995] Puma starting in cluster mode...\n[995] * Version 3.9.1 (ruby 2.4.0-p0), codename: Private Caller\n[995] * Min threads: 1, max threads: 1\n[995] * Environment: deployment\n[995] * Process workers: 1\n[995] * Phased restart available\n[995] * Listening on tcp://0.0.0.0:80\n[995] * Daemonizing...\nwhile true; do curl -v -X POST -H \"Host: example.com\" -H \"Content-Type: text/plain\" -H \"User-Agent:\" -H \"Accept:\" -H \"Content-Length: 5\" 0:80; done\n\n```\nAfter applying the patch, the problem is no longer observed.\nHope this helps!\n. I was able to reproduce the problem using 3.10.0. However, I am no longer able to reproduce this with puma 3.11.0; it correctly gives a 408 Request Timeout for each request.\nTest system:\nconsole\nroot@af8662fe8d9c:/# puma --version\npuma version 3.11.0\nroot@af8662fe8d9c:/# uname -a\nLinux af8662fe8d9c 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 GNU/Linux\nroot@af8662fe8d9c:/# ruby -v\nruby 2.5.0p0 (2017-12-25 revision 61468) [x86_64-linux]\n. ",
    "mileslane": "Hit this today while upgrading from ruby 2.3.3 to 2.4.0.\n. This looks very similar.  I don't know how to get more helpful background info on how to reproduce the problem.  Please let me know how I can help.\n\ncapybara (3.9.0)\ncapybara-selenium (0.0.6)\nchromedriver-helper (2.1.0)\nrspec (3.8.0)\nrspec-core (3.8.0)\nrspec-expectations (3.8.2)\nrspec-instafail (1.0.0)\nrspec-mocks (3.8.0)\nrspec-rails (3.8.0)\nrspec-support (3.8.0)\npuma (3.12.0)\n\nError in reactor loop escaped: Invalid argument (Errno::EINVAL)\nruby-2.3.7@clear_value_plus/gems/puma-3.12.0/lib/puma/reactor.rb:126:in 'select'\nruby-2.3.7@clear_value_plus/gems/puma-3.12.0/lib/puma/reactor.rb:126:in 'run_internal'\nruby-2.3.7@clear_value_plus/gems/puma-3.12.0/lib/puma/reactor.rb:251:in 'block in run_in_thread'. Our time related tests are using Timecop.. Examples:\nspec/models/user.rb\n68:      Timecop.freeze('2015-04-01 12:00:00')\n78:      Timecop.freeze('2015-04-01 12:00:00')\n213:      Timecop.freeze do\n225:      Timecop.freeze do\nspec/models/reporter_spec.rb\n17:    Timecop.travel(2018, 3, 25, 8, 00) { check_alert(false, 23.hours.ago) }\n23:    Timecop.travel(2018, 3, 23, 8, 1) { check_alert(true, 2.hours.ago) }. Downgrading capybara to 2.18.0 worked for me as well.  Thanks!\ntimecop (0.9.1)\npuma (3.12.0). ",
    "MattDHill": "You guys figure this out?\nUPDATE\nUpgrading Puma to 3.7.1 fixed it for me.. ",
    "zegomesjf": "I got this Today upgrading from ruby 2.3 to 2.4 and I fix updating puma.. ",
    "maier-stefan": "@nateberkopec actually that is not true.\nIt must be s.th. else because running lsof -wni tcp:3000 does not give me any processes to kill.\n. ",
    "fruwe": "\ud83d\udc4d . temporarily can be fixed by using bind instead of ssl_bind like this:\n```\nssl_options = {\n  ca: \"xxx/yyy.cer\",\n  cert: \"xxx/yyy.crt\",\n  key: \"xxx/yyy.key\",\n  verify_mode: 'peer',\n}\nssl_bind '0.0.0.0', '443', ssl_options\nbind \"ssl://0.0.0.0:443?cert=#{ssl_options[:cert]}&key=#{ssl_options[:key]}&verify_mode=#{ssl_options[:verify_mode]}&ca=#{ssl_options[:ca]}\"\n```. ",
    "karlwilbur": "Still not working with ssl_bind as of latest Puma 3.12.0 (I mention it because the title explicitly states 3.6.2):\nhttps://github.com/puma/puma/blob/v3.12.0/lib/puma/dsl.rb#L286-L295\nhttps://github.com/puma/puma/blob/f7e1e58dcba1e19e470b6eb0c44c33a0722285f6/lib/puma/dsl.rb#L286-L295\n. ",
    "sasso": "@schneems there is no way to fix this on AWS or Kubernetes (TCP checks are valid) and the only way is to silence it. The amount of health checks on big clusters overflows the logs and makes them unreadable.. ",
    "jasonheffner": "I tracked this to an issue with the eventmachine gem. Is there a way to restart the worker after crash ? Crashes do happen, just want to be able to recover. . Thanks. ",
    "davidor": "I ran a quick test to show you the problem I\u2019m trying to solve in this PR.\nI set up a very basic web app served by Puma in clustered mode with 4 workers and 1 thread for each of them, to simulate a non thread-safe application. From a different machine, I sent around 1000 requests per second during 5 minutes.\nHere are the charts with the response times (notice the non-linear scale on the Y axis).\nBefore the patch:\n\nAfter the patch:\n\nAs you can see, the mean response time is what we would expect from a very basic application that does not much, a few ms. However, in the first chart, there are some requests that are really slow (> 1 second). I am aware that this test is very simple and we cannot draw too many conclusions from it, but I think it clearly shows the problem being addressed in the PR.. @nateberkopec Thanks for your feedback. I addressed your two comments. I deleted the unnecessary conversion to Integer from my second commit. I also added a new commit to use a local var instead of an instance var to store the fast_track_ka_timeout. If you think this is OK, I can squash the last two commits to have a clean history.. I added  @fast_track_ka_timeout = fast_track_ka_timeout inside the run method, but it's a mistake. I should have defined it in initialize. There isn't an accessor defined, but we could add it.. Avoid calculating the same thing multiple times, because @max_threads is static.. You're right. In a previous version of Puma that I tried, @max_threads was a String at that point, but it's no longer the case.. OK. I can change this to just call fast_track_ka_timeout. The method would be called on every request, so I thought that it would be good to cache the result, even if it's not a costly method. However, I'm not 100% sure either that @max_threads is always static.. ",
    "eileencodes": "I ran these tests locally and they didn't fail. I'm not sure what's going on here :(. Hey @schneems I rebased with master. I also double checked that yes touching the restart file still restarts both Rails servers.. I've updated the PR so that now the lambda now takes a header hash and tested this in the Rails implementation for our test app. Let me know if you'd like other changes but I think this is good to go now.. Thank you @nateberkopec!!!!!!!! \ud83c\udf89 . Moved to an attr_accessor and deleted this method.. Fixed \ud83d\udc4d . Fixed the tests here too. :+1: I also added a test to test that early hints is off by default.. v is defined right here https://github.com/puma/puma/pull/1403/files/ea37ada76937ff6b3988e952f598c26cf373904e#diff-ef4c610f0ce605eb90f2810856394fd5R604. Oh i see what you're saying - I will fix it. ",
    "sirupsen": "@nateberkopec \ud83d\udc4b  long time no see. > I wonder if rack protocol should allow exposing this as well\nYou'll be able to assert on puma.socket which is already in the environment.\n\ndoes it not makes sense to do one more check in the last middleware before handing the req to rails?\n\nThis is certainly not a perfect heuristic, but it's been good enough for our use-case in production for almost 5 years. It's never going to be perfect, there's always going to be a race. I think the webserver should do the best job it can, and once it's in Rack-land, the user shouldn't have to worry about this anymore. I don't really want to leak this into the application-level. In other words, the complexity of having a middleware as well I don't think is a bigger downside than the upside of possibly catching a couple more requests.\n\nI guess in a super ideal world you would patch the c binding for postgres as well to always double check if the socket is still good prior to running queries which would allow you to abort mid request.\n\nI'm not entirely sure what you mean by this. Is your point that the idea behind this patch could be extended into pretty much every library that talks over a socket to fail early?. > Sort of, in the natural course of events a big pile of SQL statements get executed making a monkey patch to \"exec_async\" which double checks if the connection to client is even there prior to running sql a pretty appealing patch. That would be the only patch I would really consider. downside is that a proc gets called a bunch of extra times.\nI have a hard time parsing this, sorry. Can you elaborate further on what you mean by this? Are you saying this patch will not even be considered, or are you suggesting an alternative patch? What does SQL have to do with this?. > A feature request be made to rack to add socket.alive? proc which enables the application to handle this if it so desires\nWho from the Rack team would be appropriate to tag on this?\nI'll wait for @evanphx to weigh in on the overall direction of this patch.. @evanphx updated with the frozen string change, a check for TCPSocket and to not depend on cover? as doing a straight comparison is faster than Range#cover? for some reason (and because I added TCP_CLOSING), thanks! I verified this patch again with the script linked in the description and it works perfectly still.\nEdit: The CI failure is failing on other branches too, not sure what this is related to\u2014but doesn't seem to be this PR.. Wow, that's very unexpected. Thanks @evanphx for the quick fix.. You can see the definition of the tcp info struct here.. The enum for TCP states is defined here.. Great point, thanks @brotbert. I'll wait with updating this PR as to not lose this comment for others who may want to review.. I haven't done prolonged testing for this, however, if, say, your TCP backlog is 1000 (or whatever) it seems possible that something at the end of the backlog could go through a full closing of the socket while it's still in the backlog. To be honest I'm not entirely sure of the mechanics here in Linux' TCP/IP implementation of whether you \"have\" to accept(2) and close(2) it from the listening socket (kind of like wait(2) on a zombie process), or if you could ever accept a socket that's already in this state. I mostly included it for completeness of socket states we definitely don't care about. I found that we're also missing the TCP_CLOSING (server has buffered data that it hasn't sent yet but the other end has closed). I don't see how this would ever happen in Puma, but it's also not a socket state we're interested in. For the Unicorn patch we'll be running in production in the coming weeks, I may actually patch it so that we print the socket state that we rejected. It'd be interesting to see over a month if it's always CLOSE_WAIT (which it has been in my testing), or if it is ever one of the others. I haven't nerded out with tcpdump and netstat to be completely confident it can ever end up in the other states. If you think it's key, we can certainly go with only CLOSE_WAIT\u2014since it's the only one I've observed in testing.. ",
    "halfbyte": "@nateberkopec Yes, obviously @evanphx pushed the tag (which also pushes the commit attached to it), as otherwise the tag would not even appear in the tags page. \nAnd it looks  like git diff is pretty good at backtracking through the parents to produce a diff. Unfortunately, using the Github-API, it's not so easy to do the same backtracking or to get commits that sit between these two refs or tags. I'm up for suggestions, but I assume that's not really the scope of this issue :)\nMy problem ist this:\nbash\n$ git log --oneline | grep 'f413b6'\n(vs.)\nbash\n$ git log --oneline | grep '7dfc50'\n7dfc50a 3.7.0 (#1185)\nI made some tests, because I was confused as well and it appears that git is totally fine to push a tag to a remote from a branch that doesn't exist on the remote. You'll end up with the situation you're showing above: Something that looks like a valid commit but that isn't actually visible in the main timeline of the master branch (or, actually, any branch for that matter).\n. @nateberkopec Anyway, thanks for looking at this, I know it is somewhat of an esoteric issue.. ",
    "tannerburson": "I have not been able to see the tag change during phased restarts, only when restarting the parent.\nTotally agree that the performance of this is probably worse, but given that it only occurs during worker startup, it shouldn't be a measurable impact on actual web server performance.\nAll that said, it does look like re-setting it in before_fork works. It behaves similarly to setting it as a proc, in that the hook will be re-evaluated before each worker spins up. I'm honestly surprised it works, but definitely won't complain. \nI'll go ahead and close, as it doesn't sound like this is a common use case. I'll open a new issue if I'm able to reproduce and identify the default value for tag not being re-resolved for each worker though.\nThanks.. ",
    "havran": "Same problem here (after capistrano deploy). Seems from_file method is removed here \nhttps://github.com/puma/puma/commit/89f37432deb499ce6cb72a7e7cba5a55430efacd. ",
    "balen": "Same here (after capistrano deploy).. ",
    "aoozdemir": "Probably it is not. Because I'm running Rails under subsystem, which is Linux. But doing Windows tests might be helpful in the future. We can cc someone from WSL, if that helps.. ",
    "renchap": "No, this is a Debian Jessie. Nothing special done on it except upgrading Puma from 3.7.1 (working fine) to 3.8.1.. I dont think nginx supports the 103 early response, leading to this error.\nIf you need a server implementing early hints, you can use https://h2o.examp1e.net. Yes, but not the 103 response code (also known as early hints).. ",
    "corny": "I have the same issue on Ubuntu 16.04 with Ruby 2.3.1p112.. ",
    "rromanchuk": "when deploying after updating, haven't done any real investigation yet. Ubuntu 16.04.2 LTS\n:  2017-03-14 01:05:09 +0000: Read error: #<Errno::EOPNOTSUPP: Operation not supported - getsockopt(2)>\nMar 13 18:05:09  puma:  /var/www/[redact]//shared/bundle/ruby/2.3.0/gems/puma-3.8.1/lib/puma/server.rb:124:in `getsockopt'\nMar 13 18:05:09  puma:  /var/www/[redact]//shared/bundle/ruby/2.3.0/gems/puma-3.8.1/lib/puma/server.rb:124:in `closed_socket?'\nMar 13 18:05:09 puma:  /var/www/[redact]//shared/bundle/ruby/2.3.0/gems/puma-3.8.1/lib/puma/server.rb:563:in `handle_request'\nMar 13 18:05:09  puma:  /var/www/[redact]//shared/bundle/ruby/2.3.0/gems/puma-3.8.1/lib/puma/server.rb:425:in `process_client'\nMar 13 18:05:09  puma:  /var/www/[redact]//shared/bundle/ruby/2.3.0/gems/puma-3.8.1/lib/puma/server.rb:289:in `block in run'\nMar 13 18:05:09  puma:  /var/www/[redact]/shared/bundle/ruby/2.3.0/gems/puma-3.8.1/lib/puma/thread_pool.rb:120:in `block in spawn_thread'. Also anyone reading, be aware, with Restart=always the service will successfully start again, so if you're not paying attention to logs and status codes your restart may be broken and you may not even realize it. . @themilkman @nearapogee noticing similar, also using ActionCable, shared. \nRails 5.2.2\nRuby 2.6.1\npuma 3.12.0\nLet me gather my config (i'm using pretty vanilla systemd + socket) info and do some data collection and i'll report back. I just switched to the alternative forked systemd in order to utilize phased-restarts  with pumactl https://github.com/puma/puma/blob/master/docs/systemd.md#alternative-forking-configuration It was a huge pain, and somewhat fragile compared to the systemd simple/socket I'm going to do a quick sanity test with phased restarts and see how it handles. \nhot restarts were dragggggging causing racktimeouts and gateway timeouts from the loadbalancer with two workers 16 threads, preload_app.  My guess is with preload, and perhaps bad actors preventing connections from being drained, workers cant reboot before timeout threshholds are hit. In fact i wouldn't be surprised if my timeouts were the only thing allowing the restart to complete\nI'll post all me new/old configs in a bit, it's a painstaking process to debug this in production env. . ",
    "kassiansun": "seems related to this commit: https://github.com/puma/puma/commit/baccf80a090ed7ef990a74d4fde2f5512013d1f3#diff-ef4c610f0ce605eb90f2810856394fd5. ",
    "saubi1993": "Thanks . ",
    "axfcampos": "Thank you! \ud83d\ude47 . ",
    "brauliobo": "Thanks @nateberkopec, reported the bug at https://bitbucket.org/ged/ruby-pg/issues/260/frequent-crashes-with-multithreading. ",
    "AndrewRayCode": "Ah, I didn't know you can pass -9 to pkill. That finally gets rid of it.\nAlso, lots of rb-fsevent processes stay open too. I kill everything with:\npkill -9 -f 'rb-fsevent|rails|spring|puma'\n\nOutput when it's stuck in shutdown mode:\n$ ps aux | grep puma\n         3053  98.8  1.0  2757276 161812 s000  R+    9:24PM   0:17.78 puma: cluster worker 1: 3027 [rails-app]\n         3027   0.0  0.5  2560964  77584 s000  S+    9:24PM   0:02.46 puma 3.7.0 (tcp://0.0.0.0:3000) [rails-app]\n         3363   0.0  0.0  2451236   2064 s006  S+    9:30PM   0:00.00 grep puma\n\n. ",
    "viamin": "I'm also seeing puma processes that won't quit. Also, restarting rails servers will cause a worker to spin up the CPUs to 100%. I only started seeing this after upgrading my laptop. \nI've attached a couple of samples from the two cases, which have a couple of similar threads. \npuma_sample_100CPU.txt\npuma_sample_wont_quit.txt\n. ",
    "jvanbaarsen": "I'm noticing the same problem, but only if I have workers enabled in my config.\nThis is my config: https://gist.github.com/fe98e60c5cceff865cd0a9f485bda599\nWhen I comment line 8 it works. Hope this helps a little bit. ",
    "jameskirsop": "I'm also experiencing this when using Oxidized from the CLI.\nI end up having to open up another terminal session to issue a kill command to quit the instance.. ",
    "Machinerium": "I'm also experiencing this when using Open3 on a routine (ping command). ",
    "tadman": "I've encountered this problem as well and it looks like there's really no easy way to stop Puma from patching in with Signal.trap. Having an option to prevent it from doing this would make life a lot more pleasant than having to go to great lengths to prevent the SIGTERM trap from being injected.\nIn development mode graceful shutdown is more trouble than it's worth, so it'd be great if that feature could be disabled via options.. The only way to avoid Puma intercepting SIGINT and trying to be helpful is to patch out the Signal.trap method entirely. It's messy and should be unnecessary, but if anyone needs to keep Puma on a leash, this is how you can do it:\nmodule Signal\n  def self.trap(signal_name)\n  end\nend\nDeclaring that before require \"puma\" will prevent it from hooking in. Note that this may interfere with other things that need to trap but thankfully those situations are rare.\n. ",
    "gconwayoxblue": "Can confirm SIGQUIT worked for me.. ",
    "pedros007": "As of 2014, there is no ability to use a config file via rails server.  You need to use puma directly see also: https://github.com/puma/puma/issues/512#issuecomment-48829088. Just came across this while upgrading a rails-4.2 app to rails-5.0.  Rails-5.0 uses Puma for development by default instead of WebRick.  It also seems that rails s will correctly read config/puma.rb (tested with rails-5.0.6).\nAlso,  the puma CLI reads config/puma.rb by default.  Find some commentary on this from @schneems in Container-Ready Rails 5.. ",
    "ammarshah": "@nateberkopec You just made me cry.. ",
    "toshimaru": "Same here. \nThis was introduced by #1234, which adds 3 different sources of configuration. \nI tried to fix this, but I'm not sure my code is correct:\nrb\nhost = options[:Host] || default_options[:Host] || ::Puma::Configuration::DefaultTCPHost\nport = options[:Port] || default_options[:Port] || ::Puma::Configuration::DefaultTCPPort\nself.set_host_port_to_config(host, port, user_config)\n(This code doesn't care default_config, so I'm not sure my code is correct) . ~Seems this has been fixed in rails-side at Rails 5.1.0.rc2.~ seems not fixed yet \nsee, https://github.com/rails/rails/pull/28513\n$ rails s -b 0.0.0.0\n=> Booting Puma\n=> Rails 5.1.0.rc2 application starting in development on http://0.0.0.0:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.3.3-p222), codename: Sassy Salamander\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop. oh, sorry, I might be using my forked puma gem... It seems not fixed yet.... ",
    "tricknotes": "@schneems \nYou can see the full logs.\n\nCan you give me an example app that reproduces the problem?\n\nhere.\n$ rails s -b 0.0.0.0\n=> Booting Puma\n=> Rails 5.1.0.rc1 application starting in development on http://0.0.0.0:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.3.3-p222), codename: Sassy Salamander\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:9292\nUse Ctrl-C to stop\nI think this puma server listens tcp://0.0.0.0:9292, not http://0.0.0.0:3000.\nIncidentally, this is a just generated rails application by the following command:\n$ rails -v\nRails 5.1.0.rc1\n$ rails new demo. ",
    "matiaskorhonen": "This still seems to happen with Rails 5.1.0.rc2 (not sure who's fault it is, Rails or Puma\u2026):\n\u00bb ./bin/rails s -b 0.0.0.0\n=> Booting Puma\n=> Rails 5.1.0.rc2 application starting in development on http://0.0.0.0:3000\n=> Run `rails server -h` for more startup options\nPuma starting in single mode...\n* Version 3.8.2 (ruby 2.4.1-p111), codename: Sassy Salamander\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:9292\nUse Ctrl-C to stop. Just to confirm, I just tested this on a brand new Rails app generated with Rails 5.1.0.rc2 on Ruby 2.4.1 (on macOS Sierra). The same issue persists..\n",
    "asadakbar": "We are hitting the same issue. Any thoughts as to a fix? For now we are explicitly setting the port in our docker spin up script.\nEdit: Ah, I see the commit. Thanks for all your hard work!. ",
    "xrl": "I'm adding my TimerTask code to a full-up rails app, running with a slightly different puma config:\nLP-XLANGE-OSX:contractresearchmap-beta xlange$ kc logs contractresearchmap-2510290728-3rv49\n[1] Puma starting in cluster mode...\n[1] * Version 3.7.0 (ruby 2.3.1-p112), codename: Snowy Sagebrush\n[1] * Min threads: 8, max threads: 32\n[1] * Environment: development\n[1] * Process workers: 3\n[1] * Preloading application\nspawning timertask\nexecution_interval: 1, timeout_interval: 10\n[1] * Listening on tcp://0.0.0.0:3000\n[1] ! WARNING: Detected 2 Thread(s) started in app boot:\n[1] ! #<Thread:0x0055986e46d7f8@/usr/local/bundle/bundler/gems/semantic_logger-8446da586d18/lib/semantic_logger/processor.rb:7 sleep> - /usr/local/bundle/bundler/gems/semantic_logger-8446da586d18/lib/semantic_logger/processor.rb:139:in `pop'\n[1] ! #<Thread:0x0055986bcceda0@/usr/local/bundle/gems/concurrent-ruby-1.0.5/lib/concurrent/executor/ruby_thread_pool_executor.rb:317 sleep> - /usr/local/bundle/gems/concurrent-ruby-1.0.5/lib/concurrent/synchronization/mri_lockable_object.rb:43:in `sleep'\nInteresting that puma caught the error on that app.. Definitely a me problem! I wasn't respawning the work in the background threads. The semantic_logger project included info on how to reopen the logger and that got me unblocked. Also had to write a config.ru for the sinatra app so I could get this level of config.. ",
    "jacksonrayhamilton": "Added a comment.. Sure, but can you point me to where the test should go?. Ok, added a test.. Thanks for merging this!  Can you make a release?. ",
    "kyle776": "Did you find a solution for this ? I have a similar/same issue.. ",
    "stevenchanin": "@dekellum -- thanks for the very thorough and detailed thinking on this. I'm traveling this week, so I'm not going to have time dig into this until next week when I'm home. But at that point, I'd be happy to cross review and to respond to / incorporate your comments (though I think you may understand systemd much more deeply than I do).. @dekellum I'm sorry for not providing feedback on #1370, at the time I put up this PR, I was feeling my way through getting a systemd + puma + Capistrano setup working and I don't really have enough expertise with either systemd or puma to provide useful feedback. If you think all the valuable stuff has been included, I'm fine with this PR being closed.. ",
    "peteygao": "You can attempt to monkey patch the SIGTERM trap handler and have it log whatever info you think you might need to debug this issue. Here's where it's implemented:\nhttps://github.com/puma/puma/blob/ba7fa8a94f30ded70e4d6077c032f8814d4c49ac/lib/puma/cluster.rb#L263. ",
    "jakemack": "Thanks for the feedback! I'll give it a shot. @seyonv, I haven't had any experience with that issue. We've been using a gem called lograge to keep our Rails logs in check as Rails logs are a bit unwieldy by default. Feel free to check it out if you're using Rails, it might just solve all of your anger over log formatting.\n@peteygao, thanks for the pointer, I've been able to monkey patch successfully so we'll see if it helps with tracking down our problem! I'll close this issue as my question has been answered. ",
    "seyonv": "Somewhat Similar question but more related to formatting. After installing Puma, logs are far uglier and are on multiple lines rather than one HTTP request per line. Don't know if this a result of Puma or another change I made in a configuration setting. The same problem does occur even when booting with Webrick. @jakemack have you had any experience with this issue?. Hope I didn't come across as angry! And the source of the issues was the log entries gem, installed around the same time as I installed Puma. Removing this fixed the issue and now I nice logs while using puma. Thanks for pointing me toward logrsge @jakemack, looked interesting . ",
    "vonchristian": "I am experiencing this also. can you please share your systemd service config?. ",
    "svoboda-jan": "I can reproduce this issue with latest puma using only SSH. It seems to happen only when using SSH with the option -tt (Force pseudo-tty allocation).\nThe -tt (Force pseudo-tty allocation) option is set by default in all mina ssh commands.\nI would recommend something like the following Puma options for Rails applications:\n--redirect-stdout log/production.log --redirect-stderr log/production.log. ",
    "ivoanjo": "@evanphx and @nateberkopec, thanks for looking into this and looking forward to a puma release :). ",
    "waynerobinson": "This will also resolve some potential deadlocking issues for APIs that can sometimes end up calling themselves (i.e. authentication systems that use microservices that need authenticating). \n\ud83d\udc4d\ud83d\udc4d. ",
    "joealba": "@evanphx @schneems Do you recommend this upgrade for apps that are not yet threadsafe?  And does that answer change if the app is also behind the Heroku router?\nIf I have N dynos with X workers each -- and the Heroku router picks a single busy worker in each of my N dynos, would that request fail due to the combination of this PR and a non-threadsafe app?\n(I've tested this theory with ab and 4x higher concurrency than the number of workers, and I saw no failed requests.  But I thought it prudent to ask you folks as well.)\nReferences:\n- https://gist.github.com/schneems/c07d93d5a4ade679bbc3\n- https://devcenter.heroku.com/articles/http-routing#request-distribution\n. ",
    "eprothro": "If I'm understanding this change correctly:\nPreviously\nIf queue_requests was set, puma would eagerly accept connections, irrespective of the availability of worker threads in the pool to process them.\nNow\nPuma will only accept a connection if there is an available worker thread in the pool.\nIn effect, this leaves requests unaccepted (in the socket) instead of accepted and waiting for processing by the worker (\"in\" the queue_requests thread). And ultimately, as stated in the OP, this would result in better load balancing in a multi-worker configuration in certain scenarios.\nAround the water cooler, it seems some are concerned that queue_requests has been deprecated or no longer has any effect.\nTo clarify, my understanding is that there is still benefit in having the queue_requests option, as this allows requests to be received without blocking a worker thread from handling on another request that has already been received. This has not changed, simply how many requests could be in this state (being handled by the queue_requests thread before being handed to a worker thread) has changed. \nIs this correct?. @stereobooster replying here from your ping on #1453:\nShort answer, yes, the Reactor (e.g. queue_requests=true) could certainly be sucking up more requests than it should be.\nSome details:\nThe backlog stat may not be what you think it is. \n\nThe puma 'backlog' stat is NOT the depth of the socket backlog (see link below).\nA puma \"backlog\" of zero does NOT necessarily mean that puma is keeping up with the pressure from the client (the request \"queue\" buffer and/or the socket could have their own backlogs)\nThe puma \"backlog\" stat is only updated once every 5 seconds\n\nhttps://github.com/eprothro/puma/blob/4cad6b449e1dc89ff492752b62f4016b7261869e/docs/architecture.md\nThose things I'm pretty sure of. To be clear, I will refer to the puma backlog stat as the \"work queue\", as to clearly differentiate it from the \"socket backlog\".\nI'm less sure of the rest, but if I'm reasoning about this correctly, as of 3.9: \n\npuma should only accept a request if there is a waiting worker thread\nwaiting worker threads will be signaled immediately when a connection is added to the work queue\n\nSo, in my mind:\n The work queue should essentially always be zero or nearly zero\n  If it's not, there is a bug with Puma (e.g. 1453)\nQuestions:\n What version of puma are you using?\n  * You say above I tested queue_requests false it doesn't change the picture. But I still believe the problem is in how Puma processes queue. This should not be the case as of 3.9. disabling this option should make a drastic difference for a test driving saturation.\n How deep is your socket backlog?\n Are your errors 502's from Nginx? See #1449 \n Are you measuring response time at the nginx plane or the puma/app plane?. Regarding the last question, I was asking about the latency on the Backlog vs response time chart. I assume that is nginx $request_time or $upstream_response_time.\nTo be specific about the places a connection waits along the pipeline:\n\nNgnix waits to establish a connection to the socket\nThis will happen immediately unless the socket backlog is full\nIf socket backlog is full, the connection is refused\nIf puma work thread pool is all-busy, but the socket backlog is non-full, the connection is still established\n\n\nNginx waits for puma to accept the connection\nThe connection is in the socket backlog at this point\nThis happens as soon as there is a non-busy worker\nIf never accepted, the connection will eventually time-out\n\n\nPuma waits for the request to be read from connection\nWith queue_requests enabled, this happens asynchronously from the worker thread pool\n\n\nPuma waits for a worker thread to begin work on the request\nThis should happen immediately as of 3.9\nThis might not happen immediately because of #1453 \n\n\nPuma waits for a response from the app\nPuma waits for the response to be written to the connection\n\nSeeing the time spent in the app (step 5) as distinct from the total time nginx waited (Steps 1-6, plus other items too) could help isolate the issue further and aid diagnosis.. @stereobooster if you want to test your theory, here is a spike at a version of puma that shouldn't overcommit it's worker pool:\nhttps://github.com/eprothro/puma/tree/eprothro-14353-fix\nI would NOT use this in production, yet, for the record ;). @nateberkopec fixed, thanks.. Update: I played with this a bit with nginx and jmeter (fast and slow clients), tweaking when puma decides to accept new connections. After being out of the weeds for a bit, the thoughts that remain:\n\n\nDue to how the thread pool, server, and reactor are coupled, an implementation fully addressing this would probably not be very clean without a non-trivial refactor.\n\n\nTwo different kinds of connections build up in the Reactor queue\n\nSlow clients that we are waiting to read request from (definitely producing more work soon) \nKeep-alive connections that we are checking (may or may not produce more work soon)\n\nA Reactor refactor that threats these differently comes to mind (slight \ud83d\udc40  regarding https://github.com/puma/puma/pull/1403#issuecomment-325760655):\n Allows tuning of \"overcommit\", that is, how many requests we allow to be \"waiting for read\" that don't have associated, available worker threads. \n  * #1278 attempted to make this \"overcommit\" zero, but fails in certain scenarios, as mentioned.\n Delegates handling of keep-alive connections elsewhere so we don't have to consider this \"pool\" in the context of work that will definitely need to get done.\n\nIn theory, a reverse proxy bound to by Puma over a unix socket mitigates both of these concerns. In practice, issues like #1405 and throughput with extremely-slow-clients-behind-nginx scenarios pique my curiosity as to whether we're configured as optimally as we think we are.. @jf great question (and was mine too before digging into this).\n\nHigh level explanation of \"not ready\" yet is that the HTTP request has not come over the wire completely yet. In other words, the connection is established but we can't start work on the request yet (hand off to web application) because we haven't read all the bytes of the request yet.\nThere are many scenarios, some where this probably wont happen (simple GET) and some scenarios where this might happen (POST with a substantial body and a very slow client).\nThis is over-simplified since there are a lot of variables here (socket type, nginx config, etc), but that's a high-level explanation of my understanding.. @HoneyryderChuck I remember thinking that too and was surprised when I contrived a local scenario that proved me wrong.\nFrom the first comment:\n\nHowever, recently I was surprised to learn that accepted connections are placed in the thread pool work queue prior to getting placed \"in\" the Reactor (which would then put them back in the work queue when ready).\nI assume this is because in the common case and/or with a fast client (e.g. certain reverse proxy configurations), connections will be ready for read immediately and not need the trip to the Reactor and back into the work queue. This makes sense.\n\nSo if this is unchanged, and I was right to begin with (neither guaranteed!), slow clients may be able to fill the reactor with more requests than the pool size.\nMy instinct was/is the same as yours, this is probably not a significant problem in the real world.  On the other hand, I and others have seen some failure profiles that make me scratch my head...\n\nIn theory, a reverse proxy bound to by Puma over a unix socket mitigates both of these concerns. In practice, issues like #1405 and throughput with extremely-slow-clients-behind-nginx scenarios pique my curiosity as to whether we're configured as optimally as we think we are.\n\n. ",
    "yhirano55": "Thanks for reviewing. I got it. . ",
    "senorcinco": "I've also tried several iterations of:\n--with-openssl-include=/usr/include/openssl\n--with-openssl-lib=/usr/lib64\n--without-cryptolib\nThey all result in the same basic error:\nchecking for BIO_read() in -lcrypto... *** extconf.rb failed ***. ",
    "crigor": "I'm able to reproduce the issue ie \"You have to install development tools first.\" on centos 7 (using docker image centos:7). I only installed ruby and ruby-devel at this point.\nAfter installing a few more packages, I was able to install puma.\nyum install make automake gcc gcc-c++ kernel-devel\nThis is roughly the equivalent of build-essential on Debian/Ubuntu. https://unix.stackexchange.com/questions/1338/what-is-the-fedora-equivalent-of-the-debian-build-essential-package\n[root@1356186a625a /]# gem install puma\nBuilding native extensions.  This could take a while...\nSuccessfully installed puma-3.9.1\nParsing documentation for puma-3.9.1\nInstalling ri documentation for puma-3.9.1\n1 gem installed\n[root@1356186a625a /]# gem install puma -- --with-cppflags=-I/usr/include/openssl --with-ldflags=-L/usr/lib64\nBuilding native extensions with: '--with-cppflags=-I/usr/include/openssl --with-ldflags=-L/usr/lib64'\nThis could take a while...\nSuccessfully installed puma-3.9.1\nParsing documentation for puma-3.9.1\n1 gem installed\n. ",
    "ccarruitero": "I think it is already fixed in master with https://github.com/puma/puma/pull/1277. ",
    "fmauNeko": "We don't, the problematic function, DH_set0_pqg, was added only to OpenSSL 1.1.0, and not to LibreSSL, we just need to check whether we're using LibreSSL. ",
    "ernie": "Popped in here to say this one worked for me today, only to find it just got merged. Thanks!. ",
    "rafmagana": "This works for me. When is this going to be released?. ",
    "lucascartaxo": "I feel I've been having the same issue here.\nIn order to display some custom error pages, I've added a lowlevel_error_handler block to handle errors, such as\nruby\nlowlevel_error_handler do |error|\n  puts \"lowlevel_error_handler is being invoked\"\n  [500, { 'Content-Type': 'text/html' }, File.open('public/500.html')]\nend\nIn order to test it, I forced an exception in my ApplicationController, such as\n```ruby\nclass ApplicationController < ActionController::Base\n  before_filter :throw_error\ndef throw_error\n    throw \"error!\"\n  end\n```\nHowever, the lowlevel_error_handler was never invoked, even though the >>> setting lowlevel error handler on [...]\" debug is being printed when I launch my app\nAfter a lot of digging and by following the debug steps proposed by @jjb , the only way I could call the lowlevel_error_handler block was by manually thowing an exception around here, forcing Puma to rescue this Exception\nTo me, the lowlevel_error_handler block should work like a charm out of the box, but unfortunately that is not the case \ud83d\ude22 \nIs there any other configuration that need to be set up in order to proper call the lowlevel block?\n@jjb how are you testing and/or ensuring your lowlevel_error_handler is being called?. That's what I was afraid of. But it makes sense, though. A lowlevel_error is not a high level error in the application.\n@nateberkopec could you provide some more info about those \"particular cases internal to Puma\"?\nSince my case is not the same as @jjb , I'll slowly retreat.\nThanks for all the help.. ",
    "GBardis": "Do you want to see what my  gemfile has inside ?\n. I found a solution to this problem . The problem was that puma gem was missing from gemfile.lock so i deleted the file and run bundle install again and the new gemfile.lock has the gem inside and everything came back to normal . ",
    "noahgibbs": "So then you'd prefer two separate routes, maybe /gc and /gc-stats?\n. Okay. I've added a second route. Works for me locally. I see a bunch of CI failures, but no clear sign they're related to my PR. Did I break that, or was it already broken?\n. Huh. I \"closed\" this by pushing different commits to my repo. Hang on a sec...\n. There are some tests in test/test_cli.rb, but currently no test for /gc or /gc-stats. I'll try to put something like that together quick.\nAlso, huh, I'm finding that the tests (\"bundle exec rake test\") don't terminate for me locally. They take a very long time to run, possibly forever. At first I thought my new test-in-progress did that. But then I removed it, and nope, it still runs forever.\nOkay, I'll look into this more. But I'm not sure I'm going to be able to usefully run the tests locally, even after reverting all the changes I made.. Okay, nope. I'll try to find a way to debug this test locally, then rejigger the commits and open a new PR.. Some of this is clearly subtle enough that I'm not going to fully understand and reflect it here. So: take my comments with a grain of salt. And some of it I'm restating just to make sure I understand it.\nI agree on approach 1 that there doesn't seem to be a good mechanism to get the information you want from TCP (pending connections without accepting them) or other highly-relevant metrics (wait time before acceptance, requests that were rejected due to queue size.) And while you could maybe reimplement enough to do that, ew, don't. There are some attempts in that direction (Goliath, ZeroMQ.) But I don't see them, or that approach, helping you.\nHm. So approach 2 would do more work in the parent process (hard to scale), use more memory, have more of a single point of failure, and then have to pass the full-size request from master process to worker. In return, it would have one obvious place to handle backpressure and slow requests.\nRight now, based on #1563, it sounds like you have a choice of a hard-to-reason-about working algorithm, or a simpler, easier-to-explain method that doesn't work well. You tried switching to the simpler method, then reverted it. Am I understanding that correctly?\nLater the parent process of approach #2 might be a good choice for Guilds - multiple threads with multiple VM locks. But of course those aren't available yet.\n. I hope Discourse is threadsafe... I'm using it configured with a lot of Puma threads...\n. ",
    "FooBarWidget": "You sure this is only a JRuby bug? I remember running into macOS kernel bugs that look similar to this. Sometimes when you connect to a TCP server right after creating it, you get connection refused. Maybe a good idea to see whether you can replicate this on Linux.. It looks like the dlopen workaround is the best solution in terms of fewest side-effects. OBJC_DISABLE_INITIALIZE_FORK_SAFETY is too broad and affects child processes.\nRegarding @ticky's comment about structural changes in Ruby: I think this goes beyond Ruby itself. Forking is fundamentally problematic when there is multithreading involved. We can make it work, and in the context of Ruby app servers it usually works well so far, but that's in spite of a lack of real solutions.\nTo guarantee that forking is safe, the application must not be running any threads at the point of fork (or the other threads must only be executing async-signal-safe code). MRI can guarantee this about its own code to some degree, but third party native libraries may spawn their own threads. So any gems that the user pulls in should not spawn threads until the app server has forked. Gems with multithreaded native extensions should provide an explicit thread initialization method, which the app only calls after the app server has forked. This requires cooperation in the entire gem ecosystem.\nDoes something like the dlopen workaround belong in Puma/Unicorn/iodine/Passenger? This is more of a philosophical question than a technical one. I cannot answer for Puma/Unicorn/iodine, but Passenger's philosophy is put user experience at the forefront, so we have chosen to include the dlopen workaround in the next Passenger version.. I like the proposal of adding a modification to Ruby.\nIt is true that native libraries can call fork without going through the Ruby fork function. Having said that, I believe that such libraries already ensure that they perform async signal safe actions inside the child. And if they don't then I think that it should be up to them to fix that, or up to the  wrapping gem to work around that, because those libraries would already cause problems even outside the context of Ruby. \nIt is also true that Ruby can be embedded in another environment. But Ruby forking in such environment is at present already a problem. I think it should be up to the embedder to add mechanisms to make Ruby's fork call safe, for example with pthread_atfork.\nSo I support the notion of having Ruby initialize the objc runtime upon calling #fork.. I've published a blog post on this subject. This blog post explains:\n\nWhat the problem is and why it occurs\nThe current community concensus on what to do about this\nWhat users can expect in the short term from various parties such as app servers authors\nWhat they can do themselves\n\nHopefully this post brings more clarity to users.. @nateberkopec I think fixing it in Ruby only affects versions of Ruby from a specific version onward. What about users on older versions? Would it be reasonable to ask them to upgrade Ruby or would it be better if you activate the workaround on older Ruby versions?\n\nSince v2.1, Puma warns the user if it detects additional Ruby threads created during preloading.\n\nThis is... an awesome idea! This will solve many of the mysterious issues people have with preforking.. ",
    "henrik": "Might it be the same issue as this one? https://github.com/puma/puma/pull/1206. I added the stopgap gem to our Gemfile, bundled and deployed it yesterday, but I just saw the error again today. The same error as initially described, on the @notify.close line. Puma 3.9.1, Ruby 2.4.1.. Puma 3.10.0 has been released and includes this fix.. ",
    "vesan": "Just letting you know that we are seeing the same problem with puma 3.8.2 & Rails 5.0.2. I don't think we have anything special in the application that would cause it but you'll never know.\nWe also have quite low amounts of traffic (the application is still in development).. ",
    "mezis": "@NickolasVashchenko: the error does persist with MRI HEAD as of writing:\n...vendor/bundle/ruby/2.5.0/gems/puma-3.9.1/lib/puma/server.rb:393:in `close': stream closed in another thread (IOError)\n(after receiving a SIGTERM, in certain circumstances)\nHere's a travis build where this error regularly happens, 4 out of 5 builds, both on MRI 2.4.1 and HEAD. I don't have a reliable way to reproduce this yet I'm afraid, so the build is mostly FWIW.. ",
    "krishna-rpx": "I got the same issue in Puma 3.10.0\nems/puma-3.10.0/lib/puma/server.rb:395:in `close`\ngems/puma-3.10.0/lib/puma/server.rb:395:in `ensure in handle_servers`\ngems/puma-3.10.0/lib/puma/server.rb:398:in `handle_servers`. No, I am not using stopgap gem. Ruby version ruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]\nPlease find full backtrace here.  \nIOError: stream closed\n [GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:395 :inclose[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:395 :inensure in handle_servers[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:398 :inhandle_servers[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:327 :inblock in runNested Exceptions\nIOError: stream closed\n [GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:395 :incloseIOError: stream closed\n [GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:391 :inwrite[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:391 :inrescue in handle_servers[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:400 :inhandle_servers[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:327 :inblock in runIOError: stream closed\n [GEM_ROOT]/gems/puma-3.10.0/lib/puma/reactor.rb:210 :injoin[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:388 :inhandle_servers[GEM_ROOT]/gems/puma-3.10.0/lib/puma/server.rb:327 :inblock in run``. ",
    "msyvr": "Relevant note on https://github.com/rails/rails/issues/28971#issuecomment-301613719 by micahbf:\nThe issue occurs in relation to missed whitespace after -p.\nI.e., all clear if -p 4000; issue if -p4000. ",
    "lkananowicz": "@nateberkopec Thanks for the answer. Sure, CDN is almost a must here. How does puma handle static assets serving, is it serving them directly or leaves that job for Rails (or other framework)?. ",
    "romdim": "I've solved it for now by requiring puma 3.6.2 as suggested here: https://github.com/reevoo/jenkins_statistics/commit/de1f9bbc86b46afb346e03358ad33b56119806ba\nand while waiting for this: https://github.com/puma/puma/pull/1285 to be pushed. ",
    "ioquatix": "Thanks @nateberkopec \nWe may be able to utilize the NIO reactor into async, if that's the design Puma ultimately goes through with.\nOne reactor per thread is ideal - it maximises throughput and resource utilisation with minimal contention. In what way do you think it wouldn't work?. There are several strategies for high-performance TCP servers.\n1/ Have one or more threads calling accept (each with their own reactor). They can either directly process the request (might be hard to load balance) or\n2/ Have a pool of threads for handling reading requests/writing responses.\nYou can pass IO from 1 to 2.\nBecause Ruby has a GIL, it might be ideal if 1 is the parent process and 2 are worker processes. You can send file descriptors between processes, or you can proxy the requests from parent to child processes directly but there will be increased overhead in this case.\nThe simplest most isolated approach is just to have a bunch of workers all running and handling requests. Each one accepting new requests and running their own reactor. I'm working on a quick test case async-http which will include a server Async::HTTP::Server. I'm trying to do a quick POC of this approach. It should easily handle 10k connections.. I bet @tarcieri would have some good feedback/ideas too.. I'm not sure how your proposed system works. All asynchronous work needs to be within a reactor.. Oh right, sure well in that case you are (artificially) limiting your throughput. With an async reactor there is no upper bound on the number of in-flight requests per reactor, except that you will eventually become CPU bound, which is why you'd have multiple workers. The design is much simpler too.. The other great thing is that the reactor worker essentially runs with a single thread. Therefore, you don't need any thread synchronisation.. 1. in order to gain true asynchronous behaviour, you need to feed your requests into a reactor and process all user code. This way, if the user code calls out to, say, an RPC, it will yield until the result is ready. That way, other requests can be processed at the same time.\n\nyes.. I'm not sure what's not clear here but I'll do my best to explain it again.\n\nLet's say the user writes code like so:\nruby\nclass Middleware\n  def call(env)\n    RestClient.get(...) # will block entire worker process until finished\n  end\nend\nSo, the problem above is that IO within the context of a Puma worker is currently blocking. Correct me here if I'm wrong.\nWe can magically transform the above code to non-blocking, provided it's called within the scope of a reactor:\nruby\nAsync::Reactor.run do\n  middleware.call(env)\nend\nHowever, if you run this, Async::Reactor.run will block until the middleware is complete. The critical step comes when you realise you can do the following:\nruby\nAsync::Reactor.run do |task|\n  100.times do\n    task.async do\n      middleware.call(env)\n    end\n  end\nend\nThese 100 tasks will complete, more or less, in the same time as 1.\nThe Async::Reactor needs to wrap requests to the users code.\nYou can almost do it by using Rack's hijack API, but it's not quite right - it's a limited design suited for things like WebSockets but not for asynchronous processing - that's where async.callback made more sense, but even that design needs to be a bit better.\nThe following is non-functional but semantically equivalent to the correct solution - a rough idea of how it would look.\nruby\nAsync::Reactor.run do |task|\n  request = read_request\n  task.async do\n    response = app.call(request.env)\n    write_response(response)\n  end\nend\n(doesn't correctly track user session, for keep-alive, etc)\n. Thanks for taking the time to look at the project. I appreciate your thoughts.\n\nOk, I went and looked at async. It appears to mostly be a wrapper around Fiber, is that right? If so, that's a no-go since Fiber on JRuby isn't really performant.\n\nYep, that's right. Yes, JRuby can't implement Fibers properly. I'm aware of it and I'm aware that the design of Async doesn't work well with JRuby at the current time. It's not something that bothers me. My experience with JRuby has been that it's painfully slow. I like the idea of JRuby. In practice though, it's a fairly horrible, fracturing experience.\nPerformant isn't actually an English word. I'm not trying to be rude :). It confuses two very different things. I assume you mean both efficient and performs well. I'll address these separately.\n\n\nFrom an efficiency POV: The cost is 4kb memory for the stack per Fiber, and some (minor) processor overhead for yield/resume operations. I've also heard that MRI does a syscall per resume but I haven't looked into that - that would suck. So, it's got O(n) memory usage proportional to the number of in-flight requests and O(n) processor overhead, proportional to the number of blocking calls being made.\n\n\nFrom a performance POV: The overhead of a thread compared to a fiber is fairly significant. Threads require significantly more memory and processor time to spin up (hence the usage of thread pools rather than just spinning them up as needed). Threads (pre-emptively scheduled) are good for processor bound work, while fibers (co-operatively schedueld) are good for I/O bound work. MRI threads pretty much suck though - the GIL blows away almost every opportunity for concurrency.\n\n\nAs you've said, JRuby implements fibers inefficiently using threads. JRuby fibers are essentially the same performance as threads. So, if your choice is \"Thread per request\" or \"Fiber per request\", it's not all that much different. But on MRI and Rubinius it's a totally different picture.\nAsync is a young project. I have yet to focus my wand of optimisation at it. However, you may ask, why use fibers at all? There is an answer. Fibers transparently capture the stack. It doesn't matter what other APIs you've walked through, if you end up calling a blocking operation, it will be correctly yielded and resumed. Because of this, existing APIs can work without knowing that the underlying IO is non-blocking. The amortised cost-per-request of using Fibers so far seems reasonable but is something I'd like to investigate further.\nFinally, the alternative is callback hell. If you are implementing your own application, and you can capture all your intermediate state, callback hell may be suitable. If you don't leak these details out of your application, it might be okay. But as soon as you try to implement IO, and you start exposing that to the world, callbacks begins to infect everything.\nFibers are not that different from async/await - except we don't have async/await (AA) in Ruby and it's impossible to implement efficiently. Fibers are actually a super-set of AA, since AA modifies a specific function to be a small state machine. Fibers capture that state by saving the stack. AA functions capture that state by tracking per-function-call state machine values. I'm actually not sure why AA is more popular since it's sort of a crappy version of Fibers, and it's basically as invasive as callback hell.\n\nWhat I guess I miss is how you intend for for a Fiber to give up control back to your scheduler? For instance, when RestClient blocks, waiting for data, how do you switch Fibers to another one?\n\nThat's the magic of async-io which includes non-blocking wrappers around blocking IO operations.\nWhen RestClient eventually calls @io.read, and @io is an instance of Async::IO::Socket or one of the other wrappers, it will yield control back to the reactor and another operation may resume.. Right, and my question wasn't how to add this to puma, but how to integrate it on that basis e.g. as a plugin. Is it possible to supply a block which runs around worker threads, for example?. Also, I was just thinking, you'll never achieve C10K (with reasonable resource usage) without either AA, callback hell, or Fibers. My understanding is that a Ruby thread has 256KB of overhead - 10,000 of them is 2.5GB of ram :D. > The amount of things that can go wrong\nActually, my goal for this is zero. It should be completely transparent.. > I thought it was 8? Where did you get 256kb?\nIt really depends on the platform, how many pages are allocated. Due to how Linux manages memory, it may or may not be backed by actual RAM until it is used. The number is what I saw floating around on the internet. I've seen other numbers, like 1MB per thread on windows, for example.\n\nWe all try to ship bug-free software, but that doesn't stop us from writing bugs \ud83d\ude06\n\nThis isn't about bugs - it's about the design being largely transparent from the user, which isn't the case for AA or callback hell.\n\nI don't think the reactor is pluggable right now. I mean, it's Ruby, you can undefine Puma::Reactor and put your own in there. Not sure if that would work for you.\n\nI thought about doing this, it's an interesting idea. to just utilize parts of Puma and build a server out of that. In the end, I'm more interested in a POC at this point. So, I'll just continue to implement async-http. My first goal is C10K for a web server. I've already got that working in async-io.. FYI:\n\nOn Linux/x86-32, the default stack size for a new thread is 2 megabytes. ... Using pthread_attr_setstacksize(3), the stack size attribute can be explicitly set in the attr argument used to create a thread, in order to obtain a stack size other than the default.\n\nRuby sets it here:\nhttps://github.com/ruby/ruby/blob/daf14f713fdb2edc3572f4a73eab4bc97509f34f/thread_pthread.c#L1020\nNot sure what the actual value is, it's 1AM so I don't feel like digging any further.. So, I thought it might be of interest here, on my linux desktop with 4 cores:\n\n27,000 requests/s using ab, which is individual non-keep-alive connections (what you'd expect typical web browsers to do when hitting the server for a single page).\n100,000 requests/s using wrk, which uses keep-alive (what I hope nginx to do when proxying requests).\n\nThis is using one reactor per process, one process per cpu core.\nI'm going to push the code to async-http soon, but it's mostly just proof of concept and cobbled together with no real optimisation. However, I think it's interesting that with this approach, C10K is well and truely surpassed.. On another note, I did some trivial experimentation with 2x processes per core. It was worse by about 10-20%. It might be CPU affinity issues, or it might be something else that I'm not aware of. In any case, this is a very artificial benchmark. Once running any kind of actual workload, I suspect the most we can get is 1000-2000 requests/s per core. I'll try to hook up a real rack app to my cobbled together server and see if the whole thing doesn't fall apart entirely, it should be interesting. One side effect of this approach is that thread safety is no longer a concern since there is only ever one thread/process invoking the rack middleware (of course that doesn't mean that all concurrency issues are non-existent).. @evanphx just a question, but what kind of req/s do you get from puma, if the app is simply something like [200, {}, [\"Hello World\"]] - what's the optimal config - one worker per cpu core?. I think if you run my benchmark on this hardware you'd probably get 1m req/s max but again I haven't profiled anything or done and optimizations.. I don't think any answer to my original question has been provided, so I'll just assume it's not possible.. Was this ever integrated into the Rack SPEC? I had a look but couldn't see anything on the latest master branch.\nWhat was the final design of the API?\nI'm implementing this in falcon and support H2 server push directly so it would be fun to compare.. I implemented the design that was merged into puma.\nhttps://github.com/socketry/falcon/commit/248d0a458294914f0d694b26540afccfea7b5b18\nHere is my brief feedback:\n\nIt's very HTTP/1 centric (i.e. using headers). For HTTP/2, I need to parse the headers and turn that into push requests. I guess it's okay. I don't know how I'd handle other headers correctly.\nI only add the lambda to the rack env when it's possible to send early hints. So, it's conditionally enabled. Is that okay? Because I don't bother to support 103 Early Hints yet, but only support H2 push promises.\nI wonder if exposing this in the application layer is really a good idea. I feel like a sufficiently intelligent server could figure out what things to send and when, and additionally cache what the client has/has not received.\n\nIn summary: is this interface too generic? Would it be better to limit it to push-promise functionality, or is it good to have it allow any header?. I wrote up a brief summary regarding implementing this feature.\nhttps://www.codeotaku.com/journal/2019-02/falcon-early-hints/index. I seem to start experiencing this with 2.6.0-rc1 on travis - maybe an odd coincidence?\nhttps://travis-ci.com/socketry/falcon/jobs/163846367 - scroll to the end.. Let me see if I can reproduce the issue in 2.6.1. ",
    "yosiat": "@nateberkopec If I want to try and start implementing it can you give me pointers?. @waghanza this is RESUEADDR which is different than REUSEPORT. @waghanza I think the problem with the results is that puma creates single TCPServer in master process and in order to efficiently utilize port sharding we need each puma worker to create its own TCPServer instance.\nWe can do simple POC by running two pumas in benchmarks in a single mode.\nWDYT?. ",
    "waghanza": "is it not already implemented at https://github.com/puma/puma/blob/master/lib/puma/binder.rb#L277 ?. @yosiat agreed\nSO_REUSEPORT is behaving the same way, but only on windows\nhttps://stackoverflow.com/questions/14388706/socket-options-so-reuseaddr-and-so-reuseport-how-do-they-differ-do-they-mean-t\nmy bad. @yosiat I have started PR #1712, feel free to contribute. @BanzaiMan\nFYI, CI act differently on PR build and branch build\n\nfails on http://travis-ci.com => https://travis-ci.com/waghanza/puma/builds/98481123\nsucceed on http://travis-ci.org => https://travis-ci.org/puma/puma/jobs/483942792. @MSP-Greg I think SO_REUSEPORT should NOT be activated by default, but actionable with a command line switch. @yosiat To be honest, I'm unhappy with those results, I though SO_REUSEPORT would have a stronger impact\n\nWith ruby 2.5.3p105 and wrk 4.1.0, I have \n\nsinatra (with reuse port)\n\nRunning 10s test @ http://127.0.0.1:3000\n  2 threads and 10 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.06ms    1.58ms  39.25ms   90.09%\n    Req/Sec     7.68k     2.21k   11.43k    55.50%\n  152741 requests in 10.00s, 25.05MB read\nRequests/sec:  15269.50\nTransfer/sec:      2.50MB\n\nsinatra (without reuse port)\n\nRunning 10s test @ http://127.0.0.1:3000\n  2 threads and 10 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     1.13ms    1.76ms  36.44ms   90.56%\n    Req/Sec     7.49k     2.16k   11.73k    60.00%\n  149001 requests in 10.00s, 24.44MB read\nRequests/sec:  14896.09\nTransfer/sec:      2.44MB\n\nrails (with reuse port)\n\nRunning 10s test @ http://127.0.0.1:3000\n  2 threads and 10 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.50ms    4.08ms  41.76ms   84.66%\n    Req/Sec     2.23k   343.50     2.90k    63.00%\n  44295 requests in 10.00s, 7.39MB read\nRequests/sec:   4428.74\nTransfer/sec:    756.86KB\n\nrails (without port reuse)\n\nRunning 10s test @ http://127.0.0.1:3000\n  2 threads and 10 connections\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.85ms    4.76ms  56.01ms   85.26%\n    Req/Sec     2.21k   648.49     3.34k    53.50%\n  43952 requests in 10.00s, 7.34MB read\nRequests/sec:   4394.02\nTransfer/sec:    750.93KB\n. @yosiat After reflexion, I do not think their will be a great performance gap. ",
    "dguettler": "@grosser 3.9.1 still seems to be failing for me\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.3.1-p112), codename: Private Caller\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\n* Restarting...\nPuma starting in single mode...\n* Version 3.7.0 (ruby 2.3.1-p112), codename: Snowy Sagebrush\n* Min threads: 5, max threads: 5\n* Environment: development\n! Unable to load application: Gem::LoadError: You have already activated puma 3.7.0, but your Gemfile requires puma 3.9.1. Prepending `bundle exec` to your command may solve this.\nIt tries to use globally installed version.\nWhen uninstalling global versions I'm getting\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.3.1-p112), codename: Private Caller\n* Min threads: 5, max threads: 5\n* Environment: development\n* Listening on tcp://0.0.0.0:3000\nUse Ctrl-C to stop\n* Restarting...\n/Users/dguettler/.rvm/rubies/ruby-2.3.1/lib/ruby/2.3.0/rubygems/dependency.rb:319:in `to_specs': Could not find 'puma' (>= 0.a) among 142 total gem(s) (Gem::LoadError). ",
    "bughit": "@grosser \n\nFYI you'd be restarting with the same Gemfile/Load-paths\n\nThat does still seem to be the case, but why?  BUNDLE_GEMFILE is no longer present in the new puma process but the $LOAD_PATH is still wrong/old. I just ran \"bundle exec puma\"\nlib/puma/launcher.rb:171\nruby\nenv[\"RUBYOPT\"] = [env[\"RUBYOPT\"], bundle].join(\" \") unless env[\"RUBYOPT\"].include?(bundle)\nthis code expects RUBYOPT to be always set, when puma is run via bundler, is that a justified assumption?  It wasn't set for me.. ```\nbundle exec ruby -e 'puts ENV[\"RUBYOPT\"]'\n-rbundler/setup\nruby -v\nruby 2.2.2p95 (2015-04-13 revision 50295) [x86_64-linux]\nbundle -v\nBundler version 1.15.1\nbundle exec rails -v\nRails 4.2.8\n(bundle exec) puma -v\nNoMethodError: undefined method `include?' for nil:NilClass\npuma is from master (5bbe6f3ba610)\nuname -a\nLinux alexolinux 4.4.0-83-generic #106~14.04.1-Ubuntu SMP Mon Jun 26 18:10:19 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n``. here's the order of relevant events in abundle exec puma` invocation\n\nbundler captures the ORIGINAL_ENV\n    ruby\n    module Bundler\n      environment_preserver = EnvironmentPreserver.new(ENV, EnvironmentPreserver::BUNDLER_KEYS)\n      ORIGINAL_ENV = environment_preserver.restore\n    at this point there is no RUBYOPT in the environment\nRUBYOPT is set by bundler in Bundler::CLI::Exec#run\nwithout BUNDLE_DISABLE_EXEC_LOAD (which is the default), 'vendor/bundle/ruby/2.2.0/bin/puma\" is loaded in-process\npuma expects ORIGINAL_ENV[\"RUBYOPT\"] to be present (in lib/puma/launcher.rb:171 env refers to ORIGINAL_ENV)\n\nIt's not clear to me how this works for anyone, since puma invocation in a sub-shell is not the default behavior of bundle exec.\n. minor issue:\nIt's probably better not to mutate Bundler::ORIGINAL_ENV but call Bundler::original_env that clones it, or dup it yourself if the constant predates the method.\nruby\n    def run\n      previous_env =\n        if defined?(Bundler)\n          env = Bundler::ORIGINAL_ENV\n          # add -rbundler/setup so we load from Gemfile when restarting\n          bundle = \"-rbundler/setup\"\n          env[\"RUBYOPT\"] = [env[\"RUBYOPT\"], bundle].join(\" \") unless env[\"RUBYOPT\"].include?(bundle)\n          env\n        else\n          ENV.to_h\n        end\n. not sure if you'd want to support this scenario, but if puma is started with BUNDLE_DISABLE_EXEC_LOAD=1 bundle exec puma Bundler::ORIGINAL_ENV will already be tainted with BUNDLE_GEMFILE. @stereobooster \nthe investigation is here: https://github.com/puma/puma/issues/1344#issuecomment-313186304. @stereobooster @nateberkopec \nMy investigation (https://github.com/puma/puma/issues/1344#issuecomment-313186304) showed that in a bundle exec puma invocation if puma resolves to a ruby script rather than say a bash rbenv shim, then this puma ruby script will be loaded in process and Bundler::ORIGINAL_ENV[\"RUBYOPT\"] will not be set.  In fact nothing set by the following will be in ORIGINAL_ENV\nruby\ndef set_bundle_environment\n      set_bundle_variables\n      set_path\n      set_rubyopt\n      set_rubylib\nend\nIs that not sufficient to proceed with the fix?  Or are there other expectations/assumptions for what's in ORIGINAL_ENV than need to be dealt with?. ",
    "glebtv": "none of the options and workarounds worked for me on 3.9.1, had to rollback to 3.8.2 which works fine. ",
    "ksamc": "@stereobooster tried your simplified patch as well but with no luck.\n```\nruby -v\nruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]\ngem -v\n2.6.11\nbundle -v\nBundler version 1.15.1\nrails -v\nRails 5.1.2\nuname -a\n4.4.0-83-generic #106-Ubuntu\n```. ",
    "tagliala": "Same here. Workaround didn't help, downgrading to 3.8.2 works\n```\nruby -v\nruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]\ngem -v\n2.6.12\nbundle -v\nBundler version 1.15.1\nbundle exec rails -v\nRails 5.1.2\nuname -a\nLinux  3.10.0-514.el7.x86_64 #1 SMP Tue Nov 22 16:42:41 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n```. Hi everyone \ud83d\udc4b \nI'm getting the same issue when upgrading from Ruby 2.5.3 to Ruby 2.6.0\nI think there is something wrong in my configuration, but any help/hint/suggestion would be appreciated\nRuby 2.6.0\nJan 04 14:28:14 machine systemd[1]: Started Puma HTTP Server (app staging).\nJan 04 14:28:14 machine systemd[1]: Starting Puma HTTP Server (app staging)...\nJan 04 14:28:15 machine rvm[15524]: /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/binder.rb:65:in `for_fd': Bad file descriptor - not a socket file descriptor (Errno::EBADF)\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/binder.rb:65:in `block (2 levels) in import_from_env'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/binder.rb:63:in `times'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/binder.rb:63:in `block in import_from_env'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/binder.rb:57:in `each'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/binder.rb:57:in `import_from_env'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/launcher.rb:52:in `initialize'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/cli.rb:69:in `new'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/lib/puma/cli.rb:69:in `initialize'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/bin/puma:8:in `new'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/gems/puma-3.12.0/bin/puma:8:in `<top (required)>'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/bin/puma:23:in `load'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/bin/puma:23:in `<main>'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/bin/ruby_executable_hooks:24:in `eval'\nJan 04 14:28:15 machine rvm[15524]: from /var/www/app/shared/bundle/ruby/2.6.0/bin/ruby_executable_hooks:24:in `<main>'\nJan 04 14:28:15 machine systemd[1]: puma_app_staging.service: main process exited, code=exited, status=1/FAILURE\nJan 04 14:28:15 machine systemd[1]: Unit puma_app_staging.service entered failed state.\nJan 04 14:28:15 machine systemd[1]: puma_app_staging.service failed.\nJan 04 14:28:16 machine systemd[1]: puma_app_staging.service holdoff time over, scheduling restart.\nJan 04 14:28:16 machine systemd[1]: start request repeated too quickly for puma_app_staging.service\nJan 04 14:28:16 machine systemd[1]: Failed to start Puma HTTP Server (app staging).\nJan 04 14:28:16 machine systemd[1]: Unit puma_app_staging.service entered failed state.\nJan 04 14:28:16 machine systemd[1]: puma_app_staging.service failed.\nRuby 2.5.3\nJan 04 14:30:33 machine systemd[1]: Started Puma HTTP Server (app staging).\nJan 04 14:30:33 machine systemd[1]: Starting Puma HTTP Server (app staging)...\nJan 04 14:30:35 machine rvm[21579]: * Pruning Bundler environment\nJan 04 14:30:35 machine rvm[21579]: [21579] Puma starting in cluster mode...\nJan 04 14:30:35 machine rvm[21579]: [21579] * Version 3.12.0 (ruby 2.5.3-p105), codename: Llamas in Pajamas\nJan 04 14:30:35 machine rvm[21579]: [21579] * Min threads: 0, max threads: 16\nJan 04 14:30:35 machine rvm[21579]: [21579] * Environment: staging\nJan 04 14:30:35 machine rvm[21579]: [21579] * Process workers: 4\nJan 04 14:30:35 machine rvm[21579]: [21579] * Phased restart available\nJan 04 14:30:35 machine rvm[21579]: [21579] * Activated unix:///var/www/app/shared/tmp/sockets/puma.sock\nJan 04 14:30:35 machine rvm[21579]: [21579] Use Ctrl-C to stop\n. ",
    "vedant1811": "\n@grosser https://github.com/stereobooster/ruby-server-experiment/tree/master/puma-symlink-grosser\nUPD: @grosser why do you prefer USR2 over USR1 restart?\nPS simplified patch\nrequire 'bundler/setup'\non_restart do\n  ENV.replace(Bundler.clean_env)\nend\n\n\n@stereobooster the error is different:\nAs mentioned here, fIx is for:\nuninitialized constant #<Class:#<Puma::DSL:0x0055883f37e188>>::Bundler (NameError)\n\nbut the error I (and probably others) am getting is:\n`find_spec_for_exe': can't find gem puma (>= 0.a) (Gem::GemNotFoundException)\n\nInfo: \n$ ~/.rvm/bin/rvm ruby-2.4.1 do ruby -v\nruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]\n\n$ ~/.rvm/bin/rvm ruby-2.4.1 do gem -v\n2.6.11\n\n$ ~/.rvm/bin/rvm ruby-2.4.1 do bundle -v\nBundler version 1.15.1\n\n$ ~/.rvm/bin/rvm ruby-2.4.1 do bundle exec rails -v\nRails 5.1.2\n\n$ uname -a                                                                                                                                                                                            \nLinux ip-172-31-30-115 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\nOkay, using puma-3.8.1 is the clean and simple workaround.\nThanks @dguettler and @tagliala.. ",
    "dgutov": "The above fix doesn't seem likely to solve the problem reported in the first comment (with dependencies not being found), so forgive me if I'm not testing it.\nInstead of downgrading, though, we've settled on the --restart-cmd \"bundle exec puma\" workaround here. Works quite well.. @grosser  That works. But it works even if I use\nrb\ngem 'puma', git: 'https://github.com/grosser/puma.git', branch: 'master'\ninstead of your fix branch.. And this example fails with 3.9.1 here. Looks like a good news.. ",
    "kyontan": "I also reproduced this using with sinatra. (also 3.8.2 works well)\nsh\n$ ruby -v\nruby 2.4.0p0 (2016-12-24 revision 57164) [x86_64-darwin16]\n$ gem -v\n2.6.8\n$ bundle -v\nBundler version 1.15.3\n$ uname -a\nDarwin PC103323.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64\n$ rbenv -v\nrbenv 1.1.1-2-g615f844\nUsing with alexch/rerun\n$ rerun --restart --signal SIGUSR2 -- bundle exec puma\n...snip...\n20:02:23 [rerun] Change detected: 1 modified: app.rb\n20:02:24 [rerun] App restarting\n20:02:24 [rerun] Sending signal SIGUSR2 to 17364\n* Restarting...\n/usr/local/Cellar/ruby/2.4.0/lib/ruby/2.4.0/rubygems.rb:270:in `find_spec_for_exe': can't find gem puma (>= 0.a) (Gem::GemNotFoundException)\n        from /usr/local/Cellar/ruby/2.4.0/lib/ruby/2.4.0/rubygems.rb:298:in `activate_bin_path'\n        from /.../app/vendor/bundle/ruby/2.4.0/bin/puma:22:in `<main>'\n. It seems fixed at https://github.com/puma/puma/commit/a771ec3655f7ee7a0e0b620c1cadd0c1a1650aff !\nWhat's the reason of this issue? . ",
    "BrandonMathis": "Nice work on this fix!\nI would +1 the suggestion to do a patch level release. Not being able to restart seems like quite a major defect for puma 3.9. ",
    "wangyuan99": "I was creating new rails applications, and start rails server, everything was working. Then I started creating new applications, then when i start rails server, I got that error message. All new application I created afterward, got that error message. Then  I tried copy the previous working application, the copied application works fine too.. yeah, working now. thanks very much!. ",
    "tasos31": "@nateberkopec  Worked perfectly, thanks.. ",
    "palkan": "Oh, sorry, didn't noticed that one(. Done!. Wow, for some reason it uses Rubocop 0.49( Even though I set ~>0.50.0 in Gemfile. That's strange.\nCould you, please, try to restart the job without cache?. Oh, my bad, didn't notice that 2.1 has its own gemfile. Updated. ",
    "mraj-rpx": "@acearth: If content-length header is not present then puma client considers the content(After empty CRLF) as another chunk of header and trying to parse using HttpParser and hence parser throws Invalid HTTP format.\nFrom your code body content body_info=hello&body_title=showMe is not a http/1.0 or 1.1 request format and hence parser is raising this error. Also content-length is mandatory for http/1.0 or 1.1.\nOf course we can improve the parser to raise actual error message, I am looking at the right way. Please share if you have some suggestions.. ",
    "etscrivner": "Can confirm that this is an issue, and has been blocking an upgrade for some time.. ",
    "wodka": "Hi, with the applied changes (3.10) I get an error on heroku during a restart:\n\nis this intended (in terms of me having to catch the exception somehow)?. ",
    "rickpeyton": "I have the same issue.\nweb_1  | 2017-07-24 22:58:10 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:140760FC:SSL routines:SSL23_GET_CLIENT_HELLO:unknown protocol - 336027900>\nweb_1  | 2017-07-24 22:58:18 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408A10B:SSL routines:SSL3_GET_CLIENT_HELLO:wrong version number - 336109835>\nweb_1  | 2017-07-24 22:58:26 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408A0C1:SSL routines:SSL3_GET_CLIENT_HELLO:no shared cipher - 336109761>\nweb_1  | 2017-07-24 22:58:33 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408A0C1:SSL routines:SSL3_GET_CLIENT_HELLO:no shared cipher - 336109761>\nweb_1  | 2017-07-24 22:58:41 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408A0C1:SSL routines:SSL3_GET_CLIENT_HELLO:no shared cipher - 336109761>\nweb_1  | 2017-07-24 22:58:41 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:140A1175:SSL routines:SSL_BYTES_TO_CIPHER_LIST:inappropriate fallback - 336204149>\nweb_1  | 2017-07-24 22:58:48 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408A10B:SSL routines:SSL3_GET_CLIENT_HELLO:wrong version number - 336109835>\nweb_1  | 2017-07-24 22:58:57 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac - 336130329>\nweb_1  | 2017-07-24 22:58:57 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac - 336130329>\nweb_1  | 2017-07-24 22:58:57 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac - 336130329>\nweb_1  | 2017-07-24 22:58:57 +0000: SSL error, peer: 69.58.181.15, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac - 336130329>\nLooks fine in the browser and https://cryptoreport.websecurity.symantec.com/checker/ does not report any problems, but my logs are blowing up with the error above.\nAny thoughts? My Google-fu failed me.. Thanks @jbeyer05 I was hoping to avoid nginx for this little project, but I will do that too. Thanks again!. ",
    "jbeyer05": "@rickpeyton I resorted to using nginx as a reverse proxy.  Wish I had another solution.. ",
    "Bodacious": "Having this same issue today trying to serve a Rails app locally with SSL. Any ideas?. ",
    "fortunate-man": "I'm having the same issue with a Rails app I'm trying to serve publicly on a domain with SSL; doesn't work locally either. Any help?. I ended up using Passenger to support SSL; works beautifully.. ",
    "hseljenes": "This usually means there was an issue loading your crt or key file. In my case I accidentally mapped them into my containers as empty directories instead of files. Double check the user running puma can read your files and that the paths are correct.. ",
    "ViliusLuneckas": "@stereobooster, I forked puma more than a month ago when the master was failing. Merging latest commits from the master, tests should pass now.. This PR fixes the process which loads puma config file based on RACK_ENV. The issue is well written in #1327. Meanwhile 0584a3345e9d7ecd84494abc6ba7971309b5a338 fixes some local problem which might or not be fixed with this PR. To be honest \u2013 no idea \ud83d\ude15 . Hi @allaire, could you try version 3.11. I think the fix (#1340) was released with that version.. ",
    "tarciosaraiva": "@nateberkopec thanks for the response. The tool in question is htop.\nI did upgrade but got the same results. Not sure if it's a puma thing or a Rails thing, probably gotta investigate a bit further.\nAnyway, will do a bit more digging.\nThanks! :+1: . ",
    "wojtodzio": "I can also confirm this issue - after upgrading Puma from 3.8.2 to 3.9.1 I'm not able to start the server without bundle exec.\nAs a temporary quick fix, I've added this line:\nruby\nBundler::ORIGINAL_ENV['RUBYOPT'] ||= ENV['RUBYOPT']\nTo my config/puma.rb \ud83d\ude04. ",
    "unak": "@nateberkopec We cannot promise when next versions of Ruby will be release, but I expect its in next few months (it's only my expectation, not official).\nBut, we also cannot promise that the fix for https://bugs.ruby-lang.org/issues/13632 will be included or not. In my feeling backporting the fix to 2.3 may be too difficult.. @nateberkopec Now I'm testing the patch for https://bugs.ruby-lang.org/issues/13632 on Ruby 2.3 branch, and it seems to be no problem.  So, Ruby 2.3.5 will contain the fix.. ",
    "salimane": "any update on this issue ? we are having the same problem. ",
    "jeffling": "Hi, Any update here? . ",
    "rekha014": "https://github.com/rekha014/sawariya_kisan....\nError logs \n[crit] 32144#0: *2 connect() to unix:/home/deploy/sawariya_kisan/shared/tmp/sockets/puma.sock failed (2: No such file or directory) while connecting to upstream, client: 122.175.206.55, server: localhost, request: \"GET / HTTP/1.1\", upstream: \"http://unix:/home/deploy/sawariya_kisan/shared/tmp/sockets/puma.sock:/500.html\", host: \"34.211.143.161\". ",
    "hiteshranaut": "@rekha014 are you able to solve this issue?. ",
    "EssenKh": "I got the same error  in nginx log errors . ",
    "Nikita2k": "@stereobooster @nateberkopec  thank you both for a quick answer. \nI'm trying to have two sockets and that is what I achieve with that config:\ndeploy    2370  0.0 23.1 771736 115672 ?       Sl   Jun29   0:11 puma 3.8.2 (unix:///home/deploy/apps/app1/shared/tmp/sockets/app1-puma.sock) [20170629172103]\ndeploy    2390  0.0 16.1 764560 80672 ?        Sl   Jun29   0:09 puma 3.7.1 (unix:///home/deploy/apps/app2/shared/tmp/sockets/app2-puma.sock) [20170629172111]\nJust not sure why puma tries to start each instance 4 times =/. ",
    "agargiulo": "Same issue shows up in Ruby 2.3.4 p301. Oh awesome, I thought I tried everything but I forgot [::] \nThanks!. ",
    "batasrki": "@nateberkopec the unix address for the upstream isn't wrong. I did remove the slashes, as per advice, to no effect.\nAs you can see in the initial log, the Rails app receives the request proxied by nginx. nginx then times out waiting for the response from upstream. The Rails app completes in a few hundred milliseconds, so it's not a slow upstream client. The only other software sitting in between is puma.\nPlease re-open this ticket and try the reproduction steps.. @nateberkopec further to the point, I just created an empty Rails app, that has nothing in it other than single controller that does not hit the database.\nEverything else is the same, same version of all the things listed above. Same pattern too\nnginx logs\n```\n==> /var/log/nginx/nginx.demo.toyapp.com.error.log <==\n2017/07/08 14:26:27 [error] 27182#27182: *10 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 99.234.241.234, server: demo.toyapp.com, request: \"GET / HTTP/1.1\", upstream: \"http://unix:/opt/company/toyapp/tmp/sockets/puma.sock/\", host: \"demo.toyapp.com\"\n==> /var/log/nginx/nginx.demo.toyapp.com.access.log <==\n99.234.241.234 - - [08/Jul/2017:14:26:27 -0400] \"GET / HTTP/1.1\" 504 585 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36\"\n```\nrails log\nI, [2017-07-08T14:25:27.295651 #27512]  INFO -- : Started GET \"/\" for 99.234.241.234 at 2017-07-08 14:25:27 -0400\nI, [2017-07-08T14:25:27.801902 #27512]  INFO -- : Processing by HomesController#index as HTML\nI, [2017-07-08T14:25:28.262647 #27512]  INFO -- :   Rendered homes/index.html.erb within layouts/application (9.6ms)\nI, [2017-07-08T14:25:28.288006 #27512]  INFO -- : Completed 200 OK in 484ms (Views: 74.4ms | ActiveRecord: 0.0ms). This pretty much isolates puma as the issue. Do re-open the ticket and take a look, please..  Hey @nateberkopec, is this what you are looking for, https://github.com/batasrki/nginxconf. @nateberkopec in this commit, I've added the default nginx.conf as installed by RHEL. The only thing changed is to point to sites-enabled conf files, https://github.com/batasrki/nginxconf/commit/4ea769389809728da21985045da710f3161eebab. The problem still happens in the exactly the same manner.. @nateberkopec any news here? I've added a default nginx.conf as requested.. @nateberkopec here's a stock nginx.conf for Ubuntu 16.04. Same timeout issue happens there.\n```\nuser  nginx;\nworker_processes  1;\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\nevents {\n    worker_connections  1024;\n}\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\nlog_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                  '$status $body_bytes_sent \"$http_referer\" '\n                  '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\naccess_log  /var/log/nginx/access.log  main;\nsendfile        on;\n#tcp_nopush     on;\nkeepalive_timeout  65;\n#gzip  on;\n#include /etc/nginx/conf.d/*.conf;\ninclude /etc/nginx/sites-enabled/*.conf;\n\n}\n```\nI'm not sure why you are ignoring this.. One thing I noticed in your \"works for me\" example is that you aren't daemonizing the puma process. I have observed that passing the -d flag starts up 2 JVM processes and eventually one ends. I think this is some sort of forking going on. This also occasionally fails to complete, resulting in a hang on startup, but that's a different issue.. It's a JRuby problem, @poc7667. See the #1375 for details on how to solve it.. Re-ran strace with -f while attached to the top-level process.\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 206749490}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 206799155}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 206818404}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 206843419}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 256843419}, ffffffff <unfinished ...>\n[pid  5175] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5175] futex(0x7faf3cbd6f28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 246895233}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 246932228}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 246953979}) = 0\n[pid  5175] futex(0x7faf3cbd6f54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 346953979}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 257049120}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 257084612}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 257102560}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 257120513}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 307120513}, ffffffff <unfinished ...>\n[pid  5163] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5163] futex(0x7faf3c00ae28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5163] gettimeofday({1500599516, 210907}, NULL) = 0\n[pid  5163] gettimeofday({1500599516, 210944}, NULL) = 0\n[pid  5163] gettimeofday({1500599516, 210961}, NULL) = 0\n[pid  5163] clock_gettime(CLOCK_MONOTONIC, {4309, 297966337}) = 0\n[pid  5163] futex(0x7faf3c00ae54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 497966337}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 307465827}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 307559538}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 307633866}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 307704580}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 357704580}, ffffffff <unfinished ...>\n[pid  5166] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5166] futex(0x7faf3c074728, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5166] clock_gettime(CLOCK_MONOTONIC, {4309, 327452436}) = 0\n[pid  5166] futex(0x7faf3c074754, FUTEX_WAIT_BITSET_PRIVATE, 1, {4310, 327452436}, ffffffff <unfinished ...>\n[pid  5175] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5175] futex(0x7faf3cbd6f28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 347316020}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 347406694}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 347482742}) = 0\n[pid  5175] futex(0x7faf3cbd6f54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 447482742}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 358071259}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 358167345}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 358206858}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 358288359}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 408288359}, ffffffff) = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 408613435}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 408704464}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 408770016}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 408840526}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 458840526}, ffffffff <unfinished ...>\n[pid  5175] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5175] futex(0x7faf3cbd6f28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 447843797}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 447942619}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 447999253}) = 0\n[pid  5175] futex(0x7faf3cbd6f54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 547999253}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 459677089}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 459941142}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 460213465}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 460467902}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 510467902}, ffffffff <unfinished ...>\n[pid  5163] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5163] futex(0x7faf3c00ae28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5163] gettimeofday({1500599516, 411317}, NULL) = 0\n[pid  5163] gettimeofday({1500599516, 411408}, NULL) = 0\n[pid  5163] gettimeofday({1500599516, 411497}, NULL) = 0\n[pid  5163] clock_gettime(CLOCK_MONOTONIC, {4309, 498552073}) = 0\n[pid  5163] futex(0x7faf3c00ae54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 698552073}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 510835498}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 510922649}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 511000625}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 511384457}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 561384457}, ffffffff <unfinished ...>\n[pid  5175] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5175] futex(0x7faf3cbd6f28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 548666055}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 548771309}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 548858273}) = 0\n[pid  5175] futex(0x7faf3cbd6f54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 648858273}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 561625074}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 561667557}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 561691969}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 561719366}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 611719366}, ffffffff) = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 611950287}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 611994292}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 612046145}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 612073941}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 662073941}, ffffffff <unfinished ...>\n[pid  5175] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5175] futex(0x7faf3cbd6f28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 649117990}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 649172398}) = 0\n[pid  5175] clock_gettime(CLOCK_MONOTONIC, {4309, 649204160}) = 0\n[pid  5175] futex(0x7faf3cbd6f54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 749204160}, ffffffff <unfinished ...>\n[pid  5173] <... futex resumed> )       = -1 ETIMEDOUT (Connection timed out)\n[pid  5173] futex(0x7faf3c0bce28, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 662345424}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 662392770}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 662526773}) = 0\n[pid  5173] clock_gettime(CLOCK_MONOTONIC, {4309, 662554595}) = 0\n[pid  5173] futex(0x7faf3c0bce54, FUTEX_WAIT_BITSET_PRIVATE, 1, {4309, 712554595}, ffffffff\nThis is the entire output continuously repeated. It looks like it's a infinite loop.. @nateberkopec I have located the issue and it's JRuby. I've submitted the ticket as you can see above. . @kares thank you very much! This works. Is there a ticket on JRuby OpenSSL project for this?. ",
    "matthijskooijman": "Not sure if you understood my meaning. IIUC that command will still bind to port 80 (or whatever is the default), right? And if I specify a different port in systemd, this bit of code will not match the systemd port to the requested port 80, and end up trying to open port 80 and then close the port systemd opened.\nAs for the usecase that promted this: I was running the rails server, which binds to port 3000 by default. I'm using systemd socket activation to let systemd open up port 80 (so rails/puma does not need root to open a port < 1024). However, to let puma actually use this port passed in from systemd, I have to pass --port 80 to rails, so rails requests the same port systemd offers. I was hoping to do something like --port use-any, or something like that, so it will just use the systemd socket, regardless of which port/listen address it has.. Agreed!. ",
    "wesley6j": "I am also having this problem with puma 3.9.1. The problem seems to be pumactl restart sometimes directly start puma in non-daemon mode.\nI tried the following command: \nbash\n$HOME/.rbenv/bin/rbenv exec bundle exec pumactl -S /home/myuser/apps/myapp/shared/tmp/pids/puma.state -F /home/myuser/apps/myapp/shared/puma.rb restart\nSometimes, it works with the following output: Command restart sent success.\nBut the next time, it starts puma attached to the terminal:\nPuma starting in single mode...\n* Version 3.9.1 (ruby 2.4.1-p111), codename: Private Caller\n...\nUse Ctrl-C to stop\nThe website still works. But I have to send CTRL-C to exit from server, then puma is stopped.. ",
    "dkniffin": "I am also seeing this issue with pumactl not restarting the server correctly on v3.9.1. I found #563, which seems to be the same issue, but that's old. This looks like a new issue in recent versions, maybe\nWhen I run bundle exec pumactl --config-file config/puma.production.rb restart I get Command restart sent success, but it just stops puma; it doesn't start it up again. If I run the command again, I get:\n[3821] Puma starting in cluster mode...\n[3821] * Version 3.9.1 (ruby 2.4.0-p0), codename: Private Caller\n[3821] * Min threads: 1, max threads: 6\n[3821] * Environment: preproduction\n[3821] * Process workers: 1\n[3821] * Phased restart available\n[3821] * Listening on unix:///.../file.sock\n[3821] * Daemonizing...\nEdit: I just checked and both 3.8 and 3.10 seem to work fine.\n. ",
    "mabdelfattah": "I've the same problem! did any of you solve it or should I move to passenger?. ",
    "jimgrimmett": "I saw a similar problem. My initial setup was to have the rails project sitting in my windows drive and then use puma-dev pointing at /mnt/c/rails_project.\nFile serving seemed to be slow so I rsync'd the project over inot my WSL home directory and then saw the error you were seeing above.\nMy solution - perverse as it is - was to symlink the windows tmp folder into my rails folder.\nrm ~/rails-project/tmp\nln -s /mnt/c/rails-project/tmp ~/rails-project/tmp\nThen puma-dev works.\nI can only guess there's something special about unix sockets, WSL and mounted C drives!\nSo my suggestion would be to create a tmp directory on your windows drive, link it as I have done and try that.\nHTH. ",
    "CptBreeza": "@nateberkopec Is there any way to solve the problem temporarily?I have no idea how this error occured.. Sorry. No progress.. ",
    "bongole": "@stereobooster  Thank you for your review. I rebased to the latest master.. ",
    "brian-kephart": "Thanks @NickLaMuro. I've created an issue in the Rails repo.. ",
    "kares": "\"Ruby-0-Thread-1@puma 002: /opt/jruby/lib/ruby/gems/shared/gems/puma-3.9.1-java/lib/puma/thread_pool.rb:74\" #13 daemon prio=5 os_prio=0 tid=0x00007faf3cd68000 nid=0x1438 runnable [0x00007faf04452000]\n   java.lang.Thread.State: RUNNABLE\n    at java.io.FileInputStream.readBytes(Native Method)\n    at java.io.FileInputStream.read(FileInputStream.java:255)\n    at sun.security.provider.SeedGenerator$URLSeedGenerator.getSeedBytes(SeedGenerator.java:539)\n    at sun.security.provider.SeedGenerator.generateSeed(SeedGenerator.java:144)\n    at sun.security.provider.SecureRandom.engineGenerateSeed(SecureRandom.java:139)\n    at java.security.SecureRandom.generateSeed(SecureRandom.java:533)\n    at org.bouncycastle.jcajce.provider.drbg.DRBG.createBaseRandom(Unknown Source)\n    at org.bouncycastle.jcajce.provider.drbg.DRBG.access$100(Unknown Source)\n    at org.bouncycastle.jcajce.provider.drbg.DRBG$Default.<clinit>(Unknown Source)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n    at java.lang.Class.newInstance(Class.java:442)\n    at org.jruby.ext.openssl.SecurityHelper.findImplEngine(SecurityHelper.java:766)\n    at org.jruby.ext.openssl.SecurityHelper.getImplEngine(SecurityHelper.java:741)\n    at org.jruby.ext.openssl.SecurityHelper.getSecureRandom(SecurityHelper.java:389)\n    at org.jruby.ext.openssl.SecurityHelper.getSecureRandom(SecurityHelper.java:379)\n    at org.jruby.ext.openssl.Random$Holder.getSecureRandom(Random.java:69)\n... waiting for /dev/random as the system feels its low on entropy ... unfortunately JRuby already had several issues of those. we will need to figure out something in next JRuby-OpenSSL with the new BC generator we ended up having by doing a security upgrade. luckily, this time you still get first class oss support :)\nexport JRUBY_OPTS=-J-Djava.security.egd=file:/dev/urandom .. enjoy!. @batasrki yes - the issue is known (and not that simple to resolve) no need to open any more tickets.\n.... already a bit overflown by having to manage around JVM's low entropy, thanks. sorry about that I removed the comment - was half asleep and did not notice the approval first.\nprobably as they changed the UI style and there wasn't a green mark next to your comment.\nand thank you for the review.. not really, except for some byte copy-ing ... maybe slightly less mem use (not sure what the buff is used for). haven't seen the failures before but if they all look like this one than its very likely smt to do with RubyGems and/or Bundler. there were issues with platform gems before, the new jossl 0.10.2 did not change that much in terms of gemspec its pretty much ~ as 0.10.1 (and previous versions). \nmaybe a decent resolution would be not to update the jruby-openssl default gem, just remove : \ngem \"jruby-openssl\", :platform => \"jruby\"\n... from the Gemfile esp. since you're only testing recent JRuby versions (was necessary before 9K). which is exactly the same behaviour as puma has using ByteArrayOutputStream#reset ... thus such a debate would be for later (ideally reported in an issue if you feel its really a problem). . ",
    "pat": "Thanks Nate :). ",
    "prashantvithani": "The issue might be similar to https://github.com/puma/puma/issues/1308. @schneems: Could this be related to puma_worker_killer?. Tried that as well, the problem is still there. ",
    "coldnebo": "Actually, this commit (which unifies two rake subtasks into one) seemed to fix that issue:\n```\n$ bundle exec rake ci\n/DONOTDELETE/shared/rvm/rubies/ruby-2.3.3/bin/ruby -S rspec spec/httpi/adapter/base_spec.rb spec/httpi/adapter/curb_spec.rb spec/httpi/adapter/em_http_spec.rb spec/httpi/adapter/excon_spec.rb spec/httpi/adapter/http_spec.rb spec/httpi/adapter/httpclient_spec.rb spec/httpi/adapter/net_http_persistent_spec.rb spec/httpi/adapter/net_http_spec.rb spec/httpi/adapter/rack_spec.rb spec/httpi/adapter_spec.rb spec/httpi/auth/config_spec.rb spec/httpi/auth/ssl_spec.rb spec/httpi/cookie_spec.rb spec/httpi/cookie_store_spec.rb spec/httpi/error_spec.rb spec/httpi/httpi_spec.rb spec/httpi/request_spec.rb spec/httpi/response_spec.rb spec/integration/curb_spec.rb spec/integration/em_http_spec.rb spec/integration/excon_spec.rb spec/integration/http_spec.rb spec/integration/httpclient_spec.rb spec/integration/net_http_persistent_spec.rb spec/integration/net_http_spec.rb\n.........................................................................................................................................................................................................................................................D, [2017-08-01T15:13:21.867062 #21125] DEBUG -- : Server does not support NTLM/Negotiate. Trying NTLM anyway\n......................................................................................................................................................................................................\nPending:\n  HTTPI::Adapter::HTTPClient settings: proxy have should specs\n    # Not yet implemented\n    # ./spec/httpi/adapter/httpclient_spec.rb:72\n  HTTPI::Adapter::HTTP http requests supports chunked response\n    # Needs investigation\n    # ./spec/integration/http_spec.rb:71\nFinished in 42.26 seconds\n449 examples, 0 failures, 2 pending\nRandomized with seed 9756\n[Coveralls] Outside the CI environment, not sending data.\n```\nI assume that the original issue was trying to run two instances of the integration server from two Rake subtasks in parallel.\nClosing.. I spoke too soon.  While the issue is fixed on my local, it still occurs in Travis.. It's not fixed on my local either... it seems to be intermittent, not sure why that would be.\nPuma is invoked by Rspec before and shutdown by after:\n```ruby\n  if RUBY_PLATFORM =~ /java/\n    pending \"Puma Server complains: SSL not supported on JRuby\"\n  else\n    context \"https requests\" do\n      before :all do\n        @server = IntegrationServer.run(:ssl => true)\n      end\n      after :all do\n        @server.stop\n      end\n  it \"works when set up properly\" do\n    request = HTTPI::Request.new(@server.url)\n    request.auth.ssl.ca_cert_file = IntegrationServer.ssl_ca_file\n\n    response = HTTPI.get(request, adapter)\n    expect(response.body).to eq(\"get\")\n  end\nend\n\nend\n```. The integration server's self-signed certs seem to work ok:\n$ openssl verify -CAfile spec/integration/fixtures/ca_all.pem spec/integration/fixtures/server.cert\nspec/integration/fixtures/server.cert: OK. @nateberkopec the error is intermittent and happens in Puma.  We need assistance to figure out what this means:\nException handling servers: OpenSSL error: error:00000001:lib(0):func(0):reason(1) - 1 (Puma::MiniSSL::SSLError)\n/home/travis/.rvm/gems/ruby-2.3.0/gems/puma-2.3.2/lib/puma/minissl.rb:32:in `read'\n/home/travis/.rvm/gems/ruby-2.3.0/gems/puma-2.3.2/lib/puma/minissl.rb:32:in `read_nonblock'\n/home/travis/.rvm/gems/ruby-2.3.0/gems/puma-2.3.2/lib/puma/client.rb:142:in `try_to_finish'\n/home/travis/.rvm/gems/ruby-2.3.0/gems/puma-2.3.2/lib/puma/client.rb:212:in `eagerly_finish'\n/home/travis/.rvm/gems/ruby-2.3.0/gems/puma-2.3.2/lib/puma/server.rb:137:in `block in run'\n/home/travis/.rvm/gems/ruby-2.3.0/gems/puma-2.3.2/lib/puma/thread_pool.rb:92:in `block in spawn_thread'\nWhat does this error actually mean?\nAnd we get this afterwards:\nNet::HTTP::Persistent::Error:\n       too many connection resets (due to Net::ReadTimeout - Net::ReadTimeout) after 7 requests on 38597740, last used 60.479127685 seconds ago\n. @nateberkopec hmm, if it was misconfigured, it should happen consistently, not intermittently, no?\nI was hoping that more details from the underlying libs might shed some light on the specific problem we are running into, but barring that, I still suspect that it has something to do with rspec starting two or more IntegrationServers in parallel.\nConceptually, I can't see much difference between starting two puma servers vs. one, except maybe the server listen address bind?  Or perhaps our IntegrationServer wrapper has some singleton state that isn't threadsafe... this is where I have to dig into the actual details a bit more.\nI'll keep looking on my end.  Thanks!. Logging my investigations for posterity\nProblem solved on my end. It seemed to be a combination of two issues:\n\nOpenSSL error: a bit of a red-herring that only happened on Puma::Server.stop in a spec intentionally testing a misconfigured SSL connection.\nNet::HTTP::Persistent::Error: too many connection resets: when POSTs in the underlying adapter had too long a timeout set.\n\nHere's the details for the interested:\nfirst (non) issue\nThe Exception handling servers: OpenSSL error issue was a red herring.  Looking closely at the Travis CI logs, this error is being raised by the Puma thread pool straight to STDERR, so it doesn't actually register as an error within Rspec.\nNext, I added some logging and discovered that error always happens after the test case where the certs are intentionally not setup correctly, duh!  As @nateberkopec said, the error did refer to incorrectly setup SSL:\nruby\n      it \"raises when no certificate was set up\" do\n        expect { HTTPI.post(@server.url, \"\", adapter) }.to raise_error(HTTPI::SSLError)\n      end\nWhen this spec runs, the error is raised from HTTPI as expected (client-side), but when the Puma::Server is stopped, there is an intermittent chance of generating the OpenSSL error (server-side) as it tries to gracefully shutdown.  Looking at the Travis log, the server-side error is logged to STDERR directly by the Puma worker thread as it shuts down, so it doesn't actually register in the rspec thread as an error.\nsecond (real) issue\nThe second issue is caught by rspec as an error:\n1) HTTPI::Adapter::NetHTTP http requests executes POST requests\n     Failure/Error: response = HTTPI.post(@server.url, \"<some>xml</some>\", adapter)\n     Net::HTTP::Persistent::Error:\n       too many connection resets (due to Net::ReadTimeout - Net::ReadTimeout) after 7 requests on 38597740, last used 60.479127685 seconds ago\n     # ./lib/httpi/adapter/net_http_persistent.rb:19:in `perform'\n     # ./lib/httpi/adapter/net_http.rb:47:in `block in request'\n     # ./lib/httpi/adapter/net_http_persistent.rb:24:in `do_request'\n     # ./lib/httpi/adapter/net_http.rb:38:in `request'\n     # ./lib/httpi.rb:161:in `request'\n     # ./lib/httpi.rb:133:in `post'\n     # ./spec/integration/net_http_persistent_spec.rb:40:in `block (3 levels) in <top (required)>'\nThe default timeout in Net:HTTP::Persistent is 5, and the docs recommend lowering it to avoid resets on POST.  Setting them to 1 seems to have solved the actual issue.\n```ruby \n    it \"executes POST requests\" do\n      request = HTTPI::Request.new(url: @server.url, open_timeout: 1, read_timeout: 1, body: \"xml\")\n  response = HTTPI.post(request, adapter)\n  expect(response.body).to eq(\"post\")\n  expect(response.headers[\"Content-Type\"]).to eq(\"text/plain\")\nend\n\n```\nCase closed! :)\n. ",
    "oklaiss-gpsw": "We're seeing the same issue with Puma 3.6.0 with and without Puma Worker Killer 0.1.0. ",
    "BlackNightFury": "Gracefully shutting down workers...\nPuma server is not shutting down. And it is stucking with this message.\nPuma version is 3.11.3 and ruby version is 2.2.6\nThis is kinda urgent problem! So please help me out, then it will be appreciated.\nThanks. @perlun Thanks for your response.\nSo we have been using OpenSSL and our project needs strong security.\nWe planned to move from thin to puma because of concurrency issue but we also could see that Puma is using MiniSSL not OpenSSL.\nSo can you guarantee that MiniSSL is same as OpenSSL?. @perlun Thanks for your answers.\nThen how can I define certificate stores using MiniSSL? OpenSSL provides OpenSSL::X509::Store.\nBut I couldn't find this one.. I am facing the same problem. Is anyone here to solve this problem?. @jainankita90 Can you try with the new version? I think it will fix your issue. Try with 3.12.0. @benoittgt Thanks for your attention. But this project is very confidential and we don't use even github.\nI already shared puma configuration. It works with Passenger with no problem.\nBut it is hanging with Puma. I think this is because of minissl problem.\nBut don't know how to figure it out. It is working with http puma fine but not working over ssl.\n. ",
    "ashu210890": "This is still happening with Puma version 3.11.0 and Ruby version 2.4.2p198.. ",
    "dlanileonardo": "I have the same problem.\nPuma (3.11.3-java), jruby 9.1.16.0 Ubuntu 16.04. ",
    "hoffm": "@nateberkopec The Lord's work is never done.. ",
    "dnd": "@nateberkopec is there anything like DebugLocks for rails v4, or is it only v5? Also #1320 notes that SIGINFO is BSD only. is there a way to get that information from puma on linux?. @nateberkopec thanks. Instead of using a signal, since pickings are slim is this something the control server could have an endpoint for and return? I'm not familiar enough with it to know if that's possible.. ",
    "nitzanav": "Hi,\nthe config/puma2.rb had active_record and sinatra/activerecord gems dependency which caused the Name Error. I happened to remove those dependencies from the entire app.\nAnd I managed to use the same and default puma.rb file cross all runtime of the app (I added \"if\" based on the environment variables\").\nSo, I managed to solve it, I personally don't need the fix.\nBut you might want to fix it anyway since bundle exec puma -C config/puma2.rb config2.ru still doesn't work.\nso if you like I can verify it once the bug is fixed.. ",
    "99cm": "I run into similar error:\npuma start\n[10574] Puma starting in cluster mode...\n[10574] * Version 3.11.2 (ruby 2.5.0-p0), codename: Love Song\n[10574] * Min threads: 2, max threads: 8\n[10574] * Environment: development\n[10574] * Process workers: 4\n[10574] * Phased restart available\n[10574] ERROR: No application configured, nothing to run\n. The same question \"Failed to open TCP connection to localhost:9200 (Connection refused - connect(2) for \"localhost\" port 9200)\" answered by stackoverflow.com,  suggest changing the transport.tcp.port:\nnetwork.host: 127.0.0.1\ntransport.tcp.port: 9300\nhttp.port: 9200\nhttps://stackoverflow.com/questions/42526394/failed-to-open-tcp-connection-to-localhost9200-connection-refused-connect2\nHowever, in my case, I don't have elasticsearch.yml, setting remote elasticsearch url (amazon aws) with puma, configuring port 9200. I can't solve it.\nthank you for your time to read and answer.\n. ",
    "blelump": "With version 3.12.0 the problem still occurs.. @Overload119 , please note that the issue is not about config.ru internals, but about using config with different file name.. ",
    "Overload119": "I had a similar error message after upgrading to 3.12\nYou want to make sure in puma.rb you are configuring the application directory and the rackup.\napp_dir = File.join(File.expand_path(\"../../..\", __FILE__), 'current')\ndirectory app_dir\nrackup \"#{app_dir}/config.ru\"\nIn config.ru I have this:\n```\nThis file is used by Rack-based servers to start the application.\nrequire_relative 'config/environment'\nrun Rails.application\n```\nTo help with debugging, using something like puts app_dir to make sure the directory makes sense for your use case.. Sorry for the noise - this seemed to be an application error.\nI noticed it only affected some endpoints.\nI was using respond_to :json, :html which Rails doesn't like I guess?\nI ended up using \nrespond_to do |format|\n      format.json\n      format.html\n    end. ",
    "tja": "I would assume you're right, but It'll take a few days to get a definitive answer to that.\nHowever, do you say this is expected behavior for threads>1? Wouldn't it be useful to protect the app creation be a mutex or spin lock?. ",
    "jumph4x": "ilu guys. ",
    "ilgianlu": "sure! \nfrom chrome console network: \npuma 3.8\nCache-Control:max-age=0, private, must-revalidate\nContent-Type:text/html; charset=utf-8\nETag:W/\"9c561b253093388e19645ac3480121be\"\nSet-Cookie:_test_x_frame_session=OEFvK540; path=/; HttpOnly\nTransfer-Encoding:chunked\nX-Content-Type-Options:nosniff\nX-Request-Id:fe93ea84-df83-4594-9706-cc40ba60c01d\nX-Runtime:0.467880\nX-XSS-Protection:1; mode=block\npuma 3.9\nCache-Control:max-age=0, private, must-revalidate\nContent-Type:text/html; charset=utf-8\nETag:W/\"a730b5d9c62c65d0724f0bd2b42bf020\"\nSet-Cookie:_test_x_frame_session=MzBJeDd1a0ee; path=/; HttpOnly\nTransfer-Encoding:chunked\nX-Content-Type-Options:nosniff\nX-Frame-Options:\nX-Request-Id:dcc5dec8-d30b-40bb-8b6f-d27db99c5f3e\nX-Runtime:0.420466\nX-XSS-Protection:1; mode=block\n(I compressed Set-Cookie header for convenience). ",
    "danshultz": "Hmm - it seems queue_requests was accidentally removed in this PR https://github.com/puma/puma/pull/1278. @stereobooster - I'm using a similar process as I outlined for unicorn for tracking queue depth with Puma. The difference is, the active requests are all those within a worker, waiting to be serviced by a thread.\nIn Puma <= 1.8.x, with queue_requests(false), you should never see more requests active than you have workers. You would see queued requests in the event you have more requests than workers available.. Was there a reason queue_requests was removed here?. ",
    "bhavya0692": "Any update on this ?. ",
    "binarylogic": "Right, I understand the context and why the warning was added, but this is a scenario where it is valid to start a thread during boot. The main thread is generating logs, and as I mentioned previously, the timber gem is a logging gem. The thread created enables timber to deliver logs asynchronously, as to not block runtime execution. Does that make sense?\nI understand why the warning was added, most of the time it will prevent mistakes, but considering it is not an absolute condition, there should be a way to mute it for specific threads that are valid.. ",
    "costajob": "Thanks, i just reported this issue since Passenger seems not being afflicted (i have been forced to use it on a multi-language benchmarks i made).. Hi,\ni doubt is a warm up issue: Passenger has the same warm up time and deliver consistent throughput. \nIt seems there's something in Puma that limits the number of incoming requests from an external host (sounds weird...).\nI tried with other APIs, doing more than printing \"Hello World\": same max throughput, about 2500 req/sec.\nI'd prefer to stick with wrk, since most of the micro-benchmark rely on it.. Hi, \nsorry but i do not get the point reproducing the issue with a different loading tool and by using a Vagrant box, which probably has worse network implications than by using physical servers (i tried using one once, and wrk numbers were awful).\nConsider Puma is the only tested library which give poor throughput when stressed by an external server. All the other languages provide throughput an order of magnitude away from Puma, including Passenger which uses the same Ruby version and code than Puma.\nIf it was an issue on the network between the the server and the client, i would expect the same throughput for all of the tested platforms, but this is clearly not the case.\nThere should be something that prevent Puma from correctly responding to the wrk requests or that deceives its recorded numbers.\nThanks. ",
    "Kafkalasch": "Thanks a lot, this worked.. ",
    "ticky": "ObjC factors in because if anything within the forked process links against ObjC APIs, it will be unusable from within Puma! :)\nI\u2019m doing some more debugging to determine what exactly is causing this.. Having trouble doing more debugging, as the child exits as soon as it encounters this error, so attaching rbtrace seems difficult if not impossible.. Anything that forks and runs user-specified code within that fork without using exec may be affected by this.\nIt is unclear exactly where a solution will need to be implemented.. I\u2019ve managed to track down what appears to be the source of this error in the application I\u2019m working with, it\u2019s loading pg which has a native extension linked against Postgres\u2019 libpq.5 which in turn is linked against Kerberos.framework and LDAP.framework.\nIt seems unlikely that we can get away with our workers not being able to talk to our database on macOS\u2026 \ud83e\udd14\nAgain, this is not that you have to actively be using any features provided by these Objective-C frameworks, their inclusion in a thing you link against is enough to trigger this error. We do not use the Kerberos nor LDAP features of Postgres in our application, and yet here we have this error.. Apparently this thread has ended up on reddit where it seems to be being misunderstood.\nRegarding this comment:\n\nI'm trying to come up with a summary of the issue; please assist:\n1. macOS includes an Objective-C runtime\n2. macOS includes an OS framework based on the Objective-C runtime\n3. Objective-C classes defined by the OS framework are fork-unsafe, meaning that fork() callers should not execute code before calling exec()\n4. macOS 10.13 includes a macOS 10.13 SDK (presumably based on the OS framework but not responsible for defining the OS framework's classes)\n5. macOS applications built with the macOS 10.13 SDK are able to use new SDK features in order to execute Objective-C code between fork() and exec(), therefore achieving the fork-safe property when used correctly\n6. There remains a tension between fork-safety and thread-safety, due to the structure of the locks held by the Objective-C runtime\n7. As such, there are restrictions in the mac OS 10.13 SDK about the nature of calls to +initialize between calling fork() and exec()\n8. Thus, the new capabilities in the mac OS 10.13 SDK for calling code safely between fork() and exec() are guarded by protections which enforce the limitations to +initialize in order to maintain fork-safe and thread-safe properties\nWhat is the connection to Ruby on macOS 10.13? I'm not very familiar with this platform, but it would appear that the affected Ruby libs on macOS are making use of the SDK, calling fork(), and running into the new behavior.\n\nThe issue is that code within Ruby can call fork (as Puma does), and loading dynamic libraries from Ruby or a Ruby native extension can cause an Objective-C class to be +initialized, triggering this error message and a swift exit.. > wasn't clear to me if pg might be producing the error independently of the server (does pg use fork?) \nI don\u2019t believe pg uses fork itself. These errors are produced by workers immediately after spinning up. It appears to occur entirely within the initialisation of our application's dependencies.\n\nwhether or not pg's initialization could be handled pre-forking\n\nThis is the best hypothesis I now have. It looks like it might work if those things are all loaded before forking, however, I am not familiar enough with the way our system is set up to know how the should work!\n\nYou posted the issue summery as I was writing this and I wondered if, perhaps, some non Objective-C function calls are implemented using the Objective-C framework under the hood..?\n\nI don\u2019t believe it\u2019s that they are intentionally calling out to any particular Obj-C method, but merely linking to their Obj-C frameworks which requires the Obj-C initialisation to occur!. > in single process mode\nI had tried to activate single-process mode earlier but not had any luck. Turns out you need to remove the workers configuration statements altogether!\nRunning in single mode (confirmed with Puma starting in single mode\u2026 output!) works as expected with no crashes or errors.\nThus, it\u2019s the fork by Puma (or whatever else is using it) followed by Objective-C initialize calls within pg which is causing the ruckus. \ud83d\ude04. A bit of an update - this behaviour also doesn\u2019t cause a crash if preload_app! is set to true. I can\u2019t seem to find the implementation of this feature, though?. @mistydemeo\u2019s onto something! I just tried adding require 'pg' to our Puma before_fork, and the errors have ceased! Thanks so much!\nI wonder if there\u2019s a less strange-looking way to achieve this than loading a specific dependency we know to link ObjC. \ud83e\udd14. Okay, this doesn\u2019t look much less strange but it is, at least, only relying on side effects it\u2019s explicitly calling upon!\n```ruby\nWork around macOS 10.13 and later being very picky about\nfork usage and interactions with Objective-C code\nsee: https://github.com/puma/puma/issues/1421\nif /darwin/ =~ RUBY_PLATFORM\n  before_fork do\n    require 'fiddle'\n    # Dynamically load Foundation.framework, ~implicitly~ initialising\n    # the Objective-C runtime before any forking happens in Puma\n    Fiddle.dlopen '/System/Library/Frameworks/Foundation.framework/Foundation'\n  end\nend\n```\nAs Misty noted, this ensures the the Objective-C runtime has been initialised (a side effect instigated by the dynamic loader itself) before any fork happens, which means once pg is required and loads the Kerberos and LDAP frameworks (which in turn rely on Foundation), they do not need to perform that implicit initialize.\nWhether this is a workaround which makes sense to include in Puma is definitely debatable, but it appears that many things which make use of this sort of fork model will need to consider it, and perhaps also document it.\nNote also that I did not guard this to only occur on 10.13, just macOS in general, as this should work fine on other versions, and in fact be more \u201ccorrect\u201d behaviour by its definition.. Updated the initial post with the updated details of this issue!. @tenderlove yep, I noted that in my initial post \u263a\ufe0f but it\u2019s a brute-force workaround (i.e. it may have other consequences), and doesn\u2019t solve the fact that it may not even be possible in future OS versions!. @boazsegev my last message was regarding the use of the environment variable (OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES) - my Ruby-based workaround will likely remain viable. \u263a\ufe0f\nAs @mistydemeo suggested, it works in Unicorn too, and it likely works in any other thing which is using fork in a similar way, and running arbitrary Ruby code within.\nI would suggest that having an option to implicitly load Foundation would make some sense, however, I would be reluctant to suggest having it always implicitly do this for all users, as it feels very heavy-handed. It\u2019s unfortunate that we can\u2019t catch these kinds of errors in our code (the process is killed instantly), or I\u2019d suggest adding our own message about it!\nIt\u2019s also worth noting again that this behaviour doesn\u2019t trigger this error case outside of cluster mode, or if the application is preloaded.. I don\u2019t mean the before_fork call specifically, just that you\u2019ll need some mechanism to do the dlopen call before you fork. Indeed, its use will depend on what you\u2019re using, and could simply be to put it in whatever calls out to to Passenger or Iodine - it merely has to be done within the same process.\nYou are right, though, that the issue is bigger than Puma. It\u2019s likely that Puma, Unicorn, Passenger and Iodine will all need to consider the implications of this macOS change. Unicorn\u2019s maintainer, though, seems to have already expressed a disinterest in doing so!\nEither that or it requires some sort of structural change in Ruby (though that seems rather unlikely to fly). Unless it has other side effects, I\u2019d suggest implementing this within Puma, Unicorn and iodine would cause the fewest headaches for implementors and users.\nBarring some incompatibility we haven\u2019t yet encountered with initialising the Objective-C runtime within our Ruby process, I think it makes the most sense!. I believe the argument against that would be that things like Puma, Unicorn, iodine and Passenger actually know they will be forking and never exec\u2019ing, whilst Ruby itself doesn\u2019t necessarily know that. I don\u2019t think it would be too detrimental, though. It may even make sense for Ruby on macOS to initialise the Objective-C runtime the first time fork is called?. > Processes can be forked by C/Java extensions as well (using direct system calls).\nThat\u2019s actually a good point, which I hadn\u2019t considered!\n\nSo I support the notion of having Ruby initialize the objc runtime upon calling #fork.\n\nI have just opened an issue on Ruby\u2019s tracker for this. I hope we can work something nice out!. An update from the Ruby tracker, a patch for Ruby has been suggested and I\u2019ve confirmed it mitigates this issue when it comes to Puma!\nIt works around it by linking Ruby itself to Foundation instead of CoreFoundation. CoreFoundation doesn\u2019t initialise Objective-C, but as we know from this thread, Foundation does. \ud83d\ude04. > fixing it in Ruby only affects versions of Ruby from a specific version onward\nThat\u2019s true, however, it seems like it\u2019s both likely to be a thing worthy of back porting to any existing supported versions, and a very simple patch to apply if you need support for an even older version.. > To work around this issue today, I suggest just setting the OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES environment variable when working with Puma.\nI\u2019d warn that as discussed above, this potentially masks other issues which this new behaviour is attempting to mask. Loading Foundation explicitly will avoid the \u201cimplicit Obj-C runtime initialisation in a fork\u201d errors, but not mask similar but distinct errors in which your program actually triggers the behaviour this intends to disallow.. > I think you meant \"attempting to unmask\" right?\nYep :)\n\nPuma already warns when multiple threads exist before forking, so the safety check here is not as helpful as it might otherwise be.\n\nThe safety check won\u2019t apply to any C extensions which may fork, though, right?. @rhymes, @webark: the upstream Ruby patch hasn\u2019t been backported to Ruby 2.4. It looks like it will in fact land in Ruby 2.5, which is forthcoming.\n@nateberkopec, it might be worth reconsidering the \u201cno Puma workaround\u201d approach given there will potentially be people wanting to stay on 2.4 and I don\u2019t have confirmation of whether any backport is planned.. Update: a backport for 2.4.x is forthcoming!. Update: a backport for 2.4.x has landed, it should be in the 2.4.4 release, though I suspect that won\u2019t appear until the new year!. ",
    "boazsegev": "Sorry to butt in, but do you know if this effects iodine as well?\nI can't install High Sierra since many of the professional applications I use daily don't support it just yet (I'm also a musician).. @ticky ,\nI'm happy you're managing to track the issue down and I'm thankful for the added information.\nHowever, reading the information it wasn't clear to me if pg might be producing the error independently of the server (does pg use fork?) and  whether or not pg's initialization could be handled pre-forking (perhaps avoiding the issue)?\n\n\nDoes the error occur also in single process mode (in puma and iodine, add the -w 1 to the command line)?\n\n\nCould you initialize thepg database within the config.ru and see if pre-fork initialization might work around the issue? (remember to re-establish the DB connections, so they aren't shared acrid processes)...?\n\n\nJust my 2\u00a2, I don't know if any of it helps.\nP.S.\nYou posted the issue summery as I was writing this and I wondered if, perhaps, some non Objective-C function calls are implemented using the Objective-C framework under the hood..?. > doesn\u2019t solve the fact that it may not even be possible in future OS versions\nIt would be possible to have a quite exception instead of the before_fork - fork could be assumed, in the interest of  C extensions such as passenger and iodine, where before_fork callbacks might not get invoked due to a direct system call (rather than Kernel.fork)...\nMaybe something like:\n```ruby\nWork around macOS 10.13 and later being very picky about\nfork usage and interactions with Objective-C code\nsee: https://github.com/puma/puma/issues/1421\nif /darwin/ =~ RUBY_PLATFORM\n  begin\n    require 'fiddle'\n    # Dynamically load Foundation.framework, ~implicitly~ initialising\n    # the Objective-C runtime before any forking happens in Puma\n    Fiddle.dlopen '/System/Library/Frameworks/Foundation.framework/Foundation'\n  rescue Exception => _e\nend\nend\n```\n\nit may have other consequences\n\nSince fiddle is part of the standard library, it seems safe enough.\nHowever, the _NS* bindings might be another issue (introducing name space concerns and possible conflicts), which make this a very valid warning.\n. @ticky , you're probably right about it being heavy-handed, but...\n\nit likely works in any other thing which is using fork in a similar way...\n\nI don't think it does. Especially since before_fork is puma's DSL and not a general Ruby feature.\nOther C extensions (such as passenger and iodine) might use a different DSL before they call the system call directly (not using Ruby's fork), assuming any required actions were directly performed (rather than placed in a callback).\nI might be over thinking this, since the issue was opened here, but the discussion feels as if it's more than about a Puma specific solution (it includes Unicorn, Sidekiq etc').\nJust my 2\u00a2, of course.\n. @mperham, I would generally agree that this seems like a Ruby concern rather than a specific gem (or a group of gems).\nI would assume that Ruby (as a scripting language) should behave consistently across supposed environments without requiring code changes (except where different behaviors are desirable). For this reason, an OS specific workaround that's required for a consistent behavior across OS boundaries, should probably (IMHO) be part of the core Ruby code. \nHaving said that, I wonder if pushing this into a Ruby release is even an option.\n@ticky, In general I find your approach very wise and balanced.\nHowever, I doubt that it is safe to assume that the Ruby version of fork would be called whenever Ruby is running in the background.\nProcesses can be forked by C/Java extensions as well (using direct system calls). Besides, Ruby MRI is also available as an engine/library (much like V8 for Javascript) and could be executed from within an already running environment.\nI wonder what price we pay for loading the Objective-C runtime, but it might be safer to preemptively load the Objective-C runtime  during the Ruby VM initialization procedure (although it's somewhat high handed, as you pointed out).. > Would it be reasonable to ask them to upgrade Ruby or would it be better if you activate the workaround on older Ruby versions?\nIMHO, since the issue effects High Sierra, I believe it is acceptable to expect users to upgrade to a High Sierra compatible Ruby version.\nI also think that back porting the patch is wise.. UPDATE: It seems a patch was merged with the Ruby trunk. This probably means the issue will be resolved in future Ruby releases (~~~the next 2.4.x version should be patched~~~ EDIT: nope, still waiting on a patch, maybe 2.5?).. As an update from the iodine server perspective:\nThe iodine server now includes a fix for this issue (starting at version 0.4.10), allowing people using iodine today to have a better experience.\nThe decision was made for two reasons:\n\n\nThis allows High Sierra to be supported without requiring the Ruby runtime to be upgraded to a High Sierra compatible version.\n\n\nWaiting for Christmas (when I assume the next Ruby version will be released) seemed a bit of a wait...\n\n\nI'm not sure everyone needs to do the same, but I thought it might be of interest.. > The safety check won\u2019t apply to any C extensions which may fork, though, right?\n@nateberkopec , I'm not sure what the common practice is, but I re-assesed the observation that language extensions (C / Java) could call fork directly.\nIt seems that Ruby has a \"timer thread\" running in the background. After reading through the Ruby source code I realized that calling fork directly might have negative effects on Ruby performance and scheduling.\nExtensions that call fork directly might be considered buggy and could possibly be ignored as far as the safety check is concerned (on the grounds of these extensions leaving behind the Ruby scope).\nAs for the Thread.list, native threads don't work well with Ruby threads (due to the GIL and related data structures and signaling concerns). So the Thread.list could be considered complete as far as the safety check is concerned... with a single possible exception...\n...however, some extensions might spawn new native threads that never call the Ruby API (iodine does this for IO flushing). These threads are obviously not in the Thread.list. But I think the safety warning can safely ignore these extensions, since they are clearly aware of these details and were designed with these considerations in mind.. ",
    "mistydemeo": "\nHowever, reading the information it wasn't clear to me if pg might be producing the error independently of the server (does pg use fork?) and whether or not pg's initialization could be handled pre-forking (perhaps avoiding the issue)?\n\npg doesn't use fork, but the issue isn't active initialization by pg so much as how the ObjC runtime startup is occurring. In this context since the program itself isn't written in ObjC and doesn't directly have any ObjC linkage, the ObjC runtime only enters the equation when a dlopen call (via Ruby's dln_load) loads a shared object which has ObjC linkage. As you can see in this truncated backtrace from an aborted Ruby process the dynamic loader is initializing the ObjC runtime automatically, which causes CoreFoundation to initialize an object. That triggers the kill.\n* thread #1, stop reason = signal SIGSTOP\n  * frame #0: 0x00007fff53bde336 libsystem_kernel.dylib`__abort_with_payload + 10\n    frame #1: 0x00007fff53bd8e00 libsystem_kernel.dylib`abort_with_payload_wrapper_internal + 89\n    frame #2: 0x00007fff53bd8da7 libsystem_kernel.dylib`abort_with_reason + 22\n    frame #3: 0x00007fff52ea2962 libobjc.A.dylib`_objc_fatalv(unsigned long long, unsigned long long, char const*, __va_list_tag*) + 108\n    frame #4: 0x00007fff52ea2814 libobjc.A.dylib`_objc_fatal(char const*, ...) + 135\n    frame #5: 0x00007fff52ea343b libobjc.A.dylib`performForkChildInitialize(objc_class*, objc_class*) + 341\n    frame #6: 0x00007fff52e93e8f libobjc.A.dylib`lookUpImpOrForward + 228\n    frame #7: 0x00007fff52e93914 libobjc.A.dylib`_objc_msgSend_uncached + 68\n    frame #8: 0x00007fff52e97145 libobjc.A.dylib`+[NSObject new] + 86\n    frame #9: 0x00007fff2e5bb025 Foundation`-[NSThread init] + 61\n    frame #10: 0x00007fff2e5bf0f5 Foundation`____NSThreads_block_invoke + 64\n    frame #11: 0x00007fff53a55f64 libdispatch.dylib`_dispatch_client_callout + 8\n    frame #12: 0x00007fff53a55f17 libdispatch.dylib`dispatch_once_f + 41\n    frame #13: 0x00007fff2e5bafe3 Foundation`_NSThreadGet0 + 325\n    frame #14: 0x00007fff2e5ba7ef Foundation`_NSInitializePlatform + 341\n    frame #15: 0x00007fff52e958df libobjc.A.dylib`call_load_methods + 239\n    frame #16: 0x00007fff52e92c16 libobjc.A.dylib`load_images + 70\n    frame #17: 0x000000011125d198 dyld`dyld::notifySingle(dyld_image_states, ImageLoader const*, ImageLoader::InitializerTimingList*) + 407\n    frame #18: 0x000000011126d15b dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 309\n    frame #19: 0x000000011126d103 dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 221\n    frame #20: 0x000000011126d103 dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 221\n    frame #21: 0x000000011126d103 dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 221\n    frame #22: 0x000000011126d103 dyld`ImageLoader::recursiveInitialization(ImageLoader::LinkContext const&, unsigned int, char const*, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 221\n    frame #23: 0x000000011126c2a6 dyld`ImageLoader::processInitializers(ImageLoader::LinkContext const&, unsigned int, ImageLoader::InitializerTimingList&, ImageLoader::UninitedUpwards&) + 134\n    frame #24: 0x000000011126c33a dyld`ImageLoader::runInitializers(ImageLoader::LinkContext const&, ImageLoader::InitializerTimingList&) + 74\n    frame #25: 0x00000001112603e5 dyld`dyld::runInitializers(ImageLoader*) + 82\n    frame #26: 0x0000000111269002 dyld`dlopen + 527\n    frame #27: 0x00007fff53a90eb6 libdyld.dylib`dlopen + 86\n    frame #28: 0x0000000102d0ce3f ruby`dln_load + 175. I have a Unicorn app, rather than a Puma app, but something that's working for me is to explicitly require a gem with ObjC linkage within a before_fork block. That ensures that the ObjC runtime is loaded and initialized before the fork occurs, and it's a bit lighter-weight than preload_app!.. ",
    "webark": "this still seems to be happening for me in puma's cluster mode, not in single mode.\n$ > bundle exec puma -w 3\n[49550] Puma starting in cluster mode...\n[49550] * Version 3.10.0 (ruby 2.4.3-p205), codename: Russell's Teapot\n[49550] * Min threads: 0, max threads: 16\n[49550] * Environment: development\n[49550] * Process workers: 3\n[49550] * Phased restart available\n[49550] * Listening on tcp://0.0.0.0:9292\n[49550] Use Ctrl-C to stop\nobjc[49556]: +[__NSPlaceholderDictionary initialize] may have been in progress in another thread when fork() was called.. ",
    "mikisvaz": "My issue is not that there is a bug that stalls or anything. It's that the behavior has changed from version 3.6.0 onwards by the introduction of some code that seems to inevitably always call try_to_finish, which reads completely the content of the socket IO into the  @body variable. This is where \nhttps://github.com/puma/puma/blob/1f8e92af42627fef9172df0dc1dacd0617d8848c/lib/puma/client.rb#L276-L278\nthis gets called from here:\nhttps://github.com/puma/puma/blob/1f8e92af42627fef9172df0dc1dacd0617d8848c/lib/puma/reactor.rb#L74\nThe fact that the socket is inevitably read before I can hijack it hinders on my cunning plan to process it myself. I was wondering if you guys were aware of this and if you knew how to circumvent it. I'm hardly an expert, but I'm thinking that worst comes to worst perhaps I could setup some early Rack middleware that can do the hijacking before the puma code comes in. Does that even make sense?\n. ",
    "garybernhardt": "That did it, thanks!. ",
    "MSP-Greg": "Some info:\n\nI don't know c.\nAs stated, all MinGW builds with SSL have the same errors.  I didn't build the 1.0.2j or 1.0.2k packages, but I built the 1.1.0f package.  All OpenSSL tests passed when building the package, and all ruby trunk OpenSSL tests (test-all and spec) pass.\n\nHence, the issue seems specific to puma & MinGW.  Since I don't know c, nor puma, unlikely that I can find the issue.  Conversely, I'll be happy to check/test any suggestions.\nBackground - 'Ruby for Windows' has transitioned to a much more open & maintainable build system based on MSYS2/MinGW (for instance, current gcc is 7.2.0).  I've been very involved in building/testing Ruby trunk/head.  I would like to see most popular extension gems building correctly on Windows and also testing with trunk.  After all, in just a few months, trunk will be the current release.  At present, the Appveyor trunk build (currently, it is my build done on Appveyor) is much more current than the Travis rvm version.\nThanks.. Windows Appveyor builds are now passing all tests.  Results here.\nAt present, all changes are in one commit here.  The issue was that some (but not all) read_nonblock calls were generating an error.\nSumming up, with this fix, Ruby 2.2 thru trunk can be build with SSL and all tests will pass.\nIs this acceptable as a PR?. See PR #1439 . @nateberkopec \nYou're welcome.  I might be considered over-zealous as to testing, so the trunk build always passes the ruby/ruby tests.\nBut, I have found a few bugs while running 'local code'.\nHence, if you have any failures, feel free to ping me.  Thanks for your work on Puma.. @shayonj \nPlease accept my apology, as I should have mentioned you in the PR.  It was kind of a 'if all three are rolled into one, this should work' thing.... @nateberkopec\nI hate it when this happens.\nTravis:\nMy fork build passed.  Travis here did two builds, first #2534 passed, second #2535 failed.\nAppveyor:\nMy fork passed, Appveyor here (just one build) failed only on trunk, with a 'Could not build wrapper using Ragel' error on line 114 of the log.  I haven't seen that in previous tests.  I can add a ragel package to Appveyor, but it's not normally installed.  Odd that the error just appeared.\nEDIT: ragel can be added two ways.  If it's required to install/compile the packaged gem, it should be added to the metadata in the gemspec.  If it's only required here for CI, then it should be added in the appveyor.yml file.. See #1464 and  7165775. @nateberkopec\nGiven the results here, I'm wondering about a combination of this and #1444.. I'm certainly glad it passed the 2nd time, as the failures were intermittent.\nRe Appveyor and ruby-head (trunk), it is currently not 'allow-failure'.  It will always return a build, but it may not be the most current, as it is tested.  Said another way, if a build from yesterday passed, and subsequent builds fail, yesterday's build is what is used.  From memory, I don't recall builds being broken for more than 24 hours.\nBut, from what I can tell, Travis does ruby-head builds once a day, and they are not tested.  So, the possibility exists for a Travis build to be broken.  Also, they currently aren't installing bundled gems.\nHence, for those reasons, Appveyor ruby-head can often be included in the normal build test result, but Travis may best be left 'allow-failure'.  If it becomes an issue, feel free to change Appveyor (or ping me).   Thanks.. @nateberkopec \nThought an explanation might be helpful, as I don't know puma that well.  Most of the intermittent failures originally fail with something like the following:\ndiff\nTestCLI#test_control_for_tcp = [MinitestRetry] retry 'test_control_for_tcp' count: 1,  msg: --- expected\n+++ actual\n@@ -1 +1,2 @@\n-\"{ \\\"backlog\\\": 0, \\\"running\\\": 0 }\"\n+# encoding: ASCII-8BIT\n+\"{ \\\"backlog\\\": , \\\"running\\\":  }\"\nThe output shown at the bottom of the log is the retry info.  So, from the above, we see two things:\n\n\nThe split command is not working in /test/test_cli.rb.  Hence, the change from \"\\r\\n\" to /\\r?\\n/ for the delimiter.\n\n\nThe last line has no zeros.  The backlog and running attributes from lib/puma/server.rb are shown below:\n```ruby\n    def backlog\n      @thread_pool and @thread_pool.backlog\n    end\ndef running\n  @thread_pool and @thread_pool.spawned\nend\n```\n\n\nThat code shows that @thread_pool may be nil, and hence, the return will either be nil or a number >= 0.  Hence, the added || 0 code to lib/puma/cluster.rb and lib/puma/single.rb for their 'stats' info.\nThis fixes the issue with the test, but I don't know if the test code should generate a non nil value for @thread_pool, and hence there really is an issue...  Thanks, Greg. See PR #1538 . @schneems\n\nIt looks like you put some code into that method, would your fixes be related to that? It looks like all your appveyor tests are passing.\n\nWell, when posting those changes, I was focused on 'green', and overlooked an error in not allowing for all possible return values.  Again, apologies for that.\nSome of the failures fixed in this PR are intermittent, and from working with trunk testing in Appveyor, there are some differences re sockets, etc.  As to whether they're windows, gcc 7.3.0, or OpenSSL 1.1.0 related, I'm not totally sure.\nSo, with the exception of the patch to minissl.rb, I left all the changes in the test files.  I've run it several times on my fork, and the tests are stable for all Ruby versions.\nThanks, Greg. I'm not in a position to determine why 2.2.9 failed in Travis #2740.1, or why it passed in Travis #2741.1 (or what triggered the 2741 build)...   Thanks, Greg. Closed since #1542 added the same code.... I had several puma tabs open, and after I did the PR I noticed a green circle with a plus sign and a message like \"commits added since your last view\"...\nIt's green.  If Appveyor (especially trunk/head) ever goes red or seems unstable, feel free to ping me.\nThanks, Greg. @deepj\nRe Appveyor, I'll add it to one or another of the versions...  Today's result (built with OpenSSL 1.1.0h)\n183 runs, 351 assertions, 0 failures, 0 errors, 18 skips\nruby 2.6.0dev (2018-07-11 trunk 63941) +JIT [x64-mingw32]. @BlackNightFury\nBoth Ruby OpenSSL and Puma MiniSSL are compiled using the OpenSSL library.\nAs to how to use MiniSSL, I would suggest reviewing code in the Puma tests.\nGreg. > with a stable albeit clunky API\nGiven that:\n1. Ruby OpenSSL has deprecated ssl_version and replaced it with two attributes\n2. RubyGems is now only supporting TLSv1.2 connections\n3. OpenSSL 1.1.1 with TLSv1.3 has been released (2018-09-11)\nRather than add an no_tlsv1 option, might it be better to add a min_vers(ion) option?  People may want a no_tlsv1_1 option in a year or so...\nEDIT: just checked here (GitHub.com), and it is also only using TLSv1.2. @deivid-rodriguez \nGlad to see you're able to take a break from byebug.  Re ragel, I added it due to intermittent failures, often in just one job (ruby version) of a build.  See \nhttps://ci.appveyor.com/project/MSP-Greg/puma/build/7/job/17o5qjunlbva5q5j\nRe ruby25, I always assumed that few people were using 32 bit, but that may be an incorrect assumption.\nGreg. @deivid-rodriguez \nThanks for pointing that out.  Stupid mistake on my part, apologies.\nThe RubyInstaller2 builds are done correctly on lines 14 & 15, but RubyInstaller builds (Ruby 2.3 and older) are incorrect.  I've seen intermittent ragel failures on both, so if you could fix the issue rather than removing the code, it would be appreciated.\nThe MB on my home development system is not booting, so I'm behind...\nThanks, Greg. @olleolleolle\nRebase on master and hopefully this will pass?\nRe testing, dumb JRuby question.  On Windows UNIXSocket isn't defined.  I assume they exist in JRuby, but don't match standard Ruby in functionality?\nThanks, Greg. See PR #1601. The Build system in the PR creates two Appveyor jobs, one for the 64 bit gem & testing, and one for the 32 bit.\nFrom experience, when Appveyor is busy, builds may have problems verifying GPG keys, which are used by two of the OpenSSL packages.  You'll notice in the 64 bit job the bottom line in the log is a key error.\nI did a build locally in my repo, and forgot that a file is added to the gem for looking up the correct so file in the fat-binary gem.  That took me two revisions to fix (sorry), so there were three Appveyor tests ran.  The first two passed.  See build 1.0.79 & build 1.0.80.  The gems and the test logs are stored as artifacts for each job, and the test summaries are shown on the tests tab.\nAs to the JRuby error in Travis, that seems to be shared by most recent commits also.\nGreg\n. Revised version created with shared script files downloaded.  See #1601. The question everyone wants the answer to - what needs to be done when Ruby 2.6 is released?\nThe answer - Nothing.  Wait for Appveyor to add it to their images.  Ruby 2.5 took about a month.  I suspect they'll add it a little quicker this time around.... @nateberkopec @schneems @evanphx\nIf any of you have the time to read it and reply, I've added an issue at the repo containing the shared code for the topic of Windows pre-compiled gems and testing on Appveyor.\nhttps://github.com/MSP-Greg/av-gem-build-test/issues/1\nThanks, Greg. @schneems\nSee https://ci.appveyor.com/project/MSP-Greg/puma,  just ran a moment ago, passed all except trunk-x64-JIT.\nRe Travis, see #1620.  Most recent CI failed with the same error as several previous.\nThe Appveyor failure here is due to the old gnupg package on Appveyor, as mentioned in #1621.  This PR uses script/PowerShell files contained in another repo, so it's more complete code.\nOne benefit of that is that sometime I will change trunk-x64 to OpenSSL 1.1.1, and when I do, I don't need to make any changes here.\nReminder - this has a commit that is similar to #1616, so I may need to remove the code here...\nEDIT: I can rebase a start another CI\nThanks, Greg. Just rebased.  Issues with MSYS2 gnupg fixed after some time with pacman -Qi and pactree.  Hence, and as it should be, all packages used with Ruby 2.4 forward are checked for signatures.\nFYI, Trunk 64 bit JIT has been passing.... Re trunk JIT, Appveyor on my fork took 106 sec, this ci took 185 sec.  To see a test summary click on 'Test Summary 64 bit'.  Most of the other new Ruby versions take approx. 34 sec.\nBut, Appveyor jobs run on two cores.  Running on a quad locally, the JIT build took 34 secs.\nJust in case you were wondering.... Just built again on my fork.  Results at https://ci.appveyor.com/project/MSP-Greg/puma/build/32\nPassed.\nAlso, trunk-x64 is passing with OpenSSL 1.1.1 (built today from master) with essentially one test change.. We all know Travis & Appveyor have a lot of items needing updates.  Just the fact that all Ruby versions on Travis Ubuntu are using OpenSSL 1.0.1 says a lot.\nThe build system used for Windows Ruby is MSYS2/MinGW.  It also gets outdated on Appveyor, but if one performs a new install of Ruby (>= 2.4), the default prompts will update your local build system.  Also, MSYS2/MinGW doesn't really support incremental updates.  There are quite a few packages on Appveyor, so a full update can take anywhere from six to 12 minutes.  Given normal test architecture, that would be per job, or for each version of Ruby tested.  Lastly, the current gcc release is 8.2.0, but Appveyor only has 7.3.0.\nSo, what should be used for Appveyor testing?  Updated or existing Appveyor?\nSecondly, there's the issue of OpenSSL.  Right or wrong, Windows Ruby 2.5 is built with 1.1.0.  But, the standard MSYS2/MinGW OpenSSL package is still at 1.0.2, due to the shear number of packages built with it (including Git for Windows).  Ruby trunk is currently testing with 1.1.1.\nDue to these two issues, and the fact that compiling is required for both testing and constructing a Windows pre-compiled gem, I created the code included here.\nIt relies on code outside of this repo, but that is the code that efficiently handles updating the Appveyor build system, selecting the correct OpenSSL package, compiling, constructing the gem, and testing.  Since it only requires two build jobs, it runs faster than the traditional setup.\nThis has sat for a while, and other PR's connected with unstable testing have done the same.  Is there anything we can do to help?. See openssl.txt. @schneems\nReminder - appveyor creates & saves a pre-compiled gem file.  As I recall, the test files are saved in the gem, along with a windows specific rake file.  It's installed and the tests run from it.\nIt's built for ruby 2.2. thru 2.6.. Given the date, I'm sure trunk won't have any breaking API or ABI changes before 2.6 release.  Be the first on your block to release a 2.6 compatible pre-compiled gem!. @sebastianas\nI'm running trunk with 1.1.1 and also built puma with it (windows).\nYour test froze, my test run had the following three tests as errors with Errno::EADDRINUSE: Only one usage of each socket address (protocol/network address/port) is normally permitted. - bind(2) for \"127.0.0.1\" port 3212.  Might you be able to see if you have the same issue with the other two tests?\nRuby's OpenSSL may have an issue also.  WIP.\nTestPumaServerSSLClient#test_verify_fail_if_client_expired_cert\nTestPumaServerSSLClient#test_verify_fail_if_no_client_cert\nTestPumaServerSSLClient#test_verify_fail_if_client_unknown_ca\n. I've been testing 1.1.1 on both Ruby OpenSSL & Puma.\nIf I force all connections to use TLSv1.2, all tests pass in Puma and the Ruby OpenSSL tests.\nIn Puma, if I leave connections able to use TLSv1.3, and account for a possible issue with net/http, all tests also pass.. @sebastianas\nBeing a windows type, I'm not sure about what happens with your setup.  Is Ruby using the same OpenSSL version as Puma?  Also, Ruby built with the same version?\nGenerally with windows, the suggestion is that everything using OpenSSL (digest, ruby openssl, eventmachine, puma, etc) should be built with the same OpenSSL package.\nFWIW, using ruby 2.6.0dev (2018-08-09 trunk 64237) [x64-mingw32] built with an OpenSSL 1.1.1 package built from OpenSSL master, and using Puma built with the same, all Puma tests pass as is.. @schneems\nGreen, finally.  Found the issue with the intermittently failing test on Travis, it was in binder.rb.  That test also passes on Windows & JRuby, so I removed the skip.\nPlease have a look at Test Update - skip handling, windows .  I've run it plenty of times testing the Widows pre-compiled gem PR.\nI can remove that and do a separate PR for the test updates.  Also, an update to control_cli.rb; it's set up fine for a cli app, but could have some stability issues when used in testing...\nThanks, Greg. @schneems\n\nwe could make the str a kwarg\n\nSince kwargs are globally used strings, any preferences for the name?  Maybe suffix, msg_suffix, or message_suffix?  I'm not particular.... @schneems\nSee https://ci.appveyor.com/project/MSP-Greg/puma/history.  Certainly seems to be reliable.\nAlso, above mentioned PR #1609 and the Travis PR #1620.  I'd run tests on Travis with my fork, but since it's 'branches: only: \"master\"', It's a PITA to run another branch in a fork...\nI'm happy to help, but when CI fails all the time, it's difficult to see what intermittent issues exist.... I just rebased and changed the PR to allow showing the real location of the skip.  Previous tests and the original commit would show the location of the skip as somewhere in helper.rb.\nThe update fixes that...\nAnd I had a typo (I thought I changed something from an instance variable to a local, but I forgot one @).  Updated twice.... Closed , see 72882f2. @olleolleolle\nApologies, maybe I went a bit nuts with 'Make ci work' 72882f2, but 9.2.0.0 was added along with JRuby-head, and both are passing (head is allow failure).  Thanks, Greg. @schneems\nBoy, I really hate these keyserver errors, which is the reason that the Appveyor build failed.  I built this twice locally, and it passed.  That included building 2.2 thru trunk and trunk with JIT...\nI can't really fix the issue by using only an Appveyor.yml file.  It seems to work consistently in the 'pre-compiled' gem build code, and it works fine with ruby-loco/trunk.  Both of those have PowerShell script code with functions for a proper 'retry' function.  Also, I have seen Travis issues with keyservers timing out, so it's not just Appveyor.\nIn case you're wondering, 'new Windows Ruby' (MSYS2/MinGW and RubyInsatller2) uses the MSYS2 build tools, and they have only released OpenSSL 1.0.2 packages.  I may be partially to blame, but a long time ago, I started using OpenSSL 1.1.0 for ruby-loco trunk testing, so both trunk and 2.5 are built with 1.1.0.  Ruby 2.2 thru 2.4 use 1.0.2.  The 1.1.0 package is a custom package, so that's why a key must be imported, as there's no way to install a package with pacman without one.\nSorry for the long explanation, but if it was trivial, I would have fixed it already.  And, the custom keys are required for both trunk and 2.5, so even setting trunk to 'allow failure' may still result in false failures on 2.5...\nThanks, Greg. Closed.  See a3ac7b9. Closed , see 72882f2. Closed , see 72882f2. Closed.  See 6a7112d. Closed. See commit 72882f2.. @schneems\nInfo re recent Appveyor image update:\n\nRubies updated to 2.4.5 & 2.5.3.\nRubies 2.2.6 thru 2.4.5 are using some version of OpenSSL 1.0.2.\nRuby 2.5.1 used OpenSSL 1.1.0, 2.5.3 is using 1.1.1.\nTrunk is using 1.1.1.  I may switch to pre-release builds if they pass when building Ruby OpenSSL on trunk.\nCI sometimes runs very slowly, typically weekday US afternoons (or Noon EST to 17:00 PST).  I've seen it with trunk, often a re-start is all that's needed.  JFYI, trunk build times vary between 30 & 50 minutes.. I tested the patch for this in my fork, Appveyor & Travis passed, but Travis failed on both Ruby trunk jobs, although probably not due to this PR.. See PR #1653.  Renamed the class in test_rack_handler.rb to TestHandlerGetStrSym, along with another change.. Closed. See PR #1653 & commit 9de253d.. Apology for that, as minissl.rb:56 was from my PR 1444 (https://github.com/puma/puma/pull/1444/commits/ac5acc8bf9498fff7b44c710365aaa6feb2cb168) approx a year ago.  I normally work with trunk, then run a CI, which has passed with Ruby 2.2.x since.  But, as you mentioned, the exception: kwarg was added in Ruby 2.3.\n\nIt could be re-written to use rescue instead of the symbols, I think.... @davidben \nThanks for your patch.  Just ran it in my fork, and Travis (OpenSSL 1.0.1 thru 1.1.0) and Appveyor (OpenSSL 1.0.2 thru 1.1.1) all passed.  I don't recall the test suite that well, but green is certainly good...\nI guess BoringSSL people are as nice as the OpenSSL people.  Thanks again, Greg\n@eric-norcross if you don't want to submit a PR, I can do it tonite or tomorrow.. See Puma commit fc97fdaa\n'Implemented NID_X9_62_prime256v1 (P-256) curve over P-521 in order to support Chrome 70 and Edge', by @eric-norcross...\n. I believe this issue may have also been causing failing tests on Travis when ruby-head was 2.6.0dev, the tests are skipped on Windows/Appveyor due to lack of fork.... I believe this issue has been around since 2018-Aug, between r64316 and r64376.  Both trunk and 2.6.1 are failing currently, and have been since that time.\nSome travis build info:\nFailed builds\n2999 ruby 2.6.0dev (2018-08-15 trunk 64376) [x86_64-linux] Address in Use\n2998 ruby 2.6.0dev (2018-08-15 trunk 64376) [x86_64-linux] Address in Use\nPassing builds\n2997 ruby 2.6.0dev (2018-08-12 trunk 64316) [x86_64-linux] Cron\n2993 ruby 2.6.0dev (2018-08-07 trunk 64208) [x86_64-linux] Cron\n2981 ruby 2.6.0dev (2018-07-24 trunk 64026) [x86_64-linux]\n. @dentarg\nIs it a bug or a breaking change?\nI think #1741 seems to be working, I don't know how hard it would be for you to test it.... @dentarg\nThanks for testing.\n\nSeems like it behaves as 2.5.3 does.\n\nTo clarify, that means that the PR fixes the issue you've had with Ruby 2.6/trunk?. >  try to reproduce the problem on Heroku\nMaybe @schneems has seen the issue with 2.6/trunk somewhere?. If anyone wants to try a PR that may solve the clustered issue, the following in a gemfile:\ngem 'puma', :git => 'https://github.com/puma/puma.git', :ref => 'refs/pull/1741/head'. Thanks for checking.\n\nHowever, requests still don't get through in clustered mode in ruby 2.5.4\n\nNot good.  Any idea if 2.5.5 fixes it (or using 2.6.2)?. Good.  Is that using standard 3.12.0 or the git version with PR 1741?. Thanks again.\n\nYeah, ruby 2.5.5 fixes it even with standard 3.12.0 puma branch\n\nI don't recall any issues with Ruby 2.5.x < 2.5.4, it was only 2.5.4 that broke.\nRuby trunk started failing on clustered shutdown/restart last year some time, so the patch was for compatibility with Ruby 2.6 and current trunk (2.7.0).. PR #1747 updated the Travis matrix from 2.6.1 to 2.6.2, and it & trunk seem to be failing (2.5.5 is ok):\nhttps://travis-ci.org/puma/puma/builds/507190071. @olleolleolle\nBeing rather *nix/MacOS challenged, might you have time to look at why trunk was failing when it was 2.6.0?\nIOW, I can add it, but I think it will fail.  I believe the failing tests all involve fork, so I can't test on Windows.... @olleolleolle\nThanks.  I didn't state it above, but the 2nd commit in this PR fixes the 2.2.10 Travis failure, as shown in your PR #1695.... I believe Ruby 1.8.7 was removed from CI testing on 2016-02-25.  The Travis for that commit used Ubuntu 12.04.\nSomeone may be able to help, but.... See PR #1682.  I didn't catch the white-space, I can force a merge.  Travis ruby-head is kind of broken as to RubyGems.  Hopefully it will be fixed soon...\nUses:\nyml\nbefore_install:\n  # rubygems 2.7.8 and greater include bundler\n  - |\n    rv=\"$(ruby -e 'STDOUT.write RUBY_VERSION')\";\n    if   [ \"$rv\" \\< \"2.3\" ]; then gem update --system 2.7.8 --no-document\n    elif [ \"$rv\" \\< \"2.6\" ]; then gem update --system --no-document --conservative\n    fi\n  - ruby -v && gem --version && bundle version\nJFYI, Appveyor is building with 2.6.  Didn't add it to Travis, as I suspect it will fail like ruby-head.... Issue with Ruby 2.6.x & 2.7.x (trunk) involves bug introduced in waitpid.  See #1741.  Should revert it when Ruby Core fixes issue.. @schneems\nPlease consider releasing the pre-compiled gems (with SSL) for windows/mingw.\nThanks for all your work with Puma, Greg. @schneems \nNo rush.  One thing also is the Ruby 2.6.0 issue.  I'm not sure if it's a Ruby issue, or an issue that can be dealt with in Puma.  Because it only occurs when using fork, I can't test it with Windows...\n\nCan you give me details on how to grab and push the binaries for windows? It's not a thing I have much experience with.\n\nI also haven't pushed any actual gems for quite a while.  Go to:\nhttps://ci.appveyor.com/project/puma/puma\nThere you'll see the builds for 64 bit and 32 bit.  Click on either, and you'll go to the default log page.  Then, click on 'artifacts', and you'll see the gem file available for download.  Rather than setting up Appveyor, I assume you'll download them and push manually?\nBTW, feel free to have any Windows types give them a try, as they are saved with every commit/pr...\n\nnot providing vendored versions actually has allowed teams (unintentionally) to vendor gems and still deploy to Heroku\n\nI assume there's some way to specify something like \"--platform=ruby\"?  Not sure how deploy works, but Bundler & RubyGems should never install a gem on an improper OS, they should default to the 'Ruby' (non-compiled) gem?\nFor non-windows users, most don't have to even consider pre-compiled gems, as none match their OS/platform.  My normal ruby version is trunk, and there are several ways to force non-compiled gems to install, as very few are compiled to run on trunk...\nWhy not let windows users compile Puma?\nI don't use *nix or MacOS locally, hence, most of what I know is from working on Travis.  For non-Windows use, it's my impression that OpenSSL is typically defined by the OS.\nSince OpenSSL isn't 'native' to Windows, the version used is defined by a combination of the packager's decision, what the Ruby version works with, and what OpenSSL packages are available for the build system.  Building the gem for Ruby 2.2 thru 2.6/2.7 is using two build systems and three OpenSSL packages.\nBoth here and at EventMachine, there are always issues about \"I can't build on Windows with OpenSSL\", as some of the users are not experienced with build systems and packages.  Personally, I'd like to hope that at some point they'd want some experience with them, but I don't think that should stop them from learning Ruby/Rails/Puma, etc.. Maybe have a look at issues:\n1725 Errno::ECHILD: No child processes\nand\n1731 uninitialized constant Puma::Cluster. You're welcome.  As to Ruby conferences, at present, no.  If I do (or next year), I'll let you know.  Thanks for asking.... @waghanza\nOn Windows, Socket.const_defined? :SO_REUSEPORT is false.  If you add a conditional for that, Appveyor should pass?. Travis/Ruby OpenSSL version info below.  'trusty' is running an OpenSSL version that is over five years old.\n```\nx86_64-linux\n  trusty OpenSSL 1.0.1f   6 Jan 2014\n  xenial OpenSSL 1.0.2g   1 Mar 2016\nxenial MacOS\n  2.3.8  OpenSSL 1.0.2o  27 Mar 2018\n  2.5.3  OpenSSL 1.1.0h  27 Mar 2018\n``. Closing, see #1718. In the first commit, since/gc statseemed important, I duplicated the code intest_control_gc_statsintotest_control_gc_stats_tcp&test_control_gc_stats_unix`.  I have an unpushed commit that allows the two tests to share code.  Note that JRuby does not increment the count.\nI can 1) add the commit to this PR, 2) wait for another PR, or 3) do nothing.... Certainly don't want to impede the merge-fest...  Rebased (manual intervention required)\nIgnore the Appveyor failure, due to a 'phantom build' on my trunk repo (ruby-loco).  Should run okay now, deleted it.\nI didn't explain the Travis matrix additions, which are about OpenSSL as much as the different OS's.\n\nXenial uses OpenSSL 1.0.2 (Trusty uses 1.0.1)\nOSX/MacOS uses 1.1.0\nAppveyor uses 1.0.2 and 1.1.1 (Ruby 2.5 thru trunk). @evanphx\n\nnio4r uses libev, which isn't Windows compatible (would need to change to libuv).  Thoughts?  I think Appveyor will probably break with this.. > On Windows it will fall back to IO.select I believe\nSorry, I should have looked at it more.  Passed on Appveyor.\nnio4r is aware of the issue, but they thought switching to libuv is (understandably) something they're not interested in changing at this time.  I'm not a c type, so.... The Appveyor build after the rebase is a false positive.  Apologies, I never allowed for gem installation to fail.  It should work now.... JFYI, Using Ruby 2.6 (and earlier, trunk) Puma Travis CI has been failing on tests related to 'clustered restart' since last year.\nSince the tests require fork, the tests in question are skipped on Appveyor/Windows.. The error (uninitialized constant Puma::Cluster) is from puma/dsl.rb:466, which is required by puma/configuration.\nhttps://github.com/puma/puma/blob/ca03c520757e4ed1caab28bf4f60ec2ffe5b2585/lib/puma/dsl.rb#L464-L466\nThe code causing the error was due to commit 64db36c0, 'Enforce a minimum worker_timeout' on 2019-01-24.\nWith the error shown, the call stack starts with puma/control_cli\npuma/control_cli.rb:242 (#start) which requires:\npuma/cli                         which requires:\npuma.launcher                    which requires:\npuma/cluster\nOptions to fix:\n\n\nIn control_cli, move require 'puma/cli' from the start method to the initialize method.\n\n\nIn puma/dsl, add require 'puma/cluster' to the worker_timeout method.\n\n\nAdding require elsewhere.\n\n\n. > I generally don't like having requires in methods unless there's a really good reason.\nAgreed.  I didn't mention that puma/cli.rb requires puma, so I wasn't sure if it was a circular require issue.  I haven't looked into all the various ways one can start Puma, so I left it alone.\nhttps://github.com/puma/puma/blob/2f57c8e403fd76927b484f3945f5f7408186e0c0/lib/puma.rb#L11-L14\nSince it's autoloading three files, maybe add :Cluster?\n. I'll give a try with Autoload.  Dumb question - if we're autoloading Cluster, should the same be done with Single?\n\nActually looks like there are other release plans.\n\nnio4r?  etc?. @ahorek thanks for checking it locally.  I figured it was something unrelated to Puma, as jruby-head seems stable.\nThe Appveyor error is caused by a failed lookup at two keyservers, which is used for the MinGW OpenSSL 1.0.2 package for Ruby 2.4, which is no longer the current MinGW package (which is 1.1.1).  The lookup succeeded for the 32 bit build, but failed for the 64 bit.  Hence, not related to Puma.  I'll see if I can find a reliable way to install the package.  This isn't the first time it's happened...\n. I updated the PR to disable the rubygems update code for jruby.  It passed in my fork, and it's passed here.  When a few more builds have been completed, jruby 9.2.6.0 should be removed from 'allow failure'.... @kares\nThanks for looking into it.\n\nhaven't seen the failures before but if they all look like this one than its very likely smt to do with RubyGems and/or Bundler.\n\nI guessed the same, that's why I forced an update bypassing the gem update --system code for the two JRuby jobs.  The code solved some issues with MRI ruby on Travis.  Both JRuby jobs are now passing, but the fact that gem update --system breaks the jobs is interesting.... @guilleiguaran\nJFYI, last time I checked, Travis ruby-head build (done once a day) is not tested, so if there's an issue in Ruby core...\nAppveyor ruby-head build (3 times a day) is always the most recent passing build, so usually no issue there, but I still recommend that it run 'allow failure'.. @guilleiguaran\nAt present, I'm only windows locally, so I can't really test this.  A quick look at ruby/ruby for WNOHANG showed a spec test with a description of\n'Process.waitpid returns nil when the process has not yet completed and WNOHANG is specified'\nI tried the following code:\n```ruby\nt1 = Time.now\npid = spawn %q[ruby -e \"sleep 5\"]\nret = Process.waitpid(pid, Process::WNOHANG)\nputs \"use WNOHANG ret = #{ret.nil? ? 'nil' : ret}\", Time.now - t1, ''\nt1 = Time.now\npid = spawn %q[ruby -e \"sleep 5\"]\nret = Process.waitpid pid\nputs \"no  WNOHANG ret = #{ret.nil? ? 'nil' : ret}\", Time.now - t1\n```\nand the output was:\n```\nuse WNOHANG ret = nil\n0.003709\nno  WNOHANG ret = 1636\n5.2076754\n```\nHence, this is allowing the tests to pass, but maybe it won't handle waiting for a worker to shut down?\nI don't know if some type of while/until test for a none nil return would work.... @guilleiguaran\nWell, after two typos, the following passed on Travis (on top of #1736).:\nruby\npids = @workers.map &:pid\nuntil pids.empty?\n  pids.each do |p|\n    if Process.waitpid(p, Process::WNOHANG)\n      pids.delete p\n      sleep 0.5\n      break\n    end\n  end\nend\nWasn't sure about modifying the workers array, and I've never liked changing an array in its own loop.\nAs to whether it's ok, I'll defer to the maintainers/owners, as I don't work with forked processes a lot.  Also, some of the tests are taking 5 to 10 seconds, and I'm not sure about that.... @guilleiguaran\nAs to not using -1 with waitpid, I've messed with this previously (maybe in Q4 2018), but never added Process::WNOHANG.  I know I tried waitall, and my (not the best) memory was that it was not good.  Also, if needed, it may be a little easier to debug.  Hence, no specific 'hard' reason.... @guilleiguaran\nJFYI, I added some logging and logic around the sleep function in PR #1741.  It seems to be showing that it is waiting, as exit 0 is shown, which is the same behavior as current code with the logging added.\nHope it's ok that I listed you as a co-author.... force-pushed change - cleaned up sleep logic. @evanphx\nSee https://github.com/puma/puma/issues/1674#issuecomment-472416252\nAs to the code I added, it's replacing line 40 of:\nhttps://github.com/puma/puma/blob/ca03c520757e4ed1caab28bf4f60ec2ffe5b2585/lib/puma/cluster.rb#L35-L44\n\nLine 40 did not change the @workers array, so my code does not. \nSince Process.waitpid(w_pid, Process::WNOHANG) is essentially non-blocking on a running process, I wanted the code to loop thru the whole @workers array, as the original blocked on a running process.  Also, I added code to not add a 'sleep' if the all processes were shutdown.\nI can remove the logging, but it might be helpful to see the 'shutdown' time?. > Is Process.wait without WNOHANG fundamentally broken on ruby now?\n\nAs I think I've mentioned elsewhere, is it a bug or a breaking change?  I tried to create a small repo of it, but being a windows type, I can't fork locally.  Couldn't find a way to repo in Windows.\nThere are a few Ruby tests with waitpid & waitpid2, and also a few specs.  I believe these have continued to pass even when Puma started having issues last year with trunk.\n\nbusy polling loop\n\nI tried to make it as 'light' as possible, with the minimum number of waitpid calls.. I could add a conditional on RUBY_VERSION and/or comments about the code being needed for a bug in Ruby 2.6+.\nSee https://github.com/puma/puma/issues/1674#issuecomment-474142356. Okay with a string (RUBY_VERSION < '2.6'), which will fail on Ruby 2.10?. Done. It's a note I left in for my fork testing.  With Appveyor, there is no way of resetting a project's cache from the web ui, so, over time, the cache may contain old gem dependencies, etc.  I can delete the line if you'd like.. Confused.  Would you like it removed?  I added it for people that may not read the PR/commit description.. Just rebased from today's commits and deleted comment.... RubyGems will soon be releasing v3.x, which will only be Ruby >= 2.2 compatible.\nMaybe use a plain gem update --system and see if it's reliable?  Regardless, this has sat for a while.... Puma is currently testing with Ruby 2.2.x, and I believe <<~ is only available with Ruby 2.3.x. Good point.  My mistake. @AlexWayfer\nThanks.  As mentioned, I did it this way so it would loop thru all the workers on first pass, then go back for the nil returns.  I'd rather use Process.waitpid(-1, Process::WNOHANG), as that's a bit cleaner, but it seems to repeatedly fail on trunk JIT, I never tried with 2.6.x JIT.... I changed the logic to use reject!. I often look here:\nhttps://msp-greg.github.io/ruby_trunk/file.control_expressions.html\nloop do isn't mentioned there.  Time to review kernel methods again...\n. ",
    "ktimothy": "I found it in the docs. Is it intended behavior? If it is, why?. @nateberkopec I mean restarting puma server. Why does not puma reload config file on phased restart (USR1), while it does so with normal restart (USR2)?. ",
    "williamweckl": "I'm having the exact same issue. We moved our Rails app from unicorn to puma recently. The web_concurrency config is the same as we used with unicorn.. @nateberkopec I read it, but I don't agree about this happen the same way at any server when we have scenarios with more than one web worker... As I said, we did not have this issue using unicorn, so I'm not sure about this not being a bug.\nIf I have other free web workers, why not to use them? I just think that unicorn are managing the web workers in a better way, the worker being locked is a thing, other requests trying to use this locked worker is another.. @nateberkopec Can you reopen the issue?. ",
    "brendon": "@nateberkopec, is there a way to stop Puma from being prematurely \"free\"?. ",
    "bnassler": "@pharmmd-rich \nI have the same issue, similar architecture... Any idea how to fix this? I tried concurrency (although I wouldn't claim I fully understood it...)\nAny help welcome!. ",
    "artur-beljajev": "I have temporarily set config.allow_concurrency = true in my local development.rb and it seems working. Though I saw somewhere, this is not a proper solution, so use it at your peril.. ",
    "thiagodiogo": "Any update in this issue? I tried the config.allow_concurrency = true but without success. Yes, thanks @jlleblanc. I moved to Phusion Passenger and it worked fine. I didn't have time to check what is going on in Puma.. ",
    "jlleblanc": "@thiagodiogo can you confirm that Puma is the running webserver? I ask because I was using WEBrick and found that I had to both switch to Puma and set config.allow_concurrency = true.. ",
    "juliancheal": "@jumbosushi aw you beat me to it.\n\n. @nateberkopec All \ud83c\udf4f . I think this would be better on line 16 underneath s.extensions = [\"ext/puma_http11/extconf.rb\"] seems to be more related to extensions than homepage & license. ",
    "zquestz": "This thread saved me a lot of time. Thank you for the info and quick fix.. ",
    "edmz": "My use case is that people that look at some processes don't have a quick way of identifying which project a specific puma process belongs to.. ",
    "ernestojpg": "Hi all, I have updated to latest version of Puma (3.12.0) and I'm still having the same issue: Nginx returns 502 (Bad Gateway) error codes, and I see error messages like this in the logs:\nconnect() to unix:/path/to/socket/puma.sock failed (11: Resource temporarily unavailable) while connecting to upstream, client...\nIt only happens when connecting Nginx and Puma using Unix Sockets and under heavy load (300 new connections per second). I have tried setting the backlog to 1024 in the configuration file:\nbind 'unix:///tmp/run/puma.sock?backlog=1024'\nAnd I have also tried to leave it empty, which should default the value to 1024 as well, but nothing works.\nAny idea?\n. When you restart Puma, have you checked that the file /tmp/puma.sock is created?. ",
    "himberjack": "Any news regarding the 5xx error on load?. ",
    "mitto": "As I wrote a test case, I found that persistent_timeout andfirst_data_timeout have similar problems, so I tried fix it.. @olleolleolle thank you for review.\nI tried rebase the branch.\nbut, rbx-3 and 2.1 build failed.. \ud83d\ude22\nIt seems that the test case irrelevant to this pull request modification fails, but what should I do?. @olleolleolle I tried modifying the build step several times and the build came to be successful but how about this?. ",
    "skk2010": "Possible solution is to use --path=/app/bundle in bundle install --deployment which will move gems outside of app version. Removing old version will be available without risks.\nThis solution looks good for me.. ",
    "bblimke": "MacOS 10\nThis doesn't seem to be an issue on Heroku, just on my dev machine.. @nateberkopec thanks for pointing me to that stackoverflow question. It looks like it's a problem with Ruby on MacOS, and not something that Puma or Rails can fix.\nIn that case please feel free to close this issue.\n. ",
    "aryeh-looker": "No problem, thanks @nateberkopec. Just to clarify \u2014\u00a0this means that the socket is simply unavailable, correct? It doesn't merely mean that the socket is blocking writing for a long period of time?. ",
    "claudiob": "\nCome up with a good release name after the band\n\n\"Universal Pulse\" ?\nhttps://en.wikipedia.org/wiki/Universal_Pulse\n. ",
    "rafaelfranca": "\ndunno who 5.2 release manager is\n\nI'm \ud83d\ude04 . Thanks!. ",
    "nash8114": "The readme doesn't use config.ru, but the github wiki page suggests a config.ru, and states require 'main'. Perhaps that is what is being referred to by the OP?\nLink to the wiki page:\nhttps://github.com/puma/puma/wiki/Running-sinatra-classic-with-puma\n. ",
    "tuda2008": "rvm reinstall [your ruby version] --with-openssl-dir=/[your ssl dir]\nfor example\n\nrvm reinstall 2.4.0 --with-openssl-dir=/usr/local/src/openssl-1.1.0e\n\nthen puma start is ok. ",
    "eallison91": "Didn't mean to include those other commits.. I am in the process of adding tests, but need some guidance on how to make them succeed in travis and appveyor. Based on the existing tests in test_binder.rb, it looks like things pertaining to SSL are skipped when running on appveyor. Is that accurate?\nI also have one test that specifically should be run when the engine is JRuby (possibly still not on appveyor?). I am using skip unless Puma.jruby? Is that the correct way to achieve that?. Attempt to rebase changes resulted in branch getting recreated... lesson learned.\nYet another PR since I can't reopen it. https://github.com/puma/puma/pull/1478. https://github.com/puma/puma/issues/996. Any update here? Many of the failures do not appear to be related to my changes and I'd like to resolve this.\nIf there's a way to run the travis and/or appveyor tests locally, I would welcome any information on it. At this point, all tests pass locally.. ",
    "crevete": "I've got the same issue and go back to 3.10 as workaround for now.. @nateberkopec Why did you delete my comment ? Can you please explain me why you continue to release new versions without fixing this blocker issue ? . ",
    "jxa": "I get the same. The problem stems from an option mismatch between pumactl and puma cli. On puma cli the option is --control and in pumactl the option is called control-url. \nPutting together a PR as soon as I figure out how to write a test for this.. https://github.com/puma/puma/pull/1487 is a fix, but I need some help getting the new test to pass on CI. If anyone has advice or the time to look into it I'd appreciate the help. . The fix is merged to master but not in time for the latest release. It would be really swell if they did a bugfix release for this issue.\nWhat do you think @schneems  ?. Also reported here https://github.com/puma/puma/issues/1477. an alternative approach is to simply revert #1416, and write a spec for pumactl. downside being that the options do not match across both interfaces.. I'm open to options on how to actually fix the CLI @evanphx @nateberkopec @schneems.\nWhatever the ultimate decision, I think this is an extremely valuable test which will prevent similar accidental breakage in the future.. /sigh that is, it will be valuable if I can get it to pass reliably on CI. Travis build is failing for ruby 2.1, but not just on this PR. These other PRs have the same failure: \n https://travis-ci.org/puma/puma/builds/325036450 \n https://travis-ci.org/puma/puma/builds/323477393. @nateberkopec sorry I missed your mention somehow! I have rebased, but the appveyor build has failed on 2 versions while trying to compile with ragel. I doubt these are related to my changes. Any advice on how to deal with this issue?. Prior to this change, send_request was being called with @command == \"start\". The control server does not respond to this command, so returns a 404, which then ultimately calls exit 1, making it very difficult to figure out why my test was failing silently. This early return short-circuits all that mess.. ",
    "chickenham": "I noticed that a new version of puma was released but this is still not fixed.  Is this going to be fixed at some point - or is there a workaround?. ",
    "jdoss": "This is a blocker on us upgrading Puma as well. . ",
    "frankpinto": "Just ran into this myself while upgrading from Upstart in Puma 14.04 to 16.04 and wanting to use pumactl start as the ExecStart. Any updates?. ",
    "sigmike": "Yes it's fixed in 3.12.0. Thank you.. ",
    "arzavj": "Would love to see this in soon too! . ",
    "ranupratapsingh": "I am also getting the same behavior. This happens when state_path is specified in config/puma.rb. On stop command puma is deleting the pid file while not deleting the state file. I am not sure whether it should be deleting state file as well. As per prepare_configuration method of puma/lib/puma/control_cli.rb if @state is present then the pid value is read from state file instead of pid file and it just assumes that the process exists and proceeds to either send_request or send_signal.\nI can think of 2 scenarios to fix this.\n  1. check whether the puma process with that pid is actually running before proceeding to either send_request or send_signal.\n  2. make puma delete state file on shutdown/halt. In this case there may be some edge cases when the file is not deleted during some not graceful shutdown. \nI need some expert advice on which fix to go with. I can pick this one.. ",
    "james-ai": "I'm experiencing the same behaviour.\nOS: Amazon Linux 2017.09\nRuby: 2.4.2p198\nPuma: 3.11.2. ",
    "oneiros": "I believe the original report and @ranupratapsingh's reply do reference the same issue.\nI too ran into this problem today and I can confirm that pumactl status will happily report a running puma as long as it gets a non-nil pid from somewhere. At least when no control server is running.\nI believe this behavior crept in with ad3d867f808eb4b9c7e28a3a9622f9564a08fed0. Before that, testing if there was an actual process with this pid prevented this (in most cases).\nSadly, I do not have an idea of how to fix this either, sorry :frowning_face: . ",
    "ademars94": "+1 Also experiencing issues with chunked request.. Update: This is still an issue on v3.11.x, and the test from @sheldonh does seem to reproduce the bug (although it only fails sometimes).. ",
    "jerryjohnjacob": "Facing the same issue. Happens only on puma 3.11.0. Reverted to 3.10.0 and the error disappeared. I'm on Ruby 2.4.2 and Rails 5.1.4.. @igravious No, haven't faced that issue before. The closest is that the application server or console refuses to quit after sending a SIGINT. But this is only in development mode and I'm quite positive it is due to spring.. @Petercopter I'm no contributor or member of the Puma project, but I do think it'd be helpful if there were additional details so that when someone does work on this, they have a good foundation to build on :). ",
    "igravious": "@jerryjohnjacob Do you get any hanging Rails server sessions in development mode or production mode using the combo of Ruby 2.4.2 and Rails 5.1.4 and Puma 3.10.0 either with or without SSL enabled? I'm troubled that the stack I'm using is too bleeding edge.  :face_with_head_bandage: Apologies for polluting this bug thread folks! :cactus: . @jerryjohnjacob I've noticed that SIGINT (hitting Ctrl-c you mean?) doesn't immediately cause Puma to quit, it does eventually. I am impatient so I find to the process id and shoot it in the head.\n@drewish Which? The main SSL issue or the Rails server sessions in dev mode hanging thing?. ",
    "drewish": "I've see this on Ruby 2.3.5 with Rails 4.2.10.. I'm not sure what the main SSL issue is but I've had it hanging occasionally in dev mode with the \"no implicit conversion of nil into String\" errors but also seeing another error from the reactor. I'll bump back up to 3.11.x and get an exact copy of that error message.. ",
    "Petercopter": "Same problem here. Trying to implement SSL for localhost. Ruby 2.4.2 and Rails 5.2beta. Things appear to be fine, just seeing the error message on load. It seems intermittent, more likely to happen on empty pages (a react router endpoint). The word 'Reactor' sometimes comes up as well.\nLet me know if I can provide more details, this is happening in a small app, and I can probably isolate a test case.\nThis is Puma 3.11. Going back to 3.10 doesn't appear to have the same issue.. Seeing something similar while trying to convert our test suite from Capybara/Webrick/Poltergeist/SSL over to Capybara/Puma/Selenium/SSL. This is happening in Codeship. I haven't seen it happen locally yet.\nError in select: closed stream (IOError)\n/home/rof/cache/bundler/ruby/2.4.0/gems/puma-3.11.4/lib/puma/reactor.rb:29:in `select'\n/home/rof/cache/bundler/ruby/2.4.0/gems/puma-3.11.4/lib/puma/reactor.rb:29:in `run_internal'\n/home/rof/cache/bundler/ruby/2.4.0/gems/puma-3.11.4/lib/puma/reactor.rb:154:in `block in run_in_thread'\nEDIT:\nRunning locally on macOS I just got a bunch of this:\nError in reactor loop escaped: Invalid argument (Errno::EINVAL)\n/Users/krutzp/.rbenv/versions/2.4.4/lib/ruby/gems/2.4.0/gems/puma-3.11.4/lib/puma/reactor.rb:29:in `select'\n/Users/krutzp/.rbenv/versions/2.4.4/lib/ruby/gems/2.4.0/gems/puma-3.11.4/lib/puma/reactor.rb:29:in `run_internal'\n/Users/krutzp/.rbenv/versions/2.4.4/lib/ruby/gems/2.4.0/gems/puma-3.11.4/lib/puma/reactor.rb:154:in `block in run_in_thread'. ",
    "sanjibukai": "Same problem here.\nI'm using docker (ruby:stretch image with ruby 2.4.2p198) with rails 5.1.4.\nSwitching to puma 3.10 solves the problem.. ",
    "dayudodo": "when I change  puma from 3.11.0 to  3.10, the errors like:\n\nError reached top of thread-pool: no implicit conversion of nil into String (TypeError)\n\ndisappeared, thanks ! \nand don't forget to add config.force_ssl = true in your development.rb if you're in the development mode.\nruby version : ruby 2.4.1-p111\nrails version: 5.1.4\nhowever, the new error occur:\n\nSSL error, peer: 127.0.0.1, peer cert: , #\n\nhow should I do?. a little different:\nError reached top of thread-pool: EOFError (EOFError)\n2018-02-06 14:23:51 +0800: SSL error, peer: 127.0.0.1, peer cert: , #. ",
    "SyxALM": "# Can any one help me, There is a same issue with the production environment.\n<TypeError: no implicit conversion of Puma::MiniSSL::Socket into Integer\nin my environment,rb file config.force_ssl = false \nPuma version 3.10.0\nRails version Rails 4.0.12\nRuby version ruby 2.2.2p95\nAny fixes for same let me know.. ",
    "kdanaher1986": "I'm getting back into developing apps again after taking a break, and now having the worst problems launching a brand new app to the server, which I've never had or seen before, even in tutorials. This comes up in the terminal whenever I run rails s:\n\nBooting Puma\n=> Rails 5.1.5 application starting in development \n=> Run rails server -h for more startup options\nPuma starting in single mode...\n Version 3.11.3 (ruby 2.4.1-p111), codename: Love Song\n Min threads: 5, max threads: 5\n Environment: development\n Listening on tcp://0.0.0.0:3000\n\nI have never seen this issue before and very confused what's causing it. \nI upgraded the puma gem from 3.7 to 3.11 but still having the same error.. ",
    "gonace": "Problem solved, faulty $PATH for the user.. ",
    "nijikon": "Can this be merged?. @schneems Support for ruby 2.2 ended on March 31, 2018. I think that we should utilize new features of the language, people who use 2.2 could still use puma but to a particular version which I think it's fine.. ",
    "karpa13a": "what exact error from nginx do u see?\nfrom error_log i mean. can you post whole log?\nexample about *8 request?\ni see, there is starting a reply from upstream:\n2017/12/27 08:22:54 [debug] 5808#0: *8 http upstream process header\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http proxy status 304 \"304 Not Modified\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http proxy header: \"X-Frame-Options: SAMEORIGIN\"\n        2017/12/27 08:22:54 [debug] 5808#0: *8 http proxy header: \"X-XSS-Protection: 1; mode=block\"\nusually pair of http upstream recv(): -1 (11: Resource temporarily unavailable) / http upstream dummy handler\nmean that upstream not ready to send data. ie upstream currently processing request.\n. @benoittgt memory_profiler i'll use after i get \"fat\" request. but i need a \"light\" version to use in production(\ni'm looking at https://github.com/MiniProfiler/rack-mini-profiler, description says,\ncan log allocations/memory usage. but i cannot locate my case( . well\nafter switching to jemalloc, memory usage looks like\n\n\"commited\" going to sky. this is VSZ from ps aux output. RSS size looks ok.\nbut, i think this situation with server \"not optimal\". ",
    "EnziinSystem": "Why you closed when we did not solve the problem?\nThanks.. ",
    "evgeniy-kunitsa": "I have the same problem. I wrote simple rack app to save image which was received via \"chunked\" request. But some data was lost.\n```ruby\nrequire 'rack'\nclass Proxy\n  def call(env)\n    f = File.open(\"#{Time.now}.jpg\", 'wb')\n    f.write(env['rack.input'].read)\nputs f.size\nf.close\n\nsuccess\n\nend\ndef success\n    [200, {}, ['Success!']]\n  end\nend\nrun Proxy.new\n```\nNew file size is 4623975, but original file size is 4623987.\nCommand to send file with chunked encoding:\ncurl -v -X POST --header \"Transfer-Encoding: chunked\" --data-binary @original.jpg \"http://localhost:9292/\"\nSystem configuration\nRuby version: 2.4.2\nRack version: 2.0.3\nPuma version: 3.11. ",
    "sheldonh": "Confirmed in 3.11.2.\nI think the problem is that the Tempfile is rewound but not flushed in Puma::Server#read_body.\n. This is hard to test. I The test below passed 284 times before failing!\nThe test:\n```\n  def test_chunked_large_request\n    bytes = 0\n    reads = 0\n    @server.app = proc { |env|\n      while buf = env['rack.input'].read(4096)\n        reads += 1\n        bytes += buf.size\n      end\n      [200, {}, [\"\"]]\n    }\n@server.add_tcp_listener @host, @port\n@server.run\n\nsock = TCPSocket.new @host, @server.connected_port\nsock << \"GET / HTTP/1.1\\r\\nConnection: close\\r\\nTransfer-Encoding: chunked\\r\\n\\r\\n\"\n64.times { sock << \"4000\\r\\n\"; sock << \"#{'x' * 0x4000}\\r\\n\" } # Net::HTTP\nsock << \"0\\r\\n\\r\\n\"\n\ndata = sock.read\n\nassert_equal \"HTTP/1.1 200 OK\\r\\nConnection: close\\r\\nContent-Length: 0\\r\\n\\r\\n\", data\nassert_equal 64 * 0x4000, bytes\nassert_equal 256, reads\n\nend\n```\nThe failure:\n```\n$ i=0; while ruby test/test_puma_server.rb --name test_chunked_large_request; do i=$(($i + 1)); sleep 0.001; done; echo $i tests completed successfully\n...\n  1) Failure:\nTestPumaServer#test_chunked_large_request [test/test_puma_server.rb:581]:\nExpected: 1048576\n  Actual: 1052672\n1 runs, 2 assertions, 1 failures, 0 errors, 0 skips\n283 tests completed successfully\n```\nWhen I add a flush before rewind to the Tempfile in Puma::Server#read_body, the test still runs successfully most of the time, but then it hangs instead of failing.. ",
    "nhaglind": "Is there a better way around this? I also have a similar situation in which I need to use GET to gather long parameters and I'm not sure if's that specific of a use case.\nEdit: I guess you could just save it as a POST and capture the params in the controller. \n2018-03-22 11:08:51 -0500: HTTP parse error, malformed request (): #<Puma::HttpParserError: HTTP element QUERY_STRING is longer than the (1024 * 10) allowed length (was 16162)>. ",
    "rrosen": "seeing this too when I try to run https://github.com/puma/puma-dev/\nOSX 10.11 El Capitan\nRuby 2.4.1\nPuma 3.10\n. Thanks @seanstewart!  I had to run \ngem install puma -v '3.11.0' -- --with-cppflags=-I$(brew --prefix openssl)/include\n(note the extra --)\n. ",
    "stetsonthree": "Same error\nMac OS 10.13.2 High Sierra\nRuby 2.5.0\nRails 5.0.6\nPuma 3.11.0\nHowever, after looking at issue #1494 (which contains a similar error I experienced) I decided to run:\ngem install puma -v '3.11.0' and then rails server or bin/rails server ran without error...until I opened a new tab in my terminal. Then I received the following error:\nCould not find rake-12.3.0 in any of the sources\nRun `bundle install` to install missing gems.\nSo I tried bundle install and now the original error is back.\n  . ",
    "jmealo": "FWIW, This is related to compiling uws. I'm having the same issue right now on OSX.. ",
    "toroc": "I'm also having this issue on OSX.. ",
    "pjscopeland": "I was having this issue in a test suite. Downgrading Ruby from 2.4.2 to 2.3.6 fixed it for me.. ",
    "seanstewart": "Try pointing puma to your brew-installed OpenSSL installation:\ngem install puma -v '3.11.0' --with-cppflags=-I$(brew --prefix openssl)/include\nOR\nConfigure your system to always build puma with the additional include paths:\nbundle config build.puma --with-opt-include=/usr/local/opt/openssl/include\nbundle install. ",
    "2potatocakes": "Thanks @seanstewart \ud83d\udc4d  - the --with-opt-include flag option you suggested when using bundle.config didn't work for me, but it works fine using your other suggestion with 2.5.1p57\nbundle config build.puma --with-cppflags=-I$(brew --prefix openssl)/include. ",
    "voxik": "Now I noticed that the \"TestPumaServerSSLClient\" tests suffers the same issue. BTW this was introduced in d421a538252751b0cccf4638af. For me, the situation was improved by 6a7112d51b3de3439c4be512f68a8d4e33e02665. @guilleiguaran I applied your patch and run 30 builds on different architectures and none of the builds failed with the error in question, so you are probably right :relaxed: . The patch from #1741 appears to work for me. Thx :+1: . ",
    "jtzero": "another data point, \nI have the same issue but on\nruby 2.4.3p205 (2017-12-14 revision 61247) [x86_64-darwin16] \nand Sierra (not high sierra)\n. ",
    "rrevi": "So I updated to 3.11.2 to fix this issue. While the issue log errors mentioned here no longer show up, I am now getting the following errors in the puma.log file with most HTTP requests (over SSL of course):\n\n=== puma startup: 2018-02-01 08:00:55 -0500 ===\n=== puma startup: 2018-02-01 08:00:55 -0500 ===\nError reached top of thread-pool: EOFError (EOFError)\nError reached top of thread-pool: EOFError (EOFError)\nError reached top of thread-pool: EOFError (EOFError)\nError reached top of thread-pool: EOFError (EOFError)\nError reached top of thread-pool: EOFError (EOFError)\nError reached top of thread-pool: EOFError (EOFError)Error reached top of thread-pool: EOFError (EOFError)\nError reached top of thread-pool: EOFError (EOFError)\n\nanyone have any idea what could be going on? I haven't noticed any bad user experience yet (e.g. puma crash or http request hang)\nThanks,\nRafael. ",
    "rssluca": "Same issue here although non disruptive.. ",
    "MarkMurphy": "Also having the same problem. ",
    "gcoderre": "I'm seeing the same issue after updating to 3.11.2 while using SSL:\nPuma: 3.11.2\nRuby: 2.4.3p205\nRails: 5.1.5\nOS: macOS 10.13.1. ",
    "kylekyle": "Also getting this with the exact same setup and versions as @gcoderre but on Rack/Roda not Rails. . Can we reopen this? I'm seeing it on both OS X and RHEL7 ..... ",
    "dbpqdb": "+1\nSeeing same issue as @rrevi on Puma 3.11.2, Ubuntu 16.04, Ruby 2.4.3p205, Rails 5.1.4. ",
    "tuwukee": "hey @evanphx,\nError reached top of thread-pool: EOFError (EOFError) is fixed with your commit b9ff900\nbut I'm still getting EOFErrors with puma server\n```\n2018-03-06 14:24:05 +0300: Read error: #\n/Users/july/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/puma-3.11.3/lib/puma/client.rb:292:in try_to_finish'\n/Users/july/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/puma-3.11.3/lib/puma/client.rb:106:inreset'\n/Users/july/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/puma-3.11.3/lib/puma/server.rb:450:in process_client'\n/Users/july/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/puma-3.11.3/lib/puma/server.rb:302:inblock in run'\n/Users/july/.rbenv/versions/2.4.3/lib/ruby/gems/2.4.0/gems/puma-3.11.3/lib/puma/thread_pool.rb:120:in `block in spawn_thread'\n```\nYour change adds exception handling for client.eagerly_finish method call https://github.com/puma/puma/blob/master/lib/puma/server.rb#L280\nThe EOFError is also can be thrown by client.reset method https://github.com/puma/puma/blob/master/lib/puma/server.rb#L450\nCan you fix this one as well please?\nThanks!. ",
    "bruno-campos": "I'm also seeing the same error reported by tuwukee on 3.11.4\n2018-06-01 12:55:57 +0000: Read error: #<EOFError: EOFError>\n/usr/local/bundle/gems/puma-3.11.3/lib/puma/client.rb:292:in `try_to_finish'\n/usr/local/bundle/gems/puma-3.11.3/lib/puma/client.rb:106:in `reset'\n/usr/local/bundle/gems/puma-3.11.3/lib/puma/server.rb:450:in `process_client'\n/usr/local/bundle/gems/puma-3.11.3/lib/puma/server.rb:302:in `block in run'\nRuby 2.5.0\nRails 5.2.0\n. ",
    "florin555": "Hi! Thanks for the feedback.\nI made some exploratory commits and I was able to isolate the source of the problem to the fact that in that particular test I used 2 sleeps and wrote to the socket 2 times. All other tests sleep and write to the socket at most once.\nOtherwise the test follows the same pattern like all the other tests.\nI was not able to reproduce the failure on my computer, and also Travis CI is passing, so I believe it must be something specific to Appveyor, but I don't have a way to debug further.\nThe test needs the 2 sleeps because the bug that the test tries to reproduce happens only in that case, so I can't just change the test because then it becomes meaningless.. https://github.com/puma/puma/issues/1480 is different. This pull request fixes other issues.. @olleolleolle, @nateberkopec could you please merge this, now that the build is green?. ",
    "mubashirhanif": "@florin555 and I think that the failing test is because of a potential bug in ruby on windows.\n```\nhttps://github.com/preisanalytics/puma/blob/382cdaf2cdbb781f91d7e0ee8a36c6d8b4c88c66/lib/puma/client.rb#L203\ndef read_chunked_body\n      ...\n        rescue Errno::EAGAIN\n          return false\n        rescue SystemCallError, IOError\n          raise ConnectionError, \"Connection error detected during read\"\n        end\n      ...\nend\n``\nOn Linux and Mac the exceptionErrno::EWOULDBLOCKis just an alias toErrno::EAGAIN`.\nOn windows, these exceptions point to 2 different objects/exceptions, which causes the code to raise a wrong exception instead of returning false(the expected behaviour).\nAlso note [Errno::EAGAIN::Errno, Errno::EWOULDBLOCK::Errno] outputs\n[11, 11] on linux, [35, 35] on Mac and [11, 10035] on windows(verified on 1.9.3 and 2.3.3). \nIn this ruby-doc it says that both Errno::EWOULDBLOCK and Errno::EAGAIN are extended by IO::WaitReadable(verified on both Windows and Linux). And also since 1.9.3 is support is dropped by Puma. So I think we ca we can safely replace Errno::EAGAIN and Errno::EWOULDBLOCK with IO::WaitReadable in case of exception from IO::read_nonblock method.. @olleolleolle The current failing builds on Travis are inherited from the current state of master branch.. ",
    "pschrammel": "hi, sorry pushing: Are there any merge blockers? How can this go upstream? . ",
    "mirelon": "I have the same issue for Puma 3.7.0:\n2018-02-13 11:36:05 +0100: SSL error, peer: <ip>, peer cert: , #<Puma::MiniSSL::SSLError: OpenSSL error: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request - 336027804>\nError in reactor loop escaped: undefined method `closed?' for #<Puma::MiniSSL::Socket:0x007f969584f2c8> (NoMethodError)\n(eval):2:in `closed?'\n<path>/vendor/bundle/ruby/2.3.0/gems/puma-3.7.0/lib/puma/reactor.rb:31:in `block in run_internal'\n<path>/vendor/bundle/ruby/2.3.0/gems/puma-3.7.0/lib/puma/reactor.rb:31:in `any?'\n<path>/vendor/bundle/ruby/2.3.0/gems/puma-3.7.0/lib/puma/reactor.rb:31:in `rescue in run_internal'\n<path>/vendor/bundle/ruby/2.3.0/gems/puma-3.7.0/lib/puma/reactor.rb:28:in `run_internal'\n<path>/vendor/bundle/ruby/2.3.0/gems/puma-3.7.0/lib/puma/reactor.rb:153:in `block in run_in_thread'\n. ",
    "sb8244": "I am seeing this on 3.11.2 as well. ",
    "LeZuse": "@jjb This is awesome work! Running on Heroku too and wanted the same visibility into the runtime and I just found this issue, keep the work going \ud83d\ude4f ! I'd be happy to assist with a plugin if needed.. Based on @jjb's version I have turned it into a Puma plugin and utilized some of the plugin API. Also added dynamic support for single/clustered mode. Notice it logs it in a format that other tools like Librato can interpret and make graph out of.\nGist: https://gist.github.com/LeZuse/022c501143a5bc4b51a601ad2a1b8abc. I am curious, how does the server behave in such a situation? Is it just hanging there or does it crash? Does it accept new connections? We are experiencing a problem which might be linked to your issue.\nRunning single mode puma on heroku and occasionally one of the dynos simply stops responding and even restart attempts end with heroku exit timeout. . ",
    "vfonic": "You could specify different timeout for Rails.env.development?. I wouldn't rely on request timeout locally.. ",
    "fertobar": "ok, thanks for the update. ",
    "ryanmk54": "Sorry, I forgot to say that I am using WSL Ubuntu on Win10. I've edited the original comment\nI did a ls tmp/sockets which showed there was an existing socket. Next, I did a rm tmp/sockets/puma.sock. An ls shows the socket is gone. Running rails s gives the original error and leaves the socket file.. Using the below config, I was able to get puma listening on a tcp socket and a unix socket\n    Windows 10 Pro, Version 1709, OS build 16299.192\n    Puma 3.11.0, ruby 2.4.1-p111\nIf I do a bundle update to puma 3.11.4, I am able to listen on a tcp socket, but not a unix socket. The socket is created, but I get the Address already in use error as mentioned in a comment above.. ",
    "adampope": "We have just experienced this issue too trying to get puma-dev and puma working on WSL Ubuntu.  Running puma in the rails project directory boots fine on 0.0.0.0:3000 but using the sock file setup by puma-dev we get exactly the same error as @ryanmk54.\nDowngrading from 3.11.4 to 3.11.0 has fixed the problem.  Using ruby 2.5.1.\n. ",
    "salihsagdilek": "@adampope same problem same solution.. ",
    "Kulgar": "Also same problame with same solution. \nHad to downgrade from 3.12.0 to 3.11.0. ",
    "dstokes": "I've reproduced the same issue with USR1, USR2 and systemd restarts with a systemd.socket configured.. ",
    "b10s": "Have the same error with the next snippet. ",
    "chrisvel": "Same error, tried a small app in Arch and BSD. I don't get the pattern but it seems it randomly fails. . ",
    "aditya01933": "We are facing this issue with puma 3.12.0:\n```\nLog output:\n| Puma starting in single mode...\n\u00a0 | * Version 3.12.0 (ruby 2.3.5-p376), codename: Llamas in Pajamas\n\u00a0 | * Min threads: 1, max threads: 10\n\u00a0 | * Environment: production\n\u00a0 | ! Unable to load application: NoMethodError: undefined method stop' for nil:NilClass\n\u00a0 | /usr/local/bundle/gems/puma-3.12.0/lib/puma/single.rb:27:instop': undefined method stop' for nil:NilClass (NoMethodError)\n\u00a0 | from /usr/local/bundle/gems/puma-3.12.0/lib/puma/launcher.rb:147:instop'\n\u00a0 | from /usr/local/bundle/gems/puma-3.12.0/lib/puma/launcher.rb:412:in block in setup_signals'\n\u00a0 | from /usr/local/bundle/gems/json-1.8.3/lib/json/common.rb:155:inparse'\n```\n@grosser @b10s @chrisvel Have anyone of you found the solution of the issue?. ",
    "DanielRedOak": "This isn't a bug per-say but I'm hoping there is a way to accomplish this type of behavior without going back to Unicorn :). Is there a puma specific IRC channel or other avenue for support/discussion that would have a better turn around time?. Thanks for the reply. Unicorn essentially spawns a new master from the old, then workers, finally allowing the original master to be killed. They outline the process here: https://bogomips.org/unicorn/SIGNALS.html\nWould the best approach with Puma just be separating out the Ruby install from the rest of the package then? It's quite convenient to  be able to rely on the package to include its own supported version of Ruby but things will be changing when we complete our move to Docker.  Any other pointers are appreciated.. ",
    "letrungkien": "Thank you very much @nateberkopec \nGood day to you.. ",
    "lykaios": "Thanks for this work. \nHas it already made it into a release? When I try using the API I get these error: \n``\nNameError: uninitialized constant Puma::DSL::PumaStats\n  config/puma.rb:19:inblock in _load_from'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/configuration.rb:276:in call'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/configuration.rb:276:inblock in run_hooks'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/configuration.rb:276:in each'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/configuration.rb:276:inrun_hooks'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/cluster.rb:451:in run'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/launcher.rb:184:inrun'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/lib/puma/cli.rb:78:in run'\n  /Users/nate.allen/.rbenv/versions/2.1.10/lib/ruby/gems/2.1.0/gems/puma-3.11.4/bin/puma:10:in'\n```. You can disregard - I ended up going with a different approach. . ",
    "rianmcguire": "That's a great idea, thanks @stereobooster. Looks like something similar is possible with haproxy:\nhttp-request set-header X-Request-Start \"%tr\"\nI'll still need to subtract the request body read time, but that's an easy way to measure the socket backlog + threadpool wait times. I was a bit stuck on measuring the time in the socket backlog, since puma stops accepting connections when the pool is full, and there's no socket API for determining how long a connection has been waiting.. Looks like I have a test failure on JRuby. I'll investigate that in the next couple of days.. @schneems Yes, that's exactly what I'm after.\nI'm using the same approach as SamSaffron described with the upstream timestamp header. haproxy can't buffer the entire request body like nginx, so I wanted to account for the time spent in puma's reactor waiting for the body.\nIf there's a broader plan to make the Puma.stats backlog metric accurate, I'd prefer that to this very specific (and complicated) approach of calculating the wait time.\nI like your \"negative backpressure\" idea. The disadvantage versus a backlog or wait time metric is it doesn't let you quantify how far over capacity you are, but I'm not sure how much that matters in practice.. ",
    "hoshinotsuyoshi": "Thank you!. ",
    "esparkman": "CFPropertyList seems to be a dependency on the fog gem. I'm not sure why issuing the restart command would fail to load dependencies. Thoughts?. ",
    "DangerDawson": "@esparkman did you managed to solve this? I am seeing this behaviour.\nIf I downgrade to 3.8.1 the problem ceases to exist. @esparkman the issue for me was being caused by the following:\nI installed tmuxinator using bundler for a different gemset\nWhen I then started puma using tmuxinator it was picking up Bundler::ORIGINAL_ENV from my previous environment, so when puma restarted it was looking for gems from the gemset that tmuxinator was installed under.\nDoes the above make sense?\n. ",
    "alanhala": "@nateberkopec Yes, it works as I expect. It handles the 5 request simultaneously. I think it's a Chrome issue. I've just checked in Safari and it works the same way as curl.\nIn my opinion, for some reason, Chrome is not sending all the requests simultaneously when they are all completely identical. Maybe it could be a Chrome's bug.. ",
    "chrismo": "Well ... I've done some poking around -- some of what follows might actually be true:\nGEM_HOME/GEM_PATH together are the mechanism in Bundler that allows you to have local bundles. By default, it's not set, and so Bundler will look for gems in your Ruby's default gem location, and you've seen the paths that Bundler will default to for searching for gems. If you have multiple projects, then, by default, everything shares the same directory structure, in your system Ruby (or current rbenv version, etc). This mostly should be fine, but if you have anything like a /bin file that gets installed, you can get things clobbering other things from different versions of the same gem.\nIf you'd rather keep all your bundles isolated in each project, you can use bundle install --path ./.bundle (or bundle install --path ./my_project_gems - path name doesn't matter). Using this will store your bundle config such that GEM_PATH will get overridden and set to your specific path.\nThe setting in ansible-rbenv-role is presuming gems will always (and only) be installed into $HOME/.gems? (It's appending that to the path, but as you've seen by default it's not set to anything). I don't know enough about what it's trying to do to have an opinion about that. But you removing that setting in ansible-rbenv-role allows Bundler to look for gems in the default locations, which is probably where it should be looking.\nTurning off Puma's prune_bundler setting appears to have worked because presumably it would be running already in a production environment where Bundler was invoked with the '--deployment' flag keeping the gems in 'vendor/bundle' (https://github.com/bundler/bundler/blob/1-16-stable/lib/bundler/cli/install.rb#L172).\nprune_bundler true looks up all of the gems that are required, passes them to puma-wild and then loads them all directly with the gem command ... which if ansible-rbenv-role has had its way, has told it to only look in the home directory. \nSo ... turning off either one of them appears to keep things as expected and load the gems from (I presume) vendor/bundle.\nMy 10-and-a-half cents - that may not explain it all, but should at least keep the conversation going. :). (/me waves to @evanphx). ",
    "chadwilken": "@stereobooster I could probably reproduce it if I make an example Rails app and share the repo. Would that be sufficient?. I tried replicating locally and was unable to do so, not sure what the issue was but I guess we can close for now.. ",
    "themikejr": "Gentle bump :-) My team would benefit from this.. ",
    "mmizani": "gentle bump, would be great to have this feature.  . ",
    "hshyk": "Has there been any updates with this? This would be great if we could disable TLSv1. Also, would be nice for configuration to disable TLSv1.1.. ",
    "josler": "cc @jasquat if you wanted to try this out. ",
    "jasquat": "Thanks @josler  and @dannyfallon. This does seem to have fixed my issue.. We also had this issue.\nOur setup is using 16 workers with 1 thread each running with an nginx server in front. We were getting high wait times between nginx and rails as measured by a middleware similar to the one at https://github.com/puma/puma/issues/1405#issuecomment-330806817. We partially undid the change in this pull request with the patch below and our wait times dropped as can be seen by the attached screenshot.\nruby\nmodule PumaMonkeypatch\n  def wait_until_not_full\n    @mutex.synchronize do\n      until @todo.size - @waiting < @max - @spawned or @shutdown\n        @not_full.wait @mutex\n      end\n    end\n  end\nend\nPuma::ThreadPool.prepend(::PumaMonkeypatch)\n\nThoughts on how to proceed? Should we file a new issue?. Thanks. Ticket created: https://github.com/puma/puma/issues/1471. ",
    "sevenseacat": "I added the security exception in Firefox and I'm still seeing the error in my logs.\nRuby version: 2.5.1\nRails version: 5.1\nPuma version: 3.12.0. False alarm - there were a few other domains I needed to add security exceptions for as well!. ",
    "aardvarkk": "This may be related to the fact that the request has both a Version header and the version also being passed as part of the first line of the HTTP request. Perhaps they both get parsed (and combined), even though I don't think Version is a real HTTP header?. I have confirmed that this issue is avoided if the request does not include a Version header. That being said, this page states that the Version header is provisionally valid, but is supposed to indicate a version of an \"evolving object\", not the HTTP version. It appears to be processed as if it's a second specifier for the HTTP version, which would be incorrect.. ",
    "f3ndot": "I additionally expect that the string would be UTF-8 encoded in ruby:\n```ruby\nenv['HTTP_X_FOO']\n=> \"\u00a3 rates\"\nenv['HTTP_X_BAR']\n=> \"woo\"\nenv['HTTP_X_FOO']&.encoding\n=> #\nenv['HTTP_X_BAR'].encoding\n=> #\n```\n. ",
    "ahorek": "Well, nginx should support 103 https://www.nginx.com/blog/nginx-1-13-9-http2-server-push/\n@skyksandr but 1.12.2 may not support it yet.. I can't replicate the failure locally, but I saw this on different repositories a few times after jruby-openssl 0.10.2 was released.\nIt might be something wrong on travis, broken caches or something... @kares have you seen this before?. ",
    "skyksandr": "Thank you for answers and sorry for bothering you. Looks like it is nginx issue.. ",
    "junegunn": "Yeah, David's patch tackles the same problem, just in a slightly different way, so let me close this. Thanks.. ",
    "lucasgerard": "Hey, having the same issue here, with the same system configuration except for rails, which is 4.2.10. ",
    "kendrikat": "Same here on heroku when dynos are restarted:\nruby 2.5.0p0\npuma 3.11.2\nEDIT: Related https://github.com/puma/puma/issues/1438#issuecomment-337722378 which seems fixed in bundler 1.16.x Heroku is still using 15.x and I didn't found any painless method to set bundle config disable_exec_load true on heroku. . ",
    "DennisWormald": "@kendrikat in your heroku cli run: \nheroku config:set \"BUNDLE_DISABLE_EXEC_LOAD\"=\"true\". ",
    "hugoarpin": "Ok basically what I want is a command like include for nginx.\nLet's say I always want the number of workers to be the number of processors. I do not want to add this line into all my config files for every environments because I would have to maintain this code at multiple places. So what I do is something like this:\n```ruby\nconfig/puma/_shared.rb\nworkers(Etc.nprocessors)\n```\n```ruby\nconfig/puma/production.rb or config/puma/staging.rb\nDoes not work\nload(File.expand_path('../_shared.rb', FILE))\nI must use _load_from instead\n_load_from(File.expand_path('../_shared.rb', FILE))\n```\nI do not have experience writing tests, I would probably be able if I tried, but would be painful. Let me know. Thanks!. Well it was just to give to an example (my servers run on physical machines), I have a lot more code that is shared.\nFrom what I see 995ed8bd4585574543977a2fd3f80de55cff2236 broke DSL.load, I'm pretty sure it must have been working before. Now this method is useless inside config files.\nAnyway I just wanted to let you know that it didn't work, but frankly this is a bit trivial and I do have a personal workaround in _load_from (or I could simply use instance_eval), but surely someday someone else will be annoyed by this.\nI can close this pull request now. I suggest that you simply revert (or tweak) 995ed8bd4585574543977a2fd3f80de55cff2236.\nThanks for your time!. ",
    "alexdowad": "Looks like something is dereferencing a null pointer in puma_http11, probably something called from Init_puma_http11() (ext/puma_http11/puma_http11.c:471-500).\nCan't help you debug this because I don't have a Mac. Otherwise I would be happy to diagnose and fix the segfault.\nIf you want to have a go at it, add #include <stdio.h> at the top, put printfs in the code for that function, then rebuild the C extension (run extconf.rb to generate a Makefile, then make) and run your failing test case again. You should be able to see how far it is getting into that function before crashing.. @paulmillr, Are you also running on OS X?. It's a performance optimization.\nLook at git blame on that file. (https://github.com/puma/puma/blame/master/lib/puma/thread_pool.rb) You can see some commits with commit message: \"Add missing localvars. Accessing localvars is faster than accessing ivars.\". This is natural and logical behavior and it cannot be \"fixed\". To understand why, you need to know how environment variables work in Unix/Linux.\nPrograms are executed in Unix by an exec system call, which (as its 3rd parameter) takes an array of environment variables and values. The OS kernel copies these values into the newly initialized memory space of the process which performed the exec. Most programs just pass their own environment variables through to any other programs which they execute. Notably, this is what your shell does.\nSo when you run a program from your shell, the shell forks and execs the other program in the forked child process. After the exec, however, there is no longer any connection between the env vars in the spawned process and those in the parent shell. You can tell your shell to run a program in the background, then try changing env vars in the shell as the program runs. None of the changes in the shell will have any effect on the program which is already running, since it has its own copy of the env vars which were passed into it via exec.\nPuma restarts by execing its own executable file, passing in the same CLI params as it originally received. The env vars which Puma received from the shell when it first started are also passed through the exec, so they do not change when Puma restarts.\nNothing that you write in \".bashrc\" affects this in any way. Or if you set env vars in your shell after Puma is also running, that does not affect Puma in any way either.\nI'm sure there is still a way to do what you want to do, but using env vars is probably not \"it\". They are a useful mechanism, but not what you thought they were. Once you understand what env vars are and how they work, you can then use them when it is appropriate.. When a process (not just Puma, but any process) freezes and can't be stopped without kill -9, what I personally do is:\n\nsudo gdb --pid=<PID of defunct process>\nthread apply all bt\n\n...That will show you what each thread is doing. If you can't make sense of the output, copy the whole thing (each page) and share it here. Usually you will find that some thread is stuck trying to do an I/O operation which never completes, and the name of the C function which it is in will give you a clue what kind of I/O it is (perhaps trying to talk to another process).\nYou need to have GDB installed on your server. Also, your Ruby interpreter must be built with debugging symbols (but I have always found that to be the case).. - What does the entire HTTP response, including all the HTTP headers, look like when you get a \"spurious\" 404?\n- What does your entire middleware stack look like?\n- The part of Rack which returns a 404 if a static file cannot be served looks like this (in lib/rack/file.rb:46):\n```\n      available = begin\n        ::File.file?(path) && ::File.readable?(path)\n      rescue SystemCallError\n        false\n      end\n  if available\n    serving(request, path)\n  else\n    fail(404, \"File not found: #{path_info}\")\n  end\n\n```\nI would really like to know:\n\nWhether Rack is going into the fail branch\nAnd if so, what the return values of File.file? and File.readable? were\nWhether it went into the exception handler for SystemCallError\n\nYou can find out by editing the code in your gem cache and adding some logging print statements. Use gem which rack/file.rb to find where this file is kept on your server. After you successfully debug the problem, uninstall and reinstall the rack gem to wipe out your edits.. @rosenfeld, from your comments it sounds like you have never been able to reproduce this problem in a local instance of the app, only in production. That is a bit of a pain.\nAlthough your deploy is automated, can't you run pumactl -p <PID> restart on the server to restart your application instances? Then they will load the edited code.\nYou mentioned that you were able to reproduce the behavior by sending requests to your app with curl; I guess that does not work immediately after a restart? If so, you could write a little script to keep hitting your application with static file requests and somehow alert you as soon as it gets a 404 response back. Run that on your dev PC all day.\nThis is a tricky bug, but you can and will find and fix it. You just need a way to \"shine a light\" into the internals of your app and find out what is really happening.. Hi, @rosenfeld. Your idea that the problem might be caused by an incorrect working directory makes a lot of sense. Adding an x-pwd header, as you said, could definitely confirm if that is so.\nI'm not one of the Rack devs (just a passer-by trying to help people), but I feel that it would have been a good idea for Rack::Static to raise an error for relative paths. Of course, if they do that now, it will break a lot of people's code.\nResolving it to an absolute path during initialization is a good second choice. Do you fancy submitting a PR?\nAnother thing: could you grep your source tree for Dir.chdir? Maybe you might have a Dir.chdir { ... } block somewhere, and your workers are occasionally serving requests just at the moment when a different thread is running that block?. ",
    "paulmillr": "Same shit here. Latest puma simply segfaults on Ruby 2.4.. @alexdowad it's FreeBSD \u2014 all good on MacOS.. @alexdowad Nevermind. All is good with Ruby 2.6. We should not use non-latest rubies.. ",
    "GeminPatel": "Can someone help me understand that if we have a puma backlog that is waiting for the full request b4 putting them into todo list. And a worker always pop's the request b4 processing them, then how \"There are cases where a two slower clients might get their requests accepted and then there is a temporary backlog, otherwise the backlog metric should be zero.\"? Is it when writing back?. ",
    "mimen": "@schneems Is this ready to be merged?. ",
    "shepmaster": "@mileslane are you using Timecop or ActiveSupport's TimeHelpers in the test reporting the warning?. > don't travel to future, travel to past only\n@dzirtusss the original comment says:\n\nUse Timecop to freeze time pretty far back in time (e.g. 1970)\n\nThe OP is already going to the past. \n. ",
    "dzirtusss": "Alternative quick-fix tip - don't travel to future, travel to past only\nAfter some investigations, in our case these errors are happening only when dates are stubbed to future date. When stubbed to past date there is no errors (guess because delta is negative \ud83d\ude04 ), so..... just refactored everything to past.. ",
    "joethrive": "Upgraded from capybara (2.18.0) to capybara (3.10.1) and started receiving this error.  Rolling back to 2.18.0 solved the problem for me.  My current version of timecop is 0.9.1 and puma is 3.12.0.. ",
    "Hirurg103": "The same issue after upgrading to capybara (3.11.1). ",
    "elizabrock": "I experienced this issue as well while upgrading testing gems.  I believe that this issue appears when upgrading capybara because version 3.0.0+ changed how Puma is used, and additionally something between the 2.x version we were on and the current version of Capybara changed the default server from Webrick to Puma.\nI found that the issue with time traveling wasn't traveling to the future vs. the past but in fact seemed to be based on traveling to different years.  Our suite time traveled to a variety of recent past dates (2014, 2015, etc) and all of those tests had issues post-upgrade.  Merely changing them all to time travel to the current year (2018) fixed the issues.  I did not dig in further, but hopefully these tidbits help!\nP.S. This issue occurs with Time.travel, not just Time.freeze.. ",
    "jimryan": "I ran into this recently and spent some time tracking it down.  I don't know enough about puma or capybara to fix it or say who is responsible, but I can tell you exactly what triggers this:\n\nVisit a page at some time (current or stubbed, it doesn't matter)\nInteract with the page 100M seconds or more in the past, relative to the time the page was visited in step 1\n\nHere are some examples:\n```ruby\nThis will trigger the issue:\nVisit page in current time\nvisit '/foo'\ntravel_to 100_000_000.seconds.ago do\n  # Interact with page, click_on, etc.\nend\n```\n```ruby\nThis will NOT trigger the issue:\nVisit page in current time\nvisit '/foo'\ntravel_to 99_999_999.seconds.ago do\n  # Interact with page, click_on, etc.\nend\n```\n```ruby\nThis will trigger the issue:\nVisit page in the past\ntravel_to 50_000_000.seconds.ago do\n  visit '/foo'\nend\nInteract with the page 100M seconds in the past relative to the time you visited the page\ntravel_to 150_000_000.seconds.ago do\n  # Interact with page, click_on, etc.\nend\n```\n```ruby\nThis will NOT trigger the issue:\nVisit page in the past\ntravel_to 50_000_000.seconds.ago do\n  visit '/foo'\nend\nEven though we are interacting with the page 100M seconds in the past\nrelative to the current time, it is only 50M seconds in the past relative\nto the time we visited the page\ntravel_to 100_000_000.seconds.ago do\n  # Interact with page, click_on, etc.\nend\n```\nThis can be triggered regardless of how you time travel (Timecop, ActiveSupport, etc.).\nHere are the versions of relevant gems I'm using:\n\npuma (3.12.0)\ncapybara (3.9.0)\nrspec (3.8.0). \n",
    "2rba": "I have managed to fix this\nhttps://gist.github.com/2rba/74d57775ac83ffcb0ff1da5eb5371212. ",
    "wyattbenno777": "Have this exact same issue in Ubuntu with Puma 3.11.4 using ssl. \nWorks fine on my mac with a fake ssl certificate. Does not work on production with a real one.\nError in select: closed stream (IOError)\n/usr/share/rvm/gems/ruby-2.3.3@rails-devise/gems/puma-3.11.4/lib/puma/reactor.rb:29:in select\n/usr/share/rvm/gems/ruby-2.3.3@rails-devise/gems/puma-3.11.4/lib/puma/reactor.rb:29:in run_internal'\n/usr/share/rvm/gems/ruby-2.3.3@rails-devise/gems/puma-3.11.4/lib/puma/reactor.rb:154:in block in run_in_thread. I ended up using NGINX on prod for https. Works very fast with Puma.. ",
    "baibinja": "+1 After the test is successfully ended, appears error. Usually only last 4 lines.\nError in reactor loop escaped: closed stream (IOError)\norg/jruby/RubyIO.java:3064:in 'read'\norg/jruby/RubyIO.java:3046:in 'read'\nC:/jruby-9.2.5.0/lib/ruby/gems/shared/gems/puma-3.12.0-java/lib/puma/reactor.rb:143:in 'block in run_internal'\norg/jruby/ext/thread/Mutex.java:159:in 'synchronize'\nC:/jruby-9.2.5.0/lib/ruby/gems/shared/gems/puma-3.12.0-java/lib/puma/reactor.rb:142:in 'block in run_internal'\norg/jruby/RubyArray.java:1792:in 'each'\nC:/jruby-9.2.5.0/lib/ruby/gems/shared/gems/puma-3.12.0-java/lib/puma/reactor.rb:140:in 'run_internal'\nC:/jruby-9.2.5.0/lib/ruby/gems/shared/gems/puma-3.12.0-java/lib/puma/reactor.rb:251:in 'block in run_in_thread'\nError in select: closed stream (IOError)\norg/jruby/RubyIO.java:3549:in 'select'\nC:/jruby-9.2.5.0/lib/ruby/gems/shared/gems/puma-3.12.0-java/lib/puma/reactor.rb:126:in 'run_internal'\nC:/jruby-9.2.5.0/lib/ruby/gems/shared/gems/puma-3.12.0-java/lib/puma/reactor.rb:251:in 'block in run_in_thread'\nJRuby version - 9.2.5.0\nPuma version - 3.12.0\nWindows 10. ",
    "zw963": "As mention in above, if remove prune_bundler, it worked.\nI have another question, is there any cases need prune_bundler must be add for puma phased restart?\nThanks. ",
    "sayuridt": "Hello, \nI'm suffering the same problem, I have two Ruby on Rails applications that communicate with each other by https and it seems that after a while they are left hanging in the minssl.rb and CPU goes to 100%\nRuby Backtrace(s):\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:154:in block in run_in_thread'\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:124:inrun_internal'\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:124:in synchronize'\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:130:inblock in run_internal'\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/client.rb:112:in close'\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:144:inclose'\n    from /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:127:in `read_and_drop'\n/xxxxx/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:56: [BUG] Segmentation fault at 0x0000000000a027\nruby 2.1.2p95 (2014-05-08 revision 45877) [x86_64-linux]\n-- Control frame information -----------------------------------------------\nc:0010 p:---- s:0042 e:000041 CFUNC  :read_nonblock\nc:0009 p:0049 s:0037 e:000036 METHOD /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:56\nc:0008 p:0041 s:0029 e:000028 METHOD /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:127\nc:0007 p:0020 s:0025 e:000024 METHOD /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:144\nc:0006 p:0011 s:0022 e:000021 METHOD /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/client.rb:112\nc:0005 p:0051 s:0019 e:000018 BLOCK  /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:130 [FINISH]\nc:0004 p:---- s:0015 e:000014 CFUNC  :synchronize\nc:0003 p:0079 s:0012 e:000011 METHOD /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:124\nc:0002 p:0009 s:0005 e:000004 BLOCK  /xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:154 [FINISH]\nc:0001 p:---- s:0002 e:000001 TOP    [FINISH]\n-- Ruby level backtrace information ----------------------------------------\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:154:in block in run_in_thread'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:124:inrun_internal'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:124:in synchronize'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/reactor.rb:130:inblock in run_internal'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/client.rb:112:in close'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:144:inclose'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:127:in read_and_drop'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:56:inread_nonblock'\n/xxxxx/vendor/bundle/ruby/2.1.0/gems/puma-3.11.4/lib/puma/minissl.rb:56:in `read_nonblock'\n-- C level backtrace information -------------------------------------------\npuma: cluster worker 1: 16948 11 [0x7f793b7df7fc]\npuma: cluster worker 1: 16948 11 [0x7f793b851103]\npuma: cluster worker 1: 16948 11 [0x7f793b8522e3]\npuma: cluster worker 1: 16948 11 [0x7f793b75551e]\n/lib64/libpthread.so.0(+0xf370) [0x7f793b22a370]\npuma: cluster worker 1: 16948 11 [0x7f793b7adced]\npuma: cluster worker 1: 16948 11 [0x7f793b7ade20]\npuma: cluster worker 1: 16948 11 [0x7f793b7bc9a4]\npuma: cluster worker 1: 16948 11 [0x7f793b7c1c13]\npuma: cluster worker 1: 16948 11 [0x7f793b7b3370]\npuma: cluster worker 1: 16948 11 [0x7f793b7bc9a4]\npuma: cluster worker 1: 16948 11 [0x7f793b7c0193]\npuma: cluster worker 1: 16948 11 [0x7f793b7c283a]\npuma: cluster worker 1: 16948 11 [0x7f793b7d1943]\npuma: cluster worker 1: 16948 11 [0x7f793b67fa85]\n[0x7ffd0e1a3c4f]\nO.S.: Red Hat 7.3\nruby version 2.1.2p95 (2014-05-08 revision 45877)\nrails version 4.1.8\npuma version 3.11.4. Hi @evanphx, \nthe  ssl fix commented by @timkrins seem to me that is in this commit: 4a7a9be7bdc45425d02491bc11646a31d8703f01\nbut is only available in 3.12.0 version, and this version requeries >= 2.2 Ruby Version\nOur app was implemented years ago using ruby version 2.1.2p95 (2014-05-08 revision 45877), is it planned to merge this fix and all fixes involved in the SSL bugs into 3.11.4 branch??\nThanx\n. ",
    "timkrins": "There was a SSL fix in 3.12.0 - maybe give that a try. I would actually expect Puma to operate this way - as the -b flag to the Rails server specifies what address to bind to.\nIt's not really a Puma issue in any case - you want to bring it up with Rails if you think it should be different (but I have a feeling many people would not agree with you). I noticed a SSL fix in the latest release (3.12.0) - maybe give that a try. ",
    "jainankita90": "I am getting same issue\nRuby backtrace:\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/reactor.rb:154:in block in run_in_thread'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/reactor.rb:124:inrun_internal'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/reactor.rb:124:in synchronize'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/reactor.rb:130:inblock in run_internal'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/client.rb:112:in close'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/minissl.rb:144:inclose'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/minissl.rb:126:in read_and_drop'\n    from /usr/local/rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.11.4/lib/puma/minissl.rb:56:inread_nonblock'. @BlackNightFury  Thanks.\nI upgraded to puma 3.12.0, it worked like charm.\nBut sometimes, server response is too slow. \nTo verify if its puma issue, I installed thin server, I am not getting any issue.. ",
    "oniofchaos": "It is, the problem is that the PID doesn't match with the PID of the new process. ",
    "Abbas": "tes saja lah\nhtlm code\nundangan pernikahan\nundangan pernikahan unik\nundangan pernikahan unik\nini cuma pake ling\nhttp://www.undangancinta.com\nbb code \n[url=http://www.undangancinta.com]undangan pernikahan[/url]\n[url=http://www.undangancinta.com]undangan pernikahan simple[/url]\n[url=http://www.undangancinta.com]undangan pernikahan unik[/url]\n\u1bf8\u009b\u0480\u012c. ",
    "njirap": "We have also observed this with almost similar versions but on Ubuntu 16.04. OSX does not have this.\nPuma version: 3.11.4\nRails version: 5.0.3\nRuby version: 2.4.1. ",
    "QuantamHD": "Had this issue as well. The Google Protobuf project also had a similar issue with Ruby 2.2 and was supposedly fixed in 2.3, but the issue could have persisted. In their case, https://github.com/google/protobuf/issues/474, the issue seemed to be related to the ruby method rb_str_cat and by extension in the puma code base rb_str_cat2. These two calls show up only twice in total in the puma code base in puma_http11.c:211-212\n```c\nvoid http_field(puma_parser hp, const char field, size_t flen,\n                                 const char *value, size_t vlen)\n{\n  VALUE f = Qnil;\n  VALUE v;\nVALIDATE_MAX_LENGTH(flen, FIELD_NAME);\n  VALIDATE_MAX_LENGTH(vlen, FIELD_VALUE);\nf = find_common_field_value(field, flen);\nif (f == Qnil) {\n    /\n     * We got a strange header that we don't have a memoized value for.\n     * Fallback to creating a new string to use as a hash key.\n     /\nsize_t new_size = HTTP_PREFIX_LEN + flen;\nassert(new_size < BUFFER_LEN);\n\nmemcpy(hp->buf, HTTP_PREFIX, HTTP_PREFIX_LEN);\nmemcpy(hp->buf + HTTP_PREFIX_LEN, field, flen);\n\nf = rb_str_new(hp->buf, new_size);\n\n}\n/ check for duplicate header /\n  v = rb_hash_aref(hp->request, f);\nif (v == Qnil) {\n      v = rb_str_new(value, vlen);\n      rb_hash_aset(hp->request, f, v);\n  } else {\n      / if duplicate header, normalize to comma-separated values /\n      rb_str_cat2(v, \", \");\n      rb_str_cat(v, value, vlen);\n  }\n}\n```\nThe Protobuf team created an alternate implementation which did not leak memory shown below\nc\n// This function is equivalent to rb_str_cat(), but unlike the real\n// rb_str_cat(), it doesn't leak memory in some versions of Ruby.\n// For more information, see:\n//   https://bugs.ruby-lang.org/issues/11328\nVALUE noleak_rb_str_cat(VALUE rb_str, const char *str, long len) {\n  size_t oldlen = RSTRING_LEN(rb_str);\n  rb_str_modify_expand(rb_str, len);\n  char *p = RSTRING_PTR(rb_str);\n  memcpy(p + oldlen, str, len);\n  rb_str_set_len(rb_str, oldlen + len);\n} \nPuma version: 3.12.0\nRails: 5.1.6\nRuby: 2.3.7. ",
    "shirkavand": "@QuantamHD thanks for the reply, can you give me more guidance about what are the next steps to follow based on your answer?  Do i have to patch the code with Protobuf's version somehow?. @1c7 no, at the end we switched - sadly - to passenger. We found no way to avoid this memory leak. I know some new versions of puma have been released since then, but we have not tested those yet.\nDo you have any insight of wether these new versions solve the problem or not?. ",
    "Djurredejong": "We're running into same problem. This branch seems to fix it for us. Is this PR being considerd for master, @schneems ?. ",
    "harmdewit": "Curl will automatically send chunked requests by setting the header (link): \nFor example \ncurl -H \"Transfer-Encoding: chunked\" -d \"payload\" localhost --silent --trace-ascii -\nOutputs:\n== Info: Rebuilt URL to: localhost/\n== Info:   Trying ::1...\n== Info: TCP_NODELAY set\n== Info: Connection failed\n== Info: connect to ::1 port 80 failed: Connection refused\n== Info:   Trying 127.0.0.1...\n== Info: TCP_NODELAY set\n== Info: Connected to localhost (127.0.0.1) port 80 (#0)\n=> Send header, 154 bytes (0x9a)\n0000: POST / HTTP/1.1\n0011: Host: localhost\n0022: User-Agent: curl/7.54.0\n003b: Accept: */*\n0048: Transfer-Encoding: chunked\n0064: Content-Type: application/x-www-form-urlencoded\n0095: \n0097: 7\n=> Send data, 14 bytes (0xe)\n0000: payload\n0009: 0\n000c: \n== Info: upload completely sent off: 14 out of 7 bytes\n<= Recv header, 17 bytes (0x11)\n0000: HTTP/1.1 200 OK\n...\nDebugging this request in a receiving puma server in Puma::Client#setup_body: \nputs body.inspect => \"7\\r\\npayload\\r\\n0\\r\\n\\r\\n\"\nUnfortunately I couldn't find any info on how to set the chunk size in order to send multiple chunks. Requests larger than 16380 bytes seem to automatically get chunked, however debugging these large requests in Puma shows wrong results. I suspect these large requests are written to a Tempfile to limit memory usage or something.\n. @schneems Found a way to do it with Net::HTTP:\n```ruby\nrequire 'uri'\nrequire 'net/http'\nrequire 'tempfile'\nclass Chunked\n  def initialize(data, chunk_size)\n    @size = chunk_size\n    if data.respond_to? :read\n      @file = data\n    end\n  end\ndef read(_, offset)\n    if @file\n      @file.read(@size, offset)\n    end\n  end\n  def eof!\n    @file.eof!\n  end\n  def eof?\n    @file.eof?\n  end\nend\nuri = URI::parse('http://example.test')\nconnection = Net::HTTP.new(uri.host, uri.port)\nconnection.set_debug_output($stdout)\nfile = Tempfile.new\nfile.write(\"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\ntempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam,\nquis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo\nconsequat. Duis aute irure dolor in reprehenderit in voluptate velit esse\ncillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non\nproident, sunt in culpa qui officia deserunt mollit anim id est laborum.\")\nfile.rewind\nchunked = Chunked.new(file, 100)\nrequest = Net::HTTP::Post.new uri.request_uri, {'Transfer-Encoding' => 'chunked', 'content-type' => 'text/plain'}\nrequest.body_stream = chunked\nconnection.start do |http|\n  http.request(request)\nend\nfile.close\nfile.unlink\n```\nOutputs (response isn't relevant):\nopening connection to example.test:80...\nopened\n<- \"POST / HTTP/1.1\\r\\nTransfer-Encoding: chunked\\r\\nContent-Type: text/plain\\r\\nAccept-Encoding: gzip;q=1.0,deflate;q=0.6,identity;q=0.3\\r\\nAccept: */*\\r\\nUser-Agent: Ruby\\r\\nHost: example.test\\r\\n\\r\\n\"\n<- \"64\\r\\n\"\n<- \"Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod\\ntempor incididunt ut labore\"\n<- \"\\r\\n\"\n<- \"64\\r\\n\"\n<- \" et dolore magna aliqua. Ut enim ad minim veniam,\\nquis nostrud exercitation ullamco laboris nisi ut \"\n<- \"\\r\\n\"\n<- \"64\\r\\n\"\n<- \"aliquip ex ea commodo\\nconsequat. Duis aute irure dolor in reprehenderit in voluptate velit esse\\ncill\"\n<- \"\\r\\n\"\n<- \"64\\r\\n\"\n<- \"um dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non\\nproident, sunt in culpa qu\"\n<- \"\\r\\n\"\n<- \"2e\\r\\n\"\n<- \"i officia deserunt mollit anim id est laborum.\"\n<- \"\\r\\n\"\n<- \"0\\r\\n\\r\\n\"\n-> \"HTTP/1.1 503 Service Unavailable\\r\\n\"\n-> \"Content-Type: text/html; charset=utf8\\r\\n\"\n-> \"X-Pow-Template: application_not_found\\r\\n\"\n-> \"Date: Mon, 30 Jul 2018 11:56:22 GMT\\r\\n\"\n-> \"Connection: keep-alive\\r\\n\"\n-> \"Transfer-Encoding: chunked\\r\\n\"\n-> \"\\r\\n\"\n-> \"a99\\r\\n\"\nreading 2713 bytes...\n-> \"<!doctype html>\\n<html>\\n<head>\\n  <meta charset=\\\"utf-8\\\">\\n  <title>Application not found</title>\\n  <style>\\n    body {\\n      margin: 0;\\n      padding: 0;\\n      background: #e0e0d8;\\n      line-height: 18px;\\n    }\\n    div.page {\\n      margin: 72px auto;\\n      margin: 36px auto;\\n      background: #fff;\\n      border-radius: 18px;\\n      -webkit-box-shadow: 0px 2px 7px #999;\\n      -moz-box-shadow: 0px 2px 7px #999;\\n      padding: 36px 90px;\\n      width: 480px;\\n      position: relative;\\n    }\\n    .big div.page {\\n      width: 720px;\\n    }\\n    h1, h2, p, li {\\n      font-family: Helvetica, sans-serif;\\n      font-size: 13px;\\n    }\\n    h1 {\\n      line-height: 45px;\\n      font-size: 36px;\\n      margin: 0;\\n    }\\n    h1:before {\\n      font-size: 66px;\\n      line-height: 42px;\\n      position: absolute;\\n      right: 576px;\\n    }\\n    .big h1:before {\\n      right: 819px;\\n    }\\n    h1.ok {\\n      color: #060;\\n    }\\n    h1.ok:before {\\n      content: \\\"\\xE2\\x9C\\x93\\\";\\n      color: #090;\\n    }\\n    h1.err {\\n      color: #600;\\n    }\\n    h1.err:before {\\n      content: \\\"\\xE2\\x9C\\x97\\\";\\n      color: #900;\\n    }\\n    h2 {\\n      line-height: 27px;\\n      font-size: 18px;\\n      font-weight: normal;\\n      margin: 0;\\n    }\\n    a, pre span {\\n      color: #776;\\n    }\\n    h2, p, pre {\\n      color: #222;\\n    }\\n    pre {\\n      white-space: pre-wrap;\\n      font-size: 13px;\\n    }\\n    pre, code {\\n      font-family: Menlo, Monaco, monospace;\\n    }\\n    p code {\\n      font-size: 12px;\\n    }\\n    pre.breakout {\\n      border-top: 1px solid #ddd;\\n      border-bottom: 1px solid #ddd;\\n      background: #fafcf4;\\n      margin-left: -90px;\\n      margin-right: -90px;\\n      padding: 8px 0 8px 90px;\\n    }\\n    pre.small_text {\\n      font-size: 10px;\\n    }\\n    pre.small_text strong {\\n      font-size: 13px;\\n    }\\n    ul {\\n      padding: 0;\\n    }\\n    li {\\n      list-style-type: none;\\n    }\\n  </style>\\n</head>\\n<body class=\\\"\\\">\\n  <div class=\\\"page\\\">\\n    \\n  <h1 class=\\\"err\\\">Application not found</h1>\\n  <h2>Symlink your app to <code>~/.pow/example</code> first.</h2>\\n  <section>\\n    <p>When you access <code>http://example.test/</code>, Pow looks for a Rack application at <code>~/.pow/example</code>. To run your app at this domain:</p>\\n    <pre><span>$</span> cd ~/.pow\\n<span>$</span> ln -s /path/to/myapp example\\n<span>$</span> open http://example.test/</pre>\\n  </section>\\n\\n    <ul>\\n      <li><a href=\\\"http://pow.cx/manual\\\">Pow User&rsquo;s Manual</a></li>\\n      <li><a href=\\\"https://github.com/basecamp/pow/wiki/Troubleshooting\\\">Troubleshooting</a></li>\\n      <li><a href=\\\"https://github.com/basecamp/pow/wiki/FAQ\\\">Frequently Asked Questions</a></li>\\n      <li><a href=\\\"https://github.com/basecamp/pow/issues\\\">Issue Tracker</a></li>\\n    </ul>\\n  </div>\\n</body>\\n</html>\\n\\n\"\nread 2713 bytes\nreading 2 bytes...\n-> \"\\r\\n\"\nread 2 bytes\n-> \"0\\r\\n\"\n-> \"\\r\\n\"\nConn keep-alive\n=> #<Net::HTTPServiceUnavailable 503 Service Unavailable readbody=true>. Hi @schneems can you say when the next version will be released?. Any idea when this get released? \ud83d\ude42. ",
    "Cory-Christensen": "This will also fix #1492 \nI was about to create a PR with the same fix after doing some digging today!. ",
    "sebastianas": "I can't tell if this is the same. I can rerun the test and give you parts of the build folder or run it with the arguments you tell me to use.\nThe net/http part is probably the TLS1.3 thing. Besides the autorety that should be part of pre8 there is also https://github.com/openssl/openssl/pull/6340 which might help. I didn't have time to retry this yet, and I will be offline for the next month, just that you know.. ",
    "jiikko": "@olleolleolle \nok.\nI merged description from https://github.com/puma/puma/pull/1618.. ",
    "y-yagi": "Duplicated with #1612.. ",
    "egee-irl": "I had the same issue and I fixed it by adding my hostname to my hosts file in /etc/hosts. Not exactly sure why that fixed it or why Puma was having an issue with it.\nBasically added 127.0.0.1 workstation to my hosts file, where workstation is your host name.. ",
    "sharathchandrahosur": "\nI had the same issue and I fixed it by adding my hostname to my hosts file in /etc/hosts. Not exactly sure why that fixed it or why Puma was having an issue with it.\nBasically added 127.0.0.1 workstation to my hosts file, where workstation is your host name.\n\nIt worked.\nThank you. You made my day.. ",
    "mhulse": "Closed as this looks like an issue with rvm/bundler (seeing same issue occurring with other gems, not just puma). \nIf anyone else wants to follow along, I asked for help over here.. ",
    "monteirobrena": "I've been the same error, the application stop and I need to restart Puma always.. ",
    "eregon": "The CI fails, but it seems unrelated to this change and it already fails on master.. Could someone review this PR?\ncc @evanphx @schneems @luislavena @nateberkopec . ",
    "bclennox": "I should note that _load_from appears to work the same as import if you pass it an absolute path:\n```ruby\nconfig/puma/development.rb\n_load_from File.join(dir, 'default.rb')\nputs 'Should not get here'\nconfig/puma/default.rb\nraise 'Raising from default.rb'\n$ bundle exec puma -e development\nRuntimeError: Raising from default.rb\n  config/puma/default.rb:1:in `_load_from'\n  # ...\n```. ",
    "oLeVanNghia": "ExecStart=/home/ubuntu/.rvm/bin/rvm default do bundle exec puma -C /home/ubuntu/app/shared/puma.rb --daemon\nhow about if remove option \"--daemon\" in above command?. ",
    "harmjanblok": "Adding the metrics for the single mode was the easy part, adding the metrics for the clustered mode turned out to be more difficult. For example the logic to render multiple labels for the same metric. Searching for solutions I discovered Puma supports for Puma::Plugins with custom logic. I think it makes more sense to put all this logic into a dedicated gem as a Puma::Plugin.\nThat would keep the puma gem itself free from this logic, but still allows users to enhance puma with prometheus metrics.\nThe initial approach for this plugin is on https://github.com/harmjanblok/puma-metrics/pull/1.\nPlease let me know if it would make sense to include this logic into the puma gem instead.. ",
    "akoserwal": "This issue can be closed. . ",
    "dmfallak": "I tested my app with a patch from your f-even-accept branch, and it seems to improve request time, specifically I notice that the time spent in Redis and DB is reduced.. ",
    "cpinto": "@mipearson I know it's been a while but are you able to share what the misconfiguration was? I'm trying to understand what's the root cause for this error on our production server. Thanks.. ",
    "mipearson": "@cpinto I don't, sorry. I remember it was something specific to our code. I'd recommend sourcing this branch into your Gemfile if only to see where the source error lies, and then reverting to the gem version once fixed.. ",
    "gingerlime": "That's unfortunate. You can't easily scale or tweak settings without downtime. Any particular reason why?. ",
    "shaqq": "@evanphx I ended up starting a thread in on_worker_boot:\nhttps://github.com/shaqq/puma-runtime_env/blob/50d452f8af496137818e6a23163ed8eded66a4b2/lib/puma/plugin/runtime_env.rb#L37\nI was mainly curious if something like this should be handled as a first class API.... but if people aren't asking for it, then it's probably not worth it. Just figured I'd ask!\nThanks!. ",
    "stefansedich": "Having a play around with this, I experimented by adding a new max_worker_terminations configuration parameter which can control the number of worker terminations we accept before we want to nuke the entire process.\nAny feedback or thoughts would be appreciated and I can wrap it up into a PR if everyone agrees this would be valuable.. @evanphx that was the idea, the issue in our case is that if running in clustered mode and something happens to cause workers to restart they will reboot in a super tight loop causing logs to balloon and it gives me no real way to know that bad things are happening.\nIf the process exists it at least causes our containers to be restarted on a slower loop which reduces the issue and also gives me a way to see that the containers themselves are constantly rebooting which is something that I can pick up on easier.. @evanphx I am going to close this for now as my focus has shifted and I will get back to it when I can again.. It did cause the output to look odd which was the reason for adding, will remove after the main discussion after the PR in general.. ",
    "davidben": "Looks like the issue is this line here:\nhttps://github.com/puma/puma/blob/72882f2319e65b371e1458069723279b3196a220/ext/puma_http11/mini_ssl.c#L193\nP-521 is not a very common curve. It's not supported by Chrome or Edge. Prior to TLS 1.3, this was non-fatal but resulted in less secure settings. Starting TLS 1.3, enabled by default in OpenSSL 1.1.1, ECDH is mandatory. The immediate fix would be to use NID_X9_62_prime256v1 (P-256) instead, which is where most hardening work is focused.\nBut OpenSSL also has a fine set of defaults in 1.1.0, and an API to negotiate multiple curves. 1.0.2's defaults are a little large, but also fine. Thus, something like this may be better:\n```\nif OPENSSL_VERSION_NUMBER < 0x10002000L\n// Remove this case if OpenSSL 1.0.1 (now EOL) support is no\n  // longer needed.\n  EC_KEY *ecdh = EC_KEY_new_by_curve_name(NID_X9_62_prime256v1);\n  if (ecdh) {\n    SSL_CTX_set_tmp_ecdh(ctx, ecdh);\n    EC_KEY_free(ecdh);\n  }\nelif OPENSSL_VERSION_NUMBER < 0x10100000L || defined(LIBRESSL_VERSION_NUMBER)\n// Prior to OpenSSL 1.1.0, servers must manually enable server-side ECDH\n  // negotiation.\n  SSL_CTX_set_ecdh_auto(ctx, 1);\nendif\n```. I suppose whatever that is running is using something else for TLS than the code snippet I found. Likely Heroku has a TLS terminator of its own.\n(This would not be a property of the certificate. It's the TLS server configuration.). (You would not get that output from Chrome if the linked code were running. Those calls tell OpenSSL to only support P-521, and you're getting P-256 out.). No, what you want to do is make the code change I mentioned above.. ",
    "eric-norcross": "Given that this issue is related to the use of the P-521 curve, I'm curious how is this still working with Puma in Production on Heroku without issue? Does Heroku Automated Certificate Management somehow circumvent this?\nUsing puma 3.12.0 in Production on heroku-16 stack, I'm seeing the following from the Chrome security tab:\n\n. Thanks, I put in a support request with Heroku and point them to this thread. I'm hoping they can elaborate on how the Production server is configured so I can mirror that in Dev. \nI Really appreciate your help in figuring this out!. @MSP-Greg I just implemented it in my fork and all tests passed for me as well. That being said, I'm a bit at a loss on how to write a test for this change. Any suggestions? . @biznickman I also ended up having to recreate my self-signed certificate, making sure to add Subject Alternate Name (SAN) support: https://ksearch.wordpress.com/2017/08/22/generate-and-import-a-self-signed-ssl-certificate-on-mac-osx-sierra/. Sorry, I forgot to reference the issue this resolves:\nhttps://github.com/puma/puma/issues/1670. ",
    "dachinat": "Hi, is there a temporary fix until this PR will be merged? Thanks. @biznickman @eric-norcross thanks!. ",
    "biznickman": "@dachinat if you are developing locally for now, I've used this line in my Gemfile and it fixed it until they merge:\ngem 'puma', git: 'https://github.com/eric-norcross/puma.git', branch: 'chrome_70_ssl_curve_compatiblity'. ",
    "victorbucutea": "@biznickman @eric-norcross  I tried the above solution and chrome works, safari on the other hand doesn't ... . ",
    "avdept": "Getting same issue while trying to access to local https via ngrok. ",
    "martingalovic": "Solved it: Brew uninstall & installed puma,\nThen double-clicked on ~/Library/Application Support/io.puma.dev/cert.pem and selected Always Trust. ",
    "roccoblues": "Any updates on this? This is a pretty serious issue that's not only happening on MacOS. I have the same problem on Linux with Chrome 72. \nThere's a fix available for merge (https://github.com/puma/puma/pull/1671) that has been written by some pretty smart people here: https://bugs.chromium.org/p/chromium/issues/detail?id=899994#c8  \n. ",
    "JackWells": "I encountered the same problem with Firefox in the \"60s\" versions (different machines, different versions, different OSes, same problem).  Older versions of FF had no problems, but the newer versions would sometimes generate an error, sometimes time-out (no error in console), and sometimes work.  \nThis version of the puma-gem solved the Firefox problem.  Big Thanks to whoever did the work on it - using:\ngem 'puma', git: 'https://github.com/eric-norcross/puma.git', branch: 'chrome_70_ssl_curve_compatiblity'. ",
    "mdkent": "Seeing this on git version 1.9.1 (linux) and git version 2.17.2 (Apple Git-113) (osx). \nSeems worse here, git is actually modifying the files and preventing me from working in the repo.. Maybe a safer default is 10 seconds here? 2x the Cluster::WORKER_CHECK_INTERVAL.. ",
    "atitan": "Using dumb-init as container entrypoint solves this issue.\nAlthough GKE is still not happy about the return code.\ud83d\ude22. @guilleiguaran I've tested against master@27a09c4, but it behaves like I wrote in the original issue.\nEnvironment:\n - OS version: macOS 10.14.3\n - Ruby version: 2.6.2\n - Rails version 5.2.2.1\n - Bundler version 2.0.1\nBehavior:\n```\n$ bundle exec puma -C config/puma.rb\n[95865] Puma starting in cluster mode...\n[95865] * Version 3.12.1 (ruby 2.6.2-p47), codename: Llamas in Pajamas\n[95865] * Min threads: 5, max threads: 5\n[95865] * Environment: development\n[95865] * Process workers: 2\n[95865] * Preloading application\n[95865] * Listening on tcp://0.0.0.0:3000\n[95865] Use Ctrl-C to stop\n[95865] - Worker 0 (pid: 95907) booted, phase: 0\n[95865] - Worker 1 (pid: 95908) booted, phase: 0\nSend SIGINT here\n[95865] - Gracefully shutting down workers...\n[95865]     worker status: pid 95907 exit 0\n[95865]     worker status: pid 95908 exit 0\n[95865]     worker shutdown time:   0.50\nPuma stuck here with worker already shutdown\nHave to press ctrl-c to shut it down, sending signal again does not help\n^C[95865] === puma shutdown: 2019-03-20 10:32:19 +0800 ===\n[95865] - Goodbye!\n```. I tried packing my application in Docker. It seems the patch works on Linux platform(previously not).\n``\n$ docker run --env-file=.env ee1cc795c928 bundle exec puma -C config/puma.rb\n[6] Puma starting in cluster mode...\n[6] * Version 3.12.1 (ruby 2.6.2-p47), codename: Llamas in Pajamas\n[6] * Min threads: 5, max threads: 5\n[6] * Environment: production\n[6] * Process workers: 2\n[6] * Preloading application\nI, [2019-03-20T02:55:34.471560 #6]  INFO -- : Raven 2.9.0 ready to catch errors\n[6] * Listening on tcp://0.0.0.0:3000\n[6] ! WARNING: Detected 1 Thread(s) started in app boot:\n[6] ! #<Thread:0x00007f536ca5ec38@/usr/local/bundle/gems/activerecord-5.2.2.1/lib/active_record/connection_adapters/abstract/connection_pool.rb:299 sleep> - /usr/local/bundle/gems/activerecord-5.2.2.1/lib/active_record/connection_adapters/abstract/connection_pool.rb:301:insleep'\n[6] Use Ctrl-C to stop\n[6] - Worker 0 (pid: 17) booted, phase: 0\n[6] - Worker 1 (pid: 30) booted, phase: 0\nReceived signal here\n[6] - Gracefully shutting down workers...\n[6]     worker status: pid 17 exit 0\n[6]     worker status: pid 30 exit 0\n[6]     worker shutdown time:   0.50\n[6] === puma shutdown: 2019-03-20 02:56:55 +0000 ===\n[6] - Goodbye!\n```\n```\nin another terminal\n$ docker ps\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES\n3d6d3b768a0b        ee1cc795c928        \"/usr/bin/dumb-init \u2026\"   25 seconds ago      Up 24 seconds                           priceless_poincare\n$ docker kill -s SIGINT 3d6d3b768a0b\n3d6d3b768a0b\n```\nRunning puma as PID1 also works. (docker run without init process as entrypoint). @evanphx \nI just created a new rails app to test with. Found out adding preload_app! in puma config hanged the cluster master.\nBesides, if I don't preload the app, ActiveSupport constant will not be defined in on_worker_boot block.\nMy config looks like this\n```\nthreads_count = ENV.fetch(\"RAILS_MAX_THREADS\") { 5 }\nthreads threads_count, threads_count\nport        ENV.fetch(\"PORT\") { 3000 }\nenvironment ENV.fetch(\"RAILS_ENV\") { \"development\" }\nworkers ENV.fetch(\"WEB_CONCURRENCY\") { 2 }\npreload_app!\nbefore_fork do\n  if defined?(ActiveRecord::Base)\n    ActiveRecord::Base.connection_pool.disconnect!\n  end\nend\non_worker_boot do\n  ActiveSupport.on_load(:active_record) do\n    ActiveRecord::Base.establish_connection\n  end\nend\nplugin :tmp_restart\npersistent_timeout 620\n```. @evanphx \nHere you are.\nI noticed a strange situation that puma does not hang if I started it in production mode.\n```\nsource 'https://rubygems.org'\ngit_source(:github) { |repo| \"https://github.com/#{repo}.git\" }\nruby '2.6.2'\ngem 'rails', '~> 5.2.2', '>= 5.2.2.1'\ngem 'sqlite3', '~> 1.3.6'\ngem 'puma', '~> 3.11', github: 'puma/puma', branch: 'master'\ngem 'bootsnap', '>= 1.1.0', require: false\ngroup :development do\n  gem 'listen', '>= 3.0.5', '< 3.2'\nend\n``. @evanphx \nThe key point here isdon't use ctrl-c.\nUsekill` to send SIGINT to the process.\nThe Ctrl-C will always work even after it hanged.. To sum up, reproducing the issue requires:\n - macOS\n - puma in development and cluster mode\n - preload_app!\n - use kill to send signal instead of ctrl-c. ",
    "guilleiguaran": "I think this is fixed by https://github.com/puma/puma/pull/1738 also. @mltsy have you tested 2.5.5? afaik Ruby 2.5.5 was released to handle the regression in 2.5.4. @atitan can you test with master and confirm if this can be closed?. @atitan you're right, the fix seems to work on Linux but fails on macOS, I just confirmed it in a machine with macOS.\n/cc @MSP-Greg . I think this is fixed with https://github.com/puma/puma/pull/1738. Yes! I think this can be closed, @voxik can you test with master and confirm?. Also related: https://github.com/puma/puma/issues/1703\nI'm going to disable allow_failures for ruby 2.6 and trunk builds since this makes the build green in these versions.. @MSP-Greg ok!! I'm keeping ruby-head in allow_failures then. @MSP-Greg \nBased in other patterns in Cluster class, what do you think about handling it in that way:\n```ruby\nwhile @workers.any?\n  pid = Process.waitpid(-1, Process::WNOHANG)\n  unless pid\n    sleep 1\n    next\n  end\n@workers.delete_if { |w| w.pid == pid }\nend\n``. @MSP-Greg thanks! I'm curious: there are any reason to check all individuals pids withwaitpid(pid)instead of using-1` (that means any child process)?\nruby\npids = @workers.map &:pid\nuntil pids.empty?\n  if p = Process.waitpid(-1, Process::WNOHANG)\n    pids.delete p\n    sleep 0.5\n    break\n  end\nend. @evanphx thanks for checking, wdyt about the other solutions proposed in the comments: https://github.com/puma/puma/pull/1738#issuecomment-471690993 https://github.com/puma/puma/pull/1738#issuecomment-471637949, still wrong?. Closing in favor of #1741. Thanks for this!\nI prefer this version over #1738 since I appreciate the extra logs but I'll keep #1738 open just in case if the Puma authors don't want the extra logging.. @evanphx indeed, it seems to be caused by https://github.com/ruby/ruby/commit/48b6bd74e2febde095ac85d818e94c0e58677647 but doesn't look like it will be fixed anytime soon since the Ruby team isn't sure about how to fix it \ud83d\ude1e \nEdit: The related issue is https://bugs.ruby-lang.org/issues/15499, but it was closed as resolved even when someone pointed that the patch didn't solve the problem with Puma.. Thanks! Fixed.. ",
    "AlexWayfer": "It seems that this issue is also in Ruby 2.5.4 now.. #1738 is closed in favor of #1741, which is already merged. Is this issue should be closed?. p is reserved (Kernel#p), it's even highlights so, and it's smelling. What about something like process?. In fact, there are two cycles.\nIt can be done in one with redo. And without the second array.. There are two cycles instead of one.\nYou can use redo.\nSee https://github.com/puma/puma/pull/1738#discussion_r264748021. > As mentioned, I did it this way so it would loop thru all the workers on first pass, then go back for the nil returns.\nIt's strange algorithm.\nOption 1, with redo:\n\nGo through all workers\nSleep and redo on the first fail (wait every first fail)\n\nOption 2:\n\nGo through all workers and attempt to stop them\nDelete from array in success, wait after all if any failed\nRetry if there are any remained\n\nOption 3, your choice, as I see:\n\nGo through all workers\nDelete in success, and retry (go further)\nDon't do anything in fail, just sleep in the end\nRetry until there are any\n\nI think, the first option is simplest (but may be longest), the second is most optimal, and the third is strange.. > I changed the logic to use reject!\nYou can use loop do instead of while true do. \ud83d\ude09 . Also, about two cycles (and map): do we need for @workers after successful run of this code?\nIf no \u2014 we can use @workers.reject!.. ",
    "mltsy": "Not only that - puma doesn't work in clustered mode at all with this ruby update!  I'll put in another ticket for that though.. Ah!  Indeed.  I just saw, as I was going to put in a ticket, that there was actually ticket submitted and resolved for that very reason that I had missed previously because it was already closed :)  Thanks!  And it looks like that update also resolves the issue in this ticket - although perhaps it still exists in the Ruby 2.6 branch?. That does seem to allow puma to shutdown gracefully in ruby 2.5.4 with that branch. :+1:\n(However, requests still don't get through in clustered mode in ruby 2.5.4 even with that branch). 2.5.5 does fix both issues, yeah!. Yeah, ruby 2.5.5 fixes it even with standard 3.12.0 puma branch \ud83d\ude04 \n(And I'm not sure about ruby 2.6 - I can't seem to reproduce the problem with 2.6, and am not sure which version it exists in exactly). Interesting... well when I ran it with Ruby 2.6.2, everything worked fine for me (with the standard 3.12.0 puma branch - did they fix it in 2.6.2?). Huh... I setup a docker-compose stack with ruby 2.6.2-p47 and puma 3.12.0 in clustered mode (-w 2 flag) and it exits fine in every way I've tested it... that's odd.  I must not be hitting exactly the right test case?. ",
    "Adithya-copart": "Issue not with Puma. Opened a issue in suckerpunch.. I did some debugging and found out that at_exit hooks and the SIGINT trap introduced in https://github.com/puma/puma/commit/8903eea50a1e113b69c5f5395ed0cbd6add7b577 are running in different threads leading to a race condition which is not allowing the at_exit hook in sucker_punch.\nThis can be observed by adding a puts \"#{Thread.current}\" here and comparing it with the thread printed in the at_exit hook in test.rb.\n```\nConfig.ru\nrequire 'sinatra'\nrequire './test.rb'\nrun Test\nGemfile\nsource 'https://rubygems.org'\nruby '2.5.0', engine: 'jruby', engine_version: '9.2.0.0'\ngem 'sinatra','2.0.4'\ngem 'puma','3.12.0'\ntest.rb\nclass Test < Sinatra::Base\n  at_exit { puts \"Thread in test.rb at_exit: #{Thread.current}\"}\nend\n```. Should be fixed by https://github.com/jruby/jruby/pull/5441. ",
    "nbenz": "Closing this issue as my PR for the feature was merged. Thanks!. ",
    "matixmatix": "Nevermind, I have a problem with the deploy script (mina), which deployed empty config.ru file to the server.. ",
    "cmbankester": "Nevermind, I just saw this was proposed already in  #1667 . ",
    "michaelherold": "It looks like I'm fixing the same thing here as in #1667. Please feel free to take that one if you wish.. Done!. ",
    "mic-kul": "Thanks for the reminder. Committed my recent changes and it's ready for another look @evanphx . Cannot reproduce on Rails 5.2.2:\n```\n\u276f curl -X WTF http://localhost:3003\n* Connected to localhost (127.0.0.1) port 3003 (#0)\n\nWTF / HTTP/1.1\nHost: localhost:3003\nUser-Agent: curl/7.54.0\nAccept: /\n< HTTP/1.1 500 Internal Server Error\n< Content-Length: 77\n<\nAn unhandled lowlevel error occurred. The application logs may have details.\n* Connection #0 to host localhost left intact\n```\n\nStack is printed to STDOUT in Rails process, not to the client:\n``\n\u276f RAILS_ENV=production rails s -p 3003\n=> Booting Puma\n=> Rails 5.2.2 application starting in production\n=> Runrails server -h` for more startup options\nWARNING: Nokogiri was built against LibXML version 2.9.8, but has dynamically loaded 2.9.4\nPuma starting in single mode...\n Version 3.12.0 (ruby 2.5.1-p57), codename: Llamas in Pajamas\n Min threads: 5, max threads: 5\n Environment: production\n Listening on tcp://0.0.0.0:3003\nUse Ctrl-C to stop\n2019-02-23 14:35:56 +0100: Rack app error handling request { WTF / }\n\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/http/request.rb:426:in check_method'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/http/request.rb:135:inrequest_method'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:51:in started_request_message'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:37:inblock in call_app'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/2.5.0/logger.rb:465:in add'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/logger.rb:89:inadd'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/2.5.0/logger.rb:527:in info'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:37:incall_app'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:26:in block in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/tagged_logging.rb:71:inblock in tagged'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/tagged_logging.rb:28:in tagged'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/tagged_logging.rb:71:intagged'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:26:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/middleware/remote_ip.rb:81:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/middleware/request_id.rb:27:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/rack-2.0.6/lib/rack/runtime.rb:22:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/cache/strategy/local_cache_middleware.rb:29:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/middleware/executor.rb:14:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/rack-2.0.6/lib/rack/sendfile.rb:111:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/engine.rb:524:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/configuration.rb:225:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/server.rb:665:inhandle_request'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/server.rb:479:in process_client'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/server.rb:338:inblock in run'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/thread_pool.rb:133:in `block in spawn_thread'\n2019-02-23 14:36:04 +0100: Rack app error handling request { WTF / }\n\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/http/request.rb:426:in check_method'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/http/request.rb:135:inrequest_method'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:51:in started_request_message'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:37:inblock in call_app'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/2.5.0/logger.rb:465:in add'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/logger.rb:89:inadd'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/2.5.0/logger.rb:527:in info'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:37:incall_app'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:26:in block in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/tagged_logging.rb:71:inblock in tagged'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/tagged_logging.rb:28:in tagged'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/tagged_logging.rb:71:intagged'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/rack/logger.rb:26:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/middleware/remote_ip.rb:81:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/middleware/request_id.rb:27:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/rack-2.0.6/lib/rack/runtime.rb:22:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/activesupport-5.2.2/lib/active_support/cache/strategy/local_cache_middleware.rb:29:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/actionpack-5.2.2/lib/action_dispatch/middleware/executor.rb:14:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/rack-2.0.6/lib/rack/sendfile.rb:111:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/railties-5.2.2/lib/rails/engine.rb:524:incall'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/configuration.rb:225:in call'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/server.rb:665:inhandle_request'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/server.rb:479:in process_client'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/server.rb:338:inblock in run'\n/Users/mic-kul/.rbenv/versions/2.5.1/lib/ruby/gems/2.5.0/gems/puma-3.12.0/lib/puma/thread_pool.rb:133:in `block in spawn_thread'\n```. Found https://github.com/puma/puma/pull/1715. ",
    "enebo": "@kares I did approve the review but I can explicitly state it here as well.  This looks good to be merged.. @kares no worries.  I agree my comment would be confusing to read even with the approval.  I should have probably mentioned it after the merge.. @kares @olleolleolle I know this was motivated by Java 9+ warnings but do you anticipate any perf gains now that this landed?. Not sure this is actionable but this won't reduce/remove backing array of bytelist.  I would not worry about it but if someone loads a massive IOBuffer and it is reused it will just drag around a massive backing array as long as it lives.. @kares yeah I agree and did notice it was like this before as well.. ",
    "adurgin": "@evanphx @kares @enebo  Hi. Would it be possible to release a gem with this fix included?. ",
    "jasl": "Thank you, please merge the better one you like!. ",
    "mortik": "Still got that issue with rubygems 3.0.3 and bundler 2.0.1, ruby 2.6.1  . tried to remove prune_bundler from my puma config and now it seems to work... am i supposed to remove prune_bundler when using bundler 2?. Already did that, no change sadly. No this issue only occurs with bundle exec puma\nOk i will try that, thanks. Another weird thing is that this issue only occurs on CircleCI and not locally but only when i upgrade to bundler 2 :/. Maybe you are right and this is really an issue with bundler 2 and/or the Setup on CircleCI . ",
    "jbescoyez": "My bad. This issue is only happening in development environment. \nFixed it by running puma in single mode in dev.. ",
    "venarius": "Have the same problem! Puma is taking longer and longer with every cap deploy. ",
    "colorbox": "Oh , I overlooked those PR. \ud83d\ude48 \nI close this.\nThank you for comment \ud83d\ude03 . ",
    "themilkman": "We noticed similar issues on a few of our servers (rails >=5.2, ruby >= 2.6.0).. We don't use ActionCable in our apps.\nBut also a trivial systemd+socket setup.. ",
    "calmyournerves": "Not sure if related, but we have seen our restart times go up significantly (from a few seconds to >30s) after upgrading to Ruby 2.6(.1) from 2.5. Everything else seems fine though.. ",
    "Zyndoras": "Restart times went up to (exactly) 90 seconds for each app that i updated from ruby 2.5 to 2.6.\nThe issue persists when updating to 2.6.1.. ",
    "wkhatch": "Is there a reason you're using ssl in the container for puma, as opposed to forwarding from https to http on the ALB? How are your listeners configured for the ALB? I came across this while trying to determine what the best strategy is for configuring puma for a rails app on ECS, so I'm curious as to what you're doing. I'm planning on terminating ssl at the alb, and relying on the x-forwarded-proto header to determine if the request was over https. . ",
    "shisongsong": "@evanphx Logger is configured but no log data in it. ",
    "cesar3030": "Created a Rails issue. ",
    "jmccartie": "Also: same app setup, downgrading to Ruby 2.5.3, and all is well.\n\u279c  test-app git:(master) \u2717 ruby -v\nruby 2.5.3p105 (2018-10-18 revision 65156) [x86_64-darwin17]\n\u279c  test-app git:(master) \u2717 rs -p 5000\n=> Booting Puma\n=> Rails 5.2.2.1 application starting in development\n=> Run `rails server -h` for more startup options\n[95310] Puma starting in cluster mode...\n[95310] * Version 3.12.0 (ruby 2.5.3-p105), codename: Llamas in Pajamas\n[95310] * Min threads: 5, max threads: 5\n[95310] * Environment: development\n[95310] * Process workers: 2\n[95310] * Preloading application\n[95310] * Listening on tcp://localhost:5000\n[95310] Use Ctrl-C to stop\n[95310] - Worker 0 (pid: 95324) booted, phase: 0\n[95310] - Worker 1 (pid: 95325) booted, phase: 0\nStarted GET \"/\" for ::1 at 2019-03-13 19:07:05 -0700\nProcessing by HomeController#index as HTML\n  Rendering home/index.html.erb within layouts/application\n  Rendered home/index.html.erb within layouts/application (1.3ms)\nCompleted 200 OK in 163ms (Views: 160.9ms | ActiveRecord: 0.0ms). This is resolved with Ruby 2.5.5 . ",
    "ledowong": "same issue here.\nRails 4.2.11.1 here.. ",
    "zunda": "It might be worth while waiting for the release of 2.5.5.\nhttps://twitter.com/nagachika/status/1106317614903521281\n\nHi Rubyists. I have bad news and good news. The Ruby 2.5.4 releasd at day before yesterday has serious deadlock issue with multi thread app (ex: puma). Thanks to @k0kubun pointed me the root cause, I will release 2.5.5 sooner.. \n",
    "bennibu": "We have run into the same issue, but came to another conclusion. Not the cluster mode (--worker) was the issue, but the fixed thread pool (--thread 5:5) caused the problem. So puma --worker 2 --threads 0:2 is working just fine in ruby 2.5.4.. Sorry, I have made another checks and have to withdraw my first comment. It's the worker mode. But with the dynamic thread pool the first request will be properly served, instead with fixed threads no request will get through the stack.. ",
    "stve": "typo here: extention instead of extension\n. ",
    "martinisoft": "You probably want to make the URL point to the puma repo and not your fork.\n. ",
    "activars": "I think it might also need to include:\nrequire 'bundler/setup'\n. I think it would be better to namespace under puma:start. Because this seamless means make is harder for people to define their own deploy flows. I do prefer explicitly define it myself. \n. ",
    "jkraemer": "good point, I changed that in my second commit.\n. ",
    "brotbert": "I guess you should assure that socket.kind_of? TCPSocket.. ",
    "pushrax": "For TIME_WAIT to appear it means the application already called close() on the socket, which should be impossible here, and I'd also bet it's not legal to call close() on such a socket since it implies closing it twice.. ",
    "jumbosushi": "Just changed it to your way. I wasn't 100% sure about where to put the line so this was helpful. Thanks for the tip!. ",
    "ivanpesin": "This is not an equivalent of initial condition. With this change puma serving a single-threaded app with \"threads 1,1\" attempts to pass a request to a worker that is already serving a request. We're seeing occasional deadlocks for the requests coming right after a long-running request  in our environment with \"threads 1,1\", \"queue_requests false\", and \"workers 5\" configuration. Rolling back this single commit to the original solves the issue completely.\n. The problem is in line:\n+          return if @waiting > 0\nThis is not how the original condition worked: until @todo.size - @waiting < @max - @spawned or @shutdown. The fact that @waiting > 0 is not enough for decision not to do @not_full.wait @mutex.\nI prepared a simple test scenario which demonstrates the problem: https://gist.github.com/ivanpesin/f40a76e51a60a83f1190113fdecddaa5\nWith this commit, second request gets always blocked until first request is served, even though there are available workers. Third requests goes to available worker.\nWith original condition, second request goes to next available worker and works as expected. The condition needs to be either reverted to original form or needs logic correction. . ",
    "PikachuEXE": "Should this be fast_write client, \"#{k}: #{vs}\\r\\n\" instead?\nI don't see v defined in this level. ",
    "AnatolyShirykalov": "removed. ",
    "phstc": "@chwevans I don't know exactly about the fix, but I'm often getting RuntimeError: can't modify frozen IOError on Heroku after scaling down.\ncc/ @evanphx @schneems . @evanphx sure. I get that every day when Heroku auto restarts/recycles dynos.\n\n. "
}
{
    "hickford": "Thank you! Out of curiosity, what are you using the library for?\n. Oops, the code in the readme was a few edits behind https://github.com/hickford/MechanicalSoup/blob/master/example.py . Sorted. Thanks. \n. Hi. I don't know. I designed this library to simulate a single human interacting through a web browser. If you want to download and parse lots of pages, you're probably better off using something such as Scrapy.\n. You're right, there should be. \nThere's a small example of form submission in the tests, if it helps. My own code using the library I can't publish.\nLeaving this open, I'd like to write a Sphinx / readthedocs site.\n. Thanks!\n. Done https://github.com/hickford/MechanicalSoup/tags\n. Thanks!\nOut of curiosity how are you using MechanicalSoup?\n. Hi Oleh. Thanks for your message. Let me take a look.\nAh I remember this. The challenge is the form object doesn't know the url it came from. As a convenience I added a second parameter to browser.submit used to resolve action\nbrowser.submit(form, page.url)\nExample https://github.com/hickford/MechanicalSoup/blob/9df83dc607dad5d72bf0fb3c432d34253c43828a/example.py#L24\nThis deserves a mention in the docs.\n. Pardon that wasn't clear, it does a urljoin with action. Check the code https://github.com/hickford/MechanicalSoup/blob/9df83dc607dad5d72bf0fb3c432d34253c43828a/mechanicalsoup/browser.py#L30\n. Looks good to me, thanks.\n. Hello from Lilliput! It doesn't bother me. Glad the library is useful to you, thanks for the compliment.\n. Hi Simon. Good idea, let me take a look\nhttps://github.com/hickford/MechanicalSoup/blob/master/mechanicalsoup/browser.py#L14\n. Are any of the other options important, or would simply adding a parser argument suffice?\nhttp://www.crummy.com/software/BeautifulSoup/bs4/doc/#making-the-soup\n. Nice work, let me take a look\n. Pardon me, looks great. I suppose one of us should write documentation.\n. Cheers\n. Cheers\n. Ah presumably the warning was added by a recent version of BeautifulSoup. It's not obvious what parser MechanicalSoup should default to. Your workaround looks good to me.\n. @lawli3t try simply mechanicalsoup.Browser()?\n. You're right. Let me take a look.\n. Voila! https://pypi.python.org/pypi/MechanicalSoup/0.5.0\nSorry for the lack of attention to this project\u2014I'm getting married next year =)\n. Thanks\n. What's cookiejar?\n. I don't understand the problem. Can you explain clearly?\n. Looks good to me, thanks.\n. Like this? http://www.w3schools.com/tags/att_select_multiple.asp\n<select multiple>\n  <option value=\"volvo\">Volvo</option>\n  <option value=\"saab\">Saab</option>\n  <option value=\"opel\">Opel</option>\n  <option value=\"audi\">Audi</option>\n</select>\n. Can you give an example in HTML?\n. Looks good to me, thanks. Might add a unit test for this later.\n. I'll publish a new release with the fixes.\n. Published v0.4.0 https://github.com/hickford/MechanicalSoup/releases/tag/v0.4.0\n. Cool thanks\n. Looks good to me, thanks\n. lxml is the best parser to use. Try mechanicalsoup.Browser(soup_config=dict(features='lxml'))\nYou'll need to install it from PyPI http://lxml.de/installation.html\nSee also https://www.crummy.com/software/BeautifulSoup/bs4/doc/#differences-between-parsers\n. Did you figure it out?\n. Can we be sure every user will have html.parser ?\n. Right. mechanicalsoup.Browser wraps requests.Session, so it automatically stores cookies for the lifetime of the Browser object.\nIf you wish to save cookies between executions of your app, you can pickle the Browser object (or the session, or the cookie jar). See https://docs.python.org/3/library/pickle.html \n```\nimport pickle\npickle.dump(browser, open(\"browser.obj\", 'wb'))\npickle.load(open(\"browser.obj\", 'rb'))\n```\nPerhaps this deserves to go in the documentation\n. Try browser.session.headers.update as in the example at http://docs.python-requests.org/en/master/user/advanced/#session-objects\n. Thanks\n. Thanks, good fix\n. Thanks. Alas if you really need Javascript, there's no alternative to use a browser or something like Selenium http://www.seleniumhq.org/\nIn the case the Javascript does something very simple, like complete a form, you can try to follow it in a browser inspector and implement the same steps yourself.\n. Thanks. Let me take a look\n. You're right.\nFixed by https://github.com/hickford/MechanicalSoup/commit/014b00c9069c997b76d91968bb37ccdd764aa1b3\n. Cheers thanks. Hi John. Yes you can. The requests Session object is accessible at Browser().session. You can pickle it for reuse. I believe you can also get a cookie jar object compatible with other Python libraries\nhttp://docs.python-requests.org/en/master/user/advanced/#session-objects\nhttp://docs.python-requests.org/en/master/api/#request-sessions. Nice. The original mechanize library was stateful http://wwwsearch.sourceforge.net/mechanize/. You're welcome to make a release. I've given you the permissions on PyPI. https://pypi.python.org/pypi/MechanicalSoup/\n\nPackage Index Maintainer: moy\n. If it helps, I've transferred the repo to its own organisation https://github.com/MechanicalSoup/MechanicalSoup. Only just seen this... Awesome! I really like it. Perfectly fits how I imagined. Mechanical soup! @hemberger Please thank your friend. I want to share this with everyone I know. If it helps, I've added you as a maintainer at https://readthedocs.org/projects/mechanicalsoup/. Out of curiosity, what does this do?\n. Cool thanks for explaining\n. Thanks I didn't know select_one\n. Is soup.encode() preferable to response.body?\n. We expect method will always be \"get\" or \"post\" in practice, so no risk of UnicodeEncodeError in Python 2. \n. That makes sense\n. \n",
    "hamada2029": "Thanks for merge!\nI am trying to automation for Japanese auction site \"Yahuoku\".\nhttp://auctions.yahoo.co.jp\nUsing python3 without mechanize.\n. ",
    "jiamo": "Just find I can self add in _build_request  hanlde select just like input. close the problem.\n. ",
    "halhx": "Form Submission isn't good. I've just forked and will make some improvements there. Do you have a process for contributing code back? \n. ",
    "scarrick68": "Commenting to bring this back up. More in depth form example with submissions on consecutive pages,  filling in select field (i mean  elements), how to press a button on a page would all be very helpful.\n. The github example on the home page is unclear. I'm assuming this section\nverify we are now logged in\nmessages = page2.soup.find('div', class_='flash-messages')\nif messages:\n    print(messages.text)\nassert page2.soup.select(\".logout-form\")\nChecks for a div with that class which should only be present after log in. However, there is no div with class='flash-messages' on the login page or after being logged in. I checked in Chrome's console. the assert statement also didn't print anything when I ran the example.\nprint(page2.soup.title.text)\njust prints \"Github\"\n. ",
    "moy": "I have some pieces of code using MechanicalSoup here: https://gitlab.com/chamilotools/chamilotools if anyone wants example. I think a \"Projects using MechanicalSoup\" section could be added to the README of MechanicalSoup so that people looking for examples can find more.\n. OK, there's now a tutorial on http://mechanicalsoup.readthedocs.io/en/latest/, and there's a separate issue at #114 to track remaining missing docs.. We now have follow_link and related features in stateful_browser.py, so both projects now seem more or less equivalent in terms of features.\nRoboBrowser has more lines of codes (see openhub comparison for example), but I see the small number of lines of code in MechanicalSoup as one of its features: it makes it easy to hack. Real features are in requests and BeautifulSoup.. I think my previous comment answered the initial question, but feel free to open issues for more specific questions if needed.. Fixed conflicts and rebased as 04947a246b34a11b4cf56d793dc1a6bd87aab289. Thanks!. This is an old bug and given the absence of reply to the last comment, I'm assuming it's no longer valid.\n@lben: feel free to re-open if you're still encountering the bug with the latest version.. I don't think anything is going to happen to this issue unless we have a valid use-case. I'm closing it for now, please re-open with more details on why this could be interesting if needed.. I could get it to work with the following patch:\n```\n--- a/example.py\n+++ b/example.py\n@@ -15,7 +15,7 @@ login_page.raise_for_status()  # similar to assert login_page.ok but with full s\n# login_page.soup is a BeautifulSoup object http://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautifulsoup \n # we grab the login form\n-login_form = login_page.soup.select_one('#login form')\n+login_form = mechanicalsoup.Form(login_page.soup.select('#login form')[0])\n# specify username and password\n login_form.input({\"login\": args.username, \"password\": args.password})\n```\nBut I'm not convinced it's the right way. I don't understand why the select_one field exists but is None? Also, what I did is actually reverting 4d89436bce6817b81e9ee76495bb8e4650dbb97a ... Any idea?\nThanks,\n. Just a ping on this one as I'm seeing some activity ...\nDoes example.py work as-is for other people? If my fix seems correct, I'll resubmit as a pull-request.\n. Thanks, @gwynnebaer. Actually, I'm not having the \"'NoneType' object is not callable\" anymore, it was probably an issue with a version of beautifulsoup. So, the select_one is not an issue anymore, but creating a mechanicalsoup.Form is still needed. I understood the issue, see PR #50 for details.. Strange: I get a build failure for Python 3.2, but I tried a build for 73d7601 and it fails the same way (cannot import name utils), while the build was OK at the time it was pushed. I guess the latest version of requests does not support Python 3.2 anymore (which makes sense: people who want a recent version of Python use newer 3.x, and people who want an old version use 2.x).\n. I believe this is fixed in recent versions of MechanicalSoup (e.g. 2424deabd9fe2a58d8f11829c695fdd2e5a99d89). If so, please close the bug. If not, then can you please submit a testcase ?\nThanks,. No answer so I'll close this, but feel free to re-open if you have more info to provide... Oops, deleted the branch by mistake, but here is it again.. Is there a reason to drop support for old versions? In particular, Python 2.6 is not dead yet IMHO. Why not drop it if there's a good reason to, but then the good reason should be mentioned in the commit message.. The fact that distro don't ship 2.6 anymore doesn't mean everybody has upgraded. I still use a production server with Python 2.4 for example. There are more RedHat/CentOS 6 out there than you seem to think.\nI don't question the fact that most users should upgrade, but that doesn't seem to be a good reason for MechanicalSoup to drop support.. Just to be clear: if there's a 2.7-only feature that you really want to use, I have no strong objection to dropping 2.6 support, but then it can be a preparatory patch for the commit introducing this Python construct. All I'm saying is: if we can keep support for 2.6 without a real overhead, let's keep it.. OK, I committed a variant of this in 0798afffa03e27eb22032c9e0d5bbd37916cbe23 and applied my remark about 3.2 in setup.py in 778e019fa68c8187420db809cfa761fc7c54c016. We're all set, I'm closing the PR.\nThanks!. Never tried, but MechanicalSoup uses Requests for all HTTP accesses, and Requests does support proxies: http://docs.python-requests.org/en/master/user/advanced/#proxies\nSetting $HTTP_PROXY or passing proxies= to browser.get() should work. If not, it should be easy to fix.. Hi,\nI think I answered your question so I'm closing the issue. Feel free to reopen if the solution I suggested doesn't work.. > The original mechanize library was stateful http://wwwsearch.sourceforge.net/mechanize/\nThat's where I come from ;-). And that's what I missed when I moved to MechanicalSoup seeing that mechanize was unmaintained and Python2-only.. LICENCE and README.md are clearly a good idea. I'm unsure about the testsuite, which isn't supposed to be helpful to the end user.\nI had bad experience trying to distribute a testsuite in another context (my testsuite contained non-standard stuff like empty files which the distribution didn't like), but this one should be OK and can also be seen as example.\nI think adding the examples would be a good idea.\nI'm leaving this open in case others want to comment, but it seems OK to me.. Swamped by work, but I've just merged your pr. Another pr with examples would be nice IMHO. . You need the latest version (from git, not the latest released version yet) for this to work. Otherwise, you can also use example_manual.py which works with older releases (but requires a bit more code to write).. I'm 99.9 % sure you're still using the old version (or perhaps a mix between the old and the new one). Perhaps you have one installed user wide and the other system wide? . Not sure how many times I'll have to repeat this: the released version doesn't contain statefulBrowser. I'll make a release when I get time (might not be so soon :-().. You need the latest version (from Git, not from a release) for StatefulBrowser to work. See #59 .\nPlease re-open with more info if this doesn't solve your issue.. There's definitely room for improvement here, but there are already workarounds. See #65 and in particular this comment.. Indeed. Next time, you can add \"fixes #number\" to the commit message to get the bug referenced and closed automatically. \nThanks, . Please see other issues about the same message posted here.. Please give more details:\n\nProvide a meaningful title\nWhat does not work exactly? Are you getting an exception? An error message? If so please copy-paste it here.\nDid you upgrade MechanicalSoup? Which exact version are you using?. Can you post your Python code, or at least tell us what you are using? . MechanicalSoup does not try to guess which submit button you are using (there may be several). It could, especially in cases when there's only one. Patches welcome to improve this (I may work on that myself, but not in the near future).\n\nYou can already deal with this:\n\n\nIf you use StatefulBrowser (not in a released version yet), then submit_selected has a bntName arg exactly for that.\n\n\nOtherwise, passing data={'login' : ''} to Browser.submit() should do the trick.. Yes : #59 and #60 ;-).. Of course, statefulBrowser.py is not in a released version yet.\n\n\nI'd say,  it's probably why each of the bugs I pointed you to say \"from git, not the latest released version yet\" ;-).. > running example.py raised\n\nLinkNotFoundError()  \nmechanicalsoup.utils.LinkNotFoundError\n\nexample.py works just fine here. I suspect an issue with your HTML parser (depending on which one is installed, BeautifulSoup will use a different one and it may yield different results). I tried with html.parser and then I can reproduce. I think the proper fix to your issue is what I committed in #68.\nI'm personally in favor of keeping the big, scary warning of BeautifulSoup when the parser isn't specified explicitly, but I seem to disagree with @hickford here.\nI don't like much the idea of removing this #login: it's good in the example to show a way to specify precisely which form you're referring to.. No answer, so I'm taking this as a \"OK, #68 is the right way\", but feel free to reopen or continue the discussion on PR #68 if this is not the case and you think of a better fix.\nIn any case, thanks for the report.. Released, on GitHub and Pypi. Thanks for giving me access.. Looks good to me, but it would be really nice to have a testcase for this. Untested features tend to be broken silently rather easily ...\nI'm merging this as-is, but it would be cool if you could provide a test in another PR.\nThanks!. Hi,\nAs you may have noticed, the Travis job fails with 'No module named requests_mock'. You need to adapt .travis-ci.yml to install requests_mock before running the tests.\nThanks,. OK, there's one Travis-CI failure, but it's not because of your changes: the master branch fails the same test too (it didn't by the time it was commited, but I restarted the test and it fails now, probably because of Travis-CI's latest update). I'm merging this, and we should take care of the Travis failure too, obviously.. Should be ready to merge now (Travis fails on pypy3 because of an issue on Travis's side, but not on ours). Indeed, I knew pytest as a launcher but not as an API. Let's stay in the pytest world for both launching and the actual code.. Indeed, @ozroc, I can't reproduce. requests.Session() is not a shared variable, but a call to the constructor of Session, it creates a different object each time. I'll close the issue, but please reopen with a more specific test-case (a real program as much as possible) if you think the issue is actually there.\n@boarnasia: good suggestion, I've just added these close and __del__ methods. Thanks.. Isn't get_current_page() what you're looking for?. I'm assuming my reply above answered your question. If not, please reopen.. Would be OK to me, but the documentation should state that this is not good practice: in general the target website likes to have accurate logs by user agent, and a server side user agent detection is in principle done for a good reason (well, in principle...). . (Sorry, wrote \"would no OK\" while I meant \"would be OK\" in my initial reply, phone keyboard & spellchecker ...). That sounds like a proper use of user-agent overloading to me indeed. It could be a good example to add to the doc if you add user-agent overloading to MechanicalSoup.. Perhaps a better default may be python-requests/2.18.4 (MechanicalSoup/0.7.0), so that people who filter according to the start of the string still get python-requests, which is the one doing the hard work.. merged.. The commit message talks about {set,get}_cookie but the code defines {set,get}_cookiejar. Can you fix the commit message?. Thanks. It would be even better IMHO to test on a page containing several links (to check that we're able to return a non-singleton). But that's definitely an improvement anyway. . Good idea indeed. . Good news: coveralls.io is already active on this repo. It seems @hickford did some experiments a while ago.\nBad news: I did a quick attempt, but all I get is \"No data available in table\" on coveralls.io. See https://github.com/moy/MechanicalSoup/tree/coveralls-attempt for what I tried. I probably missed something obvious, but .... If you use StatefulBrowser, then submit_selected has a btnName argument: https://github.com/hickford/MechanicalSoup/blob/master/mechanicalsoup/stateful_browser.py#L112\nOtherwise, look at Form.choose_submit.\nWe don't really have a good doc (yet?), but testcases may help.. Does the message above answer the question? If so, please close the bug or ask me to do so. If not, please clarify what you need.. @ericbrandwein : can you send us more details about your issue? Otherwise, let's close the bug.. btnName is not tested within MechanicalSoup, but I use it in chamilotools: \nhttps://gitlab.com/chamilotools/chamilotools/blob/master/chamilolib/create_quiz_lib.py#L304\nhttps://gitlab.com/chamilotools/chamilotools/blob/master/chamilolib/documents.py#L68\nhttps://gitlab.com/chamilotools/chamilotools/blob/master/chamilolib/intro.py#L69\nUnfortunately, I don't remember exactly what/how I did it. But I'm 99% sure I added btnName for a reason and at least one of the instances above correspond to a case where not using btnName did not work. Doesn't mean it's not buggy, but there should be one instance where it does work ;-).\nIt probably makes sense to deprecate btnName in favor of choose_submit, but unfortunately we can't remove it completely because it has one user.. Indeed, the changes deserve a release. I should be able to tag a release soon.. https://pypi.python.org/pypi?:action=display&name=MechanicalSoup&version=0.8.0. Looks good to me after a quick look.\nCan you git rebase -i to squash your 3 last fixups into the relevant commits? If not I'll do that for you when merging/reviewing.\nThanks,. OK, I'll do the rebase for you actually (I'll use your import pytest for something else).. Merged, thanks for your contribution.. Thanks. There was indeed a bit more magic needed than my attempt ;-).. OK, it seems everything just works.\nEagerly waiting for other PR to improve tests, so that we can see the curve moving up ;-).. I'll make one last release supporting these old Python version and merge this after.\nPython 2.6 is the version available on RHEL 6 for example. There are still quite a few users out there.. I'm all for it, but AFAIK, only @hickford can do this. At least, I tried installing it and it showed only my fork of MechanicalSoup, not the official one.\nSo, @hickford , can you:\n\nOpen https://codecov.io/gh\nLog-in with your GitHub credential\nThen you should get an invitation to install codecov.io to your repo, you're a few clicks away.\n\nThanks,\n. Thanks for the move to an organization. However, AFAICT I still don't have permission to enable codecov.io. I'm \"outside collaborator\" in the MechanicalSoup org, and I think I need to be \"owner\" to do this.. OK, not sure whether I fixed something on my side or whether @hickford did, but I'm now a co-owner of the org. I think I enabled codecov.io properly, but I'm not 100% sure I did it right. Anyway, @hemberger, feel free to fix any mis-configuration I may have done. In the meantime, let's close this.. There's a 3rd option that you didn't mention: running individual scripts like cd tests && python ./test_$whatever.py, which does essentially what pytest does, but for individual test files (not a coincidence, it does call pytest underneath).\nI personally prefer always testing the working directory, not the installed version. Essentially for the reasons you mention, but also because it's simpler (no need for virtualenv, no risk of polluting installed libs with a broken one, ...).\nIt is relatively simple to unify everything by adding ../mechanicalsoup to sys.path in the test scripts. I can do that if there's a consensus that it's a good idea. Opinions from others are welcome, in particular what is the most \"pythonish\" way of doing this.. Pushed a PR that should do the trick. Second pair of eyes welcome.. I'm more comfortable with asynchronous communication (email, issues, ...), so I have no objection to setting up gitter, but I can't guarantee I'll be active there.. Looks good to me. I'm leaving you the pleasure of merging the PR ;-).. Should be OK now.. I was thinking the same. Actually, it's a pity we can't just run something like python setup.py install-dependencies or so, but I didn't find a better way than parsing a requirement file as you suggest.\nIOW, I'm all for the proposed change.. PS: I think it would make sense to have both test-dependencies and non-test dependencies in two requirement files. For example, readthedocs needs to install bs4 to generate the doc properly and it accepts a requirement file. For now I've ticked the \"Install your project inside a virtualenv using setup.py install\" box in the advanced settings, and the installation installs the dependencies as a side-effect, but it would be cleaner to provide the requirement file.. I think it can be closed.. The require_changes was an attempt to make the bot a bit quieter, but I must have done something wrong since we still got the report for #108 which didn't change coverage. I thought maybe comments could help non-regular contributors (who get feedback even if they don't know where to click to get the codecov report), but I also agree it's a bit too noisy (getting 2 emails per PR is a bit boring).\nSo, why not disable it completely, yes.. I like the idea very much. I think the design can be improved by having less details, to be more readable for small versions of the icon (ideally readable for favicon).. Good.\nPerhaps it would help if the noodles had more different colors. As is, it's hard to distinguish noodles, hence to notice they are noodles on the small version. Anyway, it's cool, and if it gets better, it's even cooler.\nOnce we agree on the design, we can:\n\n[ ] Commit it to the repo\n[ ] Include it in the README\n[ ] Include it in the docs (readthedocs -> docs/ directory). I'd rather avoid committing a png file too many times (poor delta-compression => repository size increased forever for little benefit). If you have a (small enough) .svg file to commit, that can be done now.. Any news from the artist?. @hemberger : ping?\n\nIf we don't get an updated version, perhaps we can decide that this version is the final one.. Any news?\nIf not, I suggest we adopt the current logo as our official one. I started getting used to it :-).. indeed, @mkormendy's proposal is really cool too. To be sure: can you confirm that you did not use any non-free content to make the logo? I don't care deeply whether the tool used to make it is free, but any non-free content would make the logo non-free too.. Well, the function was there for a while, and if we need to consider whether it can raise AttributeError by mistake, we should also consider all kinds of other possible bugs. Let's keep it simple and remove this exception handler indeed.. Perfect, it works now at http://mechanicalsoup.readthedocs.io/ (updated the text above accordingly).. OK, the logo has its own issue, let's close this one.. It seems a rename is the way to go indeed. . Done.. GitHub rendering available here: https://github.com/moy/MechanicalSoup/tree/restructure-docs\nRendering of the project description (processed README.rst): http://matthieu-moy.fr/tmp/2017/README.html. Badges re-added.\nAbout the version being installed, what do you think of 125e03b ?\nPypi accepts pre-releases, but I don't know any way to deploy some automatically, and you can't re-upload an already uploaded version so we'd need an automatic version number increment.\nOTOH, you can already install from GitHub. I'm documenting this with 4da3f3c (instructions do not work with master, but they do work with moy/MechanicalSoup so they should work once this PR is merged).\n. Untested, but I think you can also:\n4) do the self.alias = self.actual_function in the constructor.\n5) use Sphinx autodoc's :exclude-members: and point it to aliases.\nI think 4) would probably be the simplest.\nAbout a print warning, I'd rather avoid at least for now: it's not a good idea to encourage people to change their code now, as it means their new code won't work with old MechanicalSoup. It's less serious for a small lib like MechanicalSoup that people are unlikely to have installed by the system, but in general a good deprecation flow is: add the new name and keep a silent alias. A long time after, start warning. A very long time after, remove the old alias (or not).. I prefer a history nicely split into logical commits (for example, you're introducing aliases in one commit and moving them around later in history, you could have a commit that introduces them at the right place directly, git rebase -i is your friend), but it'd be too much work for little benefit to do that now. Let's merge this.. I did b5de3b4 when testing on http://httpbin.org/forms/post, and it felt really weird that I still had get_current_page() returning the form when I was visiting the JSON result.\nThere were probably cases when you could use this \"__current_page is kept\" behavior, but note that we were already updating the URL, so if your form's action field was relative, your trick didn't work. Also, if the file you're downloading happens to be text/html, you're doomed.\nSo, restoring the old behavior is clearly not a clean way to deal with your use-case.\nTwo ideas come to my mind:\n\n\nImplement a history. If we do so, I don't think we really want to memorize everything by default, so I'd do this with push_history() / pop_history(), the former setting a checkpoint and the second restoring it (on a stack, because it doesn't cost much to be generic, but I'd expect 99% of uses to be a 1-element stack). I'd do that by first grouping the __current_* fields in a BrowserState class, and then dealing with a stack of BrowserStates.\n\n\nAdd a stateless version of submit_selected, open, follow_link, and possibly others I may have forgotten (for example by adding them a track_state argument, True by default). That would be the equivalent of \"Right-click -> Save link as\" on Firefox for example.\n\n\nFYI, RoboBrowser implements back()/next(). Unlike my proposal it seems to keep all states in memory.. They are not necessarily exclusive. track_state=False gives you a short and sweet way to download something which is one click away, but push/pop is needed for\nbr.push_history()\nbr.follow_link(link1)\nbr.follow_link(link2)\nbr.pop_history()\n\nAnother typical use-case is when you have a start page from which you want to do several actions. Then the code looks like\nfor action in ...:\n    br.open(start page)\n    # Do the action\n\nand could be turned into\nbr.open(start page)\nfor action in ...:\n    br.push_history()\n    # Do the action\n    br.pop_history()\n\n. What I don't like with the \"fixed size deque\" is that it's hard to guarantee that a program using back() will reliably work, unless the size is infinite (which isn't a problem for short-lived scripts, but would be for a daemon that runs forever for example).. Modifying the Makefile would have no impact, the Makefile is just here for convenience, as far as I can tell it's not used internally by Sphinx (deleting it doesn't change anything for example). I've just pushed a PR with an alternative approach.. I agree it would be nice to have.\nI'm not sure what's the best option between being defensive (error out when there are more than 1) or just pick the first. I like defensive code, but in our case it's possible that the developer wrote the script at a time when there was just one form, and later the site adds another form to the page. Then you may as well get lucky and try the first (if it's the wrong one, you'll almost certainly get an error while filling-in the form). Probably the ideal behavior would be to have both indeed.. Why not. I have mixed feelings about milestones for community driven projects though: on the one hand they help sorting the list of issues, but on the other hand in practice milestones assignments are very often empty promises.. IIRC I'm the one who forwarded *args and **kwargs, and if so I did it just because I did not know which exact arguments BeautifulSoup's select was taking, so I forwarded for maximal generality. _candiate_generator is hardly documented (undocumented?), and the fact that it starts with _ suggests that it's an internal detail that users shouldn't use. People calling _candidate_error through MechanicalSoup would get what they deserve if we break their code ;-).\nSince limit comes after _candidate_genrator, it cannot be passed through *args, but only through *kwargs. I'd say we can safely trash *args, and I do not care deeply if we trash *kwargs as you suggest, too.\nI think there's a minor error in your explanations: \"limit specified as a kwarg\" -> there's no kwargs anymore so the error is that there's no such argument. If you keep forwarding **kwargs, then you'll get \"argument repeated\", but the first item will not raise an error.\nIn short: I'm OK with your version, I'm OK with a version that keeps **kwargs \"just in case\" (but it won't be useful anymore at least with the current BeautifulSoup.. OK, this PR keeps bugging me (having 0 pending PR is so nice ...). Let's merge this.. Indeed, specifying a default soup_config seems the best way to deal with the situation. We'll probably need to add lxml to our requirements.txt.. OK, it was a bit harder that I thought because we had several places where we relied on the default parser.\nOne issue is that the new code forces a dependency on lxml, but I think it's worth the trouble. It's just a pip install lxml away.. OK, let me know if you want me to do the push to PyPI or if you prefer waiting a bit.. We could test http://httpbin.org in some tests and https:// in others, to make sure both work. But that's essentially the job of resquests: if we find an https-related breakage, it's 99% sure a request-issue.. I tend to prefer calling browser[\"name\"] in tests, to test the longest codepath. But we should also have a few tests that check directly the set_* methods, so let's merge the PR as-is.. OK, added to the changelog and merged.. To solve remaining issues, at worse you have # noqa:... directives. http://flake8.pycqa.org/en/3.2.1/user/ignoring-errors.html. I was thinking about a file-global # flake8: noqa:501, but it seems to disable all errors for the whole file (ie. back to where you started).\nI also tried:\n```\n@@ -46,7 +47,7 @@ form_html = \"\"\"\n \n Pizza Size \n  Small \n-   Medium \n+   Medium   # noqa:E501 \n  Large \n\n\n```\nSurprisingly, a single instance of this silents the warning for the whole file. Sounds like a flake8 bug.\nBest is probably to actually cut the lines then.. I'm observing the same thing, but I didn't investigate yet.\nFor 1., we could maybe guard the whole testsuite with stg like\nif ! curl http://httpbin.org; then sleep $(($RANDOM % 100 + 100)); fi\n\nFor 2., are you sure it's flake8? My feeling was that it was occurring after flake8, at the beginning of tests, but again I did not look into the details.. > I think a guard like that would be useful, but only for Travis. \nYes, I was thinking of adding the guard to .travis-ci.yml.\n\nthe hang occurred before the first . for a file\n\nI think (but I don't find a way to access the restarted tests logs on Travis, so I don't have an example at hand to check) I saw it after at least some flake8 tests, but not sure it was after all of them. We'll see next time.\n\nWe could add -v to addopts in setup.cfg for a little more verbosity.\n\nYes, probably the way to go. When we have detailed logs about the failure it'll be easier to debug.. Strange indeed, and that's clearly between two flake8 checks. Looks a lot like a Travis-CI issue (the container of the machine freezing for a bad reason). Maybe you can try with --debug + stg like after_failure: cat pytestdebug.log in .travis-ci.yml.. This looks very much like a big with Travis' pypy setup...\nWith a bit of patience, we could maybe reproduce this in a debug VM (https://docs.travis-ci.com/user/running-build-in-debug-mode/). If we can run ps, pstree, top, strace & friends while the build is hanging, we may get hints about what's going on. . * Job #440.6 (pypy3) stalled on the flake8 test of mechanicalsoup/form.py.\n* Job #440.5 (pypy) stalled on the flake8 test of mechanicalsoup/utils.py\nYou're not the only one seeing the trend on pypy ;-).. Did you check whether running a plain flake8 or running it through pytest was making any difference?\nBut note that if Travis's message is correct, the build is killed after 10 minutes without any output on stdout. It's not just slow, it didn't move for 10 minutes.\nOne possible workaround for us would be to disable flake8 for pypy tests. No idea how to do that with the pytest launcher, but at worse, if we take the flake8 part back to a separate step, then it's easy.. Thanks!. Thanks for the patch!. Looks good to me. 0-based index shouldn't be a problem for programmers. . Reopening since I think this deserves a mention in our FAQ (close to the comparison with mechanize). . A bit more details:\nhttp://mechanicalsoup.readthedocs.io/en/latest/faq.html#how-does-mechanicalsoup-compare-to-the-alternatives. Hi,\nprint_summary is a new function, not yet released. Either upgrade (e.g. pip install git+https://github.com/MechanicalSoup/MechanicalSoup to get the unreleased version) or don't use print_summary() (it's a nice convenience feature, but you can easily do without).\nAlso, I don't think you need MechanicalSoup to interact with Jupyter Notebook, as it has a REST API (https://github.com/jupyter/jupyter/wiki/Jupyter-Notebook-Server-API).. First, our stable release essentially has no doc, so we should start worrying about this after the next release.\nYes, I like the idea of having a branch dedicated to releases, in which we can do a few adjustments. The history can typically look like:\n\n* heads/master  Post-release commit on dev (e.g. prepare the appropriate section in ChangeLog & cie)\n| * tag/vx.y.1  Possibly bugfix commits here\n| * tag/vx.y.0  Stable release\n| * Adust version number, links to badges targets to stable\n|/\n* last commit before starting release x.y.0\n* ...\n\n(We just need to be careful that with this setup, merging the maintenance branch to master includes the \"Adjust ...\" commits in master which we don't want). > First, our stable release essentially has no doc, so we should start worrying about this after the next release.\nNot true anymore.\nThe initial concern is still valid: most badges don't show up on readthedocs. Actually, I'm OK with this. Adding a read-the-docs badge on the read-the-docs website is not really useful: people are already there.\nSo, I'd say this is overkill and we can close this bug without changing our practices. Otherwise, the release checklist should be modified to add \"create branch, adjust & commit\".. The badges we have aren't affected by changes on master. This issue was about adding more badges (the ones that are on GitHub and not on Read The Docs. I don't think this would bring much (the other badges are developer-oriented, and we have GitHub for developers). So, I think we can close this.. OK, I think I understand the issue: we build data as a dictionary, which now is a dictionary of lists. Individual lists are ordered, but the dictionary isn't.\nSo, either we should\n1) build data as a list of lists, or\n2) use a comparison operator intermediate between \"check the order\" and \"only check set\": check number of repeated keys, but don't check the order of keys.\nOption 1) might be better (in case some website relies on the order of keys), but AFAIK the order of fields in the data sent by browser is unspecified, so it's probably overkill to try to get the order right. So I'd lean towards 2), and this is rather simple to do: just sort the lists before comparison.\nIn short, I think we could write this instead:\nassert sorted(query) == sorted(expected_post). The non-deterministic ordering we get is not for repeated elements, but for different keys i.e. generating `x=1&y=2` or `y=2&x=1`.. Then, we can use a sort only on the keys and keep the values ordered as they were:\n\ndef c(x, y):\n    return cmp(x[0], y[0])\nsorted(x, c). Yes, that would work, but that just makes the code a bit heavier on our side:\nFirst, you can't data[key] = value anymore. But a straightforward helper function should allow us to deal with a list of tuples while retaining a clean code on our side.\nBut more importantly, if we want the order to be the text order, we can't encode data as we do currently by iterating through elements types by types. Currently, we process all input tags, then all textarea, and then all select.. Superseded by #219 .. Thanks, fixed.\nYou may need a pip install --no-cache-dir to force downloading the latest version.. I tried restarting a build and it does use the cache. The timing gain seems to be minimal, but the cost on our side is negligible, so let's do this.. Is it possible/easy to keep the tests as they are for pypy3 and use pytest-httpbin for other builds?\nDoes adding --upgrade to all the pip install commands help?. First thing I'd do is to reproduce the issue on a minimalistic example (no MechanicalSoup, just a single .py file that imports the guilty package (regex if possible, pytest-httpbin if regex alone doesn't crash), as much as possible within a docker image, or at least a Python virtualenv.. It's actually tempting to disable the pypy3 build until this is repaired. We have python3 and pypy build, so we're already 99.9% sure we won't break pypy3, and it'll still be possible to run pypy3 tests manually before the next release.. Don't forget to update the changelog.\nI'm wondering whether we're loosing some potential flexibility by doing this. Currently, we're not, but what if we ever want to provide a hook that the user could execute between the \"prepare\" and the \"send\" step? Will it still be possible?. > without this PR we would need to change the API to allow users to modify the prepared request before sending\nYes, we would need to modify our API, but it's rather simple to provide a add_hook(callback) and to call the callback between prepare and send before the PR.\n\nI don't think this should be any harder of a task after this PR is merged.\n\nThe problem is that the lines of code between prepare and send are currently in our code, so we can still add whatever we want (like calling a user-provided callback) there. After the PR, the only lines of code between prepare and send are in the requests library, we can't modify them anymore.\n\nI think the idea behind requests.Sessions.request is to allow people to specify almost anything\n\nYes, but the question is whether this \"almost\" is sufficient for 100% use-cases. The request library provides the flexibility of allowing tweaking the prepared request before sending:\nhttp://docs.python-requests.org/en/master/user/advanced/#prepared-requests\nWe don't currently need it, but it'd be nice to keep the possibility to do it in the future.\nRe-reading the code, I think there's another way to expose the full flexibility of the requests library: merge your PR and then split _request into:\n\n_extract_form_data() that would build kwargs (i.e. the current _request(), except its last line)\n_send_form_data() that would call self.session.request(..., kwargs)\n\n(Or perhaps these would deserve a name not starting with _)\nand expose _request() that would just call these two functions. This way, the user may, if needed, call _extract_form_data() and do whatever tweaking is needed to actually send the HTTP request. This would slightly increase the flexibility of the code, and would also split the form-related code and the http-related code better.\nI short, I think we can merge the PR, and we may want to do the split I suggest later.. @bleuf1shi : any news?. That sounds like a possible improvement yes. I won't have time to add it before a while, but pull-requests are welcome (that should be an easy one hopefully ;-) ).. The feature requested here is simpler than you seem to think. Currently, select_form accept only a string as argument, and searches for the corresponding Tag object. This issue is about accepting a Tag object directly. The Tag object would be the form tag, not its children. . Note that we could easily add a browser.save(file) function that would do what the last lines of your code does. Not only this would simplify the user code a bit, but that would also make the feature easier to discover.\nSimilarly we could have a browser.download(url or link, file) function that would do both the open/follow_link and the download step, ideally streaming directly to disk instead of downloading to memory first. . Indeed, browser.download could be implemented without touching the state of the browser and that would cover 99 % of the need for a back button. . OK, I've pushed 2 commits on top of yours (the first is needed for the Travis-CI build to pass on Python 2.7). If you're OK with this, I'll squash them together and merge.\nThanks!. Merged (manually to squash everything together, and I took the liberty of amending the commit message).. Let's merge this. It'll still be time to improve it later if we get better ideas. . Indeed it's harder than it seems. My best try was:\n\n@pytest.fixture(scope='session')\ndef httpbin(request):\n    try:\n        import pytest_httpbin\n        return pytest_httpbin.plugin.httpbin(request)\n    except ImportError:\n        from utils import HttpbinRemote\n        return HttpbinRemote()\n\nIt kind of works, but it actually doesn't solve our issue: if pytest-httpbin is installed, then the code crashes before reaching this point, since pytest does the import pytest_httpbin before we get a chance to execute our code.\nUnfortunately, just deleting conftest.py is not enough, one also has to edit tests/requirements.txt to get the real pytest_httpbin fixture with python setup.py test.\nIf we want this conditional activation, the best idea I came up with would be to adapt tests/requirements.txt and delete conftest.py before launching the tests in .travis-ci.yml. Probably overkill, so I'll stop here.. Actually, we could/should even use Python's @property and avoid getters right away.. You don't need to pass a link to submit_selected, it finds it from the \"action\" field of the form.\nJust use submit_selected. . Seems related to my changes in #184, I'll see if I can do something.. BTW, about your example setting the Referer header, it's now managed by MechanicalSoup (just landed in master, not released yet).. > I am looking forward to test your work ...\nDon't hesitate to test before the next release (which we plan to tag as 1.0).\n\nI  would just have to do ...\n\nYes, it should just work. But you don't need the url_regex= part actually. Just the positional argument works.. I should be able to merge it to master soon, but I'm waiting for comments from hemberger.. Hi,\nHappy new year and a ping on this PR: @amotl: did you get time to address the issues in this PR?\nThanks,. This PR was doing too many things in one (for example the header-related changes are independent, and probably no longer needed as we now natively set the Referer, and it turns out other parts of the patch are not needed to fix the issue). I picked the important part, added tests, and included it in #184, so I'm closing this one.. MechanicalSoup does not do anything about the referer field. I would have bet that requests (the underlying HTTP library) would have set it automatically. If not, we should set it in MechanicalSoup by default (our goal is to mimic the behavior of a browser, which does set it).. We're planning a release soonish. In the meantime, you can use the master branch, it's very stable (always 100% test coverage and all tests pass).. I don't like the \"save and restore state\" approach. In general, if you don't want to modify something, it's best to leave it untouched that to save+restore it. For example, if you get an exception during the call to self.follow_link in your code, you'll skip the restore part and get the browser in an inconsistent state (this can happen if you follow a link to a 404 error).\nI've pushed 3 commits on top of yours (we should merge my commit #3 with yours if we go this way) to implement the same feature without save/restore. What do you think?. About the method name, I don't like follow_link much because it doesn't download a \"link\", but its destination. However, I'm not sure I can propose a better alternative: it is about a link since it shares the same API as follow_link. Perhaps we can just do the job within follow_link by doing something like\ndef follow_link(self, ..., download_to=None, ...):\n    ...\n    if download_to is not None:\n        # Save response content to file 'download_to'\n    else:\n        # Current behavior of follow_link\n\n. > Do you mean you don't like download_link as the name?\nJust that it suggests that we \"download a link\", while we actually follow the link to download the target page.\nBut I agree with your arguments, and I'd add that someone looking for a way to download a file is likely to grep \"download\" in your API, hence the name should be download_<something>. So download_link is not that bad. Perhaps download_link_target would be more technically accurate, but it's probably overkill.\nIn short: let's go with download_link. I'll add the missing tests and rebase+squash commits soon.. Let's continue the discussion on #184 .. OK, so I just did a new push with your suggestions, except the return_url_only part for which I disagree (see my comments above).. Streaming the result directly to the file would certainly be better (loading everything in the RAM to write it to a file is certainly a brute-force approach...). Ideally, we could allow the browser to continue browsing while the download streams to the file.\nI won't do this in this PR, but it can be added later if needed.. Just pushed a version that should address all your remarks (+ rebased on top of the Referer PR).. OK, I finally went a different way, as I could not find a better way to get this download_link and fix #177.\nIt looks like your suggestion with a bit of duplicated code, except that the fix to #177 adds much more code to potentially duplicate in follow_link, so I created a helper function. This function ends up looking a lot like what I tried to do in find_link, but it's an internal one to deal with convenience special-cases, so it doesn't need to change the existing find_link.. >     Since we are using properties, do we still want to change the getter names? I would say yes, but I'm open to alternatives.\nI don't think we need the getters. The point of properties is to avoid them or make them transparent.\nSo, I'd do:\n\nAdd the properties. This should look like\n@property\n    def form(self):\n        return self.__state.form\nKeep the existing getters, but deprecate them\nNo need to create new getters with shorter names\n\n\nDo we want complete backwards compatibility? If so, then perhaps we can consider keeping (but still deprecating) get_current_form with the exception-free behavior instead of aliasing it to get_selected_form. However, I think the exception version is safer and a fairly transparent backwards incompatibility, and I would prefer transitioning to the new version outright.\n\n\nI can imagine someone writing:\nif browser.get_current_form() == None:\n    browser.select_form(...)\n\nThat's reasonably unlikely, but it probably makes sense to keep the existing getter as-is.\n\nSince the properties are so much more convenient, should we advertise them in the docs in favor of the traditional getters?\n\n\nTo me, yes.. My vote would be:\n1) Release 0.10.0 from master\n2) wait a bit\n3) if no one complains, release 1.0\n. A bit of context:\nMechanicalSoup started with just Browser. The main feature it provided was to take a soup object containing a form and turn it into a Request (Browser._request). While using it for chamilotools I wrote an additional convenience layer to keep track of current URL, selected form, ... I proposed contributing it to MechanicalSoup (#48) and got only moderate enthousiasm (read: no response at all ;-) ). I submitted my changes as a separate class partly because I didn't want to be too intrusive for my pull-request to be accepted (the project was not actively maintained and I was only a minor contributor back then). I also thought there may be valid uses for the plain Browser, if people wanted to do weird stuff not directly following the \"follow link/submit form\" flow, but I'm not sure I was right (i.e. there are not many valid use-cases, and probably all of them can be dealt with equally well with StatefulBrowser).\nI still partly like this distinction, not necessarily from the end-user point of view but from the software architecture point of view: Browser makes sense as a simple wrapper around requests and BeautifulSoup, and more features are built on top with StatefulBrowser. Perhaps we should just consider that Browser should be a private class and shouldn't be used directly at all by end users. If so, the name StatefulBrowser is unfortunately long.\nIn any case, changing what we expose to the end user raises the issue of backward compatibility. It's likely that this discussion concludes in \"we should have done otherwise, but it's too late to change\".. > Before the release of 1.0 is the appropriate time to make any major infrastructural / API changes that you want to live with for a very long time.\nI'd actually say the opposite. Most people who want to use MechanicalSoup are already using it. I don't think anyone is holding it's breath until the 1.0 release. It's already too late to do major backward compatibility breaks. As for internal changes, a major overhaul right before a stable release is rarely a good idea.\nI think it's best to release 1.0 soon, and consider that we're already in a stabilization phase, and decide later if we want a major change.. The conclusion of the last discussion was that this was unlikely to happen. Should we close the bug?\n(Closing doesn't mean it won't happen ever, just that we stop having it in our list of open bugs i.e. on our TODO-list. I like it when the list of open bugs is short.). Oups, I see there's already #189 .. Looks good to me.. By \"browser\", do you mean a real browser (Firefox, Chrome,...)? If so, then no, there's no simple way to do that with MechanicalSoup. I think Selenium can do that. . MechanicalSoup does not do JavaScript. You may want to look at Selenium instead. See the FAQ:\nhttp://mechanicalsoup.readthedocs.io/en/stable/faq.html#how-does-mechanicalsoup-compare-to-the-alternatives. First a disclaimer: take my comments as advises, not as orders ;-).\n\nI agree that the existing design is not ideal. We could in theory process the soup object in Form.choose_submit as we do now, and then instead of deleting any non-matching submits, just store the matching submit. Then, when Browser.submit is called, we could call a new function Form.delete_unused_submits or similar before extracting the soup object to pass to Browser._request. I did consider this, but I didn't want to add a reprocessing layer for a feature that has never been requested (Form.choose_submit has always been destructive in a way that prevents calling it twice -- it seemed helpful to at least document this with a clearer error message).\n\nMy point is not that the \"non-destructive\" aspect of choose_submit() would be a helpful feature, but that it'd be would lead to more readable code. Right now, the function is destructive, but has to be called by submit_selected(), hence has to deal with the case where it's called multiple times, but only in the case when btnName is None. That makes a lot of particular cases, and they're interleaved with the actual change introduced by this PR, i.e. remove name attributes. The PR is far more difficult to review than it should be (it seems I'm complaining, but I'm not, read the disclaimer again if needed ;-) ).\nHad the function been non-destructive, the code would have looked like\n```\ndef choose_submit(self, submit):\n    if submit is a string:\n        submit = find the right tag\n    self.__selected_submit_tag = submit\ndef submit_selected(self, btnName=None):\n    if btnName is not None:\n        self.choose_submit(btnName)\n    self._remove_unused_submits()\n    ... end of submit_selected() as it is now ...\n```\nThat said, now that the code is written and that I did most of the job of understanding how/why it works, I'm OK with the plan:\n\nUnless there is a really compelling suggestion, my recommendation would be to evaluate the PR as-is and then consider refactoring Form.choose_submit as a separate task.\nAs for commit organization, I'd like to keep the original commit intact if you don't mind too much. Were they only my commits, I'd certainly agree to squash, but I don't think it muddies up the history too much here, and it's nice to recognize blackwind's contribution. I usually just look at the \"Files Changed\" tab to see the overall effect of the PR, and then only delve into the individual commits if something isn't clear there. Would that be okay?\n\nThe issue here is that not only the code is split, but the commit message is split too. It's not clear which commit message refers to which change. I'm OK to leave it as-is if you prefer, but in this kind of situations I usually edit the contributor's commit (content and commit message), and add a trailer like\nCommit-message-by:\nPatch-edited-by: ...\nActually, there's now a more-or-less standard Co-authored-by:.\nI'll take another look at the PR when I get time.. Agreed for 2, 1, 3. . Unless someone objects soon, I'll merge this.. OK, let's merge. As much as possible, we should improve the code later.. Indeed. Do you want to debug this or shall I?. Why not. It should be rather straightforward to do (essentially turning the install part of our .travis.yml file into a Dockerfile).\nOut of curiosity, what are the difficulties in setting up MechanicalSoup on Alpine Linux?. Looks good to me.. Indeed. Supporting image as a synonym for submit is straightforward. Support for .x and .y fields needs a bit of thinking: what should the MechanicalSoup API look like?\nPerhaps\nbr[\"image.x\"] = x\nbr[\"image.y\"] = y\nbr.submit_selected()\n\n?\nThen br[\"image.x\"] would just check that there's an image input with name image, and store somewhere the value x. The easy implementation would be to add a hidden field to store the value. Perhaps it'd be nicer to store the value as part of the image tag, adding fields like data-x and data-y?. Actually, looking a bit closer, I don't think the warning is legitimate:\n\n\nThe method override does follow Liskov's Substitution Principle: a call to browser.launch_browser still works with StatefulBrowser. The user has to provide the soup argument when calling a method on the base class, and may still provide it (but doesn't have to anymore) for the derived class.\n\n\nAll calls to x.launch_browser() without arguments are done as self.launch_browser() where it's clear that self is a StatefulBrowser, not a Browser.\n\n\nAm I missing something?. > This SO answer suggests that LSP is violated.\nThis is not the same case. The answer you're pointing to shows a case where the derived class forces several arguments when the base class' method took none. The one we're dealing with is:\n\nBase class forces one argument\nDerived class' method also accepts one argument, but allows no argument too. It's strictly more permissive.\n\nThe analogy with C++ is limited, since default argument value is resolved statically in C++ while Python resolves it dynamically. The notion of hiding method does not really have an equivalent in Python: Python just grabs the first method it finds with the name, all dynamically.\nFor example:\n\nclass Base():\n    def foo(self, x):\n        print(x)\n\nclass Derived():\n    def foo(self, x=42):\n        print(x)\n\nd = Derived()\nd.foo(10)\nd.foo()  # outputs 42\nb = Base()\nb.foo(10)\nb.foo()  # Forbidden\n\n. > I've deactivated pull request integrations for LGTM.\nThe fact that the only current warning is a false-positive doesn't mean the tool isn't interesting.\nActually, I like using MechanicalSoup as a real-life but small-sized test-case for this kind of tools, so I'd be in favor of re-enabling the pull-request integration. OTOH the expected actual benefit is indeed quite small.. I'm assuming that it would point out only warnings that appear in the diff, but I haven't tested. It's also possible to add in-code annotations to disable warnings, but we may not want to pollute the code if there's no real benefit.. Indeed, I'm guilty of not having read the docs well enough :-(.\nIf the only choices were to keep the code as-is or to change the base class to Exception, I'd favor breaking backward compatibility too. But there may be another option: keep both base classes. This fixes @gronka's use-case, but still works for people expecting except BaseException to catch it:\n```\n--- a/mechanicalsoup/utils.py\n+++ b/mechanicalsoup/utils.py\n@@ -1,4 +1,4 @@\n-class LinkNotFoundError(BaseException):\n+class LinkNotFoundError(Exception, BaseException):\n     \"\"\"Exception raised when mechanicalsoup fails to find something.\n This happens in situations like (non-exhaustive list):\n\n@@ -12,5 +12,11 @@ class LinkNotFoundError(BaseException):\n     * The user tried to fill-in a field which doesn't exist in a form\n       (e.g. browser[\"name\"] = \"val\" with browser being a\n       StatefulBrowser).\n+\n+    For backward compatibility reasons, this class derives from both\n+    Exception and BaseException, but it should in theory\n+    inherit only from Exception. Do not rely on BaseException\n+    to be a base class in your code as it may be removed in the\n+    future.\n     \"\"\"\n     pass\n```\nIt's a little ugly, but it should fix the issue, and keeps the door open to removing the incorrect base class in the future if we really find it too ugly. What do you think?. @hemberger : you are right. And I wrote my nonsense message after coffee ;-).\nThere's another potential change for users if we change the base class:\ntry:\n    ...\nexcept Exception:\n    <special case for Exception>\nexcept BaseException:\n    <other treatment for BaseException>\nBut I think we can safely consider this as pathological scenario (I can't think of any good reason to write this).\nSo, changing BaseException to Exception seems the right thing to do (together with a note in the ChangeLog obviously).. Which version of MechanicalSoup are you using?\nWe've fixed some garbage-collection-related issues a few weeks/month ago, the fixes may help.\nIn principle the garbage-collector should destroy the browser object which in turn should close the connection (i.e. your code is supposed to work). But note that you don't need to re-create a new browser each time. It's usually more efficient to instanciate a browser and keep using the same object.. I can't help if you don't give the version of MechanicalSoup you're using ...\nWe made no effort towards asynchronous requests: the common use-case of MechanicalSoup is when the pages just fetched are used in the following actions (find a link in the page, ...), and it this case asynchronous requests are useless.. open is a very straightforward wrapper around get, so they should really have the same behavior.\nYou should try reproducing your issue on a public website like http://httpbin.org/ and post the exact code here so that we can reproduce it.. What do you mean by path/to/some/download/without/filename/ (the \"without filename\" part)? If you mean that the actual URL should include a file name in addition to the directory name, then how is the program supposed to find out this filename?. I think you're mis-using download_link. The argument is a regex, to be matched against URLs available in the page. If you're getting a LinkNotFound, it means the link was not found in the page (this is just a search in the current page's text, no access to the remote site). If you already know the exact URL, then using a plain .open() should be easier. Otherwise, you have to make sure that 1) the link is present in the current page, and 2) you didn't mess up with regexp special characters.. Looks good to me. I'm wondering whether we should issue a warning when we detect no action= field and JavaScript to be executed on submisson.\nThere's a Travis-CI failure which deserves investigation but seems unrelated. . Indeed, reproduced here.\n\nProbably httpbin changed some bits?\n\nMore than bits. The home page is revamped, and the unfortunate news is that it's now full of JavaScript. For example, the first failure is:\ndef test_referer_follow_link(httpbin):\n    browser = mechanicalsoup.StatefulBrowser()\n    browser.open(httpbin.url)\n    response = browser.follow_link(\"/headers\")\nThere's no mention of /headers anymore, but it just seems to have been renamed to /response-headers, so that part is easy to fix. But the \"link\" to /response-headers is full-blown Javascript, and we can't use follow_link anymore. And this test is about follow_link and checking the referrer :-(.\nThe good news is: http://httpbin.org/legacy :-).\nI'll try to adapt the testsuite to use this legacy page shortly.. OK, so I have a fix. It would probably make sense to make a release, so that the Debian CI can use it painlessly. Looking at the commits since the last release, there are essentially straightforward bugfixes and documentation improvements. The only potentially harmful change I see is 3eac643 (Derive LinkNotFoundError from Exception, 2018-06-07). So I'd suggest considering it as a minor release and tagging it as 0.10.1.\n. Hmm, I forgot another minor thing: there's no ChangeLog entry in this PR. You may want to re-use the one you wrote here:\nhttps://github.com/MechanicalSoup/MechanicalSoup/pull/160/files#diff-4dc38e402d240900f2675401a08d2983. Hi,\n@Neuroforge: can you give us more info about what you did so that we can help you? Without more info, I'll close the issue.\nThanks,. Can you include your comment in the commit message?\nThanks. . > fyi, you don't run python setup.py test in CI\nNo (I don't remember whether there's a good reason to run pytest over python setup.py test actually), but users may want to. One nice thing with setuptools is that the same commands are supposed to work in almost any Python projects, so a new contributor may want to run python setup.py test before even reading the doc.. This is already possibly via Form.choose_submit.\nIt may make sense to allow passing a bs4.Tag element to submit_selected().. > Unfortunately, changing btnName to a more generic name would break backwards-compatibility\nWe can still add a new argument with a more explicit name, and keep btnName (which I don't like anyway, since it doesn't fit our usual naming conventions) as a deprecated alias.. Sounds good. I'd just write \"that is a  at its root level\". I first read your sentence as \"the tag should be the toplovel of the document\" which couldn't be right. And perhaps s/will be/may become/, if we don't want to commit to changing it later.\nCare to make a patch, or shall I?. I've just added two commits:\n\nInstructions on how to edit the logo with Inkscape (I confirm that I can do at least basic editing in Inkscape, and after installing the right fonts I get the same rendering as the PNG, so if we ever need minor adjustments it's possible).\nBetter integration in the generated HTML\n\nI think it's ready for merging (we can do a bit of rebase -i to squash @mkormendy 's commits together while merging). @hemberger , OK for merging?. Oops, one more tiny improvement that should address the \"in case there is anyone not loading images\" concern: add an alt= text to the README.. No problem.. I like the idea of the \"mechanical\" font, but I think the level of details is not right compared to the rest of the logo: if the logo is rendered with a small size, then one won't see what the font is about anymore. On a large size version the rest of the logo seems too flat compared to the text.\nPerhaps we can keep the simple font when MechanicalSoup is displayed below the logo, but find a place to write MechanicalSoup with this fun font elsewhere?\nBTW, what font is it?. OK, there's been some discussion, and I see that some comments have been edited to avoid turning this PR into a troll. I'll still try to answer the points raised.\nI think there's a misunderstanding on what it means to contribute to MechanicalSoup. MechanicalSoup is free software, MIT License. This means that anything contributed to MechanicalSoup is free and can be modified by anyone. Legally, and morally. We're developing it for fun (there was a time when I used MechanicalSoup in a work project, but I don't use it anymore actually). Part of the fun is to try to make it as nice as possible, but another very important part of the fun is to work together with other people, argue about what's the best way to do something, iterate over a proposed change, try things, throw away some attempts, try other things. Disagreeing with others is part of this too, and there's nothing wrong with it.\nThe fact that people try to modify your work is not a negative judgement on your work, but pretty much the opposite: it means we're interested in what you did, and want to have fun with it. I prefer @mkormendy's version, but I still had fun looking at @hemberger's one.\nI'm all for merging this PR (and @hemberger is too as he approved the pull-request), but before we do I want to make sure that you (@mkormendy) understand the consequences of doing so: the logo would be part of the project and be MIT Licensed. This means that you agree that people may, and probably will modify your work. I you prefer your work not to be modified, I respect that (BTW, maybe we did not insist enough on the fact that your logo is really cool and at least I clearly wouldn't be able to do anything remotely close to that awesomeness), but then you should not contribute it to free software.. Actually a single commit is sufficient I think. I squashed everything in #234. I'm closing this one in favor of it.. cc: @mkormendy \nI just squashed all commits together and reworded the commit message. The end result is the same.. I changed the target branch of readthedocs temporarily to this PR to see it live. It's there until the next build (i.e. next push to master): https://mechanicalsoup.readthedocs.io/en/latest/. Unfortunately I don't seem to be able to do much better in reasonable time. The best I could come up is:\n\nStill not readable at 16x16, but perhaps less ugly:\n\nI'm fine with not merging this, but it may make sense to merge and improve later. No strong preference.. You're right that the logo doesn't show up on PyPI (point 1): there's a <img alt=\"MechanicalSoup. A Python library for automating website interaction.\" src=\"https://warehouse-camo.cmh1.psfhosted.org/4e918f3614d3b3c95524d2d452f742c1b87f0874/2f6173736574732f6d656368616e6963616c2d736f75702d6c6f676f2e706e67\"> link on https://pypi.org/project/MechanicalSoup/, but it's broken.\nHowever, for point 2: I don't see the PNG file in the release (neither by running setup.py locally nor in https://pypi.org/project/MechanicalSoup/#files). Did I miss anything?\n@senabIsShort: fixing the broken image link this seems feasible for your project to me. The description on PyPI is generated in setup.py. We already turn relative links into absolute ones there, so it's just a matter of doing the same on image links.. OK, so just point 1 remain valid (I've edited your post to avoid confusion).. BTW, you can test that the substitution (relative -> absolute link) worked locally with:\npython3 ./setup.py sdist --keep-temp\nhead MechanicalSoup-1.0.0.dev0/README.rst\nA real test on test on https://test.pypi.org/ is welcome too though.. What do you mean by non-form? Can you give an example?. As I mentioned in the comments of your StackOverflow question (and as @hemberger guessed), the page you're trying to browse relies on JavaScript far too much for MechanicalSoup.\nI'm closing this issue, if you don't think we answered your question or if you have more information to share, feel free to add comments and/or edit your StackoverFlow question.. Not sure the upper-case version is correct HTML, but anyway, we should be tolerant in what we accept.\nPatch follows.. The stackoverflow link is about tag names, but the issue here is about the attribute's content.. Anyway, \"how HTML should be?\" is not very important in real life for tools like MechanicalSoup. \"How the HTML you're parsing is?\" is the right question (somehow, well-designed websites have a proper API to avoid having to parse HTML, so they don't need MechanicalSoup ...).. > There are a lot of other places than just \"radio\" and \"checkbox\" where we perform case-sensitive string comparison of the type value. To do this for only a subset of the types would be inconsistent.\nI didn't do a very thorough inspection, but a quick grep for type shows that there are not that many instances. I've added one commit dealing with a few other cases. I may have missed more cases, but it seems to be complete. In its current shape the patch does serve a purpose, I think it's sufficient as-is and tolerance to non-lowercase type attributes can be improved later if we find more corner-cases where it doesn't work.\n\nI'm surprised BeautifulSoup doesn't handle this issue somehow. It explicitly states that it converts the tags and attribute keys to lowercase (https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.html?highlight=tag#other-parser-problems), so it is at least tangentially aware of this problem.\n\nFor tags, it's very clear that they are case-insensitive. For attribute names, they can contain more or less arbitrary data and it would be wrong from BeautifulSoup to modify this data (think of what would happen if you lower-cased all value= attributes ...). I was tempted to lower-case all attributes when creating the Form object, but I believe this would be wrong too for the same reasons. Lower-casing only type= attributes could work, but feels like a hack too. I didn't find a definite reference for that but I believe this type=\"RaDiO\" is not proper HTML (and if so, this gives one more argument for point 1.: if it's incorrect HTML I'm much less hesitant to apply a best effort approach).\n\nThis implementation might be difficult to maintain. If you add new branch, you have to remember that you're supposed to make case-insensitive type comparisons. Maybe there's a way to do this at a lower level?\n\nWell, I'd love a case_insensitive=True argument to BeautifulSoup's search methods, but apparently they are not there. I've added a small helper function to do this in Form, which makes the code a bit more compact. Obviously, one still has to remember to use it.. I had a quick look again, and it looks good to me. I had forgotten about all this and was convinced by my own arguments while reading the thread ;-).. @rationa-kunal: thanks! Let us know if you need help, and don't hesitate to post drafts if you want feedback.. The most important point IMHO is that disabled fields should be ignored at submission time, unconditionally. Then, why not a feature to protect against edition of disabled fields. I don't think it'd be sane to allow editing disabled fields anyway (since they'll be ignored at submission time, there's no point editing them), but a feature to easily turn a disabled field to an enabled one would be nice.. The problem with disabled fields is not the ability to edit them. It is that these items are currently considered when submitting the form. For example, if you have\n<input type=\"text\" name=\"someName\" value=\"someValue\" disabled=\"disabled\">\nThen someName=someValue will be added to the form's data (i.e. in the URL in the case of GET forms). It should not.\nMechanicalSoup already allows the user to edit things that are not meant to be editable in the page: the user of MechanicalSoup gets a completely editable BeautifulSoup object representing the HTML, and can do anything with it. It is not like an interactive browser where disabled elements must be explicitly disabled in the graphical user interface, to prevent users from doing mistakes. Here, the user is a programmer who can be expected to know what he or she is doing.. Currently disabled fields are editable in MechanicalSoup, so @Quetzalcohuatl the behavior you expect is the current one (the question you link to is about mechanize, a different library).. @hemberger : fully agree with you here.\n@Quetzalcohuatl : Indeed: keep in mind that MechanicalSoup is a very thin layer on top of requests and BeautifulSoup (both very solid libraries). Everything is possible by looking under the hood ;-).. Thanks for the PR. You still have 2 flake8 issues to fix:\n/home/travis/build/MechanicalSoup/MechanicalSoup/mechanicalsoup/browser.py:235:-341: W605 invalid escape sequence '\\*'\n/home/travis/build/MechanicalSoup/MechanicalSoup/mechanicalsoup/browser.py:235:-339: W605 invalid escape sequence '\\*'\nAlso, we prefer clean history, so if possible make a git rebase -i to squash the commits together. If you don't, no problem: we'll squash on merging.\n. Also, we have a strict \"100% coverage\" policy, so this will obviously need some tests. Tests are fun to write actually :-).. OK, the flake8 issue was not your fault, it's an old issue that flake8 started complaining about recently. Fixed in master (91f48c4c1747fff6e5e2dce0b3d8d9dcee83e25e), just rebase your branch against master and you'll get the fix.. The MechanicalSoup behavior makes more sense to me, but since the goal is to mimick browsers, we should definitely change that.\nShould be straightforward to fix: https://github.com/MechanicalSoup/MechanicalSoup/blob/master/mechanicalsoup/browser.py#L179-L180\nDo you want to try a patch for that?. I meant \"try writing a patch\". The code itself should be straightforward, I pointed you to the place where we just skip (continue) when the file is not given. The harder part is writing a test. No problem if you don't want to work on that, but if you're interested in writing a bit of code for MechanicalSoup, it's a good one to start.. I don't think you need any new dependency. We already have tests to check the content of the file, using httpbin. See eg. https://github.com/MechanicalSoup/MechanicalSoup/blob/master/tests/test_browser.py#L84-L109\nThe new test should be very similar, but without the pic_path related code to submit the file, and then checking that the content is \"\" instead of \":-)\".\n(Writing this, I realize that we don't actually check that the file is found, which should be fixed in  #251). Both commits are related and should be squashed IMHO. For example, someone doing a git blame on the tests would be happy to see that the tests were added to check a particular code change. Having two separate commits makes this a bit harder.. > Thanks for the PR @tiboroche! I did a tiny bit of cleanup and then added a little more documentation in a fixup commit.\nLooks all good.\n\nI would suggest that we squash these commits together and use my commit message for the final version.\n\nOK for me. I'd suggest keeping @tiboroche as author and adding a Co-authored-by: trailer.. Squashed and pushed as 5b16e006f808748d668cfd633802bb010ab65ab7.. Weird: if I read correctly, you're creating and destroying the StatefulBrowser object at each call to poll_router (you don't keep a reference to browser after the function terminates), so we should normally free everything.\nMemory leaks in Python happen when there are circular dependencies (it's a reference-counting garbage-collector). I don't see where we could have one (I was suspecting a pointer to the browser in the soup object, but I don't think it's the case).\nPimpler (https://pythonhosted.org/Pympler/muppy.html) should help identifying which objects are eating your memory.\nMaybe you could try re-using the same StatefulBrowser object for all iterations and see if it makes a difference.. We have automatic testing, and as you see there are issues to fix in your code (at least flake8 formatting issues). This would also require documentation and more tests (we have a strict 100% coverage policy).\nBut as I said in #248, I don't think this is the right approach. If you think it is, then please rewrite the commit message to explain 1) the issue you are trying to solve and 2) why you think your modifications are the best way to fix this issue.. As I wrote in https://github.com/MechanicalSoup/MechanicalSoup/issues/248#issuecomment-443687132, I don't think edition of disabled fields is the problem. The first thing to fix is the form submission.\nAlso, the Travis build fails, we can't merge a PR with test failures, sorry.. I believe 685eb6e1396008a0b3881bea5bcc6b7119f61642 fixed the main problem with disabled fields, so let's close this.. It works for me. Just re-tested to be sure:\n$ python3 example.py moy\nPlease enter your GitHub password: \nGitHub\nOTOH, I get the same failure as you if I enter an incorrect login or password.. Thanks!\nCan you rebase on top of master and squash all commits into one? Ideally, the commit message can contain a mention of the commit introducing the xfail and explain why it's fixed.\nIf you don't have time, I can do it while merging, no problem.. @facelessuser Thanks also for taking care of reaching your users without waiting for them to come to you. Very much appreciated indeed!. Even if it doesn't happen for a long period of time, I prefer to avoid temporary breakage. This makes history bisectable and IMHO more readable.\nYou can mark the test as known failure with @pytest.mark.xfail in this commit, and the removal of this mark in the next commit documents the fact that it fixes the bug. . I didn't look closely at the code but the patch looks reasonable. This lacks tests however: first, you're fixing a bug so you should add a test demonstrating the failure, and we have a strict 100% coverage policy.\nThanks. Thanks. This still lacks a proper color message though. . Perfect thanks (and LOL about the \"color\", I probably typed this message from my phone with a \"smart\" keyboard ...). I moved the comment explaining why the item was ignored to within the corresponding if statement while applying, and merged as 10baed43148868dd6794e7b2e65db994031550cc.. Thanks for your PR. Minor comments about the commit message:\n\n\nAvoid past tens (\"Added ...\"). We usually use imperative tone (\"Add ...\" as in \"Please, dear computer, add feature ...\")\n\n\nInclude the string \"Fixes #201\" in the commit message. This way, the issue will be closed when the PR is merged, and the issues will be cross-linked.. > I didn't quite get the .x and .y thing. That's why I focused only on the image attribute type.\n\n\nThey tell you where the mouse clicked, which only half makes sense for us because we don't have a mouse, but it would be cool to have a way to specify it.\nSee e.g. https://mdn.github.io/learning-area/html/forms/image-type-example/xy-coordinates-example.html. > > Thanks for your PR. Minor comments about the commit message:\n\n\n\nAvoid past tens (\"Added ...\"). We usually use imperative tone (\"Add ...\" as in \"Please, dear computer, add feature ...\")\nInclude the string \"Fixes #201\" in the commit message. This way, the issue will be closed when the PR is merged, and the issues will be cross-linked.\n\n\nShould I change anything here?\n\nIf you can modify your commit message (git commit --amend) to address both points and then force-push to your branch, that would be perfect.. > I ran those 2 and I now understand why it's preferable to use venv ! Didn't run into any problems during install (apart from upgrading pip because it was unable to find bdist_wheel).\nIIRC, python setup.py test does the venv things for you alternatively.\n\nRunning pytest in venv though, that pointed to 3 failures (didn't quite understand them, couldn't look thoroughly)\n\nYou get them also on Travis, ran when you submit a PR. You should see some red warnings below this message, and the details are here:\nhttps://travis-ci.org/MechanicalSoup/MechanicalSoup/builds/492922831?utm_source=github_status&utm_medium=notification. Much better indeed, but there's a new test failure in Travis-CI. Do go you also get it locally? Can you investigate? . > I believe the issue is that Requests decides what the content-type should be dynamically based on the type of object you pass it. If it is a file-like object, it uses multipart/form-data; if it is a string (as is the case here where we use an empty string to \"submit an empty file\"), it apparently uses application/x-www-form-urlencoded.\nI think it actually uses multipart/form-data whenever there is a file argument. Before the change, we were not submitting the file because it was not filled-in, but with the change, we pass \"\" as file.\nLet's look at what happens when submitting a file with curl:\n$ curl -X POST -F 'image=@/tmp/filename.txt' -F name1=value1 -F name2=value2 http://httpbin.org/post --trace-ascii -\n== Info:   Trying 52.71.234.219...\n== Info: TCP_NODELAY set\n== Info: Connected to httpbin.org (52.71.234.219) port 80 (#0)\n=> Send header, 187 bytes (0xbb)\n0000: POST /post HTTP/1.1\n0015: Host: httpbin.org\n0028: User-Agent: curl/7.61.0\n0041: Accept: */*\n004e: Content-Length: 399\n0063: Content-Type: multipart/form-data; boundary=--------------------\n00a3: ----528785e66ba5b1dc\n00b9: \n=> Send data, 399 bytes (0x18f)\n0000: --------------------------528785e66ba5b1dc\n002c: Content-Disposition: form-data; name=\"image\"; filename=\"filename\n006c: .txt\"\n0073: Content-Type: text/plain\n008d: \n008f: content.\n0099: --------------------------528785e66ba5b1dc\n00c5: Content-Disposition: form-data; name=\"name1\"\n00f3: \n00f5: value1\n00fd: --------------------------528785e66ba5b1dc\n0129: Content-Disposition: form-data; name=\"name2\"\n0157: \n0159: value2\n0161: --------------------------528785e66ba5b1dc--\n<= Recv header, 17 bytes (0x11)\n0000: HTTP/1.1 200 OK\n<= Recv header, 40 bytes (0x28)\n0000: Access-Control-Allow-Credentials: true\n<= Recv header, 32 bytes (0x20)\n0000: Access-Control-Allow-Origin: *\n<= Recv header, 32 bytes (0x20)\n0000: Content-Type: application/json\n<= Recv header, 37 bytes (0x25)\n0000: Date: Tue, 05 Mar 2019 06:50:18 GMT\n<= Recv header, 15 bytes (0xf)\n0000: Server: nginx\n<= Recv header, 21 bytes (0x15)\n0000: Content-Length: 466\n<= Recv header, 24 bytes (0x18)\n0000: Connection: keep-alive\n<= Recv header, 2 bytes (0x2)\n0000: \n<= Recv data, 466 bytes (0x1d2)\n0000: {.  \"args\": {}, .  \"data\": \"\", .  \"files\": {.    \"image\": \"conte\n0040: nt\\n\".  }, .  \"form\": {.    \"name1\": \"value1\", .    \"name2\": \"va\n0080: lue2\".  }, .  \"headers\": {.    \"Accept\": \"*/*\", .    \"Content-Le\n00c0: ngth\": \"399\", .    \"Content-Type\": \"multipart/form-data; boundar\n0100: y=------------------------528785e66ba5b1dc\", .    \"Host\": \"httpb\n0140: in.org\", .    \"User-Agent\": \"curl/7.61.0\".  }, .  \"json\": null, \n0180: .  \"origin\": \"91.68.56.209, 91.68.56.209\", .  \"url\": \"https://ht\n01c0: tpbin.org/post\".}.\n{\n  \"args\": {}, \n  \"data\": \"\", \n  \"files\": {\n    \"image\": \"content\\n\"\n  }, \n  \"form\": {\n    \"name1\": \"value1\", \n    \"name2\": \"value2\"\n  }, \n  \"headers\": {\n    \"Accept\": \"*/*\", \n    \"Content-Length\": \"399\", \n    \"Content-Type\": \"multipart/form-data; boundary=------------------------528785e66ba5b1dc\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"curl/7.61.0\"\n  }, \n  \"json\": null, \n  \"origin\": \"91.68.56.209, 91.68.56.209\", \n  \"url\": \"https://httpbin.org/post\"\n}\n== Info: Connection #0 to host httpbin.org left intact\nNote that both the filename (more precisely, the basename) and the file's content is sent. Unfortunately, the response from httpbin.org contains only the file's content (it says image=\"content\" where image is the field name in HTML, content is the content of the file, but filename.txt appears nowhere). That's bad news, because this means we can't fully test our behavior with httpbin. What would be nice would be to contribute to httpbin a feature to expose the filename in the json response.\nNow, it's not clear to me what the browser's behavior is: submit an empty string as filename, content, or both? My guess is \"both\", but I didn't check.\nTo send both a file name and content, we can't just pass a string to requests, I guess we need to pass a file-like object (doesn't need to be a file, just a dummy class that returns \"\" when queried for the filename and \"\" when queried for the content should do it).\nIn short, it's much less trivial than I anticipated.... > But if this is true, why aren't we seeing \"multipart/form-data\" in this test? Everything you've said seems to indicate that we should.\nWe are seeing it, but the assertion is written in a slightly counter-intuitive way: the header is multipart/form-data; boundary=fbfdf6eb526d8951e6cbe849ff2ffd35' and the assertion complains that is does not contain application/x-www-form-urlencoded.. I've setup a minimalist test page to see what the server gets on file upload: http://matthieu-moy.fr/tmp/2019/tmp.php\nApparently my guess was right: it submits an empty file name. According to Firefox's inspector, the portion related to pic in the request when I select no file is:\n```\n-----------------------------1565619466406037082297189337\nContent-Disposition: form-data; name=\"pic\"; filename=\"\"\nContent-Type: application/octet-stream\n-----------------------------1565619466406037082297189337\n```\nname=\"pic\" is the field's name, filename=\"\" means an empty file basename, and there's no content below.. > So the behavior still stands to be corrected. How is the debate.\n\nShould test__request be modified in that case ?\n\nI think it should, yes. One option is to change the failing assertion. The other is to remove the file input field and consider that file inputs have to be tested somewhere else.\n\nEspecially the assertion that raised the error : since we're going to upload a file even if empty, requests will define the data type as multipart/form-data, as it should be, right?\n\nYes.\n\nAs for the test, I'm leaning towards a parametrized test__request_file, but correct me if I'm wrong : it would still need to be modified in order to account for filename testing.\n\nYes, and more importantly, I'm not 100% sure that passing \"\" as file parameter works, since we need to set both the file name and content to \"\", and httpbin doesn't allow us to test that (it only gives the content). So, the code is probably OK, but I'd like to be sure it is before we merge.\n. > This works great, except for one probable bug: if you use files={name: (filename, fileobj)} and filename is the empty string (fileobj can be the empty string or a real file), it creates the content body as we want:\nIn real-life, the filename cannot be empty (well, at least on POSIX a filename can't be empty), so an empty filename should be a marker for an absence of file, and then the content should be empty too.\nSo, it's likely a bug (perhaps in httpbin rather than requests?), but it doesn't sound like a harmful one. IOW, it would be nice to get it fixed, but it shouldn't disturb us if it isn't.. > Now that I've worked through this a bit, I would say that 3. files['pic'] = (\"\", \"\") is the correct choice\nI get to the same conclusion.\n\neven though httpbin places the data in form instead of files\n\nIndeed, from your previous message I thought that the content was sent to form when it was non-empty, but the bug in httpbin happens also with empty content.\nI've made a simple form that submits to httpbin:\nhttps://matthieu-moy.fr/tmp/2019/tmp-httpbin.php\nI do get the bug when using my browser (Firefox) too.\nUnfortunately, there seem to be no obvious bug in httpbin (I was hoping for an if filename: send_to_files; else: send_to_form instead of an if filename is None somewhere):\nhttps://github.com/postmanlabs/httpbin/blob/master/httpbin/core.py#L414\nCalling:\nhttps://github.com/postmanlabs/httpbin/blob/master/httpbin/helpers.py#L171\nSo my guess is that the bug is in the underlying framework, http://flask.pocoo.org/.\n@senabIsShort: interested in investigating this a bit, and report and/or fix the bug upstream?. I can reproduce the issue with flask alone:\n```\nfrom flask import Flask, flash, request, redirect, url_for, escape\napp = Flask(name)\n@app.route('/', methods=['GET', 'POST'])\ndef upload_file():\n    if request.method == 'POST':\n        return ' '.format(escape(str(request.form) + '\\n' + str(request.files)))\n    return '''\n    <!doctype html>\n    Upload new Files\nUpload new Files\n\n\n\n\n\n\n    '''\n```\nSubmitting with only one file set in my browser gives:\nImmutableMultiDict([('normal_field', 'field_value'), ('secondfile', '')])\nImmutableMultiDict([('file', <FileStorage: 'tmp-rg1.xpi' ('application/x-xpinstall')>)])\nThe first is the form's field, the second the actual files uploaded. I see no way to distinguish between an empty text field and a non-uploaded file from the request object, so it's very likely a bug (but I'm not sure what's the intended behavior). I didn't find the source of the issue in the code.\n@senabIsShort : fixing the bug is probably too involved, but a clean bug report would be nice. Mention me (@moy) in the bug if you report it.. > I'll look into it !\n\nRegarding this PR, do you want me to work towards applying the changes discussed in here ?\n* removing the file field in test__request,\n\n* parametrizing test__request_file,\n\n* modifying brower behavior to match `files['pic'] = (\"\", \"\")`\n\n\nYes, this is the way to go. Obviously, you'll hit the \"empty file sent to the form field\" bug when you test this properly, but you can deal with it with stg like\n```\nOne would expect to find 'filename' in files, but as of writing,\nhttpbin puts it in form when the filename is empty:\nassert files['fieldname'] == \"\" or form['fieldname'] == \"\"\n```. It is best to use the output of the HTML parser, because it launches the browser on the page as seen by the parser.\nI had to fight bugs where Firefox was seeing something different from html.parser (probably because of a missing closing tag), and some input fields were seen outside a form by html.parser. Launching a browser on soup.encode() showed me directly the input field outside the form, and I could work around it. Launching the browser on the unparsed HTML wouldn't.\n. While you're at it, you can also remove 3.2 which isn't supported either. I'm not doing this to avoid a conflict for you.\nThanks,. I have no experience with virtualenv, but this doesn't work for me:\n$ python3 -m venv .virtual-py\nError: Command '['/home/moy/dev/MechanicalSoup/.virtual-py/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1\nOTOH, the command I find here does work:\n$ virtualenv -p /usr/bin/python3 my_project\n...\nInstalling setuptools, pip, wheel...done.\n$ source ./my_project/bin/activate\nIs there any reason to use python3 -m venv instead of virtualenv? My setup may well be broken, but as-is I can't confirm that your instructions are correct.. It was on a Debian oldstable (I don't have it at hand, but should be 3.4.2 according to Debian's website). I tested again on another machine and your version works.\nHowever, you're suggesting python3 -m venv for Python3 and then virtualenv -p python2 for Python2. I think it'd be better to use the same syntax (no preference on which one on my side) for both.\nAlso, the link you just gave is a good one, it would be nice to include it in your explanation (for example as a hyperlink on the first occurence of \"virtualenv\").\nThanks,. Travis-CI says \"AttributeError: 'module' object has no attribute 'param'\" for Python 2.6, 3.3 and 3.4.\nAny way to get this to work for all Python versions? Not sure whether it would work, but perhaps just adding a \"pip install --update pytest\" in .travis.yml could do the trick?. I'd like to keep a way to call the script without using the executable pytest (just for simplicity, people see a *.py file and may want to run python ./test_file.py or just ./test_file.py).\nAFAICT, calling pytest.main(__file__) would do the trick.. Actually, pytest.main(sys.argv) is better (first, recent pytest warn when you pass a string to main and my initial version didn't allow ./test_foo.py -v for example).\nSearch for better option in progress here: https://stackoverflow.com/questions/35353771/invoke-pytest-from-python-for-current-module-only. For completeness, you may want to add comments management (strip #.*), but we don't really need it for now.. Oups, and it was not even the only typo. A spell checker does help ;-).\n. Indeed, thanks for the pointers.. Note that you're potentially breaking backward compatibility for people using choose_submit(el=foo). choose_submit was introduced relatively recently so I don't think it has so many users and we can hope that nobody wrote this el=foo, so I'm OK with this.. Since open_relative actually forwards from open, the old doc was also correct. Probably we can keep the old version here to avoid a double-indirection for the user.. Probably this should mention that in addition to requests's get, we remember the soup object in the browser.. At worse, you can keep el=None as argument, and do stg like\nif el is not None:\n    print(\"el is deprecated, please use submit=... instead\")\n    if submit is not None:\n        raise ...\n    submit = el\n\nWe'd really need that if we were a 10,000,000 users lib. Probably overkill here. Actually, if someone wrote el=... somewhere in a script, then this script doesn't work but it raises a reasonable error and should be straightforward to fix.\nBackward compatibility is painful for us, but it's really annoying to run a 2-year old script and notice it doesn't work anymore.. I don't fully understand the issue, but the issue does occur: http://mechanicalsoup.readthedocs.io/en/latest/faq.html#referenceerror-weakly-referenced-object-no-longer-exists\nThe warning is essentially harmless, I saw a lot of them with tests passing (i.e. python setup.py test showed the warning and still concluded that the tests were passing) before I added the .close calls. But some of the tests check stderr, and these tests were flaky. See e.g. https://travis-ci.org/MechanicalSoup/MechanicalSoup/builds/290550991 for a failure.\nMore info here:\nhttps://github.com/requests/requests/issues/2303\nEssentially the conclusion is \"don't do anything complex in __del__\" and it seems session.close() is already too complex. But I may well have missed something.. done.. example.com was really the right thing to do here. fake.com does exist, while example.com is specifically reserved for such usage: https://tools.ietf.org/html/rfc2606 / http://example.com/.. While you're there: unnecessary braces after assert (they were needed to allow the line break).. The .dev notation is the recommandation for Python: https://www.python.org/dev/peps/pep-0440/#pre-releases\nBut semantic versionning (http://semver.org) requires -dev, and since PEP 440 allows - as an alias for ., why not go for it.\nThis should actually be x.y.z-dev.. Do we need this list(...)? If so, this deserves a comment to explain why. . Shouldn't this be [value] instead of just value?\nIf you have one <input type=\"radio\"> and another input, the code is later going to attempt a .append on this value and will crash.. Perhaps we need another dict data_radio that we can fill-in with plain data_radio[key] = value, and once we're done iterating through the form, copy from data_radio to data using setdefault(..., []).append()?. At some point, we'll need some refactoring (perhaps extracting this \"form -> data\" to a separate file/class) I guess. I never liked this function ;-).. The 2 lines above sound like a complex way to write\nassert \"pic\" not in data[key]. OK, but then I'd rather make this explicit in the code with stg like `assert data[key] is None or \"pic\" not in data[key]`. That would make it clear to the reader what the possible values are, and would avoid this `with pytest.raises(Exception):` : not really dangerous here, but it's a construct in which any buggy code that raise an exception makes the test pass... . Good.. I usually omit these internal changes from the ChangeLog. I've added \"Internal changes\" section for other releases for things we should be proud of like 100% coverage and that didn't fit elsewhere, or stuff that developers should be aware of like changes in the way to launch testsuite. But it doesn't harm to have them.. Nit: missing space after `,`. I guess flake8 will tell you when the Travis-CI build completes ;-).. Oh, and extra-space after `=`.. Style: I'd add a line break before `url=`.. Ideally the 3 instances of this would be factored into a common file. If I read correctly https://docs.pytest.org/en/latest/fixture.html#conftest-py-sharing-fixture-functions, moving it to a `conftest.py` file should do it.\n\nBut since this is meant to be temporary anyway, I don't have strong objection in keeping it as-is.. Oh, and could this become a conditional code that would actually do the pytest-httpbin thing when it works, and fall back to HttpbinWrapper if not?. Unfortunately this breaks backward compatibility for people using url_regex as a positional argument. . You'll also need to deal with failure cases (follow_link has a special case to launch a browser when the debug mode is active, which is really convenient).\nMy first version (which I didn't push) factored this logic in a find_link_or_fail method. But in the end, the code was split into too many trivial methods to my taste.. I agreed with your point about not having a download_to parameter, because this would be a feature that the end user would really want to use and hiding it within the list of parameters of follow_link made it too hard to discover.\nI disagree with this case: it's just a \"dry-run\" parameter, that asks the function to evaluate its functional part but to avoid doing side effects. I preferred calling it return_url_only to dry_run because the later doesn't say what the function is returning.\nIt's essentially a private feature, the user doesn't need to know about it (I still documented it in the public API in case someone wanted to use it, but we could also strip the doc and call it _return_url_only).. Done.\n. I agree that returning the response is nice, but we must still read the file to check that the writing happened correctly. Actually, we don't in this version, but we will in my next push.. BTW, the existing code was creating the file before the download, and then asserting that the file existed, which was the case even without the download. I'm adding one more test to check that downloading to a non-existing file works, and that downloading to an existing file overwrites the file (we may want to provide other options than overwriting, but I want to test it to see if we change the behavior later, on purpose or by accident).. Actually, there's another rather simple option: move most of the logic of follow_link (launching a browser in debug mode and special-casing elements with an href attribute) to find_link. Then find_link becomes a straightforward wrapper, and download_link remains simple too. I'll push something along these lines shortly.. > Won't this break backwards compatibility for anyone who has specified *args?\nNot really. *args is for positional arguments, so what I did is to special-case the first positional argument. A call to find_link(foo, bar, boz) was earlier setting args=(foo, bar, boz) and now link=foo, args=(bar, boz). But since I later forward (link,) + args, the result is the same.\nI can think of one case where this was not backward-compatible: find_link(None, foo), i.e. pass None as a positional argument and expect it to be forwarded as url_regex (so that foo becomes the link_text argument). I'll fix it in my next push.. I actually think that any difference between find_link and list_link's list of argument is a source of problem. Changing the first to match the second will fix #178 as a side effect.. I'm implementing the first suggestion to make filename optional. I won't do the second one at least in this PR, but I've renamed the argument to file instead of filename to leave the door open to such improvements in the future.. I first mis-read this sentence as \"the values in the form obtained to get to this page are discarded\", which is incorrect.\nPerhaps: \"Any change (select_form, or any value filled-in in the form) made to the current page before refresh is discarded\"?. I'd omit this \"works only\", since most methods of StatefulBrowser already behave like this.. We usually write docstring with imperative tone, hence s/Reloads/Reload/. A hint on why this fails would be welcome. But we can also merge this as-is and improve the message later if needed.. > I think he was referring to the fact that it won't reload the page from a StatefulBrowser.get\nYes, but so does following a relative link or submitting a form with a relative action. It's not specific to reload.\n\nI almost want to get rid of the Browser class and have everything be \"stateful\" to avoid this\n\nLet's start a separate discussion: #190. This should check the Referer field too. I'm not sure what actual browser do on refresh (probably keep the old referrer?), but I'd like to test here and make sure we don't change it later without noticing.. Cool! All good now, let's merge this. . This could be case insensitive, since at least <HTML is possible too.\n. Why not, but not sure the added logo brings much. They make the badges inconsistent with other badges on the same line.. This looks suspicious.... I think downloading a PNG is closer to the typical real-life use-case, so I'd rather stay with the current code.. Fixed, thanks.\n. Good to know, but I guess I'm too lazy to change it and the current code works well ;-).. You can probably improve the test by having more than 2 inputs with the same name, and values not sorted alphabetically, to check that the order is preserved. Maybe\n<input name=\"box\" type=\"hidden\" value=\"1\"/>\n <input checked=\"checked\" name=\"box\" type=\"checkbox\" value=\"2\"/>\n <input name=\"box\" type=\"hidden\" value=\"0\"/>. I agree that we need to test the sequence, not the set, but changing from tuple ((...)) to list ([...]) seems unnecessary (but not really harmful either, just that I like immutable things when I don't need to mutate them).. Ah, and this if not name is useless since you ignore nameless attributes in the CSS selector. Nice.. I think the comment is misplaced. It should be near the if (i==0 and not multiple) above. And actually, I think the code is a bit trickier than it needs to be. I'd just do something like\noptions = tag.select(\"option\")\n                for i, option in enumerate(options):\n                    if \"selected\" in option.attrs:\n                        values.append(option.get(\"value\", \"\"))\nand then\nelif values:\n                    data.append((name, values[-1]))\n                elif options:\n                    # Select the first option if none are selected\n                    data.append((name, options[0].get(\"value\", \"\")))\n(Untested, but should be more self-explanatory and not much longer). It makes sense, indeed. . Indeed, nice use-case for a comprehension. Better than my naive for loop.. This is weird, or at least undocumented. The docstring says that submit must be a bs4.element.Tag or a string, so there should be no need to look for something else. If the goal is to let the function run when submit is not given, then 1) this should be documented, and 2) it would make sense to give a default value to the parameter, like submit=None. And then, since the behavior is otherwise to error out when there are multiple matches, I wouldn't do a \"or\" chain, but I'd check that there's one and only one match in input[type=\"submit\"],button[type=\"submit\"]. Picking an arbitrary button/submit is very fragile and shouldn't be encouraged.. Actually, I'd say this is a sign that choose_submit does not do the right thing. It's not documented to be destructive, and shouldn't be actually.\nThis name attribute stripping is in the wrong place IMHO. We should just get rid of this del inp['name'] and ignore the corresponding elements in _request. Or did I miss anything?. Ah, the doc is here, but in the next commit. Either squash both commits together (probably makes sense), or re-split the commits properly.. That's better than the or-chain I saw in the first commit, but I think this should raise when len(inps) > 1.. A detail: avoid committing unrelated changes (moving this import line has nothing to do with the conditional inclusion of pytest-runner).. IMHO this deserves a comment like\n```\nDon't install pytest-runner on every setup.py run, just for tests.\nSee https://pypi.org/project/pytest-runner/#conditional-requirement\n``. Unfortunately, forbidding this is not only a backward-compatibility problem, but also a violation of Liskov's Substitution Principle. The base.submit()accepts abtnNameinstance ofForm, so all derived methods should also accept at least that (and possibly more), so that anything doable with aBrowseris also doable with aStatefulBrowser`.\nAlso, having the same positional argument be btnName in one class and form in another is very misleading. LGTM is right to complain here IMHO.. If I were to redo this from scratch, I'd consider using composition instead of inheritance, i.e. consider that a StatefulBrowser contains a Browser, not that it is a Browser. It'd be hard to change that without breaking backward compatibility, though.\nRenaming Browser.submit is also problematic with respect to backward compatibility. Perhaps not many people call submit on a StatefulBrowser, but everyone who wrote code before StatefulBrowser was added does call submit on a Browser.\nThere's another option: add warnings for suspicious cases, and allow the user to disable the warnings as needed. We could warn on calls to StatefulBrowser.submit, or if we want to go further, on creation of a Browser that isn't a StatefulBrowser. Probably not as a deprecation warning like \"what you're doing won't work anymore soon\", but just like \"uh, are you sure you want that?\".. The doc change is obviously a good step forward.\nYour option 2. is probably doable too. I won't have time to look at this in details soon, but if you're confident enough I trust you to do the right thing.. I'd rather keep the if ... and ... as is and turn else: into else if link: ... for readability.. Why this renaming?. Ah, OK.. Creating a temporary file is a very heavyweight way to submit an empty value. In the end, we don't send a \"file\" but it's content. It should be possible to submit an empty file without actually creating it locally. . This is unit testing an internal method. I think it would be better to have an end to end test using httpbin.. I would prefer more meaningful test name (so that one can immediately know which test is about what when skimming through a list of tests, without having to open the corresponding issue each time) and a link to the issue in the docstring, but i'm OK with this if you prefer.. This would deserve a comment, like\n```\nThe submit element is an exact duplicate of the one we're looking for, just ignore it\n``. I thinkname.x=0&name.y=0by default makes sense. Ideally, there would be a way to specifyxandyfrom MechanicalSoup, but this can wait for another PR.. Moving to a parameterizedtest__request_file` is a good argument. But as a standalone test, since we're already paying the performance price for a network round-trip (possibly local), the cost/benefit ratio of testing the user-facing logic together with it would be good.. Every commit must pass tests (bisectable history), so this change should come before the commit that actually changes the behavior.\nAbout the commit message: no line longer than 80 characters (preferably <= 72 characters) please.. suggestion\n            if set_value:. suggestion\n            if not set_value:. suggestion\n    if set_value:. On overall, this is good, but this doesn't test the filename. Unfortunately, httpbin doesn't give us this information, so we can't rely on it, but it would be nice to check that the request we send does contain filename=\"...\" somewhere. It should be doable by asking the request library the content of the request before sending it to the network. http://docs.python-requests.org/en/master/api/#requests.PreparedRequest should help.. ",
    "eoghanmurray": "And another example (unfortunately an account is needed to see it in action, but might be useful as an example of a login/download/unzip-files pattern):\nhttps://github.com/GoPixie/rdg-to-db/blob/master/download. Is the problem in trying to submit the parent form or the child form?\nThis sounds like bad/malformed HTML on a page and maybe something that could be dealt with by extracting the nested form and placing it alongside the parent form before submitting: https://www.crummy.com/software/BeautifulSoup/bs4/doc/#extract\n. I also customize user-agent to better identify my script by adding a hyperlink for a webmaster to get more info (rather than just blocking the script/ip address).  E.g. Googlebot uses: Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\nLooking at the old mechanize api, it was an ugly property:\nbr = mechanize.Browser()\nbr.set_handle_robots(False)\nbr.addheaders = [('User-agent', 'MyBot/1.0: mysite.example.com/bot_info')]\n\nI reckon this is not something that should be replicated??. MechanicalSoup currently sets e.g. 'python-requests/2.18.4' as the User Agent.  I wonder if it should send 'MechanicalSoup/0.7.0' as default instead?  I've created https://github.com/hickford/MechanicalSoup/pull/81 as a first step towards this (that pull request should be useful even if you don't agree on a 'MechanicalSoup/0.7.0' as default). That sounds good. PR: https://github.com/hickford/MechanicalSoup/pull/82. Thanks for the merge - I think python3 -m venv is the recommended way for Python3 as virtualenv has been accepted as an integratal part of Python.  Hopefully people will be able to figure out they can do virtualenv -p python3 --no-site-packages .virtual-py3 if the new way doesn't work, but I'm happy to leave the python3 -m venv way there as I hadn't heard of it til I did some research!. Hmmm do you have a version of Python3 less than 3.3?\nhttps://packaging.python.org/tutorials/installing-packages/#creating-virtual-environments\nI think the only advantage of python3 -m venv is that you can skip the pip3 install virtualenv step (I didn't realize that was a separate step as already had virtualenv installed for Python2).. ",
    "svdvonde": "It's not a business production environment (if that's what I made it sound like), but it was the production environment of a website I'm building, hosted on Red Hat OpenShift (which apparently doesn't have six version >= 1.4.0).\nThe student-administration systems of the university I'm at are not very good for contacting students. For example, it is impossible for student organisations (like mine, for example) to contact specific groups of students (it's either everyone, or no one). Even the faculties can't contact specific groups of people (e.g. \"all students of computer science\"). So I'm building a scraper to create my own mailing lists. To log in to (and navigate) the online platform, I needed a simple tool (\"not a massive library\") so that I didn't have to worry about writing everything myself in the requests library. Since I couldn't find any popular libraries that do what I want, and do it for python3, I found this and used it. it's pretty neat.\n. ",
    "oprypin": "Oh this isn't so bad with urlparse.urljoin\npython\nbrowser.submit(my_form, urlparse.urljoin(my_page.url, my_form['action']))\n. page.url is not good, because action URL is different.\nIt definitely does not always work out to just send the POST request to the same URL.\n. Indeed it works exactly the same. Thanks and sorry.\n. ",
    "simon-weber": "Thanks! I like the choice to provide arbitrary config.\nI left a quick note on that commit.\n. ",
    "Xuefeng-Zhu": "Any comment?\n. I can do it.\n. Do you just want me to put the document in README?\n. ",
    "wernight": "This project seems more active but I didn't find the equivalent for clicking links:\n```\nLook up my favorite song\nsong_link = browser.get_link('trains')\nbrowser.follow_link(song_link)\n```\nso currently requires using urllib.parse.urljoin.\n. ",
    "stdex": "Need to use:\nbrowser = mechanicalsoup.Browser(parser='html.parser')\nor\nbrowser = mechanicalsoup.Browser(parser='lxml')\n. ",
    "lawli3t": "when i try to use\nbrowser = mechanicalsoup.Browser(parser = \"lxml\")\ni get the following error message:\nTypeError: __init__() got an unexpected keyword argument 'parser'\nI also cant find the keyword parser in the constructor for the mechanicalsoup browser, so why would this work?\n. ",
    "gsora": "@hickford error is still present, from both ArchLinux's packaged version and pip3 version.\n. ",
    "wernersa": "The easy fix for this is to use the parameter soup_config:\nbrowser = mechanicalsoup.Browser(soup_config={\"features\":\"lxml\"})\n. ",
    "EvilMav": "wernersa's solution works for me (while stdex'es fails with a TypeError). I'd expect some parser (e.g. html.parser as it's readily available without extra packages) to be used by default and/or a more obvious parameter to specify the expected parser. \n. ",
    "hsiaoyi0504": "It's just because the __init__() in\nbrowser.py\ndoesn't have the parser argument. Instead, there is a argument soup_config should be settled up.\nBy the way, seems the 'lxml' are not supported in recently change check this issue\nI think this issue can be closed.\n. ",
    "dark2201": "Got the same issue even after setting:\nsoup_config={'features': 'html.parser'}\n. ",
    "hugovk": "The fix https://github.com/hickford/MechanicalSoup/commit/3d3019e9f06bee860beeaffa505d97af3735a1ab was committed on 21 February 2016.\nThe last release is 0.4.0 from 25 November 2015. https://github.com/hickford/MechanicalSoup/releases\nPlease could you release this?\n. PR https://github.com/hickford/MechanicalSoup/pull/44 fixes the Requests/3.2 failure by removing  support for that verison.\n. Good idea! \nI've found Coveralls to be a bit iffy of late on some other projects, so have been using https://codecov.io instead which looks much better.. You'll need to tell py.test to run with coverage. Have a look at its - - cov and - - cov-report switches. . ",
    "GroverChouT": "Um... We use it to save cookies, here: http://wwwsearch.sourceforge.net/mechanize/doc.html#supplying-a-cookiejar\n. Because Mechanical does not support Python3 and not have a good enough substitute, (reluctantly) So I still go with Python 2.\n. ",
    "Jajcus": "Yes, like this. I have already prepared a fix, will make a pull request in a moment.\n. From a Django-generated user change form:\n<input checked=\"checked\" id=\"id_is_active\" name=\"is_active\" type=\"checkbox\" /><label class=\"vCheckboxLabel\" for=\"id_is_active\">Active</label>\n<p class=\"help\">Designates whether this user should be treated as active. Unselect this instead of deleting accounts.</p>\nWhen this form is submitted with is_active= (like MechanicalSoup currently does), Django would handle it like the checkbox is not checked and the user would be deactivated. Firefox sends is_active=on in this case.  This is probably a Django bug (HTML spec says 'value' is required here), but MS should behave like normal browsers.\nI have a patch ready, a will make a pull request soon.\n. ",
    "franciscod": "I needed this for a project and though it could be useful to have :)\n. push forced a fixup\n. @hickford any thoughts on this PR? :grinning: \n. silly deduplication\n. @hickford comments on this? :grinning: \n. thanks :)\n. ",
    "mohan3d": "isn't html.parser built-in python ?\n. ",
    "hemberger": "Nested forms do not appear to be valid html5. See, for example, https://stackoverflow.com/a/379622. I recommend closing this issue.. Alternatively, you can use a FileCookieJar, like LWPCookieJar:\n```python\nLoad cookies from file or create new cookie file\ncookiejar = cookielib.LWPCookieJar(cookie_file)\nif os.path.exists(cookie_file):\n    cookiejar.load()\nbrowser.set_cookiejar(cookiejar)\nDo something that sets cookies here\nSave cookies to file\ncookiejar.save()\n``\n. Good catch, thanks!. I tried modifying one of the tests that use a mock browser to includeenctype=\"multipart/form-data\"and the test still passed. Can you please provide more detail about the error you are seeing (and, if possible, the website or form that you are submitting)? Thanks!. I tested that MechanicalSoup is correctly handlingenctype='multipart/form-data'` on this site while testing file upload. It has the following form:\n```html\nFile upload via POST (HTTP)\n\n\n\n<table>\n    <tbody>\n        <tr>\n            <td align=\"right\" valign=\"top\" nowrap=\"\">File to upload</td>\n            <td align=\"left\" valign=\"top\"><input type=\"file\" name=\"file_upload\" size=\"45\"></td>\n        </tr>\n\n        <tr>\n            <td align=\"right\" valign=\"top\" nowrap=\"\">Send back received data</td>\n            <td align=\"left\" valign=\"top\"><input type=\"checkbox\" name=\"info\"></td>\n        </tr>\n\n        <tr>\n            <td align=\"right\" valign=\"top\" nowrap=\"\"></td>\n            <td align=\"left\" valign=\"top\"><input id=\"button\" type=\"submit\" name=\"http_submit\" value=\"Start HTTP upload\"></td>\n        </tr>\n    </tbody>\n</table>\n\n\n```\nI was able to successfully submit the form using this script:\npython\nimport mechanicalsoup as ms\nbr = ms.StatefulBrowser(raise_on_404=True)\nbr.open('http://www.csm-testcenter.org/test?do=show&subdo=common&test=file_upload')\nform = br.select_form()\nform['file_upload'] = '/path/to/some/file'\nbr.submit_selected()\nIf you are still having issues, please don't hesitate to contact us again. We're happy to help if you can provide more details!. Upon further investigation, there are three problems here:\n1. There is a bug in the return value of Form.choose_submit where it was returning True when it hadn't found the specified element.\n\n\nForm.choose_submit only has the ability to select input element names, not input element values. Because the return value was incorrect (problem 1), I didn't notice this was happening. Note: mechanize can select input elements by value using the label argument (i.e. br.submit(label=submit_value)), so perhaps it can be added as a feature here.\n\n\nBrowser.prepare_request does not respect the btnName argument passed by StatefulBrowser.submit_selected when there are multiple submit inputs/buttons. Let me describe what happens step by step. First, StatefulBrowser.submit_selected passes data={btnName: ''} to (ultimately) Browser.prepare_request. However, as Browser.prepare_request loops over the input elements (which includes the submit inputs), it indiscriminately adds all input elements to the data dict of the POST request, including the submit inputs that were not specified. If the inputs have the same name attribute, then the last one will be submitted; if the name attributes are different, then they will all be submitted and you will likely get undefined behavior from the website (depending on how they handle the POST data). Neither result is correct.\n\n\nI will provide a PR to address these problems shortly.. Yes, it looks like it will work, but @hickford should be able to enable webhooks for Codecov if he signs in there. The webhook for the coverage reports is a bit more streamlined than these comments that it's currently posting.. Thanks! This should allow us to set up the Codecov app. I'll discuss with @moy.. I like your idea of adding the working tree to the test path, because it does solve the problem in a simple way. However, I also don't know if it's the most pythonic solution, so hopefully someone more experienced can comment.. Something like gitter.im is a nice place to leave messages that aren't important enough to be a github issue. You can also set it up so that you're only notified if someone mentions you explicitly. However, it certainly isn't necessary, and it seems like it would be an extra complication for you, which I completely understand! We can keep it in mind for the future, but I'll close this issue for now.. Does anything more need to be done with the requirements files before closing this issue?. Great! The artist is happy to do another iteration of the drawing (with fewer noodles, perhaps bigger gears, ... any other thoughts?). In the meantime, I've added it as our organization profile picture to get a sense of what it currently looks like in some of the applications that use a small version of the icon. Please let me know if this is a problem!. One thing I noticed is that we might want to use a transparent background (since the white background of the image is visible against the slightly off-white background of the organization page.. Do you want the logo used as-is and then updated when the artist reworks it, or do you want to wait until the logo is \"finalized\" first?. We do have an updated image (in svg) that addresses your suggestions, but I need to talk to the artist first before I post it here.. Glad you like it! The artist was working on an SVG version, but I think they were too unsatisfied with it to share it publicly. So do we want to keep using this informally, or include it somewhere in our docs/repo as well?. Wow @mkormendy, that's pretty amazing! I like it a lot, and it's very generous of you to donate your talents here. :)\nI'll talk with @moy next chance I get to see how he'd like to proceed.. Tested with PR #117. No Codecov PR comment was added, so I'm marking this as fixed.. As a first step, it looks good to me! I haven't verified that it renders as expected, but I assume you have ;). I think it would be nice to keep the badges in the README (whether in the Development section as before, or all of them at the top, I'm not sure). It's what a lot of people look at to quickly assess a project, so hidden under a link is to our disadvantage.. I like how the two installation methods have been put in the same section, however it is now unclear that the python versions badge only refers to the version in PyPI. I'm assuming it's not possible to automatically build and package a mechanicalsoup-dev module for PyPI (or something else that would give us a nice description of the versions supported in both the \"stable\" and \"latest\" builds)?\nEDIT: alternatively, perhaps the generic install from source in the README could specify git checkout v0.8.0?. Otherwise, it's looking fantastic!. I think the version info you have now is sufficient. We can tweak it later if we feel that it's misleading.. Regarding the duplicate documentation for the method aliases, I think we have three options:\n1. Suppress the docs by putting the methods in another file and telling Sphinx to exclude that file\n2. Suppress the docs by making the aliases part of a base class that Form inherits from\n3. Add explicit documentation for each alias stating that it's deprecated. This would change something like\ntextarea = set_textarea\ninto\ndef textarea(self, *args **kwargs):\n    \"\"\"DEPRECATED. Use :func:`~Form.set_textarea` instead.\"\"\"\n    self.set_textarea(*args, **kwargs)\n(Possibly even with a print(\"Form.textarea is deprecated. Please use Form.set_textarea instead.\") as well if we want to be super explicit about the deprecation.)\nThoughts?. For every list I write that I thought was exhaustive, you have tricks up your sleeve to add to them! :)\nYour 4) works, and I agree it seems the simplest. Thanks!. I am happy with this PR now, but would like a 2nd pair of eyes before merging since it is a fairly significant change.. The \"logging\" FAQ is really helpful. I didn't know about that functionality previously!. Both ideas seem to reasonably solve the problem without a loss of efficiency, which is great! Push/pop adds more cruft to user code\nbr.push_history()\nbr.follow_link(link)\nbr.pop_history()\nbut is a bit more elegant and flexible. I'm not sure which would be the better solution, but I like both.. Another possibility (one that I don't necessarily like, but would stay more-or-less consistent with mechanize and RB) is to implement the history as a deque, whose size you could configure with a history_size option or similar in the StatefulBrowser ctor. Then you could add back/next methods.\nAgain, not really advocating for this one, but just trying to explore all possibilities.. This seems like a good enhancement to work on for the version after 0.9.0 (1.0.0?). Do you like to use GitHub's milestones?. I ran into a possible syntactical ambiguity that will affect backwards compatibility. The current interface for StatefulBrowser.select_form is:\npython\ndef select_form(self, selector=\"form\", *args, **kwargs):\n    ...\nNow, if I want to add an nr=1 argument,\npython\ndef select_form(self, selector=\"form\", nr=1, *args, **kwargs):\n    ...\nI don't see how I can do this without breaking previously valid calls, because any argument that was previously part of *args will now get slurped into nr.\nFor example, select_form(\"form\", None, 1) would previously give args=(None, 1) but now would give nr=None, args=(1,).\nAdditional reading:\nhttps://stackoverflow.com/questions/15301999/python-2-x-default-arguments-with-args-and-kwargs\nhttps://stackoverflow.com/questions/9872824/calling-a-python-function-with-args-kwargs-and-optional-default-arguments\n\nNow, we should also consider the current interface for BeautifulSoup.select:\npython\nBeautifulSoup.select(self, selector, _candidate_generator=None, limit=None)\nwhich we call from StatefulBrowser.select_form using:\npython\nfound_forms = self.__current_page.select(selector, *args, **kwargs)\nIt seems unlikely that anyone would be using _candidate_generator, but it's possible that someone was using limit (specified as a kwarg). If we cannot avoid breaking backwards compatibility for this function interface, then I would suggest to avoid variable arguments entirely with the following:\npython\ndef select_form(self, selector=\"form\", nr=1):\n    found_forms = self.__current_page.select(selector, limit=nr)\n    ...\nThis has two benefits:\n1. It makes the backwards compatibility errors more transparent:\n    - _candidate_generator specified as a kwarg -> TypeError: select_form() got an unexpected keyword '_candidate_generator'\n    - limit specified as a kwarg -> SyntaxError: keyword argument repeated (arguably the most important case)\n    - _candidate_generator and limit both specified as args -> TypeError: select_form() takes at most 3 arguments (4 given)\n    - _candidate_generator alone specified as an arg -> this cannot be disambiguated, but its value is either going to be None or \"not an int\", so it will likely be an obviously bad value for nr.\n2. It automatically adds in the efficiency gain of specifying the limit option of BeautifulSoup.select, which doesn't make sense to specify independently of an nr option.\n. I was actually about to submit an issue for this since I noticed the FAQ was no longer relevant.\nI agree that reproducibility is very important. What if we did the following:\n Add a default argument: soup_config={'features': 'lxml'}\n Remove the parser warning suppression\nThen, if someone overrides the default soup_config, but fails to specify a parser, they will get the bs4 parser warning and can consult our FAQ to see how to deal with it.. By the way, I contacted @hickford about a week ago asking if he could include me as a contributor in PyPI, but he hasn't gotten back to me yet. We will need to ping him again if we want me to perform the next release.. I'd prefer to wait, if that's okay with you. How about we merge this PR and then set up a 0.9 release checklist in Issues?. Agreed, but we will have users that will be using https, so it's probably a good idea to test https at least once to make sure requests isn't obviously broken. :). Yep! This will always work since Form.set is now a direct wrapper around Form.set_*. (The only difference is that set can only take one name-value pair, but value can be as many elements as the set_* method allows.\nI can certainly change the test from\npython\nform.set_select({'instrument': options})\nto\npython\nbrowser['instrument'] = options\nI only chose the former because it made it more explicit that it was testing set_select. Which would you prefer?. I tried using # noqa:E501 at the end of the multi-line strings, but that didn't work for me (in fact, it only generated more warnings!). It's the individual lines of the multi-line string that flake8 is complaining about, so it seems like it would be weird -- and possibly wrong -- to have the noqa's there.. I will start logging the stalls I see so that we can try to figure out what is going on.\n\nBuild #406.5 (pypy) stalled on flake8 test of tests/test_stateful_browser.py.. I think a guard like that would be useful, but only for Travis. If I were running tests locally and httpbin was down, I'd rather see the failures than have the tests appear to hang for a while.\n\nThe reason I suspect flake8 is because (at least the last couple times I've checked), the hang occurred before the first . for a file, which is the flake8 test. We could add -v to addopts in setup.cfg for a little more verbosity.. * Build #413.5 (pypy) stalled on flake8 test of mechanicalsoup/form.py.\n\nTest was restarted, waited for a long time to start, and then passed like normal.. * Build #419.5 (pypy) stalled on the flake8 test of mechanicalsoup/browser.py.. * Build #421.6 (pypy3) stalled on the flake8 test of tests/setpath.py.. * Build #424.5 (pypy) stalled on the flake8 test of mechanicalsoup/browser.py.\n* Build #424.6 (pypy3) stalled on the flake8 test of tests/test_form.py.\nI'm noticing that this is a trend with pypy. Perhaps this is something we can report to Travis?. * Build #426.6 (pypy3) stalled on the flake8 test of tests/setpath.py.. * Build #429.5 (pypy) stalled on the flake8 test of mechanicalsoup/form.py.. I have e-mailed support@travis-ci.com and will relay anything they have to say about the issue.. * Build #433.5 (pypy) stalled on the flake8 test of mechanicalsoup/form.py.\nI will continue to monitor this, but don't see the need to report the same issue repeatedly. I'll update here if anything changes.. It's almost worth considering taking PyPy out of the Travis builds so that we don't have to babysit every build.. I used the following Dockerfile to test the speeds with python3 vs. pypy3\n```dockerfile\nFROM python:latest\nFROM pypy:latest\nRUN git clone https://github.com/MechanicalSoup/MechanicalSoup.git\nWORKDIR ./MechanicalSoup\nRUN pip install pytest-runner\nRUN pip install -r requirements.txt -r tests/requirements.txt\nCMD [\"pytest\"]\n``\nWithpython:latest, the tests completed on average in 10 seconds. However, withpypy:latest`, they completed on average in 50 seconds, and it was abundantly clear that it was the flake8 tests that were slow. So there is a very real issue here in the way that flake8 works in pypy. The fact that it actually stalls on Travis seems odd, but I don't think Travis is doing anything wrong.. MechanicalSoup interacts with HTML elements (allowing you to parse web pages, fill and submit forms, etc.) whereas Selenium simulates a fully-fledged web browser. As a result, the biggest difference between the two is that Selenium can interact with JavaScript. If you don't need JavaScript, then MechancialSoup is a simple, light-weight solution!. Some badges show up on the following page:\nhttp://mechanicalsoup.readthedocs.io/en/stable/introduction.html#installation\nI think the only reason to modify the badges on our release branches would be if a change on master would affect the stable docs badges. But since these particular badges come from PyPI, they should always be consistent with the last pip release, right?\nIf so, then I'd agree that we can close this.. Hah, yes, you're right. I had forgotten the point of the request, even though it was mine to begin with! I agree completely.. Yes, when checking the expected query string (at least with TryIt), the order was always the order of the input elements on the page. I don't know if this is set by any standard, but we're treading in the murky area of bad HTML design anyway, since repeated element names is rarely a good idea. :). I see, so for different keys, I can't imagine that the ordering matters (as you were saying), but if we use sorted to check the QSL, then won't that hide any errors with the ordering of elements with the same name?. I think that if we refactor _build_request (something I was thinking about today, and may take a stab at tomorrow), then we can pass Requests a list of tuples for data. Looking at its source code, the list of tuples will create a deterministic query string:\n```python\n    def _encode_params(data):\n        \"\"\"Encode parameters in a piece of data.\n    Will successfully encode parameters when passed as a dict or a list of\n    2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n    if parameters are supplied as a dict.\n    \"\"\"\n\n```\nThen we can do a simple equality operator and test that ordering is retained.. Superseded by #219 . A note about the pypy3 build failure:\nThe last few lines of the (very long) exception are:\nFile \"/home/travis/virtualenv/pypy3.5-5.8.0/site-packages/dateparser/__init__.py\", line 4, in <module>\n    from .date import DateDataParser\n  File \"/home/travis/virtualenv/pypy3.5-5.8.0/site-packages/dateparser/date.py\", line 8, in <module>\n    import regex as re\n  File \"/home/travis/virtualenv/pypy3.5-5.8.0/site-packages/regex.py\", line 398, in <module>\n    import _regex_core\n  File \"/home/travis/virtualenv/pypy3.5-5.8.0/site-packages/_regex_core.py\", line 21, in <module>\n    import _regex\nImportError: /home/travis/virtualenv/pypy3.5-5.8.0/site-packages/_regex.pypy3-58-x86_64-linux-gnu.so: undefined symbol: PyUnicode_FromKindAndData\nI had a look around and found some similar issues with regex and pypy:\n https://bitbucket.org/mrabarnett/mrab-regex/issues/240/unable-to-build-the-project-from-source-on\n https://bitbucket.org/mrabarnett/mrab-regex/issues/253/run-into-error-under-pypy-580\nbut I honestly can't tell if this is fixed and some module along the chain hasn't updated to a version that includes the bugfix, or if it's still an open issue.\nI would really like to use this PR, because it makes the tests run 5x faster for me, but we may need to work our way along the import chain until this is fixed. :(. I tried adding --upgrade to the install commands, but that didn't help. I would rather not have both implementations in place if we can help it (I think that would get a bit complicated), and I'm willing to keep this PR on hold while I investigate further, but I'm not even sure where to start. Should I start by creating an issue for pytest-httpbin, or perhaps go straight to regex?. With an unimaginably simple Dockerfile, I can reproduce this error with only pytest-httpbin in pypy3-5.8.0 (which is the version of pypy3 that Travis uses by default).\n```Dockerfile\nFROM pypy:3-5.8.0\nFROM pypy:3-5.9.0\nRUN pip install pytest-httpbin\nRUN pip install pytest\nCMD [\"pytest\", \"-h\"]\n``\nSome probably good news: this error is _not_ encountered with pypy3-5.9.0. I'm not sure if we would be waiting on virtualenv or Travis (or both) to provide a 5.9.0 environment.. To clarify, this is an issue withregex` alone.\n```Dockerfile\nFROM pypy:3-5.8.0\nFROM pypy:3-5.9.0\nRUN pip install regex\nENTRYPOINT [\"pypy3\"]\nwith `-c \"import regex\"` passed to `docker run` gives the expected error:\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/local/site-packages/regex.py\", line 398, in \n    import _regex_core\n  File \"/usr/local/site-packages/_regex_core.py\", line 21, in \n    import _regex\nImportError: /usr/local/site-packages/_regex.pypy3-58-x86_64-linux-gnu.so: undefined symbol: PyUnicode_FromKindAndData\n```. I've submitted a bug report with the regex module. We'll see where that takes us.\nhttps://bitbucket.org/mrabarnett/mrab-regex/issues/264/failure-to-import-regex-in-pypy3-580. The issue was too extensive for the maintainer of regex to fix, so we will need to wait until Travis-CI allows using pypy3-5.9.0 before we can consider merging this.. It's up to you! I've really appreciated using this commit locally, but it's hard for me to weigh whether it's worth limiting our tests.\nI'm guessing we shouldn't expect a useful response (if we get one at all) from Travis if we asked when pypy3-5.9.0 will be added... Unfortunately it appears that Travis is still using pypy3-5.8.0. As suggested in https://github.com/MechanicalSoup/MechanicalSoup/pull/164#issuecomment-345016081, we can disable the pypy3 tests on Travis if we feel that this PR is more important than the automatic tests. Alternatively, we could try to encourage Travis to update their pypy3 environment.. This is closed by #209, which uses the new naming scheme for Travis' pypy environments to get a newer pypy3 version.. Hello, and thanks for reporting this issue!\nI believe I have found a simple way to resolve this with a little refactoring of the private methods of the Browser class. With this change, you should be able to pass a proxies keyword to the submit and submit_selected (if using StatefulBrowser) methods.\nIt might take a day or two to get the PR merged into master, but if you want to test the change in the meantime, then you can install MechanicalSoup from my PR branch, e.g.\npip install git+https://github.com/hemberger/MechanicalSoup@issue165\nPlease let us know if you have any further questions!. ChangeLog has been updated.\nYour question is a good one. From the current version of the code, we're only gaining generality. Since request construction and sending was all done internally in submit, even without this PR we would need to change the API to allow users to modify the prepared request before sending. I don't think this should be any harder of a task after this PR is merged. However, it might never be unnecessary -- I think the idea behind requests.Sessions.request is to allow people to specify almost anything you'd need to do for both the preparing and sending in a single call.. Yep, that split sounds completely natural to me. The problem with the previous split was that it combined prepare and send, but had no way of conveniently passing any parameters to send.. Hello, and thanks for the suggestion!\nI think it would be a very good idea to add some documentation about using requests_adapters. I've never used them personally, so I'll have to do some testing, but I think the basic scheme is the following:\n```python\nimport mechanicalsoup\nfrom requests.adapters import HTTPAdapter\nbrowser = mechancialsoup.StatefulBrowser(\n    requests_adapters={'http://', HTTPAdapter(max_retries=3)}\n)\n...\ntimeout in seconds\nbrowser.submit_selected(timeout=1)\n``\nIf you want more fine control of the retry behavior, pass aurllib3.util.Retrytomax_retries` instead of an int.\nDoes this type of information help, or are you looking for something more detailed?\n. Great, I look forward to hearing if this resolves the issue.. Thanks for your interest in MechanicalSoup!\nThe return value of StatefulBrowser::follow_link is a requests.Response object. If the request is successful, then the data you want is stored in the Response.content attribute. So, to download the file amounts to writing Response.content to a file!\nI was able to download the PDF you wanted with the following minor modification to your code:\n```python\nimport mechanicalsoup\nbrowser = mechanicalsoup.StatefulBrowser()\nbrowser.open(\"http://albopretorio.comune.palermo.it/albopretorio/jsp/home.jsp?modo=info&info=servizi.jsp&ARECOD=70&SERCOD=-1&sportello=albopretorio\")\nbrowser.follow_link(\"TD=60\")\nbrowser.follow_link(\"TD=2013\")\nbrowser.follow_link(\"row=0\")\nresponse = browser.follow_link(\"idx=0\")\nwith open('your_filename_here.pdf', 'wb') as f:\n  f.write(response.content)\n```. @aborruso Glad we could help!\n@moy Oh, you know what, a browser.download method would actually resolve my personal need for a back button in #127. Presumably, if browser.download followed a link and saved the result, it would not need to (or want to) update the state of the browser to the followed link (which in many cases would be None anyway). You would stay on the current page, much like when you download something from a real web browser. Does that seem reasonable?. Thanks for the contribution @Harapuia, this looks great! It seems odd to me that self._finalize could be constructed without error when weakref.finalize doesn't exist, but I'm glad that self.close() works in these versions!. This is my latest best attempt, where I use conftest.py as you suggest.\nI tried a few things to make this fixture conditional on pytest_httpbin not being installed. The most obvious and simplest was to replace the contents of conftest.py with:\npython\ntry:\n    import pytest_httpbin\nexcept ImportError:\n    @pytest.fixture\n    def httpbin():\n        from utils import HttpbinRemote\n        return HttpbinRemote()\nUnfortunately this gives a pretty messy error culminating in the line\nINTERNALERROR> _pytest.vendored_packages.pluggy.PluginValidationError: unknown hook 'pytest_httpbin' in plugin <module 'conftest' (<_pytest.assertion.rewrite.AssertionRewritingHook object at 0x7f0b4d92ee10>)>\nHowever, if I move this code back to where it originally was at the top of each test_*.py file, then this try-except trick works (apart from the warning about pytest_httpbin being unused -- that's probably an easy fix though). Or this could be moved into a free function in utils.py that we call in each of these files.\nBut perhaps the least intrusive option is to keep it as is now, with no automatic detection of pytest_httpbin. And then if we want to use pytest_httpbin, we simply delete conftest.py. Thoughts?. Hi Andreas,\nThanks for the kind words! Though our thanks must be extended to M. Hickford who was the original author of this library.\nFrom the documentation of StatefulBrowser.follow_link,\n\nIf link doesn't have a href-attribute or is None, treat\nlink as a url_regex and look it up with :func:find_link.\nAny additional arguments specified are forwarded to this function.\n\nI think this is a bit unclear, but what it really means is this:\nIf the link argument given to follow_link is not a bs4 tag, then pass it as the url_regex argument to find_link.\nTherefore, in your call to follow_link:\nresponse = self.browser.follow_link(url_regex='register/PAT_.*VIEW=pdf', headers={'Referer': result.url})\nyou're essentially making this call to find_link (since link=None):\nresponse = self.browser.find_link(url_regex=None, url_regex='register/PAT_.*VIEW=pdf', headers={'Referer': result.url})\nTo fix your error, you can simply omit url_regex= or change it to link= in your call to follow_link:\nresponse = self.browser.follow_link('register/PAT_.*VIEW=pdf', headers={'Referer': result.url})\nI'm not particularly fond of how all the argument forwarding is set up, and it certainly makes it more difficult to use correctly. At the very least we can be more explicit in the documentation, but perhaps there's also some way to refactor these methods without impacting backwards compatibility too much.\nPlease let us know if this resolves your problem!. I'm surprised (but glad!) that workaround succeeds but my suggestions don't. I think it's clear that I need to do some more digging, so I'll try to come up with a unit test that demonstrates this error. From there it should be pretty simple to suss out what is going on.\nThanks for reporting the issue, and I hope to have more to say soon!. Here is a discussion from Requests that describes why they don't set the referrer header: https://github.com/requests/requests/issues/2079\nI think with MechanicalSoup, which in some sense emulates a \"dumb\" browser better than Requests, we could reasonably assume that StatefulBrowser.follow_link and StatefulBrowser.open_relative should set the referrer header. Whereas anything that is more directly a wrapper around Requests probably shouldn't (e.g. StatefulBrowser.open, Browser.post, etc.).. @giorgiosironi This is now included in the 0.10 release. Cheers!. Thanks for the bug report!\nJust to clarify, you're getting a KeyError: 'name'?\nIf so, would you be satisfied with something like:\npython\nif inp == submit or ('name' in inp and inp['name'] == submit):. Do you mean you don't like download_link as the name? If so, I agree it's not the best, but I think it clearly conveys to the user what it does. One alternative is to adapt the right-click \"Save link as...\" wording that all browsers seem to use as the option to download the contents of a link.\nThe reason I prefer using a separate method is because I think it would be unintuitive to have an argument that completely changes the behavior of follow_link. Of course there will always be arguments in any code that can only be understood by studying the documentation, but I bet we'll get a lot more \"How do I download a link?\" questions if the option is tucked away as an argument to follow_link than if it's its own method.. To answer your question, while save+restore state is simple and easy to maintain, I think your arguments against it are sound, and the implementation you've offered is more technically correct. I'm happy to go with yours!. I can't really tell, but would the stream=True parameter avoid making extra copies of response.content by allowing us to stream directly to the file, or would it just delay it? Does it matter either way?\nSee http://docs.python-requests.org/en/master/user/advanced/#body-content-workflow.\n  . This looks good to go for me!. Okay, thanks for the feedback! I'll get to those changes soon.\nI've always liked getters, because they provide a natural way to group properties at a functional level (and if I want to see them all, I just type .get<Tab> in ipython), but I suppose you're right, as usual, that @property is the way to go. :). I would be happy to do a 0.9.1 or 0.10.0 release (or post5, if we're only cherry-picking a small change). I'll coordinate with @moy and report back.. Well, all requests are fundamentally a Requests.PreparedRequest object, which you can access from response.request. We could potentially store the response (or the response.request) in the browser's state, and then I believe StatefulBrowser.refresh would simply be:\npython\ndef refresh(self):\n    self.session.send(self._state.response.request)\n(with some extra infrastructure to check the result, store the new state, etc.)\nUnless there's something I'm missing here, this seems like a pretty easy (and very useful) enhancement.. That's pretty much exactly what I envisioned. I hope it works as expected!\nI would be happy with this implementation, but I'd be curious to hear @moy's opinion, since he is often perceptive to things that I might overlook.. > Most people who want to use MechanicalSoup are already using it. I don't think anyone is holding it's breath until the 1.0 release.\nI was originally going to concede this point, but then I remembered that just a few months ago we increased our visibility by nearly 50%. And that was without us even trying! :D \nI think there are actually three very large factions of people who might use MechanicalSoup, but aren't yet:\n1. People using Mechanize and are holding out hope that it will be available in python 3 before python 2 support is dropped. This is quite possibly a very large number of people.\n2. People using RoboBrowser, who might switch to MechanicalSoup if that project is never revived (it has an ever-increasing issues list -- not everyone will want to fork and fix it, so they may look for an alternative).\n3. People who have not used any Python web library yet, but may in the future (there are thousands of new coders being born every day).\n\nIt's already too late to do major backward compatibility breaks.\n\nI think it's not too late because this is explicitly how Semantic Versioning defines version 1.0.0. I also believe that we can do this in a way that doesn't inconvenience any established use of MechanicalSoup more than any other version update. It would have the biggest impact on someone making extensive use of Browser, but that class isn't really changing anyway. Development has been almost singularly focused on Form and StatefulBrowser.\nAt the very worst, I think we could say \"If you exclusively use the version 0.x.x Browser, then do not upgrade to version 1.0.0, unless you are interested in taking advantage of the features that were previously in StatefulBrowser\".\nIf we're really considering an infrastructural overhaul (which I think we should consider seriously), then I would recommend we release 0.10.0 soon, and keep the dev branch at 1.0.0-dev.. First and foremost: I believe this can be done without breaking backwards compatibility.\nThat said, I think the motivations for making this change are sufficiently important that it is worth breaking backwards compatibility if needed. I don't think Tim is alone in looking at this tool and wanting not to use it because of the confusing design (we understand the rationale, but it is confusing nonetheless). It is a disservice to all future users (and, I would guess, some disgruntled present users) to forego meaningful design change to accommodate users who believe their code should \"just work\" regardless of what version they're using (are there even any such users?). Furthermore, if anyone is using the Browser class today, my guess is that they either a) didn't realize StatefulBrowser existed or b) haven't updated since pre-0.7.0.\nI understand that API-breaking changes should not be made carelessly, but if users don't want to be bothered with ever changing their code, then they should keep the major version fixed in their installation. It may sound cavalier, but that is explicitly the purpose of the major version number. An extreme example is Python 3, but they decided to move forward even though the ramifications were much more serious.\nMechanicalSoup is the StatefulBrowser (and @moy should be proud that he promoted MechanicalSoup from a wrapper to a fully-fledged tool!). The continued existence of the Browser class only serves to distract from its true purpose.\nHere's roughly what I'm thinking:\n Merge stateful_browser.py and browser.py.\n Possibly make the original Browser methods (request, post, get, submit) private and have undocumented aliases to the old names.\n Add a StatefulBrowser class alias for Browser.\n Unify the documentation around the StatefulBrowser design.\nFor StatefulBrowser users, no changes needed, but to \"upgrade\" simply means a s/StatefulBrowser/Browser/g. Even switching to properties, for example, has a bigger effect on the API.. Thanks for your work on this @krlk89!\nMy experience with encodings is limited, but based on your report I would recommend we do the following:\n Add an FAQ that shows how to change the encoding.\n Add a test to verify that the input encoding is not clobbered by MechanicalSoup (as you said, we can't control how a server will handle it).\nIf you'd like to continue working on this, please feel free to; otherwise, I will get to it when I have some more free time.. @krlk89 Thanks for your contribution!. I'm going to give @moy a few days to review this if he wants, and then I'll merge it into master. Thanks again for your contribution!. Thanks for the fix!. Thanks for your contribution, @blackwind!\nAccording to the HTML standard, it does indeed look like type defaults to \"submit\" if the type is unspecified or an invalid value is specified.\nUnless someone else gets to it first, I'll write a few tests for these cases and see what changes are necessary. I think we probably want to avoid having choose_submit called by default, but other than that the patch seem fine to me (once we get the tests passing).. Hi @blackwind, my apologies for the delay. I plan to look at this again soon before we finalize the next release.. So to reiterate: the standard says that if type is button or reset, it is not treated like a submit element; otherwise (if type is submit, unspecified, or not a valid value) it is treated as a submit element.\nHowever, w3schools says that in practice this is not followed:\n\nTip: Always specify the type attribute for a <button> element. Different browsers use different default types for the <button> element.\n\nI'm going to assume that we'll just stick to the standard here, unless there are any objections.. I finally got around to adding the tests and fixing some of the issues that were causing this PR to fail existing tests. Sorry for the delay!\n@blackwind Handling typeless buttons is part of the standard, so we're all good there! I was referring to:\n\nDifferent browsers use different default types for the <button> element.\n\nBut I don't know if that's true, and I don't know if that matters, since the standard says explicitly what to do (and this PR follows the standard). I'm happy with the result if you are.\n@moy If you have a spare minute to look at my fixup commit message, there are a few changes that may or may not warrant comments in the changelog. Would appreciate your opinion here, if you're happy with the patch otherwise. Thanks!. I agree that the existing design is not ideal. We could in theory process the soup object in Form.choose_submit as we do now, and then instead of deleting any non-matching submits, just store the matching submit. Then, when Browser.submit is called, we could call a new function Form.delete_unused_submits or similar before extracting the soup object to pass to Browser._request. I did consider this, but I didn't want to add a reprocessing layer for a feature that has never been requested (Form.choose_submit has always been destructive in a way that prevents calling it twice -- it seemed helpful to at least document this with a clearer error message).\nUnless there is a really compelling suggestion, my recommendation would be to evaluate the PR as-is and then consider refactoring Form.choose_submit as a separate task.\nAs for commit organization, I'd like to keep the original commit intact if you don't mind too much. Were they only my commits, I'd certainly agree to squash, but I don't think it muddies up the history too much here, and it's nice to recognize blackwind's contribution. I usually just look at the \"Files Changed\" tab to see the overall effect of the PR, and then only delve into the individual commits if something isn't clear there. Would that be okay?. @moy I'm happy to merge this PR as-is, but I do want to address your comments in case there is room for improvement. A few points:\n I don't think there are any significant edge-cases in this code that didn't exist before, but previously they were undocumented and not explicitly handled (you would just get some indirect error, which would require you to go into the source code to figure out what it was doing). With this PR, all these edge cases are now tested too, at the expense of a few more lines of code.\n The name attribute stripping occurs in Form.choose_submit instead of Browser._request because there is no interface at the Browser-level to specify which submit to choose. If we want to change the Browser interface, I agree it would allow us to simplify the complicated (and, I agree, not ideal) layout of Form.choose_submit, but I think it would be a non-trivial backwards incompatibility.\nSo I see three paths forward:\n1. Accept the flaws in the current code and merge this PR.\n2. Merge this PR, but plan to modify Form and Browser to simplify choosing submits at some unspecified time in the future.\n3. Decide on a plan to modify Form and Browser now and abandon this PR.\nMy preferences are (in order) 2, 1, 3. What do you think?. It's odd that this passed tests here, but now that it's merged it doesn't anymore.... I've got a simple fix, which I'll submit as PR now.. It's actually not too bad, but here's the Dockerfile I was able to get working with alpine linux:\n```\nFROM python:3-alpine\nRUN apk add --no-cache g++ libxml2 libxslt-dev \\\n    && pip install mechanicalsoup \\\n    && apk del g++\n```\nThis gets you a 130MB image (whereas the full python images are usually 600-700MB).. I believe I've now enabled the pull request integration for LGTM.\nHow would you like to resolve this alert? I see two possibilities:\n1. Remove the Browser.launch_browser override\n2. Remove the soup argument from StatefulBrowser.launch_browser\nI think the 2nd option is probably the most backwards compatible, and should result in the exact same functionality that we have now.. This SO answer suggests that LSP is violated. I assume this is formally because you can't verify that the method arguments are contravariant if the signature is different. I don't understand the comment about \"external code\" though.\nIf we think about this in a more strict language, like c++, what is currently implemented by launch_browser works correctly (as it does in python):\n```c++\ninclude \nusing namespace std;\nclass A {\n    public:\n        void print(int x) { cout << \"x=\" << x; }\n};\nclass B: public A {\n    public:\n        void print(int x, int y=10) { cout << \"x,y=\" << x << \",\" << y; }\n};\nint main() {\n    B b = B();\n    b.print(20);\nreturn 0;\n\n}\n``\nThis outputsx,y=20,10as expected. However, if we makeA::printpure virtual, or if we markB::printwith theoverridekeyword, then we get a compiler error since it's not actually overridingA::print. Nevertheless, we are _hiding_A::print` since there is no way to access it without casting to the base class.\nDoes this mean that LSP is violated when base class methods are hidden? A quick google search seems to indicate that the answer is \"yes\", but the explanations seem a bit more subtle (e.g. \"LSP, but not at the syntactical level\"...).. I've deactivated pull request integrations for LGTM. I'll close this issue in a few days if there's no further discussion.. I had assumed (perhaps incorrectly) that having the PR hook enabled without addressing the error would result in a  for every PR, which would certainly be counterproductive. I have no objection to re-enabling if it doesn't do that.\n. Just checking in on this again, I see that there are no LGTM alerts for MechanicalSoup, so perhaps they fixed whatever issue was causing the warning we were discussing earlier (and I regret that I never copied the text of the warning here for reference!).\nI've re-enabled the PR checking, and I'll close this ticket once I submit a PR to include the LGTM badge somewhere.. @samlanning Thanks for the confirmation! I'm looking forward to seeing what we learn by using LGTM!. Thanks for reporting this issue!\n\nThe built-in exception classes can be subclassed to define new exceptions; programmers are encouraged to derive new exceptions from the Exception class or one of its subclasses, and not from BaseException.\n\nIndeed, it does seem like we are doing something against the recommended pattern. My guess is that it was done this way to avoid breaking backwards compatibility (LinkNotFoundError was not in the original MechanicalSoup library), since users could in theory have had a try-except that would otherwise trigger the except when LinkNotFoundError was added.\nI'm in favor of breaking backwards compatibility sometimes to better adhere to standards, but I know that @moy often disagrees with me about this. Since he originally implemented LinkNotFoundError, I'd want to hear from him before changing anything.. I'm not sure that inheriting from both classes has any benefit. There are two issues that I see:\n\n\nIf you are expecting a BaseException, then letting LinkNotFoundError inherit only from Exception still achieves that because Exception inherits from BaseException.\n\n\nIf you are NOT expecting an Exception, then letting LinkNotFoundError inherit from Exception (instead of or in addition to BaseException) still breaks backwards compatibility. The following code would behave differently after such a change:\npython\ntry:\n    raise LinkNotFoundError\nexcept Exception as e:\n    # do something\nThis is likely a pathological scenario, but at the very least would need to be documented if we break it.\n\n\n. There are a lot of ways that you can raise a LinkNotFoundError in MechanicalSoup, so it's possible that another line is causing the problem here. If you can provide the error message you're getting (or better yet, some reproducible code that we can run, as per @moy's suggestion), then we will be better equipped to help diagnose the issue.. We're glad you were able to find a solution to your issue! I'm surprised that allow_redirects=False didn't work for you, since that option goes directly into the Requests library.. The download_link method would probably be more self-evident if there was also a download method that just saved the contents of a URL to file. I'm not sure if it should be MechanicalSoup's job to implement such a function, but if you think it should, let us know!. Thanks for the bug report! I believe this would be solved by #160, but there were some complications that prevented us from immediately merging it. I'll try to get back to this soon.. @zen4ever Coming back to this, I realized that we already have a way to achieve what you need, since we allow a bs4.element.Tag as the input to Form.choose_submit. Here's how you would do it:\nLet's say your form looks like this:\nhtml\n<form>\n  <input type=\"submit\" name=\"action\" value=\"continue\" />\n  <input type=\"submit\" name=\"action\" value=\"cancel\" />\n</form>\nThen you can uniquely select between the first and second submit inputs with:\n```python\nform = browser.get_current_form()\nThe form attribute of the Form class is a bs4.element.Tag,\nso we can use a CSS selector on it!\nsubmit = form.form.select('input[value=\"cancel\"]')[0]\nform.choose_submit(submit)\n```\nWhile I agree that it would be nice to have a CSS selector option for choose_submit, it might over-complicate the interface to add a new selector argument (since submit probably has to stay, for backwards compatibility). Same goes for an nr=1 argument, to a lesser extent.\nGiven this example, is your issue resolved? If so, please close this. Thanks!\n. I've fixed the tests. The failures were caused by network issues with httpbin.org. Since the Travis pypy environments now work with pytest-httpbin, I've merged that in and rebased this PR.\nAs for a warning when no action attribute is detected, that could get annoying in cases where there is intentionally no action (since the HTML standard allows it to be omitted if the action is the current page). I'm not sure how to detect JavaScript interference without false positives, but if you have any ideas that certainly sounds useful.. Thanks for your contribution, @sbraz!. I see only three tests that fail when I disconnect my internet:\ntests/test_browser.py::test__request FAILED\ntests/test_browser.py::test__request_file FAILED\ntests/test_stateful_browser.py::test_new_control FAILED\nIt should be fairly simple to fix these, so I'll take a look at it now.. Indeed! Thanks for reporting the issue.. I can make time to do a release next week, but I'll have to coordinate with @moy first. (@moy: I sent you a message on Gitter, but we can coordinate wherever is most convenient for you :). I see the cause of the coverage loss. I'll fix this and resubmit shortly.. Hi, thanks for your question!\nCould you describe what you're using to check to see what the content of the textarea is? Are you checking it locally, or viewing the response from the remote server?\nDoing just a simple test with:\npython\nform['textarea_name'] = 'some\\ntext'\nform.print_summary()\nseemed to give me the newline I expected. I would have only expected a literal \\n if you used \\\\n to escape the backslash.\nAnother option to try in the meantime might be:\npython\nform['textarea_name'] = \"\"\"some\ntext\"\"\"\nform.print_summary()\nAnyway, let us know!. Does removing this prevent us from running tests with setup.py (e.g. python setup.py test)? Would it work to move pytest-runner into the tests/requirements.txt file? What we currently do is explicitly recommended by pytest, so I would like to understand this change a bit more before merging.. @graingert I was planning to upload wheels for the next release, which should happen asap. Is that sufficient, or do you also need the wheels for the currently released version?. @graingert I've uploaded wheels for v0.10.0 to pypi. I probably won't do a .post0 release (unless there are other blocking issues I don't know about), but I will definitely include #224 in v0.11.0.. Since the btnName argument of StatefulBrowser.submit_selected is just forwarded to Form.choose_submit, a bs4.element.Tag can be passed to either method.\nTo answer the original question explicitly, you can do something like this:\npython\nbr = mechanicalsoup.StatefulBrowser()\nbr.open(some_url)\nsave_button = br.get_current_page().find('input', id='edit-save')\nbr.submit_selected(btnName=save_button)\nUnfortunately, changing btnName to a more generic name would break backwards-compatibility, which is what I'm assuming is the reason it wasn't changed when Form.choose_submit got the ability to take a bs4.element.Tag.\nWould it be useful to have an FAQ about this type of operation?. I like your idea of making it a warning. Perhaps it could say something like this?\n\n\"Warning! Form must be passed a bs4.element.Tag that is a <form> at the root level, but it was passed a {}. This will be a LinkNotFoundError in a future version.\".format(form.name)\n\n... or whatever the correct terminology is.... Thanks for your contribution! We're really excited to have such a cool logo for MechanicalSoup.\nI have a few comments now that I see it in context:\n1. The logo text looks a bit faded in contrast to the stark black text of the README. I'm not sure if or how that should be changed, but my eye was drawn to it. I also find it hard to read the tagline. (See image below.)\n2. Now that the text isn't stylized, it looks a bit simple and out of place, especially when surrounded by similarly simple sans serif fonts. Is there a more stylized font we could use -- maybe one that looks a bit more... mechanical? :P\n3. Is it necessary to have a \"MechanicalSoup\" header followed immediately by a logo with the name of the module? Probably yes, in case there is anyone not loading images, but it feels a bit redundant, and might be nice to remove if we could so that the logo could speak for us!\n\n. I wanted to play with fonts a little bit first. I think I can do what I want to do by the end of Tuesday, if that's okay.. Now I know this looks terrible, but I found a cool font that I wanted to try out before we commit to the current design -- just try not to judge it by my poor execution. ;)\nI know it's a little busy, and definitely less professional, but I think it's more fun. I could go either way!\n\n. Agreed about the level of detail. Your plan sounds good to me!\nThe font is called Steamy. I'm attaching the font files here should they ever be inaccessible from the online font repositories for some reason: steamy.zip\n. People fork our code all the time. They modify it according to their needs and desires.\nOur intention here is not to brand MechanicalSoup, so the decision process doesn't need to be so academic. We're trying to have a little fun with it.\n. When you submitted your PR, you checked an \"Allow edits from maintainers\" box, and nowhere did you express your desire for exclusive rights to the design decision. Nevertheless, I had no intention of using a modified version of your design without your permission.\n@moy I would prefer to table this for now. If you feel super strongly that this represents your vision, then merge it -- I defer to you as the primary contributor to the project. My recommendation is to wait and re-evaluate our options at a later time.. Thanks for your understanding, @mkormendy.\n@moy: Unless I hear otherwise, I'll leave it to you to merge this, since you probably have a better idea of how you want to merge in your commits.. Hi, thanks for your interest in MechanicalSoup!\nI believe the issue is that you called the base class method submit instead of the preferred method submit_selected. That is, instead of\nb.submit(form)\nuse\nb.submit_selected()\nHowever, I recognize that the base class method does not make much sense in the context of using StatefulBrowser, so I will try to make StatefulBrowser.submit a synonym for StatefulBrowser.submit_selected in an upcoming release.. This pull request introduces 1 alert when merging f681e7ba1df5515572f7d8bb351a8dca79ee66da into 730ca4a5948a6573994da24c6da26df82da533b6 - view on LGTM.com\nnew alerts:\n\n1 for Mismatch between signature and use of an overridden method\n\n\nComment posted by LGTM.com. Here's a free favicon that could be the basis for an alternative:\nhttps://www.freefavicon.com/freefavicons/objects/iconinfo/gear-tools-152-192629.html\nPerhaps something like this in a bowl, but looking at it edge-on like the old logo.. My mistake -- it was the inclusion of docs/ that doubled the size of the PyPI tarball, not assets/ (see #217). So I think we can ignore that point.. Have you looked at the tutorial or any of our examples? This one might be useful to you: https://github.com/MechanicalSoup/MechanicalSoup/blob/master/examples/expl_google.py\nIf you have <input ... /> elements but no <form>...</form>, then you are likely dealing with a site that uses javascript, in which case, MechanicalSoup probably isn't the right tool to use here. Please refer to our FAQ about this.. Thanks for submitting this issue! It certainly seems like we should be able to respect the form enctype, assuming that requests allows it as part of their API. If you'd like to submit a pull request, we'd be happy to work with you on it; otherwise, we'll look into this as soon as possible.. Reading this issue\nIt starts to sound like haiku\nMore details needed. Since this report seems to have become inactive, I'm going to close it for now. Feel free to re-open if you can provide more information.. Hi, and thanks for reporting this issue!\nI think there are two questions here:\n1. What is the standard for encoding form data in a query string?\n2. What is the standard for parsing the query string?\nBelow are some potentially useful references.\nWikipedia: URL Normalization:\n\nHowever, the order of parameters in a URL may be significant (this is not defined by the standard)\n\nWikipedia: Query String:\n\nThe order of the queries doesn't matter (name=ferret&color=purple and color=purple&name=ferret both produce the same results) and the exact structure of the query string is not standardized.\n\nThe URI RFC seems to corroborate that there is no standard for parsing the query string.\nRegarding form content encoding, W3C seems to be a bit more explicit about it:\n\nThe control names/values are listed in the order they appear in the document. The name is separated from the value by '=' and name/value pairs are separated from each other by '&'.\n\nThe bs4.Tag.element.select method that we use in Browser._requestseems to determine the order of results based on the order of comma-delimited selectors. Our selector is [input, button, textarea, select], and it looks like they are correctly ordered within each of those 4 categories, but not correct overall.\ntl;dr: I think you are right, and we should probably find a way to order them correctly. Your suggestion seems sensible, but someone will need to write a test case. I'll do it when I can if no one else volunteers. ;)\n. Just an update on this issue: if you update to BeautifulSoup 4.7.0+, this issue will be fixed (i.e. the request will be page-ordered). See #257 for more details.\nI wonder if we should make BeautifulSoup 4.7.0 the minimum requirement for MechanicalSoup?. I'm sorry I haven't had the chance to review this again, but I hope to take a look at it again soon. If I'm holding things up, please go ahead and merge. Otherwise I'd like to think about this some more. Thanks!. Ah, I see I missed the boat here. Sorry for being so slow about this. :(. That's cute :)\nI noticed you made a bunch of little easter egg PR's. What inspired them?. Indeed, it looks like we're not dealing with this appropriately in MechanicalSoup. It will take some work to make us compliant with all the details of disabled attributes, but I think as a first pass we can easily add it into the Browser._request method when we process the form data.\nIf you'd like to submit a PR, we would happily welcome it and can provide any guidance that you need. Otherwise, we'll get to this as soon as we can.\nThanks for submitting this issue!. The change that would make the most sense to me is to simply not submit inputs that have the disabled attribute (just like the browser does), but to continue to allow that attribute to be removed by manipulating the Form object (as @moy suggests).\nIt would not be backwards compatible (e.g. if you were relying on submitting disabled inputs by default, then you would need to change your code to remove the disabled attribute), but the break seems justified since we are quite incorrectly deviating from the standard here.\nFor example, to enable all disabled inputs, you might write something like:\npython\nform = br.get_current_form()\nfor item in form.form.find_all(\"input\"):\n    if \"disabled\" in item.attrs:\n        del item.attrs[\"disabled\"]\nWould it be useful to have a function that could add/remove the disabled attribute from an input, or is this simple enough?\n. Thanks for the PR @tiboroche! I did a tiny bit of cleanup and then added a little more documentation in a fixup commit.\nI would suggest that we squash these commits together and use my commit message for the final version.. Hi, and thanks for the report!\nSince calls to StatefulBrowser.open are wrappers around requests.Session().get, this is likely an issue stemming from memory leaks in the requests library. I see a few unresolved tickets over there (e.g. https://github.com/requests/requests/issues/4191 and https://github.com/requests/requests/issues/4601).\nIf we can isolate the problem in requests, I see a few possible results:\n1. We help requests track down the bug and fix it.\n2. We implement something in MechanicalSoup to circumvent the issue.\n3. You implement something in your script to circumvent the issue.\nWithout understanding the cause of the leak yet, I'm not sure how we would do 2 or 3, but I'll try to reproduce your issue to get the ball rolling.\n. At first glance, that summary doesn't look very helpful for diagnosing where the memory leak is, but clearly there is something that seems to be growing without bound. My suspicion is that we'll find the leak in the requests Session, but it'll be nice to pin it down. Thanks for looking into this!. Hi, and thanks for the pull request!\nSo if I understand this correctly, the case you want to fix is this:\npython\nbr.find_link(None, url_regex=\"/something\")\nHowever, according to the docstring of find_link():\n\nIf link doesn't have a href-attribute or is None, treat link as a url_regex\n\nIt looks like the intention is to not specify a url_regex as one of the kwargs of find_link(), but to instead overload the link argument for that purpose (as a convenience?).\nI think your patch makes sense on its own, but I'm not 100% sure what the intended behavior of find_link() is. I see two possible ways to go forward:\n1. Change the docstring to be consistent with your new behavior\n2. Keep the current behavior, but add better argument validation to avoid unexpected results. By the way, the test failure is a separate issue. I just updated all my pip packages locally and I'm now seeing the same issue without this patch. (Looks like it's a list ordering issue...). I agree with the change (but to clarify, it would grab the first link, not a random one).\nThis should have been caught by test_link_arg_regex, but the test happened to be passing accidentally (see #261).\nPlease go ahead and make any requested changes, and then I think this is ready to be merged. It would be worth checking that the code passes the improved tests in #261, but I wouldn't worry about that yourself.\n. @tiboroche #261 has been resolved, so please rebase your branch against master when updating and remove the pytest.mark.xfail mark from test_follow_link_arg in tests/test_stateful_browser.py (since your patch should now fix that test!).\nThanks!. Confirmed that it's only an ordering issue. If I replace\npython\nassert (query == expected)\nwith\npython\nassert (sorted(query) == sorted(expected))\nthen all the tests pass.\nThis change was precipitated by a new implementation of CSS selectors in BeautifulSoup4 in version 4.7:\n\nBeautiful Soup's CSS Selector implementation has been replaced by a\n  dependency on Isaac Muse's SoupSieve project (the soupsieve package\n  on PyPI). The good news is that SoupSieve has a much more robust and\n  complete implementation of CSS selectors, resolving a large number\n  of longstanding issues. The bad news is that from this point onward,\n  SoupSieve must be installed if you want to use the select() method.\nYou don't have to change anything lf you installed Beautiful Soup\n  through pip (SoupSieve will be automatically installed when you\n  upgrade Beautiful Soup) or if you don't use CSS selectors from\n  within Beautiful Soup.\n\nI am having trouble finding a reference, but I believe that one of the longstanding issues was the way that the original bs4 implementation returned results (they weren't ordered by location on the page -- I think).\nThis is consistent with the following test. Given the following HTML:\nhtml\n<table></table>\n<p></p>\n<div></div>\n<span></span>\nResult with bs4 4.7.1 (ordered by page elements):\n```python\n\n\n\nbs4.BeautifulSoup(html).select(\"span, div, p, table\")\n[, , , ]\nResult with bs4 4.6.0 (ordered by CSS selector terms):python\nbs4.BeautifulSoup(html).select(\"span, div, p, table\")\n[, , , ]\n```\n\n\n\nSo I think this is great news for us overall, since browsers order their requests in the same way as bs4 4.7! But it leaves us with the following question:\n* How do we modify the test?\nMy recommendation is to reorder the expected result so that it is page-ordered, and then perhaps check the result with something like\npython\nfrom distutils.version import StrictVersion\nif StrictVersion(bs4.__version__) >= StrictVersion('4.7.0'):\n    assert query == expected\nelse:\n    assert sorted(query) == sorted(expected)\nThis will keep the test as stringent as possible while still permitting backwards compatibility for anyone running tests with an older bs4.. @facelessuser Hi, thanks for the confirmation and for your work on SoupSieve! So far the changes to CSS selectors brought in with bs4 4.7.0+ have been a great boon to us, resolving a longstanding consistency issue between MechanicalSoup and real browser behavior. If we run into any related issues going forward, I'll make sure the feedback reaches you.. Aha, I had forgotten about pytest.mark.xfail. Thanks for the suggestion! Should be fixed now.. Welcome back @facelessuser !\nIn my testing, I don't see SoupSieve (or at least bs4) treating type attributes as case-insensitive by default. Am I doing something silly here?\n```\nIn [10]: bs4.BeautifulSoup('').select('input[type=\"checkbox\"]')                                                    \nOut[10]: []\nIn [11]: bs4.BeautifulSoup('').select('input[type=\"checkbox\" i]')                                                  \nOut[11]: []\n``\nI certainly wouldn't object to SoupSieve handling the case-sensitivity issue oftypeso that we don't have to ;). Hi, thanks for your question. Please see [this FAQ](https://mechanicalsoup.readthedocs.io/en/stable/faq.html#form-submission-has-no-effect-or-fails) for the answer.. I believe the issue is that Requests decides what the content-type should be dynamically based on the type of object you pass it. If it is a file-like object, it usesmultipart/form-data; if it is a string (as is the case here where we use an empty string to \"submit an empty file\"), it apparently usesapplication/x-www-form-urlencoded`.\nThe Requests.request interface allows you to explicitly pass a content-type as part of the file argument, but this looks like a relatively new feature. We should identify what version it was added in and decide if we want to bump the minimum requirement for this feature.\nSee the files argument of http://docs.python-requests.org/en/master/api/#requests.request for more info.. Ah, I think you're right. I followed the rabbit hole of Requests when file is specified, and it ultimately leads to urllib3.filepost.encode_multipart_formdata, which is pretty unambiguous about the content-type it sets.\nBut if this is true, why aren't we seeing \"multipart/form-data\" in this test? Everything you've said seems to indicate that we should.. I can confirm that the request sends the filename.\nI ran the current file upload test (that uploads a tempfile with content \":-)\") and printed the response.request.body:\n```\n--aced827538a3c1b6a26afb6d84cf86b5\nContent-Disposition: form-data; name=\"pic\"; filename=\"tmpps8_kar3\"\n:-)\n--aced827538a3c1b6a26afb6d84cf86b5--\nIf I modify the test as per this PR (filename is passed as the empty string), then `repsponse.request.body` looks like:\n--319fd95615d843cbb7d1c158e0bbafd5\nContent-Disposition: form-data; name=\"pic\"; filename=\"pic\"\n--319fd95615d843cbb7d1c158e0bbafd5--\n``\nThe fact that it uses thenameattribute for the filename is an explicit choice in [requests.models](https://github.com/kennethreitz/requests/blob/a345b776fb2cf67d1a1810349e71fd60a83f53eb/requests/models.py#L153) if it cannot guess the filename from thefileobj(where, in this case,fileobjis the empty string, soguess_filenamereturnsNone`!).\nHowever, if we don't want it to make this choice, we can explicitly set the filename by passing files={name: (filename, fileobj)} instead of files={name: fileobj} (see the request API). Something like:\npython\n                    filename = value\n                    if value != \"\" and isinstance(value, string_types):\n                        value = open(value, \"rb\")\n                    files[name] = (filename, value)\nThis works great, except for one probable bug: if you use files={name: (filename, fileobj)} and filename is the empty string (fileobj can be the empty string or a real file), it creates the content body as we want:\n```\n--319fd95615d843cbb7d1c158e0bbafd5\nContent-Disposition: form-data; name=\"pic\"; filename=\"\"\n:-)\n--319fd95615d843cbb7d1c158e0bbafd5--\nbut it populates the `form` response instead of `files`, which definitely seems wrong.\n\n\n\nprint(response.json())\n{'args': {}, 'data': '', 'files': {}, 'form': {'pic': ':-)'}, 'headers': {...}}\n``\n(normally the content shown informis infiles`). I'd like to investigate this further, as it may be a bug in requests.. Well, we have a couple options for empty files. Which request would you like to see us make?\n\n\n\nReturns data in files, but has fake filename:\n1. files['pic'] = \"\" \u2192 Content-Disposition: form-data; name=\"pic\"; filename=\"pic\"\nHas empty or no filename, but returns data in form:\n2. files['pic'] = (None, \"\") \u2192 Content-Disposition: form-data; name=\"pic\";\n3. files['pic'] = (\"\", \"\") \u2192 Content-Disposition: form-data; name=\"pic\"; filename=\"\"\nHas a multipart content-type, but the content body is empty:\n4. files['pic'] = None \u2192 (empty)\n\nMy biggest concern is related to how servers will handle the data. If it's something about the request that's making httpbin put the data in form instead of files, then I worry that, for example, PHP might put the pic field in $_POST instead of $_FILES.\nAs a quick test, I ran the above 4 cases on a simple HTML form, posting to a PHP script that dumped $_POST and $_FILES.\nhtml\n<form method=\"POST\" action=\"test_processing.php\" enctype=\"multipart/form-data\">\n    <input type=\"file\" name=\"pic\">\n    <input type=\"submit\" name=\"action\" value=\"Submit\" />\n</form>\nFor reference, here's what the browser outputs:\nFILES:\ntest_processing.php:4:\narray (size=1)\n  'pic' => \n    array (size=5)\n      'name' => string '' (length=0)\n      'type' => string '' (length=0)\n      'tmp_name' => string '' (length=0)\n      'error' => int 4\n      'size' => int 0\nPOST:\ntest_processing.php:6:\narray (size=1)\n  'action' => string 'Submit' (length=6)\nNote that 'error' => int 4 is the code that maps to UPLOAD_ERR_NO_FILE: No file was uploaded, which is clearly an appropriate error. :)\nNow with the 4 variations of MechanicalSoup:\n1. files['pic'] = \"\"\nFILES:\ntest_processing.php:4:\narray (size=1)\n  'pic' => \n    array (size=5)\n      'name' => string 'pic' (length=3)\n      'type' => string '' (length=0)\n      'tmp_name' => string '/tmp/phpmFqtXB' (length=14)\n      'error' => int 0\n      'size' => int 0\nPOST:\ntest_processing.php:6:\narray (size=1)\n  'action' => string 'Submit' (length=6)\n2. files['pic'] = (None, \"\")\nFILES:\ntest_processing.php:4:\narray (size=0)\n  empty\nPOST:\ntest_processing.php:6:\narray (size=2)\n  'action' => string 'Submit' (length=6)\n  'pic' => string '' (length=0)\n3. files['pic'] = (\"\", \"\")\nFILES:\ntest_processing.php:4:\narray (size=1)\n  'pic' => \n    array (size=5)\n      'name' => string '' (length=0)\n      'type' => string '' (length=0)\n      'tmp_name' => string '' (length=0)\n      'error' => int 4\n      'size' => int 0\nPOST:\ntest_processing.php:6:\narray (size=1)\n  'action' => string 'Submit' (length=6)\n4. files['pic'] = None\nFILES:\ntest_processing.php:4:\narray (size=0)\n  empty\nPOST:\ntest_processing.php:6:\narray (size=1)\n  'action' => string 'Submit' (length=6)\n\nNow that I've worked through this a bit, I would say that 3. files['pic'] = (\"\", \"\") is the correct choice (even though httpbin places the data in form instead of files), primarily because it matches the Content-Disposition that @moy saw with cURL and is the only option that returns the UPLOAD_ERR_NO_FILE error code in PHP.. That sounds like a fair compromise. I'll give it a try!. Thanks for the suggestion! Using pip install --upgrade pytest provides a sufficiently recent version of pytest.. I ran into this suggestion as well, and it appears to provide all the necessary functionality. Thanks!. Agreed. For future reference, if we want to parse a fully generic requirements file, the syntax is outlined here.. Typo in \"understand\" here.. As of this year, there is a new maintainer of Mechanize. There are extensive docs now (https://mechanize.readthedocs.io/en/latest/), but he has no intention of porting to python3 (https://github.com/python-mechanize/mechanize/issues/9). So I would agree that it shouldn't be used anymore, but the other statements might be a bit harsh.. Agreed! I've added more complete return type information to the Browser methods that return a Response+Soup.. Why is it so painful to do the right thing! ;)\nI agree that most people would have written choose_submit(foo) instead of choose_submit(el=foo), and el is a painfully out-of-place variable name, but your instinct to preserve backwards compatibility is a good one. For now, I'll separate this change out into another PR so we can think on it for a bit longer.. Since we have a lot of methods that are wrappers for each other, there's more double-indirection going on than just this instance. I'm going to keep it how it is for now for the sake of consistency, but as we iterate on this documentation again, I would be surprised if it didn't get modified.. Completely trivial, but \"PyPi\" should be \"PyPI\". Feel free to ignore. :). I'm confused about all these calls to close(). Isn't this called when browser is destructed (due to going out of function scope)?. Not important at all since this is just documentation, but this line has a tab in it, which makes it look funny in my 4-space tab setup.. I know it's not the job of documentation to teach people how python works, but I wouldn't have know that with calls __enter__ and __exit__ without looking it up. Maybe we could add a comment in the test_with that says that \"testing with\" <==> \"testing enter and exit\"? Certainly not necessary though!. Huh! You learn something new every day! :D. Would be nice if you can change this to spaces instead of a tab, but it's not essential.. Should we print how long we're sleeping for?. Yes, we could do that to avoid errors similar to #158. However, if we're going to go that route, then we need to make even more changes. For example,\nhtml\n<form action=\"/action_page.php\">\n  <input name=\"box\" value=\"0\"/>\n  <input checked=\"checked\" name=\"box\" type=\"radio\" value=\"1\"/>\n  <input type=\"submit\" value=\"Submit\" />\n</form>\nThis should result in a box=0&box=1 query string (setting aside the fact that this seems like a bad HTML design decision). However, with just data[name] = [value], it will clobber the 0 value from the type-less input, giving only box=1.\nIf we were to change the behavior for type=radio to data.setdefault(name, []).append(value) like the others, then this HTML:\nhtml\n<form action=\"/action_page.php\">\n  <input name=\"box\" type=\"radio\" value=\"0\"/>\n  <input checked=\"checked\" name=\"box\" type=\"radio\" value=\"1\"/>\n  <input type=\"submit\" value=\"Submit\" />\n</form>\nwould result in box=0&box=1, even though it should only be box=1.\nOff the top of my head, I can't think of a way to retain the correct single-valued results for types that are only allowed to have one value, while still accounting for the possibility of poorly designed HTML like the above.. The list is needed to ensure that query and expected_post are the same type. I think query is always a list, but expected_post was sometimes a tuple. I can add a comment, but we could also consider removing the cast for query.. We could do that. Are there any other elements that are forcibly unique like radio that we would need to account for as well?\nI worry about this function (which is already not organized in the most legible way) becoming too complex.. I wish it was, but because data[key] can be both a dict and None, you can't use in (since it requires an iterator, which None doesn't have). I'm catching all exceptions here because we get both IndexError and TypeError (again, due to None).. Good point! How does it look now?. The only reason I added these was because there is no truly private method in python, so someone could have been using them. I have no problem with this item being omitted from the ChangeLog. Please feel free to do so if that's your preference!. I think it would be confusing for follow_link to have an option that makes it not follow a link and instead do something that seems more like a behavior of find_link.. Very minor, but the httpbin object implements an operator+ for strings, you can just use httpbin + \"my-referer\" to achieve the same thing. Feel free to ignore. :). It wasn't in my original implementation, but thinking about it some more, it would be nice for download_link to return the response. I don't think there are any downsides to doing this.\nIf we do, then we can simplify this test. No need to read the downloaded file; instead, we can simply check the headers in the returned response.. I mentioned it in the other PR, and I still think it would be confusing for follow_link to have an option that makes it not follow a link and instead do something that seems more like a behavior of find_link.\nI would be strongly in favor of removing this and changing the implementation of download_link (see my suggestion below), unless you can convince me that this new option is a good idea!. As per my comment above, I would rather see this as something like the following to avoid adding an option to follow_link that feels unnatural to me:\npython\n    url = self.find_link(*args, **kwargs)['href']\nDoing this means download_link can't take a bs4 tag as the link, which I'm fine with; however, if we demand it, then I think we can just reuse (or factor out) the \"link-or-tag\" input logic from follow_link, e.g.:\n```python\ndef download_link(self, filename, link=None, *args, kwargs):\n    if not hasattr(link, 'attrs') or 'href' not in link.attrs:\n        link = self.find_link(link, *args, kwargs)\n    url = link['href']. Great! Thanks for those improvements to the tests.. I have two primary concerns about such a parameter.\nFirst, one might ask why we have a dry_run-like parameter for follow_link, but nothing else? Along the same lines, if I saw a verbose or debug argument for one function, I might naturally expect it to exist for most other functions, because it represents an infrastructural design choice or philosophy. But in this case, it is just one ad-hoc parameter.\nSecond, let us consider the possible scenarios for such a parameter:\n1. The input link is a bs4 tag, in which case follow_link becomes an opaque wrapper for link['href']. \n2. The input link is not a bs4 tag, in which case follow_link becomes a wrapper for find_link that returns link['href'] instead of link.\nNeither of these scenarios seem like appropriate behavior for a method called follow_link, especially when the same functionality can be achieved using existing methods. I don't like the prospect of distorting the public API to fulfill an internal need. But even if the parameter were private, it just doesn't seem a logical thing to do.\nI don't know, am I being too difficult about something that ultimately doesn't matter?. Won't this break backwards compatibility for anyone who has specified *args? Unfortunately this whole situation is muddled by the *args situation, which doesn't seem to have a satisfying resolution. :/\nActually, in this one case, I think we can at least not make the situation worse by keeping\npython\nif not hasattr(link, 'attrs') or 'href' not in link.attrs:\n    link = self.find_link(*args, **kwargs)\nin both follow_link and download_link instead of\npython\nlink = self.find_link(link, *args, **kwargs)\nIt's only 1 line of code duplicated, but I think avoids a lot of complexity in find_link.. No change requested here. Just wanted to say that I like the idea of moving this debug conditional from follow_link to find_link.. The description of these arguments is a bit obsolete with the addition of the link param. Perhaps we can just leave it as \"Additional arguments forwarded to find_link\"?. (And, if it wasn't clear, we would not need the link param in find_link in this case.). I really think any change to the find_link interface is overcomplicating this.... I think he was referring to the fact that it won't reload the page from a StatefulBrowser.get, even though called on a StatefulBrowser object, since it's using a Browser (stateless) method. I'm not sure how to best phrase this.\nI almost want to get rid of the Browser class and have everything be \"stateful\" to avoid this kind of ambiguity, but... that's a much longer discussion for a much later time. ;). @samlanning Thanks for providing them! I like the logos a lot, but I think @moy is right that in the context of this particular page, consistency is better. I will definitely keep them in mind for other projects though.. This was simply to fix a flake8 error because the \"Extra Cheese\" line was too long, and removing \"checked\" made it shorter. My apologies for not documenting this change.. It is now documented in the revised commit message. Thanks for the reminder!. If we want to, we can change this test to use httpbin + \"/links\" instead of the home page to minimize the interference of the change to httpbin.. Similarly, this could download a page from httpbin + \"/links\", though it would be text, rather than a PNG.. Minor typos:\n which is JavaScript-only\n missing parenthesis at the end of the sentence.. I agree that would probably be preferable, but since the output of parse_qsl is explicitly a list object, I saw only two sensible options:\n1. Define the expected output as a list.\n2. Convert the expected output to a list in mock_post.\nI chose the first option since it seemed like the most direct comparison and it could not hide any errors behind a cast to another type. However, I'm open to alternatives if you see something better.. I was quite confused by this code originally (not least because of the complete lack of comments), so I am very happy to rework it. I took your idea here and made a few modifications. How does this look to you?\npython\n            elif tag.name == \"select\":\n                options = tag.select(\"option\")\n                selected_values = [i.get(\"value\", \"\") for i in options\n                                   if \"selected\" in i.attrs]\n                if \"multiple\" in tag.attrs:\n                    for value in selected_values:\n                        data.append((name, value))\n                elif selected_values:\n                    # A standard select element only allows one option to be\n                    # selected, but browsers pick last if somehow multiple.\n                    data.append((name, selected_values[-1]))\n                elif options:\n                    # Pick the first option if none are selected\n                    data.append((name, options[0].get(\"value\", \"\"))). Before I invest too much more time into this, I just wanted to check if you are generally opposed to this PR, or if you can imagine a modified PR that you would consider accepting. Even better if you already have an idea in mind! ;)\nMy preference is always to make StatefulBrowser the best class it can be. The Browser class sometimes gets in the way of that, such as in this case. I believe StatefulBrowser would benefit from a submit method, so if we can't override Browser.submit in the proposed way, the first two alternatives that come to mind (setting aside the issue of backwards compatibility for a moment) are renaming Browser.submit to either a) Browser._submit or b) Browser.submit_form.. A warning for suspicious use is a great idea, but I'm not sure that it addresses the main issues here (and might be just as much of an annoyance as a modified interface to some users). Out of curiosity, though, did you have a design in mind for warning toggling?\nOkay, two more possibilities:\n1. Simply add a comment to the Browser.submit docstring saying something like:\n\nIf you are calling this method from a StatefulBrowser instance, consider using StatefulBrowser.submit_selected instead.\n\n\nOverride Browser.submit in a Liskov-compliant way, e.g.\npython\ndef submit(self, form=None, url=None, **kwargs, btnName=None):\nwhere form and url default to the current form/url if None. It's a bit of a clunky interface, but might be the best we can do if we want to make overriding work within our constraints. Or even just\npython\ndef submit(self, form=None, url=None, **kwargs):\nto avoid complications with btnName.\n. \"is available as an SVG file\". \"can then be opened in\". @moy I'd like to get version 0.11 released as soon as possible, but I want to address this issue in some way before I do, even if it's just a minor addition to the documentation.\n\nHow about I make the documentation change proposed above for 0.11, and then we can reassess if we want to make any code changes for version 1.0 later?. Thanks for the feedback! For the 0.11 release I'll just change the docstring, unless you recommend otherwise.\nAs I think more about it, overriding submit seems increasingly important. One of the biggest issues (that I didn't appreciate until just now) is that if you call submit on a StatefulBrowser, the browser state becomes stale. I would argue that this is a pretty dangerous inconsistency for a method that looks like the most obvious way to submit a form.. suggestion\n  is due to the project being abandoned more than to its maturity.. > <input type=\"image\"> elements do not accept value attributes. The path to the image to be displayed is specified in the src attribute.\nhttps://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/image\nWhile this obviously this impacts the test (which can be easily modified), it's not clear to me whether or not it impacts the implementation of the fix. I don't think it does, but it's something to think twice about.. We're all guilty of writing docstrings that are not sufficiently detailed, but for the sake of this as a learning experience, I'll add that it would be nice if you could make the docstring say more specifically what the test is designed to do (you can even reference an Issue number here if you like).. Looks great, thanks for the extra detail!. Ultimately, we need to try to replicate what the browser does (especially in cases where this is the standard). When clicking on a <input type=\"image\" name=\"test\" ...>, the browser does not submit a name=value request, because value is an invalid attribute for type=\"image\". Instead, it will submit a request that looks like:\ntest.x=171&test.y=168\nwhere 171 and 168 are the mouse positions where the image was clicked. Obviously we don't have access to mouse positions with MechanicalSoup, so I'm not exactly sure how we should handle this, but I know we don't want to try to use a value in this case (or at the very least, we should allow for the case where value doesn't exist, and just submit a name= request). Or maybe we should submit name.x=0&name.y=0?\n@moy Do you have any thoughts about what the request should look like?. I'm not sure we need to be making temporary files here. So long as we are passing the name to requests, then we will get the multipart form data we want in the response, even if the value is empty.\nI'd recommend something like this instead:\n``python\n                    if value != \"\": \n                        value = open(value, \"rb\")\n                    # If value is the empty string, we still pass it for consistency\n                    # with browsers (see #250).\n                    files[name] = value\n. Yes, adding a test is great! However, I think this can be accomplished with less code duplication by using thepytest.mark.parametrizedecorator. For example, seetests/test_stateful_browser.pyline 430 (test_follow_link_arg`).\nOne possible way to use parameters would be to set an expected_content (\":-)\" or \"\") and set_value (a bool to determine whether or not to write the expected_content to a temporary file and set the form value for \"pic\").. Indeed, I've tested this, and so long as we are sending the empty string (e.g. files[name] = \"\"), then it will be part of the submitted request. None doesn't work as a value, but I think we don't have to worry about that since we use value = tag.get(\"value\", \"\"), which should always be a string.. It does. Personally, I think it's fine to leave as a unit test instead of an end-to-end test, especially since it should be converted into a parameterization of the existing test__request_file test (@moy can overrule me here if he disagrees).. ",
    "andpozo": "@mghadam:\nMechanicalSoup is a wrapper for python requests and uses a Session class for browser session.\nfrom http://docs.python-requests.org/en/master/api/#requests.Session.cookies:\n\nA CookieJar containing all currently outstanding cookies set on this session. By default it is a RequestsCookieJar, but may be any other cookielib.CookieJar compatible object.\n\nand from http://docs.python-requests.org/en/master/api/#requests.cookies.RequestsCookieJar \n\nCompatibility class; is a cookielib.CookieJar, but exposes a dict interface.\n\nso... I assume that the MechanicalSoup uses a CookieJar if not you can initialize your Browser with a custom session:\n``` python\nimport requests\nimport mechanicalsoup\nfrom http import cookiejar\nc = cookiejar.CookieJar() #you can chooise another cookie store like a LWPCookieJar\ns = requests.Session()\ns.cookies = c\nbrowser = mechanicalsoup.Browser(session=s)\n```\n. ",
    "scotbond": "Dumping StatefulBrowser with pickle may produce error \"RecursionError: maximum recursion depth exceeded\".\nI found this more appropriate:\n```\nSave cookieJar from active session\npickle.dump(browser.get_cookiejar(), open(\"cookiejar.dump\", 'wb'))\n\nLoad cookiejar into fresh browser\ncookiejar = pickle.load(open(\"cookiejar.dump\", 'rb'))\nbrowser.set_cookiejar(cookiejar)\n```. ",
    "yongseung": "Thank you it work's! I coudln't imagine that update function will add a header...\n. ",
    "gwynnebaer": "I can confirm that the patch above worked for me. @moy . ",
    "ZedYeung": "Well, I suppose there is no other choice.\n. ",
    "krsyoung": "In look at the code seems like it is due to this change:  https://github.com/hickford/MechanicalSoup/commit/3d3019e9f06bee860beeaffa505d97af3735a1ab\nI guess removing the warning causes it to have a fit when explicitly trying to specify the HTML parser.\n. Thanks @hickford! \n. ",
    "chadmiller": "Probably, should depend on \nhttps://pypi.python.org/pypi/rfc3987\nand use its parse(first_url) and incrementally resolve(discovered_web_page_url, current_url)\n. ",
    "lchski": "Just had this problem. By default, the library doesn\u2019t handle relative URLs. The fix is is mentioned in #10: you need to pass the current page URL to the .submit() method, which parses it and creates an absolute URL.. ",
    "JohnTendik": "Ahh brilliant, I guess my methodology was wrong, I had to use Browser().session when I was using page.cookies :D \nThanks for the super quick response! . ",
    "L1ghtn1ng": "As far as I am aware no distribution is using these versions of python and for Mac users they should be using homebrew to get new versions of python as http://pyfound.blogspot.co.uk/2017/01/time-to-upgrade-your-python-tls-v12.html?utm_source=feedburner&utm_medium=twitter&utm_campaign=Feed:+PythonSoftwareFoundationNews+(Python+Software+Foundation+News)&m=1 this is going to be happening\n\nFrom: Matthieu Moy notifications@github.com\nSent: Wednesday, February 1, 2017 7:40:50 AM\nTo: hickford/MechanicalSoup\nCc: J.Townsend; Author\nSubject: Re: [hickford/MechanicalSoup] Update .travis.yml with python 3.6 (#54)\nIs there a reason to drop support for old versions? In particular, Python 2.6 is not dead yet IMHO. Why not drop it if there's a good reason to, but then the good reason should be mentioned in the commit message.\n-\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/hickford/MechanicalSoup/pull/54#issuecomment-276592233, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ADddQha2LU9LM-WV-c1s0lrwUXMK4zAKks5rYDcCgaJpZM4LzLuu.\n. Okay will fix when I get back from work\n\nFrom: Matthieu Moy notifications@github.com\nSent: Wednesday, February 1, 2017 7:51:28 AM\nTo: hickford/MechanicalSoup\nCc: J.Townsend; Author\nSubject: Re: [hickford/MechanicalSoup] Update .travis.yml with python 3.6 (#54)\nThe fact that distro don't ship 2.6 anymore doesn't mean everybody has upgraded. I still use a production server with Python 2.4 for example. There are more RedHat/CentOS 6 out there than you seem to think.\nI don't question the fact that most users should upgrade, but that doesn't seem to be a good reason for MechanicalSoup to drop support.\n-\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/hickford/MechanicalSoup/pull/54#issuecomment-276593824, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ADddQnrNrBuRuXomWuOu2D68W-3PPD4vks5rYDmAgaJpZM4LzLuu.\n. Okay sorry about taking ages to fix this I had been ill and forgot and only remembered about this yesterday\n\nFrom: Matthieu Moy notifications@github.com\nSent: Friday, February 17, 2017 6:56:15 PM\nTo: hickford/MechanicalSoup\nCc: J.Townsend; Author\nSubject: Re: [hickford/MechanicalSoup] Update .travis.yml with python 3.6 (#54)\nClosed #54https://github.com/hickford/MechanicalSoup/pull/54.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/hickford/MechanicalSoup/pull/54#event-967415442, or mute the threadhttps://github.com/notifications/unsubscribe-auth/ADddQuicnBhdA4E8FqtbKj5-U-6jFUU2ks5rde1PgaJpZM4LzLuu.\n. ",
    "ghisvail": "\nI'm unsure about the testsuite, which isn't supposed to be helpful to the end user.\n\nThe end-user is likely to install the wheel, not the source tarball.\nI would however need the tests to check the module when the Debian package is being built, and to run the CI tests against the binary packages whenever one of its install dependencies (including a new version of Python) is changed.. > I think adding the examples would be a good idea.\nI can add the examples if you want.. Any news? . > It's up to you! I've really appreciated using this commit locally, but it's hard for me to weigh whether it's worth limiting our tests.\nThis commit could allow Debian to run the tests for the corresponding packages (python-mechanicalsoup and python3-mechanicalsoup). Debian builders prevent access to the network at build time to avoid unattended tempering of the builds.. ",
    "r0fls": "thank you. ",
    "ali-wetrill": "Still not working, I cloned MechanicalSoup and ran setup.py. I also tried just installing it directly from github. I'm still running into this issue. I have Python 3.5.2.. ",
    "willcoxDev": "I am using python 3.6 and installed mechanical soup using the pip, I get this error. How do I use a different version I clicked on the link but no link for 3.6. ",
    "dbrobins": "Can this be closed with #70?. Passing a btnName didn't help me; I posted a PR to fix choose_submit. #70 . Sure, I'll add a test later this week.. ",
    "3xp10it": "Hi @moy ,the script will execute to except branch(code=0,content='can not get...') everytime I run the script I supported upon.. I have found solution,I did not install lxml,after pip3 install lxml,it works well.. ",
    "hypatia1": "Here is my stripped down source with only the relevant code:\n``` #! /usr/bin/env python3\nimport mechanicalsoup, sys\nfrom requests import Session\nURL=\"https://172.16.32.1/\"\nsession = Session()\nsession.verify = False\nbr = mechanicalsoup.Browser(session=session,soup_config={'features':'lxml'})\nlogin_page = br.get(URL)\nlogin_page.raise_for_status()\nlogin_form = login_page.soup.find('form', {'class':'form-horizontal'})\nlogin_form.find(attrs={\"id\":\"usernamefld\"})['value']=args.username\nlogin_form.find(attrs={\"id\":\"passwordfld\"})['value']=args.password\ndash_page = br.submit(login_form, URL)   # submitting the login credentials\nif 'Username or Password incorrect' in dash_page.soup.get_text():\n    sys.stderr.write(\"\\nError: login failed. Check your URL & credentials.\\n\")\n        sys.exit(1)\nelif 'System Information' not in dash_page.soup.get_text():\n    sys.stderr.write(\"\\nError: login failed. Check your URL & credentials-2.\\n\")\n    sys.exit(1)\nelse:\n    print('I think we are logged in!')\n```\nThe web server always returns me to the login page because missing \"login\" field in the post. I don't run into this problem with mechanize and python 2.7. I'm trying to port my old scripts to python 3.. Thank you for the advice. That did the trick.\nSorry for opening an issue, when the real bug was my own misunderstanding.. ",
    "ntitty": "It seems that the whl file at pypi site does not have two files.\nstateful_browser.py\nutils.py\nhttps://pypi.python.org/pypi/MechanicalSoup/. ",
    "rachmadaniHaryono": "afaik pytest have also method to test the raised error\nhttps://docs.pytest.org/en/latest/assert.html\n```python\nwith pytest.raises(mechanicalsoup.LinkNotFoundError) as context:\n    resp = browser.get(\"http://httpbin.org/nosuchpage\")\n````. ",
    "boarnasia": "In my environment, new Session comes with new cookies.\n```python\n\n\n\nimport requests\ns1 = requests.Session()\ns1.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\n\ns1.cookies.keys()\n['sessioncookie']\ns2 = requests.Session()\ns2.cookies.keys()\n[]\n```\n\n\n\nThis behaviour is also the same as that of mechanicalsoup.\n```python\n\n\n\nfrom mechanicalsoup import Browser\nb1 = Browser(soup_config={'features': 'html'})\nret = b1.get('http://httpbin.org/cookies/set/sessioncookie/123456789')\nb1.session.cookies.keys()\n['sessioncookie']\nb2 = Browser(soup_config={'features': 'html'})\nb2.session.cookies.keys()\n[]\n```\nI think there's no issues.\n. However, it's not good that browser finishes without closing own session when  being destroyed.\n\n\n\nI think it needs close() and __del__ methods like this:\n```python\nclass Browser(object):\n    ...\ndef close(self):\n    \"\"\"Close the current session\"\"\"\n    self.session.cookies.clear()\n    self.session.close()\n\ndef __del__(self):\n    self.close()\n\n```. ",
    "ericbrandwein": "Sorry, didn't see that you answered. It does answer my question, the problem I was having in fact was that the form used a multipart data encription type, which doesn't seem to be supported by this library. I opened an issue with this: https://github.com/hickford/MechanicalSoup/issues/90. Closing this one!. ",
    "ztane": "Damn, I installed from PyPI as per readme on github, and it didn't work :D. ",
    "codecov-io": "Codecov Report\n\n:exclamation: No coverage uploaded for pull request base (master@ed48d29). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster      #95   +/-\n=========================================\n  Coverage          ?   83.06%         \n=========================================\n  Files             ?        4         \n  Lines             ?      313         \n  Branches          ?        0         \n=========================================\n  Hits              ?      260         \n  Misses            ?       53         \n  Partials          ?        0\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ed48d29...236b1e1. Read the comment docs.\n. # Codecov Report\nMerging #96 into master will increase coverage by 2.55%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster      #96      +/-\n==========================================\n+ Coverage   83.06%   85.62%   +2.55%   \n==========================================\n  Files           4        4            \n  Lines         313      313            \n==========================================\n+ Hits          260      268       +8   \n+ Misses         53       45       -8\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| mechanicalsoup/browser.py | 89.47% <0%> (+2.63%) | :arrow_up: |\n| mechanicalsoup/stateful_browser.py | 83% <0%> (+5%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4ced512...6730ec2. Read the comment docs.\n. # Codecov Report\nMerging #97 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster      #97   +/-\n=======================================\n  Coverage   85.62%   85.62%         \n=======================================\n  Files           4        4         \n  Lines         313      313         \n=======================================\n  Hits          268      268         \n  Misses         45       45\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c0e847d...edafa8d. Read the comment docs.\n. # Codecov Report\nMerging #98 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster      #98   +/-\n=======================================\n  Coverage   85.62%   85.62%         \n=======================================\n  Files           4        4         \n  Lines         313      313         \n=======================================\n  Hits          268      268         \n  Misses         45       45\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c0e847d...a2ebf03. Read the comment docs.\n. # Codecov Report\nMerging #103 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #103   +/-\n=======================================\n  Coverage   85.62%   85.62%         \n=======================================\n  Files           4        4         \n  Lines         313      313         \n=======================================\n  Hits          268      268         \n  Misses         45       45\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1deb9a2...0a504d1. Read the comment docs.\n. \n",
    "codecov[bot]": "Codecov Report\n\nMerging #105 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #105   +/-\n=======================================\n  Coverage   87.53%   87.53%         \n=======================================\n  Files           4        4         \n  Lines         313      313         \n=======================================\n  Hits          274      274         \n  Misses         39       39\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8e6fdb3...653d029. Read the comment docs.\n. # Codecov Report\nMerging #107 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #107   +/-\n=======================================\n  Coverage   87.53%   87.53%         \n=======================================\n  Files           4        4         \n  Lines         313      313         \n=======================================\n  Hits          274      274         \n  Misses         39       39\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c1391a9...7b59bdf. Read the comment docs.\n. # Codecov Report\nMerging #108 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #108   +/-\n=======================================\n  Coverage   95.26%   95.26%         \n=======================================\n  Files           4        4         \n  Lines         317      317         \n=======================================\n  Hits          302      302         \n  Misses         15       15\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f76ae76...f578ddf. Read the comment docs.\n. # Codecov Report\nMerging #112 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #112   +/-\n=======================================\n  Coverage   95.26%   95.26%         \n=======================================\n  Files           4        4         \n  Lines         317      317         \n=======================================\n  Hits          302      302         \n  Misses         15       15\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 468f694...fdd0f7d. Read the comment docs.\n. # Codecov Report\nMerging #113 into master will increase coverage by 0.59%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #113      +/-\n==========================================\n+ Coverage   95.26%   95.85%   +0.59%   \n==========================================\n  Files           4        4            \n  Lines         317      314       -3   \n==========================================\n- Hits          302      301       -1   \n+ Misses         15       13       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| mechanicalsoup/browser.py | 96.46% <100%> (+1.63%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b8ccb93...531029b. Read the comment docs.\n. # Codecov Report\nMerging #115 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #115   +/-\n=======================================\n  Coverage   95.85%   95.85%         \n=======================================\n  Files           4        4         \n  Lines         314      314         \n=======================================\n  Hits          301      301         \n  Misses         13       13\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4c90ef7...f27d5bb. Read the comment docs.\n. \n",
    "ocefpaf": "Thanks!. ",
    "mkormendy": "I'm a Graphic Designer by education. Would you guys like me to hack something in PNG format together?. Just a sample. It has elements that can be simplified into a favicon sized illustration as well.\n\n. No problem. You guys are generously making something I am trying to use for one of my projects.. @moy, the only thing non-free would be the typeface ... I needed something to bookend the illustraton.\nI can still use a Google Font, and I can find the license information so that it can be used in this project. I'm looking at using Zilla Slab as the typeface, along with Open Sans for the subtitle. They both fall under the Open Font License. Here's the updated image with the Open Font License fonts. I also fixed some of the weighting of the shapes and brought some consistency to the corners of object details.\n\n. I see! Let me make some changes before you pull the changes in.\nHere's what I propose:\n\nthe \"faded\" appearance of the text is a design aesthetic, when standing against the logo, if it is pure black, it causes a visual hierarchical conflict (your eye doesn't know where to start first)\nthe difficulty with reading the tagline is due to the typeface not being respected in the rendering on your computer \u2013 let me figure out how best to get the SVG to pull that in from google's open source fonts\nit's important to have the name of the project below the image for the often-found instance when the image is sourced on the internet (hot-linked or found in an image search), then the name of the project gets \"packaged\" with the illustration all in the one image file, so it keeps it together, and encourages people to not steal the illustration for their own project. At the very least of the issues here, @moy hit's the nail on the head about missing the detail at smaller sizes.. Were you guys planning on using my logo as is?. Why does this have to go there. Why couldn\u2019t you have had the common courtesy to just leave it be as submitted?. I\u2019m freely giving it to the project, do with it what you wish. My original intention was to contribute to the project with skills I was confident in, as thanks for creating the code which helped me with an additional project. My understanding of the nature of the project was amiss, and now that I understand the fun-nature of it, I digress and apologize for any misunderstanding. The artwork/logo is yours to do with as you wish, with further modifications or not.. \n",
    "mention-bot": "Unable to parse mention-bot custom configuration file due to a syntax error.\nPlease check the potential root causes below:\n\nHaving comments\nInvalid JSON type\nHaving extra \",\" in the last JSON attribute\n\nError message:\nError: Parse error on line 2:\n...erger\", \"hickford\"] // Users in this lis\n-----------------------^\nExpecting 'EOF', '}', ',', ']', got 'undefined'. ",
    "Kiris-tingna": "thanks @hemberger. ",
    "AndreWin": "@moy, thank you! I will install mechanicalsoup from github.\nP.S.: I don't interact with Jupyter Notebook. I work in it. I write my code in it and see results.. ",
    "KevinCooper": "Thank you!. ",
    "bleuf1shi": "@hemberger thanks. I've tried the following per your suggestion:\nresp = browser.open(\n        url=report_csv_url,\n        timeout=900\n    )\nThe endpoint url is one that takes a long time to respond since the server generates a CSV. Performing the requests in Chrome on MacOS take about 5-15min to respond and begin the download. However, when using mechanicalsoup, I'm thrown the following exception:\n\nERROR:  ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 601, in urlopen\n    chunked=chunked)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 387, in _make_request\n    six.raise_from(e, None)\n  File \"\", line 2, in raise_from\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 383, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\", line 1331, in getresponse\n    response.begin()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\", line 297, in begin\n    version, status, reason = self._read_status()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\", line 258, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/socket.py\", line 586, in readinto\n    return self._sock.recv_into(b)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 1009, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 871, in read\n    return self._sslobj.read(len, buffer)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 631, in read\n    v = self._sslobj.read(len, buffer)\nConnectionResetError: [Errno 54] Connection reset by peer\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py\", line 440, in send\n    timeout=timeout\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 639, in urlopen\n    _stacktrace=sys.exc_info()[2])\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/util/retry.py\", line 357, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/packages/six.py\", line 685, in reraise\n    raise value.with_traceback(tb)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 601, in urlopen\n    chunked=chunked)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 387, in _make_request\n    six.raise_from(e, None)\n  File \"\", line 2, in raise_from\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 383, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\", line 1331, in getresponse\n    response.begin()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\", line 297, in begin\n    version, status, reason = self._read_status()\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\", line 258, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/socket.py\", line 586, in readinto\n    return self._sock.recv_into(b)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 1009, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 871, in read\n    return self._sslobj.read(len, buffer)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py\", line 631, in read\n    v = self._sslobj.read(len, buffer)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mechanicalsoup/stateful_browser.py\", line 127, in open\n    resp = self.get(url, args, kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mechanicalsoup/browser.py\", line 106, in get\n    response = self.session.get(*args, kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py\", line 521, in get\n    return self.request('GET', url, kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py\", line 508, in request\n    resp = self.send(prep, send_kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/sessions.py\", line 618, in send\n    r = adapter.send(request, *kwargs)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/requests/adapters.py\", line 490, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n\nThe exception is thrown after exactly 5min which led me to believe it's an issue that could be solved by increasing the timeout. 300 seconds is 5min and I'm passing 900 seconds.\nAny ideas?\nFore more transparency, the reason I'm using mechanicalsoup at this point in my code is because I've performed additional auth and form filling steps beforehand. So the CookieJar is setup for performing this download request.. I may have fixed it following the advice from https://stackoverflow.com/a/38854398 \n\nWhile installing requests library it skips few of optional security packages ('pyOpenSSL', 'ndg-httpsclient', and 'pyasn1') which are required for the SSL/Https connection. You can fix it by either running this command\npip install \"requests[security]\"\nor\npip install pyopenssl ndg-httpsclient pyasn1\n\nI'll let ya know.. @moy , yes. The advice I found ( https://github.com/MechanicalSoup/MechanicalSoup/issues/168#issuecomment-345387976 ) along with extending the connection and read timeouts did resolve the issue.. ",
    "krlk89": "I thought this was a good issue to get my foot in the door in contributing to MechanicalSoup and so I looked into it.\nI'm not quite sure what OP meant with \"allow passing a Tag object\" because form is also a html tag.\nI guess the intent was to pass some other tag (or id or class) and then search the parent(s) of that for the form (and if a form is not found, then raise a LinkNotFoundError).\nI hope to do a pull request sometime this week, but if I have misunderstood the premise of this feature request then please let me know.\n. @moy Aha, that makes sense! I will still try to implement this feature this week.\n@sbraz Got it!. I will look into this issue sometime this week.\nUpdate: Have been researching this for too many hours and finally I think I know why this problem is happening. I will comment my understanding of the issue (and hopefully a possible solution) this weekend.. So.. HTML 4 set ISO-8859-1as the default character set [1] and it's possible that some websites are not using the HTML 5 standard, which sets UTF-8 as the default. UTF-8 is also the default in Python 3.\nAlso, a form can have a accept-charset attribute, which specifies the character encodings that are to be used for the form submission. [2]\nOn the first occasion I don't think we have a way of knowing how the server handles the input and I guess the sensible solution would be to update the documentation to tell the user that if they wish to change the encoding of the input, it's possible, like this:\nform.set(\"q\", \"\u00fc\u00f5\u00f6\u00e4\".encode(\"ISO-8859-1\")) [3]\nOn the second occasion I guess it's possible to change the encoding behind the scenes, but I am not sure if it's necessary and/or a good practice. Thoughts?\nAlso, adding the accept-charset to the request header didn't have any effect as far as I tested and as is written in Requests documentation: Requests does not change its behavior at all based on which custom headers are specified. [4]\nI also responded to the Stack Overflow problem but haven't gotten an answer so far, so it's possible that there's something else going on, but the tests I did back up what I have written.. @hemberger Thanks for the response. I will continue working on this next week.. @hemberger A follow up, I haven't had time to work on it yet, but I'll get to it this weekend.. ",
    "sbraz": "@krlk89 In some cases I can't just use a selector to find the right form. In this example, I chose the form based on one of its inputs.. test_new_control is using a hardcoded http://httpbin.org URL so it's quite easy to fix. The other two are using a form, I'm not sure how you can reinject the local httpbin URL into it.. Ah, we can probably replace the URL inside form_html by a {} placeholder and then form_html.format(httpbin.url). That branch works for me :) When can we hope to see a new release?. ",
    "aborruso": "@moy to add \"save\" and \"download\" would be really great.\nThank you very much for all. Thank you very much @hemberger . ",
    "Harapuia": "I've made the changes as requested\nI've no idea why it happens, but when i investigated the issue that i had with it the official advice was to do it like this, instead of using del. So i decided to change it properly here\nNow to figure out where those attribute errors are coming from..\ni don't think i'm doing anything nuts. I'm very okay with this. I thought it was a clever fix as i was already fumbling about with weakref.ref() trying to get a replacement for 2.7 to work. This was much smarter!\nGlad i could help!\n. It can be constructed because self._finalize is just a variable name. Maybe it should be renamed for additional clarity?. ",
    "Nathan-Webb": "Thanks! It works now :). ",
    "amotl": "Dear Dan,\nthanks for looking into this and especially your quick answer!\nUnfortunately, both suggested variants (as first positional argument or fudged as link keyword argument) will raise a mechanicalsoup.utils.LinkNotFoundError.\n\nI'm not particularly fond of how all the argument forwarding is set up\n\nI see your point ;]. That's the reason i was asking for help because my mind signalled it would be too lazy to wrap itself around this ;].\n\nbut perhaps there's also some way to refactor these methods without impacting backwards compatibility too much.\n\nI think there will be a way. Maybe you see a clear way already after letting this sink in for a while or otherwise i might come up with a PR at some time. But please don't hurry, there is a work around already in place right now:\n```\nWorks for now\nlink = self.browser.find_link(url_regex='register/PAT_.*VIEW=pdf')\nreturn self.browser.open_relative(link['href'], headers={'Referer': result.url})\n```\nThanks again!\nCheers,\nAndreas.\n. Hi again,\n178 addresses the problem in a way i find it reasonable without breaking the test harness.\nIt removes the additional keyword arguments url_regex and link_text on the links method to make its signature symmetric to the find_link method.\nMaybe you want to have a look (and complete it by adding an accompanying unit test)? Thanks in advance for your efforts!\nWith kind regards,\nAndreas.\n. Cool, @moy! Thanks for the update, appreciate it.\nI am looking forward to test your work on #184 with my amendments to https://github.com/ip-tools/ip-navigator/commit/a26c3a8a as soon as this hits a fresh MechanicalSoup release.\nAm i getting it right that i would just have to do\nresponse = self.browser.download_link(url_regex='register/PAT_.*VIEW=pdf') then?\n. > Don't hesitate to test before the next release (which we plan to tag as 1.0).\nSure, will be happy to do this. Currently, from your fork/branch located at https://github.com/moy/MechanicalSoup/tree/download-link, right? Or do you plan to merge this branch into mainline soon?\nTo actually use the download_link method, the obligatory filename argument would be counterproductive in my context however. I added a respective comment at https://github.com/MechanicalSoup/MechanicalSoup/pull/184/commits/ca454ed9#r159693535. Thanks again for looking into this, i hope you appreciate my feedback.\n. Thanks Matthieu and Dan, this works like a charm!. Dear Matthieu,\nthanks for your response. I will try to find some time to address your request soon.\nWith kind regards,\nAndreas.\n. Dear Matthieu,\nalso happy new year to you and your loved ones!\nI am sorry to say that i haven't been able to address your suggestions yet. As quite some things are on my plate already, i can't promise to get to it in January, but i will be happy to receive another ping from you in February.\nOtherwise: If others from the community read this and have some spare time, i will also be happy to pass this along.\nWith kind regards,\nAndreas.\n. All right @moy, thanks so much for taking over and good luck!. Thanks Matthieu and Dan, this works like a charm!. Please go ahead, appreciate it. Thanks again for your responsiveness regarding #177 and #184 and your excellent work on MechanicalSoup.. Dear @hemberger and @moy,\ni see you are still working on the upcoming 1.0.0-dev release, right? Maybe you could upload another interim version like 0.9.0.post5 to PyPI? That would be cool as we are experiencing problems with the dependency_links thing when installing PatZilla with pip directly from PyPI.\nJust doing python setup.py develop works for us but this is only suitable for development. There are the usual shenanigans regarding setuptools/pip compatibility around on the internet with hints about the parameters like --process-dependency-links --allow-all-external --allow-unverified=, which we tried to apply. However, we haven't had any success with this.\nThanks again for your efforts!\nWIth kind regards,\nAndreas.. Hi there,\nwe just wanted to let you know that we just switched to the 0.10.0 release (https://github.com/ip-tools/ip-navigator/commit/b12b2c15) and everything works as expected.\nThanks again for your efforts,\nAndreas.\n. Dear @moy, thanks for your work on this!\nMay i humbly suggest to make writing to the filesystem optional? My proposal would be to change the method signature to:\ndef download_link(self, link=None, filename=None, *args, **kwargs)\n... and then conditionally run the code like:\nif filename is not None:\n    with open(filename, 'wb') as f:\n        f.write(response.content)\n\nA further improvement could be to accept an already opened file handle as value for the filename argument like\nif filename is not None:\n    if isinstance(filename, six.string_types):\n        with open(filename, 'wb') as f:\n            f.write(response.content)\n    elif hasattr(filename, 'write'):\n        filename.write(response.content)\n    else:\n        raise ValueError('\"filename\" parameter is neither a file name nor a file handle')\nWhen doing this, one would be able to also pass an in-memory string buffer like an instance of StringIO to the download_link method.\n. Thanks!. ",
    "giorgiosironi": "Will this be released before 1.0 (e.g as 0.9.x or similar)? I have upgraded to the latest stable 0.9.0.post4 but it doesn't have this feature.. ",
    "tbolender": "That sounds nice! I did not know that you could retrieve the request from a response. Thereby also usages of the thin wrappers get() and post() could easily be handled (this caused some worries when looking into this myself).. Tests are self-evident, I just did not want to put more effort in if somebody disagrees with the basic implementation. I will implement them as soon as I have time.\nRebasing ~~will be~~ was done ~~as well~~ :)\nI thought about storing the full response, but this would have reduced _BrowserState to a response and form. From the perspective of a clean history, I would apply such a change in a different PR, but could also do it within this one.\n  . I addressed the documentation issues and created the some tests.\nIf I understood correctly, you mainly rely on open_fake_page() for testing. Unfortunately, opening a page this way this does not cause the creation of a request method which makes testing infeasible. I noticed though that there exists a setup_mock_browser() which prepares some actual mocked requests. Since I did not want to change much of the existing code I wrote my own helper function. I hope this is fine with you.. You are welcome! I added entries to the changelog as well. Just tell me if you prefer a different wording.. Just to add my two cents with some reasoning as a newcomer to MechanicalSoup: A short time ago I was searching for a library helping me to navigate through web for a small personal project. After a quick search I found MechanicalSoup and RoboBrowser. The only reason I finally went with MechanicalSoup was that it offers direct access to the page soup, that the documentation was a bit more verbose and the test coverage.\nNevertheless: When looking at the documentation I immediately saw that the Browser and StatefulBrowser split is far from a perfect solution. For me Browser appeared more like a collection of utility function with some unwanted state-keeping inside, e.g., add_soup() or all the other thin wrapper functions (and why are they public?). Although it might sounds a bit harsh: I think I would have went with RoboBrowser if I would not have needed to dig around in the page myself. For me this is a smell for bad class design.\nIn my opinion any way of improving the situation will break the backward compatibility, and if you do, then do it the \"hard\" way to actually change something. Otherwise I would not be surprised that there is a similar discussion after the release of 1.0.0 and then it would actually be too late.\nTo make things short: Please join the both classes.. @hemberger is right about my intention. Until I started to created the PR I did not know that there was a different behavior when opening pages via StatefulBrowser or Browser. Maybe I just over-read this somewhere.\nAnd yes, I would also support to get rid of Browser :D. I am not sure about this one. I extended the description, but in the end we have something similar to the not-working hint above.. Since there is currently no other hint of a distinction between StatefulBrowser and Browser methods I decided to omit the whole sentence.. This is indeed the behavior of Firefox 57.0.4, tested with this gist. I added respective checks.. ",
    "dannypurcell": "@hemberger fixed, and Travis is happy now.\nAlso moved the guesswork to a separate function so we can avoid lstrip when not needed. fixed. ",
    "BeatboxHero": "Yes, as in a real browser. Unfortunately, Selenium doesn't work for what I am trying to accomplish either. Thanks for the quick answer!. ",
    "blackwind": "Looks like the test suite doesn't like what I've done, but my solution works for me with the functions I use, and I don't have any further time to spend on my project that leverages this library at the moment. Hopefully someone can take my work here and run with it, as it's definitely a problem that needs solving.. Did you ever get a chance to take a look at this, @hemberger?. In a perfect world, we'd stick with the standard, but the fact is, all major browsers (Chrome, Edge, Firefox, and IE) accept invalid values, so invalid values are being used in the wild -- including the website I'm scraping with this module. The module, therefore, needs to handle this use case.. ",
    "samlanning": "Hey there, LGTM staff here.\nI can confirm that we won't show a red  for every PR submitted if there are existing alerts in a PR, that doesn't sound useful to me.\nWe instead calculate what alerts a particular PR would introduce, and then post a comment with the details if there would indeed be changes. And even then, if alerts are introduced, we don't fail the PR. Custom failure conditions are something we plan to add eventually however.\n(You can see more details here: https://lgtm.com/help/lgtm/using-lgtm-analysis-continuous-integration). Really glad to see that you're taking advantage of the logos now being on shields.io!\nIf you add ?logo=lgtm&logoWidth=18 to the end of the image URL, the LGTM tarsius logo will also be added to the icon. Something to consider? This is what it would look like:\n \nThese are the URLs that we'll be generating for users on the new badge markup generator that will be deployed to lgtm.com very shortly.. ",
    "innjoshka": "Thanks a lot for the help. But it still mention that I was able to get rid of the always creating and not closing new connections after server was closing it only when I moved 2 rows out of the loop and this is strange because it has to do it automatically as it happens using requests but maybe I'm not right\nbrowser = mechanicalsoup.StatefulBrowser(session=True, soup_config={'features': 'html.parser'})\npage = browser.get(url, timeout=10.0)\nSo, I had to do a) move it browser outside the loop b) close broswer manually, browser.close(). But anyway all works as I need. Thanks a lot. \nThe only one question leaved is: With what library (Asyncio , multiprocessing or grequests etc) is better to use asynchronous requests with Mechanicalsoup? And where can I find any realated information? . I've Edited issue almost immediately as you've written comment. At the end of the post: MechanicalSoup version (0.10.0). Is it normal working behaviour?\nWhen I assign browser outside the loop and try open method I get error mechanicalsoup.utils.LinkNotFoundError\n```\nbrowser = mechanicalsoup.StatefulBrowser(session=False, raise_on_404=True,\n                                                 user_agent=headers['User-Agent'],\n                                                 soup_config={'features': 'html.parser'})\ndef fetchfile(query):\n    browser.open(self.url, verify = False, timeout=6.0)   \n    browser.select_form('form[action=\"/submit\"]')\n    browser[\"address\"] = query #by name\n    browser.get_current_form().print_summary()\n    response = browser.submit_selected()\n    return response\nFile \"/Users/Kakadu/Documents/src/script.py\", line 105, in fetchfile\n    browser.select_form('form[action=\"/submit\"]')\n  File \"/Users/Kakadu/anaconda/lib/python3.6/site-packages/mechanicalsoup/stateful_browser.py\", line 207, in select_form\n    raise LinkNotFoundError()\nmechanicalsoup.utils.LinkNotFoundError\n```\nBut at the same time using get method all works fine \nbrowser = mechanicalsoup.StatefulBrowser(session=False, raise_on_404=True,\n                                                 user_agent=headers['User-Agent'],\n                                                 soup_config={'features': 'html.parser'})\ndef fetchfile(query):\n    page = browser.get(self.url, timeout=6.0)\n    search_form = page.soup.select('div.main-input')[0]\n    search_form.select(\"input\")[0][\"value\"] = query\n    response = browser.submit(search_form, self.url)\n    return response. I found what caused error LinkNotFoundError when tested it on http://httpbin.org  . It was because of site on what I tested the code do redirections, so it was immediately change the url to redirected one and because I tested the code in the loop - I got LinkNotFoundError. But didn't find the way how to turn redirection off when using browser.open(). browser.open(allow_redirects=False) didn't help to turn redirection off as well as not give me any errors. But I assume the issue can be closed. Thanks a lot for the help  . ",
    "Tai-Pach": "figured it out!. ",
    "jrobinson-uk": "The URL i'm trying to download the csv from is www.sitename.com/admin/some_long_path/survey_responses\nThe link doesn't contain the actual filename and presumably redirects me somehow. When I click on it in my browser I get a csv download.. Ahhh I see, yes I can open the link, that worked. How do I access the csv file?. Actually, got it\n. ",
    "zen4ever": "Thanks @hemberger, I don't remember exactly how I solve it, but I believe I found the workaround along the lines of your example.. ",
    "LocutusOfBorg": "thanks, I have already uploaded the fix in Ubuntu, and Debian is not yet enforcing autopkgtestsuite to pass, but I'll update the package today there too...\nNo need of a new release, unless you think otherwise!\nbig thanks!. ",
    "graingert": "this causes pytest-runner to be installed as part of the egg info and it bypasses pinned requirements from parent packages.. Hmm. Maybe that should be fixed on the pytest docs.\nIf you released universal wheels it would also fix the problem, at least for everyone that has a pip that supports wheels. https://github.com/MechanicalSoup/MechanicalSoup/issues/225. @hemberger @moy I've updated my PR to match https://github.com/pytest-dev/pytest-runner#conditional-requirement. @hemberger @moy It looks like ci doesn't even use python3 setup.py test. fyi, you don't run python setup.py test in CI, so it might be better to just remove pytest-runner completely. > Instructions have been tested up to the official upload to PyPI. \nI don't see the wheels uploaded. @hemberger wheels for the current release would be best. It's currently blocking us from deploying in certain environments. @hemberger releasing this:  https://github.com/MechanicalSoup/MechanicalSoup/pull/224 as a .post0 would be good too. Thanks, these wheels appear to be working for us\n. I moved it to import sys. I've added this comment now. ",
    "senabIsShort": "Sure does look feasible, I'll make this the next item on my bucket list !. > Thanks for the pull request! I think this is on the right track, but there are a few changes we probably want to make. See the details below for more information.\nAgreed will work on them more this weekend (~in 36h from now.~ oops, life got in the way, lesson learned), prioritizing your second point below !\n\nAlso, being able to run the tests locally makes development much easier. Let's help you get this set up! Did you run these commands (from the contributing page)?\npython3 -m venv .virtual-py3 && source .virtual-py3/bin/activate\npip install -r requirements.txt -r tests/requirements.txt\n\nI ran those 2 and I now understand why it's preferable to use venv ! Didn't run into any problems during install (apart from upgrading pip because it was unable to find bdist_wheel).\nRunning pytest in venv though, that pointed to 3 failures (didn't quite understand them, couldn't look thoroughly). With this force push, I tried to adress : \n\nThe messy commit history ; might get messier in the future, but it doesn't hurt to clean it up\nSubmitting the empty value without creating a tempfile ; felt like the if isinstance(value, string_types) was still necessary, though I think I am changing the behavior when value is not empty and not an instance of string-types\n\nDidn't work on the test yet.. > Much better indeed, but there's a new test failure in Travis-CI. Do go you also get it locally? Can you investigate?\nThe build error is an assertion failure :\nAssertionError: assert 'application/x-www-form-urlencoded' in 'multipart/form-data; boundary=18681345e388f6b641307a603af755c0'\nSo from what I understand, we have multipart/form-data type where we should be having an application/x-www-form-urlencoded\nI just don't see which of my changes could have impacted this \ud83e\udd37\u200d\u2642\ufe0f . So the behavior still stands to be corrected. How is the debate.\nShould test__request be modified in that case ?\nEspecially the assertion that raised the error : since we're going to upload a file even if empty, requests will define the data type as multipart/form-data, as it should be, right?\n\nEdit \nas it should be in the case of a form containing a file input*\n\nAs for the test, I'm leaning towards a parametrized test__request_file, but correct me if I'm wrong : it would still need to be modified in order to account for filename testing.. On the behavior, the Requests 1.0.4 doc (hasn't changed ever since AFAIK) states that you can set filenames and a string of content as a dict element.\nIs this towards what might be a fix ?\n\n\nI think it should, yes. One option is to change the failing assertion. The other is to remove the file input field and consider that file inputs have to be tested somewhere else.\n\nDo you want me to do a simple commit in this PR for it ?\nI would tend to go with Option 2, since wherever there's a file input, it'll be sent as multipart/form-data anyway, and it would allow us to test application/x-www-form-urlencoded on simple forms somewhere.\n\n\nYes, and more importantly, I'm not 100% sure that passing \"\" as file parameter works, since we need to set both the file name and content to \"\", and httpbin doesn't allow us to test that (it only gives the content). So, the code is probably OK, but I'd like to be sure it is before we merge.\n\nShould I move on to the parametrized test__request_file in the meantime (without taking into account the filename in the test) just to have a basis to work on ?\nWon't do it today, but it'll be a bullet point on my task list.\n. I'm really not sure I'd be able to see through such a huge code base and attempt to fix it. Espescially since I still am not really confident in my ability with Python in general.\nI could report the bug but I'm not sure where and how to word it properly, which points I should put emphasis on.. I'll look into it !\nRegarding this PR, do you want me to work towards applying the changes discussed in here ?\n- removing the file field in test__request,\n- parametrizing test__request_file,\n- modifying brower behavior to match files['pic'] = (\"\", \"\"). On this one, I applied all the changes discussed since the last push.\nIs it okay with you guys ?. I probably am misunderstanding this, but doesn't this already send a request to httpbin using a form manufactured and then checking the content of the response ?. Understood, will look into it!. ",
    "CaioFrancisco": "Let's take the login page of hotmail for example. It have input where not have form. How do i select the input and submit it? How do i press buttons outside of a form?\n. ",
    "jsm28": "My understanding as per e.g. https://franklingu.github.io/programming/2017/10/30/post-multipart-form-data-using-requests/ is that while requests doesn't have a feature for specifying the enctype directly, you can force it to use multipart/form-data by passing the non-file form fields as files with None as filename (I haven't tested doing this).. ",
    "noahbarnette": "@arabidopsis This hasn't really been an issue for me but have you considered offering a fix and committing it? If you have a specific use case where this is a big issue, you may be more knowledgeable as to how it should best be implemented than others.. ",
    "Pimmetje": "Never trust user input :)\nI could not find any hard reference but this one comes close to what i think is right https://stackoverflow.com/a/19808575/559333. Your right: That asks for more research.\n\"Attribute names are case-insensitive, but attribute values may be case-sensitive.\" [1]\nSo i guess you could be right. \nIf i read [2] i get the feeling this reference is more like u have to use the same casing in the document. But i can't find a reference stating the case requirements for basic attributes. So i think it's best to assume they could be anything.\n[1] http://www.htmlhelp.com/reference/html40/structure.html#attributes\n[2] http://www.htmlhelp.com/reference/html40/values.html. I could agree that this isn't be the best place to fix it. Now i know the issue i can work around it in my code. \nThe question is what is a good case to report on bs4.element.Tag that would fix the issue?. ",
    "rationa-kunal": "I would like to work on this issue !. I was thinking of adding an extra boolean variable in Form class as dev_option. If that is set to True then we can edit disabled values and if it is False we cant edit it.\nfor example .\npython\nif i.has_attr(\"disabled\") and not self.dev_op:\n    raise LinkNotFoundError(\"This is disabled field you have to set dev_op to true to edit this field \" + `name). @moy  i didnt understand your previous reply, can you please simplify\nthis is my pr with my original idea\n254 . So the idea is to just not edit the disabled inputs .... And drop the dev_op option !\nThat would be fairly simple !. How can I add test case for this issue ?\nThere is no disabled form field in httpbin !. ",
    "Quetzalcohuatl": "There are some (disabled) fields that I want to edit, so leaving the option to edit them would be appreciated. This is desired by more than just me (https://stackoverflow.com/questions/9249996/mechanize-cannot-read-form-with-submitcontrol-that-is-disabled-and-has-no-value). You're right. I post because I want to emphasize that I moved from mechanize to this library specifically for that feature. So if an edit comes, it should definitely be optional to ignore disabled elements.. Your code @hemberger is a good solution to me. Didn't realize it was so simple.. ",
    "tiboroche": "\nThanks for the PR. You still have 2 flake8 issues to fix:\n```\n/home/travis/build/MechanicalSoup/MechanicalSoup/mechanicalsoup/browser.py:235:-341: W605 invalid escape sequence '*'\n/home/travis/build/MechanicalSoup/MechanicalSoup/mechanicalsoup/browser.py:235:-339: W605 invalid escape sequence '*'\n\nI am very surprised about those. When I run the tests on the current MechanicalSoup master, I have the same errors. Do you have any input on those ? Maybe a confiugration problem ? \n\nAlso, we prefer clean history, so if possible make a git rebase -i to squash the commits together. If you don't, no problem: we'll squash on merging.\n\nOk I will do this and work on some testing. . I close this PR as I did a new one with the clean history + testing + rebase (#252). > Thanks for the PR @tiboroche! I did a tiny bit of cleanup and then added a little more documentation in a fixup commit.\nYou are welcome. I am implementing MechanicalSoup at work for automated testing, and it is working great so far. I may contribute again soon depending on the bugs I encounter or the features I would like to add. \n. > Hi, and thanks for the pull request!\n\nSo if I understand this correctly, the case you want to fix is this:\npython\nbr.find_link(None, url_regex=\"/something\")\n\nNot exactly. In my code I had a line like this : \npython\n browser.download_link(url_regex='/admin/https/csr', file='/vagrant/local.csr')\nwhich would call \npython\n self._find_link_internal(None, url_regex='/admin/https/csr')\nin this case, the code would set up the url_regex keyword to None (the value of link), thus downloading a \"random\" link from the page instead of the one I wanted.\nI think that the two following calls should have the same result, which seems coherent to the documentation in my opinion. \n```python\n self.follow_link(url_regex='/admin/https/csr')\n self._find_link_internal(None, url_regex='/admin/https/csr')\nself.follow_link('/admin/https/csr')\n self._find_link_internal('/admin/https/csr')\n```\nAnd this is what is fixed with my patch. \nI can make the change with the elif and add a test if you agree with my fix. . I made the requested changes.. I messed up the commits squashing, so I opened a new PR #272 with a single \"clean\" commit. Therefore I close this one.. ",
    "the-allanc": "\nDo you want to try a patch for that?\nDepends what you mean.\n\nDo I want to try a patch that you've written? Sure.\nDo I want to write my own patch? Err... I could do, but not entirely sure when I'd get round to it.. So I made an attempt to fix it - and writing a test was the more complicated part. In particular, it's harder to process multiform data in a sensible way.\nAs a suggestion (after a quick look here), I'm thinking that it would be best to make it easier to process multipart data in one of two ways:\n\nUse requests_toolbelt to inspect the submitted data; or\nMake it easier to write basic WSGI apps for us to use in the test suite (so we can see how that app handles the submitted output). I'm thinking of using something like Pyriform (full disclosure - I'm the author of that library) to make it easier to connect MechanicalSoup to the WSGI app directly. Then we could use any web framework to accept responses that we send to it, and we would have to explicitly mock less.\n\nEither approach would require adding new dependencies - so I'm interested in what way you think is best to proceed.. ",
    "StevenLooman": "Thank you for the effort! And thank you for writing MechanicalSoup.. I've added Pimpler/muppy to track and print, using the tracker module. I'll let it run for a while and report any findings.\nBefore, though, I added page.decompose(). This seemed to help a bit, as it has been running for a few days without running out of memory. It was not completely solved though, it still had consumed about 17% of 1024MB of memory.. After running for a while, I get this:\n2018-12-02 21:52:31,777:ubee:DEBUG:Tracker:\n                                    types |   # objects |   total size\n========================================= | =========== | ============\n                             <class 'list |        7366 |      4.51 MB\n                              <class 'str |        7353 |    343.28 KB\n                             <class 'dict |         417 |     55.68 KB\n      <class 'bs4.element.NavigableString |         145 |     10.29 KB\n                              <class 'int |         690 |      9.53 KB\n                  <class 'bs4.element.Tag |         196 |      6.12 KB\n                <class 'collections.deque |           4 |      1.25 KB\n          <class 'collections.OrderedDict |           5 |      1.21 KB\n                              <class 'set |           3 |    348     B\n              <class 'bs4.element.Comment |           5 |    318     B\n             <class 'lxml.etree._LogEntry |           6 |    312     B\n                           <class 'method |           7 |    252     B\n       <class 'builtin_function_or_method |           6 |    240     B\n                      function (<lambda>) |           2 |    144     B\n  <class 'lxml.etree._TargetParserContext |           1 |    140     B\n2018-12-02 22:04:28,836:ubee:DEBUG:Summary:\n                                types |   # objects |   total size\n===================================== | =========== | ============\n                         <class 'list |      432944 |    146.73 MB\n                          <class 'str |      451891 |     21.85 MB\n                         <class 'dict |       27782 |      4.44 MB\n                          <class 'int |       43693 |    611.58 KB\n  <class 'bs4.element.NavigableString |        8274 |    589.20 KB\n                         <class 'type |         904 |    476.30 KB\n                         <class 'code |        5884 |    459.94 KB\n              <class 'bs4.element.Tag |       11340 |    354.38 KB\n                          <class 'set |        1118 |    199.40 KB\n                        <class 'tuple |        3024 |    111.13 KB\n                      <class 'weakref |        2120 |     91.09 KB\n      <class 'collections.OrderedDict |         307 |     79.75 KB\n            <class 'collections.deque |         237 |     74.06 KB\n             <class '_sre.SRE_Pattern |         162 |     62.52 KB\n   <class 'builtin_function_or_method |        1511 |     59.02 KB\nThe Tracker is a report from the tracker module, i.e., reporting differences between two calls.\nThe Summary report is a report from the summary module.\nThis is reported every loop, every 30 seconds.\nA tracker/summary report from a bit earlier, for additional information:\n2018-12-02 13:24:01,256:ubee:DEBUG:Tracker:                                                  \n                                    types |   # objects |   total size                                                                                                                              \n========================================= | =========== | ============\n                             <class 'list |        7366 |      1.50 MB  \n                              <class 'str |        7353 |    343.28 KB                                                      \n                             <class 'dict |         415 |     55.48 KB                       \n      <class 'bs4.element.NavigableString |         143 |     10.15 KB                                                                                                                              \n                              <class 'int |         680 |      9.38 KB\n                  <class 'bs4.element.Tag |         196 |      6.12 KB  \n                <class 'collections.deque |           4 |      1.25 KB                                                      \n          <class 'collections.OrderedDict |           5 |      1.21 KB                       \n                              <class 'set |           3 |    348     B                                                                                                                              \n              <class 'bs4.element.Comment |           5 |    318     B\n             <class 'lxml.etree._LogEntry |           6 |    312     B  \n                           <class 'method |           7 |    252     B                                                      \n       <class 'builtin_function_or_method |           6 |    240     B                       \n                      function (<lambda>) |           2 |    144     B                                                                                                                              \n  <class 'lxml.etree._TargetParserContext |           1 |    140     B\n2018-12-02 13:25:29,889:ubee:DEBUG:Summary:                             \n                                types |   # objects |   total size                                                          \n===================================== | =========== | ============                           \n                         <class 'list |      116430 |     16.81 MB                                                                                                                                  \n                          <class 'str |      135936 |      7.44 MB    \n                         <class 'dict |       10324 |      2.16 MB      \n                         <class 'type |         904 |    476.30 KB                                                          \n                         <class 'code |        5884 |    459.94 KB                           \n                          <class 'int |       14230 |    204.89 KB                                                                                                                                  \n                          <class 'set |         989 |    184.79 KB    \n  <class 'bs4.element.NavigableString |        2288 |    162.44 KB    \n                        <class 'tuple |        3024 |    111.13 KB    \n              <class 'bs4.element.Tag |        3136 |     98.00 KB                           \n                      <class 'weakref |        2077 |     89.25 KB                                                                                                                                   \n             <class '_sre.SRE_Pattern |         162 |     62.52 KB    \n           <class 'wrapper_descriptor |        1258 |     54.05 KB    \n   <class 'builtin_function_or_method |        1253 |     48.95 KB    \n                  <class 'abc.ABCMeta |          75 |     37.42 KB\nI noticed the refbrowser module and will try to use that to gather additional information.. Please do note that I have changed the initial code-listing: The results are now stored in a variable and then returned. I doubt this has any impact, but just in case.... Yesterday I did some more measurements. Every run polls the router 200 times.\nThis is the code used:\n```\ndef poll_router() -> List[str]:\n    #return []  # XXX: mem leak test\n# open browser\nbrowser = mechanicalsoup.StatefulBrowser(...)\nbrowser.open(UBEE_ADDRESS)\n#return []  # XXX: mem leak test\n\n# login\nbrowser.select_form('form[name=\"loginMR3\"]')\nbrowser['loginUsername'] = 'xxx'\nbrowser['loginPassword'] = 'xxx'\nresp = browser.submit_selected()\n#return []  # XXX: mem leak test\n\n# go to wlan access page\nbrowser.open_relative('/wlanAccess.asp')\n#return []  # XXX: mem leak test\n\n# extract associated clients\npage = browser.get_current_page()\ntds = page.find_all(has_mac_addr)\naddresses = [td.text.lower() for td in tds]\n# page.decompose()  # XXX: mem leak test\n\nreturn addresses\n\ndef main():\n    from pympler import tracker\n    tr = tracker.SummaryTracker()\n    for i in range(1, 201):\n        poll_router()\n        gc.collect()\n    tr.print_diff()\n    all_objects = muppy.get_objects()\n    sum1 = summary.summarize(all_objects)\n    summary.print_(sum1)\n```\nThe parts commented with XXX: mem leak test are enabled/disabled. Some parts omitted for brevity, such as browser reuse.\nThe things I've tested, with the results.\nfull run, page.decompose() at end\n2018-12-04 22:45:23,053:ubee:DEBUG:Summary:\n                               types |   # objects |   total size\n==================================== | =========== | ============\n                         <class 'str |       28205 |      2.51 MB\n                        <class 'dict |        9111 |      1.92 MB\n                        <class 'type |         904 |    476.30 KB\n                        <class 'code |        5884 |    459.94 KB\n                        <class 'list |        6484 |    335.13 KB\n           <class 'collections.deque |         805 |    251.56 KB\n     <class 'collections.OrderedDict |        1017 |    251.14 KB\n                         <class 'set |        1343 |    224.89 KB\n                       <class 'tuple |        3024 |    111.13 KB\n                     <class 'weakref |        2282 |     98.05 KB\n  <class 'builtin_function_or_method |        2363 |     92.30 KB\n                         <class 'int |        4772 |     75.98 KB\n            <class '_sre.SRE_Pattern |         162 |     62.52 KB\n          <class 'wrapper_descriptor |        1258 |     54.05 KB\n                 <class 'abc.ABCMeta |          75 |     37.42 KB\nfull run, no page.decompose() at end\n2018-12-04 22:41:36,231:ubee:DEBUG:Summary:\n                                types |   # objects |   total size\n===================================== | =========== | ============\n                         <class 'dict |       87300 |     12.21 MB\n                          <class 'str |       74041 |      3.80 MB\n                         <class 'list |       56340 |      2.79 MB\n  <class 'bs4.element.NavigableString |       28944 |      2.01 MB\n              <class 'bs4.element.Tag |       39396 |      1.20 MB\n                         <class 'type |         904 |    476.30 KB\n                         <class 'code |        5884 |    459.94 KB\n            <class 'collections.deque |         805 |    251.56 KB\n      <class 'collections.OrderedDict |        1017 |    251.14 KB\n                          <class 'set |        1544 |    247.66 KB\n                        <class 'tuple |        3024 |    111.13 KB\n                      <class 'weakref |        2282 |     98.05 KB\n   <class 'builtin_function_or_method |        2363 |     92.30 KB\n                          <class 'int |        4789 |     76.23 KB\n             <class '_sre.SRE_Pattern |         162 |     62.52 KB\nfull run, page.decompose() at end, browser reused\n2018-12-04 22:36:49,827:ubee:DEBUG:Summary:\n                               types |   # objects |   total size\n==================================== | =========== | ============\n                         <class 'str |       25641 |      2.41 MB\n                        <class 'dict |        4100 |      1.35 MB\n                        <class 'type |         904 |    476.30 KB\n                        <class 'code |        5884 |    459.94 KB\n                        <class 'list |        5940 |    319.92 KB\n                         <class 'set |         944 |    179.69 KB\n                       <class 'tuple |        3024 |    111.13 KB\n                     <class 'weakref |        2082 |     89.46 KB\n                         <class 'int |        4315 |     68.16 KB\n            <class '_sre.SRE_Pattern |         162 |     62.52 KB\n          <class 'wrapper_descriptor |        1258 |     54.05 KB\n  <class 'builtin_function_or_method |        1163 |     45.43 KB\n                 <class 'abc.ABCMeta |          75 |     37.42 KB\n           <class 'getset_descriptor |         935 |     36.52 KB\n           <class 'method_descriptor |         873 |     34.10 KB\nfull run, no page.decompose() at end, browser reused\n2018-12-04 20:41:32,418:ubee:DEBUG:Summary:\n                               types |   # objects |   total size\n==================================== | =========== | ============\n                         <class 'str |       25406 |      2.41 MB\n                        <class 'dict |        3711 |      1.30 MB\n                        <class 'type |         904 |    476.30 KB\n                        <class 'code |        5884 |    459.94 KB\n                        <class 'list |        5684 |    307.00 KB\n                         <class 'set |         943 |    179.57 KB\n                       <class 'tuple |        3024 |    111.13 KB\n                     <class 'weakref |        2082 |     89.46 KB\n                         <class 'int |        4310 |     68.10 KB\n            <class '_sre.SRE_Pattern |         162 |     62.52 KB\n          <class 'wrapper_descriptor |        1258 |     54.05 KB\n  <class 'builtin_function_or_method |        1163 |     45.43 KB\n                 <class 'abc.ABCMeta |          75 |     37.42 KB\n           <class 'getset_descriptor |         935 |     36.52 KB\n           <class 'method_descriptor |         873 |     34.10 KB\nrun till page wlan access, no extraction of clients\n2018-12-04 22:49:53,226:ubee:DEBUG:Summary:\n                                types |   # objects |   total size\n===================================== | =========== | ============\n                         <class 'dict |       87099 |     12.19 MB\n                          <class 'str |       74041 |      3.80 MB\n                         <class 'list |       56339 |      2.79 MB\n  <class 'bs4.element.NavigableString |       28743 |      1.99 MB\n              <class 'bs4.element.Tag |       39396 |      1.20 MB\n                         <class 'type |         904 |    476.30 KB\n                         <class 'code |        5884 |    459.94 KB\n            <class 'collections.deque |         805 |    251.56 KB\n      <class 'collections.OrderedDict |        1017 |    251.14 KB\n                          <class 'set |        1544 |    247.66 KB\n                        <class 'tuple |        3024 |    111.13 KB\n                      <class 'weakref |        2281 |     98.01 KB\n   <class 'builtin_function_or_method |        2363 |     92.30 KB\n                          <class 'int |        4789 |     76.25 KB\n             <class '_sre.SRE_Pattern |         162 |     62.52 KB\nrun till page logged in\n2018-12-04 22:58:20,295:ubee:DEBUG:Summary:\n                                types |   # objects |   total size\n===================================== | =========== | ============\n                         <class 'dict |      173876 |     21.78 MB\n  <class 'bs4.element.NavigableString |       93041 |      5.60 MB\n                          <class 'str |      114205 |      5.30 MB\n                         <class 'list |       77814 |      4.02 MB\n              <class 'bs4.element.Tag |       60870 |      1.86 MB\n                         <class 'type |         904 |    476.30 KB\n                         <class 'code |        5884 |    459.94 KB\n      <class 'collections.OrderedDict |        1017 |    279.41 KB\n            <class 'collections.deque |         805 |    251.56 KB\n                          <class 'set |        1544 |    247.66 KB\n          <class 'bs4.element.Comment |        1809 |    166.06 KB\n                        <class 'tuple |        3024 |    111.13 KB\n                      <class 'weakref |        2281 |     98.01 KB\n   <class 'builtin_function_or_method |        2363 |     92.30 KB\n                          <class 'int |        4790 |     76.25 KB\nrun till page login\n2018-12-04 23:01:37,280:ubee:DEBUG:Summary:\n                                types |   # objects |   total size\n===================================== | =========== | ============\n                         <class 'dict |       40064 |      5.69 MB\n                          <class 'str |       48102 |      3.15 MB\n                         <class 'list |       22572 |      1.11 MB\n  <class 'bs4.element.NavigableString |       14673 |    860.33 KB\n                         <class 'type |         904 |    475.83 KB\n                         <class 'code |        5884 |    459.94 KB\n              <class 'bs4.element.Tag |        9045 |    282.66 KB\n            <class 'collections.deque |         805 |    251.56 KB\n      <class 'collections.OrderedDict |        1017 |    251.14 KB\n                          <class 'set |        1544 |    247.66 KB\n          <class 'bs4.element.Comment |        1608 |    201.20 KB\n                        <class 'tuple |        3024 |    111.13 KB\n                      <class 'weakref |        2259 |     97.07 KB\n   <class 'builtin_function_or_method |        2363 |     92.30 KB\n                          <class 'int |        4790 |     76.27 KB\nrun without any work\n2018-12-04 23:02:54,119:ubee:DEBUG:Summary:\n                               types |   # objects |   total size\n==================================== | =========== | ============\n                         <class 'str |       25322 |      2.40 MB\n                        <class 'dict |        3676 |      1.29 MB\n                        <class 'type |         902 |    469.77 KB\n                        <class 'code |        5877 |    459.40 KB\n                        <class 'list |        5653 |    305.05 KB\n                         <class 'set |         941 |    179.35 KB\n                       <class 'tuple |        3021 |    110.93 KB\n                     <class 'weakref |        2028 |     87.14 KB\n                         <class 'int |        4302 |     67.99 KB\n            <class '_sre.SRE_Pattern |         162 |     62.52 KB\n          <class 'wrapper_descriptor |        1258 |     54.05 KB\n  <class 'builtin_function_or_method |        1157 |     45.20 KB\n                 <class 'abc.ABCMeta |          75 |     36.55 KB\n           <class 'getset_descriptor |         932 |     36.41 KB\n           <class 'method_descriptor |         873 |     34.10 KB\nI saw Garbage collection in Python which makes a graph. I hope that would help, but given the number of objects...\nAny advice on how to proceed?. I am unable to find anything. Re-using the browser is a work-around. Closing the issue, thank you for the support and hints.. ",
    "clipka": "Oooh. I triple checked my input (and changed my pw in between)... and suddenly it works... Sorry for annoying you. And thanks for your support.\nNow it's time to explore the module :-). ",
    "facelessuser": "Hi,  I'm the author of SoupSieve, the new library that provides BeautifulSoup 4.7.0+ CSS selector support. I've been checking out issues now and then on GitHub that have \"soupsieve\" in them in order to get feedback that maybe isn't making it back to me. I've been learning a lot through this process as to how quirky the old select method was.\nJust here to confirm that SoupSieve does return things in document order, and that behavior can be relied on moving forward.\nIt seems most people who use BeautifulSoup have grown to rely on find and find_all instead of select and select_one,  which I can't blame them as the old select was very unreliable and quirky, but hopefully, for those who do use select they will find the behavior very solid and feature rich moving forward.\nThe big differences are that it it does return things in document order and selectors should follow the CSS spec in regards to syntax and usually in behavior as much as possible in a non-browser environment. There is no longer a restriction in selector complexity, and in HTML and XHTML documents, it will mimic browser behavior, for example: type attributes are treated case insensitively except when the CSS4 syntax for forced sensitivity is used [type=submit s].\nAs with most projects, I assume MechanicalSoup relies heavily upon find and find_all, so the ordering test is most likely the biggest impact that should be experienced, but if this project happens to stumble on some issues, please do file an issue.. No problem.  The ordering difference was something new to me.  I guess I had just assumed the old select method followed the logical flow of iterating the descendants, but I guess not. So I guess an unexpected bonus \ud83d\ude42 .\n. For HTML, you shouldn't need to specify the i, but specifying it will ensure it is case insensitive.  type values are treated as insensitive for HTML documents by default as that is what browsers do. It definitely doesn't hurt including the i.. It was a bug. I was testing it against HTML5. We were checking if the document was under the html namespace, but that only applies to HTML5 and XHTML. When using the default parser or the lxml html parser, there is no namespace applied to the document. And this is why I'm poking around \ud83d\ude42 .. I have a fix for type selectors here: https://github.com/facelessuser/soupsieve/pull/88.. Soup Sieve 1.7.2 has been released and fixes the insensitive issue. To avoid these issues in the future, we now iterate through all the parsers on each test (except where it doesn't make sense).  As this issue is deferred for you, you should have what you need when you get around to actually including the changes. But you are always free to use the i flag regardless.. ",
    "ricecooker": "I added a test for the bug so coverage should be 100%. I ran the test against master and the test failed because a LinkNotFoundError was raised because of duplicates.. > On overall, this looks good to me (see one minor point about test naming and another about a comment).\n\nTwo minor issues before we can merge:\n\nThis is a 2-commits PR, with one commit failing continuous integration. Each commit should pass. In this case, I see no reason to keep commits separate, so squashing them together is probably the simplest.\nThe commit messages do not explain the problem, nor justify why the patch is a good solution to the problem.\n\nYou can address both points by using git rebase -i, squashing the commits and use r to reword the commit message (or git commit --amend).\n\nSquashed into one commit.. bump. i added more color in the commit message. please let me know if there's anything more to be done.. Renamed to test_duplicate_submit_buttons. Added your suggested comment. ",
    "matheuscas": "\nThe .x and .y attributes should ideally be set on submission, and they currently are not.\nI'm OK with merging a version that doesn't support .x and .y, but it would be nice to have tests showing the current behavior (with proper comments explaining why it is like this). At least, the commit message should explain it.\n\nI didn't quite get the .x and .y thing. That's why I focused only on the image attribute type. . > Thanks for your PR. Minor comments about the commit message:\n\n\nAvoid past tens (\"Added ...\"). We usually use imperative tone (\"Add ...\" as in \"Please, dear computer, add feature ...\")\nInclude the string \"Fixes #201\" in the commit message. This way, the issue will be closed when the PR is merged, and the issues will be cross-linked.\n\n\nShould I change anything here?. Because I basically replicated this same test, but for \"image\" type. . Take a look. . I think I got what u mean. This test could be an input type submit and another input type image and making sure that the input image would be picked - with different names, of course. What do you think? . ",
    "jwodder": "@hickford In Python 2, this is necessary to make Browser a new-style class instead of an old-style class.  (In Python 3, all classes are new-style, so explicitly inheriting from object is superfluous.)\n. ",
    "eli-schwartz": "Not really true, this was always more of the \"patches welcome\" variety. And mechanize has finally been updated to support python3 on master. . "
}
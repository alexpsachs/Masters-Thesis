{
    "philips": "/cc @eyakubovich Want to look into this? \n. @marineam I would say we have a few things to benchmark before making a decision on v4 vs v6:\n1) Measure throughtput and latency of tinc vs openvpn vs unencapsulated connections.\n2) Measure the CPU/memory considerations for tinc vs openvpn.\n2) Measure throughput and latency of v4 vs v6 of tinc vs openvpn.\n. Another thing Alex and I talked about: using 6to4 instead. https://gist.github.com/rmoriz/6d5e6649ab2f5588fdbe\n. @marineam Having this sort of encapsulation should be much lighter weight than a full VPN and we sort of push stuff towards ipv6 naturally too. The downside of everyone having the same network view is noted but we could then fall back to a site to site vpn instead.\n. Add docs please :)\n. One thing I can't figure out is do you have to setup the route per mac\naddress or can you assign a subnet as we can do with our userspace thing\nnow?\nOn Sun, Aug 24, 2014 at 10:34 PM, Eugene Yakubovich \nnotifications@github.com wrote:\n\nI didn't know that there are alternate ways to manipulate the virtual MAC\n-> VTEP IP entries (bridge utility, netlink, unicast UDP). That does make\nit possible to use it without multicast. There's still extra space overhead\nwith this solution: 14 bytes Ethernet header + 8 bytes VXLAN header but\nit's not a show stopper.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos-inc/rudder/issues/18#issuecomment-53228183.\n. lgtm\n. @eyakubovich I know a lot of stuff is changing but just FYI on the commit message format: https://github.com/coreos/rudder/blob/master/CONTRIBUTING.md#format-of-the-commit-message :)\n. @eyakubovich you can fix them up with the edit flag on git rebase -i\n. LGTM\n. /cc @pquerna\n. The second option seems reasonable. I feel that we can lean on systemd-networkd to configure this network route after we bring up the flannel bridge.\n\n@eyakubovich probably has an opinion on this one.\n. I would prefer if this was a subcommand or separate tool that takes the flannel configuration and generates environment variables for docker. Something like the systemd generators. So you could be After=flannel-generate-docker or whatever.\n. Would it make sense to have a \"flannel\" plugin type that read this directly? Or just have flannel output this configuration directly?\n. Ha! Thanks rimusz. @eyakubovich \n. huh, what is this for?\n. hey @greenpau it would be nice to put this into a hacking doc something like we have for rkt: https://github.com/coreos/rkt/blob/master/Documentation/hacking.md\nCould you do that and create a CentOS section?\n. Hello @jayunit100 I am going to close this out. Please re-open if you have a chance to work on it.\n. @danielschonfeld Any chance you can help ?\n. Closing as https://github.com/coreos/etcd/issues/3260 is closed.\n. @MohdAhmad do you have any progress updates on this?\n. @jonboulle we are using github.com/coreos/pkg/capnslog everywhere else, right?\n. @tdeheurles I am going to close this out because it isn't really a flannel issue but a filesystem issue with docker toolbox. \n. @eyakubovich We would have to track this state outside of flannel on disk somewhere, right? There is no way to tag routes with metadata like \"this was added by flannel\".\n. systemctl cat flanneld.service. Perhaps something is misconfigured? Otherwise, try kicking flannel: systemctl restart flanneld.service.\n. Ping @jeremyeder and @apeeyush. What is the status?\n. @resouer Any update? I am going to close this out. But, please reopen if this didn't solve your issue.\n. @ztao1987 Nice! We plan to make github.com/coreos/coreos-kubernetes default to iptables soon.\n. @jeremyeder Is this the same as this? https://github.com/coreos/flannel/pull/397\n. @jonboulle since https://github.com/coreos/go-iptables/pull/12 is merged can we just bump the dependency to move forward?\n. Hey @dcbw how is the rework going?\n. Hrm, we haven't had any particular reports of this issue and 0.5.5 has been the default since Dec. 8th 2015. Can you show me how you are running it? The exact flags.\n. What environment did you deploy this in? Can you try dumping the packets between the hosts? Is there anything in the dmesg or flanneld logs? Can you try running some simple TCP service between hosts instead of ping?\n. @luxas Yea, that should work. Do you want to give us a PR that makes this change? I just tried it and it works.\n```\ndiff --git a/build b/build\nindex f56cabf..7973006 100755\n--- a/build\n+++ b/build\n@@ -18,7 +18,7 @@ function linker_dashX {\n ORG_PATH=\"github.com/coreos\"\n REPO_PATH=\"${ORG_PATH}/flannel\"\n VERSION=$(git describe --dirty)\n-GLDFLAGS=\"-X $(linker_dashX github.com/coreos/flannel/version.Version ${VERSION})\"\n+GLDFLAGS=\"-X $(linker_dashX github.com/coreos/flannel/version.Version ${VERSION}) -extldflags '-static'\"\nif [ ! -h gopath/src/${REPO_PATH} ]; then\n        mkdir -p gopath/src/${ORG_PATH}\n```\n. @gangadhars Any update? I am going to close this out for now. Please re-open if you can still reproduce.\n. @yogeshmsharma I don't see what your question is. Can you please provide a way for me to reproduce or a deeper explanation of what you are doing and what you expected to happen? Most importantly I need to understand how flannel is involved here.\nI am closing this for now as it is unclear where a flannel bug exists.\n. Is there a reason you don't just bond the two interfaces? I am confused why you want to have two interfaces in use.\n. Sorry about this. I am taking a look at this now.\n. Fixed via https://github.com/coreos/flannel/commit/6c95ca9086cf7f5fc6e1d415fb23c50aeec34c04\n. @okamototk did you get a chance to bump this? \nAlso, is UDPCSum = true a safe default?\n. Thank you for the fix! I submitted it directly to master after cleaning up the commit message a bit: https://github.com/coreos/flannel/commit/6c95ca9086cf7f5fc6e1d415fb23c50aeec34c04\n. Oops, I meant to file this on locksmith.\n. this seems fine\n. @hustcat I am missing why only this field needs this. Can you explain it?\n. It is not abandoned. We don't have a dedicated maintainer at the moment and are working on fixing that. I will dedicate some time over the next few days to review and cleanup the issues. Sorry for the delay on reviewing your contribution!\n. I am going to close this for now. \n. @robdaemon two commits would be helpful for review. One for the library code and a second one for the changes to flannel.\nAlso, would you maintain this overtime if the code gets merged?\nDo you have more info on how this is being used or the motivation? Are you also able to test/maintain the etcd port to Windows? I assume you are also running etcd on Windows.\n. Sorry for the delay, I was traveling.  \nI am OK with this approach overall. I would want two things:\n1. Can you write a couple of paragraphs on the theory of operation for users to read and developers to understand the flow?\n2. Can you work out an alternative to read/writing the CNI configuration through the manager? Even reading the discussion I don't understand why that needs to be done.\n. @aaronlevy side car seems OK to me.  Although, we shouldn't do a copy like that but instead we shoudl copy to .tmp-$random-config, then move\n. I know this sucks but any chance we could move this PR to the new client to slim dependencies? https://github.com/kubernetes/client-go\n. @tomdee what does flannel link against?\n. seems \"static\" enough for most linux systems. Ship it and lets see :) Just make it super clear in the ChangeLog\n. @tomdee Hrm, good idea. I will dig through that.. @tomdee https://github.com/coreos/flannel/blob/master/backend/vxlan/device.go#L85\n. @tomdee @aaronlevy the instructions in the original posting were wrong, I fixed them and I am investigating deeper now.. Changing the backend to UDP causes the same behavior! This is useful information!\nIt means we can likely point our fingers at @mikedanese :-P (love you mike). @tomdee new PR removes just the broadcast route. Think that is OK?. Overall, LGTM. LGTM, merge it.. LGTM\nnit: would have been easier to review if the rename and logs were in separate commits.. @tomdee @aaronlevy not removing local routes seems to make the tests pass. Need to investigate more.. LGTM. Posted to netdev asking about why the Kernel does this: http://marc.info/?l=linux-netdev&m=148132848222491&w=2. I don't think we should have an etcd v3 backend. Most of the users of Flannel should be using the Kubernetes subnet manager.. +1 to the removal!. We are going to remove the remote API in #606. So, I am going to close this issue.. Where is this taint documented? . @andrewrynhard @aveshagarwal would one of you please email https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle and https://groups.google.com/forum/#!forum/kubernetes-sig-networking discussing this issue?\nThanks!. How are you running flannel? Are you using the etcd or kubernetes backend?. Instead of libraries and tools it should be integrations. So, for example Kubernetes should be listed.. Do you plan on using Kubernetes with this?\nThe last time I looked at this the flannel code was fork/exec'ing the swan code. One thing I would love to see is if swan could be put in a pod alongside flanneld for Kubernetes.. cc @tomdee . cc @luxas @jbeda @lukemarsden. Gah. Needs a rebase now.\n@tomdee does this LGTY?\n@mikedanese can you rebase?. This LGTM but I haven't tested it.. Pretty neat. Cool :). This is used to generate https://coreos.com/legal/open-source/. What environment (aws, azure, etc)? Does ethtool -K eth0 tx off sg off tso off help? . Here are tcpdumps of TX on and TX off: https://gist.github.com/philips/d680458e7cc2a36b16989a20df5ae0c2. This is a \"normal\" tcpdump from this machine to a cloud CDN for comparision: https://gist.github.com/philips/e3acde0cda317ec134900323cc5d924b#file-gistfile1-txt-L44-L55. please just use coreos.com\n. you may want to initialize this as a global in an init() section to avoid making a function call on every networkorder swap.\n. Also, generally Go avoids verbs like \"To\" and just says \"NetworkOrder()\"\n. It might make more sense to return a struct with the MTU and IP instead of trying to rely on multi-return.\n. generally I like to see wg.Add(1) above each go func but this is fine.\n. This is a little hairy, can we break up this into some smaller functions?\n. path.Join is preferable here\n. path.Join here too.\n. can be struct{} instead of bool\n. can be struct{} instead of bool.\n. can be chan struct{}\n. in Go acronyms are all upper case so newVXLANDevice.\n. Eh, this should be in a seperate package with build windows tags\n. delete will return EEXIST?!\n. why rename ?\n. Please hold the rename for a separate commit or PR.\n. nit: newline above would be nice.. log output would be nice here.. why 10 seconds?. I am always skeptical of random length channels, why do you need a buffered channel?. We should probably point people to the Tectonic Installer as it installs flannel by default and isn't alpha like kubeadm. Pointing people to kubeadm as well is OK but we have seen very few people using kubeadm as it is more of a toolkit and not a tool for getting from zero to hero with Kubernetes quickly.. Maybe the language should be:\nMany Kubernetes deployment tools use flannel by default today. If you want the fastest possible experience use the Tectonic Installer, $SOMETHING, etc.\nTo learn more about how to deploy flannel manually and how it works on Kubernetes see the separate document that uses the (currently alpha) Kubernetes installer toolkit kubeadm.. ",
    "eyakubovich": "Yes. I will after label query stuff.\n. Should I close this PR, break up the commit (I'll have to force push) and then create a new PR?\n. Implemented in https://github.com/coreos-inc/kolach/pull/7\n. It does not so it's safe to use.\n. Implemented in 8661e6b383c90534862b27ab4343da81d0c1233d\n. @jpstrikesback a VPC is about isolation -- making different tenants not run into each other for example in IP space. It also provides security in the sense that one tenant cannot snoop on another's traffic. But it doesn't offer encryption so if a user doesn't trust AWS not to intercept traffic at switches and inspect it, VPC doesn't help.\nIn practice, if you can't trust your cloud provider, network level encryption is not enough. However IPSEC in flannel can be useful in cases where a cluster spans across untrusted networks.\n. @MohdAhmad is currently working on IPSec backend so stay tuned.\n. @diranged Yes, we have a PR ready: https://github.com/coreos/flannel/pull/290\nPlease help by reviewing it and providing feedback.\n. Fixed in https://github.com/coreos-inc/rudder/pull/17\n. Fixed in https://github.com/coreos-inc/kolach/pull/13\n. @bcwaldon Done. Kolach is now Rudder but still under coreos-inc\n. I didn't know that there are alternate ways to manipulate the virtual MAC -> VTEP IP entries (bridge utility, netlink, unicast UDP). That does make it possible to use it without multicast. There's still extra space overhead with this solution: 14 bytes Ethernet header + 8 bytes VXLAN header but it's not a show stopper.\n. LGTM\n. Fixed in https://github.com/coreos/rudder/pull/31\n. LGTM. @YungSang : thank you for the bug fix.\n. LGTM\n. Implemented in 5f6f8467f6275a57671f552bbee03ba0fc7c5fe4\n. @philips @jonboulle I think I addressed the concerns. Except for correcting commit msgs -- I'll try to behave next time.\n. Fixed in https://github.com/coreos/flannel/pull/45\n. Good catch. Thank you for fixing it.\n. flannel is provided as Docker image in quay.io:\ndocker pull quay.io/coreos/flannel:0.1.0\n. @superstructor You're absolutely right that it needs to be started before docker daemon. If you're running CoreOS, we have recently put support for flannel into our alpha channel. The documentation is somewhat scarce at the moment and we're working to fix that (some info is available here: https://coreos.com/docs/cluster-management/setup/cloudinit-cloud-config/). But since alpha image now includes flanneld.service, you should be able to run flannel by adding the following to cloud-config:\ncoreos:\n  units:\n    - name: flanneld.service\n      command: start\nJust make sure that this unit is listed before any units that rely on docker. And as usual, make sure etcd contains the config under /coreos.com/network/config.\nWhat this does is pull flannel docker repo from quay.io and runs it under a separate docker daemon (which we called early-docker) in privileged, host networking mode.\n. @sbhaskaran I've created binaries for v0.3.0 release: https://github.com/coreos/flannel/releases/download/v0.3.0/flannel-linux-amd64-0.3.0.tar.gz\nI'll try to make amd64 binaries from now on.\n. LGTM. Thank you for contributing.\n. @bcwaldon done. I didn't know that was editable till just now.\n. @bcwaldon just wanted to leave it in one place... but I'll amend the commit.\n. LGTM\n. LGTM\n. Yes but via config JSON as it affects the whole overlay network and not a single host.\n. @superstructor Thank you for reporting. Sorry for dropping the ball on this issue and it'll be fixed shortly.\n. @superstructor can't replicate. It first tries to create a chain and only flushes it if it already exists: https://github.com/coreos/flannel/blob/master/pkg/ip/iptables.go#L55\nCan you provide the error from the logs?\n. @mrunalp we're not considering implementing OVS support now. OVS primarily supports VXLAN and GRE as tunneling protocols and both can be done with \"plain\" Linux kernel. However, conceptually adding an OVS backend should definitely be possible. Is there a particular reason to favor OVS?\n. @mrunalp @kelseyhightower OpenFlow was designed to have a central controller (e.g. Floodlight) that is responsible for the control plane. A central controller becomes a SPF and needs a backup, replication of state, etc. flannel took a different approach with the state stored in etcd (HA data store) and the logic residing in the agents on each machine. With OVS, we could only run OpenFlow to the agent running on same host (at that point there maybe a better way to communicate with OVS). Are you interested in OpenFlow support because you would like to run your own controller?\nOne other thing that I don't know if it would even be possible to run OVS with OpenFlow but without broadcast support (which we can't implement in the overlay cloud env). I don't know how easy it would be to design some kind of proxy-arp mechanism for it. \n. @mrunalp We're currently working on VXLAN support for flannel. The data path is still through the kernel but the control plane does not use OVS. The first iteration will be like what we have in flannel today -- purely at L3 but you could still do a limited multi-tenancy as described in #50.\nHowever, since we're getting requests for better multi-tenancy (L2 so you can have overlapping IPs), we have plans to look at that next. It would still not support broadcast so probably only IP in practice. That iteration would allow you to run an instance of flannel for each tenant and point them to different paths in etcd for their configuration (including VNI to distinguish the traffic). You would also need to instruct flannel not to setup the vxlan interface with an IP and just plug it into some bridge. That version would probably follow the first iteration pretty quick (there's not that much to do).\nLater on, we have plans to add an API so that would allow you to feed arbitrary IP/MAC pairs into flannel. So if each container/VM can choose an arbitrary IP.\nAs for OVS support, it does not buy us much with our architecture. OVS is great for supporting complex flow configurations orchestrated by a central brain. I think there're existing solutions to do that. flannel is designed for simple flows but no central brain.\n. @smarterclayton you are right in that flannel is not designed for massive multi-tenancy at this point as its original goal is to assign unique IPs to containers within a cluster. Running many instances of flannel is definitely not ideal. As flannel matures, we may look into adding better support for multi-tenancy but definitely need to get vxlan done before considering that.\n. Yes, vxlan is done. Could you elaborate as to which feature you would like us to tackle next?\n. We've merged multi-network support (https://github.com/coreos/flannel/pull/169) that can be used as a start for multi-tenancy. That PR only adds support at the subnet allocation layer. The traffic isolation is something that needs to be supported at the backend layer (e.g. using multiple VNIs). @mrunalp and @rajatchopra were working on an OVS based backend that would make it multi-tenant.\n. @mrunalp flannel works at IP (L3) layer today so it's not really possible -- the routing table would not know where to route packets. So far we're planning on keeping flannel a L3 technology as it's easier to deal with. It's possible to run multiple instances of flannel and giving them unique etcd-prefix and configuring them to use distinct UDP ports. That will result in multiple interfaces (flannel0, flannel1, ...) but you have to make sure the address ranges that your configured for them in etcd do not overlap. With VxLAN, the same should again be possible except that you can use vxlan ids instead of UDP port for segregation.\nHaving two containers with same IP but on different vxlans would require flannel to support L2 semantics. The trouble there is that it becomes necessary to support broadcast (for ARP). We're planning on using VxLAN encap but not it's use of multicast for broadcast. We will therefore not have a way of doing broadcast. We may support a limited L2 functionality without broadcast that relies on flannel to do proxy ARP. \n. @helander it does not. flannel works at IP level and doing mulitcast routing is difficult.\n. The README.md does have a section \"Building flannel\" that shows how to build it, in a container or not. That part hasn't changed. This PR takes care of taking the built binary and putting it into a container.\n. @marineam I added a separate \"Packaging flannel into a container\" section. How is that?\n. flannel uses TUN device and netlink sockets which are Linux specific. I think there is some port of tun to OS X but I don't know much about it. As we develop flannel, we would further like to use Linux specific features to make it work great on that platform.\n. @gregory90 The short answer is that no, these are benign and you can ignore them.\nThis happens due to etcd watches timing out and there're problems with elegantly handling that. I've got a PR in go-etcd client that should help with that once merged.\n. @bioshrek I think this might be a separate issue. The errors described by this issue were benign and did not prevent /coreos.com/network/subnets/ from being properly populated. Could you open a new issue describing your scenario and paste the output of your logs. We could then look into what's going on.\n. LGTM. Thanks for contributing.\n. @visualphoenix I tried it on my Ubuntu box + CentOS 6.5 container and it works ok. That means it's something with the CentOS kernel. It seems to fail on netlink socket when doing RTM_GETLINK. The commit in question changed the netlink library so it makes sense. This needs to be investigated to determine what particular option CentOS 6.5 kernel does support in RTM_GETLINK call.\n. @vishvananda do you have CentOS/RHEL 6.5 handy to see why LinkByName() is failing?\n. @ernado First of all, thank you for detailed write up. The configuration and everything looks fine. Can you print out the routing table for me please: netstat -rn on both hosts. With eth0 being link-local, I want to make sure that it's not the default route (flannel will bind to default route interface). If that's the case, you can also override the interface to which flannel binds with --iface=eth1. However if that's the case, also please file an issue with github.com/coreos/coreos-cloudinit.\n. @ernado So there seem to be 2 default routes (0.0.0.0/0) -- one with gateway and one without. That's already not ideal as I'm not sure which one gets used. For that, please run ip route as it'll show us the metric. Also, eth0 has a link-local address as the primary but there should be a global (public) IP as the secondary. Can you verify that with ip addr (ifconfig only lists primary).\nSo flannel is binding to the link-local address, I see it in the logs Using 169.254.74.205 as external interface. That's the address it advertises but the two hosts might not be able to communicate via this address. There is a route:\n169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth0\nbut I'm not sure link-local comms actually work between hosts. I guess you can try to ping second from first via 169.254.74.205.\nI'll file an issue in flannel to prefer global address if there's more than one. However, that's not ideal and we'll need to look into improving networking situation on DigitalOcean. For now, just run both flannel instances with --iface=eth1 and see if that works.\n. @ernado Thank you, I think we might actually be close. Routing table looks OK -- 2 default routes by the one without gateway has a low metric and it won't get used. And there is a secondary IP on eth0. Not that it matters now that you're binding to eth1.\nIt think it might actually be OK with the exception that docker0 bridge didn't get reconfigured. When you stop docker, it doesn't bring down docker0 bridge so it doesn't pick up the new config on restart (I don't remember if it complains). Just to make sure, can you reboot the boxes and try again?\n. LGTM. Thank you for your contribution.\n. There is already an option that does something very similar to 2. If you run flanneld with --ip-masq, it will add an IP Masquerade rule for traffic that's coming out of containers and headed for the Internet. More precisely, it's traffic that originated inside the overlay address space but destined outside of it.\nThe option is undocumented (need to fix that) since I was waiting for Docker to merge my addition of --ip-masq to Docker daemon. This allows one to run Docker daemon with --ip-masq=false and flanneld with --ip-masq. Otherwise Docker also installs IP masquerading rule for traffic coming out of docker0.\nDocker 1.3 that was recently released has the --ip-masq option.\n. @ernado flannel will soon be shipped with CoreOS. It will be packaged as a Docker container and stored in our registry (Quay). The CoreOS image will have flanneld.service file that, when started, will pull down the container and start it. You're right that flanneld can't easily be run in a container as it generates config for Docker daemon. The way we work around this is by running it under a separate Docker daemon (which we call early-docker). early-docker runs with host networking.\nThe flanneld.service file that will ship with CoreOS is pegged to a particular version of flannel (via a tag). As CoreUpdate rolls out updates, the new images will have newer flanneld.service files that will point to a newer version. The version will also be over-ride-able via cloud-config.\nI hope for flannel to land in alpha in about 2 weeks (we're road blocked on some things that have to roll out first).\n. LGTM\n. @Tlvenn you're right that flannel can be useful without Kubernetes. And in fact it is today as it does not require Kubernetes and works with plain Docker just fine (in fact it does not even require Docker -- it creates an overlay network, giving a subnet to each host). If you look towards the bottom of README.md in the section titled \"Docker integration\", you'll see how flannel can be used with Docker.\nThe intro is indeed misleading but it was used as a motivation for why flannel works by allocating a an entire subnet (e.g. /24) to each host. Better wording in the intro would be valuable -- PR welcome!\n. @yangzhares this is actually normal as that's the operational state of the device and I don't think TUN driver implements setting it into UP state. The administrative state should be UP and reflected in flags as shown below (output of ip link):\n7: flannel0: <POINTOPOINT,UP,LOWER_UP> mtu 1472 qdisc pfifo_fast state UNKNOWN mode DEFAULT group default qlen 500\n    link/none \ne\nMore info on states can be found in Linux docs: https://www.kernel.org/doc/Documentation/networking/operstates.txt\nIf flannel is not working, can you first check that you can ping flannel0 interface (10.100.77.0, in your example). Then try pinging a flannel0 interface of a remote host. If ping remote flannel0 doesn't work, make sure the UDP port 8285 is open by the firewall. If it is and it still doesn't work, other things to try is running tcpdump on both machines capturing traffic from flannel0 and eth0 to see where it gets dropped. ifconfig flannel0 should also show you RX/TX/errors/dropped stats that may help in troubleshooting. \n. I agree that path rewriting makes things less flexible but it also comes with advantages of making repeatable builds easy (git pull + ./build). But even a bigger issue is that we want to keep all CoreOS projects consistent w.r.t. godep scheme. Accepting this PR would mean that all other CoreOS projects would also need to be changed and other project maintainers would not be easily convinced.\n. I'm closing this for now as we want vendoring to work consistently between CoreOS projects.\n. @eparis You can import it as env file now and just ref the two variables. Is there a strong preference for\ndocker -d $DOCKER_NETWORK_OPTIONS vs docker -d --bip=$FLANNEL_SUBNET --mtu=$FLANNEL_MTU? We can certainly add this but was trying to keep flannel as loosely coupled from Docker as possible (at least in the implementation).\n. @eparis Makes sense. We've actually ran across the same issue for CoreOS distro. What we ended up doing is some bash magic which is not ideal: https://github.com/coreos/coreos-overlay/blob/master/app-admin/flannel/files/flanneld.service\nSo it would be great to get rid of that mess but there is (at least) another thing to figure out. If you're running Docker 1.3+, you should ideally start flannel with --ip-masq=true and docker daemon with --ip-masq=false. Otherwise you end up IP masquerading within flannel network. So in this case DOCKER_NETWORK_OPTIONS needs to include --ip-masq=false if flannel was started with --ip-masq=true. However it will cause errors on older versions of Docker.\nThis is why I think it should be up to the distribution to \"put it all together\" with the necessary glue. For example, on Ubuntu this glue should be placed into /etc/default/docker.io which gets sourced in by Upstart and is expected to set DOCKER_OPTS env var.\n. How about we create and check-in a helper bash script to generate the docker flags. Then, like @philips proposed, you could call it from ExecStartPost of docker.service or create a helper flannel-generate-docker.service to execute it. Our distro would make use of this script as well.\n. Perhaps the cleanest way would be to call this helper script in flanneld's PostExec step.\n. @eparis when I did CoreOS integration, I believe I tested that ExecStartPost must complete before the service becomes \"ready\". But if you could re-test it and let me know, it would be greatly appreciated.\n. @eparis I recently retested ExecStartPost (with a long sleep) and it waited for it to complete before marking the service ready. At command line, systemctl start foo.service would not return until ExecStartPost finishes. Of course, there's a timeout on the whole service starting but that's pretty long. \n. Replaced with https://github.com/coreos/flannel/issues/93\n. @dennybritz Ignoring not a miss: should be benign and can be normal. It's logged as vxlan is new and I want extra logging. I think the problem is somewhere else. You're running in a VPC, correct? When it stops working, are you still able to reach other machines via eth0, not flannel? Next, can you ping the flannel interface (flannel0 or flannel.1) on a local machine? Then try flannel0/flannel.1 of a remote host. If remote flannel interface is not pingable, could you run tcpdump to capture traffic on UDP 8285 (if running udp backend) or UDP 8472 (if running vxlan). You can trun tcpdump via toolbox (https://github.com/coreos/toolbox) which should be installed on your CoreOS machine. The tcpdump should show where the traffic is getting dropped.\n. @dennybritz and the tcpdump confirms that as well -- we see Echo Request make it from one host to another but no sightings of Echo Reply. Let's do a few tests on the box that had nothing captured. As next step, let's see if the routes are still there: netstat -rn (we want to see a line that routes 10.10.0.0/16 via flannel.1).\nNow, keep running ping 10.10.1.0. We'll see if there's an ARP entry that maps 10.10.1.0 to some MAC:\narp. Next we want to see if vxlan's forwarding database has the route (VXLAN is L2 technology so it has a bridge-like forwarding database that maps MAC addresses to remote host's IPs). Run bridge fdb and we want to see an entry mapping the MAC we saw in arp output to 10.0.0.126.\nCan you also paste the output if ifconfig and ip addr just to make sure there's nothing odd here.\nI'm also going to setup an cluster on EC2 with VPC and run it over the next few days.\n. Please also do a quick dmesg to make sure there's nothing odd showing up there.\n. @dennybritz exactly. I'm assuming you were pinging 10.10.1.0 or 10.10.1.1 and we see ARP resolution in progress but not resolved:\nip-10-10-1-1.ec2.intern          (incomplete)                              flannel.1\nip-10-10-1-0.ec2.intern          (incomplete)                              flannel.1\nThat would be causing the problem. Now, can you look into flannel logs. At that time there should be something like:\nvxlan.go:264] L3 miss: 10.10.1.1\ndevice.go:228] calling NeighSet: 10.10.1.1, xx:xx:xx:xx:xx:xx\nvxlan.go:275] AddL3 succeeded (or failed)\nARP resolutions are forwarded up to flanneld and it inserts entries into the ARP table based on its knowledge.\n. @dennybritz no, PublicIP needs to be the IP of your eth0. And it's 0.0.0.0 for all entries. But since flannel worked for a while, it had to be valid for some period of time. This is really odd.\n. flannel gets a lease for 24 hours (via etcd) and renews it with 1 hour to go. I wonder if something broke in the renewing logic and it renews after 12 hours but sets PublicIP to 0.0.0.0. But then your TTL still shows over 10 hours remaining. I'm going to look into this.\n. @dennybritz If it does renew, it should print:\nLease renewed, new expiration: ... in the log. Also, this propagates to all other nodes which should print: Subnet added: <ip-address>\n. This happens if etcd advances too fast but flannel has logic to recover (although I wonder if there could be a bug there). Is your cluster busy? In general, it shouldn't constantly fall behind. But this gives me some clues to investigate tomorrow.\n. @dennybritz The renewals seem to be happening no less than 23 hours apart. Not sure what could be happening after 12 hours but something definitely messes up etcd. Will investigate and let you know.\n. @dennybritz I found and fixed the bug (introduced during some refactor) that was zeroing out PublicIP during the lease renewal. Can you check if it stays up now?\n. Great. Will consider this resolved.\n. This should at least help with https://github.com/coreos/flannel/issues/87\n. @anguslees For this to work, does it require connecting external interface (e.g. eth0) to docker0 bridge?\n. @anguslees I guess it would working without bridging. That is actually a great backend as it doesn't require an overlay. However, since it uses remote host as the GW, all of hosts would need to be on a same subnet (which is easy with VPC). Now I just have 2 nits:\n1. I think we should come up with a different name. routed does not convey that it won't work over routed networks (i.e. IP) and requires L2 connectivity between the hosts.\n2. Need to update README.md to give a quick explanation, describe the subnet limitation, etc.\n. @anguslees yeah, something along subnet lines. flat or l2flat? Or maybe emphasize that the host is used the gateway: host-gw?\n/cc @philips you're good with names.\n. LGTM. Thank you for the contribution.\n. LGTM.\n. As discussed in https://github.com/coreos/flannel/pull/85\n. @eparis I renamed to have .sh and changed the DOCKER_OPTS to DOCKER_NET_OPTS.\n. @eparis Changes from your diff merged.\n. @eparis I know but now there's -k to override so I don't really care.\n. @c4milo ip tool commands would roughly look like this:\nip link add flannel.1 type vxlan id 1 group 0.0.0.0 dev eth0 l2miss l3miss\nip addr add 10.100.12.0/16 dev eth0\nHowever, notice that multicast address (group) is 0.0.0.0. This is because flannel does not use regular VXLAN mulitcast based port flooding mechanism. Instead it uses DOVE extensions (that's the l2miss, l3miss) to handle ARP requests and VTEP IP resolution.\n. Actual host network interface. flannel is technically agnostic of Docker so it just creates this overlay network that terminates at flannel.1 (or flannel0 in case of udp encap). And it gives flannel.1 an IP so you can route to it.\n. @c4milo Sorry, I'm not enabling l3miss anymore because it's totally broken in the kernel and the code for it never gets exercised (at least when I tested it). It was put there to enable forwarding arp misses up to userland but Linux kernel already has support for doing that (arpd uses it) (that's why flannel enables /proc/sys/net/ipv4/neigh/flannel.1/app_solicit).\nRSC is used when you have a bridge that's configured as a gateway but all IP->MAC mappings for the network are actually known by the host. But it's not exactly how flannel is configured since flannel.1 is not connected to a bridge.\n. @c4milo yes. my bad.\n. @dennybritz There's no place where flannel does any base64 encoding... Anyway, I\"ll try to track it down tomorrow.\n. @balboah flannel takes the default route interface as the most reasonable default. It also makes sense to use IP as the identifier since it's the identifier that the network will use to route the packets to. In other words, even if flannel used some UUID for identify, the overlay would still not work as there would be duplicate IPs in its routes.\nUnfortunately, iface=$public_ipv4 is the right thing to do on Vagrant. CoreOS has to use some hackery (Ruby code + mucking over SSH) to get $public_ipv4 and $private_ipv4 set up. So while flannel doesn't work with default network configuration on Vagrant, it's easy to rectify the situation as you have identified. Of course documenting this would be a great addition -- PR welcome.\n. @RenoRainz Looks like you're using one of the latest CoreOS alpha images. In that cause, you should be able to specify this in user-data file which contains the cloud-config. See https://coreos.com/docs/running-coreos/platforms/vagrant/.\nYou should have something like:\n```\ncloud-config\ncoreos:\n  flannel:\n      interface: $public_ipv4\n``\n. @mitar For a quick and dirty hack, just execute iptables after flannel has started. Provided the overlay network has been configured to be10.1.0.0/16`:\niptables -t nat -I POSTROUTING -s 10.1.0.0/16 ! -d 10.1.0.0/16 -j MASQUERADE\n. There are no plans like that at the moment. flannel hands out a whole subnet (e.g. /24) to the host and then manages traffic within these subnets. So it could, in theory, prohibit traffic between say 10.1.15.0/24 and 10.1.20.0/24 but that's not sufficient for a few reasons. First, this would be strictly at L3 while most people want granularity of at least L4 (per specific TCP/UDP port). Second, it would not do anything for traffic within a subnet, so two pods on the same host would always be able to talk to each other.\nI guess flannel could just call out to iptables to setup arbitrary rules but why put that complexity in there. Seems like that should be handled elsewhere. \n. LGTM. Thank you for the fix.\n. @guruvan This is definitely odd. When you doing the ping's, what are you pinging? i.e. host interface or flannel0?\n. @guruvan flannel0 actually has a real address -- you can ping it. But I think that's irrelevant here. So if you have bi-directional pings going between hosts but not over flannel network, does it help flannel? I'm trying to understand if the drops are happening pre or post encap. Actually, can you get it to not work and then do ifconfig a few times and see if dropped counters are getting incremented anywhere. And if so, is it on flannel0 or eth0?\nI'll try to set this up on EC2 with VPC and see if I can reproduce.\n. @guruvan No luck reproducing it so far. Can you also try tcpdump (you can use toolbox command for that) to capture traffic on flannel0 and eth0 and see where the packets get dropped?\n. @stormltf Can you run tcpdump via toolbox on both src and dest's flannel0 and also on eth0 (or whatever the real interface is called). I'm assuming you're using \"udp\" backend. On eth0, the encapsulated packets are sent over udp port 8285 so watch for those.\nAlso, if you configure it with vxlan backend, does it help?\n. See the answer on Google groups: https://groups.google.com/forum/#!topic/google-containers/P4uh7y383oo\n. @titanous I definitely like the simplicity that this provides. I considered a very similar design initially but my main concerns were/are:\n1. There was a request for the overlay network to stay up during flannel daemon downtime (crash/restart, upgrade). At the same time, I was worried about keeping the kernel data structures (arp, fdb) out of date if flannel is not running for a prolonged period of time. I'm always a bit scared with things getting out of whack when there're mappings involved.\n2. In similar vein, if flannel is restarted, it will get the current state of the world (subnets) from etcd and will add the kernel mappings. But this patch won't remove the ones that may have disappeared. I guess this could be trivially fixed with flushing the fdb and arp tables on startup -- I think it won't cause a hiccup for exiting flows.\n3. In case of large number of hosts, many entries in the routing table (one per host) even if no traffic is ever exchanged between these hosts. Probably not a huge deal -- kernel should be able to handle that ok... although it does make it messier for people doing ip route.\n4. There was a request (and they continue to show up) to make VXLAN work at L2 (it was somewhat out of band: https://gist.github.com/philips/b32a5eb7cf5b97ce7200). I still want to add that support as more and people want to take advantage of VNI's to do multi-tenancy with overlapping IP's. Therefore I wanted to keep as much L3 out of this impl as possible (relying on host's IP routes).\nI'm definitely interested in adopting this approach but want to make sure we address these concerns.\n. > This will actually work better in that case, as they are permanent arp/fdb entries and will not get stale. Also the entire state of the network as flannel knows it will be there instead of just the state that was discovered via misses.\nThe fact that entire state will be know is definitely a plus. The concern is just around leaving stale entries behind upon start. Flush or your diff patch should help.\n\nActually, as far as I can tell, something (I haven't tracked down what it is) causes all of the relevant entries to be flushed when restarting flannel already. My understanding is that because each of the route, arp, and fdb entries are tied to the interface they get dropped when the interface is deleted (and perhaps modified in specific ways?).\n\nHmm, that's odd. When flannel exits, it doesn't delete the interface (in udp case, kernel removes the tun device but not in vxlan). Currently, since the entries are not permanent, they get aged-out, but it'd be interesting to figure out why they get removed. At least to rule out the possibility that they may get removed with flannel running.\nAlso, any particular reason why you favor static config? Is it purely to make things simpler (which I definitely favor as well) or have you encountered performance or other problems with misses?\n. > Yes, I will investigate. Just for clarification, the flush appears to happen during startup of flannel, as well as if/when the interface is manually deleted.\nWhen the interface is manually deleted, the kernel will clean up all related stuff. That makes sense. The mysterious part is the flush during a plain restart.\n. @titanous we'd like to merge this commit but need to make sure the stale entries are removed on start. Do you mind if we take these commits, rebase them and add the required functionality? Your commit (with you as an author) will be retained.\n. Thanks. Great to hear that it works well.\n. Obsoleted by https://github.com/coreos/flannel/pull/191\n. @gzoller It sounds like you have an older version of Go. Can you check and report via go version. We're using Go 1.3 (I'm on 1.3.3).\n. Problem: Suppose etcd is at index 100345 and the etcd cluster gets torn down, data deleted and restarted. etcd will start up with index 1 but flannel's watch logic will continue to ignore everything until etcd gets up to 100346. Even then, flannel will perceive it as an incremental update. In short, it will be very confused.\nNewer versions of etcd set \"X-Etcd-Cluster-ID\" HTTP header (https://github.com/coreos/etcd/pull/1426/files). I realized that it's not enough to solve this problem. etcd currently implements \"client side\" solution as described in https://github.com/coreos/etcd/issues/1225. However, it really needs to implement \"server side\" solution since flannel won't get anything back till etcd advances past index 100345 (from example above).\n. @sidharta-s your config, routing table and everything else looks fine. I am not sure why connect() would fail. I will need to fire up CentOS 6.6 to dig in.\n. @sidharta-s I couldn't find (yet) CentOS 6.6 so I tried RHEL 6.6 on AWS and Rackspace and in udp mode worked fine. On AWS, it wasn't even PV mode (only HVM was available). And both run Xen so it wasn't virtio (or at least I think so).\nThe VXLAN didn't work but I'm not too surprised. We use DOVE extensions that have been added in kernel 3.8 and RHEL6.6 is based on 2.6.32 (not sure what has been backported).\nDo you know of any public clouds that run kvm and have CentOS 6.6 image available?\nCan you try pinging your own flannel0 IP? Does it also fail?\n. @sidharta-s you're right, we should document kernel requirements. There's a PR we're evaluating to using a slightly different approach that doesn't require DOVE extensions. I don't remember when VXLAN proper got added but CentOS 6.6 seems to have it.\nCan you try ping -I flannel0 10.244.87.1? Let's see if binding to the proper interface will help.\n. @smakam This is becoming a common issue. I will need to add this to the README: https://github.com/coreos/flannel/issues/98\n. @timechanter flannel does not wait for etcd in flannel.service since etcd does not support systemd notify for when it joins the cluster. Instead, flannel keeps trying to reach etcd on start up. While flannel supports notifying systemd when it's up, having docker.service depend on that doesn't work since not everyone wants to run flannel.\nTherefore we suggest having flannel as the first entry listed in cloud-config's units list (https://coreos.com/docs/cluster-management/setup/flannel-config/). Since cloud-init will wait for the unit to start before proceeding to the next one, it should result in flannel fully starting before any docker containers come up. Docker itself starts on demand (socket activated), therefore it will also start after flannel is up.\nHowever, there are cases when flannel can fail to start due to etcd. Either by timeout (I think around 1min) or because ExecStartPre fails (if you're using it to do etcdctl). This will make cloud-init proceed and probably results in what you're seeing.\nThere're two ways to fix this.\n- Create docker.service drop-in to add a dependency on flannel (add After=flanneld.service and Requires=flanneld.service). Also add Restart=always or Restart=on-failure.\n- In cloud-config, instead of using start command, use enable:\ncoreos:\n  units:\n    - name: flanneld.service\n      command: enable\nThis will add flannel into early-docker.target and docker.service already has a dependency on that. The downside of this method, is it doesn't work on first-boot and you'll need to restart the machine after this ran once.\n. @timechanter Glad to hear it worked. We will continue to explore how to better support in CoreOS out of the box. I realize that these hacks are not ideal.\n. @mikedanese You may not need a whole range but I think you'll still need to get 1 IP to assign to the flannel interface. Otherwise what source IP will the other members of the flannel network see? Theoretically you can have the source IP be of the external interface (e.g. eth0) and have the packet travel encapsulated through flannel. But it will result in asymmetric routing as the other way will cause the packet to travel directly via external interface un-encapsulated. But asymmetric routing just makes everything harder. There maybe some issues with ICMP as well.\nOn the other hand, it maybe just good to request a /32 instead of /24 (or whatever). In other words have the ability to allocate different size subnets to different hosts. This would be a very reasonable request but unfortunately seriously complicates not only allocation logic but also routing. By having all subnets of the same length, flannel doesn't have to do the usual longest prefix match and build trie, etc.\n. @jpmx With a /30, there's only 1 IP for container (4 total -- one for flannel.1, one for docker0, and broadcast). It should still theoretically work but I wonder if it's an off by one bug. Are you able to widen the SubnetLen to 24 and see if it persists?\n. @jpmx I ran it on AWS (there's a single interface there, of course) and no dups. So let's try to understand what causes this in your setup. Without bonding, can you please print your routing table (ip route). Also, just to make sure it is flannel related, it is OK with just pinging real interface (e.g. eth0) on the host, correct? Can you also ping flannel.1 interface outside of container. \n. @jpmx By default, flannel uses the interface associated with a default route (bond0 in your case). It can be overridden with --iface cmd line parameter or interface option in flannel section of cloud-config. I'm not sure if you're already overriding it. If not, can we run two more tests, please. First, ping each host via their public IP to check for dups. Second, reconfigure flannel to use your private interface (without the bond) and see if the dups continue.\n. @jpmx so it looks like it's not a flannel issue but a bonding issue (maybe CoreOS specific, maybe hardware). I'm filing a generic CoreOS bug.\n. @WIZARD-CXY Are you running this in a public cloud? If you can ping 8.8.8.8 outside of a container but not inside, can you check that you have IP Masquerading rule turned on: sudo iptables -t nat -L -n. This rule is usually installed by Docker daemon.\n. @WIZARD-CXY This looks odd. If your Docker bridge is on 10.0.20.0/24 network (docker0 should have IP 10.0.20.1), there should be a rule for masquerading traffic from 10.0.20.0/24. However, there's usually a separate DOCKER chain with few entries. What version of Docker are you running? And can you also paste the command line that's passed to start docker daemon.\n. @WIZARD-CXY DOCKER_OPTS look good and Docker 1.3.2 is also fine. Your POSTROUTING chain in nat table should look like:\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination         \nMASQUERADE  all  --  10.0.20.0/24        0.0.0.0/0\nI really don't know what maybe causing this. Can you stop docker, clear out the nat table (iptables -t nat -F) and start docker again. Maybe also delete the bridge after you stop docker (ip link del docker0)\n. @alban do you get warnings with static linkage? I remember doing static compilation but abandoned due to nasty warnings. I think there was also an issue with resolver.\nIf flannel is started with --ip-masq, it calls out to iptables to install the rules. Therefore iptables also needs to be bundled. Also, since iptables is itself dynamically linked, it was just easier to have flannel dynamically linked.\n. @alban For docker, we base the image on flannelbox -- a small distro I built using buildroot. You're right, it would be good to fix the versions of iptables and libraries to known good ones. Even better if we use the same versions for Docker and ACI builds.\n. @jonboulle The config file form flannelbox is checked in: https://github.com/coreos/flannel/blob/master/dist/flannelbox.config\nI guess we could also create a Dockerfile that will assemble buildroot env and build flannelbox in a container. That will produce a rootfs to which we can drop-in flanneld and then build an ACI out of that. Of course this should just produce the same output as docker2aci so it maybe easier to use that.\nIt'd be great if we could produce a minimal ACI with just the required bits and also have an ability to use well known versions of dependencies and not the ones on the build machine. But I'm not sure how to do so in an automated fashion except for a long bash script.\n. Closing in favor of https://github.com/coreos/flannel/pull/143\n. @djannot The reason is that your IP is getting masqueraded by a rule that Docker has installed. If you're running Docker 1.3+, pass --ip-masq=false to docker daemon and --ip-masq to flanneld. This will tell Docker to not install IP Masquerading rules and flannel will install \"proper\" ones instead.\nIf you're using flannel with CoreOS that shipped it, it should happen automatically.\n. @djannot flannel binaries are not in CoreOS due to our desire to keep base image small. But it's integrated into it and includes flanneld.service which pulls down flannel container when started.\n. O_TMPFILE is a perfect candidate here but alas, it's too new and it'd be great to be compatible with older kernels.\n. LGTM\n. @msumme Thank you for reporting. Was doing tagging in git and quay and got them mixed up for a sec. It's labeled v0.2.1 instead of 0.2.1 in the registry. Will fix immediately.\n. Fixed.\n. @josselin-c That warning reflects that flannel fell behind, trying to monitor etcd's stream of events. That can be because etcd had a flurry of activity or some other delay. flannel recovers by doing a complete snapshot. By itself, it's harmless and it doesn't effect its operation. I'm not familiar with k8s code (/cc @kelseyhightower can you help?) but I suspect that the common link here is busy etcd. I'm not sure if it was k8s that caused the activity in the first place or some external factor.\n. @josselin-c It's normal. It's actually a bug that I need to address but it's harmless -- just an extra line get printed. The subnet you see is \"your own\" so it's a noop to remove it from yourself.\n. @philipphug I was not aware of this issue causing connectivity loss. This is with default \"udp\" backend, correct? Does this cause you to lose connectivity between all hosts or just some?\n. Is this still an issue? Needs confirmation.\n. Should be fixed by https://github.com/coreos/flannel/pull/286\n. @kaixi @rimusz We're looking into this.\n. @kelseyhightower I'm not sure I understand how the whole thing is going to work. This will make it possible for etcd to run over flannel network but how will other nodes join in? They would have the same bootstrapping problem. How would they reach etcd to get their lease and setup a routes/overlay to etcd node?\n. @kelseyhightower I understand how this works in alloc mode + GCE where something else configures the network. But I fail to understand how it can work with overlays? How will other nodes contact etcd over overlay if it's not setup for them yet?\n. This will be handled by reservations soon.\n. @erimatnor Thank you for reporting. I bet we haven't run into this due to a race -- if flannel takes a bit to startup, network-online.target gets hit first.\n. @psi-4ward you should be able to set FLANNELD_ETCD_ENDPOINTS, FLANNELD_ETCD_CAFILE and FLANNELD_ETCD_KEYFILE env vars. If you're using CoreOS and cloud-config, you should be able to set them in coreos.flannel section: https://coreos.com/docs/cluster-management/setup/cloudinit-cloud-config/#configuration-parameters\nLet me know if it doesn't work for you.\n. @psi-4ward you're right! This is a big oversight on our end. We will address that but for now you can work around it by copying those files (e.g. in ExecStartPre via dropin) into /run/flannel since that gets pulled into container.\n. Fixed in https://github.com/coreos/coreos-overlay/pull/1126\n. @sramak1396c everything i see look ok. I do see that source doesn't have docker0 but that shouldn't matter. Can you please also dump ip addr on both source and target -- I want to check that the MACs line up with the logs.\nNext step would be to use toolbox and run tcpdump. Keep pinging target.\nFirst look at source's flannel.1:\n$ sudo toolbox /sbin/tcpdump -i flannel.1 icmp\nThen source's eth0:\n$ sudo toolbox /sbin/tcpdump -i eth0 udp port 8472\nIf you see requests going out on both but not coming back, look on target's eth0 and flannel.1.\nLet me know your findings.\n. @sramak1396c btw, did it stop working or this never worked?\n. @sramak1396c It'd be interesting to understand why this helps. You had more specific routes for both flannel and private networks so default route shouldn't have been hit. \n. @lemenkov Does the current version leak sockets? And this patch fixes that by keeping the connections open longer?\nDo you know if those golang bugs got fixed?\n. @lemenkov I tested this PR with etcd 2.0 and it seems to work well. But with 0.4.6 I still get those nasty messages after 5 mins. At the same time I was not aware of leaked sockets. Yes, it destroys and re-creates the client but the old socket gets closed. On my machine I'm not seeing leaked sockets. What distro do you see this on? CoreOS?\nI also re-ran the golang/go#8946 test cases and they still seem to fail (I'm using Go 1.4.1).\n. @lvlv I completely agree that we shouldn't leak sockets. It's just tracking it down is hard. AFAIK, etcd started setting better timeouts in etcd v2.0.3 so I would try a newer version first. If possible can you run flanneld under strace: strace -t -ff -e trace=socket,close bin/flanneld. This should show how often the sockets get created and closed.\n. We don't have a way to get notified when etcd is up. Also, etcd can go down and we need to retry to connect back to it. The situation here is similar with the exception that if vxlan device gets hosed sometime into operation, there's nothing to recreate it. So we don't currently don't have the same sort of resiliency built in.\nI like this PR but do you know if it's possible to differentiate the errors and only loop on \"master device not found\" type of error? If not, can you print the error from newVXLANDevice in the warning (otherwise real errors will be hard to diagnose).\n. @linvjw Let's change it to an error (I think of a warning of something that you march past but here we're stopped in our tracks). It's not a fatal error but still.\nMaybe something like:\nlog.Error(\"VXLAN init: \", err)  // since like you say, err is already verbose\nlog.Info(\"Retrying in 1 second...\")\n. LGTM. Thank you for the contribution.\n. @AntonioMeireles all command like flags are also exposed via env vars with FLANNELD_ prefix followed by env var name in caps and hyphens changed to underscores: https://github.com/coreos/flannel/blob/master/main.go#L62\nHowever, if you're using CoreOS, it's best to use cloud-config and set the coreos.flannel.interface field: see https://coreos.com/docs/cluster-management/setup/cloudinit-cloud-config/#configuration-parameters The docs there are slightly outdated in that the cloud-config now creates an environment file that's fed into docker run --env-file=... instead creating vars in drop-in. Unfortunately env vars in drop-in don't propagate into the container. \n. Closing as it's better to have a plugin type that reads in the flannel subnet file.\n. For now I think there's value in having flannel logically separate from Rocket. We do the same thing with Docker and mk-docker-opts.sh. The glue script is ok when it's all automated by the distro via unit files or whatever. It's uglier when the user has to do that manually.\nAn advantage of keeping flannel and rkt decoupled is that it makes it possible to use flannel+veth or flannel+bridge (and flannel+ovs someday). The rkt plugins would be veth, bridge, ovs. Otherwise there's a combinatorial blow up.\nHaving said that, I think we need to have a broader discussion on how to reduce the number of networking layers with plugins between flannel, rkt, k8s, etc. \n. Closing in favor of https://github.com/appc/cni/pull/20\n. @DorianGray Is the CoreOS boxes also part of openvpn network? Is flannel running over main network or openvpn (did you explicitly specify the interface in cloud-config or --iface)? By default, flannel will run over the interface with the default route.\nCan you run tcpdump on the flannel interface on the host with the docker registry and see if the replies make it there? If so, run tcpdump on the interface over which flannel is running on. BTW, you can run tcpdump in toolbox as it's not in CoreOS image. If you don't mind, can you also paste the output of ip route please.\n. @packetcollision @DorianGray I think I understand the setup now but unfortunately it's not going to work. All of your machines need to be on the flannel network. Suppose a packet from your laptop (10.8.0.8) arrives at a CoreOS machine. To reply back, it would consult the routing table but since there's no 10.8.0.0/16 route, it'll take the default route (eth0) and the packet would leave unencapsulated and over the wrong interface. Even if you added a route, 10.8.0.0/16 -> flannel0, flannel daemon would not know what to do with it either (since it has it's own routing table that only knows about 10.200.0.0/16).\nIf you would like to use your gateway machine to bridge your laptop and docker registry on CoreOS machines, it's best to setup an HTTP proxy on that machine. It could then accept connections over 10.8.0.0/16 network and forward them to 10.200.0.0/16 network.\n. Looks good but can we add an option to either enable or disable this. The script is called build-docker.sh and if there're issues with aci conversion, I'd still like to be able to make releases without commenting out lines in the script.\n. That would be fine as well.\n. LGTM\n. @lvlv So it seems like Go doesn't relinquish the http connections and they just hang in their internal pool. Good catch!\nCan you pull out the changes to go-etcd and submit them to that project? Once it's merged there, you can update this PR to vendor in that commit. If you prefer, I can do that but then these changes won't end up being authored by you.\n. @lvlv Thanks, I'll work with the etcd team to get it merged.\n. LGTM. Thank you for tracking this down and fixing it.\n. @yichengq great, please cherry-pick and I'll update flannel.\n. @roman-shuhov I apologize for very delayed response. Is this still an open issue? If so, just want to rule out the obvious: please make sure UDP port 8472 is not firewalled off.\n. The wrong version of etcd got vendored in 325835a322dcfe1bf70fb971e74a9698f1a05ad2\n. @monkey-H apologies for the delayed response. Your docker daemon is started with --host=fd:// so it will be socket activated. Make sure docker.socket uses the standard ListenStream=/var/run/docker.sock.\nHowever, since you're running CoreOS, the latest images in all the channels ship flannel. You should be able to just follow instructions at https://coreos.com/docs/cluster-management/setup/flannel-config/ to set flannel up.\n. This is a great addition. I made some small nits. Also, let's call the backend aws-vpc to make it clear that it's for AWS. We've also planned to add the same thing for GCE, so it will help to disambiguate.\n. LGTM. Thank you for contributing.\nAlso, do you know what happens when the traffic is sent in a single VPC subnet? Does it still hop through the router (i.e. extra latency) or is does it stay within the subnet? I understand that this is all virtual so it may all look very different under the hood but it'd be interesting to understand.\n. DOCKER_OPT_BIP will actually be something like --bip=10.0.23.1/24 and 10.0.23.1 will be the IP of docker0. \n. Can  you paste the output of ip route? There should be a 10.0.0.0/16 -> flannel0 route.\n. As we don't have additional info, hard to debug.\n. The easiest is to just use Docker inspect to get the IP:\ndocker inspect -f '{{.NetworkSettings.IPAddress}}' demo_web\nHowever, since it takes a bit for container to start, you have to wrap this in a polling loop:\nExecStart=/bin/bash -c \"while ! IP=$(docker inspect -f '{{.NetworkSettings.IPAddress}}' demo_web) && [ -n \"$IP\" ]; do sleep 1; done; while true; do etcdctl set /services/loadbalancer/upstream/app1 \\\"$IP:3000\\\" --ttl 60 && etcdctl set /services/loadbalancer/domain demoweb.co.uk; sleep 45; done\"\nThere's also a registrator project that you may be interested in: https://github.com/gliderlabs/registrator\n. @philips if etcd2 is configured to listen just on the new IANA ports, this allows flannel to work without having the user specify --etcd-endpoints. Initially I thought that if you just pass a list of URL's to go-etcd, it'll cycle through them but it doesn't work.\n. That was the observed behavior. However, it has some complicated logic that I didn't follow on what to do with multiple endpoints.\n. @yichengq That's great news. I'll give it a try.\n. @mnemotiv Sorry to have dropped the ball on this. cloud-config values get put into /run/flannel/options.env file which get passed to docker via --env-file. flannel knows to use FLANNELD_* env-vars together with cmd line args (basically merges them). Any env-vars set in drop-ins unfortunately will not propagate into the flannel daemon. They can only be used to influence the flanneld.service file itself (e.g. FLANNEL_VER and ETCD_SSL_DIR).\n. @mnemotiv it's partially to save space (only pay for what your use) but also to demonstrate that you can run infrastructure pieces in a container (although currently with Docker, it does require some acrobatics). So if somebody wants to run an overlay other than flannel, they can swap it out.\n. @vaijab If you're using CoreOS, specifying etcd-endpoints is easy with cloud-config. See flannel section under https://coreos.com/os/docs/latest/cloud-config.html#coreos\nBut it's like this:\n```\ncloud-config\ncoreos:\n  flannel:\n      etcd_endpoints: http://1.2.3.4:2379\n```\n. @windwizard Few more comments but it's shaping up.\n. LGTM. Thank you for your contribution.\n. Thank you for this feedback. Having good support for aws-vpc would be very valuable to avoid the overhead. We're currently working on adding a client/server option. The idea is that all the flannel daemons talk to flannel server instead of etcd directly. This is needed for some deployments where there's desire to restrict a set of machines that can access etcd. But it will also allow for the flannel server to be the one modifying the route tables, thus restricting just the server to having access to the tokens/IAM role needed to modify the routes.\nAs part of that work, we'll try to see how we can support multiple route tables. And PR's are always welcome.\n. @rohansingh The client/server will be an opt-in option so you'll be able to keep things as is. But more importantly, the server will be stateless -- the data is still stored in etcd. If a server fails, a new one can be brought up, hopefully automatically by a cluster scheduler such as fleet or Kubernetes.\n. Have you looked at https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/networking.md\n. @cdpb flannel doesn't have static reservations right now. One option is to register the DNS IP in etcd, then query it on the node and feed it into the container via env var:\nExecStart=/bin/bash -c \"dns=$(etcdctl get /someprefix/skydns) docker run --dns=$dns ...\"\n. @elephantfries I have a PR for subnet reservations almost ready. That will allow you to statically associate flannel subnet with the IP of the host interface. However something has to allocate the same IP from that subnet to the container -- and that something has to be the container runtime, like Docker.\nEven if we were to support allocating /32 like @cdpb suggests, it would not work with Docker. I'm pretty sure Docker will fail if you specify --bip=x.x.x.x/32. But even if it didn't, it would limit your host to running one container. At this point you should just run this host without flannel -- let your DNS be on the \"outer\" network. If you're concerned about security policy, you can use iptables to restrict containers to just accessing flannel network and the DNS. \n. @elephantfries It would need to be in Docker. flannel does not allocate IPs to containers. All flannel does is allocate a subnet and ensures that this subnet is routeable between other flannel subnets.\n. @p0ns This is a bit surprising. Can you please provide a bit more information to help us figure out the problem:\n1. What is the actual type of the point-to-point interface? (e.g. gre tunnel)\n2. Can you paste the output of ip addr show $INTERFACE\n. @p0ns thanks. I'll try to replicate this to see what's going on. One theory is that the peer IP shows up in the list of interface assigned IP's. That would explain why --iface=ens3 picks it up -- it maybe first in the list. But if both xx.x5.xx.235 and xx.x9.xx.254 are returned, I don't understand why --iface=xx.x5.xx.235 does not work.\n. @p0ns I've recreated the problem. Since the ethernet interface is not point-to-point device (the address is), Go library screws up and picks the wrong address (I'm not sure why their logic is the way it is). I've switched to using the netlink library and fixed the problem. The bug fix should land soon and I'll do a point release.\n. @p0ns 0.4.1 is now available with this bug fix. You can try it by placing\nEnvironment=FLANNEL_VER=0.4.1 into you flanneld.service dropin.\n. FWIW, I hate the idea of godep patching source files to point to Godep dir. However, it's what every CoreOS project uses. I'm open to idea of using GOPATH but want to check-in with @philips, @bcwaldon, and @jonboulle as they have experimented with godep a lot more.\n. I did the cmd line arg to pick networks to join as a first cut. Totally agree that it'll need to change. One option is that once we have flannelctl to use that to instruct flannel to join or leave networks.\n. Retrying on errors may be a good or a bad idea. Seems like little hard and we had requests/PRs to make that happen at the backends.\n. @grepory Code LGTM. Can you quickly mention this in the README.md.\n. LGTM. That's great, thank you for the PR.\n. Fixes https://github.com/coreos/flannel/issues/168\n. @danielschonfeld Are you running the default or VXLAN backend?\n. yes, that changes the backend.\nflannel's vxlan backend does not mark arp or fdb entries as permanent. It lets them expire just like any other arp entry would. Once it expires, the misses are sent up to flanneld and it pushes the values back into the kernel. \nCan you let them expire then do a single ping (ping -c 1) and see what's in the logs. vxlan is verbose so you should see lines notifying of L2 and L3 misses and whether AddL2 and AddL3 succeeded. If you could paste the output, it would be helpful.\n. you said relaxing it a bit and then trying ping again causes it the first ping packet to get dropped.\nSo try once when it has reached STALE state and once when it has been completely expunged from the arp cache (just do ip neigh flush)\n. So L3 misses (arp) are handled ok -- without packet loss. But L2 misses (fdb, that's vxlan part) result in dropped packets. I now recall that I have seen this but I don't remember tracking it down in the kernel on what causes this. Marking as bug and will investigate.\n. I am not sure. I haven't followed vxlan changes in the kernel.\n. @danielschonfeld It's not yet released (we haven't tagged a version since the merge). Actually we just found a bug related to this. We can update this issue once this lands in alpha.\n. @yichengq it's definitely not ideal but i figure it's better than being completely silent when trying to connect.\n. We don't have plans at the moment. It's unclear to me how to best support it. For one thing, most cloud environments don't support multicast so it would require emulation by replicating the packet at the source, which is very inefficient. We could try to do a distribution tree but without knowing the underlying topology, it would still be inefficient.\nIf the environment does support it (e.g. bare metal), do we virutalize multicast address space? For example, a container wants to join 239.1.2.3. Can flannel just use that group \"on the wire\" or do we assume that there might be other users that utilize this group and we need to make sure we don't collide.\n. @qihuagao It's hard for me to troubleshoot this as I failed to replicate it last time. However, my theory is that the kernel does not like the IP on flannel0. Say the flannel network is 10.1.0.0/16 and the host got allocated  10.1.5.0/24. flannel0 gets assigned 10.1.5.0/16 and docker0 gets 10.1.5.1/24. Technically 10.1.5.0/16 is a perfectly legal IP but I think because of docker0's /24 \"overlaps\", flannel0 IP, it randomly causes these problems.\nCan you either delete flannel0's IP: ip addr del 10.1.5.0/24 dev flannel0 or, if this doesn't work, give it some IP outside of the allocated subnet: ip addr add 10.1.253.1/16 (just make sure .253 is unused). You will no longer be able to ping flannel0 on the remote host but it may solve the problem. It's not a solution but it will help us figure out if that's what's causing the problem.\n. @akamalov I think the problem is that Environment=\"ETCD_PROXY=readonly\" puts the proxy into read-only mode but flanneld needs to have write access in order to commit its lease.\n. @micahhausler I think checking for Black Hole is a good idea but I am not sure I understand what you mean by running multiple clusters. Do you mean that multiple flannel networks that use same IP address range but are not related to each other? In that case why not just have separate routing tables? You have to specify the route table in the config anyway.\n. @philips I actually started working this out about two weeks ago but I haven't gotten an interface that really worked. The main obstacle is per backend configuration and how to represent it and where to validate it. I also have to keep in mind that his api will be used by flannelctl so its UX is also a consideration here.\n. LGTM\n. @akamalov The problem seems to be from quotes in -etcd-endpoints=\"http://127.0.0.1:2379\". Try removing the quotes: -etcd-endpoints=http://127.0.0.1:2379. I admit that it's quite silly but the argument parsing library of Go seems to be behaving this way.\nFew other points though. While you can certainly place flannel into /opt, you can also use flannel that \"ships\" with CoreOS (it actually gets pulled down in a container on use). Please see https://coreos.com/docs/cluster-management/setup/flannel-config/\nYou can also avoid ExecStartPost polling by changing unit type to \"notify\". flannel will detect systemd and signal that it is ready just after it has finished writing out the subnet.env file. Just put Type=notify into [Service] section.\n. This should help with https://github.com/coreos/flannel/issues/164\n. This happens docker started before flannel, created a bridge and assigned an IP address to it (172.x.x.x).  However, docker does not delete the bridge when it's stopped. And when it starts up again it will error out if the IP you're trying to assign to it differs from existing.\nYou need to place a Requires= and After= clauses into docker.service to make sure it starts after flannel.\n. @greenpau Correct.\n. I think the reason this happens is because the old lease has not yet expired and gets reused instead of acquiring a new one. However, this is functionally a bug and I'm marking it as such.\n. We will do our best to get this fixed before the next release.\n. @akamalov Yes, the fix will be included in the next release -- which I'm shooting for by the end of this week. However, that will be a release of flannel, it will be a bit more before it appears in CoreOS. But you'll be able to use a drop-in with FLANNEL_VER to point it to the new version.\n. @greenpau flannel does not work this way. With flannel, all your hosts use the same etcd cluster (so they   all see the same key space) and use the same network configuration. The \"Network\" specifies the IP address space for all your nodes. It then gets carved up into subnets (based on SubnetLen) and handed out automatically to the hosts. There're no static definitions for host subnets or their underlying public IPs (at least not yet).\nAlso, FLANNEL_ETCD_KEY should be FLANNEL_ETCD_PREFIX.\n. @greenpau First, my apologies regarding FLANNEL_ETCD_PREFIX. I forgot a D at the end. It should be FLANNELD_ETCD_PREFIX. \nSecond, there is no deterministic way to assign subnets. It functions like DHCP server where the client doesn't control the assigned IP. May I ask why you'd like to have a known subnet? We do plan to add something akin to DHCP reservations when we'll have flannelctl utility. This will allow to reserve a subnet for a host like:\nflannelctl reserve-subnet 10.252.42.0/24 192.168.4.5\n. @greenpau I agree that it's not great to have these overlapping subnets. Theoretically it should work (and does for the most part) but we have found odd corner cases. I'd love to move it to the last IP but I don't think I can tell docker not to use it (you can only give docker a subnet via --bip and it'll give out all addresses up to .255). Obviously for non-docker cases, this is not a concern. Perhaps we should make IP assignment to flannel interface be more configurable. In some cases, it shouldn't even have an IP (e.g. if it's bridged into docker0).\n. docker inspect will report the IP address of the container. Please see https://github.com/coreos/flannel/issues/158 for details.\n. @greenpau flannel does not support multicast. See my response in https://github.com/coreos/flannel/issues/179 for details.\n. LGTM\n. @sanjana-bhat Are you building from git HEAD or from latest tag (v0.4.1). We just merged a change to use static/permanent entries instead of dynamic ones (https://github.com/coreos/flannel/pull/191) so it maybe related to that. If you're using HEAD, please try the tagged version and let me know if it helps.\nYou should not need to manually add ARP entries, it should just work. However, on hostA, hostB's container IP/ARP pairs should not exist. Only the hostB's flannel.1 IP/pair should exist. The reason is that while we use VXLAN, it actually works at L3. The packets from the container are routed to flannel.1 VXLAN interface (via host's routing table) and then VXLAN is used to get the packet to remote flannel.1 interface. From there on, it's routed again to the docker0 bridge and into the container.\n. @sanjana-bhat After looking into this, it is a problem with the commit I mentioned. v0.4.1 should work fine.\n/cc @MohdAhmad \n. @sanjana-bhat What version of the linux kernel are you running? I'm suspecting that it doesn't have VXLAN DOVE extensions that flannel makes use of (I think you need kernel 3.8.0+). Those extensions generate fdb misses that flannel handles. That would explain why bridge fdb doesn't show any entries. \n. @sanjana-bhat I don't have an easy way to check presence of DOVE extensions but I pulled down CentOS 7 kernel version 3.10.0-123.1.2 source and they are in there. So I'm not sure why the L2 misses are not showing up (try bridge monitor fdb to see if it shows them). I also don't know why the kernel is crashing.\nHere's some background on v0.4.1 vs HEAD. In v0.4.1 (and prior), flanneld monitors for FDB and ARP misses (it calls them L2 and L3 misses) and dynamically pushes the entries into the kernel (not permanently). The L2 misses are communicated via the DOVE extensions. Since the L2 misses never make it to flanneld in your case, they never get installed and that's why bridge fdb shows nothing.\nWith HEAD, we tried to move to installing permanent entries (both ARP and FDB) when we learn of a new host. This removes the need for monitoring the misses. However, as you discovered, the code does not fully work. We intent to fix it by rolling back to dynamic L3 entries. So FDB entries will be permanent but ARP entries will still be installed on a miss. The good news is that ARP misses are not monitored by DOVE extensions (and they seem to be working for you). We hope this lands in HEAD in a couple of days. The bad news is that there have been a number of big changes since 0.4.1 so it may be a while since we shake out all the bugs and tag 0.5.0.\n. The problem is not with log_error but with the fact that inet_ntoa uses a static buffer and inaddr_str uses inet_ntoa (so the static buffer is overwritten). Follow the example and use inaddr_str instead of inet_ntoa and it'll work fine.\n. @resouer flannel will reuse the same subnet if the IP of the host has not changed (since that's how it identifies the host). On a physical machine with DHCP, it will usually keep an IP and is fine. In a cloud environment, depending on the provider and how you restart the node, you may end up with a new IP.\nHowever, you should add flannel as a dependency to docker and wait for it to write out /run/flannel/subnet.env before satisfying the dependency. If you're using systemd, set the service type to \"notify\" since flannel signals that it's READY to systemd only after it has written out the file. If you're using upstrart, I believe there's a way to emit a signal on file creation/change.\n. @resouer Your approach seems fine although I prefer to have /run/flannel/subnet.env as a volume so it's easier to source in. Can you do the following steps:\n1. Run flannel and acquire the lease. Say you get 10.1.10.1/24.\n2. Look it up in etcd: etcdctl get /coreos.com/network/subnets/10.1.10.1-24.\n3. Check the value of PublicIP. It should match that of eth0.\n4. Restart flannel so it acquires a new lease and look it up in etcd again.\nPlease paste flannel logs to help us debug.\n. @resouer Yes, flannel acquires a 24 hour lease and refreshes every 23 hours. So if flannel is stopped for more than 1 hour, there's a possibility of it loosing the lease. We can change it so that it refreshes every 12 hours but that will only increase the window. Once the lease is lost, a new one is acquired.\n. LGTM\n. @sbaks0820 Do you want to monitor the traffic between two containers on the same host? Like traffic that goes over docker0 bridge? In that case it maybe more helpful to post on docker lists.\n. Closing as not flannel specific.\n. Obsoleted by https://github.com/coreos/flannel/pull/214\n. @0x4139 There's no one best way. You can update hosts file, just have a wrapper script that pulls values out of etcd via etcdctl, or you can use SkyDNS and use SRV records.\n. Can you include in gce.go the message from https://github.com/MohdAhmad/flannel/blob/36314ccfdb5fb771f7aef370abad51d0c35d1776/contrib/flannel-route-manager/LICENSE at the top and mention that this portion is derivative work. It's ok to re-license but we need to preserve the notice and give credit.\n. Please update README.md to document the backend.\n. LGTM. Thank you, it will really help our users out.\n. @jayunit100 Yes, this is a known issue with Vagrant, please see https://github.com/coreos/flannel/issues/98\nHowever, also make sure that the network that's been configured for flannel does not overlap with the host's IPs. From\nJun 21 12:19:09 kube0.ha flanneld[19445]: I0621 12:19:09.810994   19445 main.go:205] Using 10.0.2.15 as external interface\nJun 21 12:19:12 kube0.ha flanneld[19445]: I0621 12:19:12.199864   19445 subnet.go:83] Subnet lease acquired: 10.0.24.0/24\nI see that your host's (well, VM's) IP is 10.0.2.15 and the flannel network is probably configured as 10.0.0.0/16. That puts 10.0.2.15 within the flannel range.\n. I think https://github.com/coreos/flannel/pull/223 fixes this. Can it be closed? With latest Go (1.4), bad command line options should result in an error.\n. @misakwa Are you pulling from quay.io with credentials set (logged in) or in an anonymous fashion?\n. There's a -D option (debug) that could help us a little to troubleshoot this. With CoreOS set up, it uses a secondary version of docker (eary-docker) to pull down flannel. Can you put the following drop-in to add this option to the daemon:\n```\n/etc/systemd/system/flanneld.d/10-debug.conf\n[Service]\nExecStart=\nExecStart=/usr/lib/coreos/dockerd --daemon -D --host=fd:// --bridge=none --iptables=false --ip-masq=false --graph=/var/lib/early-docker --pidfile=/var/run/early-docker.pid\n```\nThen do sudo systemctl daemon-reload and sudo systemctl start flanneld to try again.\n. @misakwa I just updated the comment above to add missing [Service] line.\n. @misakwa I think the problem is that early-docker was still running or the /var/run/early-docker.pid file didn't get cleaned up. Can you reboot the node to make sure it starts clean.\n. @misakwa can you curl https://s3.amazonaws.com/quay-registry/okay.txt from the bad node and see if at least that works. It should return quay-okay string.\n. @misakwa well, that's still progress. Can you paste the output of journalctl -u flanneld again?\n. @misakwa Glad to hear it's working. I think you can close this issue.\n. LGTM\n. Yes, although the documentation is not the greatest. See flags: https://github.com/coreos/flannel#key-command-line-options\nAnd https://github.com/coreos/etcd/blob/master/Documentation/security.md#example-2-client-to-server-authentication-with-https-client-certificates talks a bit more about this.\n. yes but it's FLANNELD_ETCD_KEYFILE. So it's FLANNELD_* and then upper case option name with hyphens replaced with underscores. A PR to document this would be really appreciated. It was initially not documented as it was primarily done for integration with CoreOS cloud-init but it's useful on its own as well.\n. @Nalum I was updating readme last night the next next release and inserted a section on env vars. Thank you for volunteering nevertheless.\nhttps://github.com/coreos/flannel/pull/227\n. LGTM. Thank you for fixing it.\n. I don't think there's one right way to do network virtualization -- they all end up being either complex or hacky, or both. That's why we need options. Having Fan's deterministic hashing removes the need for a control plane and persistence layer. But it also comes with limitations. For example, flannel stores more than just allocated subnets and mapped IPs in etcd. VXLAN backend stores VTEP MAC address and allows for running VXLAN without multicast (which actually is a huge simplification).\n. A failed build will be vocal so you know it has failed. It is very typical to not delete a binary before starting a build. Manual deletion also impedes edit-compile-run cycle by adding an extra step.\nCan you speak to how this has been a problem for you?\n. We can add a Makefile with clean target (and others that forward to our bash scripts) but it maybe simpler to just add a simple clean script to wipe ./bin/flanneld.\n. @jonboulle thank you for the edits. PTAL.\n. Fixed by #244 \n. I think that's a good idea. If the policy permits it, it should make the change. Do you think it's reasonable to do that by default or should it require an explicit switch?\n. actually meant to but forgot. will add.\n. The readme is getting large. We will soon break it up into Documentation/ and then can have a separate document for socket activation with example.\n. @quatrix I'm not opposed to multiple k/v backends but every time we make something configurable, we add complexity, especially for the user. Therefore I first would like to understand your motivations for using Consul instead of etcd.\n. @quatrix @omribahumi Sounds reasonable. However, whomever is responsible for this PR, can you also become a flannel maintainer to help maintain the Consul support going forward? We don't have functional tests (only unit) for flannel right now but once that develops, we'd like the maintainer to ensure there're functional tests around Consul as well.\nIdeally Consul could just implement subnet.Registry interface (https://github.com/coreos/flannel/blob/master/subnet/registry.go#L30). Obviously we'll need to abstract out the return type. However b/c Consul has TTLs on sessions and not keys and b/c I don't think there's a way to \"look back\" into a watch queue, the interface may need to become more high level. At the same time, subnet.Manager (https://github.com/coreos/flannel/blob/master/subnet/subnet.go#L95) is probably too high level and will end up with lots of code duplication.\n. @omribahumi TTLs are used to implement the lease expiration. A lease is acquired by creating a key (e.g. 10.1.33.0-24) with a TTL set to 24 hours. The flannel daemon will then renew it after 23 hours. This allows the node to die and the lease to expire on its own. The 24 hours is not significant -- it just has to be \"long enough\" (I think Consul's TTLs are capped at 1 hour).\nI think we should iterate together over a common interface that will satisfy both etcd and Consul (I'm not too familiar with Consul so it's hard for to design an interface that'll work for it). Can you start by looking at the registry interface (https://github.com/coreos/flannel/blob/master/subnet/registry.go#L30), figuring out how it needs to be adjusted to work with Consul and submitting a meta-PR (not for merge but to discuss) with your proposal?\n. The problem with libkv is that it is (a) still fairly immature and (b) it suffers from the lowest common denominator problem (by the definition of what it does).\n. LGTM other than the assumption above.\n. LGTM\n. @xyleth you're correct that the best option is to use something like HAProxy as the gateway into your cluster and into the flannel network. So the idea is that HAProxy listens on non-flannel IP (either via --net=host or through a port mapping like -p 80:80 but load balances onto backend instances inside flannel network (by \"flannel\" IP). If you're using --net=host options, just make sure HAProxy binds to all interfaces (*) or to the host interface (not flannel.1).\nIt may be easier to debug with --net=host and HAProxy bound to all interfaces. If you run sudo netstat -antp, do you see HAProxy listening on 0.0.0.0? If you just nc to it from the same host (but not in a container), does that work? What about from a neighbor host (also not in a container)? If that works, shutdown flannel on this neighbor host and try again.\n. @Pensu It looks like it crashes when it tries to call into C code (or it crashes inside C code). Let's make sure it's not something wrong with your build. If you're on x86-64 arch, can you try the pre-built binary we provide: https://github.com/coreos/flannel/releases/download/v0.5.0/flannel-0.5.0-linux-amd64.tar.gz\nIf that also crashes, we'll can try running it under gdb to get a better idea. If it works, we'll try to understand why the build is broken.\n. @jcollie Your crash is alloc backend specific. I will fix it today and cut 0.5.1 release.\n. @jcollie Your crash is fixed by https://github.com/coreos/flannel/pull/245\n. @Pensu This is very odd. It seems like it is able to create a TUN device but does not return the device name (kernel gets asked to create a \"flannel%d\" device and it's supposed to replace %d with a number and return the actual name). Since there's a bit of unsafe pointer casting that happens, I wonder if this does not work well in gogcc+your arch. Is your arch supported by official Go compiler? Can you test with that?\n. @Pensu From earlier logs, it looked like you were using udp (default) backend. But if you have flannel.1 interface, that's from vxlan backend. Is the flannel.1 interface left from a previous config? If you tried vxlan backend, did that work?\nCan you run flannel under gdb and see where it segfaults and poke around? What arch are you using? I can try to check if there're any issues filed with gcc regarding that arch and calling out to C.\n. @Pensu I don't know why it would be segfaulting when just doing integer bitwise operations. Maybe it's segfaulting when calling NativelyLittle(). Can you try to determine from the gdb if NativeEndian (https://github.com/coreos/flannel/blob/master/pkg/ip/endianess.go#L26) is set to something valid? You can also try to put fmt.Printf statements around that region to see where it's segfaulting and locate the bad pointer. I wish I had access to ppc64le machine so that I could test.\n. I'm thinking it might be gccgo + ppc64le specific issue of calling into C code. Can you just make a very simple Go program that will call in to C (like just call C.getpid()) and see if that works?\n. LGTM. Thank you for fixing the docs.\n. @jon-shanks Are you using cloud-config? If so, can you make sure that flannel is listed first in your cloud-config and other units that start up docker container list flanneld.service as a dependency (via Requires=). Please see https://coreos.com/docs/cluster-management/setup/flannel-config/#enabling-flannel-via-cloud-config for more info.\nIf that does not help, could you provide your cloud-config and also the output of journalctl -b -u docker.service -u early-docker.service -u flanneld.service.\n. @Nurza flannel currently does not support IPv6. However let's leave this issue open as we should add IPv6 support in the future.\n. @patrickhoefler Can I ask about your use case for IPv6? Are you running out of RFC1918 IPv4 addresses? Or want to migrate to IPv6 throughout your org? Something else?\nI'm not against IPv6 support but it is work and considering that it's usually ran in 1918 address space, I don't see it as a high priority item. But I would like to know more.\n. @jeffbean I'm assuming you use the vxlan backend. In that case it can be flannel as the culprit although flannel does not support IPv6 so I'm not sure what that's about. Can you try the latest flannel version (0.5.1) to see if it helps. Newer versions use static vtep->mac mapping so it may help.\n. @jeffbean Good to hear although I'm not sure what actually fixed it. 0.5.1 should be hitting alpha tomorrow, I think.\n. Your DOCKER_NETWORK_OPTIONS=\"--iptables=false disables iptables including masquerade (NAT). That's actually good but you need to then pass --ip-masq to flanneld. This way flannel will install IP masquerade rule that will NAT traffic going outside of its network.\nAlso make sure sysctl net.ipv4.ip_forward returns 1. Docker will usually enable it but might not if --iptables=false is passed in.\n. @rpatrick00 Please make sure you don't have firewall blocking traffic: https://github.com/coreos/flannel#firewalls\nIf you're in a cloud environment, make sure your security groups allow the UDP traffic on the proper ports. \n. Keep pinging one container from another container (like you did) and then run tcpdump. First on source host's docker0, flannel0, ethernet interface and then do the same on the destination host. When dumping docker0 and flannel0, look for ICMP (e.g tcpdump -i docker0 icmp), when doing the ethernet interface, filter for udp (tcpdump -i eth0 udp port 8285).\n. @rpatrick00 Can you paste the output of ip addr show and iptables -L -n from the target (bad) host? Another thing to try is to tcpdump -i eth0 arp to see if it least the ARP requests go out. That will also show, if for reason, it's trying to send the traffic to the wrong IP.\nAlso, what distro are you running?\n. @rpatrick00 One other thing to check are values of /coreos.com/network/subnets/. You'll see one key per subnet. Do an etcdctl get on those keys and make sure the PublicIP is what you expect (like 10.240.182.228). Also check flannel logs on host2 to make sure you see SubnetAdded line for source's subnet.\nHang in there, we'll figure out what went wrong.\n. @rpatrick00 Glad to hear it works (even if we don't know what went wrong).\n. Fixed by https://github.com/coreos/flannel/pull/255\n. Fixed by https://github.com/coreos/flannel/pull/255\n. LGTM. This is a good short term fix but we need a general mechanism for ignoring our own leases.\n. @MohdAhmad correct.\n. LGTM except for few nits.\n. @cusspvz I'm sorry but I'm not sure I understand your proposal. flannel needs just one IP/interface over which it can operate. That's the interface over which the packets are forwarded to other nodes on the flannel network. I must admit that the name PublicIP is not great -- it's the \"real\" IP that is route-able on the network.\nCoreOS has private and public IPs that refer to LAN and WAN addresses (e.g. 10.x.x.x and 54.x.x.x). Depending on the environment, it's sometimes better to set cloud-config.coreos.flannel.interface to $private_ip so the cluster communicates between each other on RFC1918 addresses. By default, flannel uses the interface associated with the default route -- unfortunately this is often not the best choice but there's no good way to guess the right choice.\nAre you running in IaaS? If so, which one? I'm not sure why your nodes can't communicate over their private IP? Vagrant does have this problem and needs to use $public_ip.\n. I like the overall direction. I didn't want to add this NAT support prior as without encryption, going between data centers is dangerous. But we're adding IPSec backend soon so is a good addition at this time.\nI want to think through whether we need to store both PublicIP and InterfaceIP in etcd lease. The only reason is to renew the lease by that IP. We could renew by PublicIP as well but I don't know if it's more \"dangerous\".\n. You can definitely run SSL in your app if you're using udp (or any) backend. But SSL was not really designed to secure IP fragments. It was really designed to run on top of TCP (reliable stream).\n. Also, prior to getting it merged, please squash all your commits and update README.md with description of new flag.\n. @cusspvz Yes, please remove InterfaceIP from subnet.LeaseAttrs as it is no longer used anywhere.\n. Never used boot2docker. Can you launch two instances of those VMs and communicate between them? Otherwise you can test on AWS as it will give you the most realistic environment (since they do NAT).\n. LGTM except:\n- unused parameter in one of the functions\n- do one final squash\n. @cusspvz I think it's a good idea. Can you add an issue for that in https://github.com/coreos/coreos-cloudinit?\n. Looks great. Thank you for your contribution!\n. > Could you please just explain how does flannel releases flow and when are them available on CoreOS ones?\nSorry, I didn't get this.\n. @cusspvz Yes, as a maintainer, I periodically release flannel. I don't have a regular release cadence since there was often not enough commits to warrant that. Since we had some bug fixes and small features added since 0.5.1, I will do a 0.5.2 before the end of this week.\nShortly after that (usually in few days), I submit a request to bump the version to be included in alpha. Alpha releases are done weekly.\n. @cusspvz You need to make sure that your container image contains iptables utility. The easiest way is to use flannelbox as the base image. From flannel src dir:\n```\n$ cd dist\nFollowing runs docker so you may need \"sudo\"\n$ tag=docker-tag-such-as-version\n$ ./build-docker.sh $tag\nnow you have \"quay.io/coreos/flannel:$tag\" repo in docker and you can run it\nyou won't be able to push the repo though, as it'll try to push to quay.io but\nyou can re-tag it and then push wherever you want.\n``\n. @syed Thesefdb already populated with` messages are not errors -- they're just informational and just dump the fdb database. These MAC addresses are special purpose and are to be expected.\nThe fact that pings fail is something else. Do you see any log lines with \"AddL3\" in them? What about \"Subnet added:\" lines? \n. @syed Seems like some kernel versions (or variants) return these built in FDB entries and it causes a nil slice dereference in flannel code (for some odd reason, it does not trigger a panic but just hangs). We'll get the fix in shortly.\n. LGTM\n. Also reported in https://github.com/coreos/flannel/issues/260\nWe need to figure out which version fixed it again. Would actually be great to understand what the actual bug was.\n. @klizhentas I don't. I'll try to try a few versions to see which ones work but it may not be till next week. Do you have time to try other ones?\n. I'd be interested in 3.18 and something from 4.x series.\nI think the ARP misses might not be getting through: https://github.com/coreos/flannel/blob/master/backend/vxlan/device.go#L208\nThat's my only guess for now.\n. I can reproduce this on my Ubuntu box (3.19.0-23). You can see that ARP (L3) misses are not delivered to flannel. Yet when I booted CoreOS 640.0.0 (3.19.3), it works fine. Could it be that the problem is with one of the Ubuntu patches?\n. Fixed in #272\n. - Item 1 is a bug.\n- Item 2: note that -m is Do not output --ip-masq (useful for older Docker version) -- if no -m is specified, it should output DOCKER_OPT_IPMASQ\n- Item 3: CoreOS originally used /run/docker_opts.env. It later changed the name but I prefer not to change the default file name here for backward compatibility.\n. @linfan FLANNEL_IPMASQ is defined in subnet.env and is sourced in.\n. @eghobo On CoreOS flannel is started with --ip-masq=true and thus docker with --ip-masq=false. You don't want to start both flannel and docker with --ip-masq=true or they'll conflict.\n. Both will install masquerade rules. Docker will install a rule to masquerade anything coming out of docker0 and going elsewhere. That will cause container-to-container traffic (on different hosts) to get masqueraded. flannel installs a rule that only masquerades traffic if it's not destined for another flannel address.\n. I think export FLANNEL_OPTS=${FLANNEL_OPTS:-\"Network\": 15.1.0.0/16} might be wrong. { \"Network\": \"15.1.0.0/16\" } is written into etcd but it looks like FLANNEL_NET is there for that. I am assuming you're on Ubuntu. Can you try placing FLANNELD_OPTS=--ip-masq into /etc/default/flanned?\nHowever, --ip-masq controls whether or not flannel will install iptables rules on the host. The rules specify that traffic going from flannel net to the outside (i.e. from containers to the Internet) should be masqueraded. But it sounds like your instances are behind a NAT (IP Masquerade) and flannel is trying to communicate via RFC1918 addresses over the WAN (e.g. 10.x.x.x and 172.16.x.x). In that case, you can use newly added --public-ip option to specify the public (routable) IP.\n. @gangadhars --public-ip has been added very recently and is only available in 0.5.2\n. Note: not sure if this is a final solution or short term fix, will see.\n. @bcwaldon Didn't know about flagutil, this is awesome, will replace.\n. Dup of https://github.com/coreos/etcd/issues/3260\n. @bprashanth I actually have the PR for this ticket mostly done. It's being held up by a few things. First, the 2 refactoring PRs (#338 and #344), and then a patch to add lease revocations. The reservation in etcd is denoted by having the subnet node have a TTL=0 (no expiration). There's also a remote API calls for AddReservation and RemoveReservation. This does not make etcd read-only for flannel as it still has to set things like BackendData but with \"flannel server\" it is possible to gate the flannel daemons from the having general etcd access. Is that sufficient for your needs?\n. There is no authorization per API call so a node could end up deleting a subnet. The server does need to have methods like \"RevokeLease\" or \"SetNetworkConfig\" but the question is who should be allowed to call that. I think we could add access control in the future to solve this problem.\n. @YP28 You are correct that flannel was designed for container-to-container communication across hosts by giving each container a unique and routable IP within your cluster. However it was not designed to make links work across nodes. Docker links really provide service discovery via environment variables and /etc/hosts. Since flannel does not make links visible across hosts, you need to use some other way to do service discovery. There're a couple of ways to do that. You can just put values into etcd under some key that signifies your service. Or you can use SkyDNS which is a DNS server backed by etcd.\nYou were able to ping one container from another on different hosts by IP without mapping ports -- that's exactly what flannel makes possible.\nSince you're using flannel on CoreOS, it is a lot simpler to set up than what you went through. Please see this guide https://coreos.com/flannel/docs/latest/flannel-config.html\n. LGTM. Not pretty, of course, but we should be very temporary. Thank you for tracking this down.\n. @shayts7 How long is a while? The only thing that removes subnets is the TTL (lease on the subnet) but that's 24 hours. Did you loose connectivity sooner than that? It shouldn't loose the lease in general since it renews it after 23 hours but at least it's one thing I could see that would cause this.\n. The TTL is needed to remove unused leases. flannel doesn't remove its lease when it is shutdown and we rely on the the TTL to clean obsolete entries up. This is very similar to the original DHCP which didn't have DHCP Release message.\nWhen renewed, you should see Lease renewed, new expiration: XYZ in the logs. It should be 23 hours after startup. I would certainly encourage you to move to the latest flannel version (0.5.2) first though.\n. I really like the functionality, thanks for adding this.\n. @dcbw Yes, they're part of the API. Granted, I don't think too many people use this remote functionality yet so I don't think it's a big deal to break the API even if we don't bump the version. However in this case I don't think there's a reason to do so.\n. @dcbw I know it's a long review but I think this PR getting very close.\n. LGTM. Thank you for your work.\n. Good idea to document but let's brainstorm the best way to do that in these comments. My initial thought is to run multiple docker daemons. This way you can use --bridge arg to docker daemon to create multiple bridges.\nAlso note that multi-network support now does not really isolate the containers from each other on different networks. They can still cross-talk to each other although I would welcome PRs to add some iptables rules to firewall the networks from each other.\n. @resouer You can run multiple docker daemons if you point them to different unix sockets, data directory and pid file. The client can be told via DOCKER_HOST or -H which socket to use.\nIf you only need a host to join a single network, then you can definitely run just one docker daemon and then there should't be any cross talk (unless you're using aws-vpc or gce backends).\nMulti-network functionality is still limited because it's early. RedHat was planning to contribute an OVS backend that would operate at L2 and provide isolation.\n. /cc @pquerna -- do you have any feedback on the approach we're taking?\n. @gtank PTAL\n. @eparis Can you have someone on your security team gives us feedback on the approach?\n. @mjg59 PTAL\n. @npmccallum The idea is that the authentication is delegated to etcd (etcd supports TLS client certs and username/password auth). Connecting to etcd is used to \"join the cluster\" and once you're inside the cluster, you don't need to do further authentication. Notice the even the PSK is generated by flannel and not the user. If Kerberos based auth is needed, it should be added to etcd. This way we're not creating a separate set of creds and auth methods for flannel.\nThe actual encryption keys are generated via Diffie-Hellman for each host pair so compromising a single host will only allow you to initiate a conversation with any other host, not eavesdrop on other flows. \n.  @npmccallum I agree that having etcd deployed without encryption is a real issue. We were planning to do a check and bail if etcd communication is at least not over TLS. I would argue however that even if etcd is not doing auth, not all of security goes out the window. Knowing PSK does not allow for eavesdropping, just an ability to initiate new flows at L3. But auth maybe done at an application level as well.\nWe do have an IKE daemon running that will re-key long lived connections.\n. @dcbw I am trying to understand your main hesitations about delegating authentication to etcd. The main advantage is separation of concerns and ease of use.\nBut let me respond to your specific concerns:\n- gaining host<->host authentication: if you use the same certs as etcd, wouldn't you get the same set of nodes that can access any given host? What are you trying to gain in host<->host auth?\n- CRL: this is valid but this should be done at etcd level and propagated to out. So if you want to revoke a cert (node), you revoke it from accessing etcd and regeneration of PSK so it won't be able to auth when re-keying. This still needs to be implemented.\n- deploying certs and setting PSK: the PSK is generated by flannel and is an impl detail -- it does not have to be distributed so there's no administration overhead.\n- Nathaniel's concern about delegating L3 auth to L7: IKE is also an L7 protocol :)\n. @dcbw The biggest simplicity is conceptual -- you authenticate just via etcd (which you need to do anyway). Today that is done via certs but tomorrow it might be via OAuth token. And if that happens, you will first need to distribute the token for etcd authentication and then distribute the cert for IPSec/IKE.\nAlso, I want to clarify that using certs for IKE will not secure the communication any sooner. We're talking about a flannel backend. Before the backend becomes functional, it has to communicate with etcd (over plain IP, not IPsec).\n. @npmccallum @dcbw I want to discuss and understand your case for using certs. Sorry, I am still not convinced. Is there a way we could do a Hangout to discuss in person? /cc @MohdAhmad \n. This should be closed in favor of https://github.com/coreos/flannel/pull/516\n. Yeah, that folder get compiled into our docs on the site (https://coreos.com/flannel/docs/latest/gce-backend.html) and so there are special directives that don't work in raw form. Don't know if there's something that we can do to make it work both on GH and when run thru Jekyll. /cc @robszumski \n. Fixed in #329 329\n. @dcbw Apologies for slow review -- away at ContainerCon and such.\n. @dcbw Can you rebase again on top of master (just merged your singleton PR). This will make it easier to review.\n. @dcbw LGTM. Will merge after that indent fix.\n. @gangadhars Was it working fine before the new node? After the new node has been added, can the old two nodes still ping each other via flannel?\n- Please make sure the firewall/security groups allow traffic on UDP port 8285. \n- Look at the flannel logs and see if there are any errors there but also look for \"Subnet added: \" messages. Each node should have added the other two subnets.\n- While running ping, try to use tcpdump to see where the packets get dropped. Try src flannel0 (icmp), src host interface (udp port 8285), dest host interface (udp port 8285), dest flannel0 (icmp), docker0 (icmp).\n. /run/flannel/subnet.env is created by flanneld once it has acquired the subnet.\n. No new machines will be able to join the flannel network. The existing machines will continue to work, including launching new containers.  etcd is only used to grab a new subnet lease and learn about leases of other machines.\n\nOn Mar 2, 2016, at 3:27 AM, Christopher Batey notifications@github.com wrote:\nCurrently evaluating using flannel and would be very interested in these docs. We'll be testing these kinds of things out but it would be nice to have an expected behaviour to compare against.\n\u2014\nReply to this email directly or view it on GitHub.\n. flannel does not actually create any bridges. For some backends (udp and vxlan) it creates an interface through which the traffic can be tunneled through to the other hosts. What flannel really does is ensure that traffic on the flannel network will be routed appropriately. So for example, if you define a flannel network to be 10.1.0.0/16 and your host gets allocated 10.1.5.0/24, flannel ensures that other hosts sending packets to 10.1.5.0/24 will get to your host.\n\nYour allocated subnet is written out into /run/flannel/subnet.env but something else has to set up the bridge. In case of Docker, the contents of  /run/flannel/subnet.env are fed to the docker daemon and it creates the bridge (docker0).\nThe experimental multi-network mode allows you to set up multiple networks. For example 10.1.0.0/16 and 10.2.0.0/16. So your host might get 10.1.5.0/24 and 10.2.8.0/24. You can certainly then set up two bridges with each configured with the corresponding subnets. Please keep in mind that this is all done through host routing so containers attached to different bridges will still be able cross-talk to the other network. Using iptables of course, you can forbid this cross-talk. \n. @kgrvamsi Has this been resolved and can I close the issue?\n. @gangadhars I think your newly added node's clock is 3 days behind. Please see  https://coreos.com/os/docs/latest/configuring-date-and-timezone.html for making sure NTP is running (if you're on CoreOS). \n. @gangadhars Has this been resolved?\n. You're right. The API should fail with 500. The retry logic should be in the client, not the server.\n. In CoreOS, flanneld.service pulls in /usr/share/ca-certificates for \"well-known\" certs (in practice it is needed to talk to AWS metadata service). Your custom ones should go into /etc/ssl/etcd on the host which also gets mapped into /etc/ssl/etcd in the container. So you should then use config-config's coreos.flannel.etcd_certfile and set it to /etc/ssl/etcd/mycert.crt. Same goes for etcd_cafile and etcd_keyfile. Let me know if it doesn't work.\nAlso, the host location is configurable. Using flanneld.service drop-in, you can override it using Environmnet=\"ETCD_SSL_DIR=/etc/ssl/certs\". Just remember that the path inside the container is still going to be /etc/ssl/etcd so your paths in cloud-config will still need to use /etc/ssl/etcd.\n. @vaijab Did you try the ETCD_SSL_DIR to override the path?\n. @vaijab You're definitely right but there's a technical issue doing it this way. Because CoreOS is shipped completely in /usr, /etc/ssl/certs has individual symlinks to certs in /usr/share/ca-certificates. The directory itself is not a symlink so you can place custom certs there.\nBut because flannel runs in a container, we actually can't mount host's /etc/ssl/certs into the container. It will result in broken links. Instead, we mount /usr/share/ca-certificates into container's /etc/ssl/certs and have to create a separate volume for the custom ones.\nOne workaround would have been to also mount /usr/share/ca-certificates into the container at the same path to keep the links working. Yet that also seems hacky.\n. We haven't yet published any performance tests but if you Google, some blogs have posted a few measurements. I know this post has benchmarked flannel: http://www.generictestdomain.net/docker/weave/networking/stupidity/2015/04/05/weave-is-kinda-slow/\n. I don't like the duality of a backend being a singleton or not, considering there's always AddNetwork method. I think either all backends should be singletons or none of them. If backends are not singletons, it should be pretty easy to just use global variables in the concrete backend package (e.g. ovs) to keep track of any global data.\nThe reason there's currently both New and Init is that the former does not take ctx and is not interruptable and the latter is. If you want the backend to be a singleton, I would just combine New and Init as both have nothing interruptable. I would then have AddNetwork and Run take ctx as first arg (convention established by context lib authors).\n. I think the Backend interface is good now. However the implementations need to make sure that any data that is network specific is stored in some map (mapping from network name to network specific data). Stuff like lease, routes, UDP sockets, TUN devices, etc. Otherwise AddNetwork calls will stump over each other. There also needs to a mutex there to protect it as I understand the calls to AddNetwork are not serialized.\nThis is why it maybe easier to have each backend package just expose something like:\n``\nfunc NewNetwork(ctx context.Context, sm subnet.Manager, network string, config *subnet.Config, iface *Iface) Network\n// need a better name though\ntype Iface struct {\n    extIface *net.Interface\n    extIaddr net.IP\n    extEaddr net.IP\n}\ntype Network interface {\n    Run(ctx context.Context)\n}\n```\nData common to all networks in a given backend would be stored in global vars.\nI am fine with both designs but I would probably go with the NewNetwork one so that I don't have to keep all those maps in each backend. There would just be one map (with a mutex) in the network package.\n. hehe -- I did leave it ambiguous :) I meant the latter one, having the backend expose a global function func NewNetwork(ctx context.Context, sm subnet.Manager, network string, config *subnet.Config, iface *Iface) Network\n. I am sorry -- I didn't read the changes carefully enough. It's fine then with the exception that AddNetwork is now not an ideal name. Unless New() does the Once fanciness, there's a 1-1 mapping between a backend instance and a network. Therefore AddNetwork will be called only once but feels like it will be called many times.\nHowever I think it's ok for now. Once OVS backend is merged, we can revisit the design and refactor it a bit more.\nCan you add a comment in backend/common.go to explain it a bit -- I think the semantics may trip people up.\n. Let's go with RegisterNetwork\n. LGTM\n. LGTM but I'll defer to @MohdAhmad \n. LGTM except please vendor and add github.com/coreos/etcd/pkg/pathutil\n. I need to think about which log package to go with. logrus is definitely an option but there's value in keeping all CoreOS projects using the same. And not there is a preference to move to https://github.com/coreos/pkg/tree/master/capnslog\n. @bprashanth We want to move away from glog (it was added for leveled logging) but maybe to https://github.com/coreos/flannel/pull/307 or https://github.com/coreos/pkg/tree/master/capnslog. Would those be satisfactory to you?\n. You're probably building on a filesystem that does not support symbolic linux. I don't know about docker toolbox and what it uses for the filesystem.\n. @pmcao Since flannel can use a variety of options to forward traffic, both encapsulated and not, I would try to capture the traffic before it hits flannel. So I would try using a span port on a vswitch (e.g OVS) and connect containers to that instead of the default linux-bridge. If you'd like to use linux-bridge, you could use libpcap (PF_PACKET) to capture the traffic.\n. @dcbw PTAL\n. LGTM\n. @tdeheurles Unfortunately flannel drop-ins can not be used for setting environment variables for the flanneld executable. This is because flanneld runs inside a container and Docker does not pass inherited environment variables to the container. Instead we use /run/flannel/options.env to pass env vars into the flanneld. /run/flannel/options.env however is meant to be automatically generated from the cloud config. So just add\nflannel:\n    interface: 192.168.1.7\nto your cloud-config and it'll be propagated all the way to flannel.\n. The fact that $public_ipv4 on vagrant is not working is troubling. I'm opened a bug report on CoreOS.\n. @gouyang It tries to hide the actual type and just return the interface. I don't know what is more golang idiomatic, certainly returning an interface is an OOP-ism.\n. @apatil This is probably a Docker issue. flannel just allocates a subnet (/24 in your case) and passes that range over to Docker. Can you do \"ip link\" on the host? How many veth interfaces do you see? Can you also do \"docker ps -a\" and check how many are running and how many have exited. There has been lots of changes in Docker networking code lately, I wonder if the IPs are not getting released.\n. Quick Googling showed that some people had luck with restarting Docker daemon: https://github.com/docker/docker/issues/14788\n. @MohdAhmad Good catch.\n. @MohdAhmad PTAL\n. @thockin The next one :) There has been a lot of movement in key parts lately (related to more multi-network features) so we need to re-stabilize head and then cut a release. I'm hoping to do a release mid-October.\n. @rajatchopra Conceptually I don't have objections but I would like to figure out the following:\n1. What uses this API? Currently the REST API is implemented by the server. If create_lease is called by someone (e.g. flannelctl), how does flanneld (in client mode) know that it needs to pick up this lease and run with it?\n- Side note: I'm working on reservations that will work as follows. The REST API has AddReservation(subnet, publicIP), DeleteReservations(subnet) methods. When flanneld runs, it'll end up using the reservation and not a random subnet. I will also add a flag like --reservations-only that will cause flanneld to wait for reservations -- it won't acquire a lease if reservation is not there.\n- Can we use the above mechanism to have flanneld wait for create_lease?\n1. I'm also almost done with RevokeLease PR. That call deletes the lease and flanneld will go back and try to acquire a lease. We can change the behavior of what happens when the lease is lost with some policy. We're actually going to be running it in combo with --reservations-only. What behavior would you like?\n. First of all, Add/Delete Reservations definitely accept the network arg. My fault for not saying that. Second, I'm not really against flanneld (client mode) having an API but I just want to entertain the idea of communicating with it via etcd or server. So in the case of create_lease, if /coreos.com/network/subnet/10.1.6.0-24 gets created with PublicIP=192.168.5.6, the node 192.168.5.6 notices it and \"picks it up\".\n\nOn the new Reservation work, first of all, Add/Delete Reservation should include the network as an arg. The only question I have is who will really create the reservations? Does it become the responsibility of the API user to manage the subnet allocation then? What if there are clashes?\n\nThe reservations are meant to be added by the user via flannelctl or via an API to the server. If the current subnet is already in use by another node, the API call will fail.\n. I agree that going via etcd is kind of backwards on first glance. But there is a certain beauty in it as well -- we have this central store through which we coordinate (hello, Linda). You can manipulate everything with just \"etcdctl\" or any host can issue these requests to the flannel server. You will always be running flannel server, correct? If that's not the case, we'll need to have an API endpoint on the local flanneld.\nAddReservation is done with a subnet node that has TTL=0 (i.e. no TTL). Recall that leases have a TTL set to 24 hrs.\n. @rajatchopra AddReservation requires non-nil subnet in my current branch. But we would modify it (or create a new method) to take a nil subnet and allocate one. It would commit it as a reservation with no TTL. The downside of this is that it's a reservation -- it won't go away until explicitly deleted.\n. @dcbw Yes, the CNI plugin would be calling AddReservation(nil, myPublicIP) against the server flanneld. I realize however now that there will be a race. AddReservation will probably return before client flanneld has picked up the reservation and ran with it. So while AddReservation can return the allocated subnet, you won't know that it's actually routable at this point. This might not be a big deal as even if you had create_subnet against the client flanneld on the host, you wouldn't know when the rest of the cluster saw the new subnet add made it routeable. \n. With flanneld being started with --ip-masq=true, traffic going from the container but outside of flannel will get masqueraded and appear as coming from the host's interface. I wonder why MapR is not working -- I'm assuming because it has problems with masqueraded IPs.\nI don't know much about pipework, but can you use it to add a route like 10.20.44.0/22 via 10.20.44.81 dev eth0? Looking at https://github.com/jpetazzo/pipework#setting-a-default-gateway, it looks like it can manipulate the routes to some degree.\n. UnregisterNetwork gets executed right after Run. Why not do the clean up in Run? Is this just for symmetry?\n. Right, although I just realized that I don't feel too good about our interface. Sometimes Run() gets called prior to RegisterNetwork and sometimes after. Maybe this would be OK if the implementations did not rely on the ordering but they do! Maybe Run should be called prior to any RegisterNetwork to force uniformity although then there's very little it can do until the first network gets registered. So perhaps the interface should not have Run at all. RegisterNetwork can just spin up a goroutine if it need one and shut it down when all UnregisterNetworks are done. Or launch one goroutine per network -- whatever it wants.\nHaving said that, I'll review the PR as is and wait for OVS backend to get merged before redoing the interface again.\n. Correction to above. Sorry, Run get executed for every network but it is not parameterized on network name so it's semantics are really unclear now.\n. LGTM. However the backend interface needs to be reworked in the near future.\n. Yes, works as expected. If there's no quorum (>50%), etcd writes will fail.\n. I guess it is a missing feature that should be added. However in server mode, flannel does support systemd's socket activation. This way you can achieve the same \"non-waiting\" startup as with with notify. Just create a .socket file and launch flannel with --listen=fd://\n. @ingvagabund Sure, feel free to submit a PR.\nAlso, here's some info on socket activation:\nhttp://0pointer.de/blog/projects/socket-activation.html\nhttp://www.freedesktop.org/software/systemd/man/systemd.socket.html\n. @ingvagabund When flannel is running with -listen, it should not be used with mk-docker-opts.sh. You would need to write a separate service file for the server mode.\n. Fixed by #326 \n. It's not a firewall issue, is it? Can you confirm that UDP port 8472 is open between all the nodes.\n. Can you please provide flannel logs? Are you able to ping flannel.1 interfaces just from the host? -- that is ping 10.1.75.0.\n. @bprashanth sorry, just to make sure, you can ping a remote flannel.1 from the host (not on the same host)?\nCan you try v0.5.3? I think you are hitting this bug: https://github.com/coreos/flannel/pull/272\n. LGTM\n. @dcbw PTAL\n. The way things are going, I think it'll make sense to make every backend a singleton and change the interface to something closer to:\n```\ntype Backend interface {\n     // This is called first (before any networks). ctx is cancelled when flannel is exiting\n    Run(ctx context.Context)\n// Called when the backend should create or begin managing a new network\nRegisterNetwork(network string, config *subnet.Config) (*SubnetDef, error)\n\n// Called to clean up any network resources or operations\nUnregisterNetwork(network string)\n\n}\n```\nWhen flannel is exiting, it would first unregister all networks and then interrupt the ctx. Or maybe we still pass ctx to every function. But that would clean up that defer logic and I think make the interface less confusing (it is to me now). It would need to be a separate PR. And I have a bunch more of refactoring coming as I continue to work on the revoke lease patch.\n. @dcbw if you'd like, I can put up a PR to change the interface first and then rebase this patch on top.\n. Closing in favor of #338\n. The default value for --etcd-endpoints is http:// and not https://... so it won't actually end up using TLS. However this is not the behavior of --listen which looks at the presence of those other TLS flags. This is partly done for compatibility with other etcd clients but this is a bad UX. Leaving open so we can improve it.\n. Usually the kernel will add this route when an address is given to the interface. However the kernel will not do that if there is a more restrictive one already there (in this case 17.16.60.0/24). But flannel even forces this addition (https://github.com/coreos/flannel/blob/master/backend/udp/udp.go#L208) so I'm not sure what's going on here.\n. These are just left from previous runs of flannel. flannel currently doesn't clean up on exit which is good and bad. It good because you can take it down and restart without sacrificing existing connectivity (except for udp backend). It's bad because we need to be careful to check for such errors and not report them. We'll address this but it should be benign.\n. Not yet. But since we're doing more work on multi-network support and them being dynamic, it is something that we should add.\n. @philips I don't think there's a way to add a comment to a route like you can with the iptables rule. But if you clean up on exit, you still know the flannel network so you can match on the destination address. Obviously it's not robust in the face of segfaults and such.\nFor some backends, it cleans up old cruft on start.\nSent from my iPhone\n\nOn Feb 3, 2016, at 6:53 PM, Brandon Philips notifications@github.com wrote:\n@eyakubovich We would have to track this state outside of flannel on disk somewhere, right? There is no way to tag routes with metadata like \"this was added by flannel\".\n\u2014\nReply to this email directly or view it on GitHub.\n. @glerchundi We moved away from /_coreos.com to /coreos.com for our prefixes (although fleet still uses former). However if you'd like, you can use the hidden form.\n. Thank you for tracking this down. You're right, it should slow down to 1 sec retry and issue warnings. Opened https://github.com/coreos/flannel/issues/340 to track.\n. This is not yet ready for merge but seeking feedback on the new interfaces.\n\n@dcbw PTAL \n. @dcbw You're right that there's basically no cleanup in the backends -- that should be improved. My general strategy for cleanup was going to be:\n```\ntype network struct {\n    name string\n    be *backend\n}\nfunc (n *network) Run(ctx context.Context) {\n    <-ctx.Done()\n    // or use select to detect ctx.Done() getting closed\n    n.cleanup()\n}\nfunc (n *network) cleanup() {\n    be.removeNetwork(n.name)\n}\n```\nRegisterNetwork would set a pointer to the backend in the network struct.\nThe reason I don't want UnregisterNetwork is because it doesn't play nice with context.Context (which, for better or worse, is now throughout flannel). Consider an interface like\ntype Backend interface {\n    RegisterNetwork(ctx context.Context, ...) Network\n    UnregisterNetwork(ctx context.Context, ...)\n}\ntype Network interface {\n    Run(ctx context.Context)\n}\nLet's think though what UnregisterNetwork would do. It would need to stop the network which would mean Run would exit. But that's the job of ctx! And how would you implement it? You would need to pass in some channel or Context into Network so Run could select on it. It's more context idiomatic to just pass it as first arg to functions that need to be interruptable.\nBTW, UnregisterNetwork() would need to not accept Context as otherwise we have a function whose job is to interrupt but is itself interruptable. \n. @MohdAhmad Can you also take a look please\n. Regardless, It's a good bug to catch.\n. LGTM\n. @jonboulle I was hoping that it'll go into the next release (0.6) but it looks like that one is still a bit out and, more importantly, might not be too stable due to lots of turmoil in the codebase lately.  Therefore, I will do 0.5.4 this week with cherry-picked commits from the master.\n. vxlan will have lower latency overhead but the difference obviously becomes less noticeable when the overall latency increases, as is the case if you're going across cloud providers.\n. @ydhydhjanson Apologies for the late response -- GitHub has been weird lately with notifications and such.\nTwo things. First, the \"Port\" field can be left out if you're ok with default (8285) but if you do specify, it's just a number \"8285\", not \":8285\".\nSecond, 11:09:37.524284 IP localhost.localdomain > 10.10.10.7: ICMP host localhost.localdomain unreachable - admin prohibited, length 120 says that there's a firewall rule blocking this traffic. Can you check your host B to make sure it allows UDP traffic on 8285 from your host A. Are you using firewalld or another firewall software?\n. This is ready for review now.\n. @dcbw I think I fixed everything you pointed out. PTAL\n. If yum install -y etcd installs flannel, there are bigger issues to address. We don't do the packaging so it would be up to RedHat to update their rpm.\nYou can download x64 binaries from Release section on GitHub. I assume you can just re-use the .service file that gets installed with 0.5.0 rpm.\n. I'd like to also do a5e1177ea803c1de5aaaca3fb47e847a6c644f6a, c623d5666ef076de1b79c7b86d72a9d68847f672, 40b61ef499145c2a399eb4e0dae046d30c2bbd7e\n. Great idea but can you also fix up https://github.com/coreos/flannel/blob/master/dist/bump-release.sh to take this into account.\n. Closing in favor of #354\n. This one is very surprising. Can you confirm that ExecStart=/opt/everstring/flannel/bin/flanneld -etcd-endpoints=\"http://10.50.0.116:2379\" is exactly like that and not something with variable substitution? systemd is sometimes funky with variable expansion so that's the only thing I could think of.\n. @jonboulle Please rebase\n. LGTM\n. flannel does not actually know anything about Docker or kubernetnes. It's designed to start up first, allocate an entire subnet (something like a /24) to a node and then hand that subnet to Docker or kubernetes. It's assumed that the operator can properly size the subnet for the maximum number of containers a single node can run.\n. @jonboulle This one is needed to make #351 pass the tests\n. @jonboulle is this version cleaner?\n. LGTM\n. @MohdAhmad @jonboulle PTAL\n. Small nit: with vxlan, you should see flannel.1 and not flannel0 (that's for udp backend).\nWith vxlan (and udp) backends, flannel sets up encapsulation (where it wraps one packet in another). That's done by first routing those packets to this encapsulation interface (i.e. flannel.1) and then once it's wrapped it goes out the host interface (e.g. eth0).\nWith aws-vpc (and gce) backend, the packets go out of the host interface as is (unencapsulated). What that backend does is, using a REST API, configures the AWS VPC router to tell it how to route the packets with flannel IPs. Once the packets arrive at the destination host, they'll be directed to docker0 bridge because Docker has set up a route for it.\n. I'm confused why it's failing hard. There's a big for loop inside Network.Run: https://github.com/coreos/flannel/blob/master/network/network.go#L98\n. > It was failing long before the Network.Run(), because with multinetwork mode, to even get a Network object, you have to ask etcd/flannel-remote for the list of networks.\nMakes sense though the retry loop is best placed around the operation of get the list of networks (https://github.com/dcbw/flannel/blob/6b38ffa5f879e3711f0b129f5edc9c198acc11b2/network/manager.go#L315) for more clarity.\n. LGTM\n. @eveny With systemd, Requires= clause also causes all the dependent units to stop when the requirement is stopped: http://www.freedesktop.org/software/systemd/man/systemd.unit.html#Requires=\n. @resouer The container IP is getting masqueraded. There's an easy fix:\n- Tell docker not to do IP masquerading: docker daemon --ip-masq=false.\n- Tell flannel to do it instead: flanneld --ip-masq\n. @ztao1987 I don't know much about HDFS but I think what is happening is this. Because namenode is accessed as a service, the connection goes through kube-proxy, so all connections are actually coming from it (and its connections bind to docker0 interface). I think it would be best to ask Kubernetes community for best practices for working around this problem.\n. Also see https://groups.google.com/forum/#!search/hdfs$20flannel/google-containers/P4uh7y383oo/bPzIRaxhs5gJ\n. > But in my case, namenode pod ip is translated into somethings like bridge name\nNot sure what you mean\n. @xoss You're right that it's not ideal but see https://github.com/coreos/flannel/issues/302 for explanation on why it is done this way.\n. @jeremyeder That's great news. I now wonder what happens if flannel is running inside a VM. How does VXLAN play with already virtualized NIC w.r.t. to offloads.\n. Fixed in https://github.com/coreos/flannel/releases/tag/v0.5.5\n. It was to pick up a bit of performance. Since you would need two goroutines, one for each direction, it turns out that the context switching between them adds some overhead. So C version wins out.\nHowever, this is probably irrelevant now as we have vxlan (kernel based) backend so anything that needs performance (which should be everything) can just use that.\n. We don't have e2e tests. That's on a todo list and it's non-trivial task as it requires setting up etcd and multiple containers or VMs for flannel daemons. \n. @stevef1uk Are you using default (udp) backend or did you specify another one such as vxlan? Can you ping the flannel0 (udp) or flannel.1 (vxlan) interface from the other host (on the host, not in the container)?\n. Bug filed in the wrong repo\n. @amitkgupta Can you paste your flannel config that was causing the error? (what's stored in /coreos.com/network/config)\n. Probably not. I'm assuming your Office IP is something like 10.x.x.x or 192.168.y.y, aka RFC 1918 address. There's probably only a handful (or maybe one) of public addresses for the whole office and IP masquerade involved. It's quite challenging to traverse such typologies and usually involves STUN servers -- things that p2p services such as Skype do.\nflannel was not built for this use case. It was designed to run inside the data center or the cloud. \n. Weave has ability to traverse IP Masquerades (what they call firewalls) in a way very similar to STUN. That is why you were able to connect you desktop to the DC. flannel does not handle this use case. flannel can handle 1-1 NAT via --public-ip flag but that would not be sufficient in your case.\nWeave is not integrated into CoreOS like flannel is but check out http://blog.weave.works/2014/10/28/running-a-weave-network-on-coreos/ on how to get it running on CoreOS.\nLet's leave this issue open as a vote for supporting traversing 1-N NAT.\n. Thank you for doing the work. Now that registry does more, this was badly needed.\nLGTM except for one nit.\n. @xiang90 We actually do have mock registry. The registry used to be a thin wrapper around etcd and was there to basically mock etcd for testing. It has grown to do more stuff so it needs to be tested. Therefore I think MockRegistry actually has little value now and MockEtcd makes sense.\nSo I do think mock etcd makes sense but the bigger question is whether it belongs in flannel or etcd or someplace else. I know this has come up before, people want to have a drop-in mock etcd.\nWhat do you mean by \"embed etcd for testing\"?\n. It is an option, I guess. However we would need to be careful that it uses a temporary (unique) data dir. Also, for testing things like TTLs, it's desirable for it to integrate with https://github.com/jonboulle/clockwork so time can be manually advanced.\n. You're not really introducing etcd logic into flannel. You're mocking out an etcd interface. flannel already is coupled to it b/c it has to interface with etcd.\nI do feel that having a mock etcd inside github.com/coreos/etcd/client is actually a good idea. It would be very similar to how libkv does it: https://github.com/docker/libkv/tree/master/store/mock \n. My view is that etcd is a database. Mocking out databases is as old as unit testing itself. I have no qualms about merging this. @dcbw if you have a sec to slit out TestMockEtcd into a separate file, it would be awesome. But I can merge as is.\n. LGTM\n. @dcbw I'm still reviewing it. And I will also provide my feedback on the general approach.\n. @dcbw Overall I think this backend is doing too much and is too coupled to Docker and other foreign concepts like services.\nCan we reduce the scope to just creating br0 and setting up tun0 and vxlan0? Something downstream (e.g. CNI or a script) can hook up lbr0 to it (if is necessary, is it really impossible to get Docker to work directly with OVS?).\nThis will give this backend semantics similar to others:. There's this endpoint that knows how to route the flannel network addresses within the fabric.\n. Yeah, that's actually yet another race somewhere that I was going to do in another PR but looks like it has gotten worse so I need to track it down in this one.\n. @xiang90 PTAL\n. /cc @jonboulle \n. @adisood81 Newer versions of flannel install permanent L2 entries so you won't see L2 miss messages. If this is still a problem, I would double check firewall settings and then run tcpdump to see where the packets get lost.\n. @samnag Subnets getting deleted is very disturbing. I think the only thing that can delete subnets is TTL expiring before the node had a chance to renew the lease. But IIRC there was a prior report of something similar associated with a lot of etcd activity. More testing needs to be done to isolate the problem.\nAs far as Ignoring not a miss messages, they should be benign. I'd be interested to know if anyone has seen actual problems associated with them being reported. I'm not even sure why the kernel sends up these events and flannel just remove this log line.\n. /run is usually tmpfs so will be gone on a reboot. If you want settings to persist, you will need to use /etc.\n. I would say that Docker does the right thing here by first masking the --bip value with the /26 mask. I think flannel should error out in this case when SubnetMin is not at the SubnetLen boundary.\n. > Could I have multiple nodes in my cluster working as servers ? \nYes. You can have as many servers as you want. They're stateless as the state is stored in etcd.\n\nWill those servers talk to each other like in a by default setup ?\n\nNo but they don't need to as they coordinate through etcd.\n\nCan a node work as server and keep doing the same job the daemon does without --listen ?\n\nNo. You'll need to run two instances of the flanneld binary.\n. @kfox1111 Having each node request subnet size would complicate the subnet allocation algorithm in a non-trivial way (fragmentation). Having defined sets would work but it starts too look very similar to multiple networks. Perhaps multi-network feature could be modified to solve this case. Another option that has been proposed is to just for the node to get multiple subnets. For example, when it runs out of one subnet it could get another. The problem with this approach is how to integrate it with container systems -- Docker, CNI.\n. Looks like the build is getting overhauled but in the meanwhile, you can do:\ndocker run -v $SRC:/opt/go/src/github.com/coreos/flannel -i -t -e GOPATH=/opt/go/ golang:1.6 /bin/bash -c \"cd /opt/go/src/github.com/coreos/flannel && make binary\"\nWhere $SRC is the fullpath to the flannel source code.\n. MTU is calculated and set automatically by flannel. It then reports that value in subnet.env. It's not something the user can change.\nMTU calculation is backend specific. I think we should have more backend specific documentation and we can include MTU calculation there. \n. @kfox1111 flannel network is IP based (not L2) so it can have different MTU sizes. It's just like it is on the Internet. IP fragmentation, or more realistically, Path MTU discovery should take care of making it work.. @kfox1111 I think you're right. PMTUD over tunnels (and specifically VXLAN) is a hairy subject that might not be working. I think vxlan device should forward any \"Fragmentation Needed\" messages it receives, reducing Next-Hop MTU by its own overhead. But I'm not sure if it does that. So yeah, I agree that for now, adding a network-wide MTU override is a good idea.\nHowever, I'm no longer involved with the flannel project anymore. Therefore I can't influence any addition of features to it.. LGTM\n. Closing as it won't work without having busybox ACIs.\n. Your docker0 seems to be 172.16.90.1, not 172.16.90.4. Is there a container running at 172.16.90.4?\n. --iface also accepts interface name such as eth0 instead of IP. Accepting domain name is a possibility but somewhat unorthodox.\n. This is a CoreOS distro issue. Please file a bug in https://github.com/coreos/bugs\n. The build was recently overhauled and docs not updated. You should be able to just type make now.\n. @steveeJ PTAL\n. I was thinking of merging this script first (as step 1) and then integrating it into Makefile and TravisCI as steps 2 and 3. But I can combine steps 1 and 2. I'll also better check for deps and document usage.\n. Looks like something is failing with \"Bad system call\". Probably seccomp filter. I wasn't getting this but will rerun with rkt 1.12.0.\n. @tomdee Fixed seccomp issue and added a make target. Didn't touch README.md because the whole building section needs to be redone as it's badly out of date now.\n. @tomdee Ultimately the script just launches a bunch of rkt containers. So all these problems can just be blamed on rkt :stuck_out_tongue_closed_eyes: My terminal also gets messed up. I added --interactive, hoping it'll play nicer but it didn't help. I wasn't sweating this too much since I was assuming that we'll just run this stuff in a TravisCI VM and so it doesn't matter how much it messes the environment up. I just remembered that there's rkt rm that I can call to clean things up without waiting for the GC.\nI think bash is ok for launching rkt containers although @steveeJ proposed using https://github.com/mattn/goreman and a Procfile. That that is only good for running some stuff and a bash script would still be needed. I do agree that the output from multiple processes gets interleaved in a messy way.\n. @tomdee I tracked down the first run failing issue. But I'm having a devilishly hard time getting it fixed.\n. Closing in favor of #490 \n. @rojingeorge  L3 miss: 10.10.255.255 looks suspicious -- it's the broadcast address for your flannel /16 network. Are you pinging that address?\n. Currently flannel does not secure the data packets. There is PR (https://github.com/coreos/flannel/pull/290) to add an IPSEC backend that would encrypt the data packets. I hope it gets merged at some point.\nShort of that, the easiest thing is to just secure your traffic at the application layer via TLS (e.g. https). This is no different than if you were running without flannel overlay and just using regular networking.\n. @eghobo have you disabled \"src/dest checks\"? http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck\nFor host-gw to work on AWS you will need to do that. The docs do need to be improved to actually say that.\n. @eghobo if --ip-masq=true helps it means that container-to-container traffic is getting masqueraded. That is not ideal. If you disable src/dest checks, you should be able to ping without masquerade.\n. @eghobo right but you should be able to ping container to container without masquerade. Just need to find what's going wrong. That's why I suggested to check that src/dest check is disabled but it might be something else. Can you run tcpdump and see where the packets are getting dropped?\n. @tomdee Thanks! I was temporarily helping out CoreOS and took some time to update the PR. However I am no longer working on this. On a personal level, I would love it if this PR finally got merged, @MohdAhmad worked very hard on it and I think it would be a good addition to flannel. Lots of people have asked for this feature.\n. IPSec is definitely not the easiest to work with but it's really just a building block for a system that exposes more user friendly concepts. That's why we have the swans and the like.\nIf flannel can use IPSec to implement the concepts it provides then it just ends up being an implementation detail. The implementation can be based on WG but if flannel wraps it anyway, WG's simplicity is not going to matter to the end user.. @bcwaldon I had that separated. But gofmt decided that it was not good. Is there a way to tell gofmt otherwise?\n. It's only getting a few bytes of entropy so I'm not sure the overhead is great. I guess I could move seeding the PRNG to main() or init() of the module where I use it.\n. I will put it in go-systemd. I actually yanked it from https://github.com/docker/docker/blob/master/pkg/systemd/sd_notify.go\nJust didn't want to pull in the whole docker for this bit.\n. @xiangli-cmu I'll try but I thought that's what I had and it still sorted in alpha order. And I'll try to goimports.\n. Nice catch!\n. I guess but I would need to allocate a byte slice to put 4 bytes in and then take them right out. Not the most efficient thing to do.\n. -1 will only give you 0b1111.... if it's a 2's complement machine :)\n. I verified that Go compiler will inline NativelyLittle (which ends up doing an integer comparison).\n. Mount a host dir and then copy it out. I'm not sure we can add anything to Dockerfile as the user needs to specify the host dir path. But I'm open to suggestions. \n. Which one is the official golang one. Found this one: mischief/docker-golang Is that it?\nIs there a convention for Dockerfiles used just for building?\n. This is mostly on purpose to reduce the diversion between the etcd backed registry and the fake one used for unit testing. The registry is separated out to make unit testing possible not to provide alternative backends (at this point).\n. It's legacy from when it was called AllocMode. I wanted to rename this and the same in UdpBackend but didn't want to introduce even more changes into this changeset. So I wanted to do that as a separate PR.\n. Good catch. I have no idea where that came from.\n. os.Interrupt is the portable alias to SIGINT. If you meant if it's a good idea to register for SIGINT, I say it's very convenient when running at the shell.\n. Right. Forgot that it exists.\n. I wanted to let the user be able to hit Ctrl+C a few times but the way the code is written, it's useless. I'll remove.\n. GetIfaceIP4AddrMatch either returns an error or matchAddr since it checks for equality prior to return. It's better to turn this into a function that just returns true if matchAddr was found in the list of addresses and false otherwise. In that case the return would be (bool, error). An alternative would be to return just error with nil signifying the address was found.\n. No need to check again for equality here -- GetIfaceIP4AddrMatch already asserted that. However, with the proposal above, this should just check the error and boolean return code.\n. Yes but it gets passed into go-etcd and that expects chan bool\n. Good point, it's not skipping the whole project.\n. Another option would be to create a library with functionality needed for services -- args, logging, config files, etc.\n. @pquerna Unfortunately calling NetlinkSocket.Close() will not interrupt NetlinkSocket.Receive(). The reason this works for TCP/UDP/Unix connections is because their Close() method calls into Go runtime internals to interrupt blocked ops. NetlinkSocket is just a thin wrapper over syscall.Socket so there's no magic there to make it work.\n. I think this is my C/C++ habits showing through. I would usually create the variable on the stack and take it's address at call site, if needed.\n. It is intended but it's definitely contentious. The name will be flannel.X so it should be ours but one never knows. The alternative is to error out and have the user delete the interface but after thinking about it, I decided that it was an inferior solution. However, thoughts and ideas on how to handle configuration changes/upgrades better are welcome.\n. No, flannel is put into a certain \"mode\" to use one backend.\n. You can run multiple flannel instances (processes). In that case, each will be pointing to a different etcd-prefix (that contains the config). If multiple instances are all using VXLAN backend, they would need to specify a unique VXLAN ID (VNI) for each one. That VNI is what's after that dot in the interface name (e.g. VNI=42 -> flannel.42).\nSince we can end up deleting interfaces, there is a potential for deleting the a \"live\" (wrong) interface in case of a mis-configuration.\n. Maybe although the way I think about it is to make a decision based on what information is missing (since it's handling a miss).\n. I feel this is a good direction. It does help with minimizing calling poll() but it also changes the fairness of servicing sockets. Old code could starve out fds[1] if fds[0] is constantly ready. This code will fairly ping-pong between the two sockets. At the same time, if both fds are busy, it's be better to allow some batching. If there're 10 packets ready, it's better to push them through as close together as possible as it'll help with like keeping caches hot, minimize waking up blocked threads, calling h/w doorbells, etc. This mechanism could also be used to avoid neglecting the ctl socket.\n. So there was no starvation before as well so this has the same behavior as before minus the extra poll() calls. LGTM.\n. golang style is wrong :) I think if you bail early due to error, it's good to not have else. But when you have bifurcation of logic, if-else logic is clearest even if it has a return.\n. Yes. newEtcdClient with same args has already been called in newEtcdSubnetRegistry and error code checked. This code should not fail -- worse, there's very little that can be done in case of failure -- the whole subnet manager will get hosed.\n. OK, I'll panic. It'll panic on its own when it tries to access esr.cli :)\n. It has a period at the end. Thus it's a full sentence.\n. Does it need to be a member? I think it can be local var in Init.\n. Does it need to be a member? I think it can be local var in Init.\n. Instead of adding functionality to periodically list leases, it's better to keep the list in the backend itself. You can then also keep netlink.Route objects instead of Leases so they're ready to be pushed down into the kernel.\n. Typo: checkSubnetExistInRouts -> checkSubnetExistInRoutes.\n. This ends up getting called multiple times every second (as many as there are subnets). It should be called once outside the loop.\n. Can you define a helper, routeEqual\n. I think it's best to use time.After here:\nfor {\n    select {\n        case <-cancel:\n            return\n        case time.After(routeCheckRetries * time.Second):\n            rb.checkSubnetExistInRoutes()\n    }\n}\n. Better to avoid global vars and put it into HostgwBackend.\n. There's bytes.Equal that does this.\n. Check for errors and log them.\n. Having this feels like a race. Is there a way to avoid it?\n. I noticed that misspelling a long time ago but never submitted a PR to fix it because I didn't want to break the API. But someone else fixed it.\n. That comment is out of date. Yes, it's in a goroutine to be able to interrupt it (cancel request). Unfortunately that's really the only sane way to do interruption in Go.\n. RemoteManager implements subnet.Manager interface which has ctx as the first arg for consistency. Not all methods/implementations (there're two now) do anything with it yet. New etcd client is ctx based so once we move over to it, it'll be used more.\n. Great catch!\n. The idea is that the cursor is opaque to these \"helper methods\" and at the interface level. Each subnet.Manager impl is free to implement the cursor as it sees fit.\n. Please delete\n. Unused -- please remove\n. Unused -- please remove\n. Delete unused\n. Do this check right after GetL2List() call. No need to do GetL3List() if it failed. You will also be able to re-use err variable.\n. Just cast leaseAttrsList[i].VtepMAC to []byte directly.\n. Log on error\n. Log on error\n. Log on error\n. Maybe also delete the previous L2 entry (on error from AddL3)\n. This step should not be necessary. build script sets GOPATH inside and test sources it in.\n. Stylistic nit: lets put the contents of this if block into a separate function.\n. We already have instanceID. Can we use ec2.Instances([]string{instanceID}, nil) to get InstancesResp. Then it should just be resp.Reservations[0].Instances[0].SubnetId. I haven't tried it but I wonder if it'll work and maybe cleaner than going through the mac addresses.\n. It's fine to require the user to download the tooling (compiler, cover, etc) and place it wherever they want and set GOROOT and/or GOBIN as they see fit. But GOPATH we set in the scripts so that vendoring works. I think your original PR was fine, just without step 4.\n. Small nit: Error objects start with lowercase but log entries start with a capital.\n. Since we now know that special (wildcard) permissions are required for Describe methods, what do you think about not failing here but continuing with a warning if the error is Unauthorized? Just include in the warning some text on what permission is required.\n. This comment is a relic, please remove it.\n. This is not used anymore b/c we don't process L2 misses anymore. Please remove.\n. @jonboulle sentimental reasons?\n. I would put the predicate into the for statement and then change\nif operation.Status == \"DONE\" {\n    return false, nil\n}\nand last return to return false, fmt.Errorf(\"timeout waiting for delete operation to finish\")\n. Poll for DONE and check operation error.\n. It's a bug. I used a different prefix in the example initially but then decided against it to keep things simpler.\n. It does work. I considered having a separate section on that but decided that they're orthogonal enough that it should just make sense.\n. You can (since the server is stateless) and we should document that.\n. But since you asked, at least a note should be added.\n. Haha, yes. I had them named bar, baz, and foo on my machine. Good catch!\n. I'm assuming this won't fail if the source/dest check is already disabled, correct?\n. No need for word \"error\" here.\n. Same here, take our the word \"error\"\n. And here, take out \"error: \"\n. InterfaceIP is good but let's change EndpointIP to PublicIP. I would actually prefer ExternalIP but I'd rather stay with what's currently stored in etcd.\n. rename to publicIP\n. rename to \"public-ip\". Capitalize IP in description.\n. Having four-tuple is getting unwieldy. Let's move eaddr related logic out to the caller.\n. Don't serialize it to JSON (we don't want it in etcd) by adding json:\"-\"\n. Well, I'm not sure if it's worth storing it or not.\n. After thinking about this, I'd prefer not to serialize InterfaceIP and use PublicIP for identity. Here're are the reasons:\n1. Like you said, you can get two machines with the same private IP.\n2. If one re-associates public IP from one machine to another, they can both get the same subnet. But they'll need to explicitly change the --public-ip cmd arg and should remember to shut down the other node (or something). Even if they don't, the traffic will still only flow to the new node.\n3. Nothing changes in etcd. We can always add more fields later but for now everything is trivially forward and backward compatible. \n. Can you check if extIaddr and extEaddr differ and error out if so (saying it's not supported). I don't think this backend can work with NAT as it's all about L2.\n. Actually, I think you can just remove this field from this struct.\n. Use PublicIP here\n. Use PublicIP here\n. Use PublicIP here\n. You will need to remove all of its usage as well. Just go back to using PublicIP. The Interface IP/device is only needed in those places where you're binding to the interface. In fact the only place it is needed is in vxlan backend. It should probably also be used in udp backend to explicitly bind to the specified interface/IP. However that can wait. \n. InterfaceIP is only used (read) in this backend now. And it's equal to PublicIP (because you check). Therefore you can go back to using PublicIP here and delete InterfaceIP from LeaseAttrs.\n. Yes\n. extIaddr is unused.\n. Can you factor the common code out into something like:\n```\nfunc (m *RemoteManager) watch(ctx context.Context, cursor interface{}, url string, wr {}interface) error {\n    if cursor != nil {\n        c, ok := cursor.(string)\n        if !ok {\n            return subnet.NetworkWatchResult{}, fmt.Errorf(\"internal error: RemoteManager.watch received non-string cursor\")\n        }\n    url = fmt.Sprintf(\"%v?next=%v\", url, c)\n}\n\nresp, err := m.httpGet(ctx, url)\nif err != nil {\n    return err\n}\n\nif resp.StatusCode != http.StatusOK {\n    return httpError(resp)\n}\n\nif err := json.NewDecoder(resp.Body).Decode(wr); err != nil {\n    return err\n}\nif _, ok := wr.Cursor.(string); !ok {\n    return subnet.NetworkWatchResult{}, fmt.Errorf(\"watch returned non-string cursor\")\n}\n\n}\n```\n. Why a separate goroutine? The current goroutine is just going to block on line 214 anyway.\n. I don't think I fully understand the need for sleep. Is the desire to exercise the code where the server does an etcd watch prior to the network key being created? In that case I guess it's ok but the code should work regardless. That is, this also needs to work:\n1. Client side issues a watch\n2. A new network is created in etcd\n3. The server receives the watch and processes it by immediately returning\n4. The client gets notified of the new network\n. Same as comments above\n. Your approach of directly modifying the server's register is just fine -- you're just simulating an external event.\n\nThe server needs to be watching before the new network is created on the client side, otherwise the server-side will completely miss the event.\n\nThis is the scary statement. subnet.WatchNetworks internally maintains a cursor so it should not miss an event. If CreateNetwork executes before etcd.go::WatchNetworks(), that should be fine. When it does get to etcd.go::WatchNetworks(), the cursor will be pointing slightly behind the \"end\" and it will immediately return the added network. That is the whole point of maintaining and propagating the cursor.\n. Since we're using the same Event struct for both subnet and lease events (which is fine), add omitempty so it's clean on the wire.\n. Let's just have two events: Added and Removed. Or maybe call them EventAdded and EventRemoved. But they will be shared between both the lease and network events. There should be no confusion since the API endpoints and methods are different.\n. RemoteManager.WatchLeases -> RemoteManager.watch\n. leaseWatchReset -> networkWatchReset\n. What is this \"network\"? The current struct is:\n- etcd-prefix\n     |- mynetwork1\n           | - config\n           | - subnets\n                  |- mysubnet1\n                  |- mysubnet2\n     |- mynetwork2\n           | - config\n           | - subnets\n                  |- mysubnet1\n                  |- mysubnet2\n. Let's combine GetNetworks and WatchNetworks into one. GetNetworks is just a watch with nil cursor. I'm actually working on another feature and doing exactly that for leases. We can rename WatchNetworks to GetNetworks if you think it's better.\n. Why set it if not used?\n. return cmd.Run()\n. Make this a constant\n. You should use either Sprintf fmt string (prefered): Sprintf(\"%s-%s-%s-%s\", localLease.Attrs.PublicIP, localLease.Subnet, remoteLease.Subnet, remoteLease.Attrs.PublicIP.String())\nor just concatenate: ocalLease.Attrs.PublicIP.String() + \"-\" + localLease.Subnet.String() + \"-\" + remoteLease.Subnet.String() + \"-\" + remoteLease.Attrs.PublicIP.String()\n. Same as above\n. Sorry, I think you misunderstood me a bit. Yes, I proposed combining watch+get much like you've done here. However you should always return a cursor. Otherwise the watch won't work -- you kick the watch off by getting all the networks (no cursor passed in) and then watch for increments (using cursor) if you so desire. I think what you have for WatchLeases is perfect.\n. Always return a cursor\n. The watch is recursive on something like /coreos.com/network/. Wouldn't then this fire on every key change, like subnets getting added? In that case ParseConfig will fail and that error propagated.\n. @pquerna Great catch. Not sure how we missed it.\n. gofmt. Also, make these Infos instead of Warnings?\n. Stylistic nit: so far I have been following the convention of putting the wait group's add/done calls outside:\nwg.Add(1)\ngo func() {\n    defer wg.Done()\n    nm.RunNetwork(net)\n}\nI'm not sure if it's \"the right way\" but would like to keep it for consistency.\n. Same comment as above.\n. I just updated the comment above to reflect that wg.Add(1) should be before starting the goroutine to avoid a race.\n. I recently realized that a slicker way is to wrap ctx with another WithCancel (https://godoc.org/golang.org/x/net/context#WithCancel). This way you get another cancel function but the outer one still works. This way you don't have to:\n<-ctx.Done()\nfor _, net := range m.networks {\n    net.Cancel()\n}\n. passing WaitGroup by value actually copies it and I think will cause trouble. Just create a new WaitGroup here and wait for it before returning from this function.\n. stylistic nit: I would rename func name to RunDaemon or maybe even better to just Run\n. typo: Retying to Retrying\n. This should be log.Error. I think.\n. log.Infof and also fix indent / run gofmt\n. Yes, I'll merge #327 first and then I'll update this PR\n. teardownIPMasq should only get called if setupIPMasq was called first. If setupIPMasq failed, teardownIPMasq won't get called. Now, if a user/another program ends up deleting FLANNEL chain, it will error (I think correctly) but it'll just get logged and go on.\n. I see what you mean -- the chain must be empty before deletion. I think you're right, not sure how it seemed to work, I'll investigate.\n. Why? These are log statements, not error objects. Our convention is to have log statements start with a capital letter.\n. This is struct embedding. Seems appropriate in this case so that we just pick up backend.SimpleNetwork fields.\n. I don't think we need this function at all now. There's no point in creating these bump commits any more. We can just tag the release and that's it.\n. So this can go back to 0.0.0+was-not-built-properly\n. Yeah but then we still need the bump revision dance. I like that git describe magic as it cleans it up.\n. Fancy. I think this should be standardized across the company. I don't care what really gets included apart from the version but I do now like the idea of not using sed and generating those bump commits.\n. You're right. I started with a branch for doing reservations and then broke it apart into what is now 4 separate PRs (this is 2nd). The asof is only needed in the last one, the one doing reservations. I guess I could yank it here and add it into the last one where it's needed but it's not doing harm so can we let it hang out here for just a bit?\n. Same story as with asof. It's used in the next PR but I'll move it there.\n. Very good point.\n. Nice! It has After which was on my todo list to implement.\n. It's fine. I wanted to get rid of the bumping logic until https://github.com/coreos/docs/issues/637#issuecomment-152661595\n. I actually don't care that much. I want people to build with the build script so this will inform that to do so and thus I like this +was-not-built-properly. But if you have strong feelings for +git, I'll be happy to revert.\n. You're right.\n. Looks much better. But like Jon said, it's better to log the error to help the user diagnose the problem (even though in the case of etcd not being up, the error is confusing). Logging that a retry is coming is less useful.\n. It's cleaner to return map[string]string and render it to text in the function that writes out the file.\n. This one is hard to swallow as it makes it unclear what a network does if not for getting itself a lease for this node. I looked at your ovs branch and I see how you're using it but I feel uneasy merging this change. In essence the only thing that the network does in ovs branch is read in the config with VNI from etcd and output that VNI to the subnet.env file. I feel that if you'd like to use one big address space for all the networks, it's easier to have flannel just set up one network and do flow management (with VNI tagging) completely out of band from flannel. You can still store tenant->VNI mapping somewhere in etcd.\n. Nice, never knew that.\n. Great catch. Obviously I'm missing a unit test for this.\n. Yes but the real invariant is that removed reservation has non-zero expiration. 24 hr lease is an implementation detail as far as reservation logic is concerned.\n. It used to check that AcquireLease worked by making sure it go 10.3.3.0/24 subnet because it configured a very narrow range and there was only one valid subnet left to allocate. This doesn't work with reservations b/c it may allocate out of SubnetMin/SubnetMax range if there's a reservation there. I now check via inAllocatableRange and so gave \"room to breathe\" -- this way it can do more than one allocation, if needed.\n. I think that's a good point. I'll file an issue to evaluate changing a API on how the errors are reported.\n. It is not intuitive and not even ideal. But we can't just delete it as it might be in use.\n. Oh yeah, I remember why I don't have a test for it -- it's very difficult to write as I would need to perfectly time a race.\n. I added a comment on RemoveReservation.\n. nit: would be good to mock etcd testing into a separate file.\n. If hard coding, please make a constant\n. Can we get this upstreamed to vishvananda/netlink? Ok, short term to be here but should see if we can move it to netlink lib somehow.\n. This is very to implement without calling out to sysctl. Just open /proc/sys/... and write a value. vxlan backend does tweak sysctls as well and has a function for that. Maybe move it into pkg.\n. allArgs := append([]string{cmd}, args...)\n. It's better to just return fmt.Errorf this stuff and log higher up.\n. addr := netlink.Addr{\n    net.IPNet: ip.IP4Net{\n        dev.gwAddr,\n        dev.nodeNetwork.PrefixLen\n    }.ToIPNet()\n}\nand you don't need to go via parsing\n. No need to special case initial event batch\n. The defer is there because I had things like return msr.index and I needed that expression to be evaluated under lock. Otherwise I would need to introduce a temporary variable.\nBut looking at it again I see that I'm returning msr.index not under lock in one place. So I'll need to reorg the code anyway.\n. s/br0/dev.bridgeName\n. Indent doesn't match the rest\n. Is there a way to not pull out a lib out of some other container? Can we work to get this included into busybox-glibc image?\n. Why sleep here?\n. And why sleep here?\n. And why sleep here?\n. Yes but on the other hand, you can start getting spurious failures since etcd is changing underneath you.\n. Great point!\n. I was thinking that exec is a bash keyword but it's just a builtin. I'll drop it.\n. ",
    "marineam": "re the mention of openvpn, a google guy some of us met with a few months ago recommended this: http://www.tinc-vpn.org/\n. Also the readme mentions IPv4, if you need to support NAT between the overlay and external networks that's a must. If this is an isolated network lets use IPv6 please :)\n. I don't follow how 6to4 is applicable, unless you simply mean following 6to4's scheme for mapping IPv6 prefixes to IPv4 addresses so the overlay network is essentially stateless. That does place the requirement that hosts be directly IPv4 addressable between them all.\n. We already ship 3.8.0, which is when the DOVE was added. That is also the current \"stable\" version in Gentoo. Will scan the release notes for important fixes since then but otherwise I'll consider that part ready to go. I think we need to enable it in the kernel, iirc there's a bug somewhere requesting that too.\n. This may be applicable: http://git.kernel.org/cgit/linux/kernel/git/shemminger/iproute2.git/commit/?id=7cfa3802ca3e9078cd8f6c9638a0c25a63f5ddd8\nThat landed in 3.11 so we'll need to bump the ebuild to at least that version.\n. Seems ok to me, although perhaps document how to use it since building flannel outside of docker appears to be required so pointing a generic docker builder setup at this git repo won't work.\n. I would add an extra note to the existing build instructions on creating the container and that the docker build is an additional step rather than an alternative procedure.\n. \n. ",
    "bcwaldon": "@eyakubovich It would be awesome if you could at least break up your one giant commit into project bootstrapping (godeps, ./test, ./build, ./gitignore, ./cover) and actual kolach code.\n. @eyakubovich how about we do this in two steps? Let's just do the kolach -> rudder move now and we can do the coreos-inc -> coreos move later this week.\n. @eyakubovich You missed .gitignore\n. @eyakubovich the Flannel in the PR message and commit message should be lowercase, too.\n. @eyakubovich what about the commit message?\n. LGTM\n. @jonboulle ?\n. Had to close it to get travis to pick up the PR.\n. @eyakubovich travis is happy now. This is ready when you are\n. LGTM. Will -port be configurable again in the future?\n. LGTM\n@vishvananda is an OK guy\n. LGTM\n. \n. Is this really necessary for init()? I'm (possibly unnecessarily) worried about additional work on other clients of the pkg package.\n. This should live in go-systemd if it doesn't already.\n. Separate your imports into three blocks:\n```\nimport (\n    // stdlib imports\n// vendored imports\n\n// local imports\n\n)\n``\n. This smells. Could you just use google's glog package?\n. commented-out code!?\n. Could you put the401in a constant to add more context here?\n. @eyakubovich @xiangli-cmu is correct, gofmt will not remove newlines and will sort alphabetically within each block.\n. This line should not be necessary\n. All of this refactoring was just to make sure we slept during this branch.\n. Why not use google's official golang container?\n. How do I get the payload out?\n. And we should move this Dockerfile out of the root directory as that's typically used for the runtime container by convention.\n.google/golang:1.3` is on hub.docker.com\n. s/flannel/flanneld/\n. build -o ${GOBIN}/flanneld is the answer\n. ",
    "crawford": "\n. You can just force push to the existing branch. It will update the PR.\n. LGTM\n. LGTM\n. LGTM\n. Fixed by https://github.com/coreos/flannel/pull/149.\n. LGTM\n. LGTM\n. Clever. So the etcd client library only attempts to connect to the first address?\n. LGTM\n. Can you rebase to remove the previous PR?\n. LGTM\n. LGTM\n. This issue was moved to coreos/bugs#1668\n. It should be fixed by https://github.com/coreos/coreos-overlay/pull/2571.. clever. why not: return htonl(-1UL << (32 - prefix_len));?\n. This violates strict aliasing.\n. :) fair enough.\n. ",
    "jpstrikesback": "Am I correct in thinking this would obviate the need for e.g. AWS VPC? What is needed for this to happen?\n. Fair enough (on network level encryption). I'm interested in this for use on providers like DigitalOcean where there is no analogy to AWS VPC (and perhaps for hybrid solutions) so I'm looking for isolation at the host level via something like Flannel. Stampede.io has implemented an IPSec VPN in a container IIRC as part of its solution (which is interesting and perhaps a stopgap...tho I guess there's no reason it can't be a robust solution if config is/was in etcd et al).\n. ",
    "offlinehacker": "+1\n. ",
    "sporkmonger": "Honestly, it's really not so much a question of not trusting your hosting provider. Hosting providers don't necessarily get a choice in the matter of whether the fiber has been spliced and tapped. It's just a good idea to encrypt any traffic you're not in absolute control over. And even if you are... it's probably still a good idea.\n. ",
    "runningman84": "A simple encryption like openvpn would be good enough. I would really like to span a coreos cluster between different cloud providers like linode, online.fr and others.\n. ",
    "wmark": "Setting up IPSEC in transport mode with the help of Consul or etcd is what I currently use. You might find it a viable (and proven) substitute.\n. ",
    "cornelius-keller": "Hi wmark,\nis there any documentation on your consul / etcd solution available?\nI would like to try this, but beeing very new to ipsec I would like to have an example to follow.\n. Hi there,\nI created a demo with an encrypted flannel network using ting.\nYou can find it here https://github.com/cornelius-keller/coreos-vagrant-tinc\nAs I am really new to coreos systemd etcd etc. I would like to hear your comments / suggestions improvements.\nIt uses tinc for the encryption, flannel for subnet allocation and etcd for distributiong the configuration.\nIt is a long weekend hack and I am new to the whole technology stack. An attempt to dive into coreos and do something useful. But it works.\nI am sure that the whole systemd services can done much more elegant by an experienced systemd user so I would love to see your comments on that or even better some pull requests.\n. ",
    "ibotty": "hi @wmark, I am pretty sure that at least (flannel using) vxlan does not work with ipsec transport mode. It should work using tunnel mode though.\n. I have written a blog entry on how to do that.\nBut @balboah, you surely have a dedicated router on each site of your subnets, right? That should be a stock site-to-site setup. Just (transparently) encrypt traffic coming over udp port 8472.\n. What's the advantage on having it inside flannel again?\nIt's reasonable easy to set up an ipsec tunnel with the flannel subnets and the only thing missing is a (separate) daemon that watches flannel's etcd, generates the right ipsec subnet config and reloads the ipsec stack. That way, before flannel starts, there already is host-to-host ipsec. It's also very easy to support different ipsec auth schemes, you just need to generate the config a little different.\n. Generating and activating the ipsec subnet config in an internal flannel hook is reasonable. Even better if that happens before the subnet is routable.My point is that ipsec should not happen inside of flannel but with an external ipsec stack. That way: * it won't interfere with the host-to-host ipsec that might already exist; * It's also one less piece of software to audit and  * it allows for different ipsec stacks (future linux or bsd's)coreos/flannel wrote:\nWhat's the advantage on having it inside flannel again?\n@ibotty the advantage is that flannel is \"the thing that controls node networking\", and whether the node communications is encrypted is arguably something that flannel should also handle.  Granted, flannel isn't the only thing that benefits from having IPSec enabled to all other nodes in the cluster.  But having this handled in one place, where networking is done already, seems like the right place.  And if it was made into a library then all the backends could use it and it could become an option like 'IpMasq' currently is, perhaps.  Lastly, flannel is already watching for other nodes (via etcd) and that's the exact event that you'd use to set up/tear down an IPSec tunnel to that node.\nThe argument (as I see it) for having it outside of flannel is that technically, the IPSec operation really doesn't have anything to do with flannel and if it was configured before flannel, then flannel could be run un-encrypted over the encrypted link just like anything else on the node.  IPSec could be configured outside flannel, at node setup time, and then whatever is actually spawning containers on the node probably is already listening for other nodes, and it could do the required inter-node IPSec tunneling.\n@eyakubovich I'm still leaning more towards having this in flannel itself.  That avoids startup race conditions, flannel is already watching for other nodes, already knows the certificates, already knows the etcd server address (or the flanneld master address), etc.  Plus, maybe the the orchestrator on the node doesn't actually do anything network-related (openshift is possibly special here)...\n\u2014Reply to this email directly or view it on GitHub.\n. ",
    "balboah": "this would be very nice as I would like to hookup different data centers across the public internets\n. @ibotty cool. Yes setting up routers on each side is an option, but at least initially I feel that peer to peer encryption where flannel sends packets to publicly routed ip's would be more flexible and not requiring high availability setups of additional routers\n. Thanks for the reply, was mainly thought as a heads up not really critical.\nI understand why you would want to keep the ip as the identifier and not re-implement the hackery. Let me try flannel out some more before I attempt to document it ;)\n. puh, sometimes things gets done just in time before I need them :+1: \n. I guess it doesn't have to if the bridge IP range stays the same, but there's no guarantee for that. Right?\nI mean this from docker.service:\n```\nEnvironmentFile=-/run/flannel_docker_opts.env\n/run/flannel_docker_opts.env\nDOCKER_OPT_BIP=\"--bip=10.10.122.1/24\"\n```\n. ",
    "diranged": "Just chiming in here -- I agree that this would be a great and obvious feature to implement. It both allows for cross-region networking models, as well as adds a layer of warm-fuzzy encryption. Any movement on this?\n. @eyakubovich thanks for the link -- I'm not qualified to comment on the code itself, but I'm reading the PR now and will comment where I can.\n. For what its worth, I actually agree with the model of using the already-generated TLS/SSL certs ... if Flanneld detects changes in the certs automatically without restarting. I didn't see this point discussed above, so I'm curious what your thoughts are on it.\n. ",
    "cvle": "+1\n. +1 Hope this will be merged soon.\n. ",
    "mkutsevol": "PR #637 is the latest one regarding this issue.. I've made a follow-up PR #637 . I ran the previous (non-rebased) version for a couple of days with debug logging, pinging among participating nodes and a tcpdump. Everything is fine & stable for me.\nI've rebased it. Please review/comment.. I've updated mkutsevol/flannel:amd64 with the current build.. Switched default esp proposal to Suite-B-GCM-128 and made it configurable instead of hard coded. . @tomdee hi!\nWhat has to be done to merge it? \nThanks! :D. @philips, no, I need it for a standalone thing. \nFor sure, vici can be used just through a socket from another pod, but I won't do that for this PR.\n. @tomdee, @philips hi!\nSo what will be the destiny of this PR? \nI've seen discussions and many people will benefit from this code. \n. @nesc58 I'm using this in production for several  months now. Stable. . @nesc58 I can definitely say that the absence of connectivity is not due to flannel/ipsec code, you should better try to get help on k8s forums. . Hi @tomdee,\nthanks for the review. \n1. Sure, it needs rebasing.\n2. That can be done, really it just uses a socket to control charon daemon. But it needs some discussion. How to organize it when it is not run in k8s pod.\n3. I haven't seen k8s netmanager at all (or whatever it is called, that thing that is used to store config data instead of etcd).\n4. createBackendData/getBackendData are used to store/retrieve additional net configuration, specifically the network password. \nLets discuss some of the available approaches to separate things here. \nFlannel does two things for ipsec to work. It configures the charon daemon via vici over a socket and it sets up policies. So those two processes should be in one network namespace. \nk8s pods are just the thing. Different images, different disks, one net ns. \nBut how we will run in standalone mode? \n1. We can have multiple builds (image size concerns), w/ and w/o strongswan. \n2. We can put the burden of getting a working strongswan daemon on the user. Just add a param '--vici-socket' and assume that a correctly configured strongswan is there ;) \nUsing the second approach users can use OS package manager to install strongswan, for coreos and friends the user can make a systemd service that starts a container with strongswan (didn't find an official one though) prior to starting flannel.\nAlso, it will remove the need to build strongswan. \nAnd that all being said, it turns this work into the fourth? reincarnation of it? :D. I'm doing my best to allocate time to this in the nearest future.. @RRAlex definitely not in this PR. openswan which is used to control kernel ipsec is used here and the kernel does the magic and 'protocols', 'fast', etc.. Hi @tomdee,\nSo, the packaging. \nFor the tarballs, it will include the charon daemon, the full dist/ folder.\nI'll produce 3 docker images for every arch: flannel only, flannel+charon, charon only.\n * flannel only + charon only = pods in k8s.\n * flannel+charon = standalone.\nBasically, charon only image can be replaced by any charon/strongswan with enabled vici image. \nI need your opinion on tagging:\n$(TAG)-$(ARCH) -> $(TAG)-$(VARIANT)-$(ARCH)\nwhere VARIANT is one of: light, full, strongswan | standalone, ipsec, strongswan\nWe will have more dockerfiles, which is unfortunate, as they will require synchronous changes. But templating dockerfiles seems like an overkill. What's you opinion?\n\nStatus: I rebased your rebase onto the current master and it's broken and needs rework to adapt not only to the packaging changes, but to the changes in flannel itself.\n. Please see create-dockerfiles-$(ARCH) target in the https://github.com/mkutsevol/flannel/commit/3603410a3a94d322d738f9ced1485759bae6cd82 commit, as an example. \nDocker uses the last entrypoint directive.  . Nice addition to docker, thanks for the heads up. But it looks like it produces one image and let's us checkpoint state during one build, so it won't be useful for this particular situation. . @klausenbusk, ah! --target really makes a difference here! . Approximately restored the previous functionality. Removal of CreateBackendData/GetBackendData makes it straightforward to use it with k8s (hooray to no-more-multiple-nets decision) \nIt doesn't create PSK for the network itself anymore, now it's part of the configuration.  \nSplit packaging will be next. \nI'll focus on amd64  arch for now. I don't have a ready setup to test non-amd64 builds of strongswan. . @tomdee, I know, that's 'cos of the routing setup the tests do. \nI've moved to another city once again and struggling to keep up with my work, I'll try to pay some time to it this weekend. . Looks like this was taken directly from library docs. \nhttps://github.com/bronze1man/goStrongswanVici \nWhat is wrong with v1 is it just works? \nI'm still running some ikev1 without any problems. . ",
    "RyPeck": "See #929 for the latest work here. Still experimental.. ",
    "Iodun": "Are there any thoughts on using MACsec on top of VXLAN instead of IPsec?\nAvailable in mainline Linux kernel 4.6 and newer.\nhttps://developers.redhat.com/blog/2016/10/14/macsec-a-different-solution-to-encrypt-network-traffic/. ",
    "eliaslevy": "If considering using MACsec, then you may also want to look at WireGuard.. ",
    "jameskeane": "It's not Flannel, but Scrambler provides an ipsec mesh network on top of strongswan.\nIt reuses a lot of built-in stuff to stay simple: RBAC, k8s api, and cluster PKI (kubelet client certs). The mesh is created by an agent run on each node that reads the cluster nodes, and generates an ipsec config using each node's name and pod_cidr on an interval. It dynamically reconfigures on node join/leave. And it's less than 200 lines of bash!\nIt doesn't do everything flannel does, but it might be enough for some.. ",
    "G3ph4z": "so is this not yet implemented?. ",
    "artheus": "https://github.com/coreos/flannel/tree/master/backend/ipsec\nShould this issue not be closed, now that ipsec backend exists in master branch?\nEdit: I see now that it has not yet been released.. Just out of curiosity. Why is there a need for a Ipsec backend, in the case of K8s when the Network traffic between pods can be configured as TLS? In this case it just feels to me as if the only thing Ipsec will provide is double encryption and a greater load on the node cpus.\nAuthentication to K8s and Etcd is Two-way SSL, which should provide you with all the security you need. \nI might be missing something. Is there a part of the traffic in a K8s cluster that will make the cluster vulnerable over the public internet without Ipsec tunnels/transport?\n(please, read this as me trying to learn. No implicit meaning intended.) . Okay. I think I understand. VxLAN, which is the recommended backend is insecure over the public internet. Therefore it would be a good idea to have an alternate method of isolating the network over the internet. Am I right? . ",
    "zbindenren": "Will the ipsec backend be included in the 0.10.0 release?. ",
    "pquerna": "~~One kinda crazy idea, make the multi-cast address your tun/tap device, pop it up to userspace, and then your high level code can decide what to do with broadcast packets.  (if there isn't an easier way)~~\nNevermind, looked more, it seems you just want to use L2MISS and L3MISS messages from netlink and you can avoid all the multicast business.\n. @npmccallum: well, all of the flannel nodes share access to an etcd.... so doing something besides PSK for that reason isn't highly valuable as it might be otherwise.\n. can't you do a double go-routine here?\neg, go to a first level goroutine select over misses and a shutdown channel.  On shutdown channel getting hit, have that goroutine call NetlinkSocket.Close() on the Netlink file descriptor, and then the .MonitorMisses will pop out?\n. while this is going into etcd, you probably don't want to log the actual password  -- logs get shipped all kinds of places with different ACLs, often inadequate, on them.\n. i'd recommend using crypto/rand.Read here, seeding your own rand with the current time is a recipe for trouble.\n. I'm not entirely sure with how goStrongswanVici the rekey time interacts with the copmonents of strongswan, for example on this page:\nhttps://wiki.strongswan.org/projects/strongswan/wiki/ExpiryRekey\nhttps://wiki.strongswan.org/projects/strongswan/wiki/ExpiryRekey#Formula\nDoes this mean that margintime is still defaulting to 9m with the random fuzz addition... so basically as soon as the SA is setup, it's gonna be rekeyed?\n. ",
    "polvi": "Dockerfile lgtm\n. ",
    "gabrtv": "@jonboulle looks like this was never actually merged.  I just tested the fix and it works great. :+1:\n. Closing for #25.\n. ",
    "jonboulle": "right you are - now it's merged :-)\n. lgtm, thanks!\n. I think this is #25 - what build of rudder are you using?\n. LGTM\n. must... resist....\n. \n. for posterity - coreos/etcd#1082\n. You might consider just ~~cargo culting~~ re-using the function we use in fleet/etcd: getFlagsFromEnv\n. yup, LGTM\n. lgtm\n. \n. lgtm\n. LGTM, thanks for the fix.\n. Thanks, lgtm\n. @inthecloud247 want to submit a patch for this? :-)\n. This was one of the original intended usage patterns of godep, but is no longer the preferred way of using it - in fact it has been deprecated in favour of the vendoring and path rewriting as we are doing today. For more context, see this discussion:\nhttps://docs.google.com/a/coreos.com/document/d/1Dxo9PHfNVETus0UCAuviysZZAr2Hv_vPBMK1-9jBU4M/edit#heading=h.x62avzkxk8v8\n. Oops, wrong button.\nI think I'm a bit unclear on your goals with this PR, and I might have misunderstood your intention. Are you wanting to be able to build the project with different versions of dependencies based on what is in your GOPATH rather than what is vendored in the project? If so, to what end?\n. Yes, I misinterpreted the original issue raised here. As @eyakubovich says, our main goal with the rewriting is to ensure the project is easy to consistently retrieve/build - not only with the git pull && ./build workflow (for those less go-inclined), but also for it to be trivially go getable (i.e. https://github.com/tools/godep/issues/68).\n\nNot hard coding the flannel path into the dependency makes it easier for people to actually use godep and test new dependencies 'the godep way'.\n\nI don't follow, can you elaborate? godep still makes it trivial to bump dependencies with rewritten paths; go get -u github.com/foo/bar && godep update github.com/foo/bar works just fine\n. you made travis sad...\n. go for it\n. lgtm\n. lgtm other than those small things, but I'll defer to @eyakubovich \n. @eyakubovich so what needs to be done here? do we need to codify the build process for flannelbox in this repo?\n. lgtm\n. @eyakubovich good for this to go?\n. Hmm, maybe this should be dist/build-aci.sh and just source build-docker.sh inside it?\n. @alban could you please take a look at this so we can close out coreos/rkt#389?\n. LGTM, @eyakubovich can you ack/merge please?\n. lgtm assuming it works :-)\nOn Mon, Apr 6, 2015 at 12:37 PM, Eugene Yakubovich <notifications@github.com\n\nwrote:\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/coreos/flannel/pull/149\nCommit Summary\n- updated to try both new and old etcd ports\nFile Changes\n- M main.go https://github.com/coreos/flannel/pull/149/files#diff-0\n  (2)\nPatch Links:\n- https://github.com/coreos/flannel/pull/149.patch\n- https://github.com/coreos/flannel/pull/149.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/149.\n. I have a mild preference to stick with rewrites unless we come up with a compelling alternative to godep we can move to more generally.\n. lgtm\n. lgtm, thanks!\n. lgtm\n. @eyakubovich updated for your consideration, could do with the additional notes we discussed up there ^\n. can you add a note on this to the --listen flag and a doc example?\n. LGTM although would be nice to have an example in the readme, and the tests\n. This issue was moved to coreos/fleet#1283\n. Should be fixed in newer versions of flannel, please re-open if not.\n. @philips not everywhere yet but  that is the idea \n. Perhaps this could be an opportunity to move to an argument library like cobra...\n. Any chance you can reproduce this on a more recent version of flannel? (preferably v0.5.3). There have been some significant changes since v0.5.1 like moving to a new etcd client.\n. @neumino does\nETCDCTL_ENDPOINT=http://10.128.6.27:2379,http://10.128.109.98:2379,http://10.128.7.34:2379  etcdctl get /coreos.com/network/config work?\n. Oops. Try this:\n\n$ cat /etc/flannel/options.env \n FLANNELD_IFACE=107.170.177.56\n FLANNELD_ETCD_ENDPOINTS=http://10.128.6.27:2379,http://10.128.109.98:2379,http://10.128.7.34:2379\n. Fixed: https://github.com/coreos/flannel/releases/tag/v0.5.4\n. @sujitfulse what commit did you build flannel from?\n. @eyakubovich tweaked, thoughts?\n. @eyakubovich done\n. Lgtm \n. LGTM after last comment\n. coreos/go-iptables#8\n. Yes AFAICT\n. Almost always want to use the very latest version of godep.\nOn Fri, Feb 19, 2016 at 12:39 AM, David Siefert notifications@github.com\nwrote:\n\nHaving enormous trouble with godep to update dependencies... what version\nof dep are you guys using where it works to update?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/374#issuecomment-185981177.\n. @eyakubovich actually, @xiang90 had a couple of concerns about this\napproach - paging him before this lands..\n\nOn Tue, Dec 1, 2015 at 1:45 PM, Eugene Yakubovich notifications@github.com\nwrote:\n\nThank you for doing the work. Now that registry does more, this was badly\nneeded.\nLGTM except for one nit.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/380#issuecomment-161105443.\n. can you bump the Godep for netlink instead of manually patching it here? thanks\n. Check out the instructions in this page:\nhttps://github.com/coreos/docs/blob/master/golang/godep.md#update-an-existing-dependency\n\nOn Mon, Jan 25, 2016 at 11:17 AM, okamototk notifications@github.com\nwrote:\n\nI'm new for godep. Do you mean I should rebase netlink?\nHow can I rebase netlink with godeps?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/397#issuecomment-174460987.\n. @eyakubovich replace gofmt with goimports, it's smarter about these things\n. At some point it would be nice to have this package define more agnostic return structs so you don't have to pass etcd.Responses around\n. why this change? now it's inconsistent with the preceding two\n. s/%s/%v/ for consistency\n. ... elsewhere too\n. why \"m\"?\n. what's 5?\n. do you need to register SIGINT too?\n. Consider actually returning the error from fmt.Fprintf, and then checking error when calling this function.\n. This script is buggy if $GOBIN doesn't exist, can you add a mkdir?\n. whatever happened to that new CLI framework that Shantanu was working on? I think that's the only place this would make sense - otherwise it's too little code to bother factoring out.\n. Yeah, we go back and forth on this - previously we've used libraries for these things but over the last months have trended towards removing them out and just using simpler versions of our own.\n. golang style: omit the else\n. are you really comfortable ignoring the error here?\n. panic then :-)\n. Ehhhh\n. I think your indentation is a little off here\n. I guess you're using tabs ;-)\n. This should probably be bumped\n. unused?\n. yuck, use a switch statement!\n. this isn't right, should be outside the loop\n. Gross, can't you use -1 instead of an empty interface?\n. too far\n. to diff\n. do we even need the rudder reference any more? flannel has gained its own reputation :-)\n. nit, Google's\n. shouldn't this have a git clone in it?\n. > (we used one different from default)\n\n@eyakubovich what does this mean? etcd prefix isn't specified in the example unless you just mean implicitly because --networks is specified\n. what if I'm mixing client/server mode and --networks, does that work? if so what does it look like?\n. If I have a client/server set up can I also run a normal flannel daemon in the same overlay?\n. Tests for this one would be great.\n. The motivation for this was to make the go get case a bit cleaner.\nOn Oct 21, 2015 18:24, \"Eugene Yakubovich\" notifications@github.com wrote:\n\nIn version/version.go\nhttps://github.com/coreos/flannel/pull/349#discussion_r42703163:\n\n-const Version = \"0.5.3+git\"\n+var Version = \"0.5.3+was-not-built-properly\"\n\nSo this can go back to 0.0.0+was-not-built-properly\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/349/files#r42703163.\n. Kind of unpacking a can of worms I didn't want to, but thoughts on etcd's\napproach?\n\netcd Version: 2.2.1+git\nGit SHA: Not provided (use ./build instead of go build)\nGo Version: go1.5.1\nGo OS/Arch: linux/amd64\nOn Wed, Oct 21, 2015 at 6:42 PM, Eugene Yakubovich <notifications@github.com\n\nwrote:\nIn version/version.go\nhttps://github.com/coreos/flannel/pull/349#discussion_r42703984:\n\n-const Version = \"0.5.3+git\"\n+var Version = \"0.5.3+was-not-built-properly\"\n\nYeah but then we still need the bump revision dance. I like that git\ndescribe magic as it cleans it up.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/349/files#r42703984.\n. can you use jonboulle/clockwork instead? that's what we use in other CoreOS projects.\n. I think you intended to revert this bit right..?\n. er, right, but we want it to be +git don't we? or did you disagree with that premise of having it as descrptive as possible?\n. why are you only logging this every 5?\n. IMHO it'd be better to log the error when this happens rather than just that we're retrying; I think the signal-to-noise trade-off is acceptable\n. hrm, why wasn't this necessary previously?\n. Please revert this for now (at the very least for consistency with version/version.go), and we can revisit once we figure out the more general approach.\n. what's the ordering here, favourite->leastfavourite? ;-)\n. \n",
    "macb": "Is there any work under way for this? It'd be incredibly useful as right now if a machine loses a lease and gets a new one it renders any containers on the machine with no network connectivity.\n. What about if etcd is down for the duration of each lease @eyakubovich?\n. We've also run into this, box is generally around <5% idle cpu, so could also be cpu related.. ",
    "tomdee": "One implementation idea for this is in #610 . Also see #520 for some good questions about how flannel handles this at the moment.. When fixing this, we should make sure this failure scenario is discussed clearly in the docs.. This is now fixed in v0.8.0. I think multi-network support is now out of scope of flannel. see #638 . Closing as the question is answered - Flannel is L3 only. \n. @philips GUE doesn't seem to have caught on so closing this. Please reopen if you think it still needs more investigation.. This is already fixed. The http is now required. \nwithout it, you get this message\nE0313 11:35:30.968084   19078 main.go:114] Failed to create SubnetManager: parse 192.168.1.1:2379: first path segment in URL cannot contain colon. Network policy can be applied using Canal - https://github.com/projectcalico/canal. I think this issue is just caused by firewall ports not being opened. We already mention this in the docs. I'm not sure how else to progress this issue so closing.. I think we can close this issue if we add a link to the tool. This issue is out of scope for flannel. Static subnet reservation for a host can be done following this doc I added https://github.com/coreos/flannel/blob/master/Documentation/reservations.md\nStatic IP addresses for individual containers/pods is not something to flannel manages.. @vascofg Sorry, not at the moment. I would be happy to discuss designs if anyone wants to submit a PR.. See #200 for some more detailed discussion/investigation.. There is a fix in v0.7.0 which might help with this.. I'm assuming that v0.7.0 resolved this so closing. Please reopen if you're still having problems.. I'm not working on, but @jon-shanks might be. Ooops, I forgot about this PR too - #561 . There is some support for reservations documented here - https://github.com/coreos/flannel/blob/master/Documentation/reservations.md#reservations. Tracking multicast under #179 so closing this issue.. There is a dockerized build process in master now there should be OS agnostic. Closing rather than leaving a needs/rework PR open for months.\n. Closing this stale issue. I'm trying to work out what to prioritize here, are a see a few different things that \"IPv6 support\" could mean\n1) Adding IPv6 support for the control plane. This means using IPv6 for contacting the etcd server or the kubernetes API server (I presume both of these support IPv6?)\n2) Using IPv6 addresses for containers with an IPv6 host network. This should work for almost all backends (though IPIP doesn't support IPv6 and maybe some of the cloud providers might not too). \n3) Using IPv6 addresses for containers with an IPv4 host network. This might be useful for running a large number of containers on a host when there is a limited IPv4 private address range available. This would only work with backends that encapsulate data (e.g. vxlan)\n4) Using IPv4 addresses for containers with an IPv6 host network. This would be useful for running in environments that only support IPv6 on the hosts. Again, this would only work on backends that support encapsulation.\nFor both 3) and 4) there would be the issue of getting traffic between containers and hosts outside the flannel network (NAT64 could be used).\nThere's also the possibility of providing containers with both IPv4 and IPv6 addresses. This could just be an extension of 2) or it could involve elements of 3) and/or 4) if it would it be useful to do this even when the hosts don't have both IPv4 and IPv6 connectivity.\nIs doing just 1) and 2) (and maybe adding dual stack support) going to be enough for people, or is there any desire to get 3) and 4) done too. I'd love to hear people's thoughts on this.\n. I'm going to close this old issue - @YP28 please do reach out if you're still having problems though. @sekannia Currently the TTL is fixed to 24 hours.\nI'm going to close this issue - if anyone thinks there's still an issue here then please try to repro on the latest release.. Closing given #638 . No longer have client-server mode so closing. Glog is still widely used elsewhere (e.g. in kubernetes) so I don't see a compelling need to move away from it. Given that this issue has been open for >18 months without much interest I think it can be closed. \nIf people have strong feelings about moving away from glog then please write here and I can reopen the issue.. I've removed the unused sections around logging. I don't think this is an urgent feature (it's been open for so long) so closing.. Reproed on v0.7.1 and the client does loop trying to get a lease\n```\n\u279c  ~ docker run --privileged --rm -ti quay.io/coreos/flannel:v0.7.1 /opt/bin/flanneld --etcd-endpoints http://192.168.10.218:2379\nI0427 23:48:17.168473       1 main.go:132] Installing signal handlers\nI0427 23:48:17.168663       1 manager.go:136] Determining IP address of default interface\nI0427 23:48:17.168873       1 manager.go:149] Using interface with name eth0 and address 172.17.0.3\nI0427 23:48:17.168894       1 manager.go:166] Defaulting external address to interface address (172.17.0.3)\nI0427 23:48:17.181749       1 local_manager.go:179] Picking subnet in range 10.0.0.2 ... 10.0.0.2\nE0427 23:48:17.181776       1 network.go:102] failed to register network: failed to acquire lease: out of subnets\nI0427 23:48:18.192481       1 local_manager.go:179] Picking subnet in range 10.0.0.2 ... 10.0.0.2\nE0427 23:48:18.192534       1 network.go:102] failed to register network: failed to acquire lease: out of subnets\nI0427 23:48:19.201575       1 local_manager.go:179] Picking subnet in range 10.0.0.2 ... 10.0.0.2\nE0427 23:48:19.201598       1 network.go:102] failed to register network: failed to acquire lease: out of subnets\nI0427 23:48:20.211418       1 local_manager.go:179] Picking subnet in range 10.0.0.2 ... 10.0.0.2\nE0427 23:48:20.211494       1 network.go:102] failed to register network: failed to acquire lease: out of subnets\nI0427 23:48:21.219088       1 local_manager.go:179] Picking subnet in range 10.0.0.2 ... 10.0.0.2\nE0427 23:48:21.219146       1 network.go:102] failed to register network: failed to acquire lease: out of subnets\n``\nThat is the desired behavior and this issue is client/server specific so closing. Closing given #638 . Client/server mode specific so closing. See also https://github.com/coreos/flannel/issues/404. Closing as this has been open for a long time. It would be great to sort out OS packaging of flannel but this isn't the best place to track that.. @ingvagabund How do I get the latest version of flannel (v0.9.0) in stable?. Closing this old issue,please report if still having problems.. Flannel is configured through command line options and JSON in etcd. I'm going to close this issue as I can't see a better way to do this, if you have some suggestions I can reopen it.. I think this is resolved. Please comment if not and I can reopen.. Remote API is no more - closing.. @mtanino Would you be interested in submitting your patch as a PR?\n. Closing as not enough info to diagnose.. See also #439 \n. This is now merged in master.\n. I'm still not sure I understand what's being asked for here. The way the diagram is written above implies that the UDP backend is being used, but I think it would be better to just center the discussion around thevxlan` backend. This means that the flanneld isn't in the packet path.\n. @vijaykumark Sorry, flannel vxlan only supports binding to single ethernet device. I'm still not sure what flannel backend you're using - can you share that information?. @zbwright / @joshix Do you think this issue can be closed?. Now merged to master so closing\n. Does anyone have any benchmarks for how much of a performance benefit this provides?\n. @okamototk Are you still interested in getting this PR merged? If so, please rebase, thanks.\n. Looks like udpcsum is already being used:\nip  -d  link  show  flannel.1\n5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/ether c2:cb:20:31:38:1c brd ff:ff:ff:ff:ff:ff promiscuity 0 \n    vxlan id 1 local 172.17.4.201 dev eth1 srcport 0 0 dstport 8472 nolearning ageing 300 udpcsum addrgenmode eui64\n. @steveeJ Are you still working on this? Or shall I close the PR?\n. Closing this stale PR. I'm going to track this clock sync issue under #340 so closing this issue. #438 \n. I would like to merge this, but I need to coordinate it with #438 \n. I don't think changing the public IP requires the Docker daemon to be restarted? Is that documented somewhere or am I missing something obvious?\n. Good, I think the code at the moment will fetch a new lease if the public IP changes. So maybe we could add an option for migrating to a new public-ip - that doesn't sound like a bad idea.\nAs a workaround, you may just be able to stop the flannel daemon, update the public IP in etcd then start the daemon again (with the new public IP).. Small, simple and potentially useful change. We'll cross the etcd v3 bridge when we get to it. Merging.\n. If the image exists locally, Docker won't try to repull it. \nHopefully this answers your question, please feel free to reopen the issue if you need more help.\n. I'm interested to hear if anyone else has hit this or if anyone has a repro for it.\n. Can anyone reproduce this issue with flannel 0.7.0? I can't see how this can be a flannel problem but I would like to be proved wrong. I'm going to close this issue but if anyone is still seeing this problem then please reopen.. @glevand Any update on this? It would be great to be able to remove the dependency on bash.\n. We can close when we do the next release.\n. Fixed by #409 \n. Dupe of #409, closing.\n. This has now been fixed upstream. Let's fix this by taking a new upstream version\nhttps://github.com/vishvananda/netlink/pull/111\n. Closing as this requires significant rework\n. @kkirsche The original issue is still open, it's just that this PR doesn't seem like it's going anywhere so I'm not sure what value it brings leaving it open. The current flavor of the month seems to be logrus so that's probably a better project to use to replace glog.\n. Bug in netlink which has been fixed https://github.com/vishvananda/netlink/pull/111\n. It's now fixed in master. I'm planning to do a release with the fix in the next few weeks.\n. The original issue is caused by having an old kernel. I'm going to close this issue but if anyone else is still hitting it then please comment (with diags) and I can reopen.. I'm a little surprised to see service IPs in the flannel logs, shouldn't the kube-proxy be rewriting them?\n. Maybe related to #283 \n. Could you repro on v0.7.0?. Can you repro it on v0.7.1? v0.6.2 is quite old.... @aabed Can you describe your environment a little more? This is the systemd service file included in CoreOS right? Which version of Coreos did you see this problem on? \n. Closing as I'm not sure what the issue is. @aabed Please reopen if you're still having a problem here.. Looks like the --bip option to Docker doesn't behave as expected - there's not much to go on at https://docs.docker.com/engine/reference/commandline/daemon/ but potentially the behaviour here has changed. It appears to be resolving to the /26 boundary and allocating from .193 rather than .202 as hoped.\nOne workaround might be to use CNI with kubernetes and the flannel CNI plugin. \n. This etcd client has been updated so this should no longer be a problem. Please comment and I can reopen if anyone can still repro this bug.. Can you confirm what versions of docker and kubernetes you're using with the two different versions of CoreOS?\n. I think this problem was introduced in 0.5.4 and was fixed with https://github.com/coreos/flannel/pull/745 in v0.8.0. I suspect this is related to https://github.com/coreos/flannel/issues/433\n. Ah, I see you have ip-masq off though in flannel, so maybe not.\n. Closing as I assume this is now fixed.. If you followed those instructions, then your running the latest master code right?\nYour SubnetLen will be /28 so I'm assuming your first host grabbed the 10.43.116.192/28 subnet and from the logs, it looks like the 2nd host tried to get 10.43.116.208/28 but then failed to register it. Could you add a dump of (the relevant part of) your etcd so we can see if this is the case?\nI'm a little concerned that there could be multiple distinct etcd servers running here. It's odd that you're not able to connect to etcd from the second host.\n. You're setting the network to a small range (/26) - people typically use a /16 for the network.\nSince you're not setting SubnetLen it's going to default to /27 - per\nSubnetLen (integer): The size of the subnet allocated to each host. Defaults to 24 (i.e. /24) unless the Network was configured to be smaller than a /24 in which case it is one less than the network.\nI suspect there's an off-by-one bug in the subnet allocation logic when the Network is only twice the size of the SubnetLen. It only allows a single host to be allocated.\nAlthough this looks like a bug, I'm not sure how quickly it will be fixed given that this is a slightly odd scenario. Could you try using a larger network and/or a small SubnetLen?\n. This code is relevant\nif cfg.SubnetMin == ip.IP4(0) {\n        // skip over the first subnet otherwise it causes problems. e.g.\n        // if Network is 10.100.0.0/16, having an interface with 10.0.0.0\n        // makes ping think it's a broadcast address (not sure why)\n        cfg.SubnetMin = cfg.Network.IP + subnetSize\n. ping @philips @steveeJ Any objections to this going in?\n. Kind of blocked on #407 which is blocked on someone finding the time to do some GCE testing.\n. OK, this is updated to move to glide. This was much easier for me to get working than Godep. (See https://github.com/coreos/docs/issues/775 for general CoreOS support for migrating to glide)\nThis fairly large PR also updates some upstream deps which have been removed (the Google code ones) and update vishvanda/netlink\nIt updates the travis.yaml file to make it very similar to the CNI one. The crucial change is removal of Go 1.4 support (since a vendor directory is now being used)\n@steveeJ / @philips Can I get a LGTM so I can get this merged?\n. I've verified that this fixes the vxlan port number problem.\n. So, just to make I'm following the discussion here...\n1) I would like a have a standard/consistent way of building the flannel binary. This would make build (and releases) reproducible. The built binary would need to be static.\n2) There's a desire to be able to build the binary for other platforms (e.g. armv6, arm64, ppc64le) but I don't think we could call them \"supported\" platforms\n3) Flannel should be deployable in a container. But if it's a static binary, then the container isn't too important right?\nFor the building, can we not use the golang:onbuild container because of the need for CGO support?\nping @glevand @luxas \n. A build based on kube-cross looks like the best option to me.\nPackaging looks tricky though. I'm leaning towards the suggestion from @luxas \n\nI think we should compile iptables from source, and put it in a busybo:glibc image along with a statically linked flannel binary\n\n@glevand How would we go about building busybox with iptables enabled?\n. This should all be covered by #462 so closing.\n. Hi @sokoow, since there's not been a response on here I'm going to close this issue. It's probably worth asking on the coreos-users mailing list or the #coreos IRC channel on freenode.org\n(See https://github.com/coreos/flannel/blob/master/CONTRIBUTING.md#email-and-chat)\n. What operating system are you running? You need at least a 3.14+ kernel for VXLAN\n. See https://github.com/kubernetes/kubernetes/issues/20391\n. Thanks for the question but I'm not a redis expert. I've taken a quick look at the redis sentinel documentation (http://redis.io/topics/sentinel) and I can't see that it would be a problem running it on flannel. \nAlthough the redis instances are on different subnets, they can still communicate with each other. I think you're saying they're using the wrong IP addresses but I don't understand where the wrong IP addresses are coming from.\n. Hmm, looks like redis sentinel has caused problems before - #412 \n. Closing this old issue. Please comment if still a problem and I can reopen.. @gigaroby Are you able to rebase this?\n. Rebased in #644 . Dockerfile is now updated\n. LGTM\n. LGTM\n. @adiclepcea Are you still working on Windows support? Would you still this PR to be merged?\n. This now works in master so closing.\n. Ulp, this has been open for almost a year now. @adiclepcea I'm really sorry but I'm going to close this and the other Windows PR. They would need a rebase and rework before they can be merged and I'm not sure if I'm OK with adding all the additional complexity to this linux focused project.\nIf you would like to start maintaining a windows fork, I'd be happy to link to it from the readme?. LGTM\n. Fixes #450 and #454 \n. Also superseded by #462 so closing \n. Closes #417 \n. I really don't think it's worth having tests for this. It's bad enough having this ugly piece of functionality, but adding a test and a way to run the test is only going to make it harder to remove. A better long term approach would be to wrap this functionality into flanneld itself, but it's not a good use of by time to do that at the moment.\n. @steveeJ I've added a test script. PTAL.\n. OK, updates pushed @steveeJ PTAL.\n. OK, indentation fixed. @steveeJ PTAL\n. It is already being tested on travis. I'm not sure what you mean by splitting it into a separate commit?\n. OK, squashed. I'm going to merge this when it goes green.\n. Needs #461 to be merged first (since busybox doesn't have bash)\n. Close, fixes or supersedes the following PRs #460 #439 #417 and maybe #201 \nClose, fixes or supersedes the following Issues #454 #450 #389 \n. Ping @luxas @glevand\n. Thanks @luxas for the review, I've made markups and pushed another commit.\nI prefer the duplication of multiple Dockerfiles to needing to create temp files and use sed, so I've left that as it is.\n. LGTM - merging\n. Closing given #638 . LGTM\n. LGTM\n. Hi @luxas, no firm commitment right now but I'm working on it today, so possibly this week and if not then hopefully by the end of next week. Can you point me to the DaemonSet discussion?\n. @antoineco @luxas Images are now pushed https://quay.io/repository/coreos/flannel?tab=tags :fireworks: \n. It's working again now\n. The make problem should be fixed on current HEAD\n. The required rkt func is now in a release - v1.12.0\n. I've started taking a look at this but I'm finding it tricky to run. It needs a more \"scaffolding\" before it can be merged - at least\n-  A make target that runs it and builds any prereqs (e.g. creates the required ACI image)\n- Some documentation (either as a separate .md file or better comments in the bash script) - e.g. \n. Also, there are a bunch of required dependencies - these need to be documented better, so far I've found\n- rkt\n- jq\n- actool\n. Also, the script hangs without an obvious reason why when downloading the etcd aci. This happens when the CoreOS keys aren't trusted\n. @eyakubovich I've now run the script using an aci generated from master. Given the bad return code, I'm assuming the \"tests\" failed - can you help me diagnose why this isn't working?\nsudo ./functional-test.sh flanneld-amd64-v0.5.3-107-g982dbdb.aci\nimage: using image from file flanneld-amd64-v0.5.3-107-g982dbdb.aci\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name coreos.com/etcd:v3.0.3\netcd launched\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network default with type ptp\nstage1: warning: no volume specified for mount point \"data-dir\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: warning: no volume specified for mount point \"data-dir\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nRunning test for backend=udp\nimage: using image from local store for image name coreos.com/rkt/stage1-fly:1.11.0\nimage: using image from local store for image name coreos.com/etcd:v3.0.3\n{ \"Network\": \"10.10.0.0/16\", \"Backend\": { \"Type\": \"udp\" } }\nflannel config written\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name quay.io/coreos/flannel-amd64:v0.5.3-107-g982dbdb\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name quay.io/coreos/flannel-amd64:v0.5.3-107-g982dbdb\nflannel containers prepared\nflannels running\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network brkurFDJku with type bridge\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network brkurFDJku with type bridge\nnetworking: loading network default with type ptp\nnetworking: loading network default with type ptp\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\n[12952.784046] flannel-amd64[5]: Bad system call\n[12952.785562] flannel-amd64[5]: Bad system call\nenter: pod \"1ffca9ea-6200-474a-86bc-71c970adb021\" isn't currently running\nenter: pod \"1ffca9ea-6200-474a-86bc-71c970adb021\" isn't currently running\nenter: pod \"d3226a74-2e4d-4112-9d42-ca9138eaa1d2\" isn't currently running\nTest for backend=udp: exit=1\nstop: pod \"d3226a74-2e4d-4112-9d42-ca9138eaa1d2\" is not running\nstop: failed to stop 1 pod(s)\nstop: pod \"1ffca9ea-6200-474a-86bc-71c970adb021\" is not running\nstop: failed to stop 1 pod(s)\nRunning test for backend=vxlan\nimage: using image from local store for image name coreos.com/rkt/stage1-fly:1.11.0\nimage: using image from local store for image name coreos.com/etcd:v3.0.3\n{ \"Network\": \"10.10.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } }\nflannel config written\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name quay.io/coreos/flannel-amd64:v0.5.3-107-g982dbdb\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name quay.io/coreos/flannel-amd64:v0.5.3-107-g982dbdb\nflannel containers prepared\nflannels running\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network brkurFDJku with type bridge\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network brkurFDJku with type bridge\nnetworking: loading network default with type ptp\nnetworking: loading network default with type ptp\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\n[12959.024640] flannel-amd64[5]: Bad system call\n[12959.027204] flannel-amd64[5]: Bad system call\nenter: pod \"e31b7622-9796-4186-92a8-49c3a5ec95a2\" isn't currently running\nenter: pod \"e31b7622-9796-4186-92a8-49c3a5ec95a2\" isn't currently running\nenter: pod \"415dfb51-cd32-48b2-b26e-c449b62cf4c3\" isn't currently running\nTest for backend=vxlan: exit=1\nstop: pod \"415dfb51-cd32-48b2-b26e-c449b62cf4c3\" is not running\nstop: failed to stop 1 pod(s)\nstop: pod \"e31b7622-9796-4186-92a8-49c3a5ec95a2\" is not running\nstop: failed to stop 1 pod(s)\nRunning test for backend=host-gw\nimage: using image from local store for image name coreos.com/rkt/stage1-fly:1.11.0\nimage: using image from local store for image name coreos.com/etcd:v3.0.3\n{ \"Network\": \"10.10.0.0/16\", \"Backend\": { \"Type\": \"host-gw\" } }\nflannel config written\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name quay.io/coreos/flannel-amd64:v0.5.3-107-g982dbdb\nimage: using image from local store for image name coreos.com/rkt/stage1-coreos:1.12.0\nimage: using image from local store for image name quay.io/coreos/flannel-amd64:v0.5.3-107-g982dbdb\nflannel containers prepared\nflannels running\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network brkurFDJku with type bridge\nnetworking: loading networks from /etc/rkt/net.d\nnetworking: loading network brkurFDJku with type bridge\nnetworking: loading network default with type ptp\nnetworking: loading network default with type ptp\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\n[12965.298838] flannel-amd64[5]: Bad system call\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\nstage1: warning: no volume specified for mount point \"run-flannel\", implicitly creating an \"empty\" volume. This volume will be removed when the pod is garbage-collected.\nstage1: Docker converted image, initializing implicit volume with data contained at the mount point \"run-flannel\".\n[12966.290180] flannel-amd64[5]: Bad system call\nenter: pod \"ea79f540-8317-4a7a-ba3c-b6c625da62e1\" isn't currently running\nenter: pod \"ea79f540-8317-4a7a-ba3c-b6c625da62e1\" isn't currently running\nenter: pod \"31078e5b-a9e9-4779-b413-bddc4e161915\" isn't currently running\nTest for backend=host-gw: exit=1\nstop: pod \"31078e5b-a9e9-4779-b413-bddc4e161915\" is not running\nstop: failed to stop 1 pod(s)\nstop: pod \"ea79f540-8317-4a7a-ba3c-b6c625da62e1\" is not running\nstop: failed to stop 1 pod(s)\n\"db7a409f-a0cd-4643-aacc-16cb2f38bf99\"\n. I just tried running this again and the first run seems to fail, but the second got a ALL TESTS PASSED\nAnnoyingly a get a bunch of veths left over and I saw some very strange issues with having 127.0.0.1 hijacked until I rebooted which I suspect were related to this script.\nAlso, my terminal is broken after running this, I get no newlines printed. Running reset fixes it.\nBoth these issues need to be fixed before it can be merged.\nI have some reservations about bash being the right tool for the job though. Running the script produces a whole lot of hard to debug output and it's not very clear when or why a test is failing. I'm going to think a little more on whether bash is OK for now and a step in the right direction or if we should investigate if there's a better option before merging.\n. I can review this but it won't be until next week.\n. So, this all looks very cool to me and I support it!\nThe upside:\n-  It makes flannel easier to deploy on kubernetes which is a really good thing.\n-  It's a small amount of code (ignoring deps), and it's doesn't interact with any existing function - there's no big refactor required or anything like that.\nDownsides are:\n-  it makes the vendor directory much bigger and harder to manage\n-  it make the flanneld binary much bigger (20 MB -> 60 MB)\n-  (Both of the above concerns could go away as kubernetes get their client libraries sorted out)\n-  It adds a little more code to maintain.\n-  It adds a new way of deploying flannel which may need testing every release\nTo me, the upsides outweigh the downsides so it gets a \"looks good in principle\" from me.\nBefore it gets merged to master\n- there are a few comments and questions to address\n- the CNI related parts need rethinking (and probably removing from the go code)\n- it needs documentation\n. @mikedanese What do I need to do to help you get this merged (other than hit the merge button :smile:). Some thoughts:\n- Remove the vendor directory changes - but make sure that the glide.yaml and glide.lock are correct. This will make it easier for people to review the code.\n- Documentation - I don't think there is any at the moment - are you able to add a doc explaining briefly the motivation behind this, roughly how it works and how it can be configured/deployed?\n- And finally, I'm happy for their to be an example of deploying this under kubernetes, but that example can't be the \"master\", i.e. it's not a deliverable of the flannel project and I don't want other projects relying on it. I need to be able to change/update the \"example\" without worrying about it breaking other projects.\n. The example can be checked into the master branch, I just don't want other projects automatically pulling that example yaml file and using it in their deploys. There was some discussion about what should be in flannel and what shouldn't be. e.g. An example script for deploying the flannel CNI plugin would be OK, but it shouldn't be the actual script that any deployment relies on.\n. Merging - we'll add more docs in a followup PR.\n. Docs are being tracked in this issue - https://github.com/coreos/flannel/issues/587\nA PR adding any docs would be very welcome.. So that flannel doesn't need to talk to etcd. It makes configuration easier. @rojingeorge Would you be able to repro with v0.7.0?. ldd dist/flanneld-amd64\n  linux-vdso.so.1 =>  (0x00007ffdbacdd000)\n  libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f0c111e7000)\n  libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0c10e1e000)\n  /lib64/ld-linux-x86-64.so.2 (0x00005611ea470000)\nIt's compiled in the kube-cross container - here's the glibc version\ndocker run gcr.io/google_containers/kube-cross:v1.6.2-2  /lib/x86_64-linux-gnu/libc.so.6\nGNU C Library (Debian GLIBC 2.19-18+deb8u4) stable release version 2.19, by Roland McGrath et al.\nCopyright (C) 2014 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.\nThere is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE.\nCompiled by GNU CC version 4.8.4.\nCompiled on a Linux 3.16.7 system on 2016-02-29.\nAvailable extensions:\n    crypt add-on version 2.1 by Michael Glad and others\n    GNU Libidn by Simon Josefsson\n    Native POSIX Threads Library by Ulrich Drepper et al\n    BIND-8.2.3-T5B\nlibc ABIs: UNIQUE IFUNC\nFor bug reporting instructions, please see:\n<http://www.debian.org/Bugs/>.\n. Closing and tracking this just under #193 . @chenzhiwei See https://github.com/coreos/flannel/blob/master/Documentation/reservations.md#reservations for some guidance on this. Fixes #494 \n. Looks pretty nice to me. There are a few questions/comments to address. Also - have you tried running this on a CI system? Do you know if it works on travis?\n. Still no answer on the \"does it work on travis\" question, but merging anyway as I don't want it to languish any further.\n. I tested this before release and it worked for me. Can you let me know if this is working with the previous flannel release?\n. Are you able to try using the new 0.6.0 release of flannel?\n. I'm assuming that the solution from @monder works so closing. Please comment if this is still a problem and I can reopen.. This works fine for me. What version of Go are you using? Can you share a little more about your environment\n. Closing without a repro.\n. Can you share a little more about the environment you're running in? What ARCH do you have? Are you running in a container?\n. Yep, this is a problem :unamused: \nCaused by a late change to stop compiling the binary statically (since it caused problems with the gce backend).\nRelated to https://github.com/docker-library/official-images/issues/1807\n. I've now released v0.6.1 so closing\n. Ooops, good spot. LGTM, merging. \n. @steveeJ Updated, PTAL.\n. @jellonek What tools do you mean? \n. @steveeJ I would like to merge this ASAP, it's not great having the bad information sitting on master...\n. This container is no longer used so closing.\n. How are you starting flannel? I suspect that you're not passing in the location of your etcd server(s), which is why it's working on your master but no other nodes.\n. Assuming this is caused by AWS ELB timeouts. So closing and tracking elsewhere.. @gl328518397 Thanks for the report. Have you tried this with a previous flannel release? Can you confirm that it was working before?\n. Closing as there's no repro.\n. @gl328518397 How did you fix it? Was the fix something that other people would benefit from?\n. LGTM\n. @eyakubovich Congratulations on the new job! Are you looking for someone to take over this PR now?\n. Closing in favour of #637. I'd love to see the answer to this written up as a doc to go in this repo. PRs very much welcome.\n. @luxas I was trying to avoid the situation where external projects rely on this \"sample\" config file. I don't believe that the flannel project is the right place for the canonical version of this file. See my comments https://github.com/coreos/flannel/pull/483#issuecomment-247160212 and https://github.com/coreos/flannel/pull/483#issuecomment-247389118\n. @luxas I still don't really understand this change. Do you need the version pinning stuff or just the extra label?\n. Sounds like a very interesting idea. Would you be interested in trying it out and/or writing it up into a longer guide.\n. Fixed in v0.9.0. Sorry @rosenhouse I don't have a good answer for this at the moment. I'm going to close this issue and just track making this better as part of #29 . LGTM\n. LGTM\n. PRs welcome :smile: \n. A fix for this is now merged. There is no speed limit on this interface as it's virtual.. Other than the machinezone article, I'm not aware of the interaction of this parameter with flannel. Flannel itself doesn't change the value an it doesn't document that it should be a different value.\nAlso, this parameter has been removed from recent kernels - https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc\nSome more background on it is here https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux. The .1 comes from the vxlan ID. Are you saying that you want to have flannel0.1?\n. Thanks @aaronlevy. Does the flannel0.1 device persist across restarts of flanneld but not reboots. Maybe there's a code path that creates routes differently when the device already exists\n. I think that blowing away all the local table routes isn't the best idea now. Having identified the difference in the broadcast routes, there is a potential solution in this approach but the code as is stands is too heavy handed and removes more than just the broadcast routes.\nIdeally we wouldn't be manipulating the local table at all - but I need to did into the ordering of how we're creating the device, bringing it up, assigning the IP address to it and then creating the additional route. I'll keep mulling this over and I'll try to work on a fix on Monday. I think you're deleting too much in the vendor dir too\n. I changed the logging so hopefully the behaviour around the --iface and --public-ip options should be a little clearer from the logging output at least.. @TamalSaha Can you share some of kubernetes commands you were using to repro this? Where are you pinging to and from, is it from your master node to a pod on a different node?\n. @luohoufu Is there anything that you can share here (or in a PR) to help future people who run into this problem?\n. There are no current plans - feel free submit a PR though.\n. Closing as this is just a standard glide problem. It might be worth us pinning more in the glide.yaml, but I have no current plans to do this,. I have no hints but welcome any comments from the community (or PRs :smile:)\n. @toralf Sorry I can't be more helpful here. I'm going to close this issue as it's quite specific. I think if you want it resolved you'll need to submit a PR.. Hi @vinayk06, I can't work out what the problem is and this has been open for a long time so closing. If you're still having problems then get back in touch.. Thanks @vaijab looks good. Merging.\n. I think this is a Docker issue so is probably worth raising there. @Ahrotahntee are you still seeing this as a problem or can I close this issue?. I'm not able to repro this so I assume it's been fixed already on the docker side. Closing.. See also #554 . I not sure about this issue. I suggest raising it with the Azure team or maybe the linux kernel team. It doesn't seem like a flannel issue so closing.. Closing this issue from a very old version of flannel. Closing as this has gone off topic. Please open new issues with specific issues.. It should be covered in the readme. Just type make dist/flanneld-amd64 (assuming CoreOS has make installed). LGTM. @suchisubhra Sorry I wasn't able to help with your issue. Could you please retry with the latest versin of flannel and if you're still seeing a problem then share your logs?. @suchisubhra I'm not sure I understand the question, could you elaborate?. Yes, it's by design. The arp table is showing the VTEP mac of the host that \"owns\" the containers. So, it's the same for them all because they are on the same subnet.. client/server mode has been removed. Pushed #594 to improve the instructions. \nThe info from @besnik is exactly right (https://github.com/coreos/flannel/issues/552#issuecomment-269691242) Please add this to the README in a PR if you'd like to see it included.. @HerrmannHinz if you have your GOPATH set up properly you shouldn't need to go get any packages,. Sorry, there's not enough information here for me to help.. Flannel still uses the etcdv2 API. It can still talk to a etcd V3 server and share it will Kubernetes even if kubernetes is using etcdv3.\nSo it works, but it's not ideal so I would like to update flannel to use etcdv3 when I get a chance.. Yes, I hear and understand the need for etcdv3 for the heartbeat support.. That PR just updates the library version - flannel still only works with the etcdv2 API for now. @alapidas yes\n@dxdeidara Sorry, it's still not supported. LGTM. @djsly Where is that ExecStartPost line coming from?. So this is a CentOS packaging issue? Do you know who maintains the CentOS package?. I'm going to close this issue as there's not enough information to diagnose. Please add more and I can reopen the issue.. I'm not sure who owns or maintains the redhat flannel package. . This has been open for a long time and it's being tracked from the centos side so closing here.. Could you add some documentation for this? At least a small update to the README but the more the better.. LGTM - merging. @aaronlevy Awesome work on getting this fixed, looks like it was much more complicated than we initially thought (and thanks @bison for the assist) I'm still getting my head around the issue and your fix but here are some initial thoughts:\n1) Have you looked into the failing tests?\n2) How confident are you that it's safe to use the first subnet (given the slightly unhelpful comment https://github.com/coreos/flannel/blob/master/subnet/config.go#L73-L80)\n3) Given there was some concern about this also being a problem for the UDP backend, do you think it's worth investigating/fixing that too?. As discussed offline, do you want to split the vxlan bits out of this and make this PR just about the Kubernetes fixes. Then we can continue discussion of the vxlan fix back over on #533. @pingles Are you able to rebase this PR and fix the conflict?. Great, thanks @pingles . OK, I'll know for next time \ud83d\ude04 . replaced by #569 . Hi, please re-open if you're still having problems. @spacexnice Are you happy to help out with supporting this if users hit problems?. @spacexnice Excellent. Before I merge, it would be great to have a guide or something for this, similar to https://github.com/coreos/flannel/blob/master/Documentation/aws-vpc-backend.md would you be able to add that?. @spacexnice looks good. Can you just change the the images to be the official 0.7.0 release?. Merging!. LGTM :smile:\n. Very simple test only change so merging. rebased. Cool! Merging.. I have no plans to implement it imminently, but PRs are most welcome!. Thanks, looks good.. LGTM. Hi @abdoo, I can't tell from the issue report what the problem is. Could you try to net it down to something that I can repro?. Not that I'm aware of.. Hi @drishticode. If you clone to a directory in your GOPATH that matches the package name then you shouldn't have any problem. I'm going to close this issue but please raise a PR if you think the build instructions need to be improved.. I don't have this issue and I can't repro this with an official release so closing. Please reopen if you can repro with a released version.. @wksw Could you run with v0.7.0 which has improved logging in this area? And let me know the full output?. @steveeJ Is this something you could help with? It would be great to get v0.7.0 in yum too, but I have no idea where to start?. Also ping @jeremyeder would you be able to help?. Actually closing in favor of #346 . Awesome, thanks for the replies and help. I would love to get v0.7.0 in Centos and Fedora and to nderstand the way way to keep those packages up to date. @jeremyeder you can get me at tom@tigera.io if you want to discuss further. Excellent, thanks @ingvagabund!. LGTM - merging. @peiqi-caicloud are you able to reproduce this on a smaller cluster or with the latest release of flannel?. Possibly related to #414 . @xiang90 Done. But please note that this really is far from complete - I've really done the bare minimum to get something that works. Of course, any and all feedback is still appreciated!. Could this be a centos problem? What version of centos are you running?. Sorry closing as I can't diagnose this without more information. Please reopen if you're still hitting this issue - my guess would be either an old kernel/os or maybe a permissions issue.. It's not an error. The logging in this area has been improved in v0.7.0. Thanks for spotting this. Merging.. Fixed by #606 . Your kernel version is very old and I assume has limited vxlan support. Flannel really should provide some indication that it's not working, but doesn't appear to at the moment. \nFrom a quick internet search, it would seem that a Kernel version of >3.9 is normally recommended, though this page does seem to say that Centos >6.5 should be enough for VXLAN in general -http://docs.cloudstack.apache.org/en/latest/networking/vxlan.html. It's all public. Just take a look at the .travis.yml file.\n\nOnce per commit\nWhatever travis uses\nWhatever travis uses . Could you share a little more detail? In what way is it slow?. Yes, I think this sounds like a really useful feature.. Originally raised and tracked in #29 - I'm going to close this issue and just track it from #29.. Thanks @rosenhouse that information all looks correct to me. @xiaoping378 hopefully that resolves your issue to closing this.. As of v0.9.0 flannel is packaging ca-certificates (at least for amd64) so I think this can be closed.. Sounds useful to me. . Tested on kubeadm with the 1.6 beta-4 - so merging.. This is now fixed in v0.8.0. @Agrosis It would be great to understand why your're hitting this. Without some more info this can't be debugged or fixed. Unless we can find a repro for this I'm going to have to close it.. TTL of the subnets is so they are cleaned up if the host goes away.\nYou can change the renewal time so it does it more than an hour before it would expire.\n. @ZChunnian Are you using the host-gw backend?. This seems like a pretty good bug! My initial guess would be that something is going wrong in https://github.com/coreos/flannel/blob/master/subnet/watch.go#L30 - maybe the clocks on the servers are out of sync? ( @ZChunnian could you check this?). This is causing too many events to be sent to handleSubnetEvents()\n\nAnd then memory is being leaked because even through the route has already been seen (https://github.com/coreos/flannel/blob/master/backend/hostgw/network.go#L114) it still gets added to the list.\nSo we can fix the leak by using a \"set\" rather than a list (or only adding the route if it doesn't already exist), but that still doesn't fix the underlying problem of why so many events are being generated.. LGTM. This might need a tweak now that #650 has gone in.. @Pensu If you could squash the commits down to one then I can merge this.. @Pensu Sorry wrong person! @salamani Would you be able to squash the commits, then I can merge?. Closing since I've merged the commit from this PR to master already.\nBUT - the build seems to be failing and I have no way to test it. @salamaniibm would you be able to take a look? I'm seeing the following in the travis output:\ndist/iptables-s390x: ELF 64-bit MSB shared object, IBM S/390, version 1 (SYSV), dynamically linked, interpreter /lib/ld64.so.1, for GNU/Linux 3.2.0, BuildID[sha1]=8f1e00df428b34b16a92ccdb400e7520c40c5a69, not stripped\ndocker run --rm -v /home/travis/gopath/src/github.com/coreos/flannel:/host gcr.io/google_containers/kube-cross:v1.7.5-3 cp s390x-linux-gnu/libc-2.23.so /host/dist/libc.so.6-s390x\ncp: cannot stat 's390x-linux-gnu/libc-2.23.so': No such file or directory\nmake[1]: *** [dist/libpthread.so.0-s390x] Error 1\nmake[1]: Leaving directory `/home/travis/gopath/src/github.com/coreos/flannel'. The vxlan code was significantly changed in the last couple of releases so I don't think this is till a problem.. @mkutsevol Thanks for all your work on this PR, it's a really important feature that lots of people want.\nI've taken a really quick look at it and have a few comments\n It would be great to get it rebased onto v0.8.0 so that it could potentially be included in the v0.9.0 release\n The increase to the size of the binary isn't great. On k8s, it would make sense to package and run it as a different container (but in the same pod).\n And speaking of k8s, I think it would need to work on k8s too (but maybe I could help implement that if you're not a k8s user)\n Could you write a little about why you needed to create createBackendData and getBackendData?. I'll keep thinking this over but my initial though on multiple images vs. putting extra burden on the user to configure it is that we just go for multiple images. Though, I guess it would be nice to have the option of using an OS strongswan package if users really want to do that.\n. I've rebased this onto master and pushed to https://github.com/tomdee/flannel/tree/ipsec. @mkutsevol Thanks for your continued effort on this PR. I'll try to help out some more, maybe on Friday.. This PR should help on the packaging front https://github.com/coreos/flannel/pull/860. I chatted offline with @mkutsevol and we came up with a plan for getting this merged. There are four things that I think we need before this can be merged.\n\nDefining the user experience and documentation. All the config options need documenting, as well as some guidance on how to generate the PSK. The docs should be clear that changing any of the config requires a daemon restart (at least for now). \nTesting. There should be some manual testing and automated functional tests.\nPackaging. It might be OK to just have this available on amd64 initially, but it would be great if if could work on all platforms. A kube-flannel-ipsec.yml files should also be created.\nThe code. I've taken an initial look at the code and I don't see any glaring problems. I'll need a to take a more detailed look before merging, but it's definitely on the right track.\n\nMax and I will work to get through these items and try to get this merged ASAP but of course any help from the community would be appreciated!. @mkutsevol I merged the PR to switch over to Alpine so that should greatly simplify the build and packaging part of this PR.. @klausenbusk See https://github.com/coreos/flannel/pull/898. @mkutsevol I've rebased this again onto master and pushed it to https://github.com/tomdee/flannel/tree/feature/ipsec\nUnfortunately, I can't get it to work. The logs look fine to me, but I can't ping when running the e2e test - would you be able to take a look at see what's going wrong? Ping me on slack if you need help getting it running.. IPSec support has just been merged to master in #929 :fireworks:\nIt's still classed as \"experimental\" but it would be great if everyone could try it out, and provide feedback (and ideally PRs!) on the user experience, the code, the documentation, the tests etc...\nDocs: https://github.com/coreos/flannel/blob/master/Documentation/backends.md#ipsec\nImage: quay.io/coreos/flannel-git:v0.10.0-8-g6b98346d. Yes, that's the plan. If you're relying on this feature could you describe your use case (or if you're not comfortable doing it in public then feel free to email me)?. Given the port, I assume you're asking about vxlan?\nI'm sorry but I don't know the answer to your original question. Maybe you could use an iptables rule?. @timchenxiaoyu could you provide a more detailed list of steps to reproduce this problem? At the moment there's not really enough information for me to diagnose.. I see slightly better performance on my laptop, but yes, if you use vxlan you will get less than wirespeed. You can get back up towards 10 GBits/sec by using a larger MTU, more streams and by having a NIC that supports VXLAN offload.. This needs rework after the kube subnet manager was added. I'm looking for help on this one, getting it rebased and tested. ( @gunjan5 or @mgleung ). This has been open for a long time with no interest. Closing. That sounds fine to me, but I'm not sure I can promise it for the next release. If you'd like to submit a PR (changing the Makefile) then I'll happily merge it.. @cmluciano Do you have a working 1.6 file that you could submit as a PR?. LGTM. A \"legacy\" manifest is now provided here https://github.com/coreos/flannel/blob/master/Documentation/k8s-manifests/kube-flannel-legacy.yml. Interesting. UDP is the only backend with C code. Maybe something in there isn't s390x compatible?. Sounds good, thanks for tracking it down.. Closed by #703 . Thanks for raising this but it doesn't match my understanding. Flannel shouldn't be leaving any state data in etcd - can you give me a specific example of what you're seeing?. @xh3b4sd I'm going to close this issue as I think flannel is getting rid of unused config in etcd. If you're still seeing this problem then please reopen the issue. Thanks @teemow (and @calvix for reporting) - closing.. I think it would be better to change ${PWD} to the make variable $(CURDIR). LGTM - merging. @mrahbar I can't see the bug here, though the code did get changed a little here. The nodename is populated from pod.spec.NodeName. If this is still a problem, please comment and I can reopen.. You can use the latest version of etcd with flannel but flannel still stores the data in the v2 format.. So if I'm understanding this correctly, we should fairly urgently switch flannel to use the v1.6 client, and before 1.7 comes out, switch it to using patch?. The flannel kube subnet manager uses the \"k8s.io/kubernetes/pkg/client/cache\" package which doesn't exist in 1.6. Is there anyone with more kubernetes client experience than me who could help fix this? cc @mikedanese \n(Also this could be a chance to resolve/improve https://github.com/coreos/flannel/issues/674). Urghh - it also imports \"k8s.io/kubernetes/pkg/controller/framework\" which was removed in v1.4 AFAICT. Yep - the 0.7.1 release should have the PATCH fix in and the lates flannel-git (https://quay.io/repository/coreos/flannel-git?tab=tags) releases should have the client-go fixes in.. This is now fixed. LGTM. @hustljl What kubernetes version are you using? What flannel version are you using? Which manifest(s) did you use? Did you try the RBAC one?. @joshix and @zbwright Can you run with the intro section?. For the getting started guide, this is what I'm thinking so far (it's a bit rough and ready but you hopefully get the idea):\nGetting started\nIt's really easy to use flannel with Kubernetes. Flannel supports using the kubernetes API server as its backing datastore which means there's no need to deploy an etcd server for flannel, or even for flannel to access the etcd server that Kubernetes uses. This flannel mode is known as the \"kube subnet manager\" and demonstrated below.\nAdding flannel\nFlannel can be added to an existing Kubernetres clusters but it's easiest if no pods have been started yet (host networked pods are fine). A great way to create a new Kubernetes cluster is with kubeadm which helpfully points out the need to define \"a pod network CIDR\" for example by passing in --pod-network-cidr 10.244.0.0/16 when running kubeadm init. This pod network CIDR is the key prerequisite for using flannel with the kube subnet manager. If you're running flannel through some other means, then the pod cidr might be set automatically for you or you can set it manually by passing --pod-cidr to each of your kubelets. It's also possible to set it per-node using kubectl, e.g.\nkubectl patch node minikube -p '{\"spec\":{\"podCIDR\":\"10.33.12.0/24\"}}'\nAdding flannel to the cluster is then as simple as:\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\nAnd if you're using RBAC also applying\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml\nAlthough not required, it's highly recommended that you use CNI for starting your pods. This requires your kubelets to be started with --network-plugin=cni. This is already the default if you're using kubeadm.\nWhat just happened\nThe manifest creates three things\n1. A service account for flannel to use\n2. A ConfigMap containing both a CNI configuration and a flannel configuration. The network in the flannel configuration should match the pod network CIDR. The choice of backend is also made here and defaults to vxlan.\n3. A DaemonSet to deploy the flannel pod on each Node. The pod has two containers 1) the flannel daemon itself 2) a container for deploying the CNI configuration to a location that the kubelet can read it from.\nWhat can I do now\nWhen you run pods, they will be allocated IP addresses from the from the pod network CIDR. No matter which node those pods end up on, they will be able to communicate with each other. Try it by running a couple of simple pods and pinging between them, e.g. using kubectl run -i -t busybox1 --image=busybox --restart=Never\n. For running manually, I was thinking a complete etcd-based mini-guide would be instructive, e.g.\n1) Download a flannel binary - wget https://github.com/coreos/flannel/releases/download/v0.7.0/flanneld-amd64 && chmod +x flanneld-amd64\n2) Run the binary - sudo ./flanneld-amd64 it will hang waiting to talk to etcd\n3) Run etcd - see instructions at https://github.com/coreos/etcd/releases/latest or if you have docker just do docker run --rm --net=host quay.io/coreos/etcd\n4) Observe that flannel can now talk to etcd, but can't find any config. So write some config. EIther get etcdctl from the releases page above or just use docker again\ndocker run --rm --net=host quay.io/coreos/etcd etcdctl set /coreos.com/network/config '{ \"Network\": \"10.5.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}'\nNow flannel is running, it has created a vxlan tunnel device on the host and written a subnet config file\ncat /var/run/flannel/subnet.env\nFLANNEL_NETWORK=10.5.0.0/16\nFLANNEL_SUBNET=10.5.72.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=false. For interface selection, it ties in with the existing vagrant content a little. It should highlight the importance of choosing the right interface as this is how flannel registers itself in the datastore. The important options are\n-iface string\n        interface to use (IP or name) for inter-host communication\n-public-ip string\n        IP accessible by other nodes for inter-host communication\nThe combination of the defaults, the autodetection and these two flags ultimately result in the following being determined\n An interface (used for MTU detection and selecting the VTEP MAC in vxlan)\n An IP address for that interface - WIBNI each backend documents clearly how it uses this address...\n* A public IP that can be used for reaching this node - in host-gw it should match the interface address. Again would be nice to know how each backend uses this.\nThis documentation should guide users well enough that they understand the how, when, what and why of this configuration.. For k8s upgrade guidance I was thinking of tying in with the k8s 1.6 daemonset rolling upgrade feature. Help from a CoreOS kubernetes expert would be appreciated here!. For making changes at runtime I think we just needs statements that say\n- You can't change the datastore type (we don't have a migration path currently from etcd backed flannel to kube-subnet-mgr backed flannel)\n- You can't change the backend type (without stopping all your workloads and restarting all your flannel daemons)\n- You can change the the subnetlen/subnetmin/subnetmax with a daemon restart (but be careful if pods are already using IP addresses outside the new range as they will stop working)\n- I think you probably can't change the clusterwide network range (without downtime)\n. Even with the move to client-go the binary is still ~52MB. I think to make them smaller we'd need to use upx (and maybe some stripping). LGTM. @TracyBin Can you provide the logs from the flannel container?. Yes glide install --strip-vendor should be used. @mikedanese Any special reason for bumping kube-cross? (Just trying to understand it so I can make a minimal patch release if needed). I suspect the build errors are down to the kube-cross change\n/opt/bin/flanneld: relocation error: /lib/libpthread.so.0: symbol __libc_vfork, version GLIBC_PRIVATE not defined in file libc.so.6 with link time reference. @mikedanese The kube-cross upgrade is a bit tricky. You could rebase on top of #678 or just switch to using a different build container.. Includes #634 . @zbwright It would be better to split this into smaller PRs (if the changes don't overlap) as that makes it much easier for me to review and merge. You could even go as far as one PR per file/document.. So far I've looked at Documentation/alicloud-vpc-backend.md and Documentation/aws-vpc-backend.md and both look fine. Can you offer any more information? What output do you get and what is generating that output?. I can't repro any issue with etcdv3 so closing.\nThe trace below shows flanneld successfully connecting to etcd v3.1.0\n\u279c  flannel git:(master) \u2717 docker run --detach \\\n-p 2379:2379 \\\n--name flannel-etcd quay.io/coreos/etcd:v3.1.0 \\\netcd \\\n--advertise-client-urls \"http://192.168.10.218:2379,http://127.0.0.1:2379,http://192.168.10.218:4001,http://127.0.0.1:4001\" \\\n--listen-client-urls \"http://0.0.0.0:2379,http://0.0.0.0:4001\"\n12efece4e8334e17f8670f6a2d3f9b6b54e36699166750535d0fa0f2368a2f4b\n\u279c  flannel git:(master) \u2717 sudo ./dist/flanneld-amd64\nI0427 16:09:02.577665    6258 main.go:311] Determining IP address of default interface\nI0427 16:09:02.578270    6258 main.go:324] Using interface with name enp0s20f0u2u3 and address 192.168.10.218\nI0427 16:09:02.578280    6258 main.go:341] Defaulting external address to interface address (192.168.10.218)\nI0427 16:09:02.578352    6258 main.go:150] Installing signal handlers\nE0427 16:09:02.584375    6258 main.go:237] Couldn't fetch network config: 100: Key not found (/coreos.com). OK, squashed and LGTM so merging.. Thanks @salamaniibm . Ahhh, what a great PR - removing 5 million lines of code!\nDoes this need a particular (recent) version of glide? If so, could you add a comment stating that in the Makefile?. The source IP address should be for the container. How are you sending the packet from the container?. Talking to @ganlaksh offline and this issue is AWS only. @ganlaksh Is there a reason why you're using udp rather than vxlan backend? VXLAN should have much better performance. udp is only the default since it works for some older kernels. You should be using vxlan unless you have a good reason not to.. Hi @ganlaksh without a repro I can investigate or fix this issue. Are you still having a problem? If so let us know here and I can try to help.. Alloc doesn't do any networking. It only allocates the subnet in etcd.. The goal of the server/client experiment was to remove the need for flanneld to talk directly to etcd. With the introduction of the kube subnet manager this is no longer needed so the client/server mode has been removed.. Was this working previously? Did you do anything that could have stopped it from working? Are you able to retry with the latest flannel release?. Closing this stale support issue. Hi @insoz . The flannel daemon needs permissions to write to that directory. You can change the permissions with chmod. @ntquyen I can't think how flannel host-gw mode could have caused this change. I suspect the problem is more likely to be an OS problem so you might be better raising the issue with the OS team.. LGTM. This sounds like an etcd problem. I'm not an etcd expert, but try searching google \"All the given peers are not reachable\" and it looks like there are many other issues like this.. @rafagsiqueira Thanks for the update. @maxx what about having better support for selecting the public IP for flannel to use? e.g. selecting the interface or external IP by passing in a regex to flanneld (i.e. it could be put in the daemonset). @maxx that sounds reasonable - would you be able to submit a PR for this feature? Or if you can't write code, write the documentation for it?. Reopening until we have a separate issue to track the logging bug. LGTM. Looks good to me  - thanks @gunjan5 for the review and @t0mmyt for the rebase+docs and @pingles for the original code.\n@gunjan5 do you want to do the honors and hit merge once travis passes?. Is this just bad certificates in the flannel container (or a container linux flannel-wrapper problem where the certs aren't being mounted in properly)?. @glennmcallister Thanks for investigating this - I think it can be raised at https://github.com/coreos/coreos-overlay but I'll flag it with the CoreOS team too.. @drinktee Do you have the logs from the API server when this was happening?. It's possible (though I can't see how) that it could be related to the \"100\" here - https://github.com/coreos/flannel/blob/master/subnet/kube/kube.go#L129. Thanks @Capitrium  - I managed to repro the problem. I've added a fix in #729. See the flannel-git repo on quay if you want an image to try out.. Flannel doesn't assign IP addresses to pods. Maybe you have a problem with host-local IPAM plugin - in which case you should raise it in the containernetworking repo.. @andyxning I'm not sure I see the benefit of this PR - what's it actually checking (except maybe that the Go runtime hasn't crashed). I'm not totally against this, but before I can merge it you need to add some documentation for it and ensure the tests pass. I also think it should be an optional feature - and probably off by default.. @srinat999 I can't really tell if this is a bug or not. I've not done any testing with federations and so it's not currently supported with flannel. \nBut - flannel is not involved in the startup of pods, so from your description above I can't see how flannel is the issue. \nI can't spin up a federation and start switching from wired to wifi to try to repro this complex issue, so I'm closing this. \nIf you can come up with a simpler repro and a better reason why this is caused by flannel then I can reopen the issue.. This seems to be resolved now.. What do you mean by \n\nif I delete the node from k8s and reboot it\n\nFlannel needs to fetch the podCidr from the node before it can start.. @kfox1111 The node needs to have a podCidr. Can you check if it does - kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'. @gtirloni Did you see this note in the kubeadm docs\n\nThere are pod network implementations where the master also plays a role in allocating a set of network address space for each node. When using flannel as the pod network (described in step 3), specify --pod-network-cidr=10.244.0.0/16. This is not required for any other networks besides Flannel.. \ud83d\udc4d You'll need to raise that with kubeadm team. @gunjan5 looked into the increased startup cost (~0.5ms) and memory usage (~<0.5MB) from this change and approved the change.. Thanks for the suggestion @sjpotter it's a very interesting idea. \n\nI think this could be a really useful feature. First some terminology that I can reference further down\n \"Normal hosts\" have containers on them (that get their own IP addresses) and the host get a subnet lease to allow it to assign addresses to the containers.\n \"Leaseless hosts\" don't get a subnet lease so they can't give IP addresses to containers.\nA problem that I can see is that the leaseless hosts still need an address that's on the flannel network. e.g. if you want to have vxlan encapsulated packets to pass between the normal and leaseless hosts, the source and destination addresses need to be on the flannel network. \nI can't think of an easy solution for this problem.\nAnother problem I can see comes around when using the Kubernetes API to store data. In this case, kubernetes nodes objects get annotations for the flannel configuration for that node. This is what all the flannel daemons watch to know that they need to set up networking to a new node. This implies that flannel would require kubernetes node objects even for \"leaseless\" hosts. Maybe this limitation is OK, but I still think it's worth flagging.. When vxlan is being used, there is more going on that just pure routing. \nLet say we have two hosts, and a flannel network of 192.168.0.0/16\n Host A is a \"normal\" host that has the subnet 192.168.1.0/24 assigned to it.\n   * It has one container running on it that has an IP address of  192.168.1.1\n   * It has a single NIC with the address 10.0.0.1\n Host B is  \"leaseless\" host, with no subnet assigned to it.\n  * It has no containers running on it.\n  * It has a single NIC with the address 10.0.0.2 assigned to it.\nIf I send a packet from host B to the container on host A, the source address will be 10.0.0.2 and the destination will be 192.168.1.1.\nThis works fine as long as the network between host A and host B allows packets with the 192.168.1.1 address. Some cloud networks block this which is sometimes why vxlan is used.\nWhen vxlan is used, the packet from 10.0.0.2 to 192.168.1.1 gets encapsulated, which means that there's an additional IP header which has a source address of 10.0.0.2 and a destination of 10.0.0.1. Host A can then un-encapsulated the packet when it arrives there and pass it on to the container.\nBut we now have a problem - how does the container respond? If it tries to reply to 10.0.0.1  with a source address of 192.168.1.1 the packet isn't going to get encapsulated, because flannel will only encapsulate packets destined for the flannel network. And remember that in this scenario we need to use vxlan because we're on a cloud network that doesn't allow the 192.168.1.1 address to appear on the network between the hosts.\nSo, where does this leave us? I can see this feature working on the purely routed flannel network backends (e.g. host-gw, aws or gce) but I can't see how it can (easily) be made to work with vxlan.. flannel.1 isn't a bridge, it's a vxlan device, so the suggestion from @KDF5000 is what you need. Sorry, we don't have a guide for this at the moment.. Hi @alexey-medvedchikov thanks for this PR! Could you write some documentation outlining why this is an important feature from a user's perspective? \nIt's also really useful for me to know what testing you've done. What's the risk that this will introduce a regression for people not using this feature?. Sounds great to me! If you can add some documentation and hopefully some tests I can get it merged. @alexey-medvedchikov Could you add some documentation to the aws-vpc-backend.md file and I can merge this.. I would love to take this but it looks like the original author has gone away. If anyone wants to take it on, please rebase, add some documentation and open another PR.. I think your test is fine. vxlan can do many GBPS so you will only see the performance drop compared to no encapsulation if you have a fast enough network.. I don't fully understand your problem, so maybe you can add a little more detail. \nThe filesystem state is only a single file.\nThe network stack is left up on purpose, in order to facilitate zero-downtime restart/upgrade.\nI'd love to hear more about the flannel-operator that you're writing but your link above doesn't work.\n. The problem is that flannel can't contact the K8s API server\nFailed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-fxbfs': Get https://10.3.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-fxbfs: dial tcp 10.3.0.1:443: i/o timeout\nFlannel shouldn't be using the flannel network for contacting the API server. Do you know where that IP (10.3.0.1) has come from?\nMaybe try the CoreOS IRC channel for help with setting up your K8s cluster.. LGTM. I'm not sure what problem you're describing here. I appreciate the detailed diags, but could you describe the actual problem your seeing.. Do you know what created the bad route originally? Could you repro on a clean host and find out when that route gets created (using ip mon. Can you include the documentation change in this PR too?. @mgleung as discussed offline, this looks good to go but it's back with you to make the change in the EtcdConfig. Why are you trying to access the k8s-api using the the pod network? It's better to just use the host network for the API server.. Do you have the log output? Have you considered just using the kube-subnet-mgr?. Closing stale support issue. @yurchenkosv if you're still having problems let us know.. One more small comment, otherwise LGTM.. Flannel currently only supports the etcdv2 API (but you can still run etcd v3.x). @lghinet @wjp719 please open a new issue if you're still having problems. I tried running this code and it just exited:\ndist/flanneld --iface enp62s0u1u2 --iface wlp2s0\nI0627 14:17:05.582073   31184 main.go:432] Using interface with name enp62s0u1u2 and address 172.24.17.136\nI0627 14:17:05.582112   31184 main.go:449] Defaulting external address to interface address (172.24.17.136)\nI0627 14:17:05.582192   31184 main.go:432] Using interface with name wlp2s0 and address 172.24.17.98\nI0627 14:17:05.582201   31184 main.go:449] Defaulting external address to interface address (172.24.17.98). I think the UX still isn't great here - flannel prints logs saying that one interface is being used then logs saying a different one is being used. Kinda confusing.\nI0627 16:17:39.473610    6118 main.go:434] Using interface with name enp62s0u1u2 and address 172.24.17.136\nI0627 16:17:39.473648    6118 main.go:451] Defaulting external address to interface address (172.24.17.136)\nI0627 16:17:39.473721    6118 main.go:434] Using interface with name wlp2s0 and address 172.24.17.98\nI0627 16:17:39.473728    6118 main.go:451] Defaulting external address to interface address (172.24.17.98)\nI0627 16:17:39.473778    6118 main.go:201] Created subnet manager: &{registry:0xc4203df560 previousSubnet:{IP:0 PrefixLen:0}}\nAlso that created subnet manager log still needs cleaning up. I still think the logging could be less confusing:\n\u279c  flannel git:(mgleung-iface-multi) \u2717 dist/flanneld --iface madeup --iface wlp2s0 --iface enp0s20f0u1\nE0628 14:49:53.618224   10407 main.go:176] Failed to find interface to use: error looking up interface madeup: route ip+net: no such network interface\nE0628 14:49:53.618335   10407 main.go:176] Failed to find interface to use: failed to find IPv4 address for interface wlp2s0\nI0628 14:49:53.618404   10407 main.go:442] Using interface with name enp0s20f0u1 and address 192.168.10.134\nFailed to find interface to use sounds very final. And are those logs really errors?. Also, mixed --iface and --iface-regex seems to be broken still.\nflannel git:(mgleung-iface-multi) \u2717 dist/flanneld --iface madeup --iface wlp2s0 --iface-regex 'enp.*'\nE0628 14:52:28.362297   10637 main.go:176] Failed to find interface to use: error looking up interface madeup: route ip+net: no such network interface\nE0628 14:52:28.362421   10637 main.go:176] Failed to find interface to use: failed to find IPv4 address for interface wlp2s0. @luxas Thanks for raising this. I'm working on fixing it at the moment. I see that I did try to get this added to the README back in April - https://github.com/coreos/flannel/issues/672#issuecomment-293684413. LGTM. I've tested that -v=10 gives me more logs, and that -log_backtrace_at produces a backtrace. Also checked that the usage text looks right.. Some backends need to know the global cluster-cidr (host-gw doesn't IIRC). flannel reads this value from the file and not from the kubernetes API (again IIRC).. @gunjan5 @zbwright I'd like to get this merged ASAP. Could one of you take a look in the next few days.. @zq-david-wang thanks for raising this issue. It's something I've been thinking about for a little while and would welcome PRs for it. I'm also hoping to find some time to work on it in the next few months.\nIt would be similar to the cross-subnet ipip mode in calico - http://docs.projectcalico.org/v2.3/usage/configuration/ip-in-ip. Closing as a dupe of #519 . Hmmm, that seems strange. Maybe @euank has an idea? My only suggestion would be to try environment variables. https://github.com/coreos/flannel/blob/master/Documentation/configuration.md#environment-variables. Hi @Doron-offir, sorry you hit these issues. I suspect that you were hitting the problem where flannel wouldn't start with kube-subnet-mgr and >100 nodes. https://github.com/coreos/flannel/issues/719 This is fixed in v0.8.0 (https://github.com/coreos/flannel/releases/tag/v0.8.0) so I suggest you raise an issue with Kops to get it updated to the new flannel release.. Also #787 . @zq-david-wang thanks for the contribution but I'm closing this in favor of #787. Hopefully that still provides the feature you need.. plugin chaining is a CNI concept so the issue here is most likely with the CNI plugin. It's a little confusing but the CNI plugin is actually in a different repo - it's over at https://github.com/containernetworking/plugins. So I'm going to close this issue and raise a new one over there.. I just tried spinning up a free ARM vm from Scaleway (https://www.scaleway.com/instantcloud/) - I ran \nsudo apt-get update\nsudo apt-get install docker.io\ndocker run quay.io/coreos/flannel:v0.8.0-arm64\nAnd it appeared to run OK. I'm happy to merge a PR to fix this, but I don't any any hardware to test it on so I'm relying on you guys to provide the fix :smile:. @bitvector2 @stevesloka  @swestcott @asaaki  @ikester @cwalker67 @cdickson-sum Could you try the new v0.9.0-rc1 images?. @danmikita That's great to hear!. Fixed by #776 . @gunjan5 I don't follow. How does this code change affect the subnet entry in etcd?. Discussed offline and @gunjan5 says SGTM.. I amended the commits with the markups (since they were small). PTAL @gunjan5 . It looks like it might be worth trying to increase the buffer size of the netlink socket\nFrom https://www.netfilter.org/documentation/FAQ/netfilter-faq-4.html\n\nthese are standard Netlink sockets, and you can tune their receive buffer sizes via /proc/sys/net/core, sysctl, or use the SO_RCVBUF socket option on the file descriptor.\n\nThe file descriptor is available by calling GetFd() on nlsock here - https://github.com/coreos/flannel/blob/master/backend/vxlan/device.go#L214\n. Here's an example  https://github.com/coreos/flannel/blob/master/dist/functional-test.sh#L59. Hi @rickypai thanks for the contribution! I'd like to merge this but I think that the documentation should get an update to say that flannel now requires K8s 1.6 because of this. I added some information in #826 but more would of course be welcomed.. @spacexnice Can you help?. Closing this support issue without further diags/info. @gunjan5 I've made markups (https://github.com/coreos/flannel/pull/785/commits/ea2cb640318af2bb3636b692d7aaff5365ddf4af) to address all your comments, PTAL.. I have done some testing, but I would love for people to do more testing before this gets released.\nYou can try it by using the image quay.io/coreos/flannel-git:v0.8.0-14-g4973e02e. Thanks the feedback. I definitely won't merge this PR without further discussion. . @philips @squat Do you think this issue should stay open? Do you think this can or should be fixed in flannel? I could see a few options for resolving this\n1) Adding a note to the docs about this issues and the steps to (manually) resolve it.\n2) Always turning off tx checksum offloading\n3) Trying to detect if flannel is running on azure and turning off tx checksum offloading\n4) Just closing the issue as it's already been resolved in the tectonic installer :scream: . This is only going to work for the AMD64 images, but I assume that's all that works on AWS/ALI so I think it's OK to merge. I'm just slightly nervous about having these differences on the different platforms. Flannel needs to interact with the K8s API server. When running as a pod it can be given permission to access those details. I did add some experimental support for running outside a pod, but it's not currently documented (and may never be). If you want to try to understand it, you can look at the change - https://github.com/coreos/flannel/commit/c9970d056974bc8c8ffa782c2eb07478ae61e3d8 in particular, see that it's possible to pass in a kube-config file - https://github.com/coreos/flannel/commit/c9970d056974bc8c8ffa782c2eb07478ae61e3d8#diff-7ddfb3e035b42cd70649cc33393fe32cR106 and the need to specify the node name as an environment variable - https://github.com/coreos/flannel/commit/c9970d056974bc8c8ffa782c2eb07478ae61e3d8#diff-a279837d654920d2f60c5ac4db8c2401R96\n. @zengbiao Did you manage to resolve this? Without more information I don't think I can help.. @simepo do you know how that RPM is built or where it comes from? Are you able to check that you have the exact same 0.7.0 flannel binary as is hosted here on github?\nYour \"steps to repro\" and \"Current Behaviour\" aren't quite the same (if I'm reading it correctly). Do you only hit a problem when you add the VNI to the config?. @simepo I can't reproduce this. Can you come up with a simple script (e.g. using\ndocker run --privileged --net=host quay.io/coreos/flannel:v0.9.0\nand\ndocker run --rm --net=host quay.io/coreos/etcd etcdctl set /coreos.com/network/config '{ \"Network\": \"10.105.128.0/21\",\"SubnetLen\": 26, \"Backend\": {\"Type\": \"vxlan\"}}'\nThat demonstrates the problem?. I can tell from the interface name that you're using the UDP backend. This backend is not recommended and will not allow traffic to flow if the flannel daemon isn't started. Try using vxlan instead - https://github.com/coreos/flannel/blob/master/Documentation/backends.md. @clchandan Can you try with the latest version of flannel? And can you post the log output from flanneld?. This will be fixed in the v0.9.0 release (which has an RC available now). Should be fixed in v0.9.0. @vnalla It's probably best to try to solve this interactively. Try asking for help on the #coreos IRC channel on the #flannel channel on the Calico users slack (https://github.com/coreos/flannel#contact). The default changed with Docker v1.13 - https://docs.docker.com/engine/userguide/networking/default_network/container-communication/#container-communication-between-hosts\nIt's currently unclear to me how this issue shoudl be fixed. Maybe flannel you automatically change the iptables rules, or just document the docker change, or maybe the bridge CNI plugin should be doing something about it.\nAlso @limited - for NAT you should just pass the ip-masq option to flannel. Looks good, thanks @ruoshan. Your user needs the right permissions to TUN device. It's easiest to just run flannel as root.. Thanks for the fix @julia-stripe . it looks great to me so merging.. @CallMeFoxie I'm excited to hear that you're running a 2K node cluster. Feel free to raise issues (or DM me on slack) for things which you think flannel should be doing better. For example, are there additional or clearer logs that flannel could have written to allow you to more quickly realise that this wasn't a flannel problem?. Interesting idea, but this is not currently supported.. Hi @julia-stripe, thanks for another well put together PR! \nI see one key problem with this PR and then a couple of minor concerns (below) - the main concern is that if a single rule (of the four that flannel installs) is removed, then flannel just restores the one missing rule, which means it might end up being out of order! Since the ordering of the rules matters, I think this defeats the purpose of this PR a bit.\ne.g. if I remove the first flannel rules iptables -D postrouting -t nat -A POSTROUTING -s 10.0.0.0/8 -d 10.0.0.0/8 -j RETURN then I can see your code restores it, but it restores it last in the list which means it won't get hit.\nI'd be interested to hear if you have any thoughts on automated testing in this area? I know there aren't any existing tests but it would be great to have some.\nOtherwise, the code looks good to me, though I'd like to leave it up for at least a few days before merging to give others a chance to comment.\nThe slight areas of concern I'd have about this are\n1) The additional IPTables churn that this could create. But I'm not too concerned since this should just be 4 attempted appends (one for each rule) per second. I assume that this is a constant operation and isn't going to cause a massive increased load for people with huge sets of iptables rules.\n2) The potential for multiple processes to \"fight\" over keeping these rules updated. If there is another process which is making changes to the NAT table then potentially that could cause additional churn.\n@fasaxc I'd love to hear your thoughts on this.. @fasaxc thanks for the comments. I agree that longer term, a single jump to a flannel owned chain would be better.\n@julia-stripe That's looking much better thanks. This code doesn't actually delete the iptables rules when flannel stops, but after a bit of testing I see that the old code didn't either (ooops). I also think it's desirable that the rules aren't cleaned up (so the dataplane keeps working during an upgrade). It's up to you if you want to clean up the code or just leave it as is. I think in future it would be good to have a \"--cleanup\" command which which removes all traces of flannel from a system (removes the subnet.env file, any iptables rules, any flannel routes and flannel created devices).\nCould you squash the two commits then I can merge it?. I'll do my best to answer these questions, but happy to get input from others too\n1) I may have gotten these the wrong way round. But since they're now removed I'm not going to think about it too hard!\n2) Good spot! I think I must have misread the man page\n\nINFO_SPEC := NH OPTIONS FLAGS [ nexthop NH ] ...\n      NH := [ via [ FAMILY ] ADDRESS ] [ dev STRING ] [ weight NUMBER ]\n              NHFLAGS\nSo I don't think nexthop is needed but onlink is needed.\n\nSee here for details on getting master builds - https://github.com/coreos/flannel/blob/master/Documentation/building.md#obtaining-master-builds\nBut note - there's a v0.9.0-rc1 build available now too.\n. Awesome work, thanks @mkumatag! I'm going to merge this now as I think a few people are having problems with the images.. @devent I'm not sure what the problem is here. My guess would be that maybe both nodes ended up with a /run/flannel/subnet.env which told flannel to use the same lease.\nIt shouldn't be possible to have two flannel instances sharing the same lease so there could be a possible bug here. But I'm unclear on the exact sequence of events (how were you running flannel without etcd for example?). I don't think there's anything here that I can fix without a clearer repro, so I'm closing this until we can get some more clarity.. Thanks @devent for the extra info. I'll need to digest this some more but reopening the issue to keep it tracked.. @szuecs Are you able to try with the latest flannel release? The vxlan has changed quite a bit since 0.7.0. I'm assuming you just need to upgrade. Please comment with more diags if you're still having problems.. Closing as won't fix..... This is now fixed. See also #739 . I'm leaving this open to track the iptables -P FORWARD ACCEPT part. Should be fixed in the soon to be released v0.9.0 by #808 . ...but this isn't going to be easy. The udp backend needs CGO which means a full cross-compiling C environment is needed. kube-cross provides this but it produces binaries linked against glibc which won't run on alpine unless glibc packages are installed. And glibc for alpine is only (easily) available for amd64.\nI wasn't able to get kube-cross to link against musl libc and I wasn't able to make an alpine/musl based cross compiling build container. \nSo I'm giving up on this for now.... Fixed by #860 . @squeed Excellent suggestion but I don't want to hold up the release for it. \nI'm going to merge this (having reviewed it with a local colleague) and release v0.9.0, then I can look into bundling portmap and do a v0.9.1 release. Thanks for the feature request @trunet. Just so I understand the impact this has on you, are you seeing any problems with doing a full flannel restart. The dataplane should not be affected by restarting the flannel daemon. The only problem I can think of is that new hosts that are added while the flannel daemon isn't running won't be spotted until the daemon has started up again.. @trunet Thanks for the reply and clarification!. @mkumatag Looks fine in principle but it will need rebasing on top of the test refactoring that just got merged to master.. @Dao007forever could you expand on \n\nIf flanneld.service failed, the node should not be Ready.\n\nWhat component is doing the Ready reporting?. I think this should be reported at https://issues.coreos.com as the flanneld.service isn't hosted in this repo. @rakelkar If it's possible to submit smaller PRs then that's helpful for me. e.g. moving the ipmasq code out of main might be a generally applicable change which doesn't depend on the Windows changes. . Closing this PR as it's now being done in stages.. Hi @piontec your debugging seems pretty thorough but it still comes as a surprise. I've tried to reproduce this on Coreos container linux but I was unable to. I used both the UDP and vxlan backends.\nI'm not really sure what to suggest. Maybe you can run tcpdump at a few different places to try to isolate where the mangling is happening (e.g. the bridge, the vxlan device and the physical ethernet device).. @piontec I'm closing this as I can't repro. I can reopen if you can provide more info.. This is tracking the fact that iptables rules aren't deleted. Although originally rules were being deleted I no longer think this is desired behaviour. We want to leave the dataplane untouched when flanneld stops to facilitate upgrades. We should provide a cleanup command though. The logs and code should be cleaned up to reflect this as they current log that they are trying to delete the rules.. THe tests no longer display the logs. @salamaniibm This needs a rebase then I can merge. Logs are no longer displayed on failure. @alvaroaleman Thanks for this change. If you can add some documentation and ideally a test, I can get it merged. For the documentation, I would suggest adding an \"Annotations\" section to https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md\n. I added some tests for kube subnet manager recently - https://github.com/coreos/flannel/blob/master/dist/functional-test-k8s.sh\nBut I agree, they are still quite complicated!. OK, this looks good. It would be great if you could add a little more information to the documentation, such as why a user might want to set this, and any other annotations that can be set. \nBefore I merge can you squash your commits so I'm just merging a single commit?. Yes, I think you're right! The tests are now failing, I think you lost a commit when doing the squash? Hopefully you can get it back easily then I can merge.. @sgarg1 This is strange indeed. Could you share some of the output from flannel when it starts? You should see something like\nI1020 12:01:57.728043    6015 main.go:470] Determining IP address of default interface\nI1020 12:01:57.728273    6015 main.go:483] Using interface with name wlp58s0 and address 192.168.43.73\nI1020 12:01:57.728283    6015 main.go:500] Defaulting external address to interface address (192.168.43.73)\nThis will confirm that it's actually choosing eth0 as the external address.\nYou can also run ip -d link show dev flannel.1 and check that you get the right device listed on the last line\n11: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/ether 0a:96:29:53:fa:33 brd ff:ff:ff:ff:ff:ff promiscuity 0 \n    vxlan id 1 local 10.11.12.13 dev dummy0 srcport 0 0 dstport 8472 nolearning ageing 300 udpcsum addrgenmode none. Also, if I create a dummy device\nip link add type dummy && ip link set dev dummy0 mtu 5000 && ip link set dev dummy0 up && ip addr add 10.11.12.13/32 dev dummy0\nAnd start flannel\nsudo dist/flanneld --iface dummy0\nThen I see the correct MTU\ncat /run/flannel/subnet.env\nFLANNEL_NETWORK=10.105.128.0/21\nFLANNEL_SUBNET=10.105.133.65/26\nFLANNEL_MTU=4950\nFLANNEL_IPMASQ=false\nI'm going to close since I can't repro but please provide more info and I'll re-open if we can repro it.. @mikkeloscar Yes I was using vxlan. I 'm pretty sure that the correct MTU is automatically set by linux (to the \"host\" interface -20). Does this resolve to an IP address on an interface that has the 9000 MTU? . I'm able to repro this on AWS now:\ncore@ip-172-31-12-123 ~ $ ip -d link show flannel.1\n6: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/ether 8e:23:05:4d:c8:af brd ff:ff:ff:ff:ff:ff promiscuity 0 \n    vxlan id 1 local 172.31.12.123 dev eth0 srcport 0 0 dstport 8472 nolearning ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535 \ncore@ip-172-31-12-123 ~ $ ip -d link show eth0\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP mode DEFAULT group default qlen 1000\n    link/ether 0a:1b:aa:f3:64:00 brd ff:ff:ff:ff:ff:ff promiscuity 0 addrgenmode eui64 numtxqueues 2 numrxqueues 2 gso_max_size 65536 gso_max_segs 65535 \ncore@ip-172-31-12-123 ~ $. I added a few more comments (including this one which might be hidden https://github.com/coreos/flannel/pull/842#discussion_r147539848). Hi @chenchun I want to make sure I understand the design for this fully before merging it. If I understand your comment (https://github.com/coreos/flannel/pull/842#discussion_r147542937) and the code correctly then you're not setting a \"remote\" but you are setting the \"local\" address to the public IP. Can you confirm what effects this has? Which address will be used as the source IP address for packets that originate from the host network namespace? And what if the public IP isn't the same as an IP address on the host, then you can't actually use it?\nI also have some questions about the MTU settings. On the vxlan backend, the vxlan tunnel device is associated with a physical ethernet device on the host. This is what is used to determine the MTU value to use. Flanneld will always have an interface that it chooses (specified with the --iface option), so maybe you should be getting the MTU value from that device? WDYT?\nSo maybe the best option would be to \"local\" address to the address of the interface that flannel selects and also use that as the for the MTU?. When I run your code I'm seeing two IPIP devices, is that expected?\n6: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ipip 0.0.0.0 brd 0.0.0.0\n7: flannel.ipip@NONE: <NOARP,UP,LOWER_UP> mtu 1480 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/ipip 192.168.5.188 brd 0.0.0.0. Also, one final request: Would you mind adding some comments to your code explain why it's written like it is. For some examples of the style, see the vxlan code - e.g. https://github.com/coreos/flannel/blob/master/backend/vxlan/device.go#L108. Hi @chenchun I've looked at this code some more and realized how similar so much of is it to existing flannel code. It would be great to refactor and pull out the common code.\nA few other things that need to happen before this can be released (but not necessarily before this PR is merged)\n Documentation update (maybe in the troubleshooting guide too) to descrieb that the tunl0 device will be created but that's not a bug (!) - users will get confused about this.\n a ..._windows.go stub needs to be created.\n* Some manual functional and scale testing.. @chenchun and @ChenLingPeng I rebased and squashed your commits and added another commit to the PR just to make some small markups. Your original commits are at https://github.com/tomdee/flannel/tree/iptun-orig if you still want them.. thanks @chenchun this looks like a good fix. Before I merge it, can you share a little bit of information on how you hit this and how you've tested the fix. Ideally there would be an automated test case for this too, would that be possible to add?. Looks good thanks. You'll need to update the Makefile too though, the  TEST_PACKAGES variable needs to include backend/hostgw. This does mean that Docker is needed for running the unit tests, but I think that's OK now since the end to end tests need Docker too.. Yes, that line is the smoking gun. What other nodes do you have? Can you output the flannel annotation you have on your nodes (something like kubectl get nodes -o yaml  |grep flannel.alpha).\nSomehow, I think one of your nodes has a PublicIP of 172.16.0.0 which it shouldn't do. The 172.16/16 range should be reserved for the vxlan network.. @camflan please open a different issue. I suspect you just need \"iptables -P FORWARD ACCEPT\"\n. @jhorwit2 @senwangrockets I think the problem could be that you have the same IP range configured for your Docker bridge as you do for flannel. If you're using kubeadm, did you specify --pod-network-cidr 10.244.0.0/16. If you submit a PR I can merge it?. When using flannel with the kubernetes subnet manager (--kube-subnet-mgr), flannel will just use the podCIDR that's assigned to the node. See https://github.com/coreos/flannel/blob/master/Documentation/troubleshooting.md#kubernetes-specific. Sounds sensible - PRs are of course welcome :smile: . LGTM. Looks good, thanks. @phagunbaya This repo doesn't install the CNI plugin. You'll need to raise an issue with your distro that installs the CNI plugin. The docs are hard to find, sorry - https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel\nDo they answer your question?. Please let me know if you need more info. Closing for now. The tests are failing. I restarted it as it looks like a google connectivity problem.. Since there is no actual Windows specific code in here, why not use this PR to just move iptables stuff out of main.go but not do any file renaming?. Looks good. If you squash the commits I can merge.. Looks good, merging.. Should the limit be higher? Are these values suitable even for large clusters?. I think this would be a great PR to merge but with the current values I'm worried that it will create (additional) hard to diagnose problems since the limits might be too low.. OK, my testing has shown that a 1000 node vxlan cluster (unfortunately with etcd backend) uses ~22MB. Using flannel with the k8s subnet manager adds about 10MB to the memory usage so I think a limit of 100m CPU and 50Mi memory would be OK. If you make this change to all the yamls in this repo I can merge it.. Great, thanks for this @cglewis . This behavior no longer occurs with flannel release 0.8.0 and later - https://github.com/coreos/flannel/pull/745. You need to set your GOPATH and put the project in the correct location in your GOPATH.. @iramsey85 I can't help with the scaling of DNS replicas, try the K8s slack. For the flannel version, yes, you can just update the version in the daemonset and restart the pods.. @cheyang Yes, this is a change is was one of the main features of v0.9.0, so it is by design and not a bug that you see those routes. What is your concern about the routing performance?. If I'm understanding it correctly, someone executed a command that broke networking on the server which was outside the control of flannel. You're asking that flannel monitors the state that it creates to see if a third party process breaks that state. That's an enhancement.. My intention was to drop that flag and to have flannel always write the iptables rules. In this PR, flannel is only changing rules for IP addresses that it owns, whereas in the other PR it was changing the default forward policy, which is global and therefore I figured should be optional.. I plan to merge this shortly as I'm not hearing any objections. Thanks @squeed for the info on CoreOS, that solves the mystery of why nobody was hitting this problem on CoreOS!. This is now fixed. Looks good, thanks.. The TTL can't currently be changed the only solution I can think of would be to use a bigger subnet. @mkumatag As always, thanks for spotting this stuff and getting it cleaned up. . @gnodli Do you have logs from the master?. @gnodli I'm not sure what the problem is but I would guess that NetworkManager is interfering. e.g.\nNov 21 08:42:12 localhost NetworkManager[622]: <info>  [1511224932.7115] device (flannel.1): state change: unmanaged -> unavailable (reason 'managed') [10 20 2]\nNov 21 08:42:12 localhost NetworkManager[622]: <info>  [1511224932.7333] device (flannel.1): state change: unavailable -> unmanaged (reason 'connection-assumed') [20 10 41] \nI'd suggest trying other support channels to try to diagnose this further.https://github.com/coreos/flannel#contact. Looking good, but this will need a few changes before I can merge:\n Rebase - network/ipmasq files have been renamed in master\n Make sure you have newlines after the build directive lines (use go vet to spot places where this is wrong).\n Can you put the build tags under the copywright statements.\nI'll take a look at enabling appveyor on this repo\n. What are the changes to the vendor/* files?. Looks good, merging.. This is now done. @d11wtq Interesting, I've not seen this before but certainly let me know if you see this again.. Could you make this change to the other two manifest files in this repo too?. @Gacko Looks good, but I'm not sure about bumping the aliyun image - it might be safer to just leave that at 0.9.0\n. cc @xh3b4sd WDYT?. Looks good. It's probably worth seeing if someone can help you on slack https://github.com/coreos/flannel#contact. @osoriano Thanks for the summary.\nIIUC this is only a problem for people using the kube-flannel.yml from master. It would be great to find a way to stop people from doing that!\nAnd it would also be great if flannel had a better way of ensuring that the CNI plugins it needs are installed on the host, maybe https://github.com/coreos/flannel-cni could be updated to install the portmap plugin. Which file can't be checked out on windows? Also, there's a CI failure to fix.. I've enabled AppVeyor :+1: But it's failing :-1: https://ci.appveyor.com/project/tomdee/flannel. @jroggeman See the Makefile for how glide should be run - https://github.com/coreos/flannel/blob/master/Makefile#L191\nupdate-glide:\n    # go get -d -u github.com/Masterminds/glide\n    glide update --strip-vendor\n    # go get -d -u github.com/sgotti/glide-vc\n    glide vc --only-code --no-tests\nIn theory, running glide install -v && glide vc --only-code --no-tests without changing glide.lock should result in no changes to the vendor directory.\nSince you're adding a new package and updating glide.yaml that will result in a change to glide.lock (since you'll need to run glide update). This means that some other packages will be updated since they are not all pinned in glide.yaml.\nIt looks like CI is still failing on Appveyor because of this vendoring issue.\nOn the docker/distribution/contrib... issue, maybe you can update the version of k8s.io/client-go that's being used and that will stop relying on those files...\n. @jroggeman @rakelkar Appveyor is now passing. Yes, you should be able to enable DirectRouting and experience minimal downtime. I'm not really sure if existing TCP connections would be OK, I'd need to think about it some more, and you'll need to restart the flannel deamons but they can be done individually. \nPlease let us know how you get one and it would be great to get the docs updated with some additional info about this.. Can you give me some ip -d route output showing the bad routing rules when using vxlan?. @eranreshef This wireguard config isn't in a release yet, so unless you really know what you're doing I wouldn't recommend it! If you need more help then maybe swing by the #flannel channel on the calico users slack (see contact info in the readme). The config needs to look like this https://github.com/coreos/flannel/blob/master/dist/extension-wireguard\nHope that helps!. @garyyang85 Thanks for reporting this issue but I can't think what the problem could be. You are running quite an old version, if you can reproduce the problem on the latest version then maybe I can look at it.. The assumption is that flannel running with --kube-subnet-mgr is running under a kubelet. The kubernetes master components don't need to be running under the kubelet.. LGTM. s someone removing data from etcd? What about the server's clock, is it being changed?. Run as root?. This replaces #891 . ping @aoxn do you have permissions to update the aliyun image when I do the release?. Just now :smile: \n. This already got fixed in master and should make it into the next release soon.\nhttps://github.com/coreos/flannel/pull/855/files\nI'm going to close this issue but if you don't think #855 fixes it then please let me know.. @rakelkar You can just update the lock file manually. I also don't mind if you end up updating other vendored libraries. If I think it's a problem to update other libraries then 'll introduce more pins in glide.yaml.. @rakelkar What is the status of this PR? Is it ready for review and merging?. @rakelkar What is the status of this PR? Is it ready for review and merging?. Stale PR, closing.. Sorry, it's not currently possible to run multiple copies of flannel on the same host with the kube subnet manager. @Dema That sounds like a great idea. Do you know what K8s version is was introduced in?\nPRs are of course always welcome :smile: . Sounds good to me. Looks good, thanks @DiamondYuan . You just need the NODE_NAME variable to be set - see https://github.com/coreos/flannel/blob/master/dist/functional-test-k8s.sh#L81\nand the comment here - https://github.com/coreos/flannel/blob/master/subnet/kube/kube.go#L94\nHope that helps/. Thanks for the great issue report. A command line arg would be the right way to configure this. Allowing very large values would also provide a mechanism for disabling it. It would be great to add a node to troubleshooting.md about this too. I think it's worth getting this fix into the next release but longer term we'll probably want to do something better.. Going to leave this open for now to help track a longer term solution.. Looks great, thanks @SleepyBrett . Looks good, thanks.. Thanks @celskeggs . @merwan Sorry, I just merged a more recent fix for this and missed yours!. Looks great, thanks for adding this.. Let's go forward rather than back - replaced by #1016 . Sorry, I didn't spot this when I merged #1000 . Excellent, thanks for the fix.. @kopiczko Thanks for making the markups. The changes look good. Merging.. Thanks @discordianfish for the PR and @maxlaverse and @szuecs for the additional diags. Now that iptables v1.6.2 is in the latest Alpine release I'm going to take the fix from #1040, so closing this one.. LGTM. LGTM. Change looks good. Confirmed that v1.6.2 of iptables is in the latest alpine release so taking this PR.. Thanks for the fix @ncabatoff - merging.. IIUC .conflist is a supported extension. Can you explain the problem a bit more?. Or ensure the key exists in the etcd v2 API.. Looks great, thanks for the contribution!. @rajatchopra and I are working on a release at the moment so expect something soon.. Alphabetical by second letter of the first name \ud83d\ude04 \n. That sounds cool, but I don't think it's a requirement for getting this PR merged. I'm happy to change golang:1.6-alpine to something else if you think that would make future cross compilation easier. However, it looks like alpine is probably fine - e.g. see https://github.com/tianon/rawdns/blob/master/Dockerfile.cross\n. Not that I can see - look here for more info https://groups.google.com/forum/#!topic/golang-nuts/pdrN4zleUio\n. The tests always run with coverage. The cover target is just generating the html report, but it can only do that for a single package. Hence they are kept separate. \n. Yes, that makes sense, I'll do that.\n. Why do you think that?\n. Done\n. I think /dev/stderr is a bashism and >&2 is more POSIX compliant. See https://lists.debian.org/debian-user/2009/07/msg00014.html for some discussion on the subject.\n. The artifacts directory is being removed\n. Because it contains the mk-docker-opts.sh script\n. As above, because of the mk-docker-opts.sh script\n. The reasoning behind just having the amd64 one is that I don't think the .tgz is that useful. I was planning that we would attach the static flanneld for all platforms to the github releases (which is much more useful) and also the ACI. I was keeping the amd64 tar.gz for legacy reasons. If we add all the platforms then that's something that we'll need to support forever... \n. This is the container that flanneld is built in, so seems like a good choice to me.\n. I'd be interested in an update on this too. One of the less nice bits of this PR is that fact that it adds >7,000 files to the repo and increases the size of the flanneld binary from ~20MB to ~60MB.\n. I can try to find some time to help with that. But it's not clear to me how that helps. It's always a bit tricky having config in a repo that builds to versioned builds of that repo...\n. Should this be a tagged version? Is there any significance behind this particular version?\n. Is it better to us a fixed version here or to just take :latest? Tracking latest might give us an early heads-up if there's a problem using flannel with the latest etcd release?\n. Can't you just run this with --rm?\n. then you can just do \nwhile ! docker run...;do\n   sleep 1\ndone\n. A little magical. Why 5? Can you add a comment explaining where 5 comes from (even if it's just arbitrary)\n. What's with the quoting?\n. If flannel doesn't come up, then exit is called which will leave a dangling etcd container. What about naming the etcd container when you create it, and always trying to delete it (ignore the rc) before trying to create it.\n. I've made some changes to the naming of images - it's always flannel:$(TAG)-$(ARCH) now\n. What's this change for?\n. What's this change for?\n. I think this option name is too generic, including kube would help, e.g. kube-api-master\n. This comment isn't right any more. The type field is wrong here. ...must be provided. Can you change this to quay.io/coreos/flannel:v0.7.0. replaces -> replace. Change to quay.io/coreos/flannel:v0.7.0. OK, could you at least update it to the 0.7.0 release (without the -git suffix). I haven't done the part to choose between v2 and v3 yet, so this branch always uses v3. Can you think of a good reason not to just switch to this version of KUBE_CROSS in all cases?. You did :smile: I now need to make a decision about whether we want consistency across our platforms (at a cost of a bit of reduced stability) or whether to take this change as is but to have the extra complexity in the builds.\nI don't think it needs to be decided right now, but should be before the next flannel release.. Yes, it needs fleshing out. If you need me to provide the content I'll need a few days. Maybe it should say \"should not\" instead of \"cannot\". People can change it and if they really know what they're doing  it might be OK, but we don't support or recommend it.. This recommendation is really important to get right so apologies if this seems nit-picky! \n In the code, UDP is still the default (for historical and maybe compatibility reasons).\n host-gw should mention the infrastructure restriction too. \nSo, maybe something like this\n\nVXLAN is the recommended choice. host-gw is recommended for more experienced users who want the performance improvement and who's infrastructure support it (typically it can't be used in cloud environments). UDP is suggested for debugging only or for very old kernels that don't support vxlan.\n\n. I would probably not include this machinezone link as it doesn't give a very clear view of current vxlan performance. This one should be moved under experimental too. I'm not sure I understand this comment? We should have info on setting kubernetes configuration, but etcd3 support isn't something we have at the moment,. Would it be worth going through the docs and making sure that backends are consistently backticked. Awesome, thanks for adding this.. the the. Ooops, this shouldn't be in the title. Flannel uses either etcd or the kubernetes API to store the network configuration,...... Not sure if this needs to be changed here, but for accuracy, when running with the kube-subnet-mgr (yes it needs a better name), flanneld doesn't actually allocate the subnet lease. This shouldn't be here \ud83d\ude04 . As discussed offline, this could just be removed for now and put in a followup PR. \ud83d\udc4d So I think this comment is resolved right?. I think it would be better to do this under a new PR, but thanks for spotting it.. I don't think there's an impact because nothing should be looking at this expiration.. Good point, I'll add one.. typo: should be \"a health check\". Why panic here? Is it standard practice to use panic with http.ListenAndServe?. I'm not sure about this wording - if etcd is unavailable then flannel can't start, and why would etcd lose the record of its leases?. retrieve an existing lease from etcd,. This is a bit ambiguous so will be confusing for uses. Can you make it clearer and more explicit?. If you're adding a new library for reading the file, why not use the same library for writing the file?. What was the reasoning for choosing this mechanism for passing the previousSubnet through to the tryAcquireLease function?. What's the reasoning behind adding this?. This comment doesn't match the code. This doesn't mention that it's only used if --face isn't specified. It's also worth saying that it's overridden by iface here too. Ooops, actually that line isn't needed.. I changed all to lowercase. I don't think it's needed since this is a dev/test focused feature, it's more important to keep the code clean and small here.. I chose to do it the way I did to maintain existing behavior. I saw the code you mention but that would change the log messages and it would fall back to using a default config if incluster fails, which isn't what we want.. Added a comment. typo. I've added some extra words to hopefully make it clearer. I've talked about that in the troubleshooting doc so I'm going to leave this as is.. Returning would just mean that we don't try to process the rest of the events in the batch. I think it's still worth trying to process those events. What's your reasoning for why we shouldn't try?. no reason \ud83d\ude04 . gotcha, I thing maybe you're suggesting a \"continue\" rather than \"return\"? And advocating for cleaning up the failed FDB or arp entries? It sounds like a reasonable suggestion to me (though I don't think it really brings much benefit to the user and increases the code complexity).. I'll update them all to lowercase. Fixed\n. OK, I've addressed this now.. Isn't that what I'm doing with my .Gw == nil test? \nI could also check to see if the FLAG_ONLINK flag is set on the flags attribute if you think that would be better?. The directRoutingOK check is seeing if there's a direct route to the public IP address of other host. If there is, then we add a route to the IP range for that host using the public IP address as the gateway. The \"route\" to the public IP address for that host is unchanged; it's still directly connected. \nSo when the remove comes in, the public IP is still directly connected and we can remove the route to the network which uses the public IP as the gateway. Make sense?. Yes, I think that continuing to retry or returning an error in such a way that flanneld exits would be better.. good idea. the sleep is actually there to make sure the subnet.env file is written. I'll update the PR to get rid of the sleeps. This will need to be added to the glide.* files. this will also need to be added to glide\n. Do you need this file since vxlan isn't currently supported on Windows?. Why have a windows version of this file?. Shouldn't this depend on dist/$(REGISTRY):$(TAG)-$(ARCH).docker. I'm not sure I understand why you've added this? What was wrong with the dist/flanneld-$(ARCH) target?. Aha, that all makes sense. . Are you sure you want this check?. Vxlan uses flannel.<VNI> for the tunnel device and UDP uses flannel0 I think it would be good if IPIP also used flannel in the tunnel device name. Maybe just flannel.ipip. Why do you have a maximum MTU?. Is this still needed. Are these functions still needed?. Is this still needed?. What about just naming the file _linux.go?. I don't understand your comment about setting the IP address for flanneld.ipip to the public. Could you explain some more?. Can you write a little more here to help guide users. Why is this this value? Why and when would a user want to change it? And what value would they change it to?. bad indent. Sure, but it hasn't been updated in flannel yet and that change shouldn't be done in this PR. I don't want to introduce too many changes in this PR. I'll rev the Go version in a different PR. Already done in another PR :smile:. Can you update the copyright lines to 2017 please.. Can you remove this extra build statement. Shouldn't the !windows be on a new line?. Do the _windows files need a windows build tag?. Does this image actually exist? I don't have permissions to push to this repo.... I don't think that just returning an error here is good enough. If there's a flannel.ipip that isn't an ipip device then it wasn't created by flannel, so in that case it's best to just return an error. But in this case, the local address can change between flannel restarts, so flannel should be able to recreate the device and not just return an error. You should follow similar logic to vxlan (see the ensureLink function), it should delete and recreate the device if the config changes. Does this need to explicitly set the link to down first?. I think this next section of code is identical to the vxlan code, could you extract the code into a common package?. Much of this file is the same as the host-gw code. It would be great if the common code could be pulled out (and then it could be shared by the vxlan code too but I would do that in a followup PR).. Good point, I'll update the port. . @gunjan5 I updated the comments and code on these lines. PTAL.. Any reason for removing this log line?. Why remove net.JoinHostPort?. no blank line here. Wrong authors (and year) :smile: . Should be 2018 :smile: . 2018. Can you add a brief comment explaining this regex and why it was chosen (e.g. it matches the format used by the kubernetes for annotations). What's the reasoning behind supporting both with and without a slash?. ",
    "rosenhouse": "FWIW, the system design that we've converged on for Cloud Foundry is that hosts are preferentially assigned their prior lease, even if it \"expired.\"  And if a new host appears, it is assigned a lease in the following priority order:\n- prefer subnets that have never been given out before, or subnets which were explicitly relinquished by a cleanly-terminating host.\n- if none of those exist, only then does the new host take over an expired lease, and in that case it chooses the oldest such lease.\nThis is meant to minimize the probability that a lease is \"stolen\" from a live, but partitioned, container host.  But if that does occur, once the partition heals and the \"victim\" host re-connects, it will discover that its lease is no longer valid.  In this case, the victim host falls into a special, noisy failure mode which will (1) prevent any new workloads from being scheduled and (2) trigger the orchestration system to evacuate any existing workloads.  Once the evacuation is complete, the host will clean up any leftover networking state (e.g. remove the VXLAN device), acquire a new lease for itself and begin accepting new workloads.\nWe think this is the right plan.   Feedback welcome.\n. Hey there @tomdee and @steveeJ, mind taking a look?  We're adding this feature to cloudfoundry and would love to get the changes merged upstream.  Thanks!\n. @steveeJ Fixed up commit messages and rebased on master.\n. @xiaoping378 I think you're mis-reading this code -- the Network.PrefixLen is for the whole network -- in your case /12.  If you've configured things correctly, you shouldn't even get to that conditional.  Instead you should end up here.  \nAre you sure you've installed that Network config in your etcd cluster under coreos.com/network/config?\nFor what it's worth, we've tested Flannel VXLAN with a /22 per host on Cloud Foundry and found it to work fine (see here).. @xiaoping378 :\nbash\ncurl -s --cacert ca.crt --cert client.crt --key client.key \\\n   https://etcd.service.cf.internal:4001/v2/keys/coreos.com/network/config | jq\nshows\njson\n{\n   \"action\" : \"get\",\n   \"node\" : {\n      \"key\" : \"/coreos.com/network/config\",\n      \"modifiedIndex\" : 336245,\n      \"createdIndex\" : 336245,\n      \"value\" : \"{\\n  \\\"Network\\\": \\\"10.240.0.0/12\\\",\\n  \\\"SubnetLen\\\": 22,\\n  \\\"Backend\\\": {\\n    \\\"Type\\\": \\\"vxlan\\\",\\n    \\\"Port\\\": 8472,\\n    \\\"VNI\\\": 1,\\n    \\\"GBP\\\": true\\n  }\\n}\"\n   }\n}. For reference, the flanneld stderr logs show\nI0222 03:44:52.173843    8218 main.go:133] Installing signal handlers\nI0222 03:44:52.173915    8218 manager.go:136] Determining IP address of default interface\nI0222 03:44:52.174103    8218 manager.go:149] Using interface with name eth0 and address 10.0.16.20\nI0222 03:44:52.174133    8218 manager.go:166] Defaulting external address to interface address (10.0.16.20)\nI0222 03:44:52.318989    8218 local_manager.go:134] Found lease (10.241.124.0/22) for current IP (10.0.16.20), reusing\nI0222 03:44:52.326175    8218 manager.go:250] Lease acquired: 10.241.124.0/22\nI0222 03:44:52.337205    8218 network.go:58] Watching for L3 misses\nI0222 03:44:52.337237    8218 network.go:66] Watching for new subnet leases\nAnd the subnet.env file (defaults to /run/flannel/subnet.env) shows:\nFLANNEL_NETWORK=10.240.0.0/12\nFLANNEL_SUBNET=10.241.124.1/22\nFLANNEL_MTU=8951\nFLANNEL_IPMASQ=false. Ping @lxpollitt.  We're probably going to implement this in some form on a fork in order to support integration with Cloud Foundry.  Any input you have would be appreciated.. @andyxning Our team has recently decided to stop using flannel and to build something different.  So we won't be working on a flannel healthcheck. \nFeel free to open a new PR with this code or something similar.  We just won't be shepherding it. . ",
    "logicalparadox": ":+1: \nWould like to download/initialize rudder via cloud-config with an official build (even if it is in the 0.0.x range).\n. ",
    "superstructor": ":+1: \nAlready having to do this on a non-official repo so my cloud configs can work: https://github.com/joukou/joukou-docker-flannel-build/releases\n. @eyakubovich a docker image is not sufficient as when starting flannel via a cloud-config on CoreOS you need to start flannel before docker in order to configure the bridge ip correctly.\n. For example:\n``` yaml\n  - name: docker.service\n    command: start\n    enable: true\n    content: |\n      [Unit]\n      Description=Docker Application Container Engine\n      Documentation=http://docs.docker.io\n      Requires=docker.socket\n      Requires=docker-env.service\n      After=docker-env.service\n  [Service]\n  Environment=\"TMPDIR=/var/tmp/\"\n  EnvironmentFile=/run/flannel/subnet.env\n  EnvironmentFile=/run/docker.env\n  ExecStartPre=/bin/mount --make-rprivate /\n  LimitNOFILE=1048576\n  LimitNPROC=1048576\n  # Run docker but don't have docker automatically restart\n  # containers. This is a job for systemd and unit files.\n  ExecStart=/usr/bin/docker --daemon --bip=${FLANNEL_SUBNET} --dns=${DOCKER_BRIDGE_IPV4} --mtu=${FLANNEL_MTU} --storage-driver=btrfs --host=fd://\n\n  [Install]\n  WantedBy=multi-user.target\n\n```\nSo for FLANNEL_SUBNET to be available flannel must be installed and run before Docker.\n. As a temporary workaround in my cloud configs I do ExecStartPre=/usr/sbin/iptables -N FLANNEL before ExecStart=/opt/bin/flanneld is run.\n. ",
    "RobCherry": ":+1:\nI think this would be very useful.\n. ",
    "sbhaskaran": "we are running CoreOS behind firewall and have no internet access. Will it be possible to get flannel binary available ?\n. ",
    "denisura": "Link to v0.3.0 release: https://github.com/coreos/flannel/releases/download/v0.3.0/flannel-0.3.0-linux-amd64.tar.gz\n. ",
    "josselin-c": "You are right. I made this pull request so we could assess the problem, it is not \"commitable\" as is. I have a cleaner version thanks to your comments.\n. Indeed, looking at my etcd (0.4.6) logs I get lines like warning: heartbeat time out peer=\"b8\" missed=1 backoff=\"2s\". According to coreos/etcd#975 it might be because I have a lot of keys stored.\nIt appears k8s is creating a lot of 'events' entries (I have 12k) when some pods are constantly failing. Theses events take time to snapshot. I'll try to update to etcd 2.0 see if things improve.\n@eyakubovich Is it normal I get Subnet removed: 10.244.XX.0/24 when the watch fails?\n. Okay well, for now I identified and fixed the k8s problem which created a lot of events. Thanks for your help!\n. ",
    "kelseyhightower": "We should also add an example of how to set the non-default backend.\n. LGTM\n. @superstructor Again thanks for reporting this issue. In order to resolve this we need a little help reproducing the issue, can you provide the error from the logs? On that note, are you able to reproduce  this issue?\n. @mrunalp Ping. If you have a moment can you respond the the following question from @eyakubovich \nIs there a particular reason to favor OVS?\n. @gregory90 Again, thanks for asking this question. The patch that @eyakubovich submitted to go-etcd has been merged which should reduce these types of log entries in the future. Also it's safe to ignore these as flannel does the right things continues watching for changes.\n. @visualphoenix @vishvananda What version of golang are you using to build from source? We use golang 1.3 from upstream. I know RHEL patches golang in there distribution. \n. The fix for this has been merged into master at ddb1f5a0bb69e92e323e25a2478b18c8cfbad132\n. I'm +1 on host-gw. I think we should roll with that name for now and mark this backend as a tech preview until it matures a bit.\n. Thanks @mindscratch!\n. @smakam flannel should work on any Linux platform running a fairly recent kernel (3.8.x)\n. ",
    "mrunalp": "@kelseyhightower @eyakubovich One of the biggest advantages of using OVS is OpenFlow support. OpenFlow makes it very easy to program the network. It is possible to create complete isolation between tenants using vxlan with OpenFlow rules in OVS. Also, OVS has been around for a time and has support for lots of protocols and configuration making it easy to tailor it to any specific use case.\n. @eyakubovich Yes, we would have to run some kind of controller with OVS. I agree that there are problems that need to be solved with regards to SPF and storage of all configuration. One approach would be to use etcd for storing the configuration and have agents running locally on nodes watching it and applying the flows for each node. The most important feature in OVS for us is multi-tenancy support out of the box. \nAlso, it is possible to program flows that handle responses to ARP queries and that avoids having to broadcast the ARP query. \n. ",
    "smarterclayton": "The challenge is mostly that we're looking at Flow support for tens of thousands of tenants, and it sounds like the design goals of Flannel are focused on a much smaller number of tenants.  Do you guys have an upper limit in mind around number of tenants?  The larger the number becomes the less desirable flannel per tenant becomes.\n----- Original Message -----\n\n@mrunalp We're currently working on VXLAN support for flannel. The data path\nis still through the kernel but the control plane does not use OVS. The\nfirst iteration will be like what we have in flannel today -- purely at L3\nbut you could still do a limited multi-tenancy as described in #50.\nHowever, since we're getting requests for better multi-tenancy (L2 so you can\nhave overlapping IPs), we have plans to look at that next. It would still\nnot support broadcast so probably only IP in practice. That iteration would\nallow you to run an instance of flannel for each tenant and point them to\ndifferent paths in etcd for their configuration (including VNI to\ndistinguish the traffic). You would also need to instruct flannel not to\nsetup the vxlan interface with an IP and just plug it into some bridge. That\nversion would probably follow the first iteration pretty quick (there's not\nthat much to do).\nLater on, we have plans to add an API so that would allow you to feed\narbitrary IP/MAC pairs into flannel. So if each container/VM can choose an\narbitrary IP.\nAs for OVS support, it does not buy us much with our architecture. OVS is\ngreat for supporting complex flow configurations orchestrated by a central\nbrain. I think there're existing solutions to do that. flannel is designed\nfor simple flows but no central brain.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/coreos/flannel/issues/49#issuecomment-58403497\n. \n",
    "moritzheiber": "As far as I can see VXLAN support is done, right?\nHow about getting this \"back on track\"?\n. I'm referring to your earlier comment regarding multi-tenancy and adding support for it after VXLAN implementation is over.\nI'm pretty sure this is a feature flannel is suited for. And OVS is a pretty good backend for it as well.\n. What about collaborating with Socketplane? I know they're a part of Docker now, but their project looked very promising.\n. ",
    "steveeJ": "I'm definitely a supporter of the multi-tenancy feature. What's the status of this?\n. @shayts7 have you been able to reproduce this ever since?\n. @mischief pointed out that we are using https://godoc.org/github.com/Sirupsen/logrus in recent projects.\n. @fnordahl \n\nAny existing equal routes will be left untouched and will not genereate errors. Existing routes with different Gw will be replaced.\n\nIt seems like a step in the right direction.\nLong-term I think it's worth the effort to have a synchronized state between the local kernel routes and flannel's global knowledge about the routes. If flannel is (re)started, it's a reasonable assumption can be made that all routes that match the flannel subnet have and will be managed by flannel.\n. Fixed by #458.\n. @tomdee this was a rather specific request back then. Do you see any use for it?. On first sight I see that the filename:line information is gone which I personally prefer. Why would we chose the PR over the existing logging?\n\nEDIT: haven't seen the referenced issue, but am still happy to receive feedback\n. @tshprecher \nMy impression is that `https://godoc.org/github.com/Sirupsen/logrus is now the preferred logging package. This PR would now need a rebase and rework due to the recent switch from Godep to glide. Would you be open to switching to logrus or should we close this and someone else takes over?\nIn any way, thanks for the effort put into this!\n. Since the issue is fixed in the repo the issue can be closed now, thanks @tomdee! \n. @iT2afL0rd \n\nYou may try to upgrade flannel from 0.5.5 to 0.5.6, after upgrade, the issue did not happen again to me.\n\nI'm curious because there hasn't been an official flannel 0.5.6 release. Are you talking about builds from master?\n. This is not critical here but we usually conform to a specific commit message format, in short: subsystem: the change.\nFor example:\nMAINTAINERS: remove eyakubovich; add tomdee, philips, steveej\n. I'd suggest to add a note in the docs that flannel relies on either an ACCEPT rule or default policy in the nat POSTROUTING chain.\nOtherwise, LGTM\n. Please adjust the commit message(s) to conform to our CONTRIBUTING guidelines, thanks a lot!\n\nEDIT: I edited the message via squash/merge functionality\n. Please adjust the commit message(s) to conform to our CONTRIBUTING guidelines, thanks a lot!\n. @rosenhouse didn't know you have two identities ;-) I'll merge this as is but keep label for the PR to indicate it needs testing at one point.\n. LGTM with a minor nit\n. @tomdee \nWould be awesome to have a test that uses a predefined env file and tests against expected output options. Can I motivate you to add such a test?\n. @tomdee I haven't touched this script (using or writing) at all. But I think that as long as it's hosted in this repository and users rely on this to use flannel with Docker it's worth to ensure it's functionality. E.g. it's used in CoreOS directly https://github.com/coreos/coreos-overlay/blob/master/app-admin/flannel/files/flanneld-rkt.service#L48, so we definitely want this to work as long as it's there. \n. @tomdee \n\nI've added a test script. PTAL.\n\nFantastic, thanks! Shall we run it on travis?\n. @tomdee I think this one is almost good to go. The last steps before merging IMO are:\n- enable the mk-docker-opts-test in travis\n- split the mk-docker-opts rewrite / test into a separate commit\n. @tomdee somehow I was tricked by my own thoughts, I actually meant the exact opposite: squashing the commits.\n. LGTM\n. LGTM\n. @lucab mind taking a look?\n. @eyakubovich thanks! #462 is merged now so you're able to rework this on top. Please feel free to ping me to have it reviewed\n. And of course the obligatory:\n\nPlease adjust the commit message(s) to conform to our CONTRIBUTING guidelines, thanks a lot!\n. should probably be testing with 1.5\n. and then add GO15VENDOREXPERIMENT=1 as an environment variable\n. as long as there's no tests in network we should prevent it from being tested but only run gofmt and license checks against it\n. I must be missing something, so for the sake of educating me: is the current test approach only license and fmt checking?\n. This looks quite ugly, will gofmt not return non-zero if it finds a problem?\n. I'd be in favor to pass UID and GID environment variables to the container for this purpose to prevent possible issues with user namespace setups.\n. I could swear the next line wasn't there when I wrote that comment, apologies!\n. What's the reason for this change? AFAICT this isn't related to the bash->sh switch\n. I'm impressed by the effort, but I don't think we need to check the help output. Focusing on the generated output should be fine. Keeping the help output/test in sync doesn't seem worth the effort.\n. Interesting, thanks for the reference!\n. Nice catch @eyakubovich.\nLooks like a typical case of space vs. tab indentation.\n. Why is the architecture hardcoded in this case?\n. not needed in this case\n. The --interactive argument is not needed here, as the build process will not require human interaction. I hope I'm not wrong with the latter ;-)\n. IMHO container images that include the binary + dependencies are the way to go. No other binaries or archives needed. Feel free to ignore my radical opinion on this one\n. +1 for not making this a special thing but rather make this an option among others.\n. This needs to be changed to something more official. @eyakubovich @tomdee please assist in getting the automatic quay builds online again\n. Just for my education, where is the work about generating clients happening and how does a consumer - which flannel will be - know that the client needs to be updated? Shouldn't that be somewhat tied to the API?\n. Where is it declared and by what consumed that these files get written to /etc/kube-flannel?\n. I'd require a configuration file to be existent at this point. We can distribute one with flannel but I'm against writing one at runtime.\n. What's this method called by? Would it make sense to to only sleep when an error indicates that the system is not ready yet?\n. Can't we use the functionality from https://github.com/kubernetes/kubernetes/blob/master/pkg/client/cache/reflector.go to get real watching?\n. Why don't you want to keep a headline for building in a container?\n. We should mention that Docker is required for this.\n. What are the requirements for building?\n. the syscall package defines this constant. ",
    "helander": "When I started flannels using \"iface=xxx\" where xxx is the name of the device of my host only network, then subnet allocation works.\n. ",
    "pires": "Life-saver right here!\n. This means your controller-manager is not able to properly start the token manager, thus can't mount the service account. Check its logs for errors.\n. This may be related to the usage of the new flannel-wrapper script, in alpha.\n. ",
    "kiddkai": "Life-saver :+1: \n. ",
    "gregory90": "Nice, thanks for notice!\n. ",
    "bioshrek": "Hi, the flannel which i am using is experiencing the same failure. \nI built flannel with codes after commit 5647d1bbf67eca51cf4bf92f7f947fb7696f7a65, which support adding multiple etcd endpoints. But flannel is still experiencing the failure, and the values of keys \"/coreos.com/network/subnets/\" in my cluster are all empty.\nIt will be appreciated if you pay attention to this issue.\n. ",
    "pnegahdar": "+1 Encrypting the network would make this a portable VPC. \n. ",
    "zjeraar": "+1 Would be great!\n. ",
    "guruvan": "surprising that it's not an encrypted channel. making it so would make flannel super useful (as I'm already doing exactly this with openvpn for all my coreos and other docker nodes)\n. Flannel0? that's not really a usable address. Docker0 is on the flannel net, so I'm pinging that or a container running on the flannel network. \nI'm pinging that from either a container in another host, or from another host directly, but no matter - until I have an actual load on the network, it's not functional without manually starting the bi-directional flows. \nI thought for a moment that the move up to 522.2, and using the flanneld service unit provided (and binary) might change this - it did not. I found this as my skydns was originally set to run without --net=host on the container, and the announcements pointing to a container address - this was a nonstarter because I'd keep losing connectivity between HostA Containers and HostB containers. (\nWith more load on the network it's not noticeable. With my skydns set to --net=host and most of my resolv.confs pointing to DOCKER0 addresses, and rotating the resolv.conf so we get more of an even spread across the NSs we generate almost enough traffic to keep flannel rolling, but not quite, we'll see dropouts pretty regularly.\nAFAICT it's 100% reproduceable: just bring up a new flannel network, 2-3 hosts, and ping address on  SubnetB from HostA...and leave it running (with ping failing in my experience) - then go to HostB, ping anything on SubnetA and it works. Stop both pings for some period of time and it will fail again. \nhttps://registry.hub.docker/com/u/guruvan/flannel-keepalive in addition to skydns announcing each host's docker0 address for nameservice is how we're keeping a load on this. \n. One thing I realized: this is probably much more evident in a cluster+workers configuration than in just an etcd cluster. The etcd machines are more likely to generate traffic simultaneously than worker machines are. \nmy current config is a 3 machine etcd cluster with 1 standby, + an armada of workers\n. pings to host's eth0 address don't seem to make any difference. I'll try to get a dump when I fire up some more workers - I did run a dump on it several days ago, but I was using my own flannel unit and binary. Outbound traffic left via the flannel network and left via the encapped port on eth0 but failed to arrive on the other end. \nUsing the unit available in 522.2 it was not as prevalent, but evident in the workers, as they'd come up and fail to connect to nameservers running on docker0 on other hosts. Running my keepalive rectifies this. (as soon as other_host pings the new worker all is well. \nI've got to package up some cloudconfigs for @marineam - I'll package those up and package up the units I'm using to get the base network up along with some capture files. Probably be tomorrow when I get that all together\n. ",
    "visualphoenix": "With current master HEAD:\nI1022 05:40:05.394746 30590 main.go:111] Determining IP address of default interface\nI1022 05:40:05.395587 30590 main.go:188] Using 10.1.70.195 as external interface\nI1022 05:40:05.398490 30590 subnet.go:80] Subnet lease acquired: 172.30.13.0/24\nE1022 05:40:05.401863 30590 main.go:192] Could not init %v backend: %vUDPfailed to lookup interface flannel0\nE1022 05:40:05.401915 30590 main.go:173] failed to lookup interface flannel0\nI1022 05:40:05.401930 30590 main.go:248] UDP mode exited\nWith referenced prebuilt binary:\nI1022 05:40:58.462208 30638 main.go:89] Determining IP address of default interface\nI1022 05:40:58.464664 30638 main.go:164] Using 10.1.70.195 as external interface\nI1022 05:40:58.467798 30638 subnet.go:80] Subnet lease acquired: 172.30.13.0/24\nI1022 05:40:58.478100 30638 main.go:175] UDP mode initialized\nI1022 05:40:58.478164 30638 udp.go:238] Watching for new subnet leases\nI1022 05:40:58.479309 30638 udp.go:263] Subnet added: 172.30.24.0/24\nand the interface is up and working.\n. Specifically fails due to commit bc080e891d54c3afa8a7e9262d5708fa0b8c9f3f\n. CentOS / RHEL 6.5 share the same kernel version and patch set. We can't use\nnon-stock kernels.\nOn Wednesday, October 22, 2014, Eugene Yakubovich notifications@github.com\nwrote:\n\n@visualphoenix https://github.com/visualphoenix I tried it on my Ubuntu\nbox + CentOS 6.5 container and it works ok. That means it's something with\nthe CentOS kernel. It seems to fail on netlink socket when doing\nRTM_GETLINK. The commit in question changed the netlink library so it makes\nsense. This needs to be investigated to determine what particular option\nCentOS 6.5 kernel does support in RTM_GETLINK call.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/66#issuecomment-60137075.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. I'm building with FROM golang:1.3.3 in my Dockerfile.\nI will check and see if it is possible to use a EL6 version. Point is, the\nold library worked with upstream golang and no special patches to go itself.\nOn Tuesday, October 28, 2014, Kelsey Hightower notifications@github.com\nwrote:\n\n@visualphoenix https://github.com/visualphoenix @vishvananda\nhttps://github.com/vishvananda What version of golang are you using to\nbuild from source? We use golang 1.3 from upstream. I know RHEL patches\ngolang in there distribution.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/66#issuecomment-60773944.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. Thanks! Looks like it works so far!\nOn Fri, Oct 31, 2014 at 1:10 PM, Eugene Yakubovich <notifications@github.com\n\nwrote:\nClosed #66 https://github.com/coreos/flannel/issues/66 via #76\nhttps://github.com/coreos/flannel/pull/76.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/66#event-186788972.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. ",
    "vishvananda": "I don't but I wouldn't be surprised if the message to search by name is not supported. We may have to fall back to getting the whole list and matching.\n. I'm pretty sure this is just an issue with the older centos kernel not supporting the netlink query we are using.. We just need to fallback to the old working method.\n. ",
    "barakmich": "LGTM, pending \"what's the process to upgrade a running flannel?\" -- either as a bug or doc somewhere why there isn't one.\n. Hah. Yeah, LGTM. Not sure if this is documentation-worthy or not, as it's a slight change in preference, but I would think this is certainly the right thing.\n. LGTM. I think I get why you're storing a RawMessage, and this seems like a better way to go about it in general.\n. Stylistic question: Why the switch here to passing the pointer? Either this should be &LeaseAttrs, or the concrete type should be passed to AcquireLease. Probably the former though.\n. Is this the intended behavior? I suppose the name is \"ensureLink\" but it seems iffy to delete something that is incompatible (ie, maybe not ours).\n. nm. Hardcoded to be flannel.x\n. If there are multiple backends at play, could there be multiple vxlan backends?\n. Took me a second. Would these cases be better handled (if they're mutually exclusive like this) by len(miss.IP) != 0 { handleL3Miss(...) }  -- suggesting the presence of one instead of the absence of the other?\n. Could that then relate to the other point about flannel.x -- ie, that there should be one (and only one) flannel running at a time? It may not solve the update issue, but detecting other flannels on a single box (lock file, or somesuch) and bailing might help. As for updates (without losing connectivity, temporarily) that seems harder. Don't know, just thinking out loud.\n. ",
    "ernado": "@eyakubovich, thank you for responce.\ncore@first ~ $ netstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n0.0.0.0         104.131.176.1   0.0.0.0         UG        0 0          0 eth0\n0.0.0.0         0.0.0.0         0.0.0.0         U         0 0          0 eth0\n10.0.0.0        0.0.0.0         255.255.0.0     U         0 0          0 flannel0\n10.0.43.0       0.0.0.0         255.255.255.0   U         0 0          0 docker0\n10.132.0.0      0.0.0.0         255.255.0.0     U         0 0          0 eth1\n104.131.176.0   0.0.0.0         255.255.240.0   U         0 0          0 eth0\n169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth0\ncore@second ~ $ netstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n0.0.0.0         104.131.64.1    0.0.0.0         UG        0 0          0 eth0\n0.0.0.0         0.0.0.0         0.0.0.0         U         0 0          0 eth0\n10.0.0.0        0.0.0.0         255.255.0.0     U         0 0          0 flannel0\n10.0.13.0       0.0.0.0         255.255.255.0   U         0 0          0 docker0\n10.132.0.0      0.0.0.0         255.255.0.0     U         0 0          0 eth1\n104.131.64.0    0.0.0.0         255.255.192.0   U         0 0          0 eth0\n169.254.0.0     0.0.0.0         255.255.0.0     U         0 0          0 eth0\nI'm not quite sure what is default role in this case, but I reproduced subj. without private networking.\n. ```\ncore@second ~ $ ip route\ndefault via 104.131.64.1 dev eth0 \ndefault dev eth0  scope link  metric 99 \n10.0.0.0/16 dev flannel0  proto kernel  scope link  src 10.0.13.0 \n10.0.13.0/24 dev docker0  proto kernel  scope link  src 10.0.13.1 \n10.132.0.0/16 dev eth1  proto kernel  scope link  src 10.132.202.49 \n104.131.64.0/18 dev eth0  proto kernel  scope link  src 104.131.111.175 \n169.254.0.0/16 dev eth0  proto kernel  scope link  src 169.254.74.205 \ncore@second ~ $ ip addr\n...\n2: eth0:  mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 04:01:2d:15:83:01 brd ff:ff:ff:ff:ff:ff\n    inet 169.254.74.205/16 brd 169.254.255.255 scope link eth0\n       valid_lft forever preferred_lft forever\n    inet 104.131.111.175/18 brd 104.131.127.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::601:2dff:fe15:8301/64 scope link \n       valid_lft forever preferred_lft forever\n```\nI checked link-local communication - it does not work between hosts.\nI run both flannel with --iface=eth1, now it binds to eth1, but I still cant ping/curl my nginx container ip.\n``` bash\ncore@first ~ $ sudo systemctl status flannel\n\u25cf flannel.service\n   Loaded: loaded (/etc/systemd/system/flannel.service; static)\n   Active: active (running) since Thu 2014-10-23 19:50:52 UTC; 2min 38s ago\n  Process: 1428 ExecStartPre=/usr/bin/etcdctl mk /coreos.com/network/config {\"Network\":\"10.0.0.0/16\"} (code=exited, status=4)\n Main PID: 1434 (flanneld)\n   CGroup: /system.slice/flannel.service\n           \u2514\u25001434 /opt/bin/flanneld --iface=eth1\nOct 23 19:50:52 first.core.cydev.ru systemd[1]: Started flannel.service.\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.251603 01434 main.go:231] Installing signal handlers\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.253466 01434 main.go:188] Using 10.132.202.54 as external interface\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.254851 01434 subnet.go:298] Picking subnet in range 10.0.1.0 ... 10.0.255.0\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.328888 01434 subnet.go:80] Subnet lease acquired: 10.0.74.0/24\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.330670 01434 main.go:199] UDP mode initialized\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.330705 01434 udp.go:239] Watching for new subnet leases\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.337006 01434 udp.go:264] Subnet added: 10.0.13.0/24\nOct 23 19:50:52 first.core.cydev.ru flanneld[1434]: I1023 19:50:52.337715 01434 udp.go:264] Subnet added: 10.0.43.0/24\nOct 23 19:51:46 first.core.cydev.ru flanneld[1434]: I1023 19:51:46.479270 01434 udp.go:264] Subnet added: 10.0.77.0/24\ncore@first ~ $ ping 10.0.13.2\nPING 10.0.13.2 (10.0.13.2) 56(84) bytes of data.\n^C\n--- 10.0.13.2 ping statistics ---\n2 packets transmitted, 0 received, 100% packet loss, time 999ms\n```\n``` bash\ncore@second ~ $ docker run -d --name nginx -p 80:80 dockerfile/nginx\n43205afca2121b10d70ed3c15334f1293d8a7a1919d4098642ecb8a329238506\ncore@second ~ $ docker inspect nginx | grep IPA\n        \"IPAddress\": \"10.0.13.2\",\ncore@second ~ $ curl 10.0.13.2\n<!DOCTYPE html>\n\n\nWelcome to nginx!\n\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n\n\n\nWelcome to nginx!\nIf you see this page, the nginx web server is successfully installed and\nworking. Further configuration is required.\nFor online documentation and support please refer to\nnginx.org.\nCommercial support is available at\nnginx.com.\nThank you for using nginx.\n\n\ncore@second ~ $ sudo systemctl status flannel\n\u25cf flannel.service\n   Loaded: loaded (/etc/systemd/system/flannel.service; static)\n   Active: active (running) since Thu 2014-10-23 19:51:46 UTC; 2min 2s ago\n  Process: 2487 ExecStartPre=/usr/bin/etcdctl mk /coreos.com/network/config {\"Network\":\"10.0.0.0/16\"} (code=exited, status=4)\n Main PID: 2493 (flanneld)\n   CGroup: /system.slice/flannel.service\n           \u2514\u25002493 /opt/bin/flanneld --iface=eth1\nOct 23 19:51:46 second.core.cydev.ru systemd[1]: Started flannel.service.\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.439709 02493 main.go:231] Installing signal handlers\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.440861 02493 main.go:188] Using 10.132.202.49 as external interface\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.441686 02493 subnet.go:298] Picking subnet in range 10.0.1.0 ... 10.0.255.0\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.475688 02493 subnet.go:80] Subnet lease acquired: 10.0.77.0/24\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.480270 02493 main.go:199] UDP mode initialized\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.480514 02493 udp.go:239] Watching for new subnet leases\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.480982 02493 udp.go:264] Subnet added: 10.0.43.0/24\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.481020 02493 udp.go:264] Subnet added: 10.0.13.0/24\nOct 23 19:51:46 second.core.cydev.ru flanneld[2493]: I1023 19:51:46.481053 02493 udp.go:264] Subnet added: 10.0.74.0/24\n```\n. @eyakubovich \nThank you, it worked well when I restarted\n``` bash\ncore@first ~ $ curl 10.0.77.7\n<!DOCTYPE html>\n...\ncore@second ~ $ docker inspect 14b1 | grep IPA\n        \"IPAddress\": \"10.0.77.7\",\ncore@second ~ $ curl 10.0.77.7\n<!DOCTYPE html>\n\n\n...\n```\nNow I need to figure out how to make flannel-docker correctly start on boot, but it is different issue\n. ",
    "monkey-H": "Did you figure out how to make flannel-docker correctly start on boot ? @ernado \n. core@core-01 ~/flannel $ etcdctl get coreos.com/network/config\n{\"Network\":\"10.0.0.0/16\"}\n. I am using CoreOS beta (557.0.0)\nAny idea?\n. I changed to stable version , then it's ok !\n. Thank you\uff0cI restarted the computer, and it worked.\n. ",
    "infinityhacks": "I have met the same issue, @ernado Can you show us more about how fixing that ? or just a restart ?\nThanks.\n. ",
    "DenisIzmaylov": "Thank you guys. This discuss has been very useful for me. \n. ",
    "nightwolfzor": "Thanks that works for us! \n. ",
    "akaspin": "Bad design\n. ",
    "dysinger": "I've found this to be invalid.  I did not investigate hard enough.  It turned out to be a container problem. It's working for me now.  Thanks to @eyakubovich for helping me figure it out.\n. ",
    "tlvenn": "Thanks for the clarification @eyakubovich \n. ",
    "mikedanese": "Old.\n. Thanks for the info. That makes a lot of sense. Maybe it will be easier in a latter iteration of Flannel. Feel free to close this.\n. Ok, I think I got glide working\n. @luxas @aaronlevy PTAL\n. Addressed comments\n. @luxas we are not using that feature currently but I think that's a good idea.\n. > log nodename == \"\" error\nWhat are you referring to?\n\nannotation vs configmap\n\nI think we should only support configmap initially.  This is exactly the usecase that configmap was designed for. I don't see a benefit to using annotations.\n. > Sorry if it was unclear: #483 (comment)\n@luxas That is fixed?\n\nOK, but can we package them in the same file then for easy flannel turnup\n\nYes you can have a list of resources in a single file.\n. @luxas I combined the config to a single file.\n. No sleeps should be necessary. Leftover cruft from debugging and now removed.\n. Oops I take that back.\n. @luxas I don't think we can until pkg/controller/framework is moved to a versioned client.\n. @philips I don't think we can until pkg/controller/framework is moved to a versioned client.\n. This should be able to use some of client-go once https://github.com/kubernetes/kubernetes/pull/32718/ merges to reduce the amount of vendord.\n. This diff seems absurdly large.. @tomdee \"context\" (added in go1.7) is required by k8s.io/client-go but the current kube-cross has go1.6.. I tested this by hand and it seems to work fine. @aaronlevy suggested a smaller patch that would fix this.. We definitely want to do both. I do think moving to client-go sooner rather then later is safe. FWIW we are using the same packages in client-go that are used extensively in the controller-manager already. beta here means that the public API might change and you will compile errors when you rebase, but the client is tested extensively. I can look at that patch later today.. I tested this by hand on friday. We should probably merge this, cut a release then consider merging #677.. (more hand testing is always appreciated). I don't know if this works yet. We at least need \"Network\" (subnet for pods). We can conceivably do this once we configure kube-controller-manager with componentconfig and that info is stored in the API.\n. internalclientset is a way of weining people off the unversioned not generated client (old one). We can move to versioned clientsets once the controller.Framework supports them.\n. That was my thinking although I did not test to see if this would be an issue. Node objects are frequently updated because of the heartbeat.\n. I am unopinionated on this. Who wants to decide how this feature bubbles up to users?\n. Please see the daemonset config. We share /etc/cni/net.d with the host so writing this file tells kubelet to use the cni plugin to provision pod nets.\n. Done.\n. ok, looks like progress on nodeName has slowed unfortunately\n. Probably don't need this...\n. We at least need the cluster cidr to run so for now we need a config map. \n. I can add a default cni conf.\n. I will remove these logs\n. If we specified a file in a configmap, we should hard fail if it can't be read.\n. Since the subnet manager doesn't support incremental updates yet, we need to ratelimit full relists to avoid unnecessary busy looping.\n. They should match.\n. What should I call the default subnet manager?\n. Where was the discussion?\n. This is not what we did with the remote subnet manager so the cli flag api might be a little consistent. We use the remote subnet manager if --remote is passed.\n. I don't know if I understand the suggestion. This is called from here https://github.com/coreos/flannel/blob/master/subnet/watch.go#L262 inside the backend\n. We already are. The store is maintained by a reflector which is setup inside the framework.Informer. The framework.Informer has a superset of the functionality.\n. It's declared in the volumeMounts field in the flannel daemonset.\n. Done.\n. We need a v1.4 release. I can wait to use a v1.4 release once one is tagged (right now only betas are out). \n. Side note: it took way to long to update vendor (multiple hours). I have no idea what I'm doing with glide. Any straightforward doc on updating deps I should look at?\n. I had it copying once and @luxas asked me to put it in a while. https://github.com/coreos/flannel/pull/483#discussion_r77900055\nI am unopionated\n. It should never change at runtime\n. Thanks, I will give that a try. \n. ok reverted. busybox doesn't support sleep infinity and shell should have a pause builtin.\n. I'm planning on using 3.0 but still trying to figure out glide. https://github.com/kubernetes/client-go#compatibility-matrix. @aaronlevy it's a subresource under node that only allows updates to .status fields and annotations. It's less privileged then the node resource and can be acl'd seperately. This code was written pre-rbac and I'm trying to reduce the privilege required by kube-flannel.\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/registry/core/node/strategy.go#L122 status strategy zeros out changes to spec.. Also see changes to rbac file.. ",
    "anguslees": "I'm no godep expert, but I think this is the way it was intended to be used (?)\n. Yeah, I have no particular issue that this PR helps address.   I just noticed you were importing the explicit Godeps/_workspace paths and not using the godep wrapper to implicitly add Godeps/_workspace to the go build path - and that this was different from the few other projects I've seen that use godep (etcd, kubernetes).\n(For what it's worth, I think that doc only talks about how the vendored sources are fetched (recommending moving to copying to local version control, rather than pulling at build time).  It doesn't talk about dropping the use of godep path (or godep go) to find the vendored dependencies once they're available on disk.  But again, I do very little golang dev work and would be ignorant of any larger godep discussions - I trust your recommendation here.)\nI'm completely happy if you want to reject this PR.\n. @eyakubovich: I don't know about other docker use cases, but I'm running this right now and I can connect between kubernetes pods just fine (between different hosts and within the same host)\n. Yeah, I wasn't crazy about routed either.  I think layer2 is accurate but kind of misleading too.  subnet perhaps?\n. In particular, you should be able to ping 10.2.98.1 from ip-172-29-1-44.us-west-2.compute.internal (and similarly the docker0 IP on ip-172-29-1-44 from ip-172-29-1-203) without any NAT/masquerading.  If that doesn't work, I suggest tcpdumping and poking at firewalls, aws config, etc.  As far as I can see, flannel has done its job correctly and allocated/installed the correct 10.2../24 routes (assuming this is what was desired, since there was no flannel config included in the bug report).\nIf you can ping .1 from a remote host but not other (container) IPs on the docker subnet, I'd suggest looking into your firewall setup on the host (and of course you need ip_forwarding=1).\n. Better yet (and what I do locally) - extend kube-flannel.yaml to include a daemonset for each* architecture, with the appropriate beta.kubernetes.io/arch=$arch nodeSelector.  That way your cluster automatically installs the right flannel daemon, even across multiple architectures (I have mixed arm+amd64).\nThe bit that prevents this being \"easy\" on arm currently is that the quay.io/coreos/flannel-cni image only exists for amd64, and the quay.io/coreos/flannel image doesn't actually include the cni executable - making the basic \"cp flannel /opt/cni/bin/\" step harder than it needs to be on non-amd64. I reported this as https://github.com/coreos/flannel-cni/issues/10 but no action yet.. I don't know why, but you are running the pod as the node service-account: User \"system:node:crismon01-xps15\" cannot get daemonsets.extensions in the namespace \"kube-system\"\nIt needs to run as a new service-account, which has been given the required RBAC access.  See Documentation/k8s-manifests/kube-flannel-rbac.yml. ",
    "eparis": "This PR does make the flannel code more re-usable. It follows the norm for using Godeps. It also means that future people working on flannel don't have to remember this 'flannelism'.\nObviously this patch doesn't work as is, since godep isn't installed on the builders. But that's easy to handle.  Inside build use \"export GOPATH=${PWD}/gopath:${PWD}/Gopath/_workspace\" instead of calling godep go build. This is how quite a number of golang projects handle it.\nNot hard coding the flannel path into the dependency makes it easier for people to actually use godep and test new dependencies 'the godep way'.\nIt also makes it a lot easier to use flannel in organizations that require building from HEAD. Instead of allowing every piece of software to version things independently. A number of shops who build golang software do not allow godep like versioning, supporting those people is nice as well.\nI can happily create another PR which fixes the build problem, if @anguslees is not will/able\n. @nhorman\n. @eyakubovich So what I'd like to do is just import and use the env file even if flannel isn't running.  I can't write a service file\nAfter=flannel.service\nEnvironmentFile=-/run/flannel/envfile\nExec=/sbin/docker -bip=${FLANNEL_SUBNET} -mtu=${FLANNEL_MTU} -d\nBecause if flannel isn't running FLANNEL_SUBNET and FLANNEL_MTU will be unset.  So now I'm really running\n/sbin/docker -bip= -mtu= -d\nWhich will fail, since docker want to parse the bip/mtu arguments:\ninvalid value \"\" for flag -mtu: strconv.ParseInt: parsing \"\": invalid syntax\n. Since --docker-fmt is going to be an explicit request by the distro/admin it doesn't seem unreasonable to expect 1.3+    We won't break anyone, just start to clean things up....\n. And you can more coherently handle ip-masq since you know at run time true/false rather than forcing it in service files/scripts...\nthis seems like a change/work now, but a maintenance win down the road....\n. I'm ok with the idea of a helper script.\nAnything that requires me to change the docker service file kinda stinks, but it's better than what we have today...\nI also don't love the user experience of an admin having to enable both flannel and flannel-generate (nor the experience of having the admin enable just flannel-generate)\nMaybe a mechanism where flanneld itself calls the generator script? I don't love that implementation either...\n. @eyakubovich agreed...  I'll poke this a little bit, i'm worried about a race between the flanneld sd_notify() which allows docker to start, but the generator hasn't run yet...  I'll dig a bit.\n. ExecStartPost is not necessarily finished before system considers a service 'ready'.\nI tried to fix it cleanly using only systemd.  uggh.  I still think this pr is the better solution.\nhttp://pkgs.fedoraproject.org/cgit/flannel.git/commit/?id=8d0e6c275d6eb3c779d4f3f5e29b84dd865796b2\nTell me that's not horrific!  I need a new 2 new unit files and a generator...\n. I just retested and it worked as well. It was lennart who told me they weren't tied together. Liar   :)\n. I'd love for this to end in .sh, like most scripts.  no big deal.  I made a couple of editions (addressed #244 in the process I think...)\nparticularly the stuff around \"combined_opts\"\nhttp://pkgs.fedoraproject.org/cgit/flannel.git/commit/?id=8d0e6c275d6eb3c779d4f3f5e29b84dd865796b2\n. -k and the \"combined_opts\" let me define on the command line what I wanted to use. (fedora uses DOCKER_NETWORK_OPTIONS)\nI don't know how to give a patch against a PR making that name mutable from the command line.\n. wrong link last time.  so sorry!\nhttp://pkgs.fedoraproject.org/cgit/flannel.git/commit/?id=3b954b37fa5e65278a60e1945d3180c859a13be3\n. LGTM\n. except we just undid you change from DOCKER_OPTS to DOCKER_NET_OPTS\nmind you these DOCKER_OPTS will include the other DOCKER_OPTS, so maybe you don't care.\n. It follows the same pattern as what flannel does if etcd is down.  It just keeps looping....\n. Then again, systemd stopped telling people to not use network-online.  so maybe i'm fine with it now....\n. The first comment I got back:\n- They may wish to use aes_gcm128 and not aes256. That will give you a lot more bang for our buck and you'll be able to run 100x more traffic on the same hardware, see: https://libreswan.org/wiki/Benchmarking_and_Performance_testing (it's really kernel benchmarking for IPsec, so it applies to any userland used)\n- Quick skim of their patch seems to look that they are adding their own xfrm policies\n  into the kernel outside of an IKE daemon? At least based on my reading of the use of\n  netlink.XfrmPolicyAdd() and netlink.AddXFRMState(). Seeing things like generateRandomBytes()\n  in this code also does not look right.\nStill quite a bit of discussion around the whole concept, but that was the most specific feedback I got.\n. ",
    "dennybritz": "@eyakubovich  Yes, I'm running in a VPC.\n\nIt's logged as vxlan is new and I want extra logging. I think the problem is somewhere else.\n\nI see. However, the same log statements appear nowhere else in the logs and only start appearing around the same time flannel stops working.\nPinging eth0 on the remote machine works:\nPING 10.0.0.126 (10.0.0.126) 56(84) bytes of data.\n64 bytes from 10.0.0.126: icmp_seq=1 ttl=64 time=0.638 ms\n64 bytes from 10.0.0.126: icmp_seq=2 ttl=64 time=0.370 ms\nPinging flannel0 (this is docker0, right?) on the local machine works for both machines:\nPING 10.10.109.1 (10.10.109.1) 56(84) bytes of data.\n64 bytes from 10.10.109.1: icmp_seq=1 ttl=64 time=0.046 ms\n64 bytes from 10.10.109.1: icmp_seq=2 ttl=64 time=0.035 ms\nPinging flannel.1 on the local machine works for both machines:\nPING 10.10.109.0 (10.10.109.0) 56(84) bytes of data.\n64 bytes from 10.10.109.0: icmp_seq=1 ttl=64 time=0.054 ms\n64 bytes from 10.10.109.0: icmp_seq=2 ttl=64 time=0.040 ms\nPinging either docker0 or flannel.1 on the remote machine does not work. \nThis happens in tcpdump:\nSending machine:\nsh-4.2# tcpdump -v 'port 8472'\ntcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n02:56:02.327762 IP (tos 0x0, ttl 64, id 11321, offset 0, flags [none], proto UDP (17), length 134)\n    ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP (tos 0x0, ttl 64, id 16107, offset 0, flags [DF], proto ICMP (1), length 84)\n    ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 1, length 64\n02:56:03.327960 IP (tos 0x0, ttl 64, id 11922, offset 0, flags [none], proto UDP (17), length 134)\n    ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP (tos 0x0, ttl 64, id 16494, offset 0, flags [DF], proto ICMP (1), length 84)\n    ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 2, length 64\n02:56:04.327951 IP (tos 0x0, ttl 64, id 12315, offset 0, flags [none], proto UDP (17), length 134)\n    ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP (tos 0x0, ttl 64, id 17120, offset 0, flags [DF], proto ICMP (1), length 84)\n    ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 3, length 64\n02:56:05.327949 IP (tos 0x0, ttl 64, id 12358, offset 0, flags [none], proto UDP (17), length 134)\n    ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP (tos 0x0, ttl 64, id 17866, offset 0, flags [DF], proto ICMP (1), length 84)\n    ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 4, length 64\n02:56:06.327948 IP (tos 0x0, ttl 64, id 12663, offset 0, flags [none], proto UDP (17), length 134)\n    ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP (tos 0x0, ttl 64, id 18023, offset 0, flags [DF], proto ICMP (1), length 84)\n    ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 5, length 64\n02:56:07.327952 IP (tos 0x0, ttl 64, id 12918, offset 0, flags [none], proto UDP (17), length 134)\n    ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP (tos 0x0, ttl 64, id 18240, offset 0, flags [DF], proto ICMP (1), length 84)\n    ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 6, length 64\n^C\n6 packets captured\n6 packets received by filter\n0 packets dropped by kernel\nReceiving machine (packets from other machine than the sending machine are received as well):\nsh-4.2# tcpdump 'port 8472'\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n02:56:00.372779 IP ip-10-0-0-130.ec2.internal.36461 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-85-0.ec2.internal.55896 > ip-10-10-109-8.ec2.internal.amqp: Flags [P.], seq 439869726:439869734, ack 4278089106, win 218, options [nop,nop,TS val 684964124 ecr 684939401], length 8\n02:56:00.372860 IP ip-10-0-0-129.ec2.internal.44026 > ip-10-0-0-130.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-109-8.ec2.internal.amqp > ip-10-10-85-0.ec2.internal.55896: Flags [.], ack 8, win 217, options [nop,nop,TS val 684954401 ecr 684964124], length 0\n02:56:02.326245 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 1, length 64\n02:56:03.326735 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 2, length 64\n02:56:04.214092 IP ip-10-0-0-129.ec2.internal.44026 > ip-10-0-0-130.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-109-8.ec2.internal.amqp > ip-10-10-85-0.ec2.internal.55896: Flags [P.], seq 1:9, ack 8, win 217, options [nop,nop,TS val 684958242 ecr 684964124], length 8\n02:56:04.214546 IP ip-10-0-0-130.ec2.internal.36461 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-85-0.ec2.internal.55896 > ip-10-10-109-8.ec2.internal.amqp: Flags [.], ack 9, win 218, options [nop,nop,TS val 684967966 ecr 684958242], length 0\n02:56:04.326424 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 3, length 64\n02:56:05.326529 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 4, length 64\n02:56:06.326458 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 5, length 64\n02:56:07.326805 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 6, length 64\n02:56:07.761686 IP ip-10-0-0-128.ec2.internal.60215 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-78-0.ec2.internal.57891 > ip-10-10-109-8.ec2.internal.amqp: Flags [P.], seq 1085651941:1085651949, ack 4057287077, win 218, options [nop,nop,TS val 684972546 ecr 684946790], length 8\n02:56:07.761783 IP ip-10-0-0-129.ec2.internal.34077 > ip-10-0-0-128.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-109-8.ec2.internal.amqp > ip-10-10-78-0.ec2.internal.57891: Flags [.], ack 8, win 217, options [nop,nop,TS val 684961790 ecr 684972546], length 0\n02:56:08.326477 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 7, length 64\n02:56:09.326445 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 8, length 64\n02:56:10.326695 IP ip-10-0-0-126.ec2.internal.43647 > ip-10-0-0-129.ec2.internal.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP ip-10-10-1-0.ec2.internal > ip-10-10-109-1.ec2.internal: ICMP echo request, id 22316, seq 9, length 64\n^C\n15 packets captured\n15 packets received by filter\n0 packets dropped by kernel\nSo it seems like packets are arriving on the overlay network...\n. Based on the above it seems like this is a one-way issue. If I switch receiving/sending machines I get \nping 10.10.1.1\nPING 10.10.1.1 (10.10.1.1) 56(84) bytes of data.\nFrom 10.10.109.0 icmp_seq=1 Destination Host Unreachable\nFrom 10.10.109.0 icmp_seq=2 Destination Host Unreachable\nFrom 10.10.109.0 icmp_seq=3 Destination Host Unreachable\nFrom 10.10.109.0 icmp_seq=4 Destination Host Unreachable\nAnd no ICMP packets are being captured on port 8472.\n. @eyakubovich \n```\nifconfig\ndocker0: flags=4163  mtu 8951\n        inet 10.10.109.1  netmask 255.255.255.0  broadcast 0.0.0.0\n        inet6 fe80::5484:7aff:fefe:9799  prefixlen 64  scopeid 0x20\n        ether 56:84:7a:fe:97:99  txqueuelen 0  (Ethernet)\n        RX packets 81863  bytes 34079868 (32.5 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 69889  bytes 28946691 (27.6 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\neth0: flags=4163  mtu 9001\n        inet 10.0.0.129  netmask 255.255.255.0  broadcast 10.0.0.255\n        inet6 fe80::80e:65ff:fe61:cea4  prefixlen 64  scopeid 0x20\n        ether 0a:0e:65:61:ce:a4  txqueuelen 1000  (Ethernet)\n        RX packets 24867028  bytes 17914452744 (16.6 GiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 20723760  bytes 3348891261 (3.1 GiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nflannel.1: flags=4163  mtu 8951\n        inet 10.10.109.0  netmask 255.255.0.0  broadcast 0.0.0.0\n        inet6 fe80::107d:bcff:fe75:191f  prefixlen 64  scopeid 0x20\n        ether 12:7d:bc:75:19:1f  txqueuelen 0  (Ethernet)\n        RX packets 48029  bytes 26666114 (25.4 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 45038  bytes 28195557 (26.8 MiB)\n        TX errors 0  dropped 2051 overruns 0  carrier 0  collisions 0\nlo: flags=73  mtu 65536\n        inet 127.0.0.1  netmask 255.0.0.0\n        inet6 ::1  prefixlen 128  scopeid 0x10\n        loop  txqueuelen 0  (Local Loopback)\n        RX packets 27187  bytes 5091098 (4.8 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 27187  bytes 5091098 (4.8 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nveth3834: flags=67  mtu 8951\n        inet6 fe80::7c84:40ff:fecc:1f16  prefixlen 64  scopeid 0x20\n        ether 7e:84:40:cc:1f:16  txqueuelen 1000  (Ethernet)\n        RX packets 17864  bytes 5507830 (5.2 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 17913  bytes 1527268 (1.4 MiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nveth473d: flags=67  mtu 8951\n        inet6 fe80::d426:3dff:feca:d64d  prefixlen 64  scopeid 0x20\n        ether d6:26:3d:ca:d6:4d  txqueuelen 1000  (Ethernet)\n        RX packets 14456  bytes 1480514 (1.4 MiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 8313  bytes 548832 (535.9 KiB)\n        TX errors 0  dropped 1 overruns 0  carrier 0  collisions 0\nveth7d7f: flags=67  mtu 8951\n        inet6 fe80::7009:a4ff:fef9:efc2  prefixlen 64  scopeid 0x20\n        ether 72:09:a4:f9:ef:c2  txqueuelen 1000  (Ethernet)\n        RX packets 9  bytes 738 (738.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 78  bytes 4404 (4.3 KiB)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n```\nip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc pfifo_fast state UP qlen 1000\n    link/ether 0a:0e:65:61:ce:a4 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.129/24 brd 10.0.0.255 scope global dynamic eth0\n       valid_lft 3083sec preferred_lft 3083sec\n    inet6 fe80::80e:65ff:fe61:cea4/64 scope link \n       valid_lft forever preferred_lft forever\n105: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UNKNOWN \n    link/ether 12:7d:bc:75:19:1f brd ff:ff:ff:ff:ff:ff\n    inet 10.10.109.0/16 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::107d:bcff:fe75:191f/64 scope link \n       valid_lft forever preferred_lft forever\n110: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP \n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 10.10.109.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link \n       valid_lft forever preferred_lft forever\n120: veth7d7f: <BROADCAST,UP,LOWER_UP> mtu 8951 qdisc pfifo_fast master docker0 state UP qlen 1000\n    link/ether 72:09:a4:f9:ef:c2 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::7009:a4ff:fef9:efc2/64 scope link \n       valid_lft forever preferred_lft forever\n122: veth473d: <BROADCAST,UP,LOWER_UP> mtu 8951 qdisc pfifo_fast master docker0 state UP qlen 1000\n    link/ether d6:26:3d:ca:d6:4d brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::d426:3dff:feca:d64d/64 scope link \n       valid_lft forever preferred_lft forever\n124: veth3834: <BROADCAST,UP,LOWER_UP> mtu 8951 qdisc pfifo_fast master docker0 state UP qlen 1000\n    link/ether 7e:84:40:cc:1f:16 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::7c84:40ff:fecc:1f16/64 scope link \n       valid_lft forever preferred_lft forever\nnetstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n0.0.0.0         10.0.0.1        0.0.0.0         UG        0 0          0 eth0\n10.0.0.0        0.0.0.0         255.255.255.0   U         0 0          0 eth0\n10.0.0.1        0.0.0.0         255.255.255.255 UH        0 0          0 eth0\n10.10.0.0       0.0.0.0         255.255.0.0     U         0 0          0 flannel.1\n10.10.109.0     0.0.0.0         255.255.255.0   U         0 0          0 docker0\nThe ARP table while running ping, seems like there is no mapping?\nip-10-10-109-4.ec2.inte          (incomplete)                              docker0\nip-10-10-85-0.ec2.inter  ether   06:c6:77:33:ac:46   C                     flannel.1\nip-10-0-0-128.ec2.inter  ether   0a:0a:6b:b6:37:2a   C                     eth0\nip-10-10-109-5.ec2.inte  ether   d2:9f:58:80:9c:a3   C                     docker0\nip-10-10-109-8.ec2.inte  ether   16:eb:b7:b4:78:9a   C                     docker0\nip-10-0-0-130.ec2.inter  ether   0a:79:35:13:84:b0   C                     eth0\nip-10-10-109-3.ec2.inte  ether   22:31:58:f0:29:55   C                     docker0\nip-10-0-0-126.ec2.inter  ether   0a:d1:53:10:59:72   C                     eth0\nip-10-10-92-0.ec2.inter  ether   b6:15:9f:ed:a9:99   C                     flannel.1\nip-10-10-1-1.ec2.intern          (incomplete)                              flannel.1\nip-10-10-1-0.ec2.intern          (incomplete)                              flannel.1\nip-10-10-17-3.ec2.inter          (incomplete)                              flannel.1\nip-10-0-0-1.ec2.interna  ether   0a:09:1c:0d:72:c1   C                     eth0\nip-10-0-0-2.ec2.interna  ether   0a:09:1c:0d:72:c1   C                     eth0\nip-10-10-78-0.ec2.inter  ether   86:cb:0b:78:50:88   C                     flannel.1\nip-10-10-109-7.ec2.inte  ether   96:df:6b:c6:22:ba   C                     docker0\nip-10-0-0-167.ec2.inter  ether   0a:a1:99:3d:39:b7   C                     eth0\nbridge fdb\n33:33:00:00:00:01 dev eth0 self permanent\n01:00:5e:00:00:01 dev eth0 self permanent\n33:33:ff:61:ce:a4 dev eth0 self permanent\n06:c6:77:33:ac:46 dev flannel.1 dst 10.0.0.130 self \n86:cb:0b:78:50:88 dev flannel.1 dst 10.0.0.128 self \n56:84:7a:fe:97:99 dev docker0 vlan 0 permanent\n96:df:6b:c6:22:ba dev veth473d vlan 0 \n72:09:a4:f9:ef:c2 dev veth7d7f vlan 0 permanent\nd6:26:3d:ca:d6:4d dev veth473d vlan 0 permanent\n7e:84:40:cc:1f:16 dev veth3834 vlan 0 permanent\n16:eb:b7:b4:78:9a dev veth3834 vlan 0 \n33:33:00:00:00:01 dev veth7d7f self permanent\n01:00:5e:00:00:01 dev veth7d7f self permanent\n33:33:ff:f9:ef:c2 dev veth7d7f self permanent\n33:33:00:00:00:01 dev veth473d self permanent\n01:00:5e:00:00:01 dev veth473d self permanent\n33:33:ff:ca:d6:4d dev veth473d self permanent\n33:33:00:00:00:01 dev veth3834 self permanent\n01:00:5e:00:00:01 dev veth3834 self permanent\n33:33:ff:cc:1f:16 dev veth3834 self permanent\nThis is the tail of dmesg:\n[627839.752731] docker0: port 1(veth7d7f) entered forwarding state\n[627839.758218] docker0: port 1(veth7d7f) entered forwarding state\n[627854.816087] docker0: port 1(veth7d7f) entered forwarding state\n[627867.690771] docker0: port 2(vetha248) entered disabled state\n[627867.694547] device vetha248 left promiscuous mode\n[627867.696624] docker0: port 2(vetha248) entered disabled state\n[627930.186824] device veth473d entered promiscuous mode\n[627930.192813] IPv6: ADDRCONF(NETDEV_UP): veth473d: link is not ready\n[627930.201281] IPv6: ADDRCONF(NETDEV_CHANGE): veth473d: link becomes ready\n[627930.206117] docker0: port 2(veth473d) entered forwarding state\n[627930.210761] docker0: port 2(veth473d) entered forwarding state\n[627945.248064] docker0: port 2(veth473d) entered forwarding state\n[628006.978642] docker0: port 3(vethf5ca) entered disabled state\n[628006.982581] device vethf5ca left promiscuous mode\n[628006.984739] docker0: port 3(vethf5ca) entered disabled state\n[628037.723530] device veth3834 entered promiscuous mode\n[628037.729605] IPv6: ADDRCONF(NETDEV_UP): veth3834: link is not ready\n[628037.737120] IPv6: ADDRCONF(NETDEV_CHANGE): veth3834: link becomes ready\n[628037.741944] docker0: port 3(veth3834) entered forwarding state\n[628037.746370] docker0: port 3(veth3834) entered forwarding state\n[628052.768068] docker0: port 3(veth3834) entered forwarding state\n[684719.043883] device veth21bd entered promiscuous mode\n[684719.049382] IPv6: ADDRCONF(NETDEV_UP): veth21bd: link is not ready\n[684719.058117] IPv6: ADDRCONF(NETDEV_CHANGE): veth21bd: link becomes ready\n[684719.062250] docker0: port 4(veth21bd) entered forwarding state\n[684719.066120] docker0: port 4(veth21bd) entered forwarding state\n[684719.100427] docker0: port 4(veth21bd) entered disabled state\n[684719.107517] device veth21bd left promiscuous mode\n[684719.109867] docker0: port 4(veth21bd) entered disabled state\n[684858.707589] Netfilter messages via NETLINK v0.30.\n[684858.710216] device eth0 entered promiscuous mode\n[684906.146435] device eth0 left promiscuous mode\n[685098.401399] device eth0 entered promiscuous mode\n[685114.527857] device eth0 left promiscuous mode\n[685252.427875] device eth0 entered promiscuous mode\n[685264.962632] device eth0 left promiscuous mode\n[686772.549967] device eth0 entered promiscuous mode\n[686784.037602] device eth0 left promiscuous mode\n[687274.354654] device eth0 entered promiscuous mode\n[687277.540947] device eth0 left promiscuous mode\n[687446.522131] device eth0 entered promiscuous mode\n[687537.721096] device eth0 left promiscuous mode\n[691412.256376] device eth0 entered promiscuous mode\n. @eyakubovich  Yep, that's right, though I don't see any calling NeighSet: 10.10.1.1, xx:xx:xx:xx:xx:xx lines.\nNov 25 05:11:47 rabi flanneld[10068]: I1125 05:11:47.892862 10068 vxlan.go:264] L3 miss: 10.10.1.1\nNov 25 05:11:47 rabi flanneld[10068]: I1125 05:11:47.892912 10068 vxlan.go:268] Route for 10.10.1.1 not found\nNov 25 05:11:48 rabi flanneld[10068]: I1125 05:11:48.895367 10068 vxlan.go:264] L3 miss: 10.10.1.1\nNov 25 05:11:48 rabi flanneld[10068]: I1125 05:11:48.895418 10068 vxlan.go:268] Route for 10.10.1.1 not found\nNov 25 05:11:49 rabi flanneld[10068]: I1125 05:11:49.897387 10068 vxlan.go:264] L3 miss: 10.10.1.1\nNov 25 05:11:49 rabi flanneld[10068]: I1125 05:11:49.897435 10068 vxlan.go:268] Route for 10.10.1.1 not found\nThis is what my etcd looks like, if that helps? Is PublicIP supposed to be 0.0.0.0? https://gist.github.com/dennybritz/5ba9d14fd52fad51a700\n. Thanks. I'm not sure if this is related, but my log is full of these messages (on all machines):\nNov 25 05:03:47 rabi flanneld[10068]: W1125 05:03:47.174537 10068 subnet.go:405] Watch of subnet leases failed because etcd index outside history window\nHowever, these were appearing when flannel was working as well.\n. @eyakubovich  Looking at the logs:\njournalctl -u flannel.service | grep 'renewed'\nNov 19 03:42:17 rabi flanneld[2132]: I1119 03:42:17.692125 02132 subnet.go:415] Lease renewed, new expiration: 2014-11-20 03:42:17.690585721 +0000 UTC\nNov 20 02:42:17 rabi flanneld[2132]: I1120 02:42:17.696573 02132 subnet.go:415] Lease renewed, new expiration: 2014-11-21 02:42:17.694406103 +0000 UTC\nNov 21 06:46:59 rabi flanneld[22435]: I1121 06:46:59.382488 22435 subnet.go:442] Lease renewed, new expiration: 2014-11-22 06:46:59.379720337 +0000 UTC\nNov 22 09:39:30 rabi flanneld[28096]: I1122 09:39:30.420100 28096 subnet.go:442] Lease renewed, new expiration: 2014-11-23 09:39:30.419292114 +0000 UTC\nNov 23 09:51:50 rabi flanneld[22225]: I1123 09:51:50.133352 22225 subnet.go:442] Lease renewed, new expiration: 2014-11-24 09:51:50.133835176 +0000 UTC\nNov 24 15:18:17 rabi flanneld[10068]: I1124 15:18:17.699239 10068 subnet.go:442] Lease renewed, new expiration: 2014-11-25 15:18:17.694001804 +0000 UTC\njournalctl -u flannel.service | grep 'Subnet added: 10.10.1.0'\nNov 23 16:18:43 rabi flanneld[10068]: I1123 16:18:43.897713 10068 vxlan.go:190] Subnet added: 10.10.1.0/24\nNov 24 15:18:43 rabi flanneld[10068]: I1124 15:18:43.894272 10068 vxlan.go:190] Subnet added: 10.10.1.0/24\nI don't think my cluster is busy, I've mainly been testing things. However, these are t2.small instances that can't handle much. But looking at CPU/memory utilization everything should be fine.\n. That's great news, thanks! I am trying it.\n. @eyakubovich Has been up for more than 24 hours now so I think this is resolved! \n. Things are back to normal (not base64) if I restart flannel on the machines so my guess is that this is happening when the lease is renewed.\n. ",
    "tobegit3hub": "Maybe flanneld --etcd-endpoints=http://127.0.0.1:2379 should work.. We have tested with calico and it works like a charm.\nCan someone help to verify this and improve flannel for the popular gRPC services?. It may be caused by my configuration. I didn't configure the ip of docker0 and it's different from flannel's subnet.\nI find more documents and configure the flanneld and docker correctly and it works like a charm for all the gRPC services.. ",
    "mindscratch": "I've put together a fix that I'm going to polish and submit as a PR.\n. Glad I could contribute.\n. ",
    "c4milo": "Being eth0 a bridge like docker0 or the actual host network interface? \n. right, l3miss was one of the confusing parts too as I didn't see it being enabled upon device creation: https://github.com/coreos/flannel/blob/master/backend/vxlan/device.go#L40-L53 nor the route short circuit flag. Any special reason? \n. Ah cool, that makes sense. Thanks for helping me understand this much better @eyakubovich. \n. > And it gives flannel.1 an IP so you can route to it.\nSo ip addr add 10.100.12.0/16 dev eth0 should be ip addr add 10.100.12.0/16 dev flannel.1 instead?\n. It was my mistake, I was telling the kernel to create the tunnel using the \"ens33\" device, and it seems network interfaces in EC2 are name differently. (eth*)\n. ",
    "RenoRainz": "Hi guys,\nI have the same issue, where did you do the modification ?\nI put this in my vagrant file but still not working : \n  flanneld:\n    iface: $public_ipv4\nI try to modify this file /usr/lib64/systemd/system/flanneld.service  but /usr is readonly, so I suppose it's not the good way.\n. @eyakubovich thx a lot, it works !\nBy the way do you know if a list of all parameters useable in cloud config exist, and if yes do you know where I can find it ?\n. Tks a lot ! \n. ",
    "robszumski": "@RenoRainz You can find the docs here: https://coreos.com/docs/cluster-management/setup/cloudinit-cloud-config/\n. Yes, the cloud-config on the docs is correct for CoreOS 554+, which is current the alpha and beta channels. I've reproduced it here:\n```\ncloud-config\ncoreos:\n  units:\n    - name: flanneld.service\n      drop-ins:\n        - name: 50-network-config.conf\n          content: |\n            [Service]\n            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ \"Network\": \"10.1.0.0/16\" }'\n      command: start\n```\nIf you intend to use flannel and also configure docker to allow an insecure registry, a recent fix (https://github.com/coreos/bugs/issues/244) just went into the alpha channel\n. Addressed feedback, PTAL\n. You can make this look correct on Github, we now have scripts in place that rewrite links and such.\n. IE remove {{site.baseurl}}/assets/images/media/ from stuff\n. ",
    "vcaputo": "lgtm\n. lgtm, but I'm no iptables guru.  The fact that you felt it was necessary to originally make the masquerading per-backend suggests there's some doubt/murkiness.  /cc @marineam wanna take a look?\n. lgtm\n. lgtm\n. lgtm\n. hah\n. ",
    "mitar": "I can confirm.\n. cc @kostko\n. Any workaround?\n. ",
    "stormltf": "from A ping B - fail\nfrom B ping A - fail\nfrom A & B simultaneously reciprocate ping -SUCCESS! :)\nyes, i also have this problem. my docker cluster build in centos 7.0.1406 in openstack kvm.\n. ",
    "fzu-huang": "when i used \"udp\" backend ,  i  alse met that problem.\nthen  i use \u201cvxlan\u201d backend  and  VNI =1 \n i start two machines \nA:  docker0:192.168.51.1      flannel.1: 192.168.51.0\nB:  docker0:192.168.99.1      flannel.1: 192.168.99.0\nfrom A PING B   fail\nfrom B PING A fail\nfrom A & B simultaneously reciprocate ping  fail     :(\n. ",
    "alemagnani": "I am having the same issue described by @guruvan, is there a solution/explanation for it?\n. ",
    "wb14123": "Hi,\nI've had this problem twice and have both found the problem.\nThe first time is using EC2 and the default security group didn't open UDP ports. After open them, it works.\nThe second time is in our own cluster. The first net interface is bind to public IP and have firewall on it. The second interface is used to communication with other hosts in the cluster. Flanneld uses the first interface's IP by default, so it cannot communicate between hosts. After set the IP to use with -iface/--public-ip, it works.\nThe details of each time is different, but both reasons are hosts cannot communicate on the specify UDP port.\nHope this helps.. ",
    "LuqmanSahaf": "To simplify it, the ping tcpdump shows the same behavior too:\nlistening on docker0, link-type EN10MB (Ethernet), capture size 262144 bytes\n04:50:52.478576 ARP, Request who-has 10.244.41.7 tell coreos-2, length 28\n04:50:52.478631 ARP, Reply 10.244.41.7 is-at 02:42:0a:f4:29:07 (oui Unknown), length 28\n04:50:52.478638 IP 10.244.73.0 > 10.244.41.7: ICMP echo request, id 1792, seq 1, length 64\n04:50:52.478680 IP 10.244.41.7 > 10.244.73.0: ICMP echo reply, id 1792, seq 1, length 64\n04:50:53.478739 IP 10.244.73.0 > 10.244.41.7: ICMP echo request, id 1792, seq 2, length 64\n04:50:57.487619 ARP, Request who-has coreos-2 tell 10.244.41.7, length 28\n04:50:57.487713 ARP, Reply coreos-2 is-at 56:84:7a:fe:97:99 (oui Unknown), length 28\n^C\n12 packets captured\n12 packets received by filter\n0 packets dropped by kernel\nThis is a tcpdump of ping from one container to other on different hosts.\n. ",
    "ilterpehlivan": "Hi, \nI am also facing this issue and cannot ping from my Pod in HostA to my DNS service running in HosB\nThe google forum link that is posted above is not reachable, could you please give the new Url for the answer ?\nThanks. ",
    "euank": "The message id remains the same, so the updated link is https://groups.google.com/d/topic/kubernetes-users/P4uh7y383oo. Needs rebase. LGTM. The flannel-docker-opts.service unit contains the line ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/lib/coreos/flannel-wrapper2.uuid.\nThe - at the beginning of that means that an error will not cause the unit to fail and will be effectively ignored.\nIt's expected that that message is printed the first time the unit is run and it should be harmless.\nI think flannel-docker-opts is having trouble coming up for some other reason.\nCan you give more details about the error you're seeing and the other logs flanneld and flannel-docker-opts print?. The error it prints is a timeout, and the logs show it did proceed to the rkt run step, so the rkt rm error shouldn't matter (and is expected).\nTry increasing TimeoutStartSec=60 to a larger number, or removing the line altogether (default is infinity for oneshot services).. Those two ranges should not overlap.\nService ips in a K8s cluster are \"fake\" ips which are redirected by kube-proxy to real ips, irrespective of the pod network. Kube-proxy will make rules that redirect the service-ips to pod-ips, and those pod-ips are what flannel should understand.. @tomdee I believe it ended up in tools/cache in client-go 3.\nSee https://github.com/kubernetes/client-go/tree/release-3.0/tools/cache\nMoving to client-go rather than using kubernetes/pkg directly should be the easiest update path IMO.. A crazier option would be shipping different binaries for k8s vs etcd backed flanneld.. The diff size is the difference between glide install and glide install --strip-vendor I think.\nThere should probably be a makefile target or script so it's clear that flag's typically being used.. @ganlaksh can you try with flannel version 0.7.1? I think https://github.com/coreos/flannel/pull/576 might be relevant, though I'm not sure.. > type *http.Request has no field or method WithContext\nThis method was introduced in go1.7. It seems likely you're using an old, unsupported, version of go.. I think the reason it parsed them correctly when run by hand the shell ends up evaluating/stripping the quotes.\nThat is, the equivalent ExecStart line would have been ....   -etcd-endpoints=https://10.132.57.158:2379 ... (without double quotes).\nsystemd's quote-handling has also changed since 219 and on a newer systemd I think the original unit file probably would have worked because it also would have stripped the \".\nI assume double-vs-single dashes making a difference is some detail of the go libraries used to parse these flags\nRegardless, glad you figured out something that works! In general stripping ExecStart quotes on an older systemd is a good idea.. Is there a reason to use master over a tagged version, e.g. 3.0?. Just setting version to release-3.0 and then running glide install -v should work I think. ",
    "titanous": "\nThere was a request for the overlay network to stay up during flannel daemon downtime (crash/restart, upgrade). At the same time, I was worried about keeping the kernel data structures (arp, fdb) out of date if flannel is not running for a prolonged period of time. I'm always a bit scared with things getting out of whack when there're mappings involved.\n\nThis will actually work better in that case, as they are permanent arp/fdb entries and will not get stale. Also the entire state of the network as flannel knows it will be there instead of just the state that was discovered via misses.\n\nBut this patch won't remove the ones that may have disappeared. I guess this could be trivially fixed with flushing the fdb and arp tables on startup -- I think it won't cause a hiccup for exiting flows.\n\nActually, as far as I can tell, something (I haven't tracked down what it is) causes all of the relevant entries to be flushed when restarting flannel already. My understanding is that because each of the route, arp, and fdb entries are tied to the interface they get dropped when the interface is deleted (and perhaps modified in specific ways?).\nI also have a patch kicking around that does a diff and cleans up the tables if we avoid the flushing somehow.\n\nIn case of large number of hosts, many entries in the routing table (one per host) even if no traffic is ever exchanged between these hosts. Probably not a huge deal.\n\nNo idea what the scalability that is expected is, but I think even a few thousand entries shouldn't be a big deal.\n\nThere was a request (and they continue to show up) to make VXLAN work at L2\n\nI'll admit that I don't fully understand the considerations made for this use case. In the extreme scenario, two backend variants (one static, and one dynamic) could be provided.\n. > Currently, since the entries are not permanent, they get aged-out, but it'd be interesting to figure out why they get removed. At least to rule out the possibility that they may get removed with flannel running.\nYes, I will investigate. Just for clarification, the flush appears to happen during startup of flannel, as well as if/when the interface is manually deleted.\n\nAlso, any particular reason why you favor static config? Is it purely to make things simpler (which I definitely favor as well) or have you encountered performance or other problems with misses?\n\nI prototyped this implementation before flannel existed so I'm comfortable with it. I was also unable to get the miss implementation working and didn't want to troubleshoot it.\n. Go for it. We've been using this in Flynn since I opened the PR with no trouble.\n. ",
    "gzoller": "Indeed!  Upgraded to go 1.4 and it built fine.  Thanks!\nOn Fri, Jan 9, 2015 at 2:54 PM, Eugene Yakubovich notifications@github.com\nwrote:\n\n@gzoller https://github.com/gzoller It sounds like you have an older\nversion of Go. Can you check and report via go version. We're using Go\n1.3 (I'm on 1.3.3).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/109#issuecomment-69397309.\n. \n",
    "sidharta-s": "Another thing : I started with VXLAN first and saw the connect issue there too. nping wouldn't work either (tcpdump on flannel.1 would show ARP requests going out, but not responses) - so I switched to using the UDP backend. \n. @eyakubovich thank you. I am able to ping other IPs (just not the ones routed through flannel0). Let me know if you need any other information from me. In case it matters, these VMs are openstack instances. \nlspci | egrep -i 'network|ethernet'\n00:03.0 Ethernet controller: Red Hat, Inc Virtio network device\n. @eyakubovich Pinging the local IP works.  (but that is routed through docker0 as per the routing table above?)\n. @eyakubovich About VXLAN - it would be useful to modify README.md to indicate kernel requirements for various flannel backends. One of my reasons for picking flannel for CentOS 6.6 was that it didn't seem to require a recent kernel! \n. @eyakubovich any connect() calls through flannel0 (including -I) failed.  As a last resort, I destroyed this cluster I created an all new openstack cluster running CentOS 6.6/flannel and docker. ping across the cluster appears to succeed now. I am not sure what caused my first cluster to end up in this state. I'll close this issue for now. Thanks for looking into this!\n. ",
    "kjvalencik": "9Can anyone comment on whether VXLAN backend works or not in CentOS 6.6 (2.6.32 kernel)? Thanks!\nEdit: Figured it out. I needed to install the centos.plus version of the kernel to get the DOVE extensions.\n. ",
    "smakam": "Forgot to provide info on my environment:\nI am running coreos as a vagrant cluster on Windows machine with Virtualbox. \n. @eyakubovich \nThanks for your response. It worked after I added the following option(-iface=eth1) to flannel start. Looks like this is specific to Vagrant environment. \n1 unrelated question:\nIs flannel currently available only for CoreOS?\nThanks\nSreenivas\n. ",
    "timechanter": "I am running a small 3 node 557.1.0 cluster using flannel and have notice on many occasions restarting a host that the docker daemon will start running before flannel has completed its setup. This causes the docker0 interface to be the \"normal\" 172.x network instead of the desired 10.254.x network.\nLooking at the journal I see etcd being delayed joining the cluster and flannel is thus delayed as it attempts to talk to etcd. Docker then starts up without waiting for flannel to be ready. The kibernetes boys use an etcd.waiter unit that their flannel depends on. Short of overriding the units for flannel and docker via cloud-init (im on AWS) is there anything I can do to avoid the race condition?\n. In addition, restarting docker.service fails as it then discovers that the bridge it expects (10.254.x) is not the current docker0 bridge (172.x) . By deleting the docker0 interface first I can then restart docker and get the correct network configuration. Of course this kills all fleets on the node hard.\n. @eyakubovich That worked an absolute treat. The following is what I added after flanneld.service for the interested.\n- name: docker.service\n      drop-ins:\n        - name: 60-docker-wait-for-flannel-config.conf\n          content: |\n            [Unit]\n            After=flanneld.service\n            Requires=flanneld.service\n            Restart=always\n            Restart=on-failure\n      command: start\n. ",
    "jpmx": "Hi @eyakubovich - same result with /24\n64 bytes from 10.1.61.2: icmp_seq=383 ttl=62 time=0.298 ms\n64 bytes from 10.1.61.2: icmp_seq=383 ttl=62 time=2.24 ms (DUP!)\n64 bytes from 10.1.61.2: icmp_seq=384 ttl=62 time=0.275 ms\n64 bytes from 10.1.61.2: icmp_seq=384 ttl=62 time=2.41 ms (DUP!)\n64 bytes from 10.1.61.2: icmp_seq=385 ttl=62 time=0.284 ms\n64 bytes from 10.1.61.2: icmp_seq=385 ttl=62 time=2.19 ms (DUP!)\n64 bytes from 10.1.61.2: icmp_seq=386 ttl=62 time=0.292 ms\nThe containers are in different hosts connected in the same LAN via a bonded interface (bond1)\n[root@coreos1 ~]# ip link show\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP mode DEFAULT qlen 1000\n    link/ether 00:25:90:ad:3d:f5 brd ff:ff:ff:ff:ff:ff\n3: eth1: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond0 state UP mode DEFAULT qlen 1000\n    link/ether 00:25:90:ad:3d:f5 brd ff:ff:ff:ff:ff:ff\n4: eth2: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond1 state UP mode DEFAULT qlen 1000\n    link/ether 00:25:90:ad:3d:f7 brd ff:ff:ff:ff:ff:ff\n5: eth3: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc mq master bond1 state UP mode DEFAULT qlen 1000\n    link/ether 00:25:90:ad:3d:f7 brd ff:ff:ff:ff:ff:ff\n8: bond0: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT\n    link/ether 00:25:90:ad:3d:f5 brd ff:ff:ff:ff:ff:ff\n9: bond1: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT\n    link/ether 00:25:90:ad:3d:f7 brd ff:ff:ff:ff:ff:ff\n10: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT\n    link/ether 96:04:30:fa:3c:af brd ff:ff:ff:ff:ff:ff\n11: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP mode DEFAULT\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n13: veth09cce8a: <BROADCAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master docker0 state UP mode DEFAULT\n    link/ether ae:8d:37:3f:eb:a6 brd ff:ff:ff:ff:ff:ff\n. I disabled the bond1 interface on both hosts and the duplicate packages still appears\n\n. @eyakubovich - I have tested vxlan and udp modes with this setups (on two metal severs):\npublic: bond0 = eth0 + eth1\nprivate: eth2 (bond1 and eth3 disabled)\nand\npublic: bond0 = eth0 + eth1\nprivate: bond1 = eth2 + eth3\nTested host-to-host no dups, container-to-container dups found, flannel.1-to-flannel.1 dups found\n\n. @eyakubovich I had the default flannel settings and when using interface: bond1 the DUPs between containers, flannel interface and private IP disappears but I keep seeing DUPs on public interface (bond0)\nwith default flannel settings (see windows titles for details)\n\nwith interface: bond1 on flannel settings (see windows titles for details)\n\n. @eyakubovich as you suggested, my ISP just confirmed to me that this is a switch-specific problem and they are investigating with the manufacturer\nThanks for your help!\n. ",
    "WIZARD-CXY": "@eyakubovich No, I am using flannel in my lab ,a private network. The host that docker containers on can ping 8.8.8.8.\nThe output of your suggested command is\nChain POSTROUTING (policy ACCEPT)\ntarget     prot opt source               destination\nMASQUERADE  all  --  10.0.52.0/24         0.0.0.0/0\nMASQUERADE  all  --  10.1.0.0/16          0.0.0.0/0\nMASQUERADE  all  --  10.1.0.0/16          0.0.0.0/0\nMASQUERADE  all  --  10.2.0.0/16          0.0.0.0/0\nbut the flannel0 has ip 10.0.20.0, is it the source of error?\nAnd after I run command sudo iptables -t nat -A POSTROUTING -o flannel0 -j MASQUERADE or sudo iptables -t nat -A POSTROUTING -o docker0 -j MASQUERADE .The problem also exists.\n. The configuration passed to docker daemon is \nDOCKER_OPTS=\"--insecure-registry 10.10.103.215:5000 -H tcp://127.0.0.1:4243 -H unix:///var/run/docker.sock --bip=10.0.20.1/24 --mtu=1472\"\ndocker version is  1.3.2 which is not very old.\n. @eyakubovich  It is the masquerade error, I reflush the nat table with iptables -t nat -F and restart docker and the container can access the outside world.\nI think maybe it is because docker may not update the iptables DOCKER chain detail until docker find there is no existence of this chain .\nThank you \n. ",
    "alban": "@eyakubovich : yes, I get the warnings I mentioned in the commit message:\nMaybe the script should take the dynamic libraries and iptables from another base image rather than to ship whatever is on the build machine...\n. Update on the dependencies:\n- actool patch-manifest ppc/spec#258 is merged :)\n- docker2aci on local files appc/docker2aci#32 is waiting for review on https://github.com/appc/docker2aci/pull/37\n. Patch updated.\n. Branch updated.\n. You're right... I meant to save the .docker file in $tgt.\nI'm fixing this.\n. ",
    "djannot": "Thanks Eugene\nI using the one from Google.\nI'll build the one in github.\nWhy it's not part of the CoreOS binaries by default ?\nOn Jan 26, 2015 7:31 PM, \"Eugene Yakubovich\" notifications@github.com\nwrote:\n\n@djannot https://github.com/djannot The reason is that your IP is\ngetting masqueraded by a rule that Docker has installed. If you're running\nDocker 1.3+, pass --ip-masq=false to docker daemon and --ip-masq to\nflanneld. This will tell Docker to not install IP Masquerading rules and\nflannel will install \"proper\" ones instead.\nIf you're using flannel with CoreOS that shipped it, it should happen\nautomatically.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/117#issuecomment-71511461.\n. This works perfectly. Thanks again\nOn Jan 26, 2015 9:52 PM, \"Denis Jannot\" djannot@gmail.com wrote:\nThanks Eugene\nI using the one from Google.\nI'll build the one in github.\nWhy it's not part of the CoreOS binaries by default ?\nOn Jan 26, 2015 7:31 PM, \"Eugene Yakubovich\" notifications@github.com\nwrote:\n\n@djannot https://github.com/djannot The reason is that your IP is\ngetting masqueraded by a rule that Docker has installed. If you're running\nDocker 1.3+, pass --ip-masq=false to docker daemon and --ip-masq to\nflanneld. This will tell Docker to not install IP Masquerading rules and\nflannel will install \"proper\" ones instead.\nIf you're using flannel with CoreOS that shipped it, it should happen\nautomatically.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/117#issuecomment-71511461.\n. \n\n",
    "spontaneous01": "I found #115 is the solution.\n$ (close docker)\n$ iptables -t nat -F\n$ ip link del docker0\n$ (start docker)\nSorry for the duplicated issue. Issue closed !\n. ",
    "msumme": "@eyakubovich Thanks for the quick reply / fix.\n. ",
    "bytesandwich": "Thanks for the quick response :) I was on stable so that was the problem :P\n. ",
    "hug-abacus": "I have the same issue and it often results in losing connectivity between the hosts. I need to restart flanneld to restore connectivity again.\n. The problem I experienced was that the subnet entries (/coreos.com/network/subnets/XXXX-24) were overwritten with an empty value for some reason which then caused the connectivity issues.\nThis happened mostly when there was too much load on the etcd machines which caused timeouts and leader elections.\n. ",
    "lemenkov": "I personally sometimes see this message (\"Watch of subnet leases failed because etcd index outside history window\") as well. But so far it never resulted in a connectivity issues. I'm using \"UDP\" backend if it matters.\n. @eyakubovich yes, the current version (f3c0734881ce62f0a8eabba57ba7ea6da3f18479) still leaks sockets. It opens ~1 new socket per 5 minutes exacly as requested here:\nhttps://github.com/coreos/flannel/blob/f3c0734881ce62f0a8eabba57ba7ea6da3f18479/subnet/registry.go#L128\nThere is definitely an issue somewhere within etcd because it should reply faster, but still flannel must handle it properly, without eating all the available sockets.\nRegarding golang - not sure if it was fixed completely (I'm using 1.3.3) but it seems that flannel works now.\n. @lvlv like your approach! It's much better than mine so I'm going to close this PR in favour or yours one.\nEveryone please go to #144.\n. @eyakubovich this looks much better than my previous attempt.\n. Subscribing.\n. @greenpau what exactly are you trying to achieve? It seems that you're trying to run something like Avahi on top of the flannel. Is that correct?\nJust for those who might be interested - in some topology cases, it's possible to broadcast Avahi changes across the hosts w/o adjusting flannel. Just use proper network topology.\n. ",
    "rimusz": "I'm getting the same error on v593.0.0 too\n. ",
    "cescoferraro": "https://github.com/pegerto/flannelctl\n. i missed that on the repo readme\nsource /run/flannel/subnet.env\ndocker -d --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}\nthat actually should be\nsource /run/flannel/subnet.env\ndocker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU}\n. ",
    "psi-4ward": "Nice! But theres one more question. \nThe ca, cert and key files resists in a directory on the host. As far as i can see flannel gets started using the quay.io/coreos/flannel container. Cant imagine how the container can access the files? Currently i inject the folder as volume but i dont see a way how the coreos builtin flannel service can do that using the systemd drop in.\n. ",
    "sramak1396c": "I don't see requests going out on flannel.1 or eth0.\nMore findings.\ncontainer ping works between minions\nmaster to container ping not working\nIn minions, there veth interface attached to docker0 , which is missing in master. Not sure if that's a cause.\nminion :  ip addr output\nkube-slave-b54f5624-9640-4546-bd91-48f6ab459abc core # ip addr\n1: lo:  mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0:  mtu 1500 qdisc fq_codel state UP qlen 1000\n    link/ether fa:16:3e:1a:cc:f5 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.7/24 brd 192.168.1.255 scope global dynamic eth0\n       valid_lft 83sec preferred_lft 83sec\n    inet6 fe80::f816:3eff:fe1a:ccf5/64 scope link\n       valid_lft forever preferred_lft forever\n3: eth1:  mtu 1500 qdisc fq_codel state UP qlen 1000\n    link/ether fa:16:3e:f1:15:96 brd ff:ff:ff:ff:ff:ff\n    inet .../23 brd 96.119.145.255 scope global dynamic eth1\n       valid_lft 110sec preferred_lft 110sec\n    inet6 2001:558:fc0e:0:f816:3eff:fef1:1596/64 scope global dynamic\n       valid_lft 2591883sec preferred_lft 604683sec\n    inet6 fe80::f816:3eff:fef1:1596/64 scope link\n       valid_lft forever preferred_lft forever\n4: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN\n    link/ether ca:28:0c:34:26:e2 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.45.0/16 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::c828:cff:fe34:26e2/64 scope link\n       valid_lft forever preferred_lft forever\n5: docker0:  mtu 1450 qdisc noqueue state UP\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.45.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link\n       valid_lft forever preferred_lft forever\n9: veth6365233:  mtu 1450 qdisc noqueue master docker0 state UP\n    link/ether be:9f:1f:91:c3:22 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::bc9f:1fff:fe91:c322/64 scope link\n       valid_lft forever preferred_lft forever\nmaster: ip addr output\n\noutput of ip addr\nkube-master kubernetes # ip addr\n1: lo:  mtu 65536 qdisc noqueue state UNKNOWN group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0:  mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether fa:16:3e:1c:fe:e9 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.2/24 brd 192.168.1.255 scope global dynamic eth0\n       valid_lft 74sec preferred_lft 74sec\n    inet6 fe80::f816:3eff:fe1c:fee9/64 scope link \n       valid_lft forever preferred_lft forever\n3: eth1:  mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether fa:16:3e:29:97:36 brd ff:ff:ff:ff:ff:ff\n    inet .../23 brd 96.119.145.255 scope global dynamic eth1\n       valid_lft 76sec preferred_lft 76sec\n    inet6 2001:558:fc0e:0:f816:3eff:fe29:9736/64 scope global mngtmpaddr dynamic \n       valid_lft 2591847sec preferred_lft 604647sec\n    inet6 fe80::f816:3eff:fe29:9736/64 scope link \n       valid_lft forever preferred_lft forever\n4: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN group default \n    link/ether 8e:61:72:24:5d:9a brd ff:ff:ff:ff:ff:ff\n    inet 10.244.78.0/16 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::8c61:72ff:fe24:5d9a/64 scope link \n       valid_lft forever preferred_lft forever\nkube-master kubernetes # ps -ef|grep -i flannel\nroot       639     1  0 20:58 ?        00:00:00 /opt/bin/flanneld --iface=eth0\nroot      1315   718  0 21:01 pts/0    00:00:00 grep --colour=auto -i flannel\n\n. Sorry , accidently closed this issue. \nThanks\n. Ping output\nkube-master kubernetes # ping 10.244.45.3\nPING 10.244.45.3 (10.244.45.3) 56(84) bytes of data.\nFrom 10.244.78.0: icmp_seq=1 Destination Host Unreachable\nFrom 10.244.78.0: icmp_seq=2 Destination Host Unreachable\nFrom 10.244.78.0: icmp_seq=3 Destination Host Unreachable\nFrom 10.244.78.0: icmp_seq=4 Destination Host Unreachable\nFrom 10.244.78.0: icmp_seq=5 Destination Host Unreachable\nFrom 10.244.78.0: icmp_seq=6 Destination Host Unreachable\nFrom 10.244.78.0: icmp_seq=7 Destination Host Unreachable\n. kube-master kubernetes # toolbox\nSpawning container root-fedora-latest on /var/lib/toolbox/root-fedora-latest.\nPress ^] three times within 1s to kill container.\n-bash-4.3# tcpdump -i flannel.1 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes\nJust hanging , no packets there\n. Worked initially , when I built the cluster. Then it just stopped working.\nLet me rebuilt the cluster and see if I can reproduce again.\nThanks\n. Default route 0.0.0.0 on private interface of 192.168.1.0/24 , seems to be the culprit. I rebuilt the cluster , and it's running ok now. \nThe issue seems to be specific to coreos and openstack.  I added changes to the cloud-config to remove the default route for private network.\nI am going to look into better way to handle in CoreOS , but  workaround on cloud-config fixed it for me now.\nThanks\nSriram\nChanges made to master.yaml\n\nname: remove-private-network.service\n    command: start\n    content: |\n      [Unit]\n      Description=Remove default Private routing\n      Requires=network-online.target\n      After=network-online.target\n\n[Service]\n  ExecStart=/usr/bin/route del -net 0.0.0.0 gateway 192.168.1.1\n  RemainAfterExit=yes\n  Type=oneshot\n  - name: setup-network-environment.service\n    command: start\n    content: |\n      [Unit]\n      Description=Setup Network Environment\n      Documentation=https://github.com/kelseyhightower/setup-network-environment\n      Requires=remove-private-network.service\n      After=remove-private-network.service\n  [Service]\n  ExecStartPre=-/usr/bin/mkdir -p /opt/bin\n  ExecStartPre=/usr/bin/wget -N -P /opt/bin https://storage.googleapis.com/k8s/setup-network-environment \n  ExecStartPre=/usr/bin/chmod +x /opt/bin/setup-network-environment\n  ExecStart=/opt/bin/setup-network-environment\n  RemainAfterExit=yes\n  Type=oneshot\n\n. ",
    "lvlv": "+1 for this PR\nI'm also experiencing this issue, my etcd server is v2.0.0-rc1 and my client is built from master. I have flannel running on Ubuntu 14.10 and CentOS 7, all of them leak sockets. Let me know how can I help to investigate this more.\nmy 2 cents: even if etcd (server) closes the connection with zero length messages (by bug), the client should not leak sockets. Thanks for your further help in advance!\n$ netstat -alpn | grep flannel\ntcp        0      0 172.16.3.242:49143      172.16.1.42:4001        ESTABLISHED 1308/flannel\ntcp        0      0 172.16.3.242:48031      172.16.1.42:4001        ESTABLISHED 1308/flannel\ntcp        0      0 172.16.3.242:48552      172.16.1.42:4001        ESTABLISHED 1308/flannel\ntcp        0      0 172.16.3.242:47476      172.16.1.42:4001        ESTABLISHED 1308/flannel\ntcp        0      0 172.16.3.242:50595      172.16.1.42:4001        ESTABLISHED 1308/flannel\ntcp        0      0 172.16.3.242:49920      172.16.1.42:4001        ESTABLISHED 1308/flannel\n. My previous experiments show that the sockets are created exactly every 5\nminutes. I'll get more detailed logs from strace if you prefer.\nOn Sat, Mar 28, 2015 at 2:00 AM, Eugene Yakubovich <notifications@github.com\n\nwrote:\n@lvlv https://github.com/lvlv I completely agree that we shouldn't leak\nsockets. It's just tracking it down is hard. AFAIK, etcd started setting\nbetter timeouts in etcd v2.0.3 so I would try a newer version first. If\npossible can you run flanneld under strace: strace -t -ff -e\ntrace=socket,close bin/flanneld. This should show how often the sockets\nget created and closed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/135#issuecomment-87035021.\n. Attached is my logs after run flannel for 10 minutes, also netstat shows 3\nsockets are opened.\n(I added some more debug info in logs inside\ncoreos/go-etcd/etcd/requests.go to see if Body.close is always called)\n\n$ sudo netstat -alpn | grep flannel\ntcp        0      0 127.0.0.1:50290         127.0.0.1:4001\n ESTABLISHED 28592/flannel\ntcp        0      0 172.16.3.242:51877      172.16.1.42:4001\n ESTABLISHED 11792/flanneld\ntcp        0      0 127.0.0.1:50266         127.0.0.1:4001\n ESTABLISHED 28592/flannel\ntcp        0      0 172.16.3.242:39578      172.16.1.42:4001\n ESTABLISHED 11792/flanneld\ntcp        0      0 127.0.0.1:50274         127.0.0.1:4001\n ESTABLISHED 28592/flannel\nOn Sat, Mar 28, 2015 at 10:27 PM, Lv Lv wshwawa@gmail.com wrote:\n\nMy previous experiments show that the sockets are created exactly every 5\nminutes. I'll get more detailed logs from strace if you prefer.\nOn Sat, Mar 28, 2015 at 2:00 AM, Eugene Yakubovich \nnotifications@github.com wrote:\n\n@lvlv https://github.com/lvlv I completely agree that we shouldn't\nleak sockets. It's just tracking it down is hard. AFAIK, etcd started\nsetting better timeouts in etcd v2.0.3 so I would try a newer version\nfirst. If possible can you run flanneld under strace: strace -t -ff -e\ntrace=socket,close bin/flanneld. This should show how often the sockets\nget created and closed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/135#issuecomment-87035021.\n. I just sent out PR#144 https://github.com/coreos/flannel/pull/144, it\ndemonstrate the place where connections are leaked.\n\n\nOn Sat, Mar 28, 2015 at 10:52 PM, Lv Lv wshwawa@gmail.com wrote:\n\nAttached is my logs after run flannel for 10 minutes, also netstat shows 3\nsockets are opened.\n(I added some more debug info in logs inside\ncoreos/go-etcd/etcd/requests.go to see if Body.close is always called)\n$ sudo netstat -alpn | grep flannel\ntcp        0      0 127.0.0.1:50290         127.0.0.1:4001\n ESTABLISHED 28592/flannel\ntcp        0      0 172.16.3.242:51877      172.16.1.42:4001\n ESTABLISHED 11792/flanneld\ntcp        0      0 127.0.0.1:50266         127.0.0.1:4001\n ESTABLISHED 28592/flannel\ntcp        0      0 172.16.3.242:39578      172.16.1.42:4001\n ESTABLISHED 11792/flanneld\ntcp        0      0 127.0.0.1:50274         127.0.0.1:4001\n ESTABLISHED 28592/flannel\nOn Sat, Mar 28, 2015 at 10:27 PM, Lv Lv wshwawa@gmail.com wrote:\n\nMy previous experiments show that the sockets are created exactly every 5\nminutes. I'll get more detailed logs from strace if you prefer.\nOn Sat, Mar 28, 2015 at 2:00 AM, Eugene Yakubovich \nnotifications@github.com wrote:\n\n@lvlv https://github.com/lvlv I completely agree that we shouldn't\nleak sockets. It's just tracking it down is hard. AFAIK, etcd started\nsetting better timeouts in etcd v2.0.3 so I would try a newer version\nfirst. If possible can you run flanneld under strace: strace -t -ff -e\ntrace=socket,close bin/flanneld. This should show how often the sockets\nget created and closed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/135#issuecomment-87035021.\n. Sure, created https://github.com/coreos/go-etcd/pull/195\n. My branch was deleted by accident, I created a new one\nhttps://github.com/coreos/flannel/pull/145 with the go-etcd dependency\nupdate and the one line fix. Sorry for the inconvenience.\n\n\n\nOn Wed, Apr 1, 2015 at 2:25 PM, jay vyas notifications@github.com wrote:\n\nRelated, i recently found i had to increase ulimit very high when making\nlots of ETCD client connections in a stress test using go/etcd.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/pull/144#issuecomment-88367242.\n. \n",
    "linvjw": "Eric Paris had objections to using network-online.target.  Apparently its use is discouraged by the systemd folks.\n. Honestly, I'd rather use network-online.target as well...\n. That said, I don't really see the harm in retrying once per second until the devices comes up.\n. I'm not sure of all the possible error returns.  I'll research it...\nAs for printing the error, it would be easy to add something like this:\n--- a/backend/vxlan/vxlan.go\n+++ b/backend/vxlan/vxlan.go\n@@ -79,6 +79,7 @@ func (vb _VXLANBackend) Init(extIface *net.Interface, extIP net.IP) (_backend.Su\n                if err == nil {\n                        break\n                } else {\n- ```\n                   log.Warning(err)\n                  log.Warning(\"Failed to create VXLAN interface, retrying...\")\n              // wait 1 sec before retrying\n\n```\nI'm open to suggestions on the actual log message wording, of course.  The err value comes from the ensureLink function in backend/vxlan/device.go and already has fairly verbose wording.\n. Clearly I don't know the magic screen formatting bits...the \"log.Warning(err)\" part would be the addition...\n. Huzzah...\n```\n--- a/backend/vxlan/vxlan.go\n+++ b/backend/vxlan/vxlan.go\n@@ -79,6 +79,7 @@ func (vb VXLANBackend) Init(extIface net.Interface, extIP net.IP) (*backend.Su\n                if err == nil {\n                        break\n                } else {\n+                       log.Warning(err)\n                        log.Warning(\"Failed to create VXLAN interface, retrying...\")\n                    // wait 1 sec before retrying\n\n```\n. Sure, I just pushed a patch with the wording as you suggested.\n. ",
    "cgwalters": "I think network-online.target is better than a polling loop personally.\nSo for the real fix -the reason the VXLAN device creation is failing is because the specified external interface isn't up, right?  We could watch that via netlink, or alternatively via DBus to NetworkManager/systemd-networkd.\n(I have zero patches in flannel, so feel free to ignore me as well)\n. ",
    "AntonioMeireles": "oh, ok [walked the code too fast :smile:], got the picture. thanks! closing. \n. ",
    "PierreKircher": "skydns ? + registrator .. ( progrium ) storage is etcd .. that works quite well here \n. ",
    "cdpb": "@PierreKircher how can I define a static ip within the flannel layer for skydns, because that is important for the dns. I don't want to bind the service on a single node ( see #167 )\n. @eyakubovich thank you, should work :)\nJust a idea: \nflannel watches etcd for changes, if a subnet gets provide my another machine, the routing set accordingly to etcd - so far so good. \nSo e.g. I have a 10.0.1.0/24 registered to node1 till startup ( flannel overall network /8 )\nA Container starts with: \nExecPreStart=/usr/bin/etcdctl set /coreos.com/config/subnets/10.0.100-32 {\"PublicIP\": \"$(ip addr | grep default | awk '{print $3}') }\nExecStart=...docker run -p 10.0.100:53:53 ....\nExecStop=/usr/bin/etcdctl rm /coreos.com/config/subnets/10.0.100-32\nSo routing is done, the only missing part is a registered virtual ip to docker bridge, it should be really easy to realize.\n. ",
    "siteshen": "bash\nwhile true; do\n    container_name=\"db-slave-1\"\n    ip=$(docker inspect --format \"{{ .NetworkSettings.IPAddress }}\" $container_name)\n    check_cmd=$(echo \"PGPASSWORD=123456 psql -h $ip db_name username  -c \\\\\"select max(id) from things\\\\\" \")\n    docker exec $container_name bash -c \"$check_cmd\"\n    if [ $? -eq 0 ]; then\n        etcdctl set /servics/db-slave/1 $ip:5432 -ttl 30\n    else\n        etcdctl rm  /servics/db-slave/1\n    fi\n    sleep 20\ndone\n. ",
    "DorianGray": "Oh, openvpn is 10.8.0.0/16, main network is 10.0.0.0/16, flannel is 10.200.0.0/16.\n. Flannel is running on top of the main network. @packetcollision is working on this with me and has more information, he will respond in a bit to the other questions.\n. It seems like this could be solve by being able to set a static route in flannel...is that something that we can do? If we could set a route for 10.8.0.0/16 in flannel and point it at our openvpn server that would solve the issue, right?\n. ",
    "dnelson": "Sorry it took so long to respond.\nPinging from my mac, which is connected to the VPN on the gateway, I see ICMP packets coming in on the tun0 adapter and being forwarded to flannel0\nubuntu@gate:~$ sudo tcpdump -nni any icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked), capture size 65535 bytes\n07:41:26.894521 IP 10.8.0.8 > 10.200.35.8: ICMP echo request, id 50715, seq 0, length 64\n07:41:26.894545 IP 10.8.0.8 > 10.200.35.8: ICMP echo request, id 50715, seq 0, length 64\n07:41:27.896322 IP 10.8.0.8 > 10.200.35.8: ICMP echo request, id 50715, seq 1, length 64\n07:41:27.896349 IP 10.8.0.8 > 10.200.35.8: ICMP echo request, id 50715, seq 1, length 64\n07:41:28.902880 IP 10.8.0.8 > 10.200.35.8: ICMP echo request, id 50715, seq 2, length 64\n07:41:28.902894 IP 10.8.0.8 > 10.200.35.8: ICMP echo request, id 50715, seq 2, length 64\nHowever on the coreos machine that is running the target container, I see no ICMP packets.\nIf I ping from one of the other coreos machines, I do see the ICMP packets in tcpdump, and I get responses:\n-bash-4.3# tcpdump -nni any icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes\n07:46:53.071024 IP 10.200.1.0 > 10.200.35.8: ICMP echo request, id 31162, seq 1, length 64\n07:46:53.071028 IP 10.200.1.0 > 10.200.35.8: ICMP echo request, id 31162, seq 1, length 64\n07:46:53.071044 IP 10.200.35.8 > 10.200.1.0: ICMP echo reply, id 31162, seq 1, length 64\n07:46:53.071044 IP 10.200.35.8 > 10.200.1.0: ICMP echo reply, id 31162, seq 1, length 64\n07:46:53.071050 IP 10.200.35.8 > 10.200.1.0: ICMP echo reply, id 31162, seq 1, length 64\n07:46:54.072329 IP 10.200.1.0 > 10.200.35.8: ICMP echo request, id 31162, seq 2, length 64\n07:46:54.072343 IP 10.200.1.0 > 10.200.35.8: ICMP echo request, id 31162, seq 2, length 64\n07:46:54.072347 IP 10.200.1.0 > 10.200.35.8: ICMP echo request, id 31162, seq 2, length 6\nip route of the target coreos machine is:\ncore@ip-10-0-12-219 ~ $ ip route\ndefault via 10.0.12.1 dev eth0  proto dhcp  src 10.0.12.219  metric 1024\n10.0.12.0/22 dev eth0  proto kernel  scope link  src 10.0.12.219\n10.0.12.1 dev eth0  proto dhcp  scope link  src 10.0.12.219  metric 1024\n10.200.0.0/16 dev flannel0  proto kernel  scope link  src 10.200.35.0\n10.200.35.0/24 dev docker0  proto kernel  scope link  src 10.200.35.1\nip route of the gate machine is:\nubuntu@gate:~$ ip route\ndefault via 10.0.4.1 dev eth0\n10.0.4.0/26 dev eth0  proto kernel  scope link  src 10.0.4.34\n10.8.0.0/24 dev tun0  proto kernel  scope link  src 10.8.0.1\n10.200.0.0/16 dev flannel0  proto kernel  scope link  src 10.200.29.0\n10.200.29.0/24 dev docker0  proto kernel  scope link  src 10.200.29.1\nPlease let me know if any other info would be helpful, or if there's anything we should try.\n. ",
    "0x4139": "we solved it using a static route in a docker container, you can see the project here https://registry.hub.docker.com/u/0x4139/openvpn/\n. If you look at the script that starts in the container we have the same route :D. ",
    "nqbao": "FYI: i managed to make it work over the VPN by adding a new IPTables route to route all traffic from VPN to the right gateway\niptables -t nat -I POSTROUTING -s 10.8.0.0/24 -d 10.233.64.0/18  -j SNAT --to-source 10.233.110.0\nYou may need to change the subnet of flannel and the flannel gateway ip. ",
    "jayunit100": "Related, i recently found i had to increase ulimit very high when making lots of ETCD client connections in a stress test using go/etcd.    if go/etcd can do some internal logic to not require this, or to be more eager about cleanup of old connections, that would be awesome\n. I ultimately found that running tcpdump -i eth1 showed me the error buried ... \nkube1.ha > kube2.ha ICMP host kube1.ha unreachable - admin prohibited - length 68\nThe solution ? I ran \"iptables -F\" on all of my machines.  This seemed to clear up the issue.   \nSo in the end, I think there are really 3 steps to debugging this.\n- first make sure each machine makes docker containers w/ ip's on subnets that are correct and distinct.\n- then try to ping between your containers from different hosts.  if the result is unreachable\n- run tcpdump and take a good look at the logs  .  You should see something getting dropped between the hosts.  iptables -F may be necessary (even if iptables isn't enabled, the rules can bite you).  \nHope this is the right advice, if not please leave a comment in this thread, otherwise, ill close the issue.\n. Diving deeper, it appears that in my system, the MAC Addresses for all machines aren't written to etcd, rather, there is exactly one IP subnet and one mac adress .  Maybe that is the problem....\n[root@kube0 vagrant]# etcdctl ls /coreos.com/network/subnets/\n/coreos.com/network/subnets/99.0.30.0-24\n(ive deleted my earlier question around ip addresses, as it appears that etcd is only responsible for storing machines+subnets, not individual IPs)\n. NEVERMIND I foud that the eth1 argument being sent requires a --iface= option. Meanwhile, i still have other issues, but now im getting two subnets properly provisioned by flannel to different VM's container spaces.\nFor anyone curious, you have to be careful to make sure you properly use the iface option when starting flannel with vagrant provisioned VMs on a bridged network (eth0 is the bridge in some cases, which can lead to two separate machines trying to use IPs in the same subnet)\n. REopening because, flannel should provide an error if you give it an unused/invalid commandline option.     Will change the name shortly...\n. testing the binary , after build,  leads to testing an old artifact.  so its quite misleading.\n- Shall we get rid of bin/ and replace it with bin-$(timestamp) ?   That way the whole thing is immutable.\n- Or maybe have a \"Makefile\" w/ a clean task?\n. Yup Makefile is probably the safest bet.  I can look at it this wk.  good feedback thanks ;\n. ",
    "yichengq": "@eyakubovich go-etcd is not tagged correctly, and current master is only for etcd 2.0. The go-etcd for 0.4 is at https://github.com/coreos/go-etcd/commits/release-0.4. I can retag it and cherry-pick the changes that lvlv makes into release-0.4 branch.\n. @eyakubovich @crawford @philips \ngo-etcd is designed to go through all endpoints in the etcd cluster. There was some bug on the cycle, and it has been fixed in master. I just backport it to 0.4 branch.\nMoreover, we have added basic backward compatibility in go-etcd 2.0 recently, and I think we can move to use go-etcd master version in flannel.\n. lgtm\n. the code lgtm\nAFAIK, the log of go-etcd prints out details of each request. Will it be too many logs in stderr then?\n. k. lgtm then.\n. generally LGTM, though i am not familiar with flannel code base.\n. ",
    "roman-shuhov": "please check https://github.com/GoogleCloudPlatform/kubernetes/issues/6311 for more details\n. no problem, i switched to newer coreos and kubernetes and the issue is gone. \n. ",
    "sandric": "Having the same issue. What are we doing wrong?\n. ",
    "rohansingh": "Apologies for the out-of-the-blue code drop. It's pretty straightforward though, and the usage is outlined in README.md. Let me know if you see any issues, or if this isn't something that you think fits with the direction of flannel.\n. @eyakubovich Thanks for the review. Just pushed a version with the following changes:\n1. Fixed scoping of the vars you pointed out.\n2. Renamed to aws-vpc.\n3. Auto-detect the region from AWS metadata, so you just specify the route table ID in the config.\n. @eyakubovich Don't have access to my test rig at the moment, but I'd think traffic within the same subnet wouldn't go through the router. The router would just send packets back to the host on which they originated, after all.\n@benmccann The AWS Enhanced Networking thing is something else altogether. It's not required for this.\nWhat @bredanburns was referring to is exactly what this backend does, except that the Kube setup scripts do it using the AWS CLI:\nhttps://github.com/GoogleCloudPlatform/kubernetes/blob/master/cluster/aws/util.sh#L493\nBy the way, what that setup script is doing with creating a VPC subnet and automatically choosing & associating the correct route table is more sophisticated than what this backend is doing. It might be something to look into emulating.\n. Looks like it's got support: http://godoc.org/github.com/awslabs/aws-sdk-go/service/ec2#EC2.CreateRoute\n. Interesting, good to know. Are you considering the availability implications? It would be a no-go for us if the flannel server/master became a single point of failure.\n. @eyakubovich Sounds great! Thanks for the clarification :)\n. @anubhavmishra We went with this (copied from a presentation, sorry):\n\nAlternative: BGP\n\nBorder Gateway Protocol, the routing protocol of\n  the internet.\nBGP peers connect to each other and exchange routes.\n\nOMG. What did you do?\n\nWe configured our top-of-rack switches to accept\n  routes from our docker hosts.\nAnd then installed a bgp daemon, bird, on each\n  docker host.\n\nSo how does it work now?\n\nWe install flannel on docker hosts, just to do\n   IP allocation.\nDocker uses the machine subnet allocated by flannel.\nbird takes a look at what the machine subnet is,\n   and advertises this to the top-of-rack switch.\nThat top-of-rack switch exchanges routes with its\n   peers, so packets get routed correctly.\n\nIs this better?\n\nMore complexity at startup since we need both\n  bird and BGP to work.\nBut after initialization, the system is resilient.\nAlso, packets are \"normal\":\n  traceroute makes sense, etc.\n. Oops. Left over from a draft version.\n. \n\n",
    "benmccann": "Nice!  We were just talking about this over at kubernetes where I had some questions about their AWS docs (https://github.com/GoogleCloudPlatform/kubernetes/issues/7036) and Flannel. @brendanburns said \"AWS VPC Advanced networking is also capable of subnet per host\". I just wanted to clarify that means all AWS VPCs? And not AWS VPCs with \"Enhanced Networking\"?\nThis is pretty cool. If the same thing is added for GCE then Flannel could be a pretty generic way of handling this on any cloud and perhaps integrated as a default component of Kubernetes...\n. Cool. Thanks for the clarifications!\n. @justinsb was switching Kubernetes to use awslabs rather than goamz\n. ",
    "bakins": "Very cool!\nThoughts on using https://github.com/awslabs/aws-sdk-go rather than goamz?  I noticed that Hashicorp has started to use it rather than mitchell's version.  Not 100% sure it supports all the API we need for routes.\n. LGMT other than the content-type.\n. LGTM\n. the ctx isn't actually being used in most of these functions.  Is the thought to use in the future without changing the function signatures?\n. Need to set content-type header. Perhaps create a helper for the json responses rather than repeating?\ngo\nfunc JSON(w http.ResponseWriter, code int, v interface{}) {\n    w.Header().Set(\"Content-Type\", \"application/json; charset=utf-8\")\n    w.WriteHeader(code)\n        if err:= json.NewEncoder(w).Encode(v); err != nil {\n           // log something\n       }\n}\n. Got it. :+1: \n. ",
    "justinsb": "On switching to the official AWS SDK, I think it is going to happen.  I filed this issue: https://github.com/GoogleCloudPlatform/kubernetes/issues/6162 and @jsdir took up the cause!\nSounds like we're just waiting for a few final things to be supported in the AWS SDK: https://github.com/GoogleCloudPlatform/kubernetes/issues/6162#issuecomment-93910186\n. That is my hypothesis for what I have otherwise not yet been able to explain, so I thought I would ask here in flannel if it makes sense and if it has been observed.\nThere's also a general API issue which is how do we deal with this... protobuf has \"unknown fields\" but I don't think we have something similar for JSON?\nAnd I think Update == PUT:\n```\n// Update takes the representation of a node and updates it. Returns the server's representation of the node, and an error, if there is any.\nfunc (c nodes) Update(node v1.Node) (result *v1.Node, err error) {\n        result = &v1.Node{}\n        err = c.client.Put().\n                Resource(\"nodes\").\n                Name(node.Name).\n                Body(node).\n                Do().\n                Into(result)\n        return\n}\n```. Probable confirmation (I believe):  \n\nI readded the taint to the node\nI deleted the backendDataAnnotation (flannel.alpha.coreos.com/backend-data) annotation from the node\nI restarted (all) the canal pods.\nI manually polled the node for the taint and watched the pods restart.\nAs soon as the pods had restarted (and not before), I observed the taint was removed from the node.\n\nThis was actually with canal, not flannel (the original report was observed with flannel though); if there is still doubt let me know and I can repeat experiments or try to do something more scientific.. ",
    "JeanMertz": "Ah yes, you are correct. This is working as expected now :+1: \n. Okay, I think this might actually be related to the cloud-config script I use, which calls docker in a shell script before it writes the cloud-config yaml file. This docker call also does not use systemd (and thus has no dependency chain).\nI am not sure yet how to solve this, but looking at the logs, this certainly seems like the problem.\n. Confirmed: removing the lone docker run call in the cloud-config script solved the problem. I guess Docker automatically starts a daemon if it detects a docker run command and no daemon running?\nI rewrote the call to a delayed script inside a Systemd unit, which solved the problem.\n. ",
    "bencevans": "That's brilliant thanks!\nI'd got most of it together but was obviously being a bit dosey and forgot to use a container name to reference it from a separate service file!\n. ",
    "felixsanz": "@eyakubovich Sorry for bumping this old issue but since the service has After=demo_web.service, is that not enough to avoid \"since it takes a bit for container to start...\" ?\nThe discovery service should always get the main service container IP, i'm wrong?. ",
    "jcollie": "The documentation regarding command-line flags (including the built-in help) is incorrect.  I got this to work:\nflanneld -ip-masq -etcd-endpoints https://127.0.0.1:2379 -etcd-cafile /etc/etcd/ca.crt -etcd-certfile /etc/etcd/host.crt -etcd-keyfile /etc/etcd/host.key\nNote the lack of equal signs.  A lot of CoreOS' tools have similar problems.  A lot of it has to do with the fact that people are used to GNU getopt-style command line parsing where \"--opt=value\" is equivalent to \"--opt value\".  The Go \"flag\" library claims to do the same thing but clearly doesn't.\n. Not sure if it's the exact same problem, but I get the same error message.  I'm using the pre-built binaries on a Fedora 22 system.  The etcd version is 2.1.0-rc.0.\n```\n/opt/flannel/flannel-0.5.0/flanneld -ip-masq -etcd-endpoints http://127.0.0.1:2379\nI0709 13:55:06.246292 29102 main.go:275] Installing signal handlers\nI0709 13:55:06.246777 29102 main.go:141] Determining IP address of default interface\nI0709 13:55:06.248235 29102 main.go:189] Using 161.210.221.8 as external interface\ngo-etcd2015/07/09 13:55:06 DEBUG: get /coreos.com/network/config [http://127.0.0.1:2379]\ngo-etcd2015/07/09 13:55:06 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\ngo-etcd2015/07/09 13:55:06 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\ngo-etcd2015/07/09 13:55:06 DEBUG: recv.response.from \ngo-etcd2015/07/09 13:55:06 DEBUG: recv.success \ngo-etcd2015/07/09 13:55:06 DEBUG: get /coreos.com/network/config [http://127.0.0.1:2379]\ngo-etcd2015/07/09 13:55:06 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\ngo-etcd2015/07/09 13:55:06 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\ngo-etcd2015/07/09 13:55:06 DEBUG: recv.response.from \ngo-etcd2015/07/09 13:55:06 DEBUG: recv.success \ngo-etcd2015/07/09 13:55:06 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\ngo-etcd2015/07/09 13:55:06 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\ngo-etcd2015/07/09 13:55:06 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\ngo-etcd2015/07/09 13:55:06 DEBUG: recv.response.from \ngo-etcd2015/07/09 13:55:06 DEBUG: recv.success \nI0709 13:55:06.261982 29102 etcd.go:136] Found lease (10.16.9.0/24) for current IP (161.210.221.8), reusing\n go-etcd2015/07/09 13:55:06 DEBUG: put /coreos.com/network/subnets/10.16.9.0-24, {\"PublicIP\":\"161.210.221.8\"}, ttl: 86400, [http://127.0.0.1:2379]\ngo-etcd2015/07/09 13:55:06 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/10.16.9.0-24\ngo-etcd2015/07/09 13:55:06 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/10.16.9.0-24 | method PUT\ngo-etcd2015/07/09 13:55:06 DEBUG: recv.response.from \ngo-etcd2015/07/09 13:55:06 DEBUG: recv.success \nI0709 13:55:06.292205 29102 etcd.go:91] Subnet lease acquired: 10.16.9.0/24\nI0709 13:55:06.302705 29102 ipmasq.go:47] Adding iptables rule: FLANNEL -d 10.16.0.0/16 -j ACCEPT\nI0709 13:55:06.312506 29102 ipmasq.go:47] Adding iptables rule: FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE\nI0709 13:55:06.321538 29102 ipmasq.go:47] Adding iptables rule: POSTROUTING -s 10.16.0.0/16 -j FLANNEL\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x0 pc=0x4ded84]\ngoroutine 16 [running]:\ngithub.com/coreos/flannel/subnet.LeaseRenewer(0x7fc04d6783e8, 0xc20800b0c0, 0x7fc04d678310, 0xc20802ab60, 0x0, 0x0, 0x0)\n        /opt/flannel/gopath/src/github.com/coreos/flannel/subnet/renew.go:29 +0x4e4\ngithub.com/coreos/flannel/backend/alloc.(AllocBackend).Run(0xc20800b100)\n        /opt/flannel/gopath/src/github.com/coreos/flannel/backend/alloc/alloc.go:54 +0x6a\ngithub.com/coreos/flannel/network.func\u00b7005()\n        /opt/flannel/gopath/src/github.com/coreos/flannel/network/network.go:109 +0x4c\ncreated by github.com/coreos/flannel/network.(Network).Run\n        /opt/flannel/gopath/src/github.com/coreos/flannel/network/network.go:111 +0xfb\ngoroutine 1 [chan receive]:\nmain.main()\n        /opt/flannel/gopath/src/github.com/coreos/flannel/main.go:288 +0xa2a\ngoroutine 5 [chan receive]:\ngithub.com/coreos/flannel/Godeps/_workspace/src/github.com/golang/glog (*loggingT).flushDaemon(0xd4d1e0)\n        /opt/flannel/gopath/src/github.com/coreos/flannel/Godeps/_workspace/src/github.com/golang/glog/glog.go:839 +0x78\ncreated by github.com/coreos/flannel/Godeps/_workspace/src/github.com/golang/glog.init\u00b71\n        /opt/flannel/gopath/src/github.com/coreos/flannel/Godeps/_workspace/src/github.com/golang/glog/glog.go:406 +0x2a7\ngoroutine 17 [syscall, locked to thread]:\nruntime.goexit()\n        /usr/src/go/src/runtime/asm_amd64.s:2232 +0x1\ngoroutine 7 [syscall]:\nos/signal.loop()\n        /usr/src/go/src/os/signal/signal_unix.go:21 +0x1f\ncreated by os/signal.init\u00b71\n        /usr/src/go/src/os/signal/signal_unix.go:27 +0x35\ngoroutine 9 [chan receive]:\ngithub.com/coreos/flannel/network.(*Network).Run(0xc20800a880, 0x7fc04d6783e8, 0xc20800a440)\n        /opt/flannel/gopath/src/github.com/coreos/flannel/network/network.go:113 +0x134\nmain.func\u00b7003(0xc20800a880)\n        /opt/flannel/gopath/src/github.com/coreos/flannel/main.go:217 +0x268\ncreated by main.initAndRun\n        /opt/flannel/gopath/src/github.com/coreos/flannel/main.go:220 +0x84b\ngoroutine 11 [IO wait]:\nnet.(pollDesc).Wait(0xc208010760, 0x72, 0x0, 0x0)\n        /usr/src/go/src/net/fd_poll_runtime.go:84 +0x47\nnet.(pollDesc).WaitRead(0xc208010760, 0x0, 0x0)\n        /usr/src/go/src/net/fd_poll_runtime.go:89 +0x43\nnet.(netFD).Read(0xc208010700, 0xc2080bb000, 0x1000, 0x1000, 0x0, 0x7fc04d677e68, 0xc2080ca4e0)\n        /usr/src/go/src/net/fd_unix.go:242 +0x40f\nnet.(conn).Read(0xc20802c0c0, 0xc2080bb000, 0x1000, 0x1000, 0x0, 0x0, 0x0)\n        /usr/src/go/src/net/net.go:121 +0xdc\nnet/http.noteEOFReader.Read(0x7fc04d679618, 0xc20802c0c0, 0xc208090268, 0xc2080bb000, 0x1000, 0x1000, 0x7fb260, 0x0, 0x0)\n        /usr/src/go/src/net/http/transport.go:1270 +0x6e\nnet/http.(noteEOFReader).Read(0xc20801f200, 0xc2080bb000, 0x1000, 0x1000, 0xc208012000, 0x0, 0x0)\n        :125 +0xd4\nbufio.(Reader).fill(0xc20804e720)\n        /usr/src/go/src/bufio/bufio.go:97 +0x1ce\n bufio.(Reader).Peek(0xc20804e720, 0x1, 0x0, 0x0, 0x0, 0x0, 0x0)\n        /usr/src/go/src/bufio/bufio.go:132 +0xf0\nnet/http.(persistConn).readLoop(0xc208090210)\n        /usr/src/go/src/net/http/transport.go:842 +0xa4\ncreated by net/http.(*Transport).dialConn\n        /usr/src/go/src/net/http/transport.go:660 +0xc9f\ngoroutine 12 [select]:\nnet/http.(persistConn).writeLoop(0xc208090210)\n        /usr/src/go/src/net/http/transport.go:945 +0x41d\n created by net/http.(Transport).dialConn\n        /usr/src/go/src/net/http/transport.go:661 +0xcbc\n```\n. I get the same error on a CentOS 7 system.\n. @eyakubovich Thanks!\n. +1 from me.  OSPF would be the most important.  Right now I'm running flannel on CentOS with the alloc backend and using bird to propogate the Flannel networks to the router.\n. ",
    "mnemotiv": "Problem is that flanneld.service doesn't use /run/flannel/options.env file for env-vars before it starts flanneld container.\n/opt/bin/flanneld --ip-masq=true (missing additional flags here)\n. @MohdAhmad yes, with latest alpha 695\n. this problem exists if I deploy server with cloud config:\nyaml\n...\n  flannel:\n    etcd-endpoints: https://127.0.0.1:2379\n    etcd-cafile: /var/run/...../ca.crt\n    etcd-certfile: /var/run/...../master.crt\n    etcd-keyfile: /var/run/...../master.key\n...\nflanneld.service from journalctl\nJun 10 13:44:01 k8s-master-001 systemd[1]: Starting Network fabric for containers...\nJun 10 13:44:02 k8s-master-001 etcdctl[644]: Error:  cannot sync with the cluster using endpoints http://127.0.0.1:4001, http://127.0.0.1:2379\nJun 10 13:44:02 k8s-master-001 systemd[1]: flanneld.service: control process exited, code=exited status=2\nJun 10 13:44:02 k8s-master-001 systemd[1]: Failed to start Network fabric for containers.\nJun 10 13:44:02 k8s-master-001 systemd[1]: Unit flanneld.service entered failed state.\nJun 10 13:44:02 k8s-master-001 systemd[1]: flanneld.service failed.\nJun 10 13:44:07 k8s-master-001 systemd[1]: flanneld.service holdoff time over, scheduling restart.\nsystemctl status flanneld\n```\n\u25cf flanneld.service - Network fabric for containers\n   Loaded: loaded (/usr/lib64/systemd/system/flanneld.service; static; vendor preset: disabled)\n  Drop-In: /etc/systemd/system/flanneld.service.d\n           \u2514\u250050-certs-config.conf, 51-network-config.conf\n   Active: activating (auto-restart) (Result: exit-code) since Thu 2015-06-11 06:30:30 UTC; 1s ago\n     Docs: https://github.com/coreos/flannel\n  Process: 31186 ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config {\"Network\":\"10.244.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}} (code=exited, status=4)\n  Process: 31182 ExecStartPre=/usr/bin/touch /run/flannel/options.env (code=exited, status=0/SUCCESS)\n  Process: 31180 ExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR} (code=exited, status=0/SUCCESS)\n  Process: 31176 ExecStartPre=/usr/bin/mkdir -p /run/flannel (code=exited, status=0/SUCCESS)\n  Process: 31175 ExecStartPre=/sbin/modprobe ip_tables (code=exited, status=0/SUCCESS)\nJun 11 06:30:30 k8s-master-001 systemd[1]: Unit flanneld.service entered failed state.\nJun 11 06:30:30 k8s-master-001 systemd[1]: flanneld.service failed.\n```\np.s. none of flanneld drop-ins in cloud-config doesn't use any env-vars at all which could overwrite any of flanneld-native env-vars - flanneld generates correct env-vars to its env-file, but it doesn't use it for argument flags for its container.\n. I'm starting to think that's more etcdctl problem than flanneld...\n. etcd2 works fine, fleet - fine, flannel with etcdctl - not\n. @eyakubovich ok cool. as I saw on coreos documentation - flannel is being packed as docker container to save disk space for OS \"for those who won't use flannel\" - who won't use flannel? :)\n. @vaijab \nFLANNELD_ETCD_ENDPOINTS=http://127.0.0.1:2379\njust source this env-var as file or just as var in flanneld.service / docker run...\nhttps://github.com/nodetemple/nodetemple-deprecated/blob/master/daemon/environment-setup\nhttps://github.com/nodetemple/nodetemple-deprecated/blob/master/fleet/flanneld.service\n. My files doesn't include env-vars for docker container of flannel - so you should add env-vars to container runtime. You can also run flannel not in a container - then it will fetch env-vars directly.\n. ",
    "MohdAhmad": "@tanmaybinaykiya  @jcollie we tested ./flanneld -etcd-endpoints=\"http://192.168.198.130:2379\" with go1.4.2 and it worked fine. Which version of go are you guys using? \n. @mnemotiv is this the flanneld.service that is shipped with coreos?\n. LGTM\n. LGTM\n. @suyogbarve this was fixed in the flannel v0.5 release. We have had a maintenance, flannel v0.51, release out since then as well. \n. Same issue with the new PR. \n0K$ ./test\nBuilding flanneld...\nRunning tests...\nok      github.com/coreos/flannel/pkg/ip    0.005s  coverage: 15.6% of statements\nok      github.com/coreos/flannel/subnet    2.009s  coverage: 59.5% of statements\n--- FAIL: TestRemote-32 (0.00 seconds)\n    remote_test.go:72: GetNetworkConfig failed: Get http://127.0.0.1:9999/_/config: dial tcp 127.0.0.1:9999: connection refused\npanic: runtime error: invalid memory address or nil pointer dereference [recovered]\n    panic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x0 pc=0x44b7c3]\ngoroutine 21 [running]:\nruntime.panic(0x713b60, 0x918cb3)\n    /usr/local/go/src/pkg/runtime/panic.c:279 +0xf5\ntesting.func\u00c2\u00b7006()\n    /usr/local/go/src/pkg/testing/testing.go:416 +0x176\nruntime.panic(0x713b60, 0x918cb3)\n    /usr/local/go/src/pkg/runtime/panic.c:248 +0x18d\ngithub.com/coreos/flannel/remote.doTestRemote(0x7f33227e7b78, 0xc208088040, 0xc208086090, 0x7606d0, 0xe)\n    /home/travis/gopath/src/github.com/coreos/flannel/gopath/src/github.com/coreos/flannel/remote/remote_test.go:75 +0x883\ngithub.com/coreos/flannel/remote.TestRemote(0xc208086090)\n    /home/travis/gopath/src/github.com/coreos/flannel/gopath/src/github.com/coreos/flannel/remote/remote_test.go:45 +0x2d5\ntesting.tRunner(0xc208086090, 0x912ee0)\n    /usr/local/go/src/pkg/testing/testing.go:422 +0x8b\ncreated by testing.RunTests\n    /usr/local/go/src/pkg/testing/testing.go:504 +0x8db\ngoroutine 16 [chan receive]:\ntesting.RunTests(0x7f3e40, 0x912ee0, 0x1, 0x1, 0x6ee501)\n    /usr/local/go/src/pkg/testing/testing.go:505 +0x923\ntesting.Main(0x7f3e40, 0x912ee0, 0x1, 0x1, 0x922a40, 0x0, 0x0, 0x922a40, 0x0, 0x0)\n    /usr/local/go/src/pkg/testing/testing.go:435 +0x84\nmain.main()\n    github.com/coreos/flannel/remote/_test/_testmain.go:97 +0x10c\n. LGTM\n. LGTM\n. LGTM\n. Fixed by #227 \n. LGTM. Looking forward to trying this out myself. \n. LGTM\n. @diegomarangoni Thanks for pointing this out. \nDid you try this using IAM roles as well? And once you set the AWS_ACCESS_KEY_ID and the AWS_SECRET_ACCESS_KEY was flannel still unable to discover the route table id? \n. Hi @diegomarangoni, can you please share which coreos release you are using? Since this functionality was added in flannel v0.5 which only recently made it to coreos release, I am concerned that this may be an older version.\n. @diegomarangoni Thanks for sharing again. \nWith flannel v0.5 if you define an IAM role, as shown in the tutorial, then you do not need to specify the credentials or the route table id in the cloud config. But if you use AWS_SECRET_ACCESS_KEY and the AWS_SECRET_KEY then you will need to specify them in the cloud config. Running flannel inside a docker container or running it natively should not make a difference in this case. Hope this helps. \n. @diegomarangoni great news!\n. Apart from the two comments, LGTM.\n. LGTM\n. LGTM. Thanks @robszumski !\n. This fixes #254, right?\n. Hi @syed, can you share which flannel version you are using? \nThe MACADDRESS is the mac address of the flannel.1 interface. \n. LGTM\n. Is the .DS_Store binary file required? I didn't find any binaries in the etcd and rkt logos folder so just making sure. \n. LGTM. Thank you for updating. Great logo!\n. LGTM\n. LGTM apart from the minor note above.\n. LGTM\n. LGTM\n. LGTM\n. @shayts7 did you face any issues with lease renewal after moving to a newer version of flannel? \n. LGTM\n. LGTM once rebased. \n. This is not meant to be merged. Putting this up to solicit feedback. \n. LGTM\n. LGTM\n. @wulonghui here is another http://paulbakker.io/docker/docker-cloud-network-performance/\n. LGTM apart from the minor changes. \n. @eyakubovich this was fixed in #329. I think we can close.\n. @gouyang did you try this with three node etcd cluster?\netcd implements the raft algorithm and therefore requires > 50% of the nodes to be up to function. In a two node etcd cluster, if one node goes down we will no longer have > 50% of the nodes up and etcd cannot make progress. In short, the etcd cluster will be unavailable if one node of a two node etcd cluster is down. \nMore information on the raft algorithm here \n. @gouyang Please share configuration details so I can try to recreate the issue at my end, flannel, etcd versions etc. Thanks!\n. LGTM apart from the small change. \n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. @tomdee I'm happy to send a PR for this but are flannel releases tied to k8s releases in any way? Because I believe setting the priority class on kubernetes versions prior to 1.11 without setting --feature-gates=PodPriority=true in the kubelet will lead to a crash.. These correspond to blue, green and red, right?\n. Just confirmed this, it will not fail if the source/destination check is already disabled. \n. signed *by the \n. If unused shouldn't this give an error?\n. For networks that require a greater number of routes, we should clarify that flannel will can still be used but with the VXLAN backend instead. Adding something  similar to 'if your network requires a greater number of routes please consider using the VXLAN backend' should do the job. \n. can we change the filename to aws-vpc-backend or amazon-vpc-backend? \n. Can we add a small heading of 'Limitations' here \n. Can we add a small heading of 'Limitations' here \n. Same as with the gce-backend, adding something similar to 'if your network requires a greater number of route entries please consider using the VXLAN backend' should do the job.\n. When running on GCE we recommend using the gce backend which, instead of using encapsulation, manipulates IP routes to achieve maximum performance. Because of this, a separate flannel interface is not created. \n. When running in an Amazon VPC we recommend using the aws-vpc backend which, instead of using encapsulation, manipulates IP routes to achieve maximum performance. Because of this, a separate flannel interface is not created. \n. This should be 'delete failed', right?\n. Good point. Instead of margintime, Vici defines rand_time which is the time range from which to choose a random value to subtract from rekey times. It defaults to 10% of rekey_time. So in this case the rekey process could begin anytime after 9m and before 10m of the SA being established. We plan to change the rekey_time to the default of 1h before merging. \n(the strongswan website isn't currently working so sharing the github page)\nhttps://github.com/strongswan/strongswan/blob/9322e5b398efcf0a6f3bf576ef4b4b12b5ae6528/src/swanctl/swanctl.opt \n. subnetID\n. vpcID\n. Typo. Please change sent to send\n. Isn't a call to ipt.ClearChain required before ipt.DeleteChain. From the code comments here I understand that the DeleteChain requires that the chain already be empty. \n. We also need to add delete the POSTROUTING rule added in #327 .\n. Please change Failed to failed\n. Change to error from Error\n. Shouldn't this be SimpleNetwork backend.SimpleNetwork?\n. Change to failed from Failed\n. Change to failed from Failed\n.  Change to error from Error\n.  Change to error from Error\n.  Change to failed from Failed\n.  Change to failed from Failed\n. Change to failed from Failed\n. Change to invalid from Invalid\n. Ah right. I confused the two.\n. delete already called on line 327. \n. I do not follow how asof is being used. I initially thought it was passed to updateSubnet and then used in etcd.SetOptions to confirm the previous modified index of the key. But all calls to updateSubnet in local_manager pass asof=0.\n. I can't see where nodeToLease gets used. \n. Yes, that sounds good. \n. ",
    "vaijab": "@eyakubovich so how do I tell flannel that my etcd endpoint is on https://localhost:2379 ?\n. @mnemotiv I am not sure that actually works. You probably got an impression that it works, because flannel defaults to http://127.0.0.1:2379 anyway.\n```\ncore@ip-10-50-0-221 ~ $ systemctl cat flanneld\n/usr/lib64/systemd/system/flanneld.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nRequires=early-docker.service\nAfter=etcd.service etcd2.service early-docker.service\nBefore=early-docker.target\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=\"TMPDIR=/var/tmp/\"\nEnvironment=\"DOCKER_HOST=unix:///var/run/early-docker.sock\"\nEnvironment=\"FLANNEL_VER=0.5.1\"\nEnvironment=\"ETCD_SSL_DIR=/etc/ssl/etcd\"\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStartPre=/usr/bin/touch /run/flannel/options.env\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n  /usr/bin/docker run --net=host --privileged=true --rm \\\n  --volume=/run/flannel:/run/flannel \\\n  --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n  --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n  --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n  --env-file=/run/flannel/options.env \\\n  --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n  --volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro \\\n  quay.io/coreos/flannel:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=true\nUpdate docker options\nExecStartPost=/usr/bin/docker run --net=host --rm -v /run:/run \\\n  quay.io/coreos/flannel:${FLANNEL_VER} \\\n  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env -i\n/etc/systemd/system/flanneld.service.d/10-configuration.conf\n[Service]\nEnvironment=\"FLANNELD_INTERFACE=10.50.0.221\"\nEnvironment=\"FLANNELD_ETCD_ENDPOINTS=https://127.0.0.1:2379\"\n/etc/systemd/system/flanneld.service.d/50-network-config.conf\n[Service]\nExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{\"Network\":\"10.10.0.0/16\",\"Backend\":{\"Type\":\"vxlan\"}}'\ncore@ip-10-50-0-221 ~ $ journalctl -fn -u flanneld.service\n-- Logs begin at Thu 2015-07-30 13:53:22 UTC. --\nJul 30 13:54:36 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: flanneld.service: Unit entered failed state.\nJul 30 13:54:36 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: flanneld.service: Failed with result 'exit-code'.\nJul 30 13:54:36 ip-10-50-0-221.eu-west-1.compute.internal etcdctl[3843]: Error:  cannot sync with the cluster using endpoints http://127.0.0.1:4001, http://127.0.0.1:2379\nJul 30 13:54:41 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: flanneld.service: Service hold-off time over, scheduling restart.\nJul 30 13:54:41 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: Starting Network fabric for containers...\nJul 30 13:54:42 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: flanneld.service: Control process exited, code=exited status=2\nJul 30 13:54:42 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: Failed to start Network fabric for containers.\nJul 30 13:54:42 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: flanneld.service: Unit entered failed state.\nJul 30 13:54:42 ip-10-50-0-221.eu-west-1.compute.internal systemd[1]: flanneld.service: Failed with result 'exit-code'.\nJul 30 13:54:42 ip-10-50-0-221.eu-west-1.compute.internal etcdctl[3887]: Error:  cannot sync with the cluster using endpoints http://127.0.0.1:4001, http://127.0.0.1:2379\n``\n. @mnemotiv gotcha! Thanks, I have it working now. Flannel config and startup on CoreOS seems convoluted and rather complicated. But flannel is still awesome anyway.\n. @eyakubovich that's right, but I also had to drop an additional drop-in to make this all work, maybe documentation needs to be updated orflanneld.serviceto addEnvironmentFile=/run/flannel/options.env`\nyaml\n- name: 10-env-config.conf\n  content: |\n    [Service]\n    EnvironmentFile=/run/flannel/options.env\n. I believe this is fixed now and can be closed.. I think it makes sense to try and disable SrcDestCheck by default. Having a switch to turn off this behaviour would help as well I guess - similar behaviour to how routing table id gets discovered.\n. Thanks!\n. @eyakubovich thanks. That actually works. It is just a little annoying that I have to put CA cert in two different places on the host: /etc/ssl/certs and /etc/ssl/etcd.\nI am going to close this one now.\n. @eyakubovich I just added CA cert to /etc/ssl/etcd and set coreos.flannel.etcd_certfile - that works well.\nAFAIK go's TLS implementation looks in /etc/ssl/certs for CA certs by default, so it would make sense for flannel to just assume that custom CA certs live in /etc/ssl/certs, that's why I suggested flannel to mount /etc/ssl/certs as well.\n. @kkirsche yes, that's correct.\n. > from https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/\n\nIt seems as though this says to disable it within AWS, so want to make sure that this has been checked\n\nThat used to be the case, but then I suggested this: #230 \nI guess there two options here:\n- Flannel knows how to disable SourceDestCheck on either one or all ENIs\n- An option to tell flannel not to try to disable SourceDestCheck\nBecause what's happening now, is flannel is continuously trying to disable SourceDestCheck on an instance it is running and gets AWS API rate limited which affects other services and processes running in the same AWS account, i.e. nothing else can easily talk to AWS API, because how AWS does rate limiting.\n. So in terms of aws/session I figured that out. The current version of aws-sdk-go is pinned to a version which does not have aws/session. But glide still updates lots of other dependencies, because quite a few of them aren't versioned.\nAny plans to sort this out?\n. > There are no current plans - feel free submit a PR though.\nI think I am slowly working towards that. :-)\n. CI job needs to be restarted because www.netfilter.org was down.\n. Any chance of reviewing this? /cc @tomdee \nWe're using a custom built flannel version which includes this patch without any issues so far.\n. Does that mean that flannel adds private subnet routes to public network routing tables?. @t0mmyt @pingles thanks. I think we wanted to have a similar AWS networking setup for our clusters, but flannel was the show stopper back then.. ",
    "lremurphy": "I am not quite sure if this is applicable but I had the same issue working on a CentOS Atomic system.  What ultimately resolved it was changing the following parameters in /etc/etcd/etcd.conf to the following:\nETCD_LISTEN_CLIENT_URLS=\"http://0.0.0.0:2379\" \n   ETCD_LISTEN_PEER_URLS=\"http://0.0.0.0:2380\"\nAfter restarting the etcd service, this seemed to fix my problem.  Hopefully this is helpful to someone.\n. ",
    "BlueDragonX": "The VPC backend also has the limitation that all hosts must be in subnets which share the same route table.\nOur cluster layout includes a subnet to which we deploy our proxy containers. This uses a different route table as these are the only hosts with external IPs and direct internet access.\nA solution to both problems could involve adding support for multiple route tables. The implementation would likely include a mapping of host subnets to route table IDs.\nThe above solution would allow us to bypass the 100 entry limit in a single route table by assigning a new route table to each subnet.\n. ",
    "Grindizer": "Hi everyone, \nwe are also investigating the use of aws-vpc for our cluster, the most annoying for us is actually the need for the subnets to share the same route table, in our case we use different subnets (one in each AZ) and each one point to a NAT instance present in its AZ (so table are different for each subnet). \nI was wondering if updating more then one route table would be a possible solution, the list of routing table to alter would be given with an AWS tag (where the backend would update any routing table tagged a certain way, the tag name and value would be given as a parameter).\nor we can also image an autoscaling group feature, where the backend introspect the instance autoscaling group, fetch the list of subnets involved and then update any routing table associated ?\nFor now the backend inspect the instance subnet and update the routing table associated. \nThis won't solve the limitation problem but would certainly make the backend more usable many case ?\n. ",
    "bernielomax": "@Grindizer did you have any luck with this? I would like to do the same.\n. ",
    "anubhavmishra": "@rohansingh What did you guys end up using? VXLAN option?\n. ",
    "peterlamar": "I actually ended up using the flannel portion in this article. It has a few straight forward steps to send a packet around using flannel. One glance at the article and I understood a simple flannel stand alone use case. \nhttp://www.generictestdomain.net/docker/weave/networking/stupidity/2015/04/05/weave-is-kinda-slow/\n. ",
    "elephantfries": "I hope static-ip in flannel gets some traction.\nLooking to etcd for flannel IP addresses is not ideal.  It requires exposing outer network to apps running in the flannel domain. If we do that, then we might as well expose flannel DNS to outer network and then find it through that network's static IP.  That's what our solution is.  This is far better then telling each application to watch etcd to see if anything's changed. It works but it's undesirable.  Apps running on the flannel network are not supposed to know there is some other network.\nIf static IP is too problematic, then at the very least, it would be nice if flannel would allocate the same IP address to a re-started container. It is smart enough to re-allocate the same subnet to a rebooted host.  Why cannot it do the same for the container??? (well, because it has no idea the newly started container is a restart of some previous container).  I realize, flannel cannot deduce it with current implementation.  Something needs to be added to flannel.\n. I see.  So looks like there is a need for an associated docker PR.\nHowever I am after something slightly different.  I am OK with flannel randomly allocating subnets so long as it re-allocates the same subnet on the host reboot.  That is the behavior right now and that's fine.  I am after static allocation of IP addresses for containers on that subnet, or at least allowing a container regain its previous IP address upon (container) restart.  I realize flannel is lacking a hint to tell that a container wants the same IP address as allocated to some previous container.  Would allowing that require a docker PR or would it be possible to implement it entirely within flannel?\n. OK, thanks, looks like very high activity in this area in docker right now:\nhttps://github.com/docker/libnetwork/issues/489\n. ",
    "qiukeren": "Request for this feature +1.\nalmost all thing can be done with domain name,\nbut DNS itself cannot escape from static IPs.. ",
    "p0ns": "core0 machine without issues\n2: eth0:  mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:xx:xx:xx brd ff:ff:ff:ff:ff:ff\n    inet x.x.x.x/29 brd x.x.x.63 scope global eth0\n       valid_lft forever preferred_lft forever\ncore1, machine with issues\n2: ens3:  mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 02:00:00:c5:xx:xx brd ff:ff:ff:ff:ff:ff\n    inet xx.x5.xx.235 peer xx.x9.x.254/24 scope global ens3\n       valid_lft forever preferred_lft forever\nThe difference seems to be that instead of a broadcast, it broadcasts on it's own IP and has a peer address, which is the gateway.\nThe interface is an actual ethernet interface, but the difference is that, as you can see in the .network file I put in the past message, it's not setup regularly with a gateway but a Peer address that acts as such.\n. When the IP is specified, the log says it can't find the interface to which the IP address belongs to. \nOne simple theory I have is that, when the interface is specified with --iface=ens3 the code is looking for the first full xxx.xxx.xxx.xxx/yy combination in the inet line, hence finding the peer IP in the error case. \nIf you need, I can give you access to my cluster so you can debug it. \nThanks again! \nFelipe\n. It works! After setting it up in the 3 nodes, one of em hit issue #112 and was getting a 172.x address instead of the 10.x on docker0, but I used the fix proposed in there, and I got em all connected.\nThank you for your work :+1: \n. ",
    "rajatchopra": "Perfunctory review, I will spend some more time later. Looks good I think, though I feel we will have to re-organize the code at some point.\n- Split out subnet package in three parts?\n  - api+interface (what is there in subnet.go)\n  - implementation of the interface above (what is there in etcd.go)\n  - the actual registry that reads/writes/watches etcd (registry.go/watch.go)\n- The documentation needs to be updated with multiple network usage. I believe a follow-up PR can do that.\n- What @mrunalp suggested: we should Godep the dependencies, but compile it by modifying GOPATH. Using Godeps/.. in import path seems unclean.\n- More of an architectural thing:\n  - Should flannel just obtain the networks established in the config? Why do we need to supply the networks at start? Soon we will have to deal with on-demand acquire a network's lease kind of thing. \n  - Do we want to keep retrying on errors in network.Init()?\nNone of the above are a blocker though. Will get back with another round.\n. I think beyond the re-organization of code mentioned above, this looks good to me as first cut.\nDefinitely a TODO after the client-server PR.\n. Ran the setup. Works fine. LGTM.\n. TLS on the server remains as a TODO.\n. @eyakubovich \nThe APIs in the proposal were meant to be run on the client flanneld. So its a local API where a local container runtime/orchestrator (through CNI for example) will claim that it has run out of a network's current local lease and it needs a new one.\nWithout this, the method of obtaining lease is that a lease is obtained for all networks, whether a container will ever be born on that node or not.\nOn the new Reservation work, first of all, Add/Delete Reservation should include the network as an arg. The only question I have is who will really create the reservations? Does it become the responsibility of the API user to manage the subnet allocation then? What if there are clashes?\n. I think its ok to get the leases done and notified via etcd/server, although it seems like a circuitious method to achieve. We can always think of optimizing later.\nFor the AddReservation, can we pass an empty subnet, which will indicate flannel to check out a subnet from the network's pool automatically?\n. Yes we plan to run the server always. Could you confirm that we can have an AddReservation call with 'nil' subnet which will mean a subnet will automatically be allocated (and ttl set to 0 after that)?\nOtherwise it seems that the caller has to do the subnet allocation.\n. Resolve it by running:\n$ modprobe br_netfilter\n$ sysctl -w net.bridge.bridge-nf-call-iptables = 1. Closing this PR in favour of #1042. Please re-open if needed.. Closing this PR in favour of #1042. Please re-open if needed.. It only makes sense with host-gw mode on a strictly L2 network. If you are talking about vxlan mode, then masquerading is necessary because pod's source IP will likely be blocked by the underlying network.\nClosing this issue as NOT_A_BUG. Please re-open if you think this is a mistake.. Closing this issue as per this comment. 'vxlan' kernel module is a must for flannel overlay to work. Please re-open the issue if unresolved still.. @thxCode Can you address the feedback by @ksubrmnn ? The PR is ready to be merged and it will supercede #921 and #922.. @ksubrmnn We need a final lgtm from you.. Drum roll... merge!!. Fair enough. Although this is not a typo, it may cause unintended issues with leftovers.. > There hasn't been a release of flannel for a year and we need to upgrade to Kubernetes 1.12.\n\nAre there plans to have a new release anytime soon? If not, it's not a problem, we can always branch and fix it ourselves.\nThanks\n\nThere is a release planned soon. Can we have a PR that updates kube-flannel.yml with the correct tolerations?. v0.11.0 is available. Please re-open this issue if needed.. @ksubrmnn do you plan to add the tests and documentation in this PR?. @ksubrmnn Thanks.. @toothbrush Among the two options, I would pick the 'master' and freeze it.\nExpect some action on the release this week though. Hopefully no more delays.. Update: The release is blocked on certain PRs (#1061 and #1068). As soon as... @madhanrm lgty?. Correct call. Thanks @ctas582 , the contents are incompatible. To resolve the point raised in #1043 the correct answer would then be to remove pre-existing flannel conf files in /etc/cni/net.d/ !\nThanks.. #1043 stands reverted. Sorry about that hassle. Thanks for reporting this.. @tomdee lgty?. > Strangely enough, in my case I can ping from a node to a pod running on another node, but a curl to it fails. Using tcpdump on the destination pod, I can see the initial packet (TCP SYN) on the pod's network interface, but no response ever goes out.\nThis sounds like an mtu problem. Can you post the MTU for the network?\nOr just paste the output of 'ip a' from any of the hosts.\n. @madhanrm @ksubrmnn Can you advise here?\n@JohnJAS Would you mind posting a doc-update PR?. \n\n(I have to admit it is sad to flannel going, especially because I was really happy with my setup with wireguard.)\n\nIt isn't going away. Certainly not. If you have noticed, some decent work related to Windows platform has been done recently.\nIt is somewhat true that we are running thin on having people work full time on this project (at the time of this writing, to the knowledge of this writer). Even then, it is up to the community if any major work needs to be brought about.\nIf you have a fix, or a feature, please post a PR and the action taken on the PR will be the proof of the pudding :).\nYour enthusiasm is needed. Please file issues if you find something you are stuck on. Its alive and still kicking.\n(And close this issue when you are convinced either way :)).. /lgtm. @mrunalp Can you take a look and review? Thanks.. > Is there anything else to do? Can we merge this?\n@drpaneas I think we should maintain a kube-flannel.yaml for older versions. Do you mind doing that within this PR? Call the old one old-kube-flannel.yaml or pre198-kube-flannel.yaml  or something, I will update the Documentation indicating on the file to be used for older deployments. Thanks.. Missed that.\n/lgtm\nThanks.. Does v0.11.0 work? And the master? Just to bisect the target.. Did you still use the kube-flannel.yml from documentation? To deploy using daemonset?. The day that was inevitable is here: a bot changing code and putting up PRs. At this point it is cute, of course.\n/lgtm Thanks.. Create a gateway node or set of nodes which can reverse proxy layer 4 traffic. You will be implementing the service type 'load balancer' functionality. You could use macvlan on these machines.\nOr if the infrastructure is bare metal, then see if metallb (https://metallb.universe.tf) is what you need.. Closing this issue. Please re-open if needed.. The master kube-flannel.yml works for me (version 1.13.3) . May I know what version of kubernetes are you using?\nThere is known backward in-compatibility with versions prior to 1.9.8, for which you should use https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-old.yaml\n. I also see this from above comments:\nUser \"system:node:test\" cannot get resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope..\nCan you convey the output of kubectl config current-context? It could just be the wrong context when you are running the 'create -f' command.\n. Similar to #1103 ? Seems so. Please re-open this if you think it is not a duplicate.. Can you map corresponding cpu consumption for destination node? Just suspecting.. This function should be made common between linux/windows. The list of routes malloc can be part of the New/Register function itself.. A better name here possibly? 'routes'?. @thxCode Can you please take care of this feedback comment? This is ready to merge.. nit: should vsid be called vnid instead?. Just a question. How was it even working with ConjureMAC before? Not ARP right? Or was the MAC manufactured and dropped on the interface?. One would have hoped that the MAC can be learned through ARP. But I assume this is for added security.. PSP is still beta(?). Nevertheless I presume this will be a paper-over if we run a backward compatible kubernetes without PSP reconciler. Which may mean that we run quite underprivileged. Do we want to document the compatibility?. I suspect we need SYS_ADMIN because we mount /run and write to it (though I would agree with #1090 that we should be more specific). But a real test will prove it.\nI am in favour of bumping e2e to updated kube version (PR please?).\nThis PR is good, but we need some hint about the compatibility - what would be the oldest supported Kube version with this merged in?. ",
    "grepory": "How's that?\n. ",
    "danielschonfeld": "vxlan backend but just to make sure.... is the distinction is all about doing the following?\n/usr/bin/etcdctl set /coreos.com/network/config '{\"Network\":\"10.244.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}}'\n. you said to let them expire, what signifies them expiring? like how would I know it has expired\n. Before i did this, i checked all entires that use flannel.1 in ip -s neighbor list were showing STALE. This is what I got:\n```\n$ ping -c 1 10.244.75.12\nPING 10.244.75.12 (10.244.75.12) 56(84) bytes of data.\n--- 10.244.75.12 ping statistics ---\n1 packets transmitted, 0 received, 100% packet loss, time 0ms\n```\nAnd tailing journalctl -u flannel gave me these new entries after i pinged:\nMay 14 22:15:43 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:15:43.342992 00001 vxlan.go:268] L2 miss: 36:55:bc:7d:22:26\nMay 14 22:15:43 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:15:43.343242 00001 device.go:216] calling NeighAdd: 10.240.219.115, 36:55:bc:7d:22:26\nMay 14 22:15:43 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:15:43.343484 00001 vxlan.go:279] AddL2 succeeded\nMay 14 22:15:51 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:15:51.362256 00001 vxlan.go:263] Ignoring not a miss: 36:55:bc:7d:22:26, 10.244.75.12\nMay 14 22:15:52 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:15:52.364337 00001 vxlan.go:263] Ignoring not a miss: 36:55:bc:7d:22:26, 10.244.75.12\nMay 14 22:15:53 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:15:53.366234 00001 vxlan.go:263] Ignoring not a miss: 36:55:bc:7d:22:26, 10.244.75.12\n. Running sudo ip -s neigh flush dev flannel.1 and then verifying using ip -s neighbor list shows all entries pertaining to flannel.1 as FAILED.\nI get the following ping results: (notice this time how odd it is, the packet actually goes through the first time... but I've only waited 5 minutes so i'm not sure what to make out of it)\n```\n$ ping -c 1 10.244.75.12\nPING 10.244.75.12 (10.244.75.12) 56(84) bytes of data.\n64 bytes from 10.244.75.12: icmp_seq=1 ttl=63 time=2.49 ms\n--- 10.244.75.12 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 2.498/2.498/2.498/0.000 ms\n```\nAnd journalctl -f -u flannel adds the following lines:\nMay 14 22:20:21 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:20:21.801983 00001 vxlan.go:284] L3 miss: 10.244.75.12\nMay 14 22:20:21 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:20:21.802235 00001 device.go:242] calling NeighSet: 10.244.75.12, 36:55:bc:7d:22:26\nMay 14 22:20:21 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:20:21.802479 00001 vxlan.go:295] AddL3 succeeded\nMay 14 22:20:39 node1.c.my-project.internal sdnotify-proxy[725]: I0514 22:20:39.556219 00001 vxlan.go:263] Ignoring not a miss: 36:55:bc:7d:22:26, 10.244.75.12\n. In the mean time, will downgrading to a different kernel solve this issue?\n. @eyakubovich @MohdAhmad how would I know what CoreOS release this solution is included in?\n. +1 - we use Azure and could make use of this\n. @philips how can I help?\nI am not sure, but I think the current go library for Azure doesn't support the necessary abilities to make flannel achieve this.\n. I should also note that we've run into problems when setting static routes with static IPs on Azure.  We're trying to address it with Azure Tech Support, but so far it's been like banging a head into a wall.\nI am not certain however, if flannel will run into the same problems as I am not sure if its caused only because of the use of static IPs for the VMs or the use of static routes.  If it's the latter that's causing the issues we're seeing, then you can expect flannel running into the same problem.\nThe problem being that once you're committed to using this set up, if you add, stop, pause, remove VMs and/or change IPs on VMs defined in the static route table, the whole network freezes for the duration of the operation. When I say freezes, I mean ALLLLLL VMs, not just the one you're updating.\n. ",
    "yanyixing": "@eyakubovich ,does this bug fixed ,i have got this problem\n. ",
    "bobbyrullo": "a nit, and a question, otherwise LGTM.\n. remove unused code\n. What is \"f\"? Also, why run this in a goroutine? Is this just so that it exits out quickly if the requester cancels?\n. ",
    "greenpau": "@eyakubovich, for some reason PIMv2 hello do not show up on the other end of flannel0:\n03:33:21.654439 IP 10.252.46.0 > 224.0.0.13: PIMv2, Hello, length 10\n03:33:21.654477 IP 10.252.46.0 > 10.252.46.0: ICMP net 224.0.0.13 unreachable, length 36\nIs there something in flanneld that filters multicast?\n. ```\nip addr show dev flannel0\n265: flannel0:  mtu 8973 qdisc pfifo_fast state UNKNOWN qlen 500\n    link/none\n    inet 10.252.63.0/16 scope global flannel0\n       valid_lft forever preferred_lft forever\nip addr show dev docker0\n8: docker0:  mtu 8973 qdisc noqueue state DOWN\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 10.252.63.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\netcdctl ls /coreos.com/network/subnets\n/coreos.com/network/subnets/10.252.63.0-24\n/coreos.com/network/subnets/10.252.93.0-24\ntshark -i flannel0\n1   0.000000  10.252.93.0 -> 224.0.0.13   PIMv2 30 Hello\n1   2   6.071538  10.252.63.0 -> 224.0.0.13   PIMv2 30 Hello\n```\n. >  I would like to have multicast working if possible.\n@qrpike , my suggesting is moving to OVN/OVS for multicast.. @eyakubovich , like this?\nAfter=flanneld.service\nRequires=flanneld.service\n. merely changing the docker service unit did not help. The docker still has its old IP and thus failing.\nI had to run the following command each time I made a change to my etcd config.\nip link set dev docker0 down\nbrctl delbr docker0\n. > @eyakubovich , With flannel, all your hosts use the same etcd cluster (so they all see the same key space) and use the same network configuration. \nUnderstood. I will have a single configuration that looks like:\netcdctl rm /coreos.com/network/ --recursive\netcdctl mk /coreos.com/network/config '{\n    \"Network\": \"10.252.0.0/16\",\n    \"SubnetLen\": 24,\n    \"Backend\": {\n        \"Type\": \"udp\",\n        \"Port\": 7890\n    }\n}'\nIs FLANNEL_ETCD_KEY decommissioned?\nHow does Host A know not to issue an overlapping IP address on Host B? \n. @eyakubovich , is there a deterministic way to assign subnets to hosting nodes?\n. @eyakubovich , changing FLANNEL_ETCD_KEY to FLANNEL_ETCD_PREFIX results in:\n```\nsystemctl status flanneld\nflanneld.service - Flanneld overlay address etcd agent\n   Loaded: loaded (/etc/systemd/system/flanneld.service; enabled)\n   Active: failed (Result: timeout) since Tue 2015-06-02 17:47:25 UTC; 1min 2s ago\n Main PID: 20131\nJun 02 17:47:18 ip-192-168-16-146 flanneld[20131]: E0602 17:47:18.108957 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:19 ip-192-168-16-146 flanneld[20131]: E0602 17:47:19.109649 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:20 ip-192-168-16-146 flanneld[20131]: E0602 17:47:20.110394 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:21 ip-192-168-16-146 flanneld[20131]: E0602 17:47:21.111167 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:22 ip-192-168-16-146 flanneld[20131]: E0602 17:47:22.111920 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:23 ip-192-168-16-146 flanneld[20131]: E0602 17:47:23.112632 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:24 ip-192-168-16-146 flanneld[20131]: E0602 17:47:24.113395 20131 main.go:151] Failed to create SubnetManager: 100: Key not foun...fig) [5]\nJun 02 17:47:25 ip-192-168-16-146 systemd[1]: flanneld.service operation timed out. Terminating.\nJun 02 17:47:25 ip-192-168-16-146 systemd[1]: Failed to start Flanneld overlay address etcd agent.\nJun 02 17:47:25 ip-192-168-16-146 systemd[1]: Unit flanneld.service entered failed state.\nHint: Some lines were ellipsized, use -l to show in full.\n\n```\n. > @greenpau , is there a deterministic way to assign subnets to hosting nodes?\nPerhaps the best way is to set them in /run/flannel/docker and /run/flannel/subnet.env.\n. > @eyakubovich , It should be FLANNELD_ETCD_PREFIX\nwill try it out.\n\n@eyakubovich May I ask why you'd like to have a known subnet?\n\nSure. In my use case, application is configured to use IP addresses and not tied to DNS at all. This, reservation would help.\nPersonally, I am not too excited to have overlapping subnets, e.g. 10.252.42.0/24 and  (10.252.0.0/16 with gateway being 10.252.42.0). There are kernel checks that might fail because of kernel might think that 10.252.42.0 is the network ID of 10.252.42.0/24.\nI would rather have flannel%dinterface set to the last IP in the range, i.e. 10.252.42.254.\n. @eyakubovich , I understand the logic. However, given that you are defaulting on /24 on docker bridge, it seems highly unlikely that docker will lease .254 address. \n. there will not no match:\nreturn (addr & htonl(0xff000000)) == htonl(0x00000000);\ndisregard, closing.\n. re-opening.\nAdded static route to send all multicast to flannel0 interface:\n```\nip route show to match 224.0.0.13\ndefault via 192.168.16.1 dev eth0\n224.0.0.0/4 dev flannel0  scope link\n\n```\nNevertheless, PIMv2 Hello messages do not reach the other side because they are being dropped by the kernel.  \n3  30.030581  10.252.63.0 -> 224.0.0.13   PIMv2 30 Hello\n4  30.030616  10.252.63.0 -> 10.252.63.0  ICMP 56 Destination unreachable (Network unreachable)\nThe id in PIMv2 Hello and ICMP Destination unreachable match. \nThe ENETUNREACH - The network of the given addr isn\u2019t reachable from this host.\nThere are two options. It is triggered by either fib_lookup() fails or ipmr_rule_action()\n. perhaps, it is related to flanneld's code in flannel/backend/udp/proxy.c:\n./backend/udp/proxy.c:static void send_net_unreachable(int tun, char *offender) {\n./backend/udp/proxy.c:      send_net_unreachable(tun, buf);\n. It is likely that the packet to 224.0.0.13 is not matched by find_route() function:\nnext_hop = find_route((in_addr_t) iph->daddr);\n        if( !next_hop ) {\n                send_net_unreachable(tun, buf);\n                goto _active;\n        }\n. tun_to_udp() sends exactly one packet to next_hop. \nsock_send_packet(sock, buf, pktlen, next_hop);\nFor PIMv2 adjacency to work, it must send that packet to all peers.\n. let's try adding extra logging:\nif( !next_hop ) {\n                log_error(\"No next hop for %s\\n\", inet_ntoa(*(struct in_addr *)&iph->daddr));\n                send_net_unreachable(tun, buf);\n                goto _active;\n        }\nAfter adding the above log line, flanneld reports:\n```\njournalctl -u flanneld --reverse\n-- Logs begin at Wed 2015-06-03 16:25:07 UTC, end at Fri 2015-06-05 15:50:17 UTC. --\nJun 05 15:50:17 ip-192-168-16-146.inf.ise.com flanneld[11216]: No next hop for 224.0.0.13\nJun 05 15:49:48 ip-192-168-16-146.inf.ise.com flanneld[11216]: No next hop for 224.0.0.13\n```\nthat's progress :+1: \n. keeping this open to submit PR for multicast.\n. > @greenpau flannel does not support multicast. See my response in #179 for details.\n@eyakubovich , I am getting up to speed with Go and remembering some C while learning your code.\nThere are some great techniques in it!\nI was able to exchange multicast PIMv2 Hello between flannel peers. (I failed to create adjacency though). My goal is to watch/intercept local multicast traffic for IGMP membership reports from flannel peers and maintain a table with peer-subscription relationships.\nCurrently, I am using http://weave.works/ for overlay and it supports multicast. However, I would like to make flannel work with multicast because, in my humble opinion, flannel's overlay is better implemented. \nP.S. pointer math with route reallocation code hurts :+1: \n. @lemenkov, I included docker0 and flannel0 interfaces to pimd PIM-SM daemon .\nI want every node on my flannel network to receive packets destined to 224.0.0./24\nit definitely requires code change, e.g. modified tun_to_udp() in proxy.c:\n```\n        iph = (struct iphdr *)buf;\n    if ( ( ntohl(iph->daddr) & 0xffffff00) == 0xe0000000 ) {\n            iph->ttl++;\n            for (i = 0; i < peers_cnt; i++)  {\n                    sock_send_packet(sock, buf, pktlen, &peers[i]);\n                    log_error(\"local multicast packet from %s to %s was sent to TBD (%d)\\n\",\n                            inaddr_str(iph->saddr, saddr, sizeof(saddr)),\n                            inaddr_str(iph->daddr, daddr, sizeof(daddr)), i);\n            }\n    } else if ( ( ntohl(iph->daddr) & 0xf0000000) == 0xe0000000 ) {\n            log_error(\"detected multicast packet destined for %s, dropping ...\\n\",\n                            inet_ntoa(*(struct in_addr *)&iph->daddr));\n            send_net_unreachable(tun, buf);\n            goto _active;\n    } else {\n            /* log_error(\"%s is not a multicast destination\\n\",\n                            inet_ntoa(*(struct in_addr *)&iph->daddr)); */\n\n            next_hop = find_route((in_addr_t) iph->daddr);\n\n```\nWhen I receive a  packet destined to 224.0.0.24, I increment TTL, because they have TTL=1.\nThen, I unicast that packet to all my flannel peers.\nFor now, I do not forward non-local multicast.\n. Receiving sending local multicast.\nJun 08 12:56:22 ip-192-168-16-147.inf.ise.com flanneld[18471]: sent local multicast packet from 10.252.93.0 to 224.0.0.1 via 192.168.16.146\nJun 08 12:56:24 ip-192-168-16-147.inf.ise.com flanneld[18471]: sent local multicast packet from 10.252.93.0 to 224.0.0.22 via 192.168.16.146\nJun 08 12:56:24 ip-192-168-16-147.inf.ise.com flanneld[18471]: sent local multicast packet from 10.252.93.0 to 224.0.0.13 via 192.168.16.146\nJun 08 12:56:25 ip-192-168-16-147.inf.ise.com flanneld[18471]: sent local multicast packet from 10.252.93.0 to 224.0.0.2 via 192.168.16.146\nJun 08 12:56:26 ip-192-168-16-147.inf.ise.com flanneld[18471]: received packet for 224.0.0.1 from 10.252.63.0 via 192.168.16.146\nJun 08 12:56:27 ip-192-168-16-147.inf.ise.com flanneld[18471]: received packet for 224.0.0.22 from 10.252.63.0 via 192.168.16.146\nJun 08 12:56:30 ip-192-168-16-147.inf.ise.com flanneld[18471]: received packet for 224.0.0.13 from 10.252.63.0 via 192.168.16.146\nJun 08 12:56:34 ip-192-168-16-147.inf.ise.com flanneld[18471]: received packet for 224.0.0.2 from 10.252.63.0 via 192.168.16.146\nHowever, on the receipt, the packets are not getting to pimd. See https://github.com/troglobit/pimd/issues/49\n. update: the adjacency was successfully formed with 669db13\n. \u043d\u0443 \u0438 \u043d\u0430\u0432\u0435\u0440\u043d\u043e\u0435 \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0439\u0442\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u0438\u0439 :smile_cat: \n. > @greenpau do you have solve multicast between containers running on two different nodes?\n\nI have tested with your build and it doesn't work, so i am wonder where does the problem come from.\n\n@erandu , as far as I remember I did solve it. There are two things you should consider:\n1. must have multicast daemon running\n2. must have iptables NAT/mangle rules. Consider the following, when mutlticast packets hit the remote host (not a container), the packet is handled by the kernel network stack. You need to tell the kernel to send the mutlicast packets to container or containers. hint: iptables -t mangle\n. > it would be nice to put this into a hacking doc something like we have for rkt: \n\nCould you do that and create a CentOS section?\n\n@philips , in which file do you want the CentOS section? Are you asking me to create Documentation/hacking.md in this repo?\nI have more materials for the doc. I just need to dig them out.\n. where:\n- 10.252.93.0 remote subnet\n- 192.168.16.147 remote peer public ip\n. @eyakubovich , I was running ./test first (before build). The test required Cover, but the test failed without $GOPATH. Once I had it, I was able to get Cover and run test.\n. Perhaps it is better to add the following code to test:\n``\ncddirname $0` || exit 1;\nif [ -z \"${GOPATH+x}\" ]; then\n        export GOPATH=${PWD}/gopath\nfi\nif [ -z \"${GOBIN+x}\" ]; then\n        export GOBIN=${PWD}/bin\nfi\ngo get golang.org/x/tools/cmd/cover\n```\nThis way ${PWD} points to source code directory.\n. ",
    "vascofg": "Any updates on this?. ",
    "qrpike": "Are there any updates on this? I would like to have multicast working if possible. I'm using host-gw currently, but have to run the containers with host network to be able to receive the multicast packets. Is there a simple IP tables rule or something to forward the packets to the container/flannel?. Did anyone solve this issue or know the configuration needed to do so? I also have a private and public network and would like to do the same.\nThanks,. @tomdee So I can do this incrementally, one host at a time? If so I will attempt it during low traffic hours. I will try to document the process and list any issues experienced.\nThanks again for the info. So I performed the upgrade last night. I had to set the version to 0.9.1 since I was on 0.8 which didnt actually support DirectRouting. Once I had them all upgraded I tried upgrading 1 host individually which didn't work ( containers couldn't access others where directrouting wasn't enabled yet. ). \nWith all services/containers running, I restarted flanneld on all nodes at once ( which causes docker to restart, and all containers to restart ). However once they came back online, everything worked as expected. \nSo I guess takeaways are: Must update all nodes at one time, there will be minimal downtime for containers to restart. Must be on version 0.9+. Any updates when this will be in a build? This is very useful for vxlan's using small MTUs :) . @wujiandong @wangyaliyali I put in an issue into CNI ( https://github.com/containernetworking/cni/issues/532 )\nThis is fixed in master, however we are just waiting for a release. see:\n\nhttps://github.com/containernetworking/plugins/issues/102#issuecomment-355141392\nhttps://github.com/containernetworking/plugins/issues/59\n. \n",
    "qihuagao": "Thank you for response.\nI tried this:\n ifconfig docker0 down\n brctl delbr docker0\nand then start docker after flanneld finish start.\nIt seems working, so I guess your analysis should be right. but not sure whether it can solve all occasions. \n. ",
    "CocaCola183": "@qihuagao save my life. @tomdee Thanks for response.\nI thought my system support vxlan after read this: http://docs.cloudstack.apache.org/en/latest/networking/vxlan.html#check-the-capability-of-your-system\nI think flannel should give some more advise about Kernel version, there is no specific error info when I use, after all.. ",
    "akamalov": "Thanks Eugene. Indeed, once readonly was changed to on, it worked! \nThanks again,\nAlex\n. Thanks Eugene. I'll give it a shot and will keep you posted.\nAlex\nOn Mon, Jun 1, 2015 at 2:06 PM, Eugene Yakubovich notifications@github.com\nwrote:\n\n@akamalov https://github.com/akamalov The problem seems to be from\nquotes in -etcd-endpoints=\"http://127.0.0.1:2379\"\n. Try removing the quotes: -etcd-endpoints=http://127.0.0.1:2379. I admit\nthat it's quite silly but the argument parsing library of Go seems to be\nbehaving this way.\nFew other points though. While you can certainly place flannel into /opt,\nyou can also use flannel that \"ships\" with CoreOS (it actually gets pulled\ndown in a container on use). Please see\nhttps://coreos.com/docs/cluster-management/setup/flannel-config/\nYou can also avoid ExecStartPost polling by changing unit type to\n\"notify\". flannel will detect systemd and signal that it is ready just\nafter it has finished writing out the subnet.env file. Just put\nType=notify into [Service] section.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/192#issuecomment-107657355.\n. After using built-in flannel, the issue was resolved. Thanks Eugene!\n\nAlex\n. Using built-in flannel resolved the issue.\n. I am seconding this behavior. Changed the network segment to something other than it was in /coreos.com/network/config, restarted flanneld and it is still using the old network segment. I am using  CoreOS 695.0.0. \nDisplay changed values in /coreos.com/network/config:\nwwserver60 bin # etcdctl get /coreos.com/network/config\n{ \"Network\": \"10.100.0.0/16\",\"SubnetLen\":20,\"SubnetMin\":\"10.100.10.0\",\"SubnetMax\":\"10.100.99.0\",\"Backend\":{\"Type\":\"vxlan\",\"Port\":7890 }}\nwwserver60 bin #\nNote: I am moving network segment from 11.0.0.0/8 to 10.100.0.0/16.\n- Restarted flanneld\n- Displaying interface on flannel.1 interface:\n```\nwwserver60 bin # ifconfig flannel.1\nflannel.1: flags=4163  mtu 1450\n        inet 11.13.64.0  netmask 255.255.0.0  broadcast 0.0.0.0\n        inet6 fe80::dc32:82ff:fedb:3bf8  prefixlen 64  scopeid 0x20\n        ether de:32:82:db:3b:f8  txqueuelen 0  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 49 overruns 0  carrier 0  collisions 0\nwwserver60 bin # \n```\nAs you can see, interface flannel.1 is retaining old network segment IP.\nIs it possible to expedite this ?\nThanks a ton.\nAlex\n. Thanks Eugene. I am deploying k8s environment on top of CoreOS, and right now I am dead in the water without this fix.\nThanks again,\nAlex\n. Eugene, does it mean the fix is coming in next release?\nThanks,\nAlex\nOn Mon, Jun 22, 2015 at 8:31 PM, Eugene Yakubovich <notifications@github.com\n\nwrote:\nClosed #196 https://github.com/coreos/flannel/issues/196 via #221\nhttps://github.com/coreos/flannel/pull/221.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/196#event-337308274.\n. \n",
    "micahhausler": "Yea you are right. It would be better to just use separate routing tables.\n. ",
    "panga": "+1\n. +1\n. +1\n. Duplicate of #509\n. ",
    "lokesh-shekar": "Any updates on this ?. Is someone working on this feature ?\n. thanks. This PR merge will help us greatly. This PR merge will help us greatly. ",
    "suyogbarve": "Is there a temporary work-around to get the new ip range?\n. ",
    "goacid": "Regarding reserving subnet, I think it could be a huge improvment.\nI've some applications running in docker which are sticked to IP addresses, and each change means full reconfiguration, downtime and so on... \nSo if I can reserve a subnet per host, and also fix some IP per VM, it will fix all my troubles.\n. +1\n. ",
    "erandu": "@greenpau do you have solve multicast between containers running on two different nodes?\nI have tested  with your build and it doesn't work, so i am wonder where does the problem come from.\n. ",
    "sanjana-bhat": "@eyakubovich, thanks for a quick turnaround. After you pointed me to the tagged versions of flannel, I tried testing with v0.4.1. The containers still can't communicate with each other and hostB crashed. I am pasting some of the logs that can be useful. \n```\nhostA\nip addr\n4: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN \n    link/ether aa:3a:df:64:21:7b brd ff:ff:ff:ff:ff:ff\n    inet 192.168.16.0/8 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a83a:dfff:fe64:217b/64 scope link \n       valid_lft forever preferred_lft forever\n5: docker0:  mtu 1450 qdisc noqueue state UP \n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.16.1/20 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link \n       valid_lft forever preferred_lft forever\n7: veth7b0acef:  mtu 1450 qdisc noqueue master docker0 state UP \n    link/ether da:c8:e8:e9:5b:58 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::d8c8:e8ff:fee9:5b58/64 scope link \n       valid_lft forever preferred_lft forever\nbridge fdb\n56:84:7a:fe:97:99 dev docker0 vlan 0 permanent\n02:42:c0:a8:10:02 dev veth7b0acef vlan 0 \nda:c8:e8:e9:5b:58 dev veth7b0acef vlan 0 permanent\n33:33:00:00:00:01 dev veth7b0acef self permanent\n01:00:5e:00:00:01 dev veth7b0acef self permanent\n33:33:ff:e9:5b:58 dev veth7b0acef self permanent\nnetstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n192.0.0.0       0.0.0.0         255.0.0.0       U         0 0          0 flannel.1\n192.168.16.0    0.0.0.0         255.255.240.0   U         0 0          0 docker0\narp -n\nAddress                  HWtype  HWaddress           Flags Mask            Iface\n192.168.48.2             ether   96:b2:50:13:77:af   C                     flannel.1\n192.168.16.2             ether   02:42:c0:a8:10:02   C                     docker0\ndmesg\n[ 1734.829454] bio: create slab  at 2\n[ 1739.157116] Bridge firewalling registered\n[ 1739.159679] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready\n[ 1739.212087] ip_tables: (C) 2000-2006 Netfilter Core Team\n[ 1739.247599] nf_conntrack version 0.5.0 (16384 buckets, 65536 max)\n[ 2232.255317] bio: create slab  at 0\n[ 2232.284070] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready\n[ 2827.451375] bio: create slab  at 2\n[ 2827.473160] EXT4-fs (dm-1): mounted filesystem with ordered data mode. Opts: discard\n[ 2827.599642] device veth7b0acef entered promiscuous mode\n[ 2827.603603] IPv6: ADDRCONF(NETDEV_UP): veth7b0acef: link is not ready\n[ 2827.627762] IPv6: ADDRCONF(NETDEV_CHANGE): veth7b0acef: link becomes ready\n[ 2827.628855] docker0: port 1(veth7b0acef) entered forwarding state\n[ 2827.629810] docker0: port 1(veth7b0acef) entered forwarding state\n[ 2827.630919] IPv6: ADDRCONF(NETDEV_CHANGE): docker0: link becomes ready\n[ 2842.656038] docker0: port 1(veth7b0acef) entered forwarding state\n[ 2852.048262] nr_pdflush_threads exported in /proc is scheduled for removal\n[ 2852.049732] sysctl: The scan_unevictable_pages sysctl/node-interface has been disabled for lack of a legitimate use case.  If you have one, please send an email to linux-mm@kvack.org.\n[ 3228.165128] device flannel.1 entered promiscuous mode\n[ 3362.882239] device eth0 entered promiscuous mode\nsudo tcpdump -i flannel.1 \ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel.1, link-type EN10MB (Ethernet), capture size 65535 bytes\n01:43:21.979052 IP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 43, length 64\n01:43:22.979017 IP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 44, length 64\n01:43:22.985855 ARP, Request who-has 192.168.48.2 tell localhost, length 28\n01:43:23.979087 IP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 45, length 64\n01:43:23.988019 ARP, Request who-has 192.168.48.2 tell localhost, length 28\n01:43:24.979992 IP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 46, length 64\n01:43:24.989851 ARP, Request who-has 192.168.48.2 tell localhost, length 28\nsudo tcpdump -i eth0 | grep 192.168.48.2\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 2, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 3, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 4, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 5, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 6, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 7, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 8, length 64\nIP localhost > 192.168.48.2: ICMP echo request, id 52753, seq 9, length 64\ntail flannel.log \ngo-etcd2015/06/08 01:42:56 DEBUG: recv.response.from \nI0608 01:43:17.466205 10235 vxlan.go:263] Ignoring not a miss: 96:b2:50:13:77:af, 192.168.48.2\nI0608 01:43:26.980657 10235 vxlan.go:284] L3 miss: 192.168.48.2\nI0608 01:43:26.980918 10235 device.go:242] calling NeighSet: 192.168.48.2, 96:b2:50:13:77:af\nI0608 01:43:26.981221 10235 vxlan.go:295] AddL3 succeeded\nI0608 01:44:04.442223 10235 vxlan.go:263] Ignoring not a miss: 96:b2:50:13:77:af, 192.168.48.2\nI0608 01:44:13.980232 10235 vxlan.go:284] L3 miss: 192.168.48.2\nI0608 01:44:13.980346 10235 device.go:242] calling NeighSet: 192.168.48.2, 96:b2:50:13:77:af\nI0608 01:44:13.980583 10235 vxlan.go:295] AddL3 succeeded\nI0608 01:44:51.418171 10235 vxlan.go:263] Ignoring not a miss: 96:b2:50:13:77:af, 192.168.48.2\ngo-etcd2015/06/08 01:47:56 DEBUG: rawWatch /coreos.com/network/subnets []\ngo-etcd2015/06/08 01:47:56 DEBUG: get /coreos.com/network/subnets [http://localhost:4001]\ngo-etcd2015/06/08 01:47:56 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?recursive=true&wait=true&waitIndex=238194\ngo-etcd2015/06/08 01:47:56 DEBUG: send.request.to http://localhost:4001/v2/keys/coreos.com/network/subnets?recursive=true&wait=true&waitIndex=238194 | method GET\ngo-etcd2015/06/08 01:47:56 DEBUG: recv.response.from \n```\n```\nhostB\nip addr\n4: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN \n    link/ether 96:b2:50:13:77:af brd ff:ff:ff:ff:ff:ff\n    inet 192.168.48.0/8 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::94b2:50ff:fe13:77af/64 scope link \n       valid_lft forever preferred_lft forever\n5: docker0:  mtu 1450 qdisc noqueue state UP \n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.48.1/20 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link \n       valid_lft forever preferred_lft forever\n7: veth2e03f10:  mtu 1450 qdisc noqueue master docker0 state UP \n    link/ether f6:01:48:60:59:c1 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::f401:48ff:fe60:59c1/64 scope link \n       valid_lft forever preferred_lft forever\nbridge fdb\nf6:01:48:60:59:c1 dev veth2e03f10 vlan 0 permanent\n56:84:7a:fe:97:99 dev docker0 vlan 0 permanent\n02:42:c0:a8:30:02 dev veth2e03f10 vlan 0 \n33:33:00:00:00:01 dev veth2e03f10 self permanent\n01:00:5e:00:00:01 dev veth2e03f10 self permanent\n33:33:ff:60:59:c1 dev veth2e03f10 self permanent\nnetstat -rn\nKernel IP routing table\nDestination     Gateway         Genmask         Flags   MSS Window  irtt Iface\n192.0.0.0       0.0.0.0         255.0.0.0       U         0 0          0 flannel.1\n192.168.48.0    0.0.0.0         255.255.240.0   U         0 0          0 docker0\narp -n\nAddress                  HWtype  HWaddress           Flags Mask            Iface\n192.168.48.2             ether   02:42:c0:a8:30:02   C                     docker0\ndmesg\n[ 2023.021750] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready\n[ 2023.056454] ip_tables: (C) 2000-2006 Netfilter Core Team\n[ 2023.078099] nf_conntrack version 0.5.0 (16384 buckets, 65536 max)\n[ 2817.203067] bio: create slab  at 0\n[ 2817.308500] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready\n[ 2999.115571] bio: create slab  at 2\n[ 2999.140853] EXT4-fs (dm-1): mounted filesystem with ordered data mode. Opts: discard\n[ 3012.028012] [sched_delayed] sched: RT throttling activated\n[ 3120.104145] device eth0 entered promiscuous mode\n[ 3290.905691] bio: create slab  at 2\n[ 3290.951982] EXT4-fs (dm-1): mounted filesystem with ordered data mode. Opts: discard\n[ 3291.153589] device veth2e03f10 entered promiscuous mode\n[ 3291.155377] IPv6: ADDRCONF(NETDEV_UP): veth2e03f10: link is not ready\n[ 3291.176441] IPv6: ADDRCONF(NETDEV_CHANGE): veth2e03f10: link becomes ready\n[ 3291.178456] docker0: port 1(veth2e03f10) entered forwarding state\n[ 3291.180075] docker0: port 1(veth2e03f10) entered forwarding state\n[ 3291.181848] IPv6: ADDRCONF(NETDEV_CHANGE): docker0: link becomes ready\n[ 3306.208065] docker0: port 1(veth2e03f10) entered forwarding state\n[ 3316.408368] nr_pdflush_threads exported in /proc is scheduled for removal\n[ 3316.409822] sysctl: The scan_unevictable_pages sysctl/node-interface has been disabled for lack of a legitimate use case.  If you have one, please send an email to linux-mm@kvack.org.\nsudo tcpdump -i flannel.1\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel.1, link-type EN10MB (Ethernet), capture size 65535 bytes\n01:42:41.712204 IP 192.168.16.0 > 192.168.48.2: ICMP echo request, id 52753, seq 2, length 64\n01:42:41.739851 IP 192.168.48.2 > 192.168.16.0: ICMP echo reply, id 52753, seq 2, length 64\nsudo tcpdump -i eth0 | grep 192.168.48.2\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\nIP 192.168.16.0 > 192.168.48.2: ICMP echo request, id 52753, seq 2, length 64\ntail -f flannel.log \nI0608 01:42:41.712579 10628 vxlan.go:284] L3 miss: 192.168.16.0\nI0608 01:42:41.732656 10628 device.go:242] calling NeighSet: 192.168.16.0, aa:3a:df:64:21:7b\ntail -f /var/log/all\nJun  8 01:42:41 localhost kernel: BUG: unable to handle kernel NULL pointer dereference at 0000000000000046\n```\nI don't see any entry for flannel.1 in the output of bridge fdb. That shouldn't be the case right? /var/log/all on hostB seem to indicate NULL pointer dereference which may have crashed the system. Any help here is really appreciated.\nThanks!\n. @eyakubovich, I use kernel version 3.10.0, a RHEL7 host. bridge fdb would show me entries for flannel.1 when I built code from git HEAD. It started crashing soon after I switched to v0.4.1. How do I check if I have the VXLAN DOVE extensions that flannel makes use of? \n. I built the code from git HEAD and I now see flannel.1 under bridge fd output. \na2:59:01:6c:5b:ec dev flannel.1 dst 10.88.217.91 self permanent\nWith v0.4.1 it seems to be missing. \n. Great! This is working now. \n@eyakubovich, I will wait for the tag 0.5.0\n. ",
    "resouer": "@eyakubovich It seems my flannel did not reuse the same subnet.\nI'm using DHCP VMs which has fixed IP.\nBefore I restart flanneld:\nroot@ubuntu:/home/vcap# cat subnet.env \nFLANNEL_SUBNET=10.1.33.1/24\nFLANNEL_MTU=1472\nFLANNEL_IPMASQ=false\nAfter I restart the flanneld:\n```\nroot@ubuntu:/home/vcap# sudo -b docker -H unix:///var/run/docker-bootstrap.sock ps \nroot@ubuntu:/home/vcap# CONTAINER ID        IMAGE                     COMMAND                CREATED             STATUS              PORTS               NAMES\nfc346a53985d        wizardcxy/flannel:0.3.0   \"/opt/bin/flanneld -   3 weeks ago         Up 25 minutes                           distracted_pike     \nroot@ubuntu:/home/vcap# sudo docker -H unix:///var/run/docker-bootstrap.sock cp fc346a53985d:/run/flannel/subnet.env .\nroot@ubuntu:/home/vcap# cat subnet.env \nFLANNEL_SUBNET=10.1.10.1/24\nFLANNEL_MTU=1472\nFLANNEL_IPMASQ=false\nroot@ubuntu:/home/vcap# source subnet.env \n```\nThis is how I start flannel in docker:\n```\nsudo docker -H unix:///var/run/docker-bootstrap.sock run -d --net=host --privileged -v /dev/net:/dev/net wizardcxy/flannel:0.3.0 /opt/bin/flanneld --etcd-endpoints=http://${ETCD_IP}:4001 -iface=\"eth0\"\n```\nIf flannel can still use 10.1.33.1, it will save me a lot of time, but now, It changed, then I have to reconfigure my docker opts to 10.1.10.1 to make it work.\nAny tips?\n. ping @eyakubovich any idea about this?\n. @eyakubovich I can not reproduce it now, flannel works well and reuse the subnet.\nIs it possible that because my flannel has been stopped for too long time, after I start it again, it create a new subnet?\n. Thanks, issue closed!\n. @eyakubovich  I don't think start multiple dockerd is good, that will make clients complain...\nMaybe just allocate hosts among the networks? like:\nhost1:red\nhost2:blue\nhost3:red\nAnd it's interesting, then what's multi-network designed for if not for isolating groups? Is there any work-around if I want to create isolated groups for now?\n. @eyakubovich Thanks for your reply. Any issue/pr about OVS backend progress?\nI once used SocketPlane, but I found it's wrapping of OVS is buggy and that's why I turned to Flannel: much steady and easy to integrate with Docker/Kubernetes.\n. The configuration file I used: this link\nBut you can also reproduce this without kubernetes as I mentioned above\n. And disable iptables like this does not help:\niptables -P INPUT ACCEPT\niptables -P OUTPUT ACCEPT\niptables -P FORWARD ACCEPT\niptables -F\n. Oh, yes. I will test it \n. ",
    "sbaks0820": "I want to be able to do it between different mahines, but given the time I have is too much work. I do want to do it on the same host through the docker0 interface. It's probably a better idea to post about this on docker lists.\n. ",
    "jzelinskie": "Is this happening across all of the machines or is this a transient issue?\n. Let me be pedantic and point out that comments should be full sentences\n. ",
    "misakwa": "Its happening across all the machines.\n. I also tried pulling the flannel image manually on one of the boxes and it showed the same error with pulling one of the layers.\n. I'm pulling anonymously.\n. I tried manually with credentials though but still no luck.\n. @eyakubovich, I added the debug drop-in you suggested and this is what I got back from journalctl\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal systemd[1]: Starting Network fabric for containers...\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal etcdctl[2010]: { \"Network\": \"10.1.0.0/16\" }\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"waiting for daemon to initialize\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=info msg=\"+job serveapi(fd://)\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=info msg=\"Listening for HTTP on fd ()\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /commit\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /images/load\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/create\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /exec/{name:.*}/start\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /exec/{name:.*}/resize\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/restart\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/wait\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/resize\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/attach\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/copy\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /auth\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /images/create\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /images/{name:.*}/push\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /images/{name:.*}/tag\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/kill\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/unpause\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /build\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/pause\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/start\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/stop\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/exec\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/rename\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering DELETE, /containers/{name:.*}\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering DELETE, /images/{name:.*}\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering OPTIONS, \"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering GET, /images/get\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/export\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal dockerd[2017]: time=\"2015-06-23T00:39:16Z\" level=fatal msg=\"Shutting down daemon due to errors: pid file found, ensure docker is not running or delete /var/run/early-docker.pid\"\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal systemd[1]: flanneld.service: main process exited, code=exited, status=1/FAILURE\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal systemd[1]: Failed to start Network fabric for containers.\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal systemd[1]: Unit flanneld.service entered failed state.\nJun 23 00:39:16 ip-172-31-36-218.ec2.internal systemd[1]: flanneld.service failed.\n. I manually removed the pid file and then started getting this:\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal systemd[1]: flanneld.service holdoff time over, scheduling restart.\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal systemd[1]: Starting Network fabric for containers...\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal etcdctl[4984]: { \"Network\": \"10.1.0.0/16\" }\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"waiting for daemon to initialize\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"+job serveapi(fd://)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"Listening for HTTP on fd ()\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /info\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/{name:.*}/history\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/{name:.*}/json\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/changes\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/{name:.*}/get\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/export\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/attach/ws\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /events\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /version\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/json\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/get\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/json\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/top\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/logs\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /_ping\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/viz\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /images/search\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/ps\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/json\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /containers/{name:.*}/stats\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering GET, /exec/{id:.*}/json\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /images/create\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/pause\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/restart\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/resize\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/copy\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/exec\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /auth\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /build\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/rename\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /images/{name:.*}/push\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /images/{name:.*}/tag\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/stop\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/wait\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/attach\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /exec/{name:.*}/start\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /commit\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /images/load\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /exec/{name:.*}/resize\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/unpause\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/start\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/create\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering POST, /containers/{name:.*}/kill\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering DELETE, /containers/{name:.*}\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering DELETE, /images/{name:.*}\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Registering OPTIONS, \"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Using graph driver overlay\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Creating images graph\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Restored 0 elements\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Creating repository list\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: No sockets found\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"-job serveapi(fd://) = ERR (1)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=error msg=\"ServeAPI error: No sockets found\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal systemd[1]: Scope libcontainer-4991-systemd-test-default-dependencies.scope has no PIDs. Refusing.\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"Restarting containers...\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"docker daemon: 1.6.2 7c8fca2-dirty; execdriver: native-0.2; graphdriver: overlay\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"+job acceptconnections()\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"-job acceptconnections() = OK (0)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"Daemon has completed initialization\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=debug msg=\"starting clean shutdown of all containers...\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[4991]: time=\"2015-06-23T00:49:09Z\" level=fatal msg=\"Shutting down due to ServeAPI error: No sockets found\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal systemd[1]: flanneld.service: main process exited, code=exited, status=1/FAILURE\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"POST /v1.18/containers/create\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"+job create()\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: No such image: quay.io/coreos/flannel:0.4.0 (tag: 0.4.0)\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"-job create() = ERR (1)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=error msg=\"Handler for POST /containers/create returned error: No such image: quay.io/coreos/flannel:0.4.0 (tag: 0.4.0)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=error msg=\"HTTP Error: statusCode=404 No such image: quay.io/coreos/flannel:0.4.0 (tag: 0.4.0)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal docker[5000]: Unable to find image 'quay.io/coreos/flannel:0.4.0' locally\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"POST /v1.18/images/create?fromImage=quay.io%2Fcoreos%2Fflannel&tag=0.4.0\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"+job pull(quay.io/coreos/flannel, 0.4.0)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"+job resolve_repository(quay.io/coreos/flannel)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal dockerd[589]: time=\"2015-06-23T00:49:09Z\" level=info msg=\"-job resolve_repository(quay.io/coreos/flannel) = OK (0)\"\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal docker[5000]: Pulling repository quay.io/coreos/flannel\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal docker[5000]: 73b8daf50cec: Pulling image (0.4.0) from quay.io/coreos/flannel\nJun 23 00:49:09 ip-172-31-36-218.ec2.internal docker[5000]: 73b8daf50cec: Pulling image (0.4.0) from quay.io/coreos/flannel, endpoint: https://quay.io/v1/\nJun 23 00:49:10 ip-172-31-36-218.ec2.internal docker[5000]: 73b8daf50cec: Pulling dependent layers\nJun 23 00:49:10 ip-172-31-36-218.ec2.internal docker[5000]: 91a6195f52a2: Pulling metadata\nJun 23 00:49:10 ip-172-31-36-218.ec2.internal docker[5000]: 91a6195f52a2: Pulling fs layer\nJun 23 00:49:11 ip-172-31-36-218.ec2.internal docker[5000]: 91a6195f52a2: Error pulling dependent layers\nJun 23 00:49:11 ip-172-31-36-218.ec2.internal docker[5000]: 73b8daf50cec: Error pulling image (0.4.0) from quay.io/coreos/flannel, endpoint: https://quay.io/v1/, Server error: Status 403 while fetching image layer (91a6195f52a2c01646c3c5952c1fcc8e15f9d2a5ac049edb0ec8412a1cd1c6fe)\nJun 23 00:49:11 ip-172-31-36-218.ec2.internal docker[5000]: 73b8daf50cec: Error pulling image (0.4.0) from quay.io/coreos/flannel, Server error: Status 403 while fetching image layer (91a6195f52a2c01646c3c5952c1fcc8e15f9d2a5ac049edb0ec8412a1cd1c6fe)\nJun 23 00:49:11 ip-172-31-36-218.ec2.internal dockerd[589]: Error pulling image (0.4.0) from quay.io/coreos/flannel, Server error: Status 403 while fetching image layer (91a6195f52a2c01646c3c5952c1fcc8e15f9d2a5ac049edb0ec8412a1cd1c6fe)\nJ\n. I get the same errors after reboot.\nI don't know if this is important but I recreated my cluster from scratch so all the instances are completely new.\nI may also have missed this information from the start but I am running my cluster in a vpc with 4-subnets which can all talk to each other - I'm using a security group that allows that.\n. I did create a vpc endpoint with an s3 bucket policy to run a private registry but I'm not using that at the moment. Also I am able to get the cluster running with flannel disabled.\n. @josephschorr: The initial s3 policy I created for my vpc endpoint was restricting access but it works after relaxing the policy (completely removing the endpoint or allowing *)\n@eyakubovich: I was able to get quay-okay from curling the s3 key provided above. \nHowever flanneld is still unable to start.\n. I got this back from systemctl status flanneld.service:\n```\n\u25cf flanneld.service - Network fabric for containers\n   Loaded: loaded (/usr/lib64/systemd/system/flanneld.service; static; vendor preset: disabled)\n  Drop-In: /etc/systemd/system/flanneld.service.d\n           \u2514\u250010-debug.conf, 50-network-config.conf\n   Active: activating (auto-restart) (Result: exit-code) since Tue 2015-06-23 01:36:18 ; 858ms ago\n     Docs: https://github.com/coreos/flannel\n  Process: 2460 ExecStart=/usr/lib/coreos/dockerd --daemon -D --host=fd:// --bridge=none --iptables=false --ip-masq=false --graph=/var/lib/early-docker --pidfile=/var/run/early-docker.pid (code=exited, status=1/FAILURE)\n  Process: 2452 ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config { \"Network\": \"10.1.0.0/16\" } (code=exited, status=0/SUCCESS)\n  Process: 2450 ExecStartPre=/usr/bin/touch /run/flannel/options.env (code=exited, status=0/SUCCESS)\n  Process: 2448 ExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR} (code=exited, status=0/SUCCESS)\n  Process: 2446 ExecStartPre=/usr/bin/mkdir -p /run/flannel (code=exited, status=0/SUCCESS)\n  Process: 2443 ExecStartPre=/sbin/modprobe ip_tables (code=exited, status=0/SUCCESS)\n Main PID: 2460 (code=exited, status=1/FAILURE)\nJun 23 01:36:18 ip-172-31-1-222.ec2.internal systemd[1]: flanneld.service: main process exited, code=exited, status=1/FAILURE\nJun 23 01:36:18 ip-172-31-1-222.ec2.internal systemd[1]: Failed to start Network fabric for containers.\nJun 23 01:36:18 ip-172-31-1-222.ec2.internal systemd[1]: Unit flanneld.service entered failed state.\nJun 23 01:36:18 ip-172-31-1-222.ec2.internal systemd[1]: flanneld.service failed.\n```\n. And btw, I was able to pull the flannel image manually on the node.\n. Weirdly I got the cluster working after destroying and rebuilding with the original cloud config. The only difference is that the vpc endpoint no longer exists and it took a couple of recreations to get it working.\nIt looks like the problem is with having a vpc endpoint that does not allow pulling the flannel image from the s3 endpoint. I'm not sure why it persisted a couple of times after I removed the vpc endpoint but everything seems to be working now.\n. I really appreciate the prompt and consistent response from you all. Great work on the tools btw and thanks again.\n. Let me know when I can close this issue or if there is any more information I can provide that will be useful to others trying to setup a cluster with flannel on ec2.\n. ",
    "josephschorr": "@misakwa Does your VPC allow access to Amazon S3 machines outside of the cluster?\n. @misakwa To clarify: Can the machines trying to pull that image access the public S3 endpoints? When you do a pull from quay.io, we 302 redirect to S3, and if your VPC cannot access those URLs, the pull will fail.\n. ",
    "Nalum": "Thanks, so does flannel use the environment vars FLANNEL_*? e.g. FLANNEL_ETCD_KEYFILE\n. If I get my set up working and have time I'll look at doing so. Where would I add this documentation?\n. ",
    "frank-dspeed": "can some one explain here plz if we can do 2x listen 1x for the 0.0.0.0:8888 and fd:// ?\nhow does the client use the socket activation should the client use then unix:///run/flannel.sock ?\ni dont understand that?\n. ",
    "fcantournet": "You are understanding it wrong =)\nThe Kubernetes services get an IP inside the address space you defined for all Kubernetes services.\nThis address space HAS to be separated from the flannel address space.\nFlannel is used to connect pods between each others.\nService IPs are just used to serve as a constant (over time) IP. Traffic targetting this IP gets routed by the kube-proxy (via IPtable rules set on the host) to the appropriate Pods.\n. Yes exactly.\nFor the external IPs I can't give you a definite answer. This is completely dependent on your setup.\nI have sort of described the way we do it on a POC cluster (5 nodes) in https://github.com/GoogleCloudPlatform/kubernetes/issues/10456\nThat is one way of doing it : No cloud provider, just an external load-balancer (http for us) that routes traffic to the NODES:PORT combo.\nThis node:port combo IPtables re-routing rule is created for every service you declare on every node by the kube-proxy.\nEssentially we use a http load-balancer to get to a node, at which point the traffic follows the same route it would if it came from the inside of the kubernetes cluster.\nYou could however, do something totally different, like an HAProxy with an IP inside the flannel CIDR, and then load balance directly to the pods.\n. ",
    "devurandom": "Ok, so setting\n/coreos.com/network/config '{\"Network\":\"10.1.0.0/16\"}'\nand\n/opt/bin/kube-apiserver --service-cluster-ip-range=10.1.0.0/16\nis wrong?\nIt should be (e.g.)?\n/opt/bin/kube-apiserver --service-cluster-ip-range=10.2.0.0/16\nThen the service-cluster-ip-range can also point to \"external IPs\", i.e. IPs that are globally routed? Is that the recommended way? And if I do that, can I still define internal-only services?\n. Thanks!\n. ",
    "omribahumi": "@eyakubovich we're already using Consul. Maintaining an etcd cluster just for flannel is adding more complexity :)\nI think we can do this in a simple way - a parameter for controlling which kv store to use, having the default set to etcd for maintaining backwards compatibility\n. @eyakubovich could you elaborate a bit more on the TTL issue? Where are TTLs used? What for?\nAlso, could you do the abstraction and we'll just do the Consul implementation?\n. Adding this as reference for TTL on Consul: https://github.com/hashicorp/consul/issues/172\n. ",
    "philk": "I haven't messed with it yet but this seems highly relevant for implementing Consul support and potentially getting Zookeeper for free https://github.com/docker/libkv\n. ",
    "nathanleiby": "Also interested in adding support for Consul. Similarly, we already use consul and don't wish to also run an etcd cluster\n. ",
    "inadarei": "+1 for Consul!\n. ",
    "Azulinho": "+1\n. ",
    "nphase": "+1\n. ",
    "mrinalwadhwa": "+1 would prefer not to have to maintain an etcd cluster just for flannel.\n. ",
    "hubayirp": "+1 on Consul \n. ",
    "fieryvova": "+1 on Consul\n. Never mind. All traffic to a local ethernet interface has been allowed.\nI should've paid more attention to iptables.. ",
    "crhan": "+1 on Consul\n. ",
    "yellowmegaman": "+1 Any love for this issue? Etcd is surely great, but indeed, running it only to make flannel work is an overkill.\n. ",
    "signull": "\ud83d\udc4d . ",
    "claytonsilva": "+1. ",
    "ror6ax": "+1 on Consul\n. ",
    "h4m24": "+1 on consul. ",
    "ppomes": "+1 on consul. ",
    "osterman": "Resolved. Configuration error realated to MachineOf=%p.service. In an effort to document the problem, I uncovered the problem. Thanks for being a sounding board.\n. Getting this as well. Can't confirm or deny it's a flannel issue. Restarting docker \"works\" by it's a pretty disruptive/violent operation. Running CoreOS stable (835.8.0) - docker 1.8.3.\n. Oh, when this happens it's cluster-wide. All docker daemons report the same problem. I have to restart all docker daemons on all cluster nodes.\n. ",
    "xyleth": "So - fresh day, fresh eyes, solved the problem!\nThe problem wasn't actually docker or flannel at all.  For the reference of any future googlers who end up on this page - running docker as eyakubovich described works perfectly.  Either --net=host or a more specific -p HOST IP:HOST PORT:CONTAINER PORT works and enables external traffic to reach the HAProxy instance.\nIf it doesn't seem to be working despite everything being setup correctly be sure to check any firewalls in your environment.  Just because you have SSH access to a docker host doesn't mean you also have HTTP access!  As I found out to the cost of several fruitless hours twiddling Docker when I should have been looking elsewhere o_O\nI have it all working now and it's as brilliant as I expected.  Thanks for the response and I'll try not to be so daft next time.\n. ",
    "mootezbessifi": "@eyakubovich @xyleth according to this post, may you answer my question ? I need some advanced details. ",
    "diegomarangoni": "Looks like when flannel run from inside a docker container it can't auto-discovery aws metadata.\nIf I manually specify AWS environment variables and route table id, like example below, it works fine.\n``` yaml\ncloud-config\nwrite_files:\n- path: /etc/systemd/system/docker.service.d/increase-ulimit.conf\n  owner: core:core\n  permissions: 0644\n  content: |\n    [Service]\n    LimitSIGPENDING=481114\n    LimitNOFILE=65535\n    LimitNPROC=481114\ncoreos:\n  etcd2:\n    advertise-client-urls: http://$private_ipv4:2379\n    initial-advertise-peer-urls: http://$private_ipv4:2380\n    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n    listen-peer-urls: http://$private_ipv4:2380\n    initial-cluster: 8e7efe97ba82470e8d6e26dfbbdc3604=http://10.10.10.127:2380,1c77cb5484ec4542b83493a4dd0fbebe=http://10.10.10.128:2380,da2e027b248f4f63adf3d3efb4238e8d=http://10.10.10.129:2380\n    initial-cluster-state: new\n  fleet:\n    public-ip: $private_ipv4\n    metadata: region=us-west-1,public_ip=$public_ipv4\n  flannel:\n    interface: $private_ipv4\n  units:\n  - name: etcd2.service\n    command: start\n  - name: fleet.service\n    command: start\n  - name: docker-tcp.socket\n    command: start\n    enable: true\n    content: |\n      [Unit]\n      Description=Docker Socket for the API\n  [Socket]\n  ListenStream=2375\n  Service=docker.service\n  BindIPv6Only=both\n\n  [Install]\n  WantedBy=sockets.target\n\n\nname: docker.service\n    command: restart\nname: flanneld.service\n    command: start\n    drop-ins:\nname: 50-network-config.conf\n  content: |\n    [Service]\n    Environment=\"AWS_ACCESS_KEY_ID=MY_KEY_ID\"\n    Environment=\"AWS_SECRET_ACCESS_KEY= MY_KEY_SECRET\"\n    ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ \"Network\": \"10.20.0.0/16\", \"Backend\": { \"Type\": \"aws-vpc\", \"RouteTableID\": \"MY_VPC_ROUTE_TABLE_ID\" } }'\n```\n. Hi @MohdAhmad\n\n\n\nYes, I tried with IAM roles as well but didn't worked.\nWorks perfectly when I set AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and RouteTableID.\nMy guess is that using IAM roles only works when flannel runs natively (from outside a docker container).\nI'm using the cloud-init below, and its working fine:\n```\ncloud-config\ncoreos:\n  etcd2:\n    advertise-client-urls: http://$private_ipv4:2379\n    initial-advertise-peer-urls: http://$private_ipv4:2380\n    listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001\n    listen-peer-urls: http://$private_ipv4:2380\n    initial-cluster: node1=http://10.10.10.127:2380,node2=http://10.10.10.128:2380,node3=http://10.10.10.129:2380\n    initial-cluster-state: existing\nfleet:\n    public-ip: $private_ipv4\n    metadata: region=us-west-1,public_ip=$public_ipv4\nflannel:\n    interface: $private_ipv4\nunits:\n  - name: flanneld.service\n    drop-ins:\n    - name: 50-network-config.conf\n      content: |\n        [Unit]\n        After=clickwings.service\n    [Service]\n    Environment=\"AWS_ACCESS_KEY_ID=my_aws_access_key_id\"\n    Environment=\"AWS_SECRET_ACCESS_KEY=my_aws_secret_access_key\"\n    ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ \"Network\": \"10.20.0.0/16\", \"Backend\": { \"Type\": \"aws-vpc\", \"RouteTableID\": \"my-rtb-id\" } }'\ncommand: start\n\n\n\nname: etcd2.service\n    command: start\n\n\nname: fleet.service\n    command: start\n\n\nname: docker-tcp.socket\n    content: |\n      [Unit]\n      Description=Docker Socket for the API\n[Socket]\n  ListenStream=2375\n  Service=docker.service\n  BindIPv6Only=both\n[Install]\n  WantedBy=sockets.target\ncommand: start\nenable: true\n\n\nname: docker.service\n    drop-ins:\n\nname: 60-wait-for-flannel.conf\n  content: |\n    [Unit]\n    After=flanneld.service\n    Requires=flanneld.service\n    Restart=always\ncommand: restart\n``\n. Hi, @MohdAhmad, I'm running the AMICoreOS-stable-681.2.0-hvm (ami-c967938d)` and the flannel version is v0.4.\n\n\n\nAccording with this blog post about flannel 0.5.0 at CoreOS official blog:\n... Amazon Virtual Private Cloud (Amazon VPC) backend introduced in flannel v0.4 and the newly added GCE backend.\nMy entire flanneld.service is:\n```\n$ systemctl cat flanneld\n/usr/lib64/systemd/system/flanneld.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nRequires=early-docker.service\nAfter=etcd.service etcd2.service early-docker.service\nBefore=early-docker.target\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=\"TMPDIR=/var/tmp/\"\nEnvironment=\"DOCKER_HOST=unix:///var/run/early-docker.sock\"\nEnvironment=\"FLANNEL_VER=0.4.0\"\nEnvironment=\"ETCD_SSL_DIR=/etc/ssl/etcd\"\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStartPre=/usr/bin/touch /run/flannel/options.env\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n  /usr/bin/docker run --net=host --privileged=true --rm \\\n  --volume=/run/flannel:/run/flannel \\\n  --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n  --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n  --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n  --env-file=/run/flannel/options.env \\\n  --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n  --volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro \\\n  quay.io/coreos/flannel:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=true\nUpdate docker options\nExecStartPost=/usr/bin/docker run --net=host --rm -v /run:/run \\\n  quay.io/coreos/flannel:${FLANNEL_VER} \\\n  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env -i\n/etc/systemd/system/flanneld.service.d/50-network-config.conf\n[Unit]\nAfter=clickwings.service\n[Service]\nEnvironment=\"AWS_ACCESS_KEY_ID=my_key_id\"\nEnvironment=\"AWS_SECRET_ACCESS_KEY=my_secret_key\"\nExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ \"Network\": \"10.20.0.0/16\", \"Backend\": { \"Type\": \"aws-vpc\", \"RouteTableID\": \"my-route-table-id\" } }'\n```\n. @MohdAhmad, you think is a issue? Should I reopen?\nAnyway, I doing a new test with coreos-alpha, and I let you know.\nThanks\n. Ok, now I saw the v0.5 changelog:\n* AWS-VPC: Auto-detect route table ID if DescribeInstances permission is granted. Thank you, @MohdAhmad\nBut anyway, the error contested about getting AWS credentials from environment: AWS_SECRET_ACCESS_KEY or AWS_SECRET_KEY not found in environment\n. @MohdAhmad, I've just made a test by upgrading to v0.5 and works perfectly!\nThanks :)\n. ",
    "tomzhang": "should we consider to include more cloud as backend service .because this is trend that more and more cloud computing providers has been support programable network routing functions.  like some chinese cloud computing provider like ucloud, aliyun.\nmy suggestion is why we can creating all of this kind of backend service by using a method like Apache libcloud does , just create a SDK hub for every existed cloud vendors and allow users to select what they need to create a specific cloud provider related network solution to eliminate the duplicated works. \n. ",
    "Pensu": "@eyakubovich Hi, sorry for  the delayed reply. I am not on x86-64 arch, so I have to build it manually. Though it builds up cleanly:\npensu@ubuntu:~/flannel$ ./build \nBuilding flanneld...\npensu@ubuntu:~/flannel$\nIs there any other way to figure out what is wrong with my build?\nAnd yeah, I am using gccgo. \n. Update: So, I tried to run it once again, after building it again, and here is the new error I am getting:\ngo-etcd2015/07/15 03:32:52 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\ngo-etcd2015/07/15 03:32:52 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\ngo-etcd2015/07/15 03:32:52 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\ngo-etcd2015/07/15 03:32:52 DEBUG: recv.response.from \ngo-etcd2015/07/15 03:32:52 DEBUG: recv.success \nI0715 03:32:52.866715 12871 etcd.go:136] Found lease (10.16.0.0/20) for current IP (192.168.122.42), reusing\ngo-etcd2015/07/15 03:32:52 DEBUG: put /coreos.com/network/subnets/10.16.0.0-20, {\"PublicIP\":\"192.168.122.42\"}, ttl: 86400, [http://127.0.0.1:2379]\ngo-etcd2015/07/15 03:32:52 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/10.16.0.0-20\ngo-etcd2015/07/15 03:32:52 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/10.16.0.0-20 | method PUT\ngo-etcd2015/07/15 03:32:52 DEBUG: recv.response.from \ngo-etcd2015/07/15 03:32:52 DEBUG: recv.success \nI0715 03:32:52.943912 12871 etcd.go:91] Subnet lease acquired: 10.16.0.0/20\nE0715 03:32:52.944113 12871 network.go:71] Failed to initialize network  (type UDP): failed to lookup interface flannel%d\ngo-etcd2015/07/15 03:32:53 DEBUG: get /coreos.com/network/config [http://127.0.0.1:2379]\ngo-etcd2015/07/15 03:32:53 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\ngo-etcd2015/07/15 03:32:53 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\ngo-etcd2015/07/15 03:32:53 DEBUG: recv.response.from \ngo-etcd2015/07/15 03:32:53 DEBUG: recv.success \ngo-etcd2015/07/15 03:32:53 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\ngo-etcd2015/07/15 03:32:53 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\ngo-etcd2015/07/15 03:32:53 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\ngo-etcd2015/07/15 03:32:53 DEBUG: recv.response.from \ngo-etcd2015/07/15 03:32:53 DEBUG: recv.success \nI0715 03:32:53.947115 12871 etcd.go:136] Found lease (10.16.0.0/20) for current IP (192.168.122.42), reusing\ngo-etcd2015/07/15 03:32:53 DEBUG: put /coreos.com/network/subnets/10.16.0.0-20, {\"PublicIP\":\"192.168.122.42\"}, ttl: 86400, [http://127.0.0.1:2379]\ngo-etcd2015/07/15 03:32:53 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/10.16.0.0-20\ngo-etcd2015/07/15 03:32:53 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/10.16.0.0-20 | method PUT\ngo-etcd2015/07/15 03:32:54 DEBUG: recv.response.from \ngo-etcd2015/07/15 03:32:54 DEBUG: recv.success \nI0715 03:32:54.082400 12871 etcd.go:91] Subnet lease acquired: 10.16.0.0/20\nE0715 03:32:54.083174 12871 network.go:71] Failed to initialize network  (type UDP): failed to lookup interface flannel%d\n@eyakubovich And it just keeps on looping like that. Is that my system's issue not to be able to work with udp or gccgo problem?\n. @eyakubovich I have a flannel device with me:\n```\npensu@ubuntu:~/flannel$ ifconfig\ndocker0   Link encap:Ethernet  HWaddr 56:84:7a:fe:97:99\n          inet addr:10.10.112.1  Bcast:10.10.127.255  Mask:255.255.240.0\n          inet6 addr: fe80::5484:7aff:fefe:9799/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:106 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:81 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:8128 (8.1 KB)  TX bytes:9034 (9.0 KB)\neth0      Link encap:Ethernet  HWaddr 52:54:c7:a8:7a:40\n          inet addr:192.168.122.119  Bcast:192.168.122.255  Mask:255.255.255.0\n          inet6 addr: fe80::5054:c7ff:fea8:7a40/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:1408181 errors:0 dropped:7 overruns:0 frame:0\n          TX packets:1339446 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:349216618 (349.2 MB)  TX bytes:141199500 (141.1 MB)\nflannel.1 Link encap:Ethernet  HWaddr 02:46:8f:eb:c9:b6\n          inet addr:10.0.87.0  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::46:8fff:feeb:c9b6/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:41 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:42916 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:42916 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:2748640 (2.7 MB)  TX bytes:2748640 (2.7 MB)\nvethcdc728a Link encap:Ethernet  HWaddr 96:3a:d5:58:b8:77\n          inet6 addr: fe80::943a:d5ff:fe58:b877/64 Scope:Link\n          UP BROADCAST RUNNING  MTU:1450  Metric:1\n          RX packets:26 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:2244 (2.2 KB)  TX bytes:774 (774.0 B)\n```\nNow, sometimes I get that panic error and sometimes this device error. And my arch isn't supported by official go compiler. Is there any other way to figure out what exactly could be causing this problem?\n. @eyakubovich Yeah, I tried with vxlan too and it didn't work. Looks like with vxlan, I am not able to return that particular device and with udp I am getting that panic error. \nSure, I will try to run flannel under gdb. I am using ppc64le. \n. @eyakubovich So I ran flannel with gdb and looks like NetworkOrder() is failing. I don't know for sure, as my gdb skills are not that strong. Here is what I am getting:\n```\n(gdb) \ngithub_com_coreos_flannel_pkg_ip.NetworkOrder.N36_github_com_coreos_flannel_pkg_ip.IP4 (pointer=0xc208010660)\n    at /home/pensu/work/flannel/gopath/src/github.com/coreos/flannel/pkg/ip/ipnet.go:67\n67          a, b, c, d := byte(ip>>24), byte(ip>>16), byte(ip>>8), byte(ip)\n(gdb) \n68          return uint32(a) | (uint32(b) << 8) | (uint32(c) << 16) | (uint32(d) << 24)\n(gdb) \n65  func (ip IP4) NetworkOrder() uint32 {\n(gdb) \nudp.setRoute (ctl=0xc208011070, param=3232266794, param=3232266794, nextHopPort=7890)\n    at /home/pensu/work/flannel/gopath/src/github.com/coreos/flannel/backend/udp/cproxy.go:67\n67          cmd:           C.CMD_SET_ROUTE,\n(gdb) \n66      cmd := C.command{\n(gdb) \nProgram received signal SIGSEGV, Segmentation fault.\nudp.setRoute (ctl=0xc208011070, param=3232266794, param=3232266794, nextHopPort=7890)\n    at /home/pensu/work/flannel/gopath/src/github.com/coreos/flannel/backend/udp/cproxy.go:66\n66      cmd := C.command{\n```\nAny thoughts?\n. Okay, I have one more thought. I put one fmt.Println(C.CMD_SET_ROUTE), just before C.command{} statement, and it segfaulted even before reaching that. Could the issue be with C call only? Can you try to replicate the issue with gccgo? \n. @eyakubovich The C calls are working fine, here is the program I am running:\n```\npensu@ubuntu:~$ cat test_new.go \npackage main\n// #include\n// #include\nimport \"C\"\nimport \"fmt\"\nfunc main(){\n    fmt.Println(C.getpid())\n}\n```\nAnd here is the output:\npensu@ubuntu:~$ go run test_new.go\n27927\nAnd I checked NativeEndian too, it's working completely fine. It returns \"ture\" only.\n. @tomdee Seems like this PR is owned by @salamani, I am not sure what am I supposed to do here?. Sure, I will submit a PR soon.. Figured out, selinux was enforcing, a docker problem, not a flannel one!. ",
    "jon-shanks": "-- Reboot --\nJul 09 21:03:21 dvcorshnode01 systemd[1]: Started Docker Application Container Engine.\nJul 09 21:03:21 dvcorshnode01 systemd[1]: Starting Docker Application Container Engine...\nJul 09 21:03:21 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:21Z\" level=info msg=\"+job serveapi(fd://)\"\nJul 09 21:03:21 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:21Z\" level=info msg=\"Listening for HTTP on fd ()\"\nJul 09 21:03:21 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:21Z\" level=info msg=\"+job init_networkdriver()\"\nJul 09 21:03:21 dvcorshnode01 systemd[1]: Starting Network fabric for containers...\nJul 09 21:03:22 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:22Z\" level=info msg=\"-job init_networkdriver() = OK (0)\"\nJul 09 21:03:23 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:23Z\" level=info msg=\"Loading containers: start.\"\nJul 09 21:03:23 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:23Z\" level=info msg=\"Loading containers: done.\"\nJul 09 21:03:23 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:23Z\" level=info msg=\"docker daemon: 1.6.2 7c8fca2-dirty; execdriver: n\nJul 09 21:03:23 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:23Z\" level=info msg=\"+job acceptconnections()\"\nJul 09 21:03:23 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:23Z\" level=info msg=\"-job acceptconnections() = OK (0)\"\nJul 09 21:03:23 dvcorshnode01 dockerd[606]: time=\"2015-07-09T21:03:23Z\" level=info msg=\"Daemon has completed initialization\"\nJul 09 21:03:25 dvcorshnode01 etcdctl[690]: { \"Network\": \"10.102.0.0/16\", \"Backend\" : { \"Type\" : \"vxlan\" } }\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.217817 00001 main.go:278] Installing signal handlers\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.218007 00001 main.go:138] Determining IP address of default interface\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.218243 00001 main.go:228] Using 10.70.2.18 as external interface\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.543657 00001 subnet.go:97] Subnet lease acquired: 10.102.20.0/24\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.588362 00001 ipmasq.go:47] Adding iptables rule: FLANNEL -d 10.102.0.0/\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.591035 00001 ipmasq.go:47] Adding iptables rule: FLANNEL ! -d 224.0.0.0\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.593387 00001 ipmasq.go:47] Adding iptables rule: POSTROUTING -s 10.102.\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.596659 00001 main.go:245] VXLAN mode initialized\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.596710 00001 vxlan.go:141] Watching for L2/L3 misses\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.596750 00001 vxlan.go:147] Watching for new subnet leases\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.602333 00001 vxlan.go:210] Subnet added: 10.102.24.0/24\nJul 09 21:03:26 dvcorshnode01 sdnotify-proxy[747]: I0709 21:03:26.602564 00001 vxlan.go:210] Subnet added: 10.102.3.0/24\nJul 09 21:03:26 dvcorshnode01 systemd[1]: Started Network fabric for containers.\n. Hi actually I have resolved this, it was some dependency chain in other things causing this to mess up. i've cleaned up the cloud config a lot and now it \"Almost\" works, though, docker0 doesn't come up. is there a reason why docker0 actually requires a start unit in the cloud-config for it to start. It's almost like the coreos-overlay with early-docker prevents it actually coming up without this? \n. I'm going to close this issue and let smana raise one separately to relate it as the issue i have is now different to what i've put in and that issue isn't an issue for me right now\n. So to add, it looks like flannel runs AFTER the target as oppose before anyway, and should then notify the early-docker target once it has run but the logs show that this isn't happening. Though this doesn't explain really to me why docker wouldn't start but the behavior still seems wrong to me?\n. would it not make sense just to make flannel a binary on coreos built into the image? This seems like a lot of bits of string to make things run in a kind of ordered way, (that isn't working) but i can see could quite easily break for a lot of people due to the flexibility of cloud-init and the cloud-config, people make do things or restart docker or whatever else breaking flannel very easily and preventing things working. \n. OK you can ignore the docker.service part. It isn't enabled, it works when this is in the cloud-config though as a unit start . However it's good to note that the order matters in the unit start i.e. it has to be AFTER flanneld start\n. -- Logs begin at Thu 2015-07-09 21:31:54 UTC, end at Fri 2015-07-10 15:15:32 UTC. --\nJul 10 03:15:59 cimgtcorshnode01 systemd[1]: Reached target Run Docker containers before main Docker starts up.\nJul 10 03:15:59 cimgtcorshnode01 systemd[1]: Starting Run Docker containers before main Docker starts up.\nJul 10 03:16:00 cimgtcorshnode01 systemd[1]: Started Early Docker Application Container Engine.\nJul 10 03:16:00 cimgtcorshnode01 systemd[1]: Starting Early Docker Application Container Engine...\nJul 10 03:16:00 cimgtcorshnode01 systemd[1]: Starting Network fabric for containers...\nJul 10 03:16:00 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:00Z\" level=info msg=\"+job serveapi(fd://)\"\nJul 10 03:16:00 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:00Z\" level=info msg=\"Listening for HTTP on fd ()\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"Loading containers: start.\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"Loading containers: done.\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"docker daemon: 1.6.2 7c8fca2-dirty; execdriver: native-0.2; graphdriver: overlay\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job acceptconnections()\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"-job acceptconnections() = OK (0)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"Daemon has completed initialization\"\nJul 10 03:16:04 cimgtcorshnode01 etcdctl[729]: { \"Network\": \"10.102.0.0/16\", \"Backend\" : { \"Type\" : \"vxlan\" } }\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"POST /v1.18/containers/create\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job create()\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job log(create, da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef, quay.io/coreos/flannel:0.4.0)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"-job log(create, da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef, quay.io/coreos/flannel:0.4.0) = OK (0)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"-job create() = OK (0)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"POST /v1.18/containers/da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef/attach?stderr=1&stdout=1&stream=1\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job container_inspect(da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"-job container_inspect(da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef) = OK (0)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job attach(da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"POST /v1.18/containers/da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef/start\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job start(da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"+job log(start, da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef, quay.io/coreos/flannel:0.4.0)\"\nJul 10 03:16:04 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:04Z\" level=info msg=\"-job log(start, da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef, quay.io/coreos/flannel:0.4.0) = OK (0)\"\nJul 10 03:16:05 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:05Z\" level=info msg=\"-job start(da0218e2199f91f7694ddfc08284163796b1066cd18c8d2aceb07f10eb0dc8ef) = OK (0)\"\nJul 10 03:16:06 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:06.268817 00001 main.go:278] Installing signal handlers\nJul 10 03:16:06 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:06.268881 00001 main.go:138] Determining IP address of default interface\nJul 10 03:16:06 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:06.269352 00001 main.go:228] Using 10.70.2.2 as external interface\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.037061 00001 subnet.go:97] Subnet lease acquired: 10.102.23.0/24\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.332723 00001 ipmasq.go:47] Adding iptables rule: FLANNEL -d 10.102.0.0/16 -j ACCEPT\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.337708 00001 ipmasq.go:47] Adding iptables rule: FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.343025 00001 ipmasq.go:47] Adding iptables rule: POSTROUTING -s 10.102.0.0/16 -j FLANNEL\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.351752 00001 main.go:245] VXLAN mode initialized\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.351798 00001 vxlan.go:141] Watching for L2/L3 misses\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.351821 00001 vxlan.go:147] Watching for new subnet leases\nJul 10 03:16:07 cimgtcorshnode01 sdnotify-proxy[789]: I0710 03:16:07.371354 00001 vxlan.go:210] Subnet added: 10.102.42.0/24\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"POST /v1.18/containers/create\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job create()\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job log(create, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job log(create, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job create() = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"POST /v1.18/containers/254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152/attach?stderr=1&stdout=1&stream=1\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job container_inspect(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job container_inspect(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job attach(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"POST /v1.18/containers/254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152/start\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job start(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job log(start, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job log(start, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job start(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job log(die, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job log(die, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job attach(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"POST /v1.18/containers/254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152/wait\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job wait(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job wait(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"GET /v1.18/containers/254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152/json\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job container_inspect(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job container_inspect(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"DELETE /v1.18/containers/254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152?v=1\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job rm(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"+job log(destroy, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job log(destroy, 254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152, quay.io/coreos/flannel:0.4.0) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 dockerd[706]: time=\"2015-07-10T03:16:07Z\" level=info msg=\"-job rm(254d6d3ce23a8f143c89a818d8028b9bd2af79dd48879c222c9505e84b211152) = OK (0)\"\nJul 10 03:16:07 cimgtcorshnode01 systemd[1]: Started Network fabric for containers.\nJul 10 04:55:36 cimgtcorshnode01 sdnotify-proxy[789]: I0710 04:55:36.341713 00001 vxlan.go:210] Subnet added: 10.102.74.0/24\n. the behavior is still odd however with early-docker.target and flanneld.service, more logs requested, so please see above. \n. So closing this too:\nAppears a) docker service needs a unit as it's not enabled by default in CoreOS\nb) systemd notify is quirky in the sense that it outputs the description of the target in journalctl saying it's \"starting\" but yet it isn't \"started\" .. making things a bit confusing and misleading when you're troubleshooting. \n. ",
    "Smana": "Hi all,\nI've the same issue there\u00a0: docker starts the bridge docker0 instead of binding on the flannel network.\nBased on this doc https://github.com/GoogleCloudPlatform/kubernetes/blob/master/docs/getting-started-guides/coreos/coreos_multinode_cluster.md, \nplease find my cloud-config :\nhttps://gist.github.com/Smana/d337d6e987261fd60b0d\nflannel seems to be working : \n```\n\ufffd\ufffd flanneld.service - Network fabric for containers\n   Loaded: loaded (/usr/lib64/systemd/system/flanneld.service; static; vendor preset: disabled)\n  Drop-In: /etc/systemd/system/flanneld.service.d\n           \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd50-network-config.conf\n   Active: active (running) since Fri 2015-07-10 12:46:41 UTC; 11min ago\n     Docs: https://github.com/coreos/flannel\n  Process: 825 ExecStartPost=/usr/bin/docker run --net=host --rm -v /run:/run quay.io/coreos/flannel:${FLANNEL_VER} /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env -i (code=exited, status=0/SUCCESS)\n  Process: 728 ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config {\"Network\":\"10.244.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}} (code=exited, status=0/SUCCESS)\n  Process: 726 ExecStartPre=/usr/bin/touch /run/flannel/options.env (code=exited, status=0/SUCCESS)\n  Process: 724 ExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR} (code=exited, status=0/SUCCESS)\n  Process: 720 ExecStartPre=/usr/bin/mkdir -p /run/flannel (code=exited, status=0/SUCCESS)\n  Process: 718 ExecStartPre=/sbin/modprobe ip_tables (code=exited, status=0/SUCCESS)\n Main PID: 735 (sdnotify-proxy)\n   Memory: 48.0K\n      CPU: 49ms\n   CGroup: /system.slice/flanneld.service\n           \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd735 /usr/libexec/sdnotify-proxy /run/flannel/sd.sock /usr/bin/docker run --net=host --privileged=true --rm --volume=/run/flannel:/run/flannel --env=NOTIFY_SOCKET=/run/flannel/sd.sock --env=AWS_ACCESS_KEY_ID= --env=AWS_SECRET_ACCESS_KEY= --env-file=/run/flannel/options.env --volume=/usr/share/ca-certif...\n           \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd739 /usr/bin/docker run --net=host --privileged=true --rm --volume=/run/flannel:/run/flannel --env=NOTIFY_SOCKET=/run/flannel/sd.sock --env=AWS_ACCESS_KEY_ID= --env=AWS_SECRET_ACCESS_KEY= --env-file=/run/flannel/options.env --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro --volume=/etc/ssl/etcd:/...\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.495377 00001 subnet.go:97] Subnet lease acquired: 10.244.19.0/24\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.510800 00001 ipmasq.go:47] Adding iptables rule: FLANNEL -d 10.244.0.0/16 -j ACCEPT\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.518456 00001 ipmasq.go:47] Adding iptables rule: FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.525240 00001 ipmasq.go:47] Adding iptables rule: POSTROUTING -s 10.244.0.0/16 -j FLANNEL\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.538302 00001 main.go:245] VXLAN mode initialized\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.538472 00001 vxlan.go:141] Watching for L2/L3 misses\nJul 10 12:46:41 coreos0 sdnotify-proxy[735]: I0710 12:46:41.538939 00001 vxlan.go:147] Watching for new subnet leases\nJul 10 12:46:41 coreos0 systemd[1]: Started Network fabric for containers.\nJul 10 12:47:07 coreos0 sdnotify-proxy[735]: I0710 12:47:07.405754 00001 vxlan.go:210] Subnet added: 10.244.42.0/24\nJul 10 12:47:08 coreos0 sdnotify-proxy[735]: I0710 12:47:08.895302 00001 vxlan.go:210] Subnet added: 10.244.39.0/24\n```\nmy ip configurtion : \n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:a2:99:99 brd ff:ff:ff:ff:ff:ff\n    inet 10.115.200.90/16 brd 10.115.255.255 scope global dynamic eth0\n       valid_lft 20778sec preferred_lft 20778sec\n    inet 10.115.77.86/16 brd 10.115.255.255 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5054:ff:fea2:9999/64 scope link \n       valid_lft forever preferred_lft forever\n3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default \n    link/ether 7a:a8:12:ea:6e:d9 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.42.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::78a8:12ff:feea:6ed9/64 scope link \n       valid_lft forever preferred_lft forever\n4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default \n    link/ether 4e:78:4b:34:06:f5 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.19.0/16 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::4c78:4bff:fe34:6f5/64 scope link \n       valid_lft forever preferred_lft forever\nroutes\ndefault via 10.115.255.253 dev eth0  proto static \n10.0.0.0/8 via 10.115.255.254 dev eth0  proto static \n10.115.0.0/16 dev eth0  proto kernel  scope link  src 10.115.200.90 \n10.244.0.0/16 dev flannel.1  proto kernel  scope link  src 10.244.19.0 \n172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.42.1 \n192.168.0.0/17 via 10.115.255.254 dev eth0  proto static\nmy logs are available here https://gist.github.com/Smana/0dd847b0fe52d0e44ab4\nPlease let me know if you need further info.\nRegards,\nSmana\n. Hi that's ok now with my new configuration : https://gist.github.com/Smana/6518172dd818fbe79716\nanother issue helped me to find out the proper configuration: https://github.com/coreos/bugs/issues/407\n. ",
    "pierrebeaucamp": "+1\n. ",
    "patrickhoefler": "@eyakubovich You can definitely ask me, but it would probably make more sense to ask @Nurza instead ;)\n. ",
    "Nurza": "Because I own a few servers with free IPv6 blocks (IPv4 is SO expensive) and I would like to use flannel with them.\n. ",
    "fskale": "I think there are enough use cases, which require ipv6 addresses. I'm working on such and the lack of ipv6 support in kubernetes, flannel forces me to use a single docker host with pipework. Docker supports ipv6 using the command line option: --ipv6 --fixed-cidr-v6=\\\"xxxx:xxxx:xxxx:xxxx:/64\\\"\". I'm Working with Unique Local Unicast Addresses. The usecase is clear. Ipv6only with DNATS ans SNATS to the pods. Personally i think, you should give ipv6 more priority. So i'm wirting on something like kiwi to track changes to a pod, and then issue pipework cmd to go back to kubernetes.\n. ",
    "colinrgodsey": "+1\nEDIT: also pitching in a use case- microservices. The bare-metal or VM way would generally have you putting a few microservices on one node (with one IP, then segregate by port for service), just for conservation of strictly allocated resources  and... probably ipv4 addresses (not even public per-se, we actually have pretty high demand internally at our company for private 10.x.y.z subnets and its getting hard to reserve even a /20) you'll have a few services on a box.\nDocker you're basically running a 'machine' per service (or can, not much reason to bunch things together), so you're naturally going to need more IPs than normal. IP addresses are also really the only strictly allocated resources when using docker (at least in the flannel/fabric way). IPv6 definitely fixes this... \n. ",
    "hwinkel": "+1\n. ",
    "atta": "+1\n. ",
    "glennswest": "Currently if you start doing sizing, you can get 300-400 hosts in a rack. (Supermicro microblades), two racks will go over the flannel/ipv4 limites. If you start looking at maximums, the flannel/ipv4 limit sounds ok, at around 261K containers max, but the reality its only 1-2 racks today. If you actually start applying devops/microservices into realworld apps, the container counts explode. I did a sizing for a day trading app im designing and its about 1.8million containers. \nThere only a very small number of container types (20+) yet there reused in alot of ways for the app.\nAnd there 4000 equities, and a average of 450 plus containers per equity.\nHardware Config:\nhttps://www.linkedin.com/pulse/building-data-center-deep-neural-networks-thinking-wall-glenn-west\nApp Overview\nhttps://www.linkedin.com/pulse/using-containers-docker-change-world-glenn-west\nIf you consider real wall street system, or if you look at credit risk or a lot of typical commercial apps,\nthis is not alot of hardware, or even really big apps. The trading app would be alot bigger if you do equities on a global scale with multiple exchanges. \nWhen looking at hardware preference, you would actually want to do this across multiple data centers, multiple floors in data centers, and multiple racks. When you start looking at this, the number of nodes needs to be bigger. \n. ",
    "choppsv1": "+1 Use case: no IPv4 in the network infrastructure. None of my internal servers (next gen internet provider infrastructure) have ipv4 addresses.\n. @tomdee We have no IPv4 addresses, we don't need them or want the complexity of running a dual stack. So I guess this means: 1 and 2.. ",
    "phillipCouto": "+1\n. ",
    "stevemcquaid": "+1 - this is probably the most critical issue for my company.\n. ",
    "kkirsche": "Instead of +1's, please use the github +1 feature using the smiley face in the upper right of the original post. This will help organize this and allow the actual discussions of how to solve this occur rather than polluting the issue with +1's. Thank you for your understanding and cooperation.\n. Mobile applications for Apple also require IPv6 \u2014 https://developer.apple.com/news/?id=05042016a which may increase the desire to transition for some use cases. @tomdee what kind of testing do you need for this?\n. @steveej So is the goal only to replace the logging or is to also normalize logging style as with other packages?\n. May I ask what that actually means? It sounds like it just isn't prioritized due to work, -'d should stay open. It doesn't make sense to me at least with what you've said that this is really something that shouldn't be done thus needing to be closed.\n. Ah, ok. Thanks for explaining that @tomdee \u2014 that logic makes more sense to me\n. Just trying to make sure I understand, so the issue here is that Flannel is not disabling the SourceDestCheck on AWS instances with more than one network interface. Specifically, SourceDestCheck is an Elastic Network Interface attribute which AWS exposes instead as part of the instance when the instance has only one interface but does not expose this when the instance has more than one interface. Is this accurate?\n. Personally, I think a PR would probably be appreciated though I don't want to speak for the Flannel core team as to the solution itself. I'd just want to double check if you had done or if this was still needed in the current version of flannel \n from https://coreos.com/blog/introducing-flannel-0.5.0-with-aws-and-gce/\nIt seems as though this says to disable it within AWS, so want to make sure that this has been checked\n. Ah, ok. Sorry about that. Just wanted to make sure. I personally think that flannel should know how to disable SourceDestCheck then with an option to override this automatic decision should it have been incorrectly identified. Does that sound reasonable?\nCertainly don't want this hitting API Rate Limiting as that can certainly be very problematic\n. @tomdee wouldn't the fact that you need to be able to change and update an example be exactly why it should be in master? Examples should always represent the current state of the codebase not what used to be, which is what often happens when people end up storing their examples separate from repos they are for.\n. ",
    "rahulwa": "Please prioritize this. Nowadays IPv6 support is becoming common among the providers (like aws, scaleway, Hetzner, Online.net).\nOur company is becoming IPv6 only network for internal traffic due to easiness of doing cross cloud setup securely. (For example, as on aws, globally routable IPv6 range can be configured on VPC level. This makes easy to do cross providers setup securely as we already know ip6 address range for any machine on that VPC. It especially helps for autoscaling). \nIts sufficient for us to have support IPv6 on alloc backend, we don't need overlay network.. I believe 1 and 2 should be good enough.. ",
    "oneiros-de": "Just curious since this ticket will be two years old soon: Is anybody working on it?. ",
    "burton-scalefastr": "It's surprising that this isn't implemented yet since IPv6 is very attractive since most hosts have a full /64 to play with.. ",
    "abh": "For us the use case is similar. We can do pure IPv6 (and have a load balancer in front of the cluster for IPv4 ingress), but IPv4 itself is hard.\nFor a project at work we're out of IPv4 addresses. Even allocating a not-to-be-routed /19 in 10/8 is difficult.. ",
    "petrus-v": "In our context 1 and 2 would be great!. ",
    "onitake": "Are you aware of https://github.com/leblancd/kube-v6 and https://github.com/kubernetes/features/issues/508 ?\nI'm actually surprised that IPv6 wasn't chosen as the default networking layer for Kubernetes from the beginning. It makes a lot of sense due to the sheer size and structure of the address space and would virtually eliminate the need for NAT inside a cluster (NAT64/DNS64 access to IPv4 networks notwithstanding).. @CalvinHartwell That's a valid concern; however, it's 2018 and there's not much of an excuse left for not supporting IPv6. In fact, I'm quite certain that most software these days is implicitly IPv6 capable.. Exactly the same problem here.\nI was using 0.10.0 before, but that crashed on startup due to #977 .\n1016 was supposed to fix this - but it didn't for some reason?. I built 0.11.0 using Go 1.11, but the segfault persists.\nEdit: It's the same with Go 1.8.7.\ngo spits out the following messages:\n```\ngithub.com/coreos/flannel\n/usr/bin/ld: /tmp/go-link-114095991/000022.o: in function mygetgrouplist':\n/build/golang-1.11-U2p3Pq/golang-1.11-1.11.5/src/os/user/getgrouplist_unix.go:16: warning: Using 'getgrouplist' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/usr/bin/ld: /tmp/go-link-114095991/000021.o: in functionmygetgrgid_r':\n/build/golang-1.11-U2p3Pq/golang-1.11-1.11.5/src/os/user/cgo_lookup_unix.go:38: warning: Using 'getgrgid_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/usr/bin/ld: /tmp/go-link-114095991/000021.o: in function mygetgrnam_r':\n/build/golang-1.11-U2p3Pq/golang-1.11-1.11.5/src/os/user/cgo_lookup_unix.go:43: warning: Using 'getgrnam_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/usr/bin/ld: /tmp/go-link-114095991/000021.o: in functionmygetpwnam_r':\n/build/golang-1.11-U2p3Pq/golang-1.11-1.11.5/src/os/user/cgo_lookup_unix.go:33: warning: Using 'getpwnam_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/usr/bin/ld: /tmp/go-link-114095991/000021.o: in function mygetpwuid_r':\n/build/golang-1.11-U2p3Pq/golang-1.11-1.11.5/src/os/user/cgo_lookup_unix.go:28: warning: Using 'getpwuid_r' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n/usr/bin/ld: /tmp/go-link-114095991/000004.o: in function_cgo_18049202ccd9_C2func_getaddrinfo':\n/tmp/go-build/cgo-gcc-prolog:49: warning: Using 'getaddrinfo' in statically linked applications requires at runtime the shared libraries from the glibc version used for linking\n```\nPerhaps incompatible glibc versions are the root of the problem?\nIf this is really a bug/race condition in Go, it was never fixed.. Some more details on the involved systems:\nBuild: Debian buster (gcc 8.2.0, glibc 2.28, go 1.11.5)\nBuild 2: Docker build according to Makefile, tested with go 1.8.7, 1.10 and 1.11.5\nHost: CentOS 7 (glibc 2.17). ",
    "CalvinHartwell": "@onitake my guess is that some workloads (running in the containers) might not support IPv6 so I guess they may have wanted it to be backwards compatible for some stuff. Annoying I know! . ",
    "jeffbean": "So the new bits (0.5.1) seem to solve my issue. Now the issue is getting these build into the CentOS repos. :)\n. Also for the record just disabling ipv6 via sysctl did not resolve the panics. \nnet.ipv6.conf.all.disable_ipv6 set to value 1\n. ",
    "rpatrick00": "Thanks so much, that resolved my issue!\n. Sorry, one more twist to the problem.  Adding the --ip-masq flag to the flanneld command line resolved the issue with accessing external addresses.  I can now see external (non-flannel) IP addresses from inside my Docker containers.  However, I am still not able to see other Docker containers running on different hosts.\nOn host1:\n-bash-4.2$ hostname\nslc06ptm.us.oracle.com\n-bash-4.2$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n65b9e54d3da5        ubuntu:14.04        \"/bin/bash\"         9 seconds ago       Up 8 seconds                            silly_nobel\n-bash-4.2$ docker inspect 65b9e54d3da5 | grep IPA\n        \"IPAddress\": \"192.168.83.8\",\n-bash-4.2$ ping 192.168.83.8\nPING 192.168.83.8 (192.168.83.8) 56(84) bytes of data.\n64 bytes from 192.168.83.8: icmp_seq=1 ttl=64 time=0.070 ms\n64 bytes from 192.168.83.8: icmp_seq=2 ttl=64 time=0.045 ms\n64 bytes from 192.168.83.8: icmp_seq=3 ttl=64 time=0.052 ms\n--- 192.168.83.8 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 1999ms\nrtt min/avg/max/mdev = 0.045/0.055/0.070/0.013 ms\n-bash-4.2$\nOn host2:\n-bash-4.2$ hostname\nslc07jey.us.oracle.com\n-bash-4.2$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\nc08c68c475ec        ubuntu:14.04        \"/bin/bash\"         12 seconds ago      Up 11 seconds                           sick_einstein\n-bash-4.2$ docker inspect c08c68c475ec | grep IPA\n        \"IPAddress\": \"192.168.55.4\",\n-bash-4.2$ ping 192.168.55.4\nPING 192.168.55.4 (192.168.55.4) 56(84) bytes of data.\n64 bytes from 192.168.55.4: icmp_seq=1 ttl=64 time=0.091 ms\n64 bytes from 192.168.55.4: icmp_seq=2 ttl=64 time=0.037 ms\n64 bytes from 192.168.55.4: icmp_seq=3 ttl=64 time=0.042 ms\n--- 192.168.55.4 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2000ms\nrtt min/avg/max/mdev = 0.037/0.056/0.091/0.025 ms\n-bash-4.2$\nInside Docker container on host1:\n-bash-4.2$ docker run --rm=true -it ubuntu:14.04\nroot@65b9e54d3da5:/# ping 192.135.82.180\nPING 192.135.82.180 (192.135.82.180) 56(84) bytes of data.\n64 bytes from 192.135.82.180: icmp_seq=1 ttl=60 time=0.385 ms\n64 bytes from 192.135.82.180: icmp_seq=2 ttl=60 time=0.370 ms\n64 bytes from 192.135.82.180: icmp_seq=3 ttl=60 time=0.426 ms\n^C\n--- 192.135.82.180 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2000ms\nrtt min/avg/max/mdev = 0.370/0.393/0.426/0.032 ms\nroot@65b9e54d3da5:/# ping 192.168.83.8\nPING 192.168.83.8 (192.168.83.8) 56(84) bytes of data.\n64 bytes from 192.168.83.8: icmp_seq=1 ttl=64 time=0.040 ms\n64 bytes from 192.168.83.8: icmp_seq=2 ttl=64 time=0.026 ms\n64 bytes from 192.168.83.8: icmp_seq=3 ttl=64 time=0.025 ms\n^C\n--- 192.168.83.8 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 1999ms\nrtt min/avg/max/mdev = 0.025/0.030/0.040/0.008 ms\nroot@65b9e54d3da5:/# ping 192.168.55.4\nPING 192.168.55.4 (192.168.55.4) 56(84) bytes of data.\n^C\n--- 192.168.55.4 ping statistics ---\n5 packets transmitted, 0 received, 100% packet loss, time 3999ms\nInside Docker container on host2:\n-bash-4.2$ docker run --rm=true -it ubuntu:14.04\nroot@c08c68c475ec:/# ping 192.135.82.180\nPING 192.135.82.180 (192.135.82.180) 56(84) bytes of data.\n64 bytes from 192.135.82.180: icmp_seq=1 ttl=60 time=0.376 ms\n64 bytes from 192.135.82.180: icmp_seq=2 ttl=60 time=0.329 ms\n64 bytes from 192.135.82.180: icmp_seq=3 ttl=60 time=0.412 ms\n^C\n--- 192.135.82.180 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 1999ms\nrtt min/avg/max/mdev = 0.329/0.372/0.412/0.037 ms\nroot@c08c68c475ec:/# ping 192.168.55.4\nPING 192.168.55.4 (192.168.55.4) 56(84) bytes of data.\n64 bytes from 192.168.55.4: icmp_seq=1 ttl=64 time=0.028 ms\n64 bytes from 192.168.55.4: icmp_seq=2 ttl=64 time=0.054 ms\n64 bytes from 192.168.55.4: icmp_seq=3 ttl=64 time=0.048 ms\n^C\n--- 192.168.55.4 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 1999ms\nrtt min/avg/max/mdev = 0.028/0.043/0.054/0.012 ms\nroot@c08c68c475ec:/# ping 192.168.83.8\nPING 192.168.83.8 (192.168.83.8) 56(84) bytes of data.\n^C\n--- 192.168.83.8 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 1999ms\nAs you can see, from inside the container I can ping external (non-flannel) IP addresses and the flannel IP on the same machine but I am unable to ping the flannel IP on the other machine.  Any additional insight on how to make this work would be greatly appreciated!\nThanks again,\nRobert\n. I am not in a public cloud environment and the UDP ports seem to be working at the host level:\nHost1:\n-bash-4.2$ sudo systemctl stop docker\n-bash-4.2$ sudo systemctl stop flannel\n-bash-4.2$ nc -l -u 8285\nHello it's me\n^C\n-bash-4.2$ echo \"Well hello me\" > source_file\n-bash-4.2$ nc -u slc07jey.us.oracle.com 8285 < source_file\nHost2:\n-bash-4.2$ sudo systemctl stop docker\n-bash-4.2$ sudo systemctl stop flannel\n-bash-4.2$ echo \"Hello it's me\" > source_file\n-bash-4.2$ nc -u slc06ptm.us.oracle.com 8285 < source_file\n-bash-4.2$ nc -l -u 8285\nWell hello me\n^C\n-bash-4.2$\n. On the source machine (host1), I see the outbound traffic being sent to the target machine (host2) but I do not see any reply traffic.  On the target machine, I see the ICMP request and reply on both docker0 and flannel0 but I see only see the inbound requests and no evidence (unless I am reading the tcpdump output wrong) of the reply being sent with eth0.\nSource machine:\n-bash-4.2$ hostname\nslc06ptm.us.oracle.com\n-bash-4.2$ sudo /sbin/tcpdump -i docker0 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on docker0, link-type EN10MB (Ethernet), capture size 65535 bytes\n07:40:55.083410 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 47, length 64\n07:40:56.083395 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 48, length 64\n07:40:57.083421 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 49, length 64\n07:40:58.083415 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 50, length 64\n07:40:59.083392 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 51, length 64\n07:41:00.083382 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 52, length 64\n07:41:01.083403 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 53, length 64\n07:41:02.083412 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 54, length 64\n8 packets captured\n8 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$ sudo /sbin/tcpdump -i flannel0 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel0, link-type RAW (Raw IP), capture size 65535 bytes\n07:41:13.083410 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 65, length 64\n07:41:14.083435 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 66, length 64\n07:41:15.083454 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 67, length 64\n07:41:16.083471 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 68, length 64\n07:41:17.083463 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 69, length 64\n07:41:18.083440 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 70, length 64\n07:41:19.083447 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 71, length 64\n07:41:20.083420 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 72, length 64\n07:41:21.083439 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 73, length 64\n07:41:22.083440 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 74, length 64\n10 packets captured\n11 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$ sudo /sbin/tcpdump -i eth0 udp port 8285\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n07:41:40.083439 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:41.083445 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:42.083464 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:43.083429 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:44.083422 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:45.083487 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:46.083475 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:47.083494 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:48.083462 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:49.083462 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:50.083449 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:41:51.083501 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n12 packets captured\n13 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$\nOn the target machine:\n-bash-4.2$ hostname\nslc07jey.us.oracle.com\n-bash-4.2$ sudo /sbin/tcpdump -i eth0 udp port 8285\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n07:43:20.081152 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:21.081249 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:22.081264 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:23.081217 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:24.081295 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:25.081235 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:26.081168 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:27.081137 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:28.081235 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:29.081183 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:30.081261 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n07:43:31.081260 IP slc06ptm.us.oracle.com.8285 > slc07jey.us.oracle.com.8285: UDP, length 84\n12 packets captured\n13 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$ sudo /sbin/tcpdump -i flannel0 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel0, link-type RAW (Raw IP), capture size 65535 bytes\n07:43:41.081217 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 213, length 64\n07:43:41.081265 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 213, length 64\n07:43:42.081313 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 214, length 64\n07:43:42.081357 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 214, length 64\n07:43:43.081329 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 215, length 64\n07:43:43.081382 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 215, length 64\n07:43:44.081228 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 216, length 64\n07:43:44.081278 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 216, length 64\n07:43:45.081300 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 217, length 64\n07:43:45.081342 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 217, length 64\n07:43:46.081260 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 218, length 64\n07:43:46.081302 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 218, length 64\n07:43:47.081259 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 219, length 64\n07:43:47.081294 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 219, length 64\n07:43:48.081324 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 220, length 64\n07:43:48.081373 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 220, length 64\n16 packets captured\n18 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$ sudo /sbin/tcpdump -i docker0 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on docker0, link-type EN10MB (Ethernet), capture size 65535 bytes\n07:43:59.081256 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 231, length 64\n07:43:59.081274 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 231, length 64\n07:44:00.081338 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 232, length 64\n07:44:00.081364 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 232, length 64\n07:44:01.081323 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 233, length 64\n07:44:01.081340 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 233, length 64\n07:44:02.081312 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 234, length 64\n07:44:02.081331 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 234, length 64\n07:44:03.081314 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 235, length 64\n07:44:03.081343 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 235, length 64\n07:44:04.081285 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 236, length 64\n07:44:04.081303 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 236, length 64\n07:44:05.081277 IP 192.168.83.2 > 192.168.55.2: ICMP echo request, id 16, seq 237, length 64\n07:44:05.081297 IP 192.168.55.2 > 192.168.83.2: ICMP echo reply, id 16, seq 237, length 64\n14 packets captured\n16 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$\n. Reversing the direction of the ping proves there is a communication breakdown between flannel0 and eth0 on host2.  I see no traffic on host1 at all.\n-bash-4.2$ hostname\nslc07jey.us.oracle.com\n-bash-4.2$ sudo /sbin/tcpdump -i docker0 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on docker0, link-type EN10MB (Ethernet), capture size 65535 bytes\n07:51:33.147186 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 212, length 64\n07:51:34.147172 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 213, length 64\n07:51:35.147177 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 214, length 64\n07:51:36.147163 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 215, length 64\n07:51:37.147206 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 216, length 64\n07:51:38.147184 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 217, length 64\n07:51:39.147196 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 218, length 64\n7 packets captured\n7 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$ sudo /sbin/tcpdump -i flannel0 icmp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel0, link-type RAW (Raw IP), capture size 65535 bytes\n07:51:45.147165 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 224, length 64\n07:51:46.147199 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 225, length 64\n07:51:47.147191 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 226, length 64\n07:51:48.147161 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 227, length 64\n07:51:49.147174 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 228, length 64\n07:51:50.147206 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 229, length 64\n07:51:51.147187 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 230, length 64\n07:51:52.147249 IP 192.168.55.2 > 192.168.83.2: ICMP echo request, id 15, seq 231, length 64\n8 packets captured\n8 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$ sudo /sbin/tcpdump -i eth0 udp port 8285\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n0 packets captured\n0 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$\nHow do I go about determine what is causing this breakdown?\n. I am running on Oracle Enterprise Linux (which is based on RedHat):\n-bash-4.2$ uname -a\nLinux slc07jey.us.oracle.com 3.8.13-35.3.1.el7uek.x86_64 #2 SMP Wed Jun 25 15:27:43 PDT 2014 x86_64 x86_64 x86_64 GNU/Linux\n-bash-4.2$ /scratch/flannel/bin/flanneld -version\n0.4.1+git\nHere is the output you requested.  The arp messages do not seem to be related to the ping...\nOn host2:\n-bash-4.2$ /sbin/ip addr show\n1: lo:  mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0:  mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 00:21:f6:2c:0d:c8 brd ff:ff:ff:ff:ff:ff\n    inet 10.240.182.228/21 brd 10.240.183.255 scope global eth0\n    inet6 2606:b400:2010:4055:221:f6ff:fe2c:dc8/64 scope global dynamic\n       valid_lft 2591844sec preferred_lft 604644sec\n    inet6 fe80::221:f6ff:fe2c:dc8/64 scope link\n       valid_lft forever preferred_lft forever\n43: flannel0:  mtu 1472 qdisc pfifo_fast state UNKNOWN qlen 500\n    link/none\n    inet 192.168.55.0/16 scope global flannel0\n44: docker0:  mtu 1472 qdisc noqueue state UP\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.55.1/24 scope global docker0\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link\n       valid_lft forever preferred_lft forever\n48: veth1af52cc:  mtu 1472 qdisc noqueue master docker0 state UP\n    link/ether 96:99:a8:54:c5:d1 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::9499:a8ff:fe54:c5d1/64 scope link\n       valid_lft forever preferred_lft forever\n-bash-4.2$ sudo /sbin/iptables -L -n\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination\n-bash-4.2$ sudo /sbin/tcpdump -i eth0 arp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes\n12:39:17.511609 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:17.831438 ARP, Request who-has ucf-c1z1-rtr-1-v121-hsrp.ucf.oracle.com tell slc03qmy.us.oracle.com, length 46\n12:39:18.018123 ARP, Reply slc05gdy.us.oracle.com is-at 02:16:3e:52:b3:2f (oui Unknown), length 46\n12:39:18.249454 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:18.437566 ARP, Request who-has slc03qnm.us.oracle.com tell slc03qwg.us.oracle.com, length 46\n12:39:18.514568 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:18.668144 ARP, Request who-has ucf-c1z1-rtr-1-v121-hsrp.ucf.oracle.com tell slc05krg.us.oracle.com, length 46\n12:39:18.813865 ARP, Request who-has slc09fkf.us.oracle.com (Broadcast) tell ucf-c1z1-rtr-2-v121.ucf.oracle.com, length 46\n12:39:19.098823 ARP, Request who-has slc07jey.us.oracle.com tell slc07jdo.us.oracle.com, length 46\n12:39:19.098834 ARP, Reply slc07jey.us.oracle.com is-at 00:21:f6:2c:0d:c8 (oui Unknown), length 28\n12:39:19.517608 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:19.812425 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:20.016368 ARP, Reply slc05gdx.us.oracle.com is-at 02:16:3e:52:b3:2e (oui Unknown), length 46\n12:39:20.519606 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:20.815397 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:20.888729 ARP, Request who-has ucf-c1z1-rtr-1-v121-hsrp.ucf.oracle.com tell slc05kqp.us.oracle.com, length 46\n12:39:20.979774 ARP, Request who-has slc05gdv.us.oracle.com (Broadcast) tell slc05gdv.us.oracle.com, length 46\n12:39:21.256104 ARP, Request who-has slc05gdy.us.oracle.com (Broadcast) tell slc05gdy.us.oracle.com, length 46\n12:39:21.440497 ARP, Request who-has ucf-c1z1-rtr-1-v121-hsrp.ucf.oracle.com tell slc05kvo.us.oracle.com, length 46\n12:39:21.521636 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:21.817435 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:22.523586 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:22.825122 ARP, Request who-has slc09fkf.us.oracle.com (Broadcast) tell ucf-c1z1-rtr-2-v121.ucf.oracle.com, length 46\n12:39:22.855255 ARP, Request who-has slc03qtz.us.oracle.com tell slc03psg.us.oracle.com, length 46\n12:39:23.106625 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:23.525614 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:23.688743 ARP, Request who-has slc09fjc.us.oracle.com tell slc09fjf.us.oracle.com, length 46\n12:39:23.689262 ARP, Request who-has slc09fjf.us.oracle.com tell slc09fjc.us.oracle.com, length 46\n12:39:23.776303 ARP, Request who-has slc09fjb.us.oracle.com tell slc09fix.us.oracle.com, length 46\n12:39:24.107465 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:24.614436 ARP, Request who-has slc07jfm.us.oracle.com (Broadcast) tell slc07jfm.us.oracle.com, length 46\n12:39:25.109527 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:26.215549 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:27.217570 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:27.538667 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:27.759531 ARP, Request who-has slc03pzb.us.oracle.com (Broadcast) tell slc03pzb.us.oracle.com, length 46\n12:39:27.979521 ARP, Request who-has ucf-c1z1-rtr-1-v121-hsrp.ucf.oracle.com tell slc09fjb.us.oracle.com, length 46\n12:39:28.114673 ARP, Request who-has slc09fqo.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:28.219529 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:28.506451 ARP, Request who-has slc09flz.us.oracle.com tell slc03qwv.us.oracle.com, length 46\n12:39:28.541656 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:29.117748 ARP, Request who-has slc09fqo.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:29.150238 ARP, Request who-has slc07jix.us.oracle.com (Broadcast) tell slc07jix.us.oracle.com, length 46\n12:39:29.257504 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:29.543724 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:29.819022 ARP, Request who-has ucf-c1z1-rtr-1-v121-hsrp.ucf.oracle.com tell slc09fje.us.oracle.com, length 46\n12:39:30.119743 ARP, Request who-has slc09fqo.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:30.259559 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n12:39:30.546726 ARP, Request who-has slc09fqh.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:30.827183 ARP, Request who-has slc09fkf.us.oracle.com (Broadcast) tell ucf-c1z1-rtr-2-v121.ucf.oracle.com, length 46\n12:39:31.122735 ARP, Request who-has slc09fqo.us.oracle.com tell slc09fqg.us.oracle.com, length 46\n12:39:31.261568 ARP, Request who-has slc09fqm.us.oracle.com tell slc09fqi.us.oracle.com, length 46\n52 packets captured\n61 packets received by filter\n0 packets dropped by kernel\n-bash-4.2$\nThanks,\nRobert\n. Ugh, the output of ip addr show was mangled above...here it is again replacing greater than and less than signs with curly braces\n-bash-4.2$ /sbin/ip addr show\n1: lo: {LOOPBACK,UP,LOWER_UP}--- mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: -{BROADCAST,MULTICAST,UP,LOWER_UP} mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 00:21:f6:2c:0d:c8 brd ff:ff:ff:ff:ff:ff\n    inet 10.240.182.228/21 brd 10.240.183.255 scope global eth0\n    inet6 2606:b400:2010:4055:221:f6ff:fe2c:dc8/64 scope global dynamic\n       valid_lft 2591844sec preferred_lft 604644sec\n    inet6 fe80::221:f6ff:fe2c:dc8/64 scope link\n       valid_lft forever preferred_lft forever\n43: flannel0: {POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP} mtu 1472 qdisc pfifo_fast state UNKNOWN qlen 500\n    link/none\n    inet 192.168.55.0/16 scope global flannel0\n44: docker0: {BROADCAST,MULTICAST,UP,LOWER_UP} mtu 1472 qdisc noqueue state UP\n    link/ether 56:84:7a:fe:97:99 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.55.1/24 scope global docker0\n    inet6 fe80::5484:7aff:fefe:9799/64 scope link\n       valid_lft forever preferred_lft forever\n48: veth1af52cc: {BROADCAST,UP,LOWER_UP} mtu 1472 qdisc noqueue master docker0 state UP\n    link/ether 96:99:a8:54:c5:d1 brd ff:ff:ff:ff:ff:ff\n    inet6 fe80::9499:a8ff:fe54:c5d1/64 scope link\n       valid_lft forever preferred_lft forever\n. Yeah, I had already looked at these...everything looks correct.  Not sure where to find the flannel logs...\nOn host1 (working):\n-bash-4.2$ hostname\nslc06ptm.us.oracle.com\n-bash-4.2$ ping slc06ptm.us.oracle.com\nPING slc06ptm.us.oracle.com (10.240.212.144) 56(84) bytes of data.\n64 bytes from slc06ptm.us.oracle.com (10.240.212.144): icmp_seq=1 ttl=64 time=0.028 ms\n64 bytes from slc06ptm.us.oracle.com (10.240.212.144): icmp_seq=2 ttl=64 time=0.024 ms\n--- slc06ptm.us.oracle.com ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 999ms\nrtt min/avg/max/mdev = 0.024/0.026/0.028/0.002 ms\n-bash-4.2$ etcdctl ls /coreos.com/network/subnets/\n/coreos.com/network/subnets/192.168.55.0-24\n/coreos.com/network/subnets/192.168.83.0-24\n-bash-4.2$ etcdctl get /coreos.com/network/subnets/192.168.83.0-24\n{\"PublicIP\":\"10.240.212.144\"}\n-bash-4.2$ etcdctl get /coreos.com/network/subnets/192.168.55.0-24\n{\"PublicIP\":\"10.240.182.228\"}\n-bash-4.2$\nOn host2(not working):\n-bash-4.2$ hostname\nslc07jey.us.oracle.com\n-bash-4.2$ ping slc07jey.us.oracle.com\nPING slc07jey.us.oracle.com (10.240.182.228) 56(84) bytes of data.\n64 bytes from slc07jey.us.oracle.com (10.240.182.228): icmp_seq=1 ttl=64 time=0.047 ms\n64 bytes from slc07jey.us.oracle.com (10.240.182.228): icmp_seq=2 ttl=64 time=0.034 ms\n--- slc07jey.us.oracle.com ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 999ms\nrtt min/avg/max/mdev = 0.034/0.040/0.047/0.009 ms\n-bash-4.2$ etcdctl ls /coreos.com/network/subnets/\n/coreos.com/network/subnets/192.168.55.0-24\n/coreos.com/network/subnets/192.168.83.0-24\n-bash-4.2$ etcdctl get /coreos.com/network/subnets/192.168.55.0-24\n{\"PublicIP\":\"10.240.182.228\"}\n-bash-4.2$ etcdctl get /coreos.com/network/subnets/192.168.83.0-24\n{\"PublicIP\":\"10.240.212.144\"}\n-bash-4.2$\n. Yesterday, on the machine that is failing, flannel was adding both subnets.  Today, it is not...looks like it is failing on a put to etcd:\nJul 17 17:50:57 slc07jey flanneld: I0717 17:50:57.534711 01331 main.go:275] Installing signal handlers\nJul 17 17:50:57 slc07jey flanneld: I0717 17:50:57.535177 01331 main.go:189] Using 10.240.182.228 as external interface\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: get /coreos.com/network/config [http://127.0.0.1:4001]\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: send.request.to http://127.0.0.1:4001/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: network error: Get http://127.0.0.1:4001/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false: dial tcp 127.0.0.1:4001: connection refused\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: Connecting to etcd: attempt 2 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: network error: Get http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false: dial tcp 127.0.0.1:2379: connection refused\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: Connecting to etcd: attempt 3 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: network error: Get http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false: dial tcp 127.0.0.1:2379: connection refused\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: Connecting to etcd: attempt 4 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: send.request.to http://127.0.0.1:4001/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: network error: Get http://127.0.0.1:4001/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false: dial tcp 127.0.0.1:4001: connection refused\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: Connecting to etcd: attempt 5 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\nJul 17 17:50:57 slc07jey flanneld: go-etcd2015/07/17 17:50:57 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: recv.response.from\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: recv.success\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: get /coreos.com/network/config [http://127.0.0.1:2379]\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/config?quorum=false&recursive=false&sorted=false | method GET\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: recv.response.from\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: recv.success\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: recv.response.from\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: recv.success\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: put /coreos.com/network/subnets/192.168.55.0-24, {\"PublicIP\":\"10.240.182.228\"}, ttl: 86400, [http://127.0.0.1:2379]\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:50:58 slc07jey flanneld: go-etcd2015/07/17 17:50:58 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:04 slc07jey flanneld: go-etcd2015/07/17 17:51:04 DEBUG: recv.response.from\nJul 17 17:51:04 slc07jey flanneld: go-etcd2015/07/17 17:51:04 WARNING: bad response status code500\nJul 17 17:51:04 slc07jey flanneld: go-etcd2015/07/17 17:51:04 DEBUG: Connecting to etcd: attempt 2 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:04 slc07jey flanneld: go-etcd2015/07/17 17:51:04 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:10 slc07jey flanneld: go-etcd2015/07/17 17:51:10 DEBUG: recv.response.from\nJul 17 17:51:10 slc07jey flanneld: go-etcd2015/07/17 17:51:10 WARNING: bad response status code500\nJul 17 17:51:10 slc07jey flanneld: go-etcd2015/07/17 17:51:10 DEBUG: Connecting to etcd: attempt 3 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:10 slc07jey flanneld: go-etcd2015/07/17 17:51:10 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:16 slc07jey flanneld: go-etcd2015/07/17 17:51:16 DEBUG: recv.response.from\nJul 17 17:51:16 slc07jey flanneld: go-etcd2015/07/17 17:51:16 WARNING: bad response status code500\nJul 17 17:51:16 slc07jey flanneld: go-etcd2015/07/17 17:51:16 DEBUG: Connecting to etcd: attempt 4 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:16 slc07jey flanneld: go-etcd2015/07/17 17:51:16 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:22 slc07jey flanneld: go-etcd2015/07/17 17:51:22 DEBUG: recv.response.from\nJul 17 17:51:22 slc07jey flanneld: E0717 17:51:22.829497 01331 etcd.go:90] Failed to acquire subnet: 501: All the given peers are not reachable (failed to propose on members [http://127.0.0.1:4001 http://127.0.0.1:2379] twice [last error: Unexpected HTTP status code]) [0]\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: recv.response.from\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: recv.success\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: put /coreos.com/network/subnets/192.168.55.0-24, {\"PublicIP\":\"10.240.182.228\"}, ttl: 86400, [http://127.0.0.1:2379]\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:23 slc07jey flanneld: go-etcd2015/07/17 17:51:23 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:29 slc07jey flanneld: go-etcd2015/07/17 17:51:29 DEBUG: recv.response.from\nJul 17 17:51:30 slc07jey flanneld: go-etcd2015/07/17 17:51:30 WARNING: bad response status code500\nJul 17 17:51:30 slc07jey flanneld: go-etcd2015/07/17 17:51:30 DEBUG: Connecting to etcd: attempt 2 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:30 slc07jey flanneld: go-etcd2015/07/17 17:51:30 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:36 slc07jey flanneld: go-etcd2015/07/17 17:51:36 DEBUG: recv.response.from\nJul 17 17:51:36 slc07jey flanneld: go-etcd2015/07/17 17:51:36 WARNING: bad response status code500\nJul 17 17:51:36 slc07jey flanneld: go-etcd2015/07/17 17:51:36 DEBUG: Connecting to etcd: attempt 3 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:36 slc07jey flanneld: go-etcd2015/07/17 17:51:36 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:42 slc07jey flanneld: go-etcd2015/07/17 17:51:42 DEBUG: recv.response.from\nJul 17 17:51:42 slc07jey flanneld: go-etcd2015/07/17 17:51:42 WARNING: bad response status code500\nJul 17 17:51:42 slc07jey flanneld: go-etcd2015/07/17 17:51:42 DEBUG: Connecting to etcd: attempt 4 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:42 slc07jey flanneld: go-etcd2015/07/17 17:51:42 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:48 slc07jey flanneld: go-etcd2015/07/17 17:51:48 DEBUG: recv.response.from\nJul 17 17:51:48 slc07jey flanneld: E0717 17:51:48.610513 01331 etcd.go:90] Failed to acquire subnet: 501: All the given peers are not reachable (failed to propose on members [http://127.0.0.1:4001 http://127.0.0.1:2379] twice [last error: Unexpected HTTP status code]) [0]\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: recv.response.from\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: recv.success\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: put /coreos.com/network/subnets/192.168.55.0-24, {\"PublicIP\":\"10.240.182.228\"}, ttl: 86400, [http://127.0.0.1:2379]\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:49 slc07jey flanneld: go-etcd2015/07/17 17:51:49 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:51:55 slc07jey flanneld: go-etcd2015/07/17 17:51:55 DEBUG: recv.response.from\nJul 17 17:51:55 slc07jey flanneld: go-etcd2015/07/17 17:51:55 WARNING: bad response status code500\nJul 17 17:51:55 slc07jey flanneld: go-etcd2015/07/17 17:51:55 DEBUG: Connecting to etcd: attempt 2 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:51:55 slc07jey flanneld: go-etcd2015/07/17 17:51:55 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:01 slc07jey flanneld: go-etcd2015/07/17 17:52:01 DEBUG: recv.response.from\nJul 17 17:52:02 slc07jey flanneld: go-etcd2015/07/17 17:52:02 WARNING: bad response status code500\nJul 17 17:52:02 slc07jey flanneld: go-etcd2015/07/17 17:52:02 DEBUG: Connecting to etcd: attempt 3 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:02 slc07jey flanneld: go-etcd2015/07/17 17:52:02 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:08 slc07jey flanneld: go-etcd2015/07/17 17:52:08 DEBUG: recv.response.from\nJul 17 17:52:08 slc07jey flanneld: go-etcd2015/07/17 17:52:08 WARNING: bad response status code500\nJul 17 17:52:08 slc07jey flanneld: go-etcd2015/07/17 17:52:08 DEBUG: Connecting to etcd: attempt 4 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:08 slc07jey flanneld: go-etcd2015/07/17 17:52:08 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:14 slc07jey flanneld: go-etcd2015/07/17 17:52:14 DEBUG: recv.response.from\nJul 17 17:52:14 slc07jey flanneld: E0717 17:52:14.391613 01331 etcd.go:90] Failed to acquire subnet: 501: All the given peers are not reachable (failed to propose on members [http://127.0.0.1:4001 http://127.0.0.1:2379] twice [last error: Unexpected HTTP status code]) [0]\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: recv.response.from\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: recv.success\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: put /coreos.com/network/subnets/192.168.55.0-24, {\"PublicIP\":\"10.240.182.228\"}, ttl: 86400, [http://127.0.0.1:2379]\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:15 slc07jey flanneld: go-etcd2015/07/17 17:52:15 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:21 slc07jey flanneld: go-etcd2015/07/17 17:52:21 DEBUG: recv.response.from\nJul 17 17:52:21 slc07jey flanneld: go-etcd2015/07/17 17:52:21 WARNING: bad response status code500\nJul 17 17:52:21 slc07jey flanneld: go-etcd2015/07/17 17:52:21 DEBUG: Connecting to etcd: attempt 2 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:21 slc07jey flanneld: go-etcd2015/07/17 17:52:21 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:27 slc07jey flanneld: go-etcd2015/07/17 17:52:27 DEBUG: recv.response.from\nJul 17 17:52:27 slc07jey flanneld: go-etcd2015/07/17 17:52:27 WARNING: bad response status code500\nJul 17 17:52:27 slc07jey flanneld: go-etcd2015/07/17 17:52:27 DEBUG: Connecting to etcd: attempt 3 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:27 slc07jey flanneld: go-etcd2015/07/17 17:52:27 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:33 slc07jey flanneld: go-etcd2015/07/17 17:52:33 DEBUG: recv.response.from\nJul 17 17:52:34 slc07jey flanneld: go-etcd2015/07/17 17:52:34 WARNING: bad response status code500\nJul 17 17:52:34 slc07jey flanneld: go-etcd2015/07/17 17:52:34 DEBUG: Connecting to etcd: attempt 4 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:34 slc07jey flanneld: go-etcd2015/07/17 17:52:34 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:40 slc07jey flanneld: go-etcd2015/07/17 17:52:40 DEBUG: recv.response.from\nJul 17 17:52:40 slc07jey flanneld: E0717 17:52:40.172270 01331 etcd.go:90] Failed to acquire subnet: 501: All the given peers are not reachable (failed to propose on members [http://127.0.0.1:4001 http://127.0.0.1:2379] twice [last error: Unexpected HTTP status code]) [0]\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?quorum=false&recursive=true&sorted=false | method GET\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: recv.response.from\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: recv.success\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: put /coreos.com/network/subnets/192.168.55.0-24, {\"PublicIP\":\"10.240.182.228\"}, ttl: 86400, [http://127.0.0.1:2379]\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets/192.168.55.0-24\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets/192.168.55.0-24 | method PUT\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: recv.response.from\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: recv.success\nJul 17 17:52:41 slc07jey flanneld: I0717 17:52:41.255426 01331 etcd.go:83] Subnet lease acquired: 192.168.55.0/24\nJul 17 17:52:41 slc07jey flanneld: I0717 17:52:41.275771 01331 ipmasq.go:47] Adding iptables rule: FLANNEL -d 192.168.0.0/16 -j ACCEPT\nJul 17 17:52:41 slc07jey flanneld: I0717 17:52:41.278856 01331 ipmasq.go:47] Adding iptables rule: FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE\nJul 17 17:52:41 slc07jey flanneld: I0717 17:52:41.303215 01331 ipmasq.go:47] Adding iptables rule: POSTROUTING -s 192.168.0.0/16 -j FLANNEL\nJul 17 17:52:41 slc07jey flanneld: I0717 17:52:41.315317 01331 udp.go:221] Watching for new subnet leases\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: rawWatch /coreos.com/network/subnets []\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: get /coreos.com/network/subnets [http://127.0.0.1:2379]\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: Connecting to etcd: attempt 1 for keys/coreos.com/network/subnets?recursive=true&wait=true\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: send.request.to http://127.0.0.1:2379/v2/keys/coreos.com/network/subnets?recursive=true&wait=true | method GET\nJul 17 17:52:41 slc07jey flanneld: go-etcd2015/07/17 17:52:41 DEBUG: recv.response.from\n. I just updated flannel from Git, rebuilt, and restarted.  Now, everything is working fine.  Thanks again for your patience and help!\n. ",
    "bhouse": "sorry just saw #253, closing this as a duplicate\n. thanks!\n. ",
    "cusspvz": "After reading a huge part of this repo code and submitted issues I would suggest the addition of an attribute on SubNet called PrivateIp.\nPublicIp would be the endpoint for node external relative connections and PrivateIp for internal configurations such as interface and ip tables. For avoiding BC on future CoreOS updates, PrivateIp should default to PublicIp in case it isn't defined or is invalid;\nPros:\n- Real distinguish between PublicIP and PrivateIP (as CoreOS does);\n- udp backend could be used on different network structures (depending on them, vxnet could be also), allowing expansion of a cluster on different IaaS providers or even bare metal without having to configure their network structures manually;\nCons: \n- Don't know if it could be a BC on updates. Suppose that it hasn't with defaulting of PrivateIp by PublicIp, but as I'm not sure it could be a Con.\nI would like to hear your oppinion @eyakubovich .\n. @eyakubovich sorry for not being 100% verbose. :p \nflannel accepts an interface configuration which can be an interface name or ip, and uses that config to define internal variables extIf and extIp. Those variables should match so it can add routes properly on running system.\nWe have 5 Azure accounts and 2 AWS ones. As far as i know, there isn't a way to connect multiple azure clouds subnets, or even to connect them with AWS subnets.\nIf I set interface as our $public_ip, it wont work since Azure and AWS have their Public IP traffic routed into virtual subnet and there aren't interfaces on nodes configured with their public ips.\nMy proposal is to:\n- add a configuration where you can set an host ip endpoint;\n- renaming of PublicIp to InterfaceIp (makes more sense, i think);\n- in case host has received and endpoint ip, it should be set on subnet config as PublicIp or EndpointIp;\n\nAre you running in IaaS?\n\nAlthough I've already referenced Azure and AWS, the only PaaS we will use from them is their cloud storage.\n\nI'm not sure why your nodes can't communicate over their private IP?\n\nWe are configuring each node on Azure: is unique on its cloud (one cloud per node), has a storage (one storage per node). Same will happen on AWS. That will allow us to move and scale along providers and be IaaS independent.\n. @eyakubovich could you please review implementation on backends?\nSpecially on vxnet, not sure if i did it right. :)\n. > I want to think through whether we need to store both PublicIP and InterfaceIP in etcd lease. The only reason is to renew the lease by that IP. We could renew by PublicIP as well but I don't know if it's more \"dangerous\".\nWell, it might happen two nodes have the same private IP, since subnets are auto-generated by default (at least on Azure), so I see PublicIP as more reliable on that aspect, but as I don't see the whole thing as you do, I cannot say whats better or not. :P \n\nI like the overall direction. I didn't want to add this NAT support prior as without encryption, going between data centers is dangerous. But we're adding IPSec backend soon so is a good addition at this time.\n\nNot so bad if proxied connections are secured, but definitely it should have some SSL on top, at least on UDP backend (don't know if it is possible to implement SSL on VXNET).\nSo, IPSec feature implementation is already on a milestone? If so, nice to hear! :)\n. By the way, whats the difficulty of implementing an optional SSL support on top of udp backend?\n. Waiting for your overall before squashing.\n. @eyakubovich do you have more notes to do on this? :)\n. @eyakubovich Is there a way to check if this is working by compiling and launching 2 containers?\nI'm using Mac OS X and docker under boot2docker vm.\n. Done! :)\n. Since this is gonna to be merged, what do you think about adding an option to this on cloudinit flannel file?\n. Would be awesome and easy to configure on machine provisioning for those who are on NAT:\n```\ncloud-config\ncoreos:\n    flannel:\n        public-ip: $public_ipv4\n```\n. > Can you add an issue for that in https://github.com/coreos/coreos-cloudinit?\nCreated a PR instead. :)\n\nCan you launch two instances of those VMs and communicate between them? Otherwise you can test on AWS as it will give you the most realistic environment (since they do NAT).\n\nWill try that right now.\nCould you please just explain how does flannel releases flow and when are them available on CoreOS ones?\n. Sorry, let me try to fragment my question:\nAre you in charge of releasing version of flannel?\nDoes Alpha releases include always the newest release of flannel?\nWhat are the requisites (if there are some) to release a new version of flannel and of CoreOS?\n. With those questions I pretend to predict when this changes would be available on CoreOS release channels. :)\n. @eyakubovich I'm currently testing by trying to get up test clusters but i keep getting this error message repeatedly:\nsdnotify-proxy[1127]: E0724 17:26:38.766011 00001 network.go:80] Failed to set up IP Masquerade for network : failed to setup IP Masquerade. iptables was not found\nHere is my flanneld.service\n```\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nRequires=early-docker.service\nAfter=etcd.service etcd2.service early-docker.service\nBefore=early-docker.target\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=/etc/environment\nEnvironment=TMPDIR=/var/tmp/\nEnvironment=DOCKER_HOST=unix:///var/run/early-docker.sock\nEnvironment=FLANNEL_VER={{flannel_ver}}\nEnvironment=ETCD_SSL_DIR=/etc/ssl/etcd\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStartPre=/usr/bin/touch /run/flannel/options.env\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock /usr/bin/docker run --net=host --privileged=true --rm --volume=/run/flannel:/run/flannel --env=NOTIFY_SOCKET=/run/flannel/sd.sock --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} --env-file=/run/flannel/options.env --env=FLANNEL_PUBLIC_IP=${COREOS_PUBLIC_IPV4} --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro --volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro cusspvz/flannel:latest /go/bin/app --ip-masq=true\nExecStartPost=/usr/bin/docker run --net=host --rm -v /run:/run cusspvz/flannel:latest /go/src/app/dist/mk-docker-opts.sh -d /run/flannel_docker_opts.env -i\n```\nPS: cusspvz/flannel HEAD is at this PR merge commit.\nAm I doing something wrong?\n. I was in doubt, as so I let it over there. What do you prefer?\n. Since InterfaceIP is being used on backends, isn't better to add only json:\"-\"?\nAs I said, it is the first time I am developing on go, and as far as I had understood if I remove this line it will complain. Or am I wrong?\n. Done! :)\n. Added json:\"-\", please warn me if you want and its safe to remove this line.\n. Done, once again, warn me to squash in case you think everything is as it should be. :)\n. should be PublicIP here?\n. Done and squashed. :)\n. ",
    "syed": "Update: I am using the VXLAN backend. I have to mention that I rebooted with a more recent kernel (3.19) from the default (3.13). I rebooted back with the older kernel (3.13) and it seems to work now. \nDoes anyone else have this problem? \n. I'm running the master.  Here is the error\nI0723 05:40:20.880912 03061 vxlan.go:280] fdb already populated with: <nil> 33:33:00:00:00:01\nI0723 05:40:20.881117 03061 vxlan.go:280] fdb already populated with: <nil> 01:00:5e:00:00:01\nI0723 05:40:20.881280 03061 vxlan.go:280] fdb already populated with: <nil> 33:33:ff:c0:7c:2a\nMy host has the following interfaces and their mac addresses.\ndocker0:  56:84:7a:fe:97:99\neth0: 02:00:09:0e:00:0a\nflannel1.1: 2e:68:05:c0:7c:2a\nAdding more debug info\nroot@i-sahmed-node1: ~ # bridge fdb show\n33:33:00:00:00:01 dev eth0 self permanent\n01:00:5e:00:00:01 dev eth0 self permanent\n33:33:ff:0e:00:0a dev eth0 self permanent\n33:33:00:00:00:01 dev docker0 self permanent\n01:00:5e:00:00:01 dev docker0 self permanent\n56:84:7a:fe:97:99 dev docker0 vlan 1 permanent\n56:84:7a:fe:97:99 dev docker0 permanent\n33:33:00:00:00:01 dev flannel.1 self permanent\n01:00:5e:00:00:01 dev flannel.1 self permanent\n33:33:ff:c0:7c:2a dev flannel.1 self permanent\n. ",
    "PCQ": "I found the problem. Ubuntus repros currently have the go version go1.2.1 linux/amd64. The go version >1.4 is needed. So at the moment do not install golang with apt-get install on Ubuntu\n. ",
    "meghanschofield": "removed that file and rebased\n. ",
    "klizhentas": "just tried 3.18.18-031818-generic with the same result. Seems something have changed since 3.13\n. @eyakubovich do you know the version > 3.13 that works? \n. sure, is there any particular version you would like to check?\n. also, can you point me to the chunk of code that you think is not triggered? I can add debugging info and see what's not working \n. I've tried latest stable 3.18.18-031818-generic and it has the same problem, I will try 4.x and will update the ticket\n. tested on the 3.19 and it now works fine, thanks!\n. ",
    "changx": "I have the same issue on 3.19.3-031903-generic,  it works on 3.16.0-0.bpo ( aka 3.16.7-ckt11   wheezy-backports ) , but I can not reverse back due to 3.16.7 racing condition under kvm :/ .\n. ",
    "linfan": "For Item 2: I understand the -m is for remove DOCKER_OPT_IPMASQ from output.\nBut the actually result is, no matter '-m' is used or not, DOCKER_OPT_IPMASQ won't added. See below:\n$ sudo flannel-0.5.2/mk-docker-opts.sh -i -m\n$ cat /run/docker_opts.env\nDOCKER_OPT_BIP=\"--bip=172.17.78.1/24\"\nDOCKER_OPT_MTU=\"--mtu=8973\"\n$ sudo flannel-0.5.2/mk-docker-opts.sh -i\n$ cat /run/docker_opts.env\nDOCKER_OPT_BIP=\"--bip=172.17.78.1/24\"\nDOCKER_OPT_MTU=\"--mtu=8973\"\nAnd it's caused by judgement variable 'FLANNEL_IPMASQ' no defined.\n. Got the point, FLANNEL_IPMASQ is sourced from /run/flannel/subnet.env file.\n. Hi Eugene,\nI had a look in the new script, it seems that FLANNEL_IPMASQ variable still undefined.\nSo DOCKER_OPT_IPMASQ still never output.\n```\n$ sudo flannel-0.5.2/mk-docker-opts.sh -i -m        <-- with 'm' parameter\n$ cat /run/docker_opts.env\nDOCKER_OPT_BIP=\"--bip=172.17.78.1/24\"\nDOCKER_OPT_MTU=\"--mtu=8973\"\n$ sudo flannel-0.5.2/mk-docker-opts.sh -i        <-- without 'm' parameter\n$ cat /run/docker_opts.env\nDOCKER_OPT_BIP=\"--bip=172.17.78.1/24\"\nDOCKER_OPT_MTU=\"--mtu=8973\"\n```\n. Hi Eugene,\nYes, I just got the point!!\nIn this case, the fix looks good \ud83d\ude00\n. ",
    "eghobo": "@eyakubovich: but how I can pass -m option if i run flannel inside docker container at CoreOS. for host-gw case we need --ip-masq=true for flannel and docker.\n. @eyakubovich: what kind of conflict? we run like thins at Ubuntu, i enabled --ip-masq=true to make kub-proxy works. And for CoreOS I need to enable it for host-gw mode, otherwise containers cannot ping each other #506\n. ip-172-29-1-203.us-west-2.compute.internal\n```\ndocker run -it busybox\n/ # ifconfig\neth0      Link encap:Ethernet  HWaddr 02:42:0A:02:62:02\n          inet addr:10.2.98.2  Bcast:0.0.0.0  Mask:255.255.255.0\n          inet6 addr: fe80::42:aff:fe02:6202/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1\n          RX packets:6 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:7 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:508 (508.0 B)  TX bytes:598 (598.0 B)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n/ #\n```\nip-172-29-1-44.us-west-2.compute.internal\n```\nping 10.2.98.2\nPING 10.2.98.2 (10.2.98.2) 56(84) bytes of data.\n64 bytes from 10.2.98.2: icmp_seq=1 ttl=63 time=0.479 ms\n64 bytes from 10.2.98.2: icmp_seq=2 ttl=63 time=0.456 ms\ndocker run -it busybox\n/ # ping 10.2.98.2\nPING 10.2.98.2 (10.2.98.2): 56 data bytes\n``\n. @eyakubovich: yes i did and i found that--ip-masq=true` at docker helps (containers can ping each other but i need to test kube-proxy).\n. @eyakubovich: i cannot ping container to container without masquerade, but i can ping container from host.\n. @anguslees: your are right, i need to add flannel network cidr to AWS security rules to allow inbound traffic. i own you a drink ;), unfortunately I am not going to Barcelona but definitely will be in Boston.\n@eyakubovich: thx a lot for your help as well.\n. we see exact same behavior with Stable/1185.3.0 and flannel v0.6.2, @eyakubovich: any suggestions?\n. @chulkilee: increasing ELB idle timeout helps, I see these errors less often. I think it's happens because ELB close inactive connection and flannel trying reconnect.\n. ",
    "gangadhars": "@eyakubovich, As you said I changed the content of  /etc/default/flanneld to FLANNEL_OPTS=\"--public-ip=172.16.2.10\"\nNow flanneld is not running and log says:\nflag provided but not defined: -public-ip\nUsage of /opt/bin/flanneld:\n  -alsologtostderr=false: log to standard error as well as files\n  -etcd-cafile=\"\": SSL Certificate Authority file used to secure etcd communication\n  -etcd-certfile=\"\": SSL certification file used to secure etcd communication\n  -etcd-endpoints=\"http://127.0.0.1:4001,http://127.0.0.1:2379\": a comma-delimited list of etcd endpoints\n  -etcd-keyfile=\"\": SSL key file used to secure etcd communication\n  -etcd-prefix=\"/coreos.com/network\": etcd prefix\n  -help=false: print this message\n  -iface=\"\": interface to use (IP or name) for inter-host communication\n  -ip-masq=false: setup IP masquerade rule for traffic destined outside of overlay network\n  -log_backtrace_at=:0: when logging hits line file:N, emit a stack trace\n  -log_dir=\"\": If non-empty, write log files in this directory\n  -logtostderr=false: log to standard error instead of files\n  -stderrthreshold=0: logs at or above this threshold go to stderr\n  -subnet-file=\"/run/flannel/subnet.env\": filename where env variables (subnet and MTU values) will be written to\n  -v=0: log level for V logs\n  -version=false: print version and exit\n  -vmodule=: comma-separated list of pattern=N settings for file-filtered logging\nAnd also I tried with ./flanneld --public-ip=172.16.2.10 and printing same error.\nI'm using flannel version 0.4.0\nHow to run --public-ip option?\n. Thanks @eyakubovich \nProblem is flannel is using eth0 interface by default. When I run flannel with --iface=tap0 it's working good.\n. I run reconfDocker.sh script agan and everything is working fine. You can close the issue. \nBut when I look into the  reconfDocker.sh:\n```\nsource ~/kube/config-default.sh\nattempt=0\nwhile true; do\n  /opt/bin/etcdctl get /coreos.com/network/config\n  if [[ \"$?\" == 0 ]]; then\n    break\n  else\n        # enough timeout??\n   ...\n   ...\n  fi\ndone\nwait some secs for /run/flannel/subnet.env ready\nsleep 15\nsudo ip link set dev docker0 down\nsudo brctl delbr docker0\n...\n```\nAfter running /opt/bin/etcdctl get /coreos.com/network/config, script is waiting for 15sec to create  /run/flannel/subnet.env file. I didn't understand how does it create /run/flannel/subnet.env file?\n. Yes. I forget to close the issue. It resolved on that time only by configuring the data and time.Thanks.\n. ",
    "vinayk06": "@eyakubovich  can you give a solution for this in coreos\n. ",
    "bprashanth": "I solved a similar problem on Kubernetes with a proxy flannel server https://github.com/kubernetes/kubernetes/pull/13877. The flannel daemons block till the master assigns the node a subnet, @eyakubovich let me know if you're interested on collaborating to a similar end goal. With my patch, the flannel daemon can only read subnets, not create or destroy them. \n. sounds like the proxy flannel server will still have to manage any persistence to etcd and redirect to apiserver. That's ok for node specific metadata (eg VTEP), not ok if it's going to delete a subnet from the global pool. IMO global resources should fall under the purview of the cluster manager, so as long as the daemon doesn't go mad if the flannel server no-ops a Delete/Add we should be ok. \nI actually don't need reservation mode currently because the daemon just hangs on /subnet/leases (whichever one of those endpoints it hits first) till the master tells it what subnet to use. \n. Just grepping the codebase, I think those should work. Will reopen if that isn't the case. \n. Providing some more context. Specifically this is using our docker multinode setup which bootstraps flannel in docker: https://github.com/kubernetes/kubernetes/blob/f94f6c48fdcb0c313d14ce852105465196b391fb/docs/getting-started-guides/docker-multinode.md\nI ran 2 containers (10.1.103.2, 10.1.75.8) on 2 nodes (10.240.0.5, 10.240.0.2). After trying to ping between them from 103.2 -> 75.8:\narp cache on 240.0.5:\n10.1.75.8 dev flannel.1  FAILED\n10.1.103.2 dev docker0 lladdr 02:42:0a:01:67:02 DELAY\n10.240.0.1 dev eth0 lladdr 42:01:0a:f0:00:01 REACHABLE\nBridge database on 240.0.5:\n01:00:5e:00:00:01 dev eth0 self permanent\n33:33:00:00:00:01 dev flannel.1 self permanent\n01:00:5e:00:00:01 dev flannel.1 self permanent\n33:33:00:00:00:01 dev docker0 self permanent\n01:00:5e:00:00:01 dev docker0 self permanent\n02:42:c0:6b:46:32 dev docker0 vlan 1 master docker0 permanent\n02:42:c0:6b:46:32 dev docker0 master docker0 permanent\n7e:aa:cd:b1:c1:af dev vethb621ff8 vlan 1 master docker0 permanent\n02:42:0a:01:67:02 dev vethb621ff8 master docker0 \n7e:aa:cd:b1:c1:af dev vethb621ff8 master docker0 permanent\n33:33:00:00:00:01 dev vethb621ff8 self permanent\n01:00:5e:00:00:01 dev vethb621ff8 self permanent\nI'm confused becaues I don't see the entries I'm expecting. The traffic leaves the container bridge, and the subnets match up, and docker is running with the right mtu/bip. Did I miss something obvious? \n. All ports are allowed between nodes in the cluster. netstat -ulv on port 8473 worked between the nodes in question. Tcpdump on flannel during the ping just shows arp requests\n$ ping 10.1.75.9\n23:56:29.374944 ARP, Request who-has 10.1.75.9 tell 10.1.103.0, length 28\n23:56:30.374957 ARP, Request who-has 10.1.75.9 tell 10.1.103.0, length 28\n23:56:34.374948 ARP, Request who-has 10.1.75.9 tell 10.1.103.0, length 28\n23:56:35.374948 ARP, Request who-has 10.1.75.9 tell 10.1.103.0, length 28\n23:56:36.374949 ARP, Request who-has 10.1.75.9 tell 10.1.103.0, length 28\n23:56:40.375006 ARP, Request who-has 10.1.75.9 tell 10.1.103.0, length 28\n. I noticed previous too that the logs are strangely quiet (I expected to see some miss log lines): https://gist.github.com/bprashanth/8b605c838117889b2400\nYes I can ping flannel.1\n. Ah, no I can't (I would be so confused if that worked). Yeah we're running flannel 0.5.0 and kernel 3.19.0-28-generic, @fgrzadkowski can you please upgrade flannel and update? \n. ",
    "andrewkrug": "How do you use this feature?   It looks like it was merged without updating any docs.\n. ",
    "YP28": "Thanks for your feedback @eyakubovich. I used that guide at first but modified it to fit my case (DigitalOcean uses the 10.0.0.0 address space) and tried some things out to test it.\nSo I did set it up properly. What I'm wondering though, is it comparable to this: https://docs.docker.com/articles/networking/#docker-ipv6-cluster\nOr does that do enable cross-host linking of containers? I can't really get that from the documentation and I'm guessing you looked into that before creating flannel? \nIf that is not the same based on ipv6, and does implement the ability to use the native docker link parameter to link containers, why not implement the ipv6 networking in flannel to enable it? And if it is the same as what flannel does, why didn't Docker choose to just use ipv4?\nEdit: thanks for mentioning SkyDNS, looks interesting and I'm definitely going to check it out!\n. ",
    "dcbw": "So the commit fixes the hang on my local machine, but doesn't fix the hang in Travis.  Clearly a more sufficient fix is required.  Any thoughts on this problem?\nThe reason I ask is that I'm trying to implement Network watching, and the Lease listeners are hanging around and screwing up the Network watch testcases.\n. Ok, so we end up having to suck in most of http.Transport and then hack in some cancelation code until we can use Go 1.5.  The latest push fixes the issue on both my local machine and on the Travis builds.\n. I'm not quite sure why the Travis build is failing, I cannot reproduce on two different machines with either Go 1.4.2 or Go 1.3.3... @rajatchopra any thoughts?  In the remote test on the Travis build, client.go::doTestWatchNetworks() times out waiting to receive the NetworkAdded event from the server.\n. [test]\n. @eyakubovich ok I'll rework to separate the watch structs.  Still chasing down why the Travis tests are failing too.\n. Ignore any asdfasdfadsf commits too, that's just my sad attempt to figure out why the Travis builds are failing.\n. @eyakubovich are the EventType strings part of the API/ABI too?  Do I need to keep those backwards compatible?\n. @eyakubovich I've reverted the EventType API string changes and fixed up the failing Travis test.  I think this branch is ready for more review.  Thanks!\n. I've got the stuff commented as \"Done\" done locally; I have to fix up one or two other things as a result of these, then figure out the cursor testcase issue, then I'll re-push, hopefully tomorrow.  Keep the comments coming if you're still reviewing, I'll get to them tomorrow or next week.  Thanks!\n(my next steps are factoring out the main.go network code into a network/manager.go that watches for networks coming and going, and creates or removes networks as needed, but that's another PR...)\n. @eyakubovich repushed; I believe all your comments have been addressed.  Thanks!\n. Repushed with fixes for all your previous comments.\n. > I know it's a long review but I think this PR getting very close.\nYeah, I've been trying to track down the remote testcase hang before replying, not there yet.  Appears to happen about 1/5 the time on the Travis CI machines.\nSince the server-side misses the \"add\" event it just sits in etcd::WatchNetworks()/mockreg::watch() waiting for something to happen, but nothing does.  The root cause appears to be some previous call waiting in the MockRegistry's watch() and grabbing the event from the queue before the server starts the new watch.\nSay the cursor gets updated to \"5\" in response to some random event or call.  Then a client watches networks with an older cursor \"4\", and we get to server.go::handleNetworks().  That calls into etcd.go::WatchNetworks() and then to mock_registry.go::watch() with cursor \"4\".  Which just sits there waiting for some new event which (in the case of the remote tests) never comes.\nShouldn't something on the server-side return a snapshot when the client-provided cursor is less than the registry cursor?  Something like this perhaps?\n```\nfunc (msr MockSubnetRegistry) watch(ctx context.Context, network string, since uint64) (etcd.Response, error) {\n    for {\n+       if since < msr.index {\n+           return nil, etcd.Error{\n+               Code: etcd.ErrorCodeEventIndexCleared,\n+               Cause: \"out of date\",\n+               Message: \"cursor is out of date\",\n+               Index: msr.index,\n+           }\n+       }\n    select {\n    case <-ctx.Done():\n\n```\n. > Shouldn't something on the server-side return a snapshot when the client-provided cursor is less than the registry cursor? Something like this perhaps?\nSo I added this and pushed it, and it seems to pass the testcases in Travis now much more reliably.  It seems like the correct thing to do, but your input would be appreciated here.  Thanks!\nRepushed, hopefully addressing all your previous comments.\n. @eyakubovich we discussed this internally yesterday with Nathan and others and so I'll dump some more comments in.\nThis solution is already requiring X.509, implying that certificates must exist on the system before flannel starts.  So why not use those certificates with the IPSec daemon and get host <-> host authentication via the CA certificate and the CRL, and encryption derived from the client keys?  The Swan daemon (at least libreswan, openswan, and freeswan; I have no clue about StrongSwan) can already use certificates to handle all this, and no static PSK in etcd is required.  That way we're using one source for everything here instead of creating/deplying certificates and setting a PSK somewhere.\nIn any case, if you still think static PSK is a good idea, we (Red Hat/OpenShift) really want to ensure that the authentication type (eg, PSK or X.509) can be specified with flannel, so that we can do the X.509 thing in the future anyway.\n. > ```\n\ngaining host<->host authentication: if you use the same certs as etcd, wouldn't you get the same set of nodes that can access any given host? What are you trying to gain in host<->host auth?\n```\n\nYes, you get the same set of nodes.  But the authentication would happen before any access to etcd, since the L3 communication channel between the node and etcd host would be authenticated long before etcd is even in the picture.  So you don't just gain protection for etcd, you gain protection for all traffic that might happen before accessing etcd.\nAgain, we already have the certificates and can secure communication before talking to etcd, so why wouldn't we want to use them?\n\nCRL: this is valid but this should be done at etcd level and propagated to out. So if you want to revoke a cert (node), you revoke it from accessing etcd and regeneration of PSK so it won't be able to auth when re-keying. This still needs to be implemented.\n\nIf you use certificates to secure communication before talking to etcd, then we don't need to do any of these steps, because the etcd node (and all other nodes) will simply refuse to talk to any node using a revoked certificate.  We don't even need to rekey anything or regenerate the PSK, because none of the nodes will accept traffic from the revoked node.  This also means there would be no network hiccups when revoking a cert, because the the PSK doesn't need to be regenerated and redistributed to all the nodes.\n\ndeploying certs and setting PSK: the PSK is generated by flannel and is an impl detail -- it does not have to be distributed so there's no administration overhead.\n\nIsn't the PSK distributed to nodes by etcd?  I guess I just mean administration overhead when doing revocations.  With a PSK you need to (a) distribute the CRL to the etcd node (and possibly to all nodes) and then change the PSK to kick the old node off.  When using certs, you just have the CRL step (albeit to all nodes).  That can all probably be scripted, but it's an additional step.\n\nNathaniel's concern about delegating L3 auth to L7: IKE is also an L7 protocol :)\n\nYes; but I feel like I'm not adequately expressing why I (we) think the certificate-not-PSK path is better/simpler.  Forgive me if this sounds like a broken record...\nThe current proposal is:\n1) use node certificates to talk to etcd via TLS to get the PSK\n2) send that PSK to the IKE daemon and start talking to other hosts\n3) start doing normal flannel stuff; reading networks, etc\nA pure certificate proposal is:\n1) send node certificate to the IKE daemon and start talking to other hosts\n2) start doing normal stuff; reading networks, etc\nJust using certificates means we don't have to bother with any of the etcd PSK handling.  Yes that's automated, but ISTM doing pure certificates does the following:\n1) cuts out a couple round-trips in the architecture to go to etcd and get the PSK\n2) cuts out required code in flannel to set/get the PSK in both the local and remote cases\n3) removes one point of failure (anything to do with PSK retrieval)\n4) secures host communication from the start and doesn't actually require etcd+TLS in the end\nTo me, PSK just seems like more work, and I don't get the benefit?  I could be wrong, but I don't think it's any simpler or less overhead than certs we're already installing and using?  Honestly interested in the advantages you see from the PSK scheme, probably I just don't realize what they are...\n(also, I'm happy to pitch in here, not just spout hot air)\n. @npmccallum can you double-check my understanding of the pure cert method?  Hopefully I got it all right based on the conversation Tuesday.\n. > I want to discuss and understand your case for using certs. Sorry, I am still not convinced. Is there a way we could do a Hangout to discuss in person? /cc @MohdAhmad\nI'll poke @npmccallum to see if we can set something up.\n. @npmccallum @eyakubovich @simo5 @MohdAhmad How about tomorrow (9/10) at 16:00 UTC (12:00 noon US EDT)?\n. @eyakubovich @MohdAhmad also I'd personally like to see the ipsec stuff as an internal package or library that can be used from any backend, not a backend on its own.  I'm sure it would be useful to secure routes for many of the backends (UDP, vxlan, ovs, whatever) via IPSec instead of having those backends be mutually exclusive with the ipsec backend.\n. > In meetings all morning till 12:30, maybe 1pm EST (17:00 UTC) ?\n@simo5 I'm fine with that.\n. 1pm EST works for me too\n. Hangout link: https://talkgadget.google.com/hangouts/_/g4jytwjgja3yepse7jaqebaqjea\n. @eyakubovich @MohdAhmad any chance you could join us for a chat now?\n. Thinking through how we'd most likely utilize IPSec, there are a few pieces:\n1) talking to etcd - flannel is passed the IP address of the etcd master on the command line, so it's trivial to set up an IPSec tunnel between the node and etcd using certificates\n2) talking to a master flannel - same thing as (1) here, we know the IP address of the master so it's easy to set up an IPSec tunnel between them\n3) talking to other nodes - here we do need to talk to etcd to figure out what other nodes are in the cluster; once we read the list of networks and leases (which include the IP address of the lessee) we'd create an IPSec tunnel for that node\nHowever, this could all be done outside flannel, if the management plane (eg, openshift/kube) created the IPSec links.  Since openshift/kube are already watching for other nodes it could create the IPSec tunnels outside of flannel, and flannel could simply be used in unencrypted/non-TLS mode.  One problem here is that flannel and openshift/kube startup is often controlled by an init system like systemd, and ensuring that openshift/kube was started and had set up the IPSec tunnel to etcd before flannel starts would not be simple.\n. > For what its worth, I actually agree with the model of using the already-generated TLS/SSL certs ... if Flanneld detects changes in the certs automatically without restarting. I didn't see this point discussed above, so I'm curious what your thoughts are on it.\n@diranged I don't think flanneld currently doesn't detect cert changes for its etcd communication or for the flannel remote capability.  So unless those parts get updated to handle cert changes along with the ipsec code, then you'd be left with working IPSec but completely broken etcd/remote still.  Doesn't seem helpful to me.\nBut I agree that having flannel detect this (through inotify perhaps) and handle it is a great long-term goal...\n. > What's the advantage on having it inside flannel again?\n@ibotty the advantage is that flannel is \"the thing that controls node networking\", and whether the node communications is encrypted is arguably something that flannel should also handle.  Granted, flannel isn't the only thing that benefits from having IPSec enabled to all other nodes in the cluster.  But having this handled in one place, where networking is done already, seems like the right place.  And if it was made into a library then all the backends could use it and it could become an option like 'IpMasq' currently is, perhaps.  Lastly, flannel is already watching for other nodes (via etcd) and that's the exact event that you'd use to set up/tear down an IPSec tunnel to that node.\nThe argument (as I see it) for having it outside of flannel is that technically, the IPSec operation really doesn't have anything to do with flannel and if it was configured before flannel, then flannel could be run un-encrypted over the encrypted link just like anything else on the node.  IPSec could be configured outside flannel, at node setup time, and then whatever is actually spawning containers on the node probably is already listening for other nodes, and it could do the required inter-node IPSec tunneling.\n@eyakubovich I'm still leaning more towards having this in flannel itself.  That avoids startup race conditions, flannel is already watching for other nodes, already knows the certificates, already knows the etcd server address (or the flanneld master address), etc.  Plus, maybe the the orchestrator on the node doesn't actually do anything network-related (openshift is possibly special here)...\n. > Generating and activating the ipsec subnet config in an internal flannel hook is reasonable. Even better if that happens before the subnet is routable.My point is that ipsec should not happen inside of flannel but with an external ipsec stack. That way: * it won't interfere with the host-to-host ipsec that might already exist; * It's also one less piece of software to audit and * it allows for different ipsec stacks (future linux or bsd's)coreos/flannel wrote: What's the advantage on having it inside flannel again?\n@ibotty Oh, I think I misunderstood your earlier comment.  In this PR and in our discussion, I don't think anyone is saying flannel should be doing a lot of IPSec by itself.  This PR only adds the XFRM policy in the kernel in a few LoC and then shells out to an external IKE to handle the key rotation.  I suppose the patch could instead do everything through the IKE daemon (xSwan or racoon or whatever) but those parts are obviously going to be implementation specific.\nMost of my comments have been about the general architecture, where the original key/auth material come from, and where the IPSec setup happens (outside or inside flannel, regardless of whether it touches the kernel directly or uses a xSwan), not about the specific implementation.  Mainly because I think the current implementation could be generalized so we could use it in our backend too :)\n. > @dcbw Apologies for slow review -- away at ContainerCon and such.\nNo problem; I was on vacation last week anyway.\n. Rebased and repushed with fixes for comments...\n. Rebased on backend-split-init, updated for your comments, and a further fix to the previous getNetworks() functionality to match what etcd actually returns.\n. @eyakubovich will do.  I'm working on a few fixups (of which the etcd getNetworks() patch is one) first though that were impacting further testing of this branch.  Hope to get those and another push of this branch done today.\nSince I don't have a PR for the fixups yet, I'll ask a question about them here.  It turns out that when the etcd client got switched to etcd/client, it broke cancel handling.  Because etcd/client returns ClusterError objects from its HTTP methods, those get passed down to registry.go and etcd.go, and then stuff like:\ncase err == context.Canceled, err == context.DeadlineExceeded:\ndoesn't work because 'err' is a ClusterError, not a context error.  How should this get fixed?  I had a couple approaches:\n1) strip ClusterErrors in registry.go and return only the first internal error; but this doesn't work because etcd.go checks for etcd.Error too.\n2) strip ClusterErrors in etcd.go and return only the first internal error; but this doesn't quite work either because callers are looking for only one error, and ClusterError can hold multiple errors\n3) create some kind of MatchError() function that knows about ClusterError and etcd.Error internals and does the right thing; this is kinda ugly because it leaks ClusterError semantics out to stuff that shouldn't know about etcd at all (like when the backends check for cancelation of AcquireLease()).  I guess this function should go into the subnet.Manager interface and we mandate that all callers of subnet.Manager use it for error checking?\nThoughts?  The orignal problem I had was that at Ctrl^C/exit time, flannel never exited because watch.go::WatchLeases() never sees the cancel because it's checking for context.Canceled, not ClusterError.Errors[0] == context.Canceled.\n. Repushed with fixups; also added network package to 'test' to get gofmt checking.  Should the 'gofmt -l' be changed to 'gofmt -d' to display diffs?  That's been useful to me in the past.\n. Ok, fair enough.  How about now?\n. > hehe -- I did leave it ambiguous :) I meant the latter one, having the backend expose a global function func NewNetwork(ctx context.Context, sm subnet.Manager, network string, config subnet.Config, iface Iface) Network\nAh, so you mean having the backend take responsibility for creating the Network objects, rather than the Manager?\n. > I think the Backend interface is good now. However the implementations need to make sure that any data that is network specific is stored in some map (mapping from network name to network specific data). Stuff like lease, routes, UDP sockets, TUN devices, etc. Otherwise AddNetwork calls will stump over each other. There also needs to a mutex there to protect it as I understand the calls to AddNetwork are not serialized.\nI was attempting modify the existing backends as little as possible here, and have singleton backends be opt-in.  So our in-progress OVS plugin uses Once in the New() call to create the singleton backend, and then protects AddNetwork() with a mutex.  So the backend itself handles storing the individual network info internally.\nExisting backends still create a new backend instance for each Network object, so there would be no clashes for AddNetwork() in the existing backends with the branch as it currently stands.\n. Would SetNetwork() or RegisterNetwork() or even NewNetwork() sound better to you?\n. Updated.\n. @xiang90 Yeah, that looks helpful, and it turns out that flannel's etcd/client is not up-to-date and does not contain https://github.com/coreos/etcd/commit/a1ef699aebbe7ebb4da2f90a42e0f612eec08384 so I guess we should update it.  Thanks!\n. vendored in pkg/pathutil; repushed\n. For the second patch, the issue is that when we added listening for network deletion, that is signified by an empty snapshot, but that was also returned by the etcd watch code when it wanted to ignore a key.  So we need a way for the network watch bits to do that without terminating the watch.\nAn alternative approach (if you don't like goto) would be to have an internal error that parseNetworkWatchResult could return that watch.go::WatchNetworks() would look for.  Happy to do that too.\n. Rajat touched on this in the initial comment, but our driver here is   In our case we want all the nodes to be aware of the network and Register it with the backend, but we don't actually want to obtain the lease or add a reservation until we know we're going to actually use it.  And that only happens through the CNI plugin, not through flanneld (becuase flanneld doesn't care about the actual containers, just the inter-node plumbing).\nSo the CNI plugin (or orchestrator like kubernetes/openshift) would be the thing calling flannelctl to allocate the network subnet lease when the first container in the network is started on the node.  Or the CNI plugin could hit up etcd I suppose, except that we'd rather use the flannel 'remote' functionality and have the server flanneld arbitrate access so that the nodes don't require write access to etcd.\nSo with your reservations code, would our CNI plugin then be expected to call AddReservation or AcquireLease itself against the server flanneld?  That wouldn't require a flannelctl at all, but that seems roundabout because we'd need to have the CNI plugin know the flanneld server IP and certificates and stuff.  To me it makes more sense to have this in flanneld, but since flanneld doesn't care about containers that's kinda hard...\n. I could move it up to the be.Run() gofunc, if that's what you mean?  eg:\ngo func() {\n    n.be.Run(n.ctx)\n    n.be.UnregisterNetwork(n.ctx, n.Name)\n    wg.Done()\n}()\n. I didn't have it cleaned up in the backends themselves to make it easier for singleton backends, where Run() might be called only once.  For example in our OVS plugin we have:\nfunc (ovsb *OVSBackend) Run(ctx context.Context) {\n    onceRun.Do(func() { ovsb.start(ctx) })\n}\n. Fixes look mostly OK to me and I've tested it with the OVS singleton plugin.\nThe only thing causing concern is the move of UnregisterNetwork() to a defer.  We're currently using this for our Run():\nfunc (ovsb *OVSBackend) Run(ctx context.Context) {\n    onceRun.Do(func() { ovsb.start(ctx) })\n}\nwhich means that no networks actually return from Run() until they all do.\nI do like the defer handling for UnregisterNetwork() though, so I'm happy to adapt our Run() to just start our plugin's loop on the first network registration, and then select{} on every network's context.\nBut there's one missing piece; how does a singleton plugin know to exit?  Networks come and go, and even when the last network is removed, that doesn't mean we should cancel a singleton plugin's event loop.  So now the singleton plugin needs some kind of cancel too...  I'm not sure what to do there, except to pass the Manager's ctx in newBackend() so a singleton can grab it?\n. @eyakubovich Sure, lets see what that looks like.  Maybe it's just easier in the long run.\n. @eyakubovich Looks great from 10,000ft; I'll rebase our stuff on top of it today and see if I can shake anything out, especially with the parseNetworkConfig() changes.\n. One thing I noticed; hostgw has a 'networks' map, but a network never gets removed from the map.  We need to do the same thing in the ovs backend since UnregisterNetwork() is now gone.\nI can work around this by passing a channel to each backend network subclass that the backend itself selects on, but there are two problems.  First, when flannel exits and cancels the context, the backend gets canceled before it can read all the events from the \"network is canceled\" channel and clean things up.  Second, we need to use nonblocking channel operations with select/default.\nSeems that UnregisterNetowrk() would be simpler for all backends, but if you don't want it for some reason it's not hard to implement in other ways.\n. @eyakubovich I'm fine with the be.removeNetwork() strategy; after prototyping that I don't see any problems on my end.\n. Overall looks like a great cleanup; I'm on PTO until early next week but I'll test it out when I get back.\n. Other than the last findSubnet() thing, LGTM\n. Missed this before because I was only testing with one network.\n. If we want testcase coverage for the localmanager subnet watch fix, we're probably going to have to bring back the mock etcd in some form.\n. Overall LGTM\n. > I'm confused why it's failing hard. There's a big for loop inside Network.Run: https://github.com/coreos/flannel/blob/master/network/network.go#L98\nIt was failing long before the Network.Run(), because with multinetwork mode, to even get a Network object, you have to ask etcd/flannel-remote for the list of networks.\n. @eyakubovich @jonboulle PTAL\n. @eyakubovich does that look like what you meant now?\n. Thanks!\n. @eyakubovich Right you are, that's much better. Updated, PTAL\n. Fair enough, I'll take a bit of time to think about how to rework. Thanks for the review.\n. I was trying to entirely avoid the need for things like on-disk storage during tests and figuring out what IP address and port to use for etcd itself on test machines.  Hence all the mockage :)  But if we did import etcd we could get rid of the testcase for testing the mock etcd at least.\nEither way, just let me know what to do.  Obviously the mock etcd is much less of a real-world test than running etcd itself is, but it has a much smaller bug surface and much more control over how we test things internally and what we can tweak to exercise behavior of flannel itself.\n. @eyakubovich I'll split it out and fix the comment I posted about.\n. @eyakubovich PTAL\n. @eyakubovich in reviews on the corresponding OpenShift/Kube parts of this effort, a good point was brought up.  Which is that the orchestration manager parts of this are fairly tied to the flannel/backend parts, so they should probably all live in the same place.  eg, the flannel side (or appc/cni?) would provide the scripts that the orchestration side would need to call to connect the endpoints/containers to the infrastructure.  Part of the reason for that is that the OVS flow rules must be coordinated between the two parts, and the interfaces are obviously specifically named.\nThe lbr0/vovsbr/vlinuxbr parts are mainly for IPAM and secondarily for OpenShift image builder support.  We'd like to transition that to CNI IPAM in the near future instead of relying on docker, which should simplify this.  I'll look at moving our stuff to a CNI plugin and see if that makes things somewhat simpler.\n. @eveliovila I'm reworking it substantially so it's going to be a bit before it can go through another review.\n. @philips still going in this general direction, but some of this is on hold while we sort out upstream Kubernetes plugin and multi-network issues, since this would eventually be used in conjunction with OpenShift and Kubernetes.  I have not forgotten about it...\n. Way out of date, just going to close this one for now.. @jeremyeder it looks like this PR only requires IFLA_VXLAN_UDP_CSUM and that should be in 3.16+ kernels.  Previous kernels will likely return an error since I think unrecognized netlink attributes fail validation.  So the flannel code should probably be updated to retry without UDPCsum on failure.\n. Seems fine to me too\n. > I don't think I fully understand the need for sleep. Is the desire to exercise the code where the server does an etcd watch prior to the network key being created?\nThe server needs to be watching before the new network is created on the client side, otherwise the server-side will completely miss the event.  So subnet.WatchNetworks() must be running and sitting in RemoteManager::httpDo() (which triggers the server's handleWatchNetworks() function -> etcd.go::WatchNetworks()) before we execute the CreateNetwork() call.  But since subnet.WatchNetworks() is done in a goroutine and the actual HTTP request that triggers the server's handleWatchNetworks is way down the callstack, there's no way to guarantee that this will all happen because they are concurrent.\nObviously this is only a problem in the testcases, because normal clients will never be creating networks.\nSo possibly my approach of directly modifying the server's registry to add the network and generate the event isn't the right way to go about this, because of this problem...\n. > This is the scary statement.\nFair enough; I'll figure out why the cursor isn't doing what it should be doing here.\n. > Can you factor the common code out into something like:\nDone.\n. > add omitempty so it's clean on the wire.\nDone\n. > Let's just have two events: Added and Removed. Or maybe call them EventAdded and EventRemoved.\nDone.\n. > Why a separate goroutine? The current goroutine is just going to block on line 214 anyway.\nThis got removed in some later cleanups.\n. Yeah, just found that out when refactoring the network/ code to watch networks.  It's bogus; removed.\n. Done in the latest push.  I opted for \"WatchNetworks\" instead of GetNetworks, but could go either way if you really prefer GetNetworks.\n. Done in latest push.\n. Done in latest push.\n. > The watch is recursive on something like /coreos.com/network/. Wouldn't then this fire on every key change, like subnets getting added? In that case ParseConfig will fail and that error propagated.\nYes, it should be recursive, since we consider the 'config' key authoritative; if it's missing the network is dead to us.  I've fixed that up now to ignore sub-keys and not return an error.\n. > gofmt. Also, make these Infos instead of Warnings?\nThat was actually a left-over from debugging that I've since removed.\n. Oh nice.  Done.\n. \"Watch of networks failed...\"\n. There's at least 5 places here with the \"for sub in n.subnets; if Equal(sn)\" thing happens; maybe just have a getter that does the search and returns a *Lease?  Then:\nif sub := findSubnet(n, sn); sub != nil {\n    return time.Time{}, ...\n}\n. 'prefix' doesn't seem to be used anywhere\n. Don't need this anymore since we're not emulating etcd.  It's only used by normalizeNetKey() at the bottom of the file, which can now be deleted since it's not called anywhere either.\n. This function is now unused.\n. Could use findSubnet() here.\n. To reduce logspam, otherwise it would get printed every second.  I'd imagine this would often be a short-term log message though, until the etcd master or flannel master gets started up.\n. Ok.\n. I added the GetNetworkConfig() for consistency with the multinetwork case.  Previously, this would have simply passed because NewNetwork() doesn't require any kind of etcd/server connectivity.  So you would then get to Network.Run() and hit that retry loop, I suppose.  This way, both codepaths ask for the etcd/server data at the same time.\nI'm happy to remove this new bit if you like, and leave it up to Network.Run()?  Just thought some consistency of behavior might be useful.\n. Need to swap 'network' and 'subnets' here.\n. Maybe use \"MakeSubnetKey(sn)\" here (and everywhere else) instead of StringSep?\n. Also this comment is out-of-date, I'll respin.\n. ",
    "shayts7": "Thanks for the quick response. \nIndeed it was ~24 hours after I provisioned the machines, the renew probably did not work as expected.\nDo you have more info about the TTL mechanism (why do we need it, etc..?), Is there a way where I can check that the renewal worked?\n. Thanks for the info.\n. Did not reproduce the use case, so I cannot tell...\nIf I'll see it again, I'll let you know...\n. @steveeJ  - I did not check that since than...\n. Thanks for the detailed explanation!\n. @tomdee - how can we update from 0.5.5 to 0.6.0?\n. We tried to make the update in: /usr/lib64/systemd/system/flanneld.service, but this file is readonly in our env...\n. @monder - Thanks!\nI managed to update flannel version in the service file to v0.6.1, but now I get this error:\nSep 14 08:54:09 ip-10-10-4-76.ec2.internal rkt[2704]: image: using image from file /usr/lib/rkt/stage1-images/stage1-fly.aci\nSep 14 08:54:10 ip-10-10-4-76.ec2.internal rkt[2704]: image: using image from local store for image name quay.io/coreos/flannel:v0.6.1\nSep 14 08:54:12 ip-10-10-4-76.ec2.internal rkt[2704]: I0914 08:54:12.581360 02704 manager.go:99] Register: alloc\nSep 14 08:54:12 ip-10-10-4-76.ec2.internal rkt[2704]: log: exiting because of error: log: cannot create log: open /var/tmp/flanneld.ip-10-10-4-76.root.log.INFO.20160914-085412.2704: no such file or directory\np.s. - I was able to create a file under /var/tmp and to edit it.\np.s.2 - I think the error is coming from here: https://github.com/coreos/flannel/blob/v0.6.1/vendor/github.com/golang/glog/glog_file.go#L123\n. ",
    "ddysher": "I've see this issue in ubuntu 14.04, kubernetes 1.3.3 and flannel 0.5.5\n0806 07:48:02.038625 09149 main.go:275] Installing signal handlers\nblabla\nI0807 08:37:32.625364 09950 vxlan.go:248] Subnet removed: 192.168.68.0/24\nI was running it with vagrant on local machine; and it stopped working after computer sleep. Based on the log time, it seems TTL is the cause ?\n. What's the relationship between Network and cluster-cidr flag in controller manager?  Do they have to match each other?\n. ",
    "sekannia": "Is there a way to increase the TTL or is it 24 hrs\n. ",
    "npmccallum": "It would be helpful if PSK was not the only method. IKE supports multiple methods and we are actively working on Kerberos support. This would have multiple benefits for the current scenario. For instance, a compromise of a single host does not entail a compromise of all hosts. Additionally, transient connections can be supported via the KDC, including opportunistic encryption.\n. @pquerna: That seems somewhat orthogonal to me.\n. @eyakubovich I understand. There are two modes here: delegated and parallel. If IPSec delegates to etcd, then if etcd is deployed without encryption IPSec gives the illusion of security while in reality there is none. There is also the general problem of delegating authentication from OSI level 3 to OSI level 7. This should be inverted. If you are going to delegate authentication, you should delegate the other direction (etcd should delegate to IPSecc).\nA better alternative would be to do parallel authentication. That is, use the same X.509 cert for both etcd and IPSec. There is no cryptographic harm in doing this. It also means that the same certificate rotation can be used for both services. This also has other technical benefits (including the perpetual benefit of simplicity). This also permits other forms of host-based identity to be integrated at a later point (such as the aforementioned Kerberos).\nUsing PSK looks like a nice simple solution at first, but going down that road will be fraught with difficulty in the future.\nAlso, without a proper IKE, how do you plan to rotate the key-material in long-lived connections?\n. I'm game! @simo5 should probably come as well.\n. I can do 1pm EST.\n. ",
    "gtank": "@eparis My thought on the choice of cipher was that the underlying CTR implementation is more likely to be solid than the GCM, but I don't have specific knowledge here- do your people? GCM is certainly a better choice than CBC, and I agree that the somewhat marginal security gains of 256-bit aren't worth that performance hit.\n. ",
    "pwnall": "@MohdAhmad If you need any encouragement, I could have really used this today! Best of luck finishing it up!\n. ",
    "simo5": "In meetings all morning till 12:30, maybe 1pm EST (17:00 UTC) ?\n. @gtank AES GCM means authenticated encryption, and its faster than CTR + MAC, it is recommended in any case where you exchange ephemeral session keys and have and can use sequence numbers.\n. ",
    "israelshirk": "Hey there, any updates?  Thanks!\n. ",
    "feliksik": "Did not look at this in detail, but note that strongswan has an IKEv2 api, take a look at Vici + charon if you care! \n. ",
    "abruehl": "+1 I have some FDA/HIPAA requirements mandating 100% over the wire encryption.  Encryption between hosts would mean I don't need to  worry about implementing TLS in every service.  I would only have to worry about code-level (TLS) encryption when it leaves the flannel network. \n. ",
    "Calpicow": "+1--really looking forward to a global, turn-key encryption solution that satisfies Kubernetes network requirements\n. ",
    "chbatey": "Currently evaluating using flannel and would be very interested in these docs. We'll be testing these kinds of things out but it would be nice to have an expected behaviour to compare against.\n. Thanks @eyakubovich \n. ",
    "wulonghui": "@eyakubovich Thanks very much\n. @MohdAhmad \nThanks \n. ",
    "kdomanski": "@eyakubovich @MohdAhmad updated\n. ",
    "xiang90": "@dcbw This might be helpful https://github.com/coreos/etcd/tree/master/client#error-handling\n. Feel good about the overall approach.\n. Do we have an e2e test or integration test yet? \nThe code LGTM.\n. LGTM\n. @robtaireeo Why this is closed? Have you figured it out yourself? Sorry for the delay. @eyakubovich is on a vacation.\n. OK. Thank you.\n. Given a /24 subnet I see:\n\"Picking subnet in range 10.244.8.128 ... 10.244.8.128\"\n\"Failed to acquire subnet: out of subnets\"\nThe SubnetMax calculation looks wrong, fixed it.\nCan you provide more information about how to reproduce this issue? \nI do not feel the fix is right.\n/cc @eyakubovich \n. @amitkgupta \nIf I understand this correctly (I am new to flannel codebase). The subnet is the subnet to be allocated inside the network. If you have a network that is larger than 24, say 16, it will allocate subnet with 24. \n. > Given 1.2.0.0/16, the subnet should be 1.2.1.0/24, which is 1.2.1.0 to 1.2.2.0.\nThe network is 1.2.0.0/16. Then the minSub is 1.2.1.0/24 and max is 1.2.255.0/24. Each subnet is a /24. So you can allocate up to 255 subnets.\n. @eyakubovich @dcbw \nCan we simply embed etcd for testing? I do not think we need to mock etcd. \nAnother simpler solution I can see is to mock the registry instead of etcd.\n. > What do you mean by \"embed etcd for testing\"?\nYou do not have to mock it. You can simply import etcd and run it within your application for testing. \nYou can do https://github.com/coreos/etcd/blob/master/integration/cluster_test.go#L93, then you get a running etcd cluster.\n. I would still argue we can mock a registry for exercising flannel without the assumption that etcd is the only backend. Mocking etcd is hard since you have to make assumptions about etcd's behavior.\nI just do not feel good about introducing etcd logic into flannel codebase.\n. LGTM\n. The test fails due to a deadlock. Let's fix it.\n. LGTM\n. @tomdee Can split the vendoring and code part? I would like to give this a quick review :). @eyakubovich You need to put an empty line between different kinds of imports. Or gofmt will reorganize them in alphabetical order.\n. you might want to use http://golang.org/pkg/encoding/binary/#ByteOrder\n. you do not have to alloc the slice every time you call this function. if there is a std lib, i prefer to use it. but not a big problem for this case. \n. The handler does not need to close the request body. The http server will do that for you.\nRefer: https://golang.org/pkg/net/http/#Request\n. The error string does not need to be json format?\n. You do not have to write http.StatusOK. But it looks OK.\n. we should return right after 305, right?\n. also add a comment for this handler like the above for consistency?\n. Should this be isErrNodeNotExist instead of Exist?\n. The public error usually have a prefix. \ngo\nErrLeaseTake = errors.New(\"subnet: lease already take\")\n. I feel this function deserves a comment. RemoveReservation remove the subnet by setting TTL back to subnetTTL (24hours)\n. I would like to have comments for all public interface. This might be a good start.\n. I failed to see why do we need this change in this commit.\n. I feel this fatal msg is not clear enough. How about unexpected adding reservation outside of configured network\n. nit: I would suggest to move the last two subtests to line 426. So that we have reservation related tests all together. Then we try to acquireLease on a reservation.\n. shouldnt we get a 24 hour ttl?\n. Hmm... OK. Can you explain why it is 24hours not 1hour if this is just an implementation detail?\n. The node should have a TTL?\n. Got you. Thanks!\n. Also I actually feel the removed one with ttl and the reservation without ttl is a little bit counter-intuitive. But I have no better suggestion. \n. This does not seem right. The expected subMax should be a 10.3.255.0. So subMin-subMax(10.3.1.0->10.3.255.0) is about a whole subnet (/16).\n. I think the previous one is correct. And the change is incorrect. We\nshould get the next network and minus the subnetSize to get the largest one in the current network.\n. can we add a comment for why 10.3.31.0 is here?\nFor example: a hand created lease outside the range of subnetMin-SubnetMax for testing removal.\n. I feel just unlock where you want is more simpler.\n. ",
    "pbx0": "LGTM\n. LGTM\n. ",
    "tdeheurles": "I have just tried with vxlan, here are the journalctl -fu flanneld for each machine.\nThe communication seems to pass as the log says after a ping from a machine to the other :\nbaremetal (IPv4:  10.1.18.0 and 192.168.1.13):\n\u279c  ~  journalctl -fu flanneld\n-- Logs begin at Thu 2015-09-03 09:55:16 UTC. --\nSep 12 15:09:42 bm0-coreos sdnotify-proxy[827]: I0912 15:09:42.708636 00001 vxlan.go:232] Subnet added: 10.1.46.0/24\nSep 12 15:09:42 bm0-coreos sdnotify-proxy[827]: I0912 15:09:42.708728 00001 device.go:164] calling NeighAdd: 10.0.2.15, b6:a2:bc:32:d7:74\nSep 12 15:11:02 bm0-coreos sdnotify-proxy[827]: I0912 15:11:02.982397 00001 vxlan.go:345] L3 miss: 10.1.46.2\nSep 12 15:11:02 bm0-coreos sdnotify-proxy[827]: I0912 15:11:02.982456 00001 device.go:187] calling NeighSet: 10.1.46.2, b6:a2:bc:32:d7:74\nSep 12 15:11:02 bm0-coreos sdnotify-proxy[827]: I0912 15:11:02.982601 00001 vxlan.go:356] AddL3 succeeded\nSep 12 15:11:23 bm0-coreos sdnotify-proxy[827]: I0912 15:11:23.134641 00001 vxlan.go:340] Ignoring not a miss: b6:a2:bc:32:d7:74, 10.1.46.2\nVM (IPv4: 10.1.46.0 and  10.0.2.15 and 192.168.1.4):\ncore@tdeheurles-vm-coreos ~ $ journalctl -fu flanneld\n-- Logs begin at Sat 2015-09-12 15:08:55 UTC. --\nSep 12 15:09:42 tdeheurles-vm-coreos sdnotify-proxy[1242]: I0912 15:09:42.720068 00001 device.go:164] calling NeighAdd: 192.168.1.13, 22:de:e0:39:62:8d\nSep 12 15:09:42 tdeheurles-vm-coreos systemd[1]: Started Network fabric for containers.\nSep 12 15:11:02 tdeheurles-vm-coreos sdnotify-proxy[1242]: I0912 15:11:02.958771 00001 vxlan.go:345] L3 miss: 10.1.18.3\nSep 12 15:11:02 tdeheurles-vm-coreos sdnotify-proxy[1242]: I0912 15:11:02.958835 00001 device.go:187] calling NeighSet: 10.1.18.3, 22:de:e0:39:62:8d\nSep 12 15:11:02 tdeheurles-vm-coreos sdnotify-proxy[1242]: I0912 15:11:02.958995 00001 vxlan.go:356] AddL3 succeeded\nSep 12 15:11:21 tdeheurles-vm-coreos sdnotify-proxy[1242]: I0912 15:11:21.871119 00001 vxlan.go:340] Ignoring not a miss: 22:de:e0:39:62:8d, 10.1.18.3\nSep 12 15:15:26 tdeheurles-vm-coreos sdnotify-proxy[1242]: I0912 15:15:26.547585 00001 vxlan.go:345] L3 miss: 10.1.18.1\n. I found the problem, but not how to solve for now.\nHere is the journalctl -fu flanneld from the VM :\njournalctl -fu flanneld\n-- Logs begin at Sat 2015-09-12 15:58:16 UTC. --\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.349934 00001 main.go:188] Using 10.0.2.15 as external interface\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.350178 00001 main.go:189] Using 10.0.2.15 as external endpoint\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.355440 00001 etcd.go:204] Picking subnet in range 10.1.1.0 ... 10.1.255.0\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.357291 00001 etcd.go:84] Subnet lease acquired: 10.1.23.0/24\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.370172 00001 ipmasq.go:48] Adding iptables rule: FLANNEL -d 10.1.0.0/16 -j ACCEPT\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.373826 00001 ipmasq.go:48] Adding iptables rule: FLANNEL ! -d 224.0.0.0/4 -j MASQUERADE\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.385713 00001 ipmasq.go:48] Adding iptables rule: POSTROUTING -s 10.1.0.0/16 -j FLANNEL\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.395429 00001 udp.go:222] Watching for new subnet leases\nSep 12 15:59:22 tdeheurles-vm-coreos sdnotify-proxy[1179]: I0912 15:59:22.400209 00001 udp.go:247] Subnet added: 10.1.5.0/24\nSep 12 15:59:22 tdeheurles-vm-coreos systemd[1]: Started Network fabric for containers.\nAt the top of the log :\nUsing 10.0.2.15 as external interface\nSo my configuration is not taken in account :\n```\n/etc/systemd/system/flanneld.service.d/10-publicip.conf\n[Service]\nEnvironment=FLANNELD_INTERFACE=192.168.1.7\n```\nI know that on vagrant, we have to add that parameter :\nflannel:\n    interface: $public_ipv4\nBut the $public_ipv4 is not changed with a good parameter for my configuration. It's translated to 10.0.2.15 and not 192.168.1.7.\nSo I do not use cloud_config :\n- I run vagrant up\n- I run a script that :\n  - add etcd2 unit and start etcd2 (it's working)\n  - add flanneld drop-ins and start flanneld :\n```\n/etc/systemd/system/flanneld.service.d/10-publicip.conf\n[Service]\nEnvironment=FLANNELD_INTERFACE=192.168.1.7\n```\nSo it seems that the problem come from that drop-ins.\n. Finally I write FLANNELD_IFACE=192.168.1.7 to /run/flannel/options.env.\nIt's working.\n. For now my solution is a workaround.\nWhat is the good name of that environment ?\nAll these ones doesn't work :\nEnvironment=FLANNELD_IFACE=192.168.1.7\nEnvironment=FLANNELD_INTERFACE=192.168.1.7\nEnvironment=FLANNEL_INTERFACE=192.168.1.7\nEnvironment=FLANNEL_IFACE=192.168.1.7\n. Thanks for your answer.\nThe fact that drop-ins does not write that Environments is good to know.\nBut the problem is that the ip is allocated at runtime, so I can't write it like this.\nOn vagrant, the $public_ipv4 seems to not be working. I found a solution by putting a first unit to extract the ipv4, write it to a file and then the next unit can acces it.\nIf someone need, here is a snippet, or you can look to the project :\n``` yaml\n    # Here is a unit, that will extract the Public_IP\n    - name: write_public_ip.service\n      command: start\n      content: |\n        [Unit]\n        Description=Systemd test\n        [Service]\n        Type=oneshot\n        ExecStartPre=/usr/bin/mkdir -p PUBLIC_IP_FILE_PATH\n        ExecStart=PROGRAMS_PATH/write_ip.sh PUBLIC_IP_FILE_PATH/public_ip\n        #ExecStartPost=PROGRAMS_PATH/sed_kubernetes_public_ip.sh\n        [Install]\n        WantedBy=multi-user.target\nwrite_files:\n  # This script will get the public ipv4 that correspond\n  #   to the network mask given in the config\n  - path:        PROGRAMS_PATH/write_ip.sh\n    permissions: \"0755\"\n    owner:       \"root\"\n    content: |\n      #!/bin/bash -\n      FILE_TO_WRITE=$1\n      echo \"PUBLIC_IP=ifconfig | grep __NETWORK_MASK__ | awk '{print $2}'\" > $FILE_TO_WRITE\n```\n. ",
    "suquant": "My proposal force set \"eth1\" in interface parameter of flannel\n```\ncloud-config\ncoreos:\n  flannel:\n    public-ip: $public_ipv4\n    interface: eth1\n```\n. +1 problem NOT fixed\napp01.gluster.skydns.local \nst01.gluster.skydns.local \nAll operations from st01\n```\ndocker exec gluster gluster peer probe app01.gluster.skydns.local \npeer probe: failed: Probe returned with Transport endpoint is not connected\n```\nlog from st01.gluster.skydns.local :\n[2016-03-06 02:03:10.019532] I [MSGID: 106487] [glusterd-handler.c:1239:__glusterd_handle_cli_probe] 0-glusterd: Received CLI probe req app01.gluster.skydns.local 24007\n[2016-03-06 02:03:10.027733] I [MSGID: 106129] [glusterd-handler.c:3661:glusterd_probe_begin] 0-glusterd: Unable to find peerinfo for host: app01.gluster.skydns.local (24007)\n[2016-03-06 02:03:10.044133] I [rpc-clnt.c:984:rpc_clnt_connection_init] 0-management: setting frame-timeout to 600\n[2016-03-06 02:03:10.045234] W [socket.c:869:__socket_keepalive] 0-socket: failed to set TCP_USER_TIMEOUT -1000 on socket 13, Invalid argument\n[2016-03-06 02:03:10.045245] E [socket.c:2965:socket_connect] 0-management: Failed to set keep-alive: Invalid argument\n[2016-03-06 02:03:10.045336] I [MSGID: 106498] [glusterd-handler.c:3589:glusterd_friend_add] 0-management: connect returned 0\n[2016-03-06 02:03:10.053491] E [MSGID: 106165] [glusterd-handshake.c:1828:__glusterd_mgmt_hndsk_version_cbk] 0-management: failed to get the 'versions' from peer (10.1.100.20:24007) [Invalid argument]\n[2016-03-06 02:03:10.053553] I [MSGID: 106004] [glusterd-handler.c:5127:__glusterd_peer_rpc_notify] 0-management: Peer <app01.gluster.skydns.local> (<00000000-0000-0000-0000-000000000000>), in state <Establishing Connection>, has disconnected from glusterd.\n[2016-03-06 02:03:10.053619] W [glusterd-locks.c:681:glusterd_mgmt_v3_unlock] (-->/usr/lib/x86_64-linux-gnu/glusterfs/3.7.8/xlator/mgmt/glusterd.so(glusterd_big_locked_notify+0x4c) [0x7ff43d2656ac] -->/usr/lib/x86_64-linux-gnu/glusterfs/3.7.8/xlator/mgmt/glusterd.so(__glusterd_peer_rpc_notify+0x162) [0x7ff43d26f7b2] -->/usr/lib/x86_64-linux-gnu/glusterfs/3.7.8/xlator/mgmt/glusterd.so(glusterd_mgmt_v3_unlock+0x58a) [0x7ff43d30d61a] ) 0-management: Lock for vol media not held\n[2016-03-06 02:03:10.053628] W [MSGID: 106118] [glusterd-handler.c:5149:__glusterd_peer_rpc_notify] 0-management: Lock not released for media\n[2016-03-06 02:03:10.054075] I [MSGID: 106549] [glusterd-volgen.c:1141:get_vol_nfs_transport_type] 0-glusterd: The default transport type for tcp,rdma volume is tcp if option is not defined by the user \n[2016-03-06 02:03:10.057599] I [MSGID: 106132] [glusterd-proc-mgmt.c:83:glusterd_proc_stop] 0-management: glustershd already stopped\n[2016-03-06 02:03:10.057642] I [MSGID: 106132] [glusterd-proc-mgmt.c:83:glusterd_proc_stop] 0-management: quotad already stopped\n[2016-03-06 02:03:10.057673] I [MSGID: 106132] [glusterd-proc-mgmt.c:83:glusterd_proc_stop] 0-management: bitd already stopped\n[2016-03-06 02:03:10.057702] I [MSGID: 106132] [glusterd-proc-mgmt.c:83:glusterd_proc_stop] 0-management: scrub already stopped\n[2016-03-06 02:03:10.057719] I [MSGID: 101053] [mem-pool.c:616:mem_pool_destroy] 0-management: size=588 max=2 total=2\n[2016-03-06 02:03:10.057726] I [MSGID: 101053] [mem-pool.c:616:mem_pool_destroy] 0-management: size=124 max=2 total=2\nlog from app01.gluster.skydns.local\n```\n[2016-03-06 02:05:59.166806] E [MSGID: 106170] [glusterd-handshake.c:1043:gd_validate_mgmt_hndsk_req] 0-management: Rejecting management handshake request from unknown peer 10.1.16.3:65532\n```\nHave any idea ? \n. ",
    "gouyang": "I understand it now. Close this bug.\n. @eyakubovich I'd like to know what if by changing Manager to EtcdManager? isn't it more golang idiomatic?\n. @eyakubovich I thought newEtcdManager() returns EtcdManager is a bit more readable than Manager, please never mind if it does not looks so.\n. @MohdAhmad I just tried it with three nodes etcd cluster, see the same thing with this one.\n. @MohdAhmad The flanneld and etcd were built from the master branch.\nSteps:\n1. Create the etcd cluster manually, or use https://github.com/gouyang/ansible-etcd to create it by ansible.\n2. Set /coreos.com/network/config\n3. Stop one node in the etcd cluster\n4. Run flanneld  -etcd-endpoints=\"xxx\".\n```\n$\u00a0./flannel/bin/flanneld -version\n0.5.3+git\n$ ./etcd/bin/etcd -version\netcd Version: 2.2.0+git\nGit SHA: 89acdd6\nGo Version: go1.4.2\nGo OS/Arch: linux/amd64\n```\n. Sorry, I just aware of that I've stopped two nodes during my testing.\nNow I correct it, just stop one node, the problem does not happen any more, so this is not a bug?\n. ",
    "apatil": "@eyakubovich It looks like you may be right. I had only 25 containers running, but 1k+ stopped containers; so if Docker is not releasing their IP addresses, that would exhaust the subnet. I'm deleting the stopped containers to see if that helps.\n. @eyakubovich I spoke too soon, unfortunately that didn't fix the problem.\n```\ndocker ps -a | wc -l\n25\nip link | grep veth | wc -l\n11\ndocker run -ti --rm ubuntu:14.04 /bin/bash\nError response from daemon: Cannot start container 54a522db212f2adc7e1c3c38699a84b1dd7dd3ddfe589fdbc84f19c3906b0c0e: no available ip addresses on network\n```\n. @eyakubovich That works for me too, but I'd like to avoid restarting docker if possible. Anyway, it looks like this isn't a flannel issue, so I'll close it. Thanks for looking into it.\n. ",
    "thockin": "In what flannel release can we depend on this fix?\nOn Mon, Sep 28, 2015 at 10:23 AM, Eugene Yakubovich \nnotifications@github.com wrote:\n\nClosed #318 https://github.com/coreos/flannel/issues/318 via #327\nhttps://github.com/coreos/flannel/pull/327.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/coreos/flannel/issues/318#event-420996333.\n. \n",
    "streamnsight": "FYI: there may be unintended consequences to this PR\nhttps://github.com/coreos/flannel/issues/433#issuecomment-222380448\n. CoreOS stable (899.15.0)\nDocker version 1.9.1, build 9894698\n-> this version shows the docker0 bridge IP as remote IP\nCoreOS stable (835.11.0)\nDocker version 1.8.3, build cedd534-dirty\n-> this version is older but show correct remote IP\nI use Kubernetes v1.2.2 on both\nThanks\n. @tomdee does that help?\n. ",
    "viatcheslavmogilevsky": "Thanks for advice\n. ",
    "astropuffin": "I'd also like to add that intra-pod communication works fine via localhost. It's only when you want to talk to a container in a different pod (or between non-kubernetes containers) that things don't work.\n. I solved my own problem:\nbash\niptables -I FORWARD -i docker0 -j ACCEPT\n. ",
    "bnevis-i": "Docker 1.13 also broke inter-container communications across hosts (https://github.com/docker/docker/issues/30338). Docker 1.13 also broke inter-container communications across hosts (https://github.com/docker/docker/issues/30338). ",
    "ingvagabund": "Thanks Eugene. I am not familiar with creating the socket file. Can you enlighten me how to do that? Or maybe what would be the correct place to put daemon.SdNotify(\"READY=1\")? From my point of view I see it as a one liner. What about https://github.com/coreos/flannel/blob/master/remote/server.go#L278 ?\n. Thanks for the links. Here is the PR https://github.com/coreos/flannel/pull/326.\n. One minor glitch. When -listen is not specified, flanneld creates /run/flannel directory as a consequence of creating /run/flannel/subnet.env file. If -listen is not specified, it does not create /run/flannel. Running mk-docker-opts.sh with -d /run/flannel/docker fails as the directory does not exist. The default file for it is /run/docker_opts.env (if -d not specified). This is service file issue related, not the flannel itself.\n. @eyakubovich, I suppose client-server feature is still not stable, right?\n. The update will lie in updates until it gets 3 karmas. @apeeyush, can you test the build if it is working properly? Thanks\n. Would be great to have at least one karma. To make sure flannel-0.5.4 is installable and working elsewhere than on my machine. E.g. it could collide with other packages or introduce missing dependencies.\n. flannel-0.7.0-2.fc25 is in stable and available since January 2017.. I still do. Already updating flannel in Fedora rawhide. It just takes time to update all its deps and generate new spec file.. flannel-0.7.0 in f25 updates-testing: https://bodhi.fedoraproject.org/updates/FEDORA-2017-013e53830f. ",
    "fgrzadkowski": "Yes! Upgrading to 0.5.3 helped! Now I can ping pods running on different hosts. After downgrading back to 0.5.0 I again see the problem.\nThanks for help!\n. ",
    "kayrus": "There is an issue reported by @jbkc85 at irc channel. On old stable release (835.13.0) his web server has logged real address:\nsh\n192.168.122.1-host ~$ curl -so/dev/null 192.168.122.146\n......\ncore@coreos1 ~ $ /opt/bin/docker run -ti --publish=192.168.122.146:80:80 nginx\n192.168.122.1 - - [12/Apr/2016:16:59:24 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.35.0\" \"-\"\nafter the coreos upgrade to 899.15.0 web server started to show this log:\nsh\n192.168.122.1-host ~$ curl -so/dev/null 192.168.122.146\n......\ncore@coreos1 ~ $ /opt/bin/docker run -ti --publish=192.168.122.146:80:80 nginx\n10.1.58.1 - - [12/Apr/2016:17:04:12 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.35.0\" \"-\"\n@jbkc85 your issue with the masquerade occurred by this change. temporarily workaround is to use -ip-masq=false i.e. use drop-in:\n``` ini\n/etc/systemd/system/flanneld.service.d/10-nomasq.conf\n[Service]\nExecStart=\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n  /usr/bin/docker run --net=host --privileged=true --rm \\\n  --volume=/run/flannel:/run/flannel \\\n  --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n  --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n  --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n  --env-file=${FLANNEL_ENV_FILE} \\\n  --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n  --volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro \\\n  quay.io/coreos/flannel:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=false\n```\n. ",
    "jbkc85": "Thanks @kayrus.  I had to restart all machines to get this to work, but it indeed works at this time.\n. Not sure if this will help, but added this issue to my CoreOS Unit Test via Ansible script (https://github.com/jbkc85/coreos-unit-tests):\n...\n- name: Start NGINX Container (tests hub and docker daemon)\n  raw: docker run -d --name nginx-coreos-unit-test --publish=8888:80 nginx\n  ignore_errors: true\n  register: docker_run_check\n- name: CURL NGINX Container on host IP\n  raw: curl -XGET http://{{ host_ip.stdout }}:8888\n  register: docker_curl_check\n- name: Get LOGS from NGINX Container\n  raw: docker logs nginx-coreos-unit-test\n  register: docker_logs_check\n- name: Verify Logs contain host IP (verifies iptables/ip masq is working)\n  debug: var=docker_logs_check\n  when: docker_logs_check.stdout.find('{{ host_ip.stdout }}') != -1\n- name: stop/rm NGINX Container on CoreOS node\n  raw: docker stop nginx-coreos-unit-test ; docker rm -v nginx-coreos-unit-test\n``` OUTPUT\nTASK [Start NGINX Container (tests hub and docker daemon)] ***\nok: [csn01cou]\nTASK [CURL NGINX Container on host IP] *******\nok: [csn01cou]\nTASK [Get LOGS from NGINX Container] *******\nok: [csn01cou]\nTASK [Verify Logs contain host IP (verifies iptables/ip masq is working)] **\nok: [csn01cou] => {\n    \"docker_logs_check\": {\n        \"changed\": false,\n        \"rc\": 0,\n        \"stderr\": \"\",\n        \"stdout\": \"10.50.1.10 - - [12/Apr/2016:20:08:12 +0000] \\\"GET / HTTP/1.1\\\" 200 612 \\\"-\\\" \\\"curl/7.43.0\\\" \\\"-\\\"\\r\\n\",\n        \"stdout_lines\": [\n            \"10.50.1.10 - - [12/Apr/2016:20:08:12 +0000] \\\"GET / HTTP/1.1\\\" 200 612 \\\"-\\\" \\\"curl/7.43.0\\\" \\\"-\\\"\"\n        ]\n    }\n}\n```\n. Please note, when the change to RKT for Flannel occurred [currently at Version CoreOS stable (1122.3.0)] this issue seems to of crept back up, and the previous adjustments did not work due to docker/rkt conflicts.\n$ curl -XGET 10.50.1.13:8088\n```\ndocker run -it --rm --publish=10.50.1.13:8088:80 nginx\n10.51.34.1 - - [27/Feb/2017:19:43:04 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.51.0\" \"-\"\n```\nTherefore I did the suggested action from above, but in the RKT format:\n- name: flanneld.service\n      command: start\n      drop-ins:\n        - name: 10-nomasq.conf\n          content: |\n            [Service]\n            ExecStart=\n            ExecStart=/usr/bin/rkt run --net=host \\\n              --stage1-path=/usr/lib/rkt/stage1-images/stage1-fly.aci \\\n              --insecure-options=image \\\n              --set-env=NOTIFY_SOCKET=/run/systemd/notify \\\n              --inherit-env=true \\\n              --volume runsystemd,kind=host,source=/run/systemd,readOnly=false \\\n              --volume runflannel,kind=host,source=/run/flannel,readOnly=false \\\n              --volume ssl,kind=host,source=${ETCD_SSL_DIR},readOnly=true \\\n              --volume certs,kind=host,source=/usr/share/ca-certificates,readOnly=true \\\n              --volume resolv,kind=host,source=/etc/resolv.conf,readOnly=true \\\n              --mount volume=runsystemd,target=/run/systemd \\\n              --mount volume=runflannel,target=/run/flannel \\\n              --mount volume=ssl,target=${ETCD_SSL_DIR} \\\n              --mount volume=certs,target=/etc/ssl/certs \\\n              --mount volume=resolv,target=/etc/resolv.conf \\\n              ${FLANNEL_IMG}:${FLANNEL_VER} \\\n              --exec /opt/bin/flanneld \\\n              -- --ip-masq=false\nNow it appears to work again...\n$ curl -XGET 10.50.1.13:8088\n```\ndocker run -it --rm --publish=10.50.1.13:8088:80 nginx\n10.50.130.9 - - [27/Feb/2017:19:47:10 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.51.0\" \"-\"\n```. ",
    "PhilibertDugas": "Manually running the following will do the trick.. but still wondering why it's not appearing by default\nroute add -net 172.16.0.0 netmask 255.255.0.0 dev flannel0\n. I tried to change the docker0 netmask from /24 to /16. Afterwards the ip route is added automatically. But by letting docker0 with a /24 netmask and flannel with a /16 netmask, it doesn't add it automatically\n. ",
    "DreadPirateShawn": "Thanks for the clarification! The restart-without-losing-connectivity scenario makes sense.\nOut of curiosity, apart from the default behavior which doesn't clean up, is there a way to tell flannel that you do want to clean up? That is, do the routes created by flannel remain in existence \"forever\", or is there a way to tell flannel to stop and remove the routes, to return the system to its original pre-flannel state?\n. ",
    "fnordahl": "I would suggest that tearing down the configuration on startup or exit could be harmfull. If a daemon can be restarted for updates or other service without affecting the data plane it will make it much easier for operators to service and it will also make the system seem much more robust and stable.\nI have proposed a patch that will look up the routes before attempting to create them. Any existing equal routes will be left untouched and will not genereate errors. Existing routes with different Gw will be replaced.\nWhat do you think @eyakubovich (Cc @philips )\n. Thanks @steveeJ \n. Commented out the automatic address family detection for now. We can add it back when we have a net library with IP.To4()\n. ",
    "glerchundi": "ok, i'll use /coreos.com/{fleet,network,whatever} then, thanks.\n. Forget about this, after cat-ing flanneld.service I found that it is mounting a volume inside the container.\n```\n[...]\n--volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro \\\n[...]\n```\n. ",
    "ssergiienko": "We already started to build new package(v0.5.3) and soon i'll write results. This bug is not always reproducible, i have seen it in older versions too but this time even restart of flanneld, etcd, or even VM is not helps.\n. Solved. With 0.5.3 flannel bug is the same as with other versions, but more meaningful logs helped to see where problem was.\nOn master (on which etcd is running) was failed ntpd and system time was in past (one week). So flanneld recieved new ttl that already expired and, as far as i understand, it tries to reconnect continuously without any wait or \"max retry count\" and thus eats all cpu and memory.\nflanneld[27346]: I1007 11:34:53.837620 27346 renew.go:41] Lease renewed, new expiration: 2015-09-28 10:41:08.292993055 +0000 UTC\nAfter restarting ntpd on master and restarting flanneld all works fine again. I think flannel should somehow handle such cases and print warnings or even errors. Maybe pause pulling for a certain time or stop after max count.\n. ",
    "ravilr": "@eyakubovich is there a release coming soon that this is going to be part of? Thanks.\n. Does flannel continue to work even after etcdctl migrate of v2 data to v3 data. @hongchaodeng @xiang90  does migrate keep around v2 data.  Can someone confirm, if they have gone through this process. Thanks.. https://github.com/coreos/etcd/issues/7406 confirms that etcdctl migrate leaves the existing v2 data as-is.. ",
    "ydhydhjanson": "Thanks to @eyakubovich , has found a problem , I expose 8285 port like this : firewall-cmd --add-port=8285/tcp , be revised as : firewall-cmd --add-port=8285/udp , flannel successful working ! \n. ",
    "petersunquest": "Hi,\nI try to clear my problem, It looks working:\nI think that may be can help someone.\nThis is USER-DATA file:\ncoreos:\n    fleet:\n        interface: $public_ipv4                                         #\n        etcd_endpoints: https://$private_ipv4:4001         #    let CA file to Docker share folder\n        etcd_keyfile: \"/etc/ssl/etcd/client.key\"                  #    it looks Flannel can access that folder\n        etcd-certfile: \"/etc/ssl/etcd/client.crt\"\n        etcd-cafile: \"/etc/ssl/etcd/ca.crt\"\n   units:\n       -- name: flanneld.service\n       drop-ins:\n           -- name: 50-network-config.conf\n               content: |\n                    [Service]\n                    ExecStartPre=/usr/bin/etcdctl --cert-file=/etc/ssl/etcd/client.crt --key-file=/etc/ssl/etcd/client.key --ca-file=/etc/ssl/etcd/ca.crt --endpoint=https://127.0.0.1:2379 set /coreos.com/network/config '{\"Network\": \"10.244.0.0/16\"}'\n      command: start\nPerfect working.\nThanks\nPeter\n. ",
    "neumino": "I'm basically just following \nhttps://coreos.com/kubernetes/docs/latest/deploy-master.html \n```\ncore@whitedraft-0 ~ $ systemctl cat flanneld.service\n/usr/lib64/systemd/system/flanneld.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nRequires=early-docker.service\nAfter=etcd.service etcd2.service early-docker.service\nBefore=early-docker.target\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=\"TMPDIR=/var/tmp/\"\nEnvironment=\"DOCKER_HOST=unix:///var/run/early-docker.sock\"\nEnvironment=\"FLANNEL_VER=0.5.3\"\nEnvironment=\"ETCD_SSL_DIR=/etc/ssl/etcd\"\nEnvironment=\"FLANNEL_ENV_FILE=/run/flannel/options.env\"\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStartPre=-/usr/bin/touch ${FLANNEL_ENV_FILE}\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n  /usr/bin/docker run --net=host --privileged=true --rm \\\n  --volume=/run/flannel:/run/flannel \\\n  --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n  --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n  --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n  --env-file=${FLANNEL_ENV_FILE} \\\n  --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n  --volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro \\\n  quay.io/coreos/flannel:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=true\nUpdate docker options\nExecStartPost=/usr/bin/docker run --net=host --rm -v /run:/run \\\n  quay.io/coreos/flannel:${FLANNEL_VER} \\\n  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env -i\n/etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf\n[Service]\nExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env\n```\nI thought that the issue might be that my etcd setup doesn't include certificates yet, but I looked a bit at flannel's source code and TLS is not require if I'm not mistaken.\nRestarting flannel fails because of a timeout, I guess since it doesn't properly start, it eventually times out.\n. @jonboulle -- yes, that works. It returns\n{\"Network\":\"10.2.0.0/16\"}\nManually passing directly the endpoints through --endpoint also works.\n. Ah yes that works, thanks @jonboulle for your help!\n. ",
    "jeremyeder": "Ahh - I meant to reply to this.  Assuming OP meant yum install -y flanneld gets him 0.5.0 and he wants a later version.  That's true in Fedora 22.  Fedora rawhide has 0.5.3.\nI've not tried it, but you might be able to just install the rawhide version of flannel on F22.\nhttp://mirror.cc.vt.edu/pub/fedora/linux/development/rawhide/x86_64/os/Packages/f/\nI asked in BZ https://bugzilla.redhat.com/show_bug.cgi?id=1273211 if we intend to update flannel in F22/F23.  Please feel free to comment there as well @apeeyush thanks!\n. By then, we might have 0.5.4 in F22:  https://bodhi.fedoraproject.org/updates/FEDORA-2015-52e1ca8443\n. 0.5.4 is in updates-testing in Fedora 23.  0.5.5 is in Fedora Rawhide.  I've pinged in the BZ about moving it to the regular updates repo in F23.  Some feedback on the testing F23 package would likely be appreciated.  https://dl.fedoraproject.org/pub/fedora/linux/updates/testing/23/x86_64/f/\n@philips I also added you to the cc-list on the BZ.\n. Nothing special.  It gets enabled/disabled with \n```\nethtool -k $NIC tx-udp_tnl-segmentation off\n```\nIt's been enabled by default on all the NICs we've experimented with, including the Intel ones you linked to.\n. If you terminate inside the guest, then no, you won't use the offload.  virtio_net doesn't support VXLAN-offload.  AFAICT vmxnet doesn't either.  If you terminate on the host and send decap'd traffic into a guest, then you're OK.\nA team mate of mine wrote a whitepaper detailing what OpenStack Neutron does with VXLAN-offloads:  https://access.redhat.com/articles/1749113\n. @philips nope.\n. We need NIC-agnostic vxlan offload...until then, 10G is a pipe-dream in the public cloud.\n. Where is the kernel support for this ?  Ed Cree's patch set isn't even in net-next afaict...\n. @tomdee what is a good email address for you, I'd like to connect and try to smooth out this process.  Note that I'm actually not involved in the process at all -- just want to help.. @kanor1306 centos7.3 has 0.5.5. centos7.2 has 0.5.3.  you can grab 0.5.3 from here:  http://mirror.linux.duke.edu/pub/centos/7.2.1511/extras/x86_64/Packages/\nAs a side note, I see that fedora rawhide has 0.6.2. @ingvagabund do you still own the flannel package?. Thanks!  Question remains how to update it in CentOS.. ",
    "apeeyush": "Sorry! I meant yum install -y flannel and not yum install -y etcd. I'll try using the rawhide version of flannel sometime next week. Thanks for the help :)\n. ",
    "sujitfulse": "i was building with old commit. just now i cloned it with latest commit and it solved this problem. apologies .\nroot@ubuntu:/home/temp/flannel# git log -n2\ncommit 86ec8945474a159f20dbc6128f53b19471129aa3\nMerge: 9889a4d 878c525\nAuthor: Eugene Yakubovich <eyakubovich@gmail.com>\nDate:   Sun Oct 18 15:06:37 2015 -0700\n. ",
    "xdays": "Yes, you are right. I change the nit file to:\n\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nLimitNOFILE=40000\nLimitNPROC=1048576\nEnvironmentFile=/opt/everstring/flannel/etc/option.env\nExecStart=/opt/everstring/flannel/bin/flanneld\nExecStartPost=/opt/everstring/flannel/bin/mk-docker-opts.sh\n\nand option.env is:\n\nFLANNELD_ETCD_ENDPOINTS=\"http://10.50.0.149:2379\"\n\nit works now. \n. ",
    "coocla": "Thank you very much.\n. ",
    "eveny": "Pardon, just noticed that sudo journalctl -u docker -f was copied from the same test I did a little bit earlier. Behaviour - the same.\n. ",
    "ztao1987": "@eyakubovich Thanks for you reply. I already posted this question in Kubernetes community. I checked this google group thread, it seems he workarounds it by using namenode pod ip. But in my case, namenode pod ip is translated into somethings like bridge name, do you happend to known why?\n. I mean if I used namenode actual pod ip to start datanode, the datanode regcognize it as \"k8s_POD-2fdae8b2_namenode-controller-keptk\" and failed to start. I got workaround for this from @Luqman in the google group thread you provide. He mentioned that I need DNS to solve this problem. \n. Is this issue caused by flannel or kubernetes?  I mean \"if using namenode actual pod ip to start datanode, the datanode regcognize it as \"k8s_POD-2fdae8b2_namenode-controller-keptk\"\"\n. I got the answer from kubernetes community.  use the latest kubernetes and pass the params --proxy-mode=iptables to kube-proxy start command, HDFS cluster works now\n. @ksr1 Assumed you already have a namenode/datanode docker image, and then deploy it using ReplicationController/deamonset in k8s. Rember to start kube-proxy using params --proxy-mode=iptables. If you want to do data persistent, you should use volumes to volumeMounts the data whatever you want. No special things for other parts.\n. ",
    "ksr1": "@ztao1987 \nHi, I'm trying to setup a HDFS cluster in Kubernetes. Do you have any info you can share on setting up the cluster? Any set of instructions would be appreciated if have them documented. Thanks.\n. @ztao1987  Thanks for the reply. Will try setting the params in Kubernetes and try the setup.\n. ",
    "gurvindersingh": "+1: \n. ",
    "garo": "+1 for both, OSPF being more important to me.\n. ",
    "vboginskey": "This doesn't appear to be the case anymore:\ncore@ip-10-88-20-186 /etc/ssl/certs $ grep -i version /etc/os-release\nVERSION=1185.5.0\ncore@ip-10-88-20-186 /etc/ssl/certs $ ls -l *.pem | head -5\n-rw-r--r--. 1 root root 1383 Dec  7 09:11 A-Trust-nQual-03.pem\n-rw-r--r--. 1 root root 2772 Dec  7 09:11 ACCVRAIZ1.pem\n-rw-r--r--. 1 root root 2041 Dec  7 09:11 ACEDICOM_Root.pem\n-rw-r--r--. 1 root root 2281 Dec  7 09:11 AC_Ra\u00edz_Certic\u00e1mara_S.A..pem\n-rw-r--r--. 1 root root 2049 Dec  7 09:11 Actalis_Authentication_Root_CA.pem\nflanneld shouldn't enforce a separate SSL directory for certs and keys that work just fine when placed in the standard locations /etc/ssl/certs and /etc/ssl/private.. ",
    "ranbochen": "Thank you for your answer.\nI see a commit log here: https://github.com/coreos/flannel/commit/23c28ecee4059d5f18acfa271bf687fba8909058\n. ",
    "stevef1uk": "Simply tried udp. I didn't try that ping test.\n. OK, I have recreated my environment.\nNode 1:\npi@rpi2fr ~ $ cat /run/flannel/subnet.env \nFLANNEL_NETWORK=10.20.0.0/16\nFLANNEL_SUBNET=10.20.38.1/24\nFLANNEL_MTU=1472\nFLANNEL_IPMASQ=false\nNode 2:\nsteve@mark-mint ~ $ cat /run/flannel/subnet.env \nFLANNEL_NETWORK=10.20.0.0/16\nFLANNEL_SUBNET=10.20.41.1/24\nFLANNEL_MTU=1472\nFLANNEL_IPMASQ=false\nFrom Node 1 ping test (hopefully to the right thing?):\npi@rpi2fr ~ $ ping 10.20.41.1\nPING 10.20.41.1 (10.20.41.1) 56(84) bytes of data.\nFrom 10.20.41.0: icmp_seq=2 Redirect Host(New nexthop: 10.20.41.1)\nFrom Node 2 ping test:\nsteve@mark-mint ~ $ ping 10.20.38.1\nPING 10.20.38.1 (10.20.38.1) 56(84) bytes of data.\nFrom 10.20.38.0: icmp_seq=2 Redirect Host(New nexthop: 10.20.38.0)\nIs this so far so good?\n. Then on Node 1:\nroot@rpi2fr:~# source /run/flannel/subnet.env\nnohup docker daemon -H tcp://127.0.0.1:4243 -H unix:///var/run/docker.sock --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &\nroot@rpi2fr:~# ps ax | grep docker\n 2455 ?        Ss     0:00 runsv docker\n 2464 ?        S      0:00 svlogd -t /var/log/docker\n20404 pts/0    Sl     0:01 docker daemon -H tcp://127.0.0.1:4243 -H unix:///var/run/docker.sock --bip=10.20.38.1/24 --mtu=1472\n20448 pts/0    S+     0:00 grep docker\nroot@rpi2fr:~# route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         192.168.0.1     0.0.0.0         UG    202    0        0 eth0\ndefault         192.168.0.1     0.0.0.0         UG    303    0        0 wlan0\n10.20.0.0       *               255.255.0.0     U     0      0        0 flannel0\n10.20.38.0      *               255.255.255.0   U     0      0        0 docker0\n172.18.0.0      *               255.255.0.0     U     0      0        0 br-24597bc23fd3\n192.168.0.0     *               255.255.255.0   U     202    0        0 eth0\n192.168.0.0     *               255.255.255.0   U     303    0        0 wlan0\nOn node 2:\nThe same steps to start docker\nsteve@mark-mint ~ $ ps ax | grep docker\n25036 pts/3    Sl     0:00 docker daemon -H tcp://127.0.0.1:4243 -H unix:///var/rundocker.sock --bip=10.20.41.1/24 --mtu=1472\n27341 pts/3    S+     0:00 grep --colour=auto docker\nmark-mint ~ # route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         192.168.0.1     0.0.0.0         UG    0      0        0 eth0\n10.20.0.0       *               255.255.0.0     U     0      0        0 flannel0\n10.20.41.0      *               255.255.255.0   U     0      0        0 docker0\n172.17.0.0      *               255.255.0.0     U     0      0        0 br-52b68ce74035\n172.19.0.0      *               255.255.0.0     U     0      0        0 docker_gwbridge\n192.168.0.0     *               255.255.255.0   U     1      0        0 eth0\nOn node 1:\nroot@c1c5e685c11e:/# ping 10.20.41.2\nPING 10.20.41.2 (10.20.41.2) 56(84) bytes of data.\n64 bytes from 10.20.41.2: icmp_req=1 ttl=60 time=1.42 ms\nOn Node 2:\nping 10.20.38.2\nPING 10.20.38.2 (10.20.38.2) 56(84) bytes of data.\n64 bytes from 10.20.38.2: icmp_seq=1 ttl=60 time=3.59 ms\nSo it now works! I must have messed up one of the manual steps previously, probably on starting docker!\n. ",
    "davidsiefert": "Having enormous trouble with godep to update dependencies... what version of godep are you guys using where it works to update?\n. Okay I figured out this godep issue (I think) and have another open issue #406 which blocked me from updating the coreos/go-iptables dependency using godep.  Once that PR is merged, I have another fix on a branch to update go-iptables with the fix needed.  I really need a critical eye on #406 if anybody can help me out here.\n. Be so nice to get this merged... +1 :smiley: \n. Little bit confused on whether or not my commit message is formatted properly per contribution guidelines.  Please lemme know and I'll fix and repush.  Thanks!\n. Rookie mistake alert! Rookie mistake alert!  Nothing to see here!\n. i'm assuming it would be ideal to get these deps updated to their latest version wherever they were moved to.\n. This touches oauth of GCE.  I do not have a GCE account, nor any GCE experience.  Would someone be able to help out and verify this still works in GCE?  Thanks!!  I am also a Golang+Godep newb.  I do not know what I am doing.  Please someone save me from my mistakes.\n. omg! omg! really? i almost gave up on this.  thanks!!\n. Okay, figured out the problem--restarting flanneld requires me to reconfigure docker to be set with the right subnet written out to /run/flannel/subnet.env.  I think setting up my configs to reconfigure docker will solve this issue.  Closing.\n. ",
    "rob-64": "@xiang90 I did not figure it out, but I believe my issue is not flannel, but really libreswan/ipsec, as I am trying to encrypt flannel traffic between the two hosts.\n. ",
    "cristifalcas": "Sorry, it seems that it was taking the bridge from docker configuration\n. I think you have to check your PR, because it says you are modifying more than 10000 files. It looks like you are deleting everything?\n. For vxlan we need the Network key?. ",
    "esecules": "SOLVED: I had flannel's FLANNELD_ETCD_ENDPOINTS variable set to the wrong endpoints\n. ",
    "amitkgupta": "@xiang90 \nThe comments say:\n```\n// try to give each host a /24 but if the whole network\n// is /24 or smaller, half the network\n// skip over the first subnet otherwise it causes problems. e.g.\n// if Network is 10.100.0.0/16, having an interface with 10.0.0.0\n// makes ping think it's a broadcast address (not sure why)\n```\nSo if the network is bigger than /24, then give a /24 subnet, and if it's smaller than or equal to a /24, then give half the subnet (e.g. if it's a /26 network, give a /27 subnet).  In particular, it tries to give it the second subnet.  So I interpret it to imply the following examples:\n- Given 1.2.0.0/16, the subnet should be 1.2.1.0/24, which is 1.2.1.0 to 1.2.2.0.\n- Given 1.2.224.0/19, the subnet should be 1.2.225.0/24, which is 1.2.225.0 to 1.2.226.0.\n- Given 1.2.3.0/24, the subnet should be 1.2.3.128/25 which is 1.2.3.128 to 1.2.4.0.\n- Given 1.2.3.4/30, the subnet should be 1.2.3.6/31, which is 1.2.3.6 to 1.2.3.8.\nThere's definitely something off with the current code, because when I give 10.244.8.0/24 I get the error message seen in my commit message:\nPicking subnet in range 10.244.8.128 ... 10.244.8.128\nFailed to acquire subnet: out of subnets\n. I'm essentially running flannel as follows:\n$ etcdctl -C ${ETCD_ENDPOINT} set \\\n        /coreos.com/network/config \\\n        '{\"Network\":\"10.244.8.0/24\",\"Backend\":{\"Type\":\"vxlan\"}}'\n$ flanneld \\\n        --etcd-endpoints=${ETCD_ENDPOINT} \\\n        --public-ip=${PUBLIC_IP}\n. Okay, I think I may have misunderstood what the intention of the code was.  I thought it was determining one subnet, of length cfg.SubnetLen, and determining the minimum IP in that subnet as cfg.SubnetMin and the (exclusive) maximum IP for that same subnet as cfg.SubnetMax.\nI think what you're saying is that it's describing all (except the first) smaller-sized subnets, each of size cfg.SubnetLen, and cfg.SubnetMin is the minimum IP of the first smaller-sized subnet, cfg.SubnetMax is the minimum IP of the last smaller-sized subnet.\nIn the case where the network is /23 or smaller, the \"smaller-size\" is half the original network, so there's just one smaller subnet, and it makes sense that cfg.SubnetMin and cfg.SubnetMax are equal (and 10.244.8.128 is the correct value in the case I mentioned).\nI'll have to dig in more to see then what's causing the errors in the logs, but for now I've worked around it by setting a larger network than 10.244.8.0/24.\n. @eyakubovich I don't have that deployment anymore, but you can see from https://github.com/coreos/flannel/pull/378#issuecomment-160389993 I was setting it to:\n{\"Network\":\"10.244.8.0/24\",\"Backend\":{\"Type\":\"vxlan\"}}\n. ",
    "nodkz": "Yep, it's quite challenging via STUN. But may be exists more simple solution?! \nThis ability implemented in weave https://github.com/weaveworks/weave/issues/1717 \nI was able to create network with two hosts:\n- at first host with public ip at DC, I ran weave launch.\n- at the second host behind office-gateway (with internal ip), run weave launch 76.13.1.9.\n- and all works. No STUN, no any changes on office-gateway. I also can pass internal network from data center to local dev machine.\nWeave is a little bit foreign for CoreOS ecosystem. So right now I choose old-school ssh tunelling for my devOps tasks.\nflannel with such connection ability will give better cooperation with coreOS cluster for developers, administrators. \nPS. My task was create a hidden mongodb's member for backup and analytics at office. Other members are in private network at DC. \nPSS. So I want skilled up with CoreOS and flannel, check it how good its works. And if all goes well, change deployment operations and rolling out staging versions.\n. ",
    "dalanlan": "/sub\n. ",
    "mtanino": "@tomdee Yes. May I send a pull request? \n. @tomdee @steveeJ \nHi Tom, Stefan,\nCould I get a review for the patch? Thank you for your attention.\n. @tomdee \nThank you for your help.\n. ",
    "ruissalo": "hi! any ideas when is this going to be released?\n. ",
    "chrissnell": "CoreOS version:\ncore@coreos-001 ~ $ cat /etc/os-release\nNAME=CoreOS\nID=coreos\nVERSION=835.8.0\nVERSION_ID=835.8.0\nBUILD_ID=\nPRETTY_NAME=\"CoreOS 835.8.0\"\nANSI_COLOR=\"1;32\"\nHOME_URL=\"https://coreos.com/\"\nBUG_REPORT_URL=\"https://github.com/coreos/bugs/issues\"\nflanneld is being started by the stock unit file:\n```\ncore@coreos-001 ~ $ cat /lib/systemd/system/flanneld.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nRequires=early-docker.service\nAfter=etcd.service etcd2.service early-docker.service\nBefore=early-docker.target\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=\"TMPDIR=/var/tmp/\"\nEnvironment=\"DOCKER_HOST=unix:///var/run/early-docker.sock\"\nEnvironment=\"FLANNEL_VER=0.5.3\"\nEnvironment=\"ETCD_SSL_DIR=/etc/ssl/etcd\"\nEnvironment=\"FLANNEL_ENV_FILE=/run/flannel/options.env\"\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStartPre=-/usr/bin/touch ${FLANNEL_ENV_FILE}\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n  /usr/bin/docker run --net=host --privileged=true --rm \\\n  --volume=/run/flannel:/run/flannel \\\n  --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n  --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n  --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n  --env-file=${FLANNEL_ENV_FILE} \\\n  --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n  --volume=${ETCD_SSL_DIR}:/etc/ssl/etcd:ro \\\n  quay.io/coreos/flannel:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=true\nUpdate docker options\nExecStartPost=/usr/bin/docker run --net=host --rm -v /run:/run \\\n  quay.io/coreos/flannel:${FLANNEL_VER} \\\n  /opt/bin/mk-docker-opts.sh -d /run/flannel_docker_opts.env -i\n```\n. I just found this:  https://github.com/coreos/flannel/issues/367\nI will try 0.5.5 and close if fixed.\n. Yep, I can confirm that this is a duplicate #367 \nThanks for the fix, @eyakubovich !\n. ",
    "macropin": "Not sure if it is related but we are having an race condition with /run/flannel/subnet.env generation. \nThe flannel systemd service starts the flannel docker container (ExecStart) and then runs /opt/bin/mk-docker-opts.sh in a following container (ExecStartPost) but the subnet.env file has not yet been generated, so the resulting /run/flannel_docker_opts.env is empty. \n. ",
    "stepanstipl": "I'll close this one, to be honest not sure what was the issue. I've since changed to use flannel in client/server mode and I don't experience this anymore even with flannel 0.5.5. Quite possible it was related to some race condition in my CoreOS services config. I shamefully admit I didn't note enough details and changed too many things in my setup to be able to reproduce easily. Thanks though for coming back\n. ",
    "luxas": "Yeah, I can make the PR soon.\n. Ping @philips \n. @glevand Updated. Merge if you think it's fine\n. Please merge this. Another thing, when will a new release be out?\n. @glevand Yeah, I see no one is actively working on it.\n@philips Who's gonna take over? From an outside perspective it's a bit strange for a project like this not to have a maintainer. I'm absolutely not blaming anyone, just saying that would be great if a such great piece of code like this is maintained :)\n. @tomdee @steveeJ Please take a look\n. Oh, didn't see #439\nClosing in favor for that one.\n. Here I've built flannel for Kubernetes for four different arches. \nI'll help with getting this into core.\nSee the contents in this dir: https://github.com/kubernetes/kubernetes/tree/master/cluster/images/flannel\nFull proposal: kubernetes/kubernetes#17981\n. Yes, the ideal thing would be to use a buildrooted env, but it's much harder.\nKubernetes already had the debian-iptables image for amd64, so I'm cross-building that one instead, and using it as base. It is larger yes (~100MB), which is a bad thing of course.\nIf you've got buildroot configs for armv6, arm64 and ppc64le, ping me and I'll take a look\n. We may use gcr.io/google_containers/kube-cross:v1.6.2-2 as the building image like I do in: https://github.com/kubernetes/kubernetes/tree/master/cluster/images/flannel\nThe only thing we need in the container is iptables, and it's dynamically linked, so the container have to include glibc => we may use busybox:glibc, armel/busybox:glibc, aarch64/busybox:glibc and ppc64le/busybox:glibc\nI think we should compile iptables from source, and put it in a busybo:glibc image along with a statically linked flannel binary\n. Looks overall good to me.\nMinor nit: to avoid duplication (although it's little code), you could refactor the Dockerfiles to a Dockerfile.arch or .tmpl and sed it with the right values to a Dockerfile before building.\n. Great!\n. I hope you will push the images ASAP (I'm waiting for them :smile:), then we may close this issue\n. @tomdee @steveeJ is it possible to get this into the v0.6.0 release?\nIt will probably be required for v1.4 turnup UX...\n. @mikedanese Should it validate the minimum k8s version maybe?\nIt seems worthwhile to do, and since https://github.com/kubernetes/kubernetes/pull/27880#issuecomment-241068443 is on it's way in, we maybe can use it and require v1.4.0-alpha.3+.\nWDYT?\n. Yes, do you have bandwidth to update this PR with the (hopefully) last comments today?\n- k8s compability check (at least v1.4.0-alpha.2 or 3)\n- log nodename == \"\" error\n- annotation vs configmap\nI can make the PR with v0.6.0 + this PR cherrypicked to gcr.io\n. > log nodename == \"\" error\nSorry if it was unclear: https://github.com/coreos/flannel/pull/483#discussion_r74844847\n\nI think we should only support configmap initially. This is exactly the usecase that configmap was designed for. I don't see a benefit to using annotations.\n\nOK, but can we package them in the same file then for easy flannel turnup when running kubectl apply -f https://k8s.io/turnup/network/flannel.yaml for example?\n(But I know that's an implementation question, not direct coding question for this PR)\n. @mikedanese Maybe we can use kubernetes/client-go for talking to the apiserver here?\nMaybe client go is leaner than the current vendored packages.\nI'd like to get a working version of this in ASAP, as I have to submit the manifests to hyperkube before the v1.4 release, so please come back to this soon :)\n. We've debated about the multiarch naming for kubernetes as well, and came to the conclusion that binary-arch:version is what we're using. See: https://github.com/kubernetes/kubernetes/pull/26863 and https://github.com/kubernetes/kubernetes/pull/23059#issuecomment-201924090\nOf course you may change to binary:version-arch, but I strongly prefer the current naming.\nAnother thing: earlier you've released flannel without the \"v\" prefix (current release is flannel:0.5.5).\nWith this new build script, it seems like the v has appeared in the tag; which is a breaking change.\nI'm not opposed to it; v is good, but you should be consistent with other coreos images.\nAdding pthread LGTM\n. You should get this merged and release v0.6.1\n. @tomdee @mlbiam Can you verify CONFIG_VXLAN is enabled when running \nbash\ncurl -sSL https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash\nI think the problem is that vxlan support isn't present\nAlso, try going down to udp and see if it works\n. Oops, I did that all the time and it worked for me, sorry for not informing good enough that you should have done that.\nWell, seems like you did figure it out :)\n. Anyway, for others encountering this problem:\nThe --allocate-node-cidrs=true and --cluster-cidr=10.244.0.0/16 flags have to be set on controller-manager for it to function\n. Okay, the only thing I want here is that this manifest will be multiarch, so I can easily can run kubectl apply on it.\nPlease merge this as soon as possible. I removed the strange parts from this PR now\nIf you want to create a separate project for kubeadm integration, feel free to.\nIf you do, it would be the same as weave has done, but a separate directory of the flannel project is also fine for me.\nAt least, please merge and I'll add this to http://kubernetes.io/docs/admin/addons/, it will be good for users.\n. ping @tomdee \n. a) We need somewhere to point users to regarding flannel and kubeadm\nb) The manifest has to point to a specific -ARCH tag (-amd64 by default) in order to be able to sed it to arm or arm64 easily\nSee https://github.com/kubernetes/kubernetes/pull/30356 for reference\nThanks, and please let this though!\n. I've documented it under kubeadm is multi-platform here: \nhttp://kubernetes.io/docs/getting-started-guides/kubeadm/\nLet me know what more you need\n. Yes.\nYou need to have set --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin and have the cni bins in /opt/cni/bin\nYou should set --allocate-node-cidrs=true and --cluster-cidr as well in order for it to work.\nIf you're using the manifest linked below, you should set --cluster-cidr=10.244.0.0/16.\nThe manifest will use the flannel binary in /opt/cni/bin and put a file in /etc/cni/net.d.\nWould love to have a doc of this as well.\n. That's not expected either. \nAll binaries should be present on-host before (flannel is one of the \"official\" binaries in the CNI tarball)\n. kube-flannel.yml already requires k8s v1.6+, you should really add the RBAC contents to the main manifest file. Forcing the user to apply two manifests when everyone else has one isn't optimal.. Example: https://github.com/kubernetes/kubernetes.github.io/pull/2389#issuecomment-311076232. //go maybe should be /go\n. I'd prefer if you set IMAGE_SUFFIX to -$(ARCH) for all cases\nAnd then do an ifeq ($(ARCH),amd64) and push without suffix as well for amd64\n. First it checks for the license, then gofmt and finally runs the go test command below.\nThe license-check and gofmt targets are dependencies of the test target\n. @tomdee Why isn't cover a dep of test?\n. ... are [amd64 arm arm64 ppc64le]\n. I think it's much safer to do this outside the container.\n. curl -sSL is more reliable I think. Also, it might be worth it to make the iptables version a variable\n. Same here, preferably outside the container\n. nit: Why not ignore the whole dist?\n. missing newline\n. Again, why not just rm -rf dist to make it look cleaner?\n. Why not build tar.gz for all arches while we're at it?\n. If you're uploading the flannel binary and ACI for all arches, then it's okay for me.\nProbably worth leaving a comment about the decision though.\n. Why not make a --subnet-manager flag that defaults to etcd?\n. Voting for ns/podname + lookup initially\n. A default NetConf file should exist, it shouldn't be dependent on the ConfigMap\n. Why are you reading/writing the cni netconf?\nIt's not used in the flannel image, but with kubelet\nSo I don't see the point of it here, but if it's required, you probably just should copy from cniConf to cniConfTargetPath\n. The even better solution would be that after flannel knows in which pod it runs, it could just fetch the values of it's annotations (with default values) and create a NetConf from those values.\n. node name can't be calculated from pod spec %s/%s\n. Alright, missed that hostPath mount.\n. If cniconf doesn't exist, it should just pick the default:\ngo\ndefaultCniConf := []byte(`\n{\n    \"name\": \"cbr0\",\n    \"type\": \"flannel\",\n    \"delegate\": {\n      \"isDefaultGateway\": true\n    }\n}`)\nI think the ConfigMap is great for customization, but the flannel daemonset should do just fine without it.\n. I meant that it initially could read from the pod's (aka from the daemonset metadata) own annotations, e.g.\nflannel.alpha.coreos.com/backend-type would default to vxlan\nflannel.alpha.coreos.com/network-cidr would default to 10.1.0.0/16 or 10.244.0.0/16 or whatever.\nLater, it should try to read the cidr option from controller-manager like you said.\nIt would then be easy to customize the values by (kubectl apply) instead of using the configmap\nyaml\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n      annotations:\n        flannel.alpha.coreos.com/network-cidr: 10.155.0.0/16\n        flannel.alpha.coreos.com/backend-type: udp\n. go\nglog.Warning(\"failed to read cni conf: %v\", err)\ncniConf = defaultCniConf\n. @mikedanese @aaronlevy \nSince we have the node name here, could we as a intermediate step while waiting for componentconfig parse the two first bytes in `.spec.PodCIDR and make a /16 network of it?\nWhile I think the configmap is good, I also think that the one-line kubectl apply -f https://k8s.io/flannel-network.yaml is more important, and should\na) default to something (like weave does here) along with the vxlan backend.\nb) guess it and hope for the best\nc) entirely skip the configmap and use annotations on the Pod instead, since it is only read once (updating the configmap doesn't update the flannel conf anyway)\nWDYT?\n. Ah, yea, thinking about it that way makes sense\n. @mikedanese I let you decide about annotation vs configmap in this initial PoC, but it would be nice to have something easy in the beginning, just to kick the tires.\nAt least it would be great to finish this PR (fixing Aarons last comment), and maybe build a unofficial release to gcr.io with v0.6.0 flannel + this PR.\nI can do that if we want to\n. @tomdee @steveeJ ^\n. Does calico and weave also do this?\nI guess we should be consistent about how a network daemonset behaves...\n. Was thinking about that, but that would force v1.4.0-alpha.3+ (which soon is gonna be released)\n. Can you update this to 10.2.0.0/16?\nWe thought that would be the best flannel CIDR (discussed in the kubeadm PR)\n. Maybe update to quay.io/coreos/flannel:v0.7.0-amd64? (although it isn't released yet)\n. busybox should be sufficent\n. maybe include a while loop in order to be safe\n. just etcd?\n. ",
    "Zpandas": "check it out to make sure:\n 1 etcd sever works with specific ip interface you want\netcdctl mk /coreos.com/network/172-17/config '{\"Network\":\"172.17.0.0/16\", \"Backend\": {\"Type\": \"udp\",\"Port\": 7890}}'\n2 flanneld deamon starts with right like this \n    --networks=172-17 \n. ",
    "vijaykumark": "AFAIK, bonding is used when you need redundant links, fault tolerance or load balancing networks. Bonding allows you to aggregate multiple ports into a single group, effectively combining the bandwidth into a single connection.\nIn our scenario, the two ethernet interfaces (eth0 and eth1) are connected to two different networks and both interfaces have IP address from different subnets (eg. 10.0.0.1/20, 192.168.0.0/20).\nNot sure if we could use bonding in our scenario.\n. @philips : After your question, added more information in the comments sections. Also updated the issue with a diagram, which should explain the issue more clearly. Hope this helps.\n. There is no response from @philips or from flannel team since last year.. @tomdee @philips @joshrosso \nWe are not using vxlan backend and we are not using UDP. \nWe use two physical ethernet/fiber networks attached over two ethernet interfaces to the same server. All the networking is managed externally by different switches.\nSo, the server will not know how the networks are managed once the traffic goes out of the ethernet interfaces. \nIs it possible to attach flanneld to such a network? If yes, do you have documentation which I can refer to?. @tomdee  We are running flannel on baremetal servers (CentOS 7) which have two ethernet ports connecting to two different physical ethernet networks. There is no other SDN solution involved other than flannel.. Closing the issue as it's stale.. ",
    "yogeshmsharma": "to add on this, for traffic management incoming traffic can be pushed over eth0 and outgoing on eth1.   or  eth0 talking  to external world and eth1 to internal world.\nHow flannel can be configured to do so? \n. ",
    "NikolayMurha": "Hello. \nWe have the same issue.\nWe have three dedicated servers in one datacenter (DC1). This servers have  public interface eth0 with public IP. One server is kubernetes master+worker, another two - workers. All worked fine.\nBut we have a issue to add workers from another DC with private subnet 172.25.0.0/16.\nI've openvpn tunnel from servers (in DC1) to DC2.\nSo all servers in DC1 have two interfaces (eth0 - public IP, and tun0 - IP in private subnet)\nAll nodes see each other, traffic for nodes in private DC routed to the tun0 and another traffic\u00a0routed to eth0.\nUnfortunately, in this case, PODs started on servers in the DC1 cannot see the PODs on servers in the DC2 because traffic for DC1 should be passed to another interface (tun0).\nI can change --iface to the tun0, and reconfigure all nodes, to communicate through private IPS, but then all the traffic will goes through the VPN tunnel, even the between nodes in the same DC.\nI think it is better use the routing table and not to send everything through one interface.\nRouting table contains everything you need for this: \ndefault         x.x.x.x   0.0.0.0         UG    0      0        0 eth0\n 10.10.0.0       *               255.255.0.0     U     0      0        0 flannel0\n 10.10.38.0      *               255.255.255.0   U     0      0        0 docker0\n 172.16.0.0       172.16.5.1      255.255.0.0     UG    0      0        0 tun0\n. ",
    "celevra": "we have also 2 interfaces on each node in a 3 node cluster\nwe want the \"real\" traffic through eth0 and the \"Cluster interconnect\" traffic through eth1\n. same Problem here\n. ",
    "AnberLu": "@vijaykumark  hi\uff0chave you slove it ?. ",
    "joshrosso": "@tomdee @philips \nI setup a cluster routing to 2 nics (not bonded) yesterday using the vxlan backend as described by @tomdee. I believe this issue can be closed?. ",
    "warmchang": "@vijaykumark , flannel used to support multi networks (as EXPERIMENTAL), but removed later.\nMulti-network mode (EXPERIMENTAL) & https://github.com/coreos/flannel/pull/633.\n. ",
    "wzrdtales": "That is very unfortunate @warmchang :/. ",
    "zbwright": "may be partially addressed.\n1st item: not addressed. there was no additional info added to answer Where is the webapp client? How are messages routed from the public?\n2nd item: addressed? references to multi network subnets have been removed. (experimental client-server doc is still available from the CoreOS site, but is slated for removal. https://coreos.com/flannel/docs/latest/client-server.html)\n. adding \nMTU is calculated and set automatically by flannel. It then reports that value in subnet.env. This value cannot be changed.\nto configuration.md\n. Added to the ReadMe under Using flannel.. I believe all this issue asks is for the following to be added to the Flannel instructions (so that it can be removed from \"kubeadm is multi-platform\" on https://kubernetes.io/docs/getting-started-guides/kubeadm/)\nInstalling on ARM\nInstalling flannel on ARM requires that you customize the curl command, depending on your platform.\nexport ARCH=amd64\ncurl -sSL \"https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml?raw=true\" | sed \"s/amd64/${ARCH}/g\" | kubectl create -f -\nReplace ARCH=amd64 with ARCH=arm or ARCH=arm64 depending on the platform you\u2019re running on. Note that the Raspberry Pi 3 is in ARM 32-bit mode, so for RPi 3 you should set ARCH to arm, not arm64.\n. @tomdee should I create a PR for this addition to the running.md doc?. I believe Josh is going to take a stab at this, and I'll edit/massage. We've added notes to the original google doc plan for this project.. I can add this to the ReadMe. Question: Should I remove the Theory of Operation text and diagram? is it outdated? Or should I leave it, and add the Getting Started materials after?. This sentence doesn't make any sense. Maybe a little rewrite? does kubeadm tell you to create a CIDR?\nA great way to create a new Kubernetes cluster is with kubeadm which helpfully points out the need to define \"a pod network CIDR\" for example by passing in --pod-network-cidr 10.244.0.0/16 when running kubeadm init. . I believe that the ReadMe could still use some introductory information. Some of your original plan is still missing.\n- Flannel is k8s focused and offers different datastores and different networking backends \n- mention K8s (/CNI) and Docker integration, and maybe Container Linux integration. \n- Flannel's relationship to Calico isn't mentioned yet\n- networking details are not introduced:  L3 routed, no L2 traffic between hosts\n- kube subnet manager info was removed - pinging between two pods is going to become another doc. I'm going to pull the remaining tasks out of this issue, create a new one referencing them, and close this issue. \nping between two pods: https://github.com/coreos/flannel/issues/741\nadd a few items to the readme: https://github.com/coreos/flannel/issues/742\nclosing this ticket.. I've got a draft going of this - will need more input from Josh or another source to flesh it out more. I can continue working from this Issue's comments, and generate a PR when you think it's time.\n. this item (still) has not yet been addressed:\n\n[ ] zero-downtime restarts should be moved into an upgrade section (or standalone doc) with some extra k8s focused content. Created a new issue to add a doc on zero downtime restarts, in order to close this issue. Closing.. Looks like this was fixed with https://github.com/coreos/flannel/pull/690, and this issue should be addressed as a release note, not a troubleshooting item.. @tomdee sorry about that - still learning how this works. I will certainly do smaller PRs next time. Would you like me to figure out how to do it to this one as well?. I'm not going to squash these commits. There is other work interleved with mine, and I don't have the git mojo to not make a mess of it.. @joshix I can neither assign nor label issues in this repo.. maybe @athai can help with this?\n. done. done.. question: is the packet diagram still accurate? Looks to me like it should be, but someone might want to confirm, please.. done.. @tomdee will have to answer that.. done.. done.. @tomdee . for this section I need flannel-specific things:\nMailing list: etcd-dev (no flannel group)\nIRC: #etcd on freenode.org (is there one? i get '#flannel-dev: (no topic set)')\nroadmap  (flannel 404s)\nreporting_bugs.md (flannel 404s) . Updated: \nMailing list: coreos-dev\nIRC: #coreos on freenode.org \nroadmap: https://github.com/kubernetes/kubernetes/milestones\nreporting_bugs.md: will create and add file to  Flannel/Documentation (on PR #679). still an open question for @tomdee . Many Kubernetes deployment tools use flannel by default, including xxx, xxx, and xxx. For the fastest possible experience, use the open source Tectonic Installer, which provides good defaults, and enables install automation.\n\nFor more information on using flannel with Kubernetes, see xxx. For information on deploying flannel manually, using the (currently alpha) Kubernetes installer toolkit kubeadm, see Installing Kubernetes on Linux with kubeadm.\n(Which other deployment tools should we call out? Minikube, kops for AWS, and Kargo? https://kubernetes.io/docs/home/). I cannot find a good reference page for simply k8s + flannel, but we might call this one out somewhere: https://kubernetes.io/docs/setup/pick-right-solution/. @tomdee @joshix please note - I removed much of the kubeadm material, but I'm really not clear on whether the stuff I left is more generic or not. I think this section should should fulfill Brandon's comment, and \"show them indepth ways on how to manage it\". The specifics can live in running.md. This section should simply tell people what you can do with flannel, once it's up and running / why it's great. I think pinging pod to pod is one of these things. what are the others?. We need to add a note here that flannel ships with etcd2 - for etcd3 go to xxx.. ok - I was overly worried about the etcd2 > 3 stuff.. nit picky and precise is good.. I think I've done it, but will do again.. And I don't know why, but I cannot reply to your previous comment. The etcd comment must have been original to the doc. Where can I find info on setting up k8s config?. removing comment for now - we can address later.. yes. resolved.. ... If kubeadm is used, pass .... this section is confusing. first you say you must apply both, then you list errors if you don't apply one, then you say you can just apply one. if it's true, i think just say 1) you must apply both, then 2) if you don't apply the rbac manifest, you'll see errors.. Note: The image name is coreos/flannel-git, and releases come from coreos/flannel.\ncome from? may be found at?. ... current HEAD of master. Use with caution.\n...\nThe platform is amd64. ... Flannel does not make ...\nwe're capitalizing project names at the beginning of sentences. feel free to disregard this note.. CoreOS Container Linux - always capitalized.. This address is chosen when flannel starts. Because leases are tied to the public address, if the address changes, flannel must be restarted.. this paragraph is confusing, in that I'm not sure why all VMs will have two interfaces. is that just how they work (i don't think so)? or is it a result of the combination of flannel and Vagrant? Please make sure my suggestions are true before accepting them, cause i very well may be wrong.\nVagrant assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address 10.0.2.15, is for external traffic that gets NATed. \nThis may lead to problems with flannel. By default, flannel selects the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this issue, pass the --iface eth1 flag to flannel so that the second interface is chosen.. ... VXLAN ... If you see errors similar to the following, confirm that the user running flannel has the right permissions (or try running with sudo).. cap Flannel at the start of sentences.\n... A delay in contacting pods in a newly created host may indicate control plane problems.  ... . 1) The type of backend. For example, if encapsulation is used, vxlan will always perform better than udp. For maximum data plane performance, avoid encapsulation.. 2. The type of MTU. ...  To troubleshoot, first ensure that the ... . needs ticks for commands. check that I haven't misplaced them, please.\n--pod-cidr kubelet command-line option or the --allocate-node-cidrs=true --cluster-cidr=<cidr> controller-manager command-line options.. if it's not recommended, are you sure you want to mention it?. ... small, single binary agent, called ... on each host, and is .... VXLAN. ... containers are networked .... need a space\nFor example, CoreOS's ....  ... see TODO ?. :+1: . I meant, if it's not recommended, you might want to remove the entire paragraph, and just not tell people how to do it.. ",
    "smessmer": "Any news on this? Is this a bug or am I using it wrongly?\n. Sure. The pull request I attached is fixing it.\n. ",
    "shanegibbs": ":+1: here\n. For those that are interested -> https://github.com/coreos/flannel/pull/409\n. @philips, what would the process to get this merged be? Anything I can help with?\n. Looks like the etcd client v3 won't be using basic auth\n. ",
    "ysh7": "This problem was due to removal of docker0 interface. No problems with flannel.\n. ",
    "zhulinhong": "Everything is OK, after the k8s installed. But I restart the flanneld, the route flannel0 not exist. I need to restart the docker0 to fix the issue. What's the root case is ? \nthx\n. ",
    "okamototk": "I'm new for godep. Do you mean I should rebase netlink? \nHow can I rebase netlink with godeps?\n. ",
    "danderson": "Apologies, this is not the right project to bug, since the systemd unit is not shipped in this repository. I'll refile in the right place.\n. ",
    "glevand": "It looks OK, but it would be more flexible if the static link was set by a variable which could be turned off.    Maybe something like:\nSTATIC_LINK=...\nGLDFLAGS=\"... ${STATIC_LINK})\"\nI'll test next time I build flannel.\n. @luxas I added static link to my buildroot flannel package https://github.com/glevand/buildroot--buildroot/commits/master.  I'm hoping this will eventually replace the old flannel build system.\nRight now there is no flannel maintainer, so we just need to wait until a new maintainer is found.\n. I noticed my 1st version doesn't do quite the same thing as the original.  I'll push out an update when ready.\n. @jipperinbham Your patch seems to do some whitespace cleanup and add a feature.  I think you should split that into two separate patches.\n. This is a lot of change for one patch.  I recommend you break it up with  each smaller patch a self contained update. \n. @robdaemon As I said, it is a big patch, it would be better to review and to maintain if it was broken up.\n. You are removing flannelbox.config, but I didn't see how you create the final flannel docker image.  What are your ideas?\n. @luxas It seems you are using this debian-iptables-xxx image as your base for the final flannel image.  The current flannel build uses buildroot to create the final docker image.  I would think your debian based image is considerably larger, and takes longer to start up?\n. @luxas I have this https://github.com/glevand/buildroot--buildroot.  I used the master branch to create the arm64 flannel image I use for CoreOS. It should be straight forward to add defconfigs for armv6 and ppc64le.\n. @tomdee What do we need to make arm64  a \"supported\" platform?\nI think the build system should at least have a way to output a container, or file system for a container.\n@luxas Would just building busybox with iptables enabled be simpler?\n. @qianzhangxa CoreOS currently packages flannel as a docker container.\nSee https://github.com/glevand/coreos--coreos-overlay/blob/master/app-admin/flannel/files/flanneld.service\n. Rebased to latest, changed $(shell pwd) to $(CURDIR.. I'd like to see a Dockerfile that works for cross compiling arm64 builds.\nDoes alpine have aarch64 cross gcc?  If not, maybe golang:1.6-wheezy would be better.\nDoes golang:1.6-alpine have arm64 cgo support?\n. It seems we'll need to propagate the makefile go environment to the flannel_build container so it picks up GOARCH, etc.\n. @tomdee In that link they are disabling cgo support for the build, but flannel needs cgo.\nI checked wheezy, and it does not have aarch64 compiler support.\nI think an answer is to use a recent debian or ubuntu image and install the cross compiler packages (gcc-aarch64-linux-gnu and libc6-dev-arm64-cross), then use a gimme script that can install a go compiler with arm64 cgo.  See https://github.com/coreos/ignition/pull/235/commits/787b5104676d2477bd460eac343c1d307d7dc5a5\n. ",
    "sjpotter": "Are you just asking for a way to programatically tell k8s that a node has to go offline for maintenance purposes so stop scheduling pods to it?  i.e. should it care about why its going down for maintenance?  i.e. seems generic functionality.  \nthinking re https://github.com/kubernetes/kubernetes/issues/3885\n. I'm digging into this in trying to understand flannel and how one would approach this\nThere are 2 aspects from what I can see to flannel\n1) getting a subnet for the local machine (and setting it up as necessary)\n2) ensuring that the local machine can access the rest of the network\n1 isn't really necessary in what I'm proposing, but without doing major surgery to the code base probably doesn't fit in\n2 is still required.\nSo in looking into how #1 is handled, basically on startup it will get a network eventually call network.init()  which calls the backend interface method RegisterNetwork()  which gets the subnet lease.\nIt would make sense in a hybrid situation that RegisterNetwork() actually not do anything of value, but it has to return something that Flannel can make sense of, as the value is used elsewhere (i.e. network.runOnce() calls subnet.WatchLease() on it.\nInstinctually, what I'd like to propose for the hybrid case is a wrapper backend that can wrap any other backend, the wrapped backend will handle the 2nd issue describe above, while the wrapper will provide a RegisterNetwork() that returns some form of sentinel value for a lease (perhaps the network is 0.0.0.0) and this lease is handled appropriately by the rest of the code (i.e. never times out and WatchLease() will handle it correctly.\nthis is just my initial attempt at understanding the code, any pointers / suggestions would be appreciated.. so vxlan is a bit over my head at this point, but doesn't it come down to how packets are routed?\ni.e. flannel assigns a /24 space to leaseless hosts.  This wouldn't be a routable subnet (unlike the subnet a \"normal\" host gets), flannel will have to route each ip assigned from the /24 independently, its in some ways can be viewed as simply a dhcp space.\nleaseless hosts will then has a physical ip of X and a flannel overlay IP of Y, if flannel knows to route packets for Y via the physical IP of X, it would work.  In practice isn't this why I can do this today with aws/gcp (non vxlan), because in those cases I can make it that X == Y and the underlying route table of the cloud takes care of routing.\nI should note, this does seem possible in ovs/ovn (though haven't tried to implement it there yet either). Ok, I believe I have more clarity on the issue, namely there's lack of support for controlling the \"source\" address.\nso a thought experiment that I don't know if is possible\nImagine we allocated 192.168.10.0/24 to be the ip space for \"leaseless\" hosts.\nFor Host B, we allocate ip (ala dhcp) 192.168.10.54 to be its \"vxlan\" ip address.  If outgoing packets from Host B looked like they were coming from 192.168.10.54 and vxlan knew that packets being sent to 192.168.10.54 had to be encapsulated to be sent to 10.0.0.2 it be similar to trying to connect to 192.168.1.1\nso from a nat perspective, one would want to rewrite the packet so that any packet destined for the flannel network space would first be rewritten so that its source is 192.168.10.54.  so we took our packet that was \n   source:10.0.0.2 dest:192.168.1.1\nto\n   source:192.168.10.54 dest:192.168.1.1\nits then encapsulated so that its has a source of 10.0.0.2 and a dest of 10.0.0.1 and can get routed to host A, unpacked and look to host B as a packet from 192.168.10.54 to 192.168.1.1 and when it tries to respond to 192.168.10.54 it can be encapsulated similarly.\nor tldr: is there someway to give leaseless hosts a flannel/vxlan address and encapsulate to them individually.. Where I can believe my thought experiment falls down is that the socket host B uses to connect to host A's container will be bound to IP 10.0.0.2 and routing the packets from 10.0.0.1's container will still be problematic.\non the flips side, what if 10.0.0.1 had a nat type mechanism so that any packets its sending destined for 192.168.10.54 get rewritten for 10.0.0.2.  Dont know if vxlan type mechanism can do NAT or just encapsulation.  Would seem I want both.. ",
    "someword": "Check out the list of tags here https://quay.io/repository/coreos/flannel?tab=tags\nIn this repo they don't have a 'v' in the version tag.  So you should be able to pull the following\n quay.io/coreos/flannel:0.5.5\nAlso fwiw doing 'docker pull -a quay.io/coreos/flannel' would pull every tag which could help you find the specific tag you want. \n. ",
    "idvoretskyi": "@someword thank you.\n. ",
    "euclid-geometry": "Additional Information about TCP connection between docker containers of different host\nContainer A from Node1( 192.168.0.202 )\nifconfig\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n        inet 172.16.30.2  netmask 255.255.255.0  broadcast 0.0.0.0\n        inet6 fe80::42:acff:fe10:1e02  prefixlen 64  scopeid 0x20<link>\n        ether 02:42:ac:10:1e:02  txqueuelen 0  (Ethernet)\n        RX packets 6  bytes 508 (508.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 6  bytes 508 (508.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nContainer B from Node2( 192.168.0.97 )\nifconfig\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450\n        inet 172.16.83.3  netmask 255.255.255.0  broadcast 0.0.0.0\n        inet6 fe80::42:acff:fe10:5303  prefixlen 64  scopeid 0x20<link>\n        ether 02:42:ac:10:53:03  txqueuelen 0  (Ethernet)\n        RX packets 6  bytes 508 (508.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 6  bytes 508 (508.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nTCP connection information at Container A\n( Container A is redis master, Container B is redis slave )\nss -tanp\n| State | Recv-Q | Send-Q | Local Address:Port | Peer Address:Port | Process |\n| --- | --- | --- | --- | --- | --- |\n| LISTEN | 0 | 128 | :6379 | : | users:((\"redis-server\",pid=15,fd=5)) |\n| ESTAB | 0 | 0 | 172.16.30.2:6379 | 172.16.83.0:53216 | users:((\"redis-server\",pid=15,fd=6)) |\n| LISTEN | 0 | 128 | :::6379 | ::: | users:((\"redis-server\",pid=15,fd=4)) |\nMy problem is 172.16.83.0:53216. \nI expected 172.16.83.3:53216.\nCould anyone tell me why ip address is 172.16.83.0:53216?\n. I solved my problem.\nI think there is a problem of documentation about flannel and kubernetes.\nI would like to discuss that later.\n. ",
    "oussama1983": "Could you also share your solution?\n. ",
    "ganting": "@euclid-geometry could you share your solution? we suffer with the same problem, thank you.\n. ",
    "libn-net": "@euclid-geometry could you share your solution?  thank you.. ",
    "gembin": "Because the VM cannot access the Internet, so the images was pulled from local docker registry and re-tag to quay.io/coreos/flannel:0.5.5.  And the images was already exists, I don't understand why it pulls again, as a result it will fail to pull because it cannot access the Internet.\n. Yes, it works if i change the image points to local docker registry. But if image is already pulled locally, and then if the vm cannot access the local registry because of some network issues, it failed to work, complains about cannot find the image and will try to pull it again.\nSo, does it mean it will always try to access the docker registry to check if the images already pulled or not ?\n. ",
    "brianredbeard": "In the flanneld.service file there are two environment variables one may set:\nEnvironment=\"FLANNEL_VER=0.5.5\"\nEnvironment=\"FLANNEL_IMG=quay.io/coreos/flannel\"\nWhich are then consumed as:\nExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n  /usr/bin/docker run --net=host --privileged=true --rm \\\n  --volume=/run/flannel:/run/flannel \\\n  --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n  --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n  --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n  --env-file=${FLANNEL_ENV_FILE} \\\n  --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n  --volume=${ETCD_SSL_DIR}:${ETCD_SSL_DIR}:ro \\\n  ${FLANNEL_IMG}:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=true\nIf you fix those up with a systemd drop-in, everything should work just fine.\n. ",
    "teestone": "I met the same problem.\nJun 18 13:20:00 localhost sdnotify-proxy[1931]: See '/usr/bin/docker run --help'.\nJun 18 13:20:00 localhost systemd[1]: flanneld.service: Main process exited, code=exited, status=125/n/a\nJun 18 13:20:00 localhost systemd[1]: Failed to start Network fabric for containers.\nJun 18 13:20:00 localhost systemd[1]: Dependency failed for Docker Application Container Engine.\nJun 18 13:20:00 localhost systemd[1]: docker.service: Job docker.service/start failed with result 'dependency'.\nJun 18 13:20:00 localhost systemd[1]: flanneld.service: Unit entered failed state.\nJun 18 13:20:00 localhost systemd[1]: flanneld.service: Failed with result 'exit-code'.\nJun 18 13:20:05 localhost systemd[1]: flanneld.service: Service hold-off time over, scheduling restart.\nJun 18 13:20:05 localhost systemd[1]: Stopped Network fabric for containers.\nJun 18 13:20:05 localhost systemd[1]: Starting Network fabric for containers...\nJun 18 13:20:06 localhost dockerd[1374]: time=\"2016-06-18T13:20:06.541049401Z\" level=error msg=\"Handler for POST /v1.22/containers/create returned error: No such image: quay.io/coreos/flannel:0.5.5\"\nJun 18 13:20:06 localhost sdnotify-proxy[1969]: Unable to find image 'quay.io/coreos/flannel:0.5.5' locally\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: 0.5.5: Pulling from coreos/flannel\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: 7bfac8493465: Pulling fs layer\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: a3ed95caeb02: Pulling fs layer\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: fdaeea203ca1: Pulling fs layer\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: 1f0ee8606937: Pulling fs layer\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: a3ed95caeb02: Download complete\nJun 18 13:20:17 localhost sdnotify-proxy[1969]: 1f0ee8606937: Download complete\nJun 18 13:21:36 localhost systemd[1]: flanneld.service: Start operation timed out. Terminating.\nJun 18 13:21:36 localhost systemd[1]: flanneld.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\nJun 18 13:21:36 localhost systemd[1]: Failed to start Network fabric for containers.\nJun 18 13:21:36 localhost systemd[1]: Dependency failed for Docker Application Container Engine.\nJun 18 13:21:36 localhost systemd[1]: docker.service: Job docker.service/start failed with result 'dependency'.\nJun 18 13:21:36 localhost systemd[1]: flanneld.service: Unit entered failed state.\nJun 18 13:21:36 localhost systemd[1]: flanneld.service: Failed with result 'exit-code'.\nJun 18 13:21:41 localhost systemd[1]: flanneld.service: Service hold-off time over, scheduling restart.\nJun 18 13:21:41 localhost systemd[1]: Stopped Network fabric for containers.\nJun 18 13:21:41 localhost systemd[1]: Starting Network fabric for containers...\nJun 18 13:21:42 localhost dockerd[1374]: time=\"2016-06-18T13:21:42.076656400Z\" level=error msg=\"Handler for POST /v1.22/containers/create returned error: No such image: quay.io/coreos/flannel:0.5.5\"\nJun 18 13:21:42 localhost sdnotify-proxy[2016]: Unable to find image 'quay.io/coreos/flannel:0.5.5' locally\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: 0.5.5: Pulling from coreos/flannel\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: 7bfac8493465: Pulling fs layer\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: a3ed95caeb02: Pulling fs layer\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: fdaeea203ca1: Pulling fs layer\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: 1f0ee8606937: Pulling fs layer\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: a3ed95caeb02: Download complete\nJun 18 13:21:50 localhost sdnotify-proxy[2016]: 1f0ee8606937: Download complete\nRepeat and repeat again.\n. Found the root cause.\nflannel is pulled by early-docker service, not docker service.\n. ",
    "voa808": "teestone: I am facing a similar issue. Could you please help how did you surpass the error.\n. ",
    "xalex84": "Hi, we are also encountering the same issue.\nSince we don't have the problem on all the boxes, we are analysing  the issue in order to understand if there is a match for specific pods hosted by specific boxes.\nFlanneld v0.7.0\nEtcd2 2.3.7\nKubernetes v1.5.2\nAverage cpu load of the nodes is 25%\n. ",
    "stregatto": "Same problem.\nApr 06 06:47:10 node204 flanneld[2584]: E0406 06:47:10.852782    2584 device.go:222] Failed to receive from netlink: no buffer space available\nApr 06 06:51:44 node204 flanneld[2584]: E0406 06:51:44.384812    2584 device.go:222] Failed to receive from netlink: no buffer space available\nApr 06 06:56:01 node204 flanneld[2584]: E0406 06:56:01.500101    2584 device.go:222] Failed to receive from netlink: no buffer space available\nApr 06 07:04:31 node204 flanneld[2584]: E0406 07:04:31.445611    2584 device.go:222] Failed to receive from netlink: no buffer space available\nUbuntu 16.04 on baremetal x86_64\nFlanneld v0.6.2 and Flanneld v0.7.0\nIs not related to the pods, nodes without pods produce errors as nodes with pods do not produce error.\nNode without pod and with errors:\n%Cpu(s):  0.3 us,  0.8 sy,  0.0 ni, 98.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nNode without pod and without errors:\n%Cpu(s): 19.1 us,  2.0 sy,  0.0 ni, 77.2 id,  0.0 wa,  0.0 hi,  1.7 si,  0.0 st\nIt seems to be some (too small) default setup during the netlink call that permits the communication between the kernel and the userspace process \n. Only as info.\nThese parameters remove the log entry increasing the netlink buffer.\nnet.core.rmem_default = 524280\nnet.ipv4.udp_rmem_min = 10240. ",
    "zihaoyu": "Seeing the same thing #779 . Our cluster is about 250 to 300 nodes.. core@ip-10-72-148-29 ~ $ ifconfig flannel.1\nflannel.1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 8951\n        inet 10.6.171.0  netmask 255.255.255.255  broadcast 0.0.0.0\n        inet6 fe80::6c89:a7ff:febd:578f  prefixlen 64  scopeid 0x20<link>\n        ether 6e:89:a7:bd:57:8f  txqueuelen 0  (Ethernet)\n        RX packets 56700389  bytes 74290145890 (69.1 GiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 105960931  bytes 79748645904 (74.2 GiB)\n        TX errors 0  dropped 22871 overruns 0  carrier 0  collisions 0\nDropping packets is seen on minions in a ~ 270-node cluster.. Still repro on v0.7.0. See two kinds of messages:\n2017/07/13, 23:59:57.000 I0714 03:59:57.610107 01590 network.go:225] L3 miss: 10.4.30.10\n2017/07/13, 23:59:57.000 I0714 03:59:57.353027    1539 network.go:243] L3 miss but route for 10.6.52.22 not found. ",
    "andrejvanderzee": "This is what systemctl status flanneld on the host has to say:\nflanneld[6681]: I0304 08:35:52.757607 06681 vxlan.go:356] AddL3 succeeded\nflanneld[6681]: I0304 08:36:32.530947 06681 vxlan.go:340] Ignoring not a miss: 4e:25:39:d7:76:68, \nflanneld[6681]: I0304 08:36:38.066940 06681 vxlan.go:340] Ignoring not a miss: 4e:25:39:d7:76:68, 10.1.58.0\nflanneld[6681]: I0304 08:36:41.066954 06681 vxlan.go:340] Ignoring not a miss: 4e:25:39:d7:76:68, 10.1.58.0\nflanneld[6681]: I0304 08:36:42.066987 06681 vxlan.go:340] Ignoring not a miss: 4e:25:39:d7:76:68, 10.1.58.0\nflanneld[6681]: I0304 08:36:43.066908 06681 vxlan.go:340] Ignoring not a miss: 4e:25:39:d7:76:68, 10.1.58.0\nflanneld[6681]: I0304 08:36:44.140476 06681 vxlan.go:345] L3 miss: 10.1.58.0\nflanneld[6681]: I0304 08:36:44.140515 06681 device.go:187] calling NeighSet: 10.1.58.0, 4e:25:39:d7:76:68\nflanneld[6681]: I0304 08:36:44.140622 06681 vxlan.go:356] AddL3 succeeded\n. ",
    "vladar": "See the same on flannel 0.5.5 on Ubuntu 14.04 OpenStack VM\n. Have exactly the same issue with Flannel 0.5.5 on ubuntu14.04 and Kubernetes 1.3.4\n. ",
    "jkong85": "Is this problem fixed? . ",
    "asridharan": "I was using clang with ccache as the CC compiler and that seems to confuse the build scripts (they expect gcc).  Closing this issue. \n. ",
    "colemickens": "This should be fixed now (though I haven't explicitly tested).\n. ",
    "jipperinbham": "@glevand sorry about the whitespace cleanup, editor must have done something there, I'll get it cleaned up.\n. ",
    "hustcat": "Here is a test case. When I  create vxlan device with vishvananda/netlink, the port from kernel is not  the port I specified:\n``` go\npackage main\nimport (\n        \"fmt\"\n        \"github.com/vishvananda/netlink\"\n)\nfunc main() {\n        parent, err := netlink.LinkByName(\"eth0\")\n        if err != nil {\n                fmt.Printf(\"Get host interface error: %v\", err)\n                return\n        }\n    vxlan := &netlink.Vxlan{\n            LinkAttrs:    netlink.LinkAttrs{Name: \"vxlan1\"},\n            VxlanId:      258,\n            VtepDevIndex: parent.Attrs().Index,\n            Port:         8472,\n    }\n    err = netlink.LinkAdd(vxlan)\n    if err != nil {\n            fmt.Printf(\"add vxlan interface error: %v\", err)\n    }\n\n}\n```\n``` sh\ngo build vxlan.go\n./vxlan\nip -d link show vxlan1\n51: vxlan1:  mtu 1450 qdisc noop state DOWN mode DEFAULT \n    link/ether f6:56:51:cb:da:85 brd ff:ff:ff:ff:ff:ff promiscuity 0 \n    vxlan id 258 dev eth0 srcport 0 0 dstport 6177 nolearning ageing 300\nip link set vxlan1 up\nnetstat -ulnp\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name \nudp        0      0 0.0.0.0:6177            0.0.0.0:*                           -           \n```\nAs shown above, I want use 8472(0x2118), but result is 6177(0x1821).\n. @philips From here, kernel use big endian order for port:\nc\n    if (data[IFLA_VXLAN_PORT])\n        conf.dst_port = nla_get_be16(data[IFLA_VXLAN_PORT]);\nMaybe  vishvananda/netlink should convert it.\n. PR #421 to fix this bug\n. ",
    "tshprecher": "@steveeJ the difference is logging by package instead of file. It's been a while since I took a look at this, but I believe other coreOS projects use the PackageLogger. This PR is a matter of making the projects consistent.\n. Yeah, the godep dependency was one of the reasons why this PR has been sitting around for so long. Thanks for reaching out, but you can hand this off to someone else.\n. ",
    "xcompass": "Great. Looking forward for that to be merged.\n. ",
    "jonlangemak": "Any update on this one?  Im running into a bug where even when I specify a VXLAN port for flannel and it ends up being off each time.  Im thinking this could be related.\n. ",
    "aopha": "I encountered the same problem with two vms (create from opentack).\nI can ping docker0 each other between two vms,  but I can not ping any ip into container.\nWith tcpdump, I found flannel.1 could not get the arp reply.\n. @adisood81, I change the VM image from ubuntu 14.04 to Redhat 7, the Vxlan can work now.\n. ",
    "adisood81": "Hi Flannel Team\nAny updates on this one we are on a tight timeline and have not been able to make flanneld work with VXLAN only UDP works\nThanks\n. ",
    "huggsboson": "We hit this or a similar issue to this in our production cluster.  Nothing really jumps out as abnormal in the flannel logs.  A lot of AddL3 succeeded, Ignoring not a miss, calling NeighSet, L3 miss, related to the troublesome ip etc.  It isn't totally reproducible and happens sporadically.  The work load is such that the flannel ip's might be getting re-used as the servers containers are being taken down and put back up. I would love help trying to dig into the root cause.\nHere's our setup:\nWe have a kubernetes master running apiserver, etc.\nWe have a minion running a load balancer.\nAnother minion running a containerized java service.\nSometimes when we deploy the java service via kubernetes it will come up with a flannel ip and be pingable from the masters, but does not seem to be pingable from the loadbalancer node.  Restarting the flannel daemon on the loadbalancer node has fixed the problem, so does deleting the pod so it gets rescheduled sometimes. \nvlxland back\nLinux kernel: 3.10.0-327.18.2.el7.x86_64\nCentOS6 userspace with centos 7 kernel.  (don't ask, or do ask but be prepared for a tale).\nethtool -k flannel.xx\nFeatures for flannel.xx:\nrx-checksumming: on\ntx-checksumming: on\n    tx-checksum-ipv4: off [fixed]\n    tx-checksum-ip-generic: on\n    tx-checksum-ipv6: off [fixed]\n    tx-checksum-fcoe-crc: off [fixed]\n    tx-checksum-sctp: off [fixed]\nscatter-gather: on\n    tx-scatter-gather: on\n    tx-scatter-gather-fraglist: off [fixed]\ntcp-segmentation-offload: on\n    tx-tcp-segmentation: on\n    tx-tcp-ecn-segmentation: on\n    tx-tcp6-segmentation: on\nudp-fragmentation-offload: on\ngeneric-segmentation-offload: on\ngeneric-receive-offload: on\nlarge-receive-offload: off [fixed]\nrx-vlan-offload: off [fixed]\ntx-vlan-offload: on\nntuple-filters: off [fixed]\nreceive-hashing: off [fixed]\nhighdma: off [fixed]\nrx-vlan-filter: off [fixed]\nvlan-challenged: off [fixed]\ntx-lockless: on [fixed]\nnetns-local: off [fixed]\ntx-gso-robust: off [fixed]\ntx-fcoe-segmentation: off [fixed]\ntx-gre-segmentation: off [fixed]\ntx-ipip-segmentation: off [fixed]\ntx-sit-segmentation: off [fixed]\ntx-udp_tnl-segmentation: off [fixed]\ntx-mpls-segmentation: off [fixed]\nfcoe-mtu: off [fixed]\ntx-nocache-copy: off\nloopback: off [fixed]\nrx-fcs: off [fixed]\nrx-all: off [fixed]\ntx-vlan-stag-hw-insert: on\nrx-vlan-stag-hw-parse: off [fixed]\nrx-vlan-stag-filter: off [fixed]\nbusy-poll: off [fixed]\n. @djsly @tomdee Isn't the fix described in @aclisp's response?  you just do ip-masq=true on flannel and ip-masq=false on docker.  Is there a change to flannel that needs to be made?\n. ",
    "yytt5301": "I hit the same issue with Flannel 0.5.5(0.5.0, 0.5.4,0.6.2) on ubuntu14.04, use the Flannel backend \"udp\" is ok, and use the Flannel backend \"vxlan\" is fail.so, we can only use the \"udp\" now.. ",
    "noahzaozao": "Seems I have same issue with kube 1.5.1, Flannel 0.5.5. ",
    "iT2afL0rd": "This happens to me also.\n. @steveeJ  Sorry, it just the latest bug fix release.\n. ",
    "cpg1111": "It seems Flannel changes its subnet right before this happens, making each service's IPs invalid, I assume the same applies to the pods' IPs.  Anyone have insight as to what could cause this?\n. I have since upgraded to 1.2.X of Kubernetes and switched to the gce backend and I haven't seen this, but I was seeing this with 1.1.x of Kubernetes and the vxLan backend.  This might go away with the Kubernetes upgrade, but I'd be surprised, this seems a little more Flannel centric. I'd assume it could be seen with vxlan and 1.2.x of Kubernetes.\n. @mattwang123 Yeah reassigning this \"fixed\" it for me to, but it's kinda a \"duct tape\" fix if you will.  I would assume there's a reason this happens that could either be prevented or have things (maybe flannel, maybe kubernetes, I'm not sure where this responsibility would lie) adjust accordingly.\n. Sounds a good bit similar\n. ",
    "hermanjunge": "Got the same problem.\n. ",
    "Rastusik": "seems like I run into this as well\n. ",
    "rvadim": "Got the same problem. Cross node network not work due to docker load flannel_docker_opts.env, but in my case flunnel starts after docker, so restart docker fix this problem.\n```\n$ systemctl cat docker\n/usr/lib64/systemd/system/docker.service\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=http://docs.docker.com\nAfter=docker.socket early-docker.target network.target\nRequires=docker.socket early-docker.target\n[Service]\nEnvironmentFile=-/run/flannel_docker_opts.env\nExecStart=/usr/lib/coreos/dockerd daemon --host=fd:// $DOCKER_OPTS $DOCKER_OPT_BIP $DOCKER_OPT_MTU $DOCKER_OPT_IPMASQ\n[Install]\nWantedBy=multi-user.target\n```\nAdd below config to avoid problem in future (cloud-config or something)\n```\n/etc/systemd/system/docker.service.d/40-flannel.conf\n[Unit]\nRequires=flanneld.service\nAfter=flanneld.service\n```\nNow logs looks like this:\nMay 27 16:23:33 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:33.936393 00001 device.go:187] calling NeighSet: 10.244.13.3, 12:6b:73:a2:7f:98\nMay 27 16:23:33 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:33.936735 00001 vxlan.go:356] AddL3 succeeded\nMay 27 16:23:35 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:35.717548 00001 vxlan.go:340] Ignoring not a miss: 12:6b:73:a2:7f:98, 10.244.13.5\nMay 27 16:23:36 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:36.719527 00001 vxlan.go:340] Ignoring not a miss: 12:6b:73:a2:7f:98, 10.244.13.5\nMay 27 16:23:37 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:37.721512 00001 vxlan.go:340] Ignoring not a miss: 12:6b:73:a2:7f:98, 10.244.13.5\nMay 27 16:23:38 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:38.724654 00001 vxlan.go:345] L3 miss: 10.244.13.5\nMay 27 16:23:38 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:38.724697 00001 device.go:187] calling NeighSet: 10.244.13.5, 12:6b:73:a2:7f:98\nMay 27 16:23:38 k8s-master-1 sdnotify-proxy[31142]: I0527 16:23:38.724827 00001 vxlan.go:356] AddL3 succeeded\n. ",
    "tonyks": "L3 miss: <-Service's IP->\nRoute for <-Service's IP-> not found\nI have the same issue. I solved my problem as:\n$ etcdctl set /coreos.com/network/subnets/<-Service's IP->-24 '{\"PublicIP\":\"<-Service's IP->\",\"BackendType\":\"vxlan\",\"BackendData\":{\"VtepMAC\":\"<-Service's MAC->\"}}'\n. ",
    "1e3oss": "Hi, i have a similar issue with CoreOS 1010.5.0, flannel v0.5.5 in vxLan mode and Kubernetes 1.2.4.\nMy setup is 1 schedulable master and 2 nodes on virtualbox - installed with the \"bare metal\" guide.\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:340] Ignoring not a miss: <Mac:address>, <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:340] Ignoring not a miss: <Mac:address>, <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:340] Ignoring not a miss: <Mac:address>, <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:340] Ignoring not a miss: <Mac:address>, <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:248] Subnet removed: <Pod-Subnet>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 device.go:176] calling NeighDel: <k8s-master-IP>, <Mac:address>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:345] L3 miss: <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:349] Route for <Pod-IP> not found\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:345] L3 miss: <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:349] Route for <Pod-IP> not found\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:345] L3 miss: <Pod-IP>\n<Timestamp> <host> sdnotify-proxy[770]: <Timestamp> 00001 vxlan.go:349] Route for <Pod-IP> not found\n... looping ...\nMy cluster was running fine. Systemd docker service requires flannel, both did not show any error logs.\nThe issue appeared after some time. Subnets started getting deleted on all nodes: pods lost all connection outside the host node.\nThe machines had been rebooted a lot, but the issue did NOT appear after a reboot or a systemd restart - in which case a race could be considered.\nIt could be linked to heavy CPU and memory load. I experienced this with several kubernetes and coreos versions, and although I cannot give any evidence, it seems to me that it always happened on a over-loaded cluster. Maybe network load to be considered too ?\nAfter restarting flannel everything seems fine from the pods, but I still see a LOT of Ignoring not a miss: <Mac:address>, <Some-Pod-IP> in flanneld logs.\n. ",
    "jsoriano": "@eyakubovich is there any way to reduce log level to hide the Ignoring not a miss messages?\n. Thanks!. ",
    "tedzhang2891": "I encounter this problem. . ",
    "holygits": "I'm facing this problem but the IP in question is for an end point rather than a service\nflannel-wrapper[21821]: I0531 02:51:48.031256   21821 network.go:243] L3 miss but route for 10.2.77.2 not found\n. ",
    "garyyang85": "The issue can be reproduced on v0.6.2. @tomdee can we reopen this issue?   I use kargo to deploy a kubernetes env with flannel, the flannel subnet changed by some unknown reason, which is a serious issue. . ",
    "YEXINGZHE54": "the same to me, on v0.7.0. ",
    "JingYang0725": "I also met ,on 0.7.0-1.el7. ",
    "dev-e": "flannel: 0.7.1\ncoreos: 1409.7.0 stable\nk8s: 1.7.3 from hyperkube\nI had the same problem when I removed calico cni configs and left only flannel routing. Check your docker and flannel network configurations, they should have the same subnet.\n```\n~ $ ifconfig\ndocker0: flags=4099  mtu 1500\n        inet 10.64.209.1  netmask 255.255.255.0  broadcast 0.0.0.0\n        ether 02:42:f6:36:29:1c  txqueuelen 0  (Ethernet)\n        RX packets 0  bytes 0 (0.0 B)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 0  bytes 0 (0.0 B)\n        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\nflannel.1: flags=4163  mtu 1450\n        inet 10.64.209.0  netmask 255.255.255.255  broadcast 0.0.0.0\n        inet6 fe80::3c3c:eeff:fedd:2753  prefixlen 64  scopeid 0x20\n        ether 3e:3c:ee:dd:27:53  txqueuelen 0  (Ethernet)\n        RX packets 1156  bytes 848161 (828.2 KiB)\n        RX errors 0  dropped 0  overruns 0  frame 0\n        TX packets 1602  bytes 71988 (70.3 KiB)\n        TX errors 0  dropped 47 overruns 0  carrier 0  collisions 0\n```\nIf not so check flannel and Docker configs. Removing this configs and reloading daemons solved the problem in my case:\n```\n/etc/systemd/system/docker.service.d/40-flannel.conf\n[Unit]\nRequires=flanneld.service\nAfter=flanneld.service\n---REMOVE LINES BELOW\n[Service]\nEnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env\n---ALSO REMOVE THIS:\n/etc/kubernetes/cni/net.d/10-flannel.conf\n{\n    \"name\": \"podnet\",\n    \"type\": \"flannel\",\n    \"delegate\": {\n        \"isDefaultGateway\": true\n    }\n}\n```. ",
    "robdaemon": "If this PR is accepted, I'll make another PR that can register flannel as a Windows service.\n. The largest part of this is new code (to talk to the Windows API) - I'm not sure I understand how you'd like it broken up?\nEdit: I had considered moving the Windows API code itself into a separate library like netlink. Would that address your concerns?\n. @glevand Can I get some additional feedback on the changes you'd like to see in this PR? Thanks!\n. @glevand I can move the interaction with the iphlpapi to a separate project, but when I add it as a Godep, the effective size of this PR will remain the same. I'm struggling to understand how I can make this a smaller PR without it being an incomplete implementation of host-gw networking on Windows. \n. @philips - Thanks so much for the review! I'll redo it into two commits and move the iphlpapi wrapper to a separate project.\nAs for maintenance, it would be myself and other members of my team at HPE that would definitely maintain it. Our use-case is a Kubernetes cluster where we have pods on the Kubernetes minions, and workloads that are technically part of the same namespace but run on a Windows VM. We don't have a need to port etcd over to Windows, since we have the Kubernetes cluster running alongside it at this time.\n. Closing this, this work was taken on by my colleagues at HPE.\n. This isn't a rename as much as a creating of a Route object to abstract between netlink and iphlpapi. I didn't want to create a copy/paste of the entire network.go file, so I created a small wrapper around the routes.\n. In Windows. :(\n. Yeah, I was weighing that. I was trying to avoid a copy/paste of this entire file, but the small differences between Windows and Linux had to leak in if I don't do that.\n. ",
    "clinta": "It looks like this issue only occurs when using the cli flags, using environment variables works fine.\n. ",
    "styxlab": "I experience the same problem.\nTracked it down to https://github.com/coreos/flannel/pull/327\nTo fix, I created a new flanneld drop-in and set the -ip-masq flag to false:\n- name: flanneld.service\n  drop-ins:\n    - name: 10-nomasq.conf\n      content: |\n        [Service]\n        ExecStart=            \n        ExecStart=/usr/libexec/sdnotify-proxy /run/flannel/sd.sock \\\n        /usr/bin/docker run --net=host --privileged=true --rm \\\n        --volume=/run/flannel:/run/flannel \\\n        --env=NOTIFY_SOCKET=/run/flannel/sd.sock \\\n        --env=AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n        --env=AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n        --env-file=${FLANNEL_ENV_FILE} \\\n        --volume=/usr/share/ca-certificates:/etc/ssl/certs:ro \\\n        --volume=${ETCD_SSL_DIR}:${ETCD_SSL_DIR}:ro \\\n        ${FLANNEL_IMG}:${FLANNEL_VER} /opt/bin/flanneld --ip-masq=false\nThat change fixed it for me, but I think the coreos team should look closer into this issue because the default setting is --ip-masq=true.\n. @philips flannel reads its configuration, allocated subnets and auxiliary data from etcd. How does using the Kubernetes subnet manager change that?. ",
    "fatduo": "+1 Any update?\nWhen a pod accept a socket connection from another, it use\n\nint conn = accept(server_sockfd, (struct sockaddr*)&client_addr, &length);\nprintf(\"IP: %s \\n\", inet_ntoa(client_addr.sin_addr));\n\nto get the client ip address.\nIt gets the flannel ip instead of the pod ip.\n. ",
    "aclisp": "Found the root cause.\nTo get the correct src IP, these options are critical!\n- flannel's -ip-masq=true\n- docker's -ip-masq=false\nIn other words, flannel should insert ACCEPT rule at the head of POSTROUTING chain:\n$ sudo iptables -t nat -S POSTROUTING\n-P POSTROUTING ACCEPT\n-A POSTROUTING -s 192.168.0.0/16 -d 192.168.0.0/16 -j ACCEPT\n<... other MASQUERADE rules omitted>\nIn my case (the wrong src IP scenario), the head of POSTROUTING chain is something like:\n-P POSTROUTING ACCEPT\n-A POSTROUTING -s 192.168.18.64/26 ! -o docker0 -j MASQUERADE  (set by docker masq=true)\nSo every packets destined to interface flannel0 was SNAT'ed  with flannel0's IP.\n. ",
    "djsly": "Any expectation to have this fixed in a near 0.6.X release ?\n. We went ahead and updated our init files and installer accordantly. It wasn't clear though if flannel was expecting to resolve the issue them self at one point by defaulting their config and altering the docker one. That's what I though this Issue was for. \nSylvain \n\nOn Sep 23, 2016, at 10:25 PM, huggsboson notifications@github.com wrote:\n@djsly @tomdee Isn't the fix described in @aclisp's response? you just do ip-masq=true on flannel and ip-masq=false on docker. Is there a change to flannel that needs to be made?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. it looks like the file is created upon service start\n\nExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker. /usr/lib/systemd/system/flannel.service on centOS . Im not sure about @gufranmmu but for me on centOS I don't have any issues. \nI suspect that the OP is running on a different distribution . Cloned Issues on the CentOS side...\nhttps://bugs.centos.org/view.php?id=12300\n. ",
    "mmkonrad": "@tomdee  Since I used https://github.com/coreos/flannel.git I assumed it would be version 0.5.5. \nIn fact there are two etcd-processes. I followed the instructions of the guide and started on both nodes an etcd process. They found each other and raft succeeded. etcdctl member list then produces the output:\n586dbf8b73b61d39: name=node1 peerURLs=http://10.43.116.137:2380 clientURLs=http://10.43.116.137:2379 isLeader=true\n752ee4714249b716: name=node2 peerURLs=http://10.43.116.155:2380 clientURLs=http://10.43.116.155:2379 isLeader=false\nWhat confuses me is the SubnetLen. I configured as network 10.43.116.192/27 and assumed that then the 27 bits would be used for the descritpion of the networks. But I guess that's me lacking knowledge about networking, subnets and address-spaces. The reason I chose this as a network was the fact, that all my nodes are within a vpc in a subnet, with the configuration: 10.43.116.128/26.\nNevertheless the node1 that starts flannel first has the following ifconfig output:\nflannel.1 Link encap:Ethernet  HWaddr 5a:3e:21:8c:91:28\n          inet addr:10.43.116.208  Bcast:0.0.0.0  Mask:255.255.255.224\n          inet6 addr: fe80::583e:21ff:fe8c:9128/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:8951  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:8 overruns:0 carrier:0\n          collisions:0 txqueuelen:0\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\nwith flannel output:\nI0512 08:08:31.286710 01728 main.go:120] Installing signal handlers\nI0512 08:08:31.286820 01728 manager.go:133] Determining IP address of default interface\nI0512 08:08:31.286992 01728 manager.go:163] Using 10.43.116.137 as external interface\nI0512 08:08:31.287019 01728 manager.go:164] Using 10.43.116.137 as external endpoint\nI0512 08:08:31.294691 01728 local_manager.go:179] Picking subnet in range 10.43.116.208 ... 10.43.116.208\nI0512 08:08:31.298252 01728 manager.go:246] Lease acquired: 10.43.116.208/28\nI0512 08:08:31.298424 01728 network.go:58] Watching for L3 misses\nI0512 08:08:31.298444 01728 network.go:66] Watching for new subnet leases\nUPDATE: I see flannel-config 10.43.116.192/27 matches with Mask:255.255.255.224..that's why node1 has picked the 10.43.116.208\nBut still I don't get it why I cannot join with node2..\n@tomdee what dump exactly do you want? a real dump of all keys/values within etcd or the logs?\n. any idea?\n. UPDATE:\nIn the meantime I am reading the docker cookbook which contains in the networking chapter (chapter 3) a sub-chapter (3.13) contibuted by Eugene Yakubovich who is (afaik) mainly responsible for flannel. He proposed the usage of the --iface=http://IP_ADDRESS and --ip-masq flags. I applied these flags and changed the setup as proposed to only the master node running etcd.\nSETUP:\n- 2 aws ec2 ubuntu-14.04 instances \n- both are in a vpc and a subnet (flannel config is chosen so it does not conflict with vpc/subnet)\n- both are behind a proxy (is set in the /etc/default/docker file)\n- each having docker 1.11\n- each having latest flannel version\n- master has etcd-2.3.3\non master:\n- stop docker\n- start etcd with following settings:\netcd -name node1 \\\n-listen-peer-urls http://0.0.0.0:2380   \\\n-listen-client-urls http://0.0.0.0:2379,http://127.0.0.1:4001   \\\n-initial-advertise-peer-urls http://10.43.116.137:2380 \\\n-initial-cluster node1=http://10.43.116.137:2380  \\\n-initial-cluster-state new  \\\n-initial-cluster-token etcd-cluster   \\\n-advertise-client-urls http://10.43.116.137:2379\n- set network config as: etcdctl set /coreos.com/network/config  '{\"Network\": \"10.43.116.192/26\", \"Backend\": { \"Type\": \"vxlan\"}}'\n- run flannel with: sudo ./bin/flanneld --iface=http://10.43.116.137 --ip-masq that produces following output:\nI0518 07:43:21.062926 01884 main.go:120] Installing signal handlers\nI0518 07:43:21.063175 01884 manager.go:163] Using 10.43.116.137 as external interface\nI0518 07:43:21.063237 01884 manager.go:164] Using 10.43.116.137 as external endpoint\nI0518 07:43:21.073439 01884 local_manager.go:179] Picking subnet in range 10.43.116.224 ... 10.43.116.224\nI0518 07:43:21.074995 01884 ipmasq.go:47] Adding iptables rule: -s 10.43.116.192/26 -d 10.43.116.192/26 -j ACCEPT\nI0518 07:43:21.078817 01884 ipmasq.go:47] Adding iptables rule: -s 10.43.116.192/26 ! -d 224.0.0.0/4 -j MASQUERADE\nI0518 07:43:21.083156 01884 ipmasq.go:47] Adding iptables rule: ! -s 10.43.116.192/26 -d 10.43.116.192/26 -j MASQUERADE\nI0518 07:43:21.085813 01884 manager.go:246] Lease acquired: 10.43.116.224/27\nI0518 07:43:21.085936 01884 network.go:58] Watching for L3 misses\nI0518 07:43:21.086011 01884 network.go:66] Watching for new subnet leases\n- run following commands for docker: \nservice docker stop\nsource /run/flannel/subnet.env\nsudo ifconfig docker0 ${FLANNEL_SUBNET}\nsudo docker daemon --bip=${FLANNEL_SUBNET} --mtu=${FLANNEL_MTU} &\nwhich produces following output:\nINFO[0000] New containerd process, pid: 1930\nWARN[0000] containerd: low RLIMIT_NOFILE changing to max  current=1024 max=4096\nINFO[0001] [graphdriver] using prior storage driver \"aufs\"\nINFO[0001] Graph migration to content-addressability took 0.00 seconds\nINFO[0001] Firewalld running: false\nWARN[0001] Your kernel does not support swap memory limit.\nWARN[0001] mountpoint for pids not found\nINFO[0001] Loading containers: start.\n...\nINFO[0001] Loading containers: done.\nINFO[0001] Daemon has completed initialization\nINFO[0001] Docker daemon                                 commit=5604cbe graphdriver=aufs version=1.11.1\nINFO[0001] API listen on /var/run/docker.sock\n...So far so good. Let's go to the second node..\non worker node:\n- also stop docker\n- NOT running etcd (since master already has it running..)\n- instead run flannel as recommended (pointing to master):\n  sudo ./bin/flanneld --etcd-endpoints=http://10.43.116.137:2379 --iface=10.43.116.155 --ip-masq\nwhich results again in the known output:\nI0518 07:49:38.420397 01805 main.go:120] Installing signal handlers\nI0518 07:49:38.420642 01805 manager.go:163] Using 10.43.116.155 as external interface\nI0518 07:49:38.420675 01805 manager.go:164] Using 10.43.116.155 as external endpoint\nI0518 07:49:38.430341 01805 local_manager.go:179] Picking subnet in range 10.43.116.224 ...10.43.116.224\nE0518 07:49:38.430367 01805 network.go:106] failed to register network: failed to acquire lease: out of subnets\nAny idea what's wrong here?\nCould the proxy settings be part of the problem? I don't think so. If that was the case docker wouldn't be able to pull images from the hub....\n. Also tried it with a 10.0.0.0/24 Subnet...with the exact same results\n. NOOOOOOOO......it worked...6 weeks of headache are more or less over.\nI would be interested in the source of the bug, but the problem is, that my active phase on this project will end really soon.\nNevertheless thank you very much for this hint.\nIn the meantime I/we managed to create a cluster with docker overlay network and dockerswarm...it helped a lot of understanding more of the processes within such a cluster and the setup...but of course our solution is not so sophisticated as a k8s solution.\nFor me this issue could be closed. Do you want to leave it open until the bug is solved?\n. ",
    "xidui": "@tomdee it is CentOS 7\n. @JoshuaAndrew \nI tried CentOS 7 on digital ocean, where the deploy succeeded.\nBut when I try CensOS 7 on linode, it failed and the output was as I described.\nSeems that it is not the operation system thing.\n. The kernel version I think is high enough:\n[root@kubernetes-slave-2 ~]#  uname -r \n4.5.3-x86_64-linode67\n. @tomdee \nI found the slave I succeeded start flanneld has a kernel version below 3.14. But with kernel 4.5.3, it failed? strange?\n```\n[root@kubernetes-slave ~]# uname -r\n3.10.0-327.10.1.el7.x86_64\n[root@kubernetes-slave ~]# systemctl status flanneld.service\n\u25cf flanneld.service - Flanneld overlay address etcd agent\n   Loaded: loaded (/usr/lib/systemd/system/flanneld.service; enabled; vendor preset: disabled)\n   Active: active (running) since Tue 2016-05-24 05:34:53 EDT; 21h ago\n  Process: 14536 ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker (code=exited, status=0/SUCCESS)\n Main PID: 14518 (flanneld)\n   Memory: 4.4M\n   CGroup: /system.slice/flanneld.service\n           \u2514\u250014518 /usr/bin/flanneld -etcd-endpoints=http://node-1-master:2379 -etcd-prefix=/cluster.local/network\nMay 25 03:02:02 kubernetes-slave flanneld[14518]: I0525 03:02:02.594997 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.0\nMay 25 03:02:03 kubernetes-slave flanneld[14518]: I0525 03:02:03.262913 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.0\nMay 25 03:02:04 kubernetes-slave flanneld[14518]: I0525 03:02:04.267140 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.0\nMay 25 03:02:05 kubernetes-slave flanneld[14518]: I0525 03:02:05.267023 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.0\nMay 25 03:02:10 kubernetes-slave flanneld[14518]: I0525 03:02:10.903122 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.3\nMay 25 03:02:20 kubernetes-slave flanneld[14518]: I0525 03:02:20.309652 14518 vxlan.go:345] L3 miss: 172.16.92.3\nMay 25 03:02:20 kubernetes-slave flanneld[14518]: I0525 03:02:20.309929 14518 device.go:187] calling NeighSet: 172.16.92.3, 7e:b0:d1:ef:54:38\nMay 25 03:02:20 kubernetes-slave flanneld[14518]: I0525 03:02:20.310253 14518 vxlan.go:356] AddL3 succeeded\nMay 25 03:02:38 kubernetes-slave flanneld[14518]: I0525 03:02:38.423138 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.0\nMay 25 03:02:47 kubernetes-slave flanneld[14518]: I0525 03:02:47.639118 14518 vxlan.go:340] Ignoring not a miss: 7e:b0:d1:ef:54:38, 172.16.92.3\n```\n. I found that in my machine, the following command may fail:\n[root@kubernetes-slave-2 ~]# ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth0 dstport 4789\nRTNETLINK answers: Operation not supported\nSeems that this is a kernel problem that the kernel does not support vxlan0.\nThe Operation not supported message came from here.\n. @JoshuaAndrew \nxidui@kubernetes-slave-2:~$ lsmod\nModule                  Size  Used by\nThe linode machine is empty.\n. I finally solved this issue by reinstall the kernel in linode machine.\nGuided by this link: https://www.linode.com/docs/tools-reference/custom-kernels-distros/run-a-distribution-supplied-kernel-with-kvm\n. Thanks all! You helped me a lot.\n. ",
    "JoshuaAndrew": "\n```\nfor {\n    vb.dev, err = newVXLANDevice(&devAttrs)\n    if err == nil {\n        break\n    } else {\n        log.Error(\"VXLAN init: \", err)\n        log.Info(\"Retrying in 1 second...\")\n    // wait 1 sec before retrying\n    time.Sleep(1 * time.Second)\n}\n\n}\n```\nnewVXLANDevice(&devAttrs)  return err\nyour operating system's  kernel  may  not support vxlan\n. @xidui\nif you kernel support vxlan you will find this: \nkube@newdevcaas-node-1:~$ lsmod |grep vxlan\nvxlan            37619 0 \nip_tunnel        23768 1 vxlan\nif you kernel do not support vxlan you will find this: \n[root@joshua ~]# lsmod |grep vxlan\n[root@joshua ~]#\nso you can add vxlan module into kernel with this:\n[root@joshua ~]# modprobe vxlan\n. ",
    "ghost": "I use udp mode is ok, but vxlan mode is fail:\nI0525 17:49:17.972309 13374 local_manager.go:179] Picking subnet in range 10.1.1.0 ... 10.1.255.0\nI0525 17:49:18.014720 13374 manager.go:246] Lease acquired: 10.1.37.0/24\nI0525 17:49:18.015534 13374 network.go:58] Watching for L3 misses\nI0525 17:49:18.015737 13374 network.go:66] Watching for new subnet leases\nI0525 17:55:08.039796 13374 network.go:153] Handling initial subnet events\nI0525 17:55:08.039914 13374 device.go:159] calling GetL2List() dev.link.Index: 45 \nI0525 17:55:08.040156 13374 device.go:164] calling NeighAdd: 192.168.2.110, fe:da:99:67:06:a7\nI0525 18:02:34.727390 13374 network.go:225] L3 miss: 10.1.42.2\nI0525 18:02:34.727728 13374 device.go:187] calling NeighSet: 10.1.42.2, fe:da:99:67:06:a7\nI0525 18:02:34.728465 13374 network.go:236] AddL3 succeeded\nI0525 18:03:09.456014 13374 network.go:220] Ignoring not a miss: fe:da:99:67:06:a7, 10.1.42.2\nwanghl@wanghl-vm:~/mygo/src/github.com/coreos/flannel/bin$ lsmod |grep vxlan\nvxlan                  37619  0 \nip_tunnel              23768  1 vxlan\nmy system: Linux wanghl-vm 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux (vmware)\n. I test vxlan on ubuntu 14.04(vmware) kernel 3.19 success\n. thanks for the help. Would it be possible to just put a comment in the kube-flannel.yml file saying this is ignored with a pointer to the correct doc page?  Everything I found via google said it worked this way, the troubleshooting page never came up in search.\n. I have used flannel-cni:v0.3.0 as initContainer to copy portmap (and flannel!) from the container to the host and ~~... portmapping is still not working, hostPort has no effect~~\nI was checking with the netstat -lptn command and it was not showing me open ports, I think because they are in the different network namespace. Accessing the host on the hostPort from the outside works as expected.. @klausenbusk after I discovered that hostPort is accessible, I have also used iptables to check the rules. Just didn't mention this in the previous comment.. > The 0.9.1 kube-flannel.yml is used\nYou mean this file? As you can clearly see, the portmapping is not enabled in the ConfigMap. Cf. https://github.com/coreos/flannel/blob/ce1f224dbd4f00f4abf6009bf412c31618d73921/Documentation/kube-flannel.yml#L55-L73. Oh\uff0cI found the right way, there need choose a different vxlan Identifier.\nas the previous network use 1, the new network need set VNI to a different number like 2.. ",
    "uschtwill": "Thanks @xidui. I had to install a generic kernel, because my hosting provider's custom kernel didn't like vxlan.\n. ",
    "hectorj2f": "I did some tests and it seems to behave correctly a deployment of multiple server/client hosts and multiple clients that depend of each server. However I'd like to know the answer of some of my questions in here.\n. Thanks @eyakubovich I moved forward. I am happily using the client/server model.\n. ",
    "emmanuel": "This sounds (potentially) very powerful, and I think I want it :). However, the APIs for ELB (the ones I've found, at least) are all based on registering EC2 instances with an ELB, not an arbitrary IP. I'm confused how to achieve the described config.\nIs there any chance that you can point me to something that would help explain how to setup an ELB to route directly to a Pod IP? . ",
    "rc-olx": "@emmanuel you are correct. You can't register an IP to an ELB, currently the supported targets have to be running EC2 instances. Even considering ECS you register an instance id with a port --https://docs.aws.amazon.com/cli/latest/reference/elbv2/register-targets.html. ",
    "xxyztwz": "same as #412\n. ",
    "qianzhangxa": "Great, thanks @glevand !\n. It works, thanks @adaiguoguo !!!. ",
    "sunflower9973": "Still an error:\ncore@coreos01vt ~ $ docker run -i -t google/golang /bin/bash -c \"go get github.com/coreos/flannel\"\ngithub.com/coreos/flannel/subnet\n../github.com/coreos/flannel/subnet/registry.go:79: not enough arguments in call to transport.NewTransport\n. ",
    "nwjeeper33": "I am using the same docker command above, and got same exact error. Can you help please?\nIs it related to etcd2 configuration, not installing CA on the box? Any feedback would be appreciated!!\n. I have also tried to build on Ubuntu machine, and couldn't find the \".build\" file. \nKindly help!\n. ",
    "migeorge": "@sunflower9973 @nwjeeper33 I had success building with the newer golang container:\ndocker run -i -t golang:1.6 /bin/bash -c \"go get github.com/coreos/flannel\"\nThe binary will be located at /go/bin/flannel\n. ",
    "kdx99": "Many thanks, with the version 1.6 it worked !\n(...\nStatus: Downloaded newer image for golang:1.6)\n. ",
    "kfox1111": "Yeah. I can see it getting complicated fast. For my current use case, I'm going to have one k8s system that has a lot of machines intended mostly for compute jobs, so will have a very small number of containers. ~4 usually. maybe as much as 16. And a very few nodes with maybe in the hundreds for support services. So to handle the support case, I have to allocate a large number of ips to computes that will never use them. It works as is, but seems very wasteful.\n. Not sure this really is the fix?\nI have a cluster with some vm's in a cloud, and some bare metal nodes in the same flannel. the mtu on the physical hosts is different from the mtu on the vm's. So the overall flannel mtu should be the min(mtu) of all the nodes.\nI'm also looking closely at using the kube-api setting. we need a way to manually specify the mtu in that mode I think.. flannel is l3 on top of l2 inside l3 on top of l2 via vxlan... there isn't a problem with overlay l2 packets being received if the receiving end has its overlay mtu smaller then the sender? I would expect this to cause packet drops.\ncontainer -> l3 -> l2 -> vxlan  ------------ to different host ----------------- vxlan -> l2 (here) -> l3 -> container\nIt might work for tcp. but I don't think MSS will save you from that, nor is ip fragmentation always an option. (udp?)\nI still think it would be safer/more performant to allow overriding the overlay mtu?. nm. I finally figured out that selinux reenabled.. for some reason that one node isn't working. I did a:\nkubectl delete node k8s-test-2.novalocal\nthen rebooted the node to ensure it wasn't carrying any state in k8s so it would retry to register the node to see if it woudl get a new pod network configured to fix the issue. it didn't help.\ndoing a diff between a working node and the non working one shows:\n-Annotations:           node.alpha.kubernetes.io/ttl=0\n+                       kubernetes.io/hostname=k8s-test-4.novalocal\n+Annotations:           flannel.alpha.coreos.com/backend-data={\"VtepMAC\":\"e2:a9:8b:50:dc:eb\"}\n+                       flannel.alpha.coreos.com/backend-type=vxlan\n+                       flannel.alpha.coreos.com/kube-subnet-manager=true\n+                       flannel.alpha.coreos.com/public-ip=172.20.207.11\n+                       node.alpha.kubernetes.io/ttl=0\nflannel annotations are not being added to the broken node. I'm not sure if thats relevant or not.\nHow does flannel acquire an ip range when using the k8s manager?. yes. not all nodes didn't get their allocations, only some nodes. that really seems like a bug to me.. If some nodes get assigned a pod cidr but others not, is that a kubernetes issue or a kubeadm issue? I would think k8s itself maybe?. or does flannel ask for a pod cidr to be allocated by the api server?. ",
    "adiclepcea": "Yes @tomdee we are still working on Windows support. I have just rebased the other PR for windows.\n. ",
    "brendonmatheson": "I just hit this too - switching to the code as at the latest release (0.5.5) worked for me: \ngit checkout cb8284fb60737793596dd2fc98d9608d3d0d66f0\ndocker run -v pwd:/opt/flannel -i -t google/golang /bin/bash -c \"cd /opt/flannel && ./build\"\n. ",
    "lucab": "Hey @rbucker, it looks you are trying to retrofit rkt-builder as a generic build environment. I'm not sure if there is a bit of misunderstanding here, at that aci is meant to \"build rkt\" and not \"build via rkt\"; as such, it is custom tailored to rkt needs (which may be the same as flannel at the moment, just by chance).\nI would not recommend going this way, as it requires some ugly hacks and we can easily find ourself entangled in diverging requirements.\nOn the other hand, I have no idea about how flannel is currently built (there are multiple reference to a missing ./build script) and what needs to be changed, but I guess there is some margin for improvements.\n. ",
    "rbucker": "Since I'm not a regular user or DEV on the project I do not have a strong\nopinion one way or the other. I can offer that since I use ChromeOS devices\nfor my UI and SSH into my CoreOS boxes for development having a container\nbased build system gets me one step closer to idempotent CI/CD. While I\nprefer rkt to docker I think having both types of build agents would be\nideal. As for using rkt-builder... that is what was recommended when I\nprovided the same changes to acbuild and docker2aci. It's possible that\nthere is a better image implementation for flannel, however, I'm not the\none to build it. Maybe someone with CoreOS knowledge could float a common\nbuild image upstream.\n*--Richard Bucker *\nhttp://about.me/richard.bucker http://about.me/richard.bucker\nOn Tue, Jul 19, 2016 at 11:38 PM, Luca Bruno notifications@github.com\nwrote:\n\nHey @rbucker https://github.com/rbucker, it looks you are trying to\nretrofit rkt-builder as a generic build environment. I'm not sure if there\nis a bit of misunderstanding here, at that aci is meant to \"build rkt\" and\nnot \"build via rkt\"; as such, it is custom tailored to rkt needs (which may\nbe the same as flannel at the moment, just by chance).\nI would not recommend going this way, as it requires some ugly hacks and\nwe can easily find ourself entangled in diverging requirements.\nOn the other hand, I have no idea about how flannel is currently built\n(there are multiple reference to a missing ./build script) and what needs\nto be changed, but I guess there is some margin for improvements.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/pull/466#issuecomment-233830284, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA13o0R3B07-Ex8q63jKy7JqdqJB7Ermks5qXZhEgaJpZM4JP_aT\n.\n. please elaborate, the two volume params seem to be a part if CoreOS' rkt-builder environment,\n. On Tue, Jul 19, 2016 at 5:50 PM, Stefan Junker notifications@github.com\nwrote:\nhe --interactive argument is not needed here, as the build process will\nnot require human interaction. I hope I'm not wrong with the latter ;-)\n\nprobably correct. I'm indifferent. This is what was presented to me as how\nrkt was built withing rkt. I did not question it; just passed it along\n*--Richard Bucker *\nhttp://about.me/richard.bucker http://about.me/richard.bucker\n. ",
    "cizixs": "Any comment?\n. Found the documentation for this:\n\nThe flannel kube subnet manager relies on the fact that each node already has a podCIDR defined.\nIf your nodes do not have a podCIDR, then either use the --pod-cidr kubelet command-line option or the --allocate-node-cidrs=true --cluster-cidr=<cidr> controller-manager command-line options.\nIf kubeadm is being used then pass --pod-network-cidr=10.244.0.0/16 to kubeadm init which will ensure that all nodes are automatically assigned a podCIDR.\n\nsource: https://coreos.com/flannel/docs/latest/troubleshooting.html\n. Need this! Before the official documentation is out, is there any link/doc I can read to set this thing up?. Has this issues too, but it does not happen every 24 hour, but occasionally.\nThe headache is there is no easy way to reproduce, and the issue does not have enough information/logs to tell me why it is behaving like this.. @Agrosis So you manually set TTL for all subnets to 0?. @tomdee  What is the purpose of TTL of subnets? \nIn my opinion, flannel will renew the lease 1 hour before expiration by default. In the worst case, flannel will reallocate a subnet if it stopped for one hour and restarted.\nDoes it make more sense if flannel  periodically update TTL? \nOr if I'm not using TTL, can I turn it off with a parameter instead of setting etcd subnets key TTL to 0 by hand?. ",
    "keontang": "Yes, there is another guys touched my network. Now I used 192.168.76.1, it works perfect. Thank you!\n. ",
    "dghubble": "This is undocumented, but the default behavior is to parse the interface of the default route https://github.com/coreos/flannel/blob/master/network/manager.go#L134, which is suitable for our use cases. I'll remove my workaround ip route show 0.0.0.0/0 | cut -f5 -d ' ' which scripts the variable setting.\n. Yeah, my first workaround was to get the interface name and pass it that way. Then I noticed the default that is chosen and that it wasn't needed. I don't have a need for the feature after all, esp. if you'd consider  it unorthodox. Closing.\n. ",
    "DanHoerst": "This was a CoreOS issue. upgrading to 1109.1.0 fixed it\n. ",
    "antoineco": "Flannel 0.6.0 is now released. Would it be possible to push it to quay.io before closing this issue?\n. I guess this was just to get people covered in case the --cluster-cidr kube-controller-manager flag is not set.\n. ",
    "rojingeorge": "If set is given in one line, it works, and flannel starts, but then my system goes done, unable to connect\n./etcdctl set /coreos.com/network/config '{\"Network\": \"10.0.0.0/8\", \"SubnetLen\": 20,\"SubnetMin\": \"10.10.0.0\",\"SubnetMax\": \"10.99.0.0\",\"Backend\": {\"Type\": \"udp\",\"Port\": 7890}}'\nBut then my system went down, unable to connect\n. I will give some more info.\nI have two linux VMs, installed with etcd clusters, both running. As soon as I run flanneld on one of the machine, that machine crashes.\nFirst VM which crashed\n\nSecond VM\n\n. @eyakubovich - No...I just started my flanneld after changing to vxlan from UDP backend which was working. I am not pinging any IPs. The subnet is not getting added to the second machine. Do we need to do any other changes after from changing the backend, for vxlan to work. \nOS Details\nrojin@BLR1000009684:~/etcd-v3.0.4-linux-amd64$ cat /etc/os-release \nNAME=\"Ubuntu\"\nVERSION=\"14.04, Trusty Tahr\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 14.04 LTS\"\nVERSION_ID=\"14.04\"\n. ",
    "yuxiangning": "Got error like this, seems like it doesn't find pthread and libc, while apparently those are installed on my system. Am I missing anything?\nmake\ngo build -o dist/flanneld \\\n  -ldflags \"-extldflags -static -X github.com/coreos/flannel/version.Version=v0.5.3-107-g982dbdb\"\ngithub.com/coreos/flannel\n/usr/lib/golang/pkg/tool/linux_amd64/link: running gcc failed: exit status 1\n/usr/bin/ld: cannot find -lpthread\n/usr/bin/ld: cannot find -lc\ncollect2: error: ld returned 1 exit status\nmake: *** [dist/flanneld] Error 2\n. ",
    "smugcloud": "My apologies for the false alarm on this; it was actually an inadvertent firewall restricting traffic.\n. ",
    "aaronlevy": "Only a few minor comments. I'll start testing this out using the documented objects.\nPing @steveeJ @tomdee @eyakubovich -- as maintainers mind taking a pass on this?\n. FWIW we very likely will backport the required patches so that this will work -- so a version check could block us from using this even though it would work. Wouldn't be opposed to a log message warning though.\nping @tomdee @eyakubovich @steveeJ -- any chance we can get a review?\n. @luxas per release notes (https://github.com/coreos/flannel/releases/tag/v0.6.0) the release images were broken in 0.6.0. If you are going to build this out of band, maybe use 0.6.1 /cc @tomdee \n. Without the sleeps will the watch functions just be called repeatedly? If so, maybe an option is to add handler functions to the informer which just throw events on a channel, which we can pull/block from in the watch functions?\n. An option for removing the cni config write might just be a side car container with something like:\ncontainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel\n        command: [ \"/bin/bash\", \"-c\", \"cp\", \"/etc/kube-flannel/cni-conf.json\", \"/etc/cni/net.d/10-flannel.conf\", \"while\", \"true;\", \"sleep 3600;\", \"done\" ]\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      - name: kube-flannel\n        image: quay.io/coreos/flannel\n        command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n. To add some more details.\nI ran an audit script while broken / after fixed (restarting kube-flannel pod on worker node):\n```\n!/bin/bash\nCMD=\"ip link show && ip addr && ip route && arp -an && bridge fdb show && ip route show table local && sudo iptables-save\"\nfor SERVER in w1 c1; do echo \"Server: $SERVER\"; vagrant ssh $SERVER -- \"${CMD}\"; done\n```\nThen diff-ing the output I saw that:\nBroken ip route showed a flannel.1 route with scope link and src 10.2.1.0:\n10.2.0.0/16 dev flannel.1  proto kernel  scope link  src 10.2.1.0\nWorking ip route showed a flannel.1 with a global scope and no src:\n10.2.0.0/16 dev flannel.1\nBroken ip route list table local has two extra broadcast entries:\nbroadcast 10.2.0.0 dev flannel.1  proto kernel  scope link  src 10.2.1.0\nbroadcast 10.2.255.255 dev flannel.1  proto kernel  scope link  src 10.2.1.0\nSo a manual resolution that worked. \nOn worker machine:\nsudo ip route del 10.2.0.0/16 dev flannel.1  proto kernel  scope link  src 10.2.1.0\nsudo ip route add 10.2.0.0/16 dev flannel.1 scope global\nsudo ip route del table local broadcast 10.2.0.0 dev flannel.1  proto kernel  scope link  src 10.2.1.0\nsudo ip route del table local broadcast 10.2.255.255 dev flannel.1  proto kernel  scope link  src 10.2.1.0\n. Related: https://github.com/coreos/flannel/issues/535\nThe proposed change in that issue is to force flannel to skip to the next subnet - but I don't think that is a viable option, as we would have all nodes essentially off-by-one from what kubernetes thinks is the subnet assigned to the node.\nI'll try and make a change to stomp on the route even if it already exists, and see how well that works.\nAnother suggestion from @philips is to make sure the cni* interfaces are ignored by networkd: https://github.com/coreos/coreos-overlay/blob/22746580d4ac75ddbbd0a1330c98eb9273d0d699/app-admin/flannel/files/50-flannel.network. Gave this patch a try (remove all routes, then explicitly re-add) but does not seem to help:\ndiff --git a/backend/vxlan/device.go b/backend/vxlan/device.go\nindex a8c6a92..17c18cf 100644\n--- a/backend/vxlan/device.go\n+++ b/backend/vxlan/device.go\n@@ -135,7 +135,18 @@ func (dev *vxlanDevice) Configure(ipn ip.IP4Net) error {\n                Scope:     netlink.SCOPE_UNIVERSE,\n                Dst:       ipn.Network().ToIPNet(),\n        }\n-       if err := netlink.RouteAdd(&route); err != nil && err != syscall.EEXIST {\n+\n+       existingRoutes, err := netlink.RouteList(dev.link, netlink.FAMILY_ALL)\n+       if err != nil {\n+               return fmt.Errorf(\"failed to get route list for %s: %v\", dev.link.Attrs().Name, err)\n+       }\n+\n+       for _, er := range existingRoutes {\n+               log.Infof(\"Removing route: %s\", er.String())\n+               netlink.RouteDel(&er)\n+       }\n+\n+       if err := netlink.RouteAdd(&route); err != nil {\n                return fmt.Errorf(\"failed to add route (%s -> %s): %v\", ipn.Network().String(), dev.link.Attrs().Name, err)\n        }\nThe resolution is still to, on the destination node, delete the local route table entry for:\nbroadcast 10.2.0.0 dev flannel.1  proto kernel  scope link  src 10.2.1.0\n. Awesome, thanks. I was just looking into copying RouteList but removing the \"ignore all but main table\" segment -- this looks much cleaner.\nMy naive understanding is that we shouldn't need to muck directly with the local route table - and I'm still not really understanding what about restarting the kube-flannel process would change this behavior. . With your patch it will work after bootstrap, but does not work after machine reboot. My one test was rebooting the worker machine. After that:\nc1 host -> c1 pod (works)\nc1 host -> w1 pod (does not work)\nw1 host -> c1 pod (does not work)\nw1 host -> w1 pod (works). One more random datapoint. I tried just changing the kubernetes code to skip first subnet range. After bootstrap everything works. If I reboot the worker, it has the same behavior as above.\ndiff --git a/vendor/k8s.io/kubernetes/pkg/controller/node/cidr_set.go b/vendor/k8s.io/kubernetes/pkg/controller/node/cidr_set.go\nindex c4d9aaa..d86ad3a 100644\n--- a/vendor/k8s.io/kubernetes/pkg/controller/node/cidr_set.go\n+++ b/vendor/k8s.io/kubernetes/pkg/controller/node/cidr_set.go\n@@ -45,6 +45,7 @@ func newCIDRSet(clusterCIDR *net.IPNet, subNetMaskSize int) *cidrSet {\n                clusterMaskSize: clusterMaskSize,\n                maxCIDRs:        maxCIDRs,\n                subNetMaskSize:  subNetMaskSize,\n+               nextCandidate:   1,\n        }\n }. Also fwiw the above was just meant as short-term test (would also need to carry patch in hyperkube releases if we wanted to go this route).. Dug deeper, and my hunch now is that after a reboot, the mac address for the vxlan device has changed. You can see that the mac address was updated in the kubernetes node annotations, but doesn't seem to be reflected in bridge fdb show and also the stale mac address gets added to the arp table on L2 misses.\nSo this makes me think flannel is caching the old mac address and never updating - but if you restart the flannel process on the node with the stale entries, everything starts working.\nMy hunch is the issue is here: https://github.com/coreos/flannel/blob/master/subnet/watch.go#L142\nWe are only comparing existing leases against the subnet - but not a changed mac address. We would likely need to extend this to also consider changes to the vxlan mac address (which should be in the lease attrs).\nHowever, I'm now wondering, if this is the issue -- how has this ever worked? The vxlan mac address will always change regardless of using the kube backend...\nAnyway -- enough digging for tonight, I'll try and look some more tomorrow.. I tested this using the existing coreos-kubernetes installation, and everything worked as expected (after reboots, etc). So it's not that we've magically missed this always being a problem.\nSo the mac address is actually checked, but in the handleInitialSubnetEvents (https://github.com/coreos/flannel/blob/master/backend/vxlan/network.go#L187) but that will only run once on start (which makes sense, the mac address should not change except if the process is restarted).\nWhat I'm thinking is happening is that our cache of the node objects is not yet populated, so the call to WatchLeases() is returning nothing, so we don't reconfigure after the initial check.\nI'm going to test a change where we block until the informer has succesfully synced from the apiserver and see if that helps.. Okay, the above was a red herring.\nI think I found the issue, but gonna have to test tomorrow.\nIn subnet/kube we always return a snapshot as the LeaseWatchResult:\nhttps://github.com/coreos/flannel/blob/master/subnet/kube/kube.go#L220\nIn subnet/local_manager we return anevent:\nhttps://github.com/coreos/flannel/blob/master/subnet/local_manager.go#L288-L291\nWhen returning a snapshot (subnet/kube), we call listwatcher.reset:\nhttps://github.com/coreos/flannel/blob/master/subnet/watch.go#L55\nAnd this determines if this is a \"new\" lease based on subnet comparison (not mac address):\nhttps://github.com/coreos/flannel/blob/master/subnet/watch.go#L79\nIn our case, it's never new because the subnet has not changed, and continues to use incorrect mac address.\nWhereas an event only has \"added/removed\" states, so no such check is done:\nhttps://github.com/coreos/flannel/blob/master/subnet/watch.go#L53\nhttps://github.com/coreos/flannel/blob/master/subnet/watch.go#L115-L120\nThis causes any event that is received to trigger an add/remove.\n. WIP fix for this:\nhttps://github.com/coreos/flannel/compare/master...aaronlevy:533?expand=1\nThere may be two separate issues at play here:\n- One is the issue of using the network address as a subnet assigned to a node (what @bison identified)\n- Another is that after reboot, other nodes have stale lease information.\nThe fix linked above is for the latter issue -- I was skipping the network address to just test this issue in isolation. I'll test both tomorrow.. @spacexnice this should have been resolved by: https://github.com/coreos/flannel/pull/576\nDoes that change resolve the issue for you?. closed by https://github.com/coreos/flannel/pull/576. @TamalSaha can you give a little more info into why you need this? I'm not very clear on what \"we need to set public_ip for flannel daemon\" means.\nIf this is running \"in-cluster\", which is the expected design, then you should be able to use the internal service account (api-server location as well assets to securely contact api-server).. Is there any reason you can't tell the api-server to advertise itself on the private ip (if that's the one you want to use?), then I would think it should work out of box with no changes:\non api-server:\n--advertise-address=<private-ip>\nin the flannel pod manifest command:\n--iface=<private-ip> or --public-ip=<private-ip>. Also, to address the concern about daemonsets, you can use the downward api in the pod template to extract the local node's podIP. An example of this is in bootkube:\nhttps://github.com/kubernetes-incubator/bootkube/blob/master/pkg/asset/internal/templates.go#L585\nhttps://github.com/kubernetes-incubator/bootkube/blob/master/pkg/asset/internal/templates.go#L597-L600. To be honest, without looking at the code I'm not entirely sure the full interaction between --public-ip and --iface -- maybe @tomdee has a bit more info.\nI guess if your interface has two IPs you could set both flags if the default route on that interface may not be the IP you want (e.g. --iface is defaulting to the wrong IP)? Just a guess (networking is not my strongpoint... PRs to this repo notwithstanding...)\n. @TamalSaha also yes, your understanding of the nodeIP -> podIP in the daemonset is correct. . In my tests, after rebooting a node it can take a few seconds for a network request to go through the first time, as the remote destination will still have stale arp table entries. After a few seconds it will see the L3 miss and update the table. As an optimization, we might be able to flush the cache when we see that the lease has changed (but I'd leave that to a follow up).. > Have you looked into the failing tests?\nThe failing test looks unrelated (invalid docker logs parameter). I'll try to verify locally.\n\nHow confident are you that it's safe to use the first subnet\n\nIt seems from my brief internet reading, it is valid to use (but for some legacy reasons some do not). However, for any cases where kubernetes is assigning the pod cidrs - this is going to be used (so for anywhere that an overlay tool is not being used: aws/gce/etc.)\nAdditionally, the change is just for an external lease assignment - the existing flannel functionality doesn't change. What does change, is blowing away all previous routes for the vxlan interface. As long as this is acceptable -- that would be the only change to existing functionality.\nMaybe someone with more network-chops sign off on the use of the network address / safety of removing all existing routes when creating interface?\n\nGiven there was some concern about this also being a problem for the UDP backend, do you think it's worth investigating/fixing that too?\n\nThere were actually two issues at play here, and I believe only one of them affected the UDP backend (using the network address) and this is also resolved (but, I haven't explicitly tested this yet). The other issue had to do with only determining a lease change if there was a change to the subnet -- which is true for UDP -- but for VXLAN the mac address of the vxlan device could also change.\nThe latter issue existed for vxlan anyway, but the window for it occurring outside the kubernetes implementation was very small: you had to have a failed watch from etcd, which triggers a reset of all leases and in that window a mac address of one of the leases would need to change. I can add a TODO about this if we want: https://github.com/coreos/flannel/blob/808b3061e1dc69fe08af6b36e75e3c155b9e91d7/subnet/watch.go#L79 -- really this check should be backend specific (e.g. what constitutes a lease change for a particular backend).\n. Going to close this PR in favor of https://github.com/coreos/flannel/pull/564 which just pulls out the subnet/kube changes. Discussion of the remaining issue should probably stay in https://github.com/coreos/flannel/issues/533 -- let me know if this should be re-opened instead.. I will do manual testing of this a bit later today, just wanted to get it up for review.\n/cc @tomdee \nRelated discussion: https://github.com/coreos/flannel/issues/533\nReplaces part of: https://github.com/coreos/flannel/pull/560. Prior to this merging I still need to do a manual test. Then I also want to test the renew lease logic (which is 24hrs right now, but I'll lower for testing).. Manual tests went fine.\nThe only thing I'm noticing is that after the mac address changes (reboot of machine or restart of flannel), the first request will take ~10seconds because we initially treat it as \"not a miss\" (eventually I'm assuming the arp table MAC entry is cleared and we then repopulate with correct info from the lease). Likely some optimizations we can make here like flushing the arp table entry on lease change (but I can open a separate issue for this).\n@philips I responded to your questions, but wondering if you would prefer changes (also this got \"hidden\", but still unaddressed: https://github.com/coreos/flannel/pull/564#discussion_r91019413\nIf all looks well, should be ready for merge.. In my testing it did not, however, resolve the pod routing issue.. /cc also @bison . Issue this PR was trying to address was resolved by: https://github.com/coreos/flannel/pull/576. @tomdee I did manual tests against the repo steps in https://github.com/coreos/flannel/issues/533 and all went well.\nOn 2 nodes tested:\nhost to pod in both directions\npod to pod in both directions\nRebooted each node and retried the above then rebooted the next node and tried again.\nAll worked!. This LGTM - but I'm also not a flannel maintainer.\n/cc @tomdee (I also know you were going to try and do an official release this week too). I was thinking we might just switch to using Patch() rather than Update(), which we should eventually do anyway. Then wait on changing vendor to client-go (also client-go v3 is still technically beta - so we might need to bump this again soon anyway)\nUsing PATCH was brought up in: https://github.com/coreos/flannel/issues/667#issuecomment-293150531 and an example https://github.com/kubernetes/kubernetes/blob/3b1d2343a84d3b8e3fd21554568e06b8594cdf26/pkg/controller/controller_utils.go#L956-L961. /cc @diegs. @diegs and/or I will try and start testing this and https://github.com/coreos/flannel/pull/677 today.. The changes themselves LGTM to me then.. I also agree that it is somewhat of a universal baseline that we've used in the past. Feel like this might need to be discussed maybe in mailing list? I don't have a good sense of how many users rely on this.. I believe we will need:\n{\n  \"name\": \"cbr0\",\n  \"type\": \"flannel\",\n  \"delegate\": {\n    \"isDefaultGateway\": true\n  }\n}\nref: https://github.com/containernetworking/cni/pull/50\n. Random question: When do you want to use versioned clientset package vs the internal clientset?\n. Given that this still requires an un-merged upstream change, what do you think about also trying to determine nodename using downward api namespace/pod name (then retrieving nodeName from pod object)?\n. Why re-retrieve node here? Minimize the chance that nodestore is stale?\n. nit: adding node.name to err might help in debug scenarios\n. Might need to replace this with place-holder. Or a follow up PR once there is a tagged (or built from hash) release image. Just making note, but prob up to maintainers on preference.\n. convention nit: s/10_flannel/10-flannel/\n. The ultimate source of truth for the network cidr is going to be the --cluster-cidr in the controller-manager. Eventually we should be using that configuration (as @mikedanese mentions, we can do this once we are sourcing that info from api based config).\nI think in the interim, I could go either way. Sourcing directly from a configMap makes it pretty straight-forward / easy to understand. Or, if we go with the annotations, I think we should just leave the whole config as raw JSON in a single annotation, because there are other options a user might want to configure (https://github.com/coreos/flannel#example-configuration-json).\n. Unlikely error, but should we verify this prior to updating the node object annotations? If my understanding is correct, once the annotations are updated it is effectively a successful lease from the perspective of other nodes.\n. It looks like we do the same check in nodeToLease so if the podCidr didn't actually parse, other nodes would ignore it.\n. Started testing this, and both \"watch lease\" and \"watch leases\" are spamming logs, and not particularly helpful (granted it's a v4 log -- so maybe just a matter of changing the default documented daemonset to be less verbose\n. > parse the two first bytes in `.spec.PodCIDR and make a /16 network of it? \nI don't think we can safely assume the cluster-cidr size just based on the podCidr.\n\nWhile I think the configmap is good, I also think that the one-line kubectl apply -f https://k8s.io/flannel-network.yaml is more important\n\nI agree ease of use should be a priority, but I'd be more or less fine with either configMap or annotation. \nI think I'd lean toward annotation (on daemonset) right now just for the sake of ease-of-deployment. But longer term we are going to need to source info from configMaps regardless (e.g. kcm --cluster-cidr should be source of truth for flannel network cidr). Also, \"configMap + component\" is going to be a really common deployment pattern -- so not opposed to following that right now.\nReally we're just talking about the backend options. In general these options should not be changed after deployment (not even sure why you would want to) -- but neither configMap nor annotations would protect against this.\n. Can you log error here? Otherwise api connectivity errors give generic \"node name not in pod spec\" error.\n. I think we should be able to drop this. Looks to be just init step - and we would only support the default network in this mode.\n. nit: sorry I may have missed this in my previous comment (which this would have addressed). But err will always be empty here.\n. https://github.com/kubernetes/kubernetes/pull/27880 landed, maybe switch this back to using nodeName? \n. This can be follow up / don't want to block on this, but: We may want to have this in retry loop. In my tests it's pretty common to see this crashloop a few times during bootstrap. Not a big deal because it recovers, but to an operator the pod restart count might signal something is wrong, when it's really not.\n. fwiw I believe @philips suggested cp then mv based on mv(rename) being atomic if moving from same device. In some cases /tmp may not be same device and kind of defeats purpose. Maybe:\ncp /etc/kube-flannel/cni-conf.json /cni/net.d/.tmp.10-flannel.conf\nmv /cni/net.d/.tmp.10-flannel.conf  /cni/net.d/10-flannel.conf`\n. Also, do we want to continually re-copy this forever (vs copy once, sleep forever)? \n. I believe we want to have a restartPolicy=true on the pod so I think we don't want this container to exit (otherwise it will be in a restart loop). But it does seem a little odd to keep copying repeatedly (or incorrect if it should never change at runtime).\ncp cni-conf.json tmp-10-flannel.conf\nmv tmp-10-flannel.conf 10-flannel.conf\nwhile true; do\n    sleep 3600\ndone\n. I wasn't around for the discussion, but we have been using 10.2.0.0/16 for default pod network for a while in the coreos-kubernetes docs/tools. I don't feel strongly about this though - as it's easily changed by the user.\n. Somewhat arbitrary. If this is not able to sync, the pod will fail and we'll see a restart count in kubectl output. Seemed that would at least make some debugging easier than waiting forever (e.g. in the case it cannot contact apiserver). 10 seconds seemed like a reasonable amount of time to wait on this.. ack, will add.. Somewhat arbitrary. This can block the informer when an add/update/delete event is triggered. In the case of the updateFunc, it will be triggered every resynPeriod (5 min) for every object watched by the informer. If you have 50 nodes then it will be called 50 times - thought having a little bit of a buffer might not hurt for those spikes.\nThat being said -- this isn't really based on any real data -- and an unbuffered chan could work just as well. Or if we started processing batched events it might make more sense to keep the buffer (but would need to look into batched events implementation)\nDon't have strong feelings on this as I don't have data to really back up one or the other.. really minor nit: may want this in the caller if want to keep setAddr4 generic. Or if this is the only use case (which I believe it is), may as well only require caller to provide anIP rather than IPNet.. What does providing the \"status\" subresource do/mean?. ah k. sgtm. ",
    "chancez": "So I was able to get glide working with the following changes applied to glide.yaml:\n```\ndiff --git a/glide.yaml b/glide.yaml\nindex de8f578..2f93117 100644\n--- a/glide.yaml\n+++ b/glide.yaml\n@@ -30,7 +30,7 @@ import:\n   - activation\n   - daemon\n - package: github.com/coreos/pkg\n-  version: v2 \n+  version: v2\n   subpackages:\n   - flagutil\n - package: github.com/golang/glog\n@@ -68,7 +68,9 @@ import:\n - package: google.golang.org/appengine\n   version: 6a436539be38c296a8075a871cc536686b458371\n - package: google.golang.org/cloud\n-  version: 872c736f496c2ba12786bedbb8325576bbdb33cf\n+  repo: https://github.com/GoogleCloudPlatform/gcloud-golang.git\n+  vcs: git\n+  version: eb47ba841d53d93506cfbfbc03927daf9cc48f88\n   subpackages:\n   - compute/metadata\n   - internal\n```\nAs mentioned in my comment above, the google cloud API import URL changed, and maybe the underlying repo changed too, but regardless, the import url changed from what everything expects, so we need to specify where to clone from so we can keep the import URL as google.golang.org/cloud.\nOnce I fixed that, glide up -v --skip-test took ~40 seconds (unsure of how much I had cached) using glide 0.12.1. Additionally, you might consider using https://github.com/sgotti/glide-vc/ to remove unused files/packages/dependencies from vendor.\n. Unsubscribing. LMK if you have issues w/ glide.\n. Use glide 0.12.x, it's faster/better. Also, what command are you running exactly?\nIf on glide 0.12.x+ it should be roughly:\nglide update --strip-vendor --skip-test\nAlso, can you separate out changing glide.yaml and glide.lock from the changes in vendor? It's impossible to see what changed in the manifest without checking out locally. (But I will so I can look).\n. Im getting this error which is new to me:\n[ERROR] Update failed for google.golang.org/cloud: The Remote does not match the VCS endpoint\nLooking at https://github.com/GoogleCloudPlatform/gcloud-golang makes me thing the dependency should be \ncloud.google.com/go instead in glide.yaml rather than google.golang.org/cloud\n. Changing that import let me run glide. So I'll try to update imports and see if it compiles.\n. Okay, so I had to do the following, since it seems like the repo for the google cloud libraries may have moved and, not only did they move, but the import URL changed. This will keep the existing import URL. (Glide can't figure it out due to the import URL changing i think)\ndiff --git a/glide.yaml b/glide.yaml\nindex de8f578..2f93117 100644\n--- a/glide.yaml\n+++ b/glide.yaml\n@@ -30,7 +30,7 @@ import:\n   - activation\n   - daemon\n - package: github.com/coreos/pkg\n-  version: v2 \n+  version: v2\n   subpackages:\n   - flagutil\n - package: github.com/golang/glog\n@@ -68,7 +68,9 @@ import:\n - package: google.golang.org/appengine\n   version: 6a436539be38c296a8075a871cc536686b458371\n - package: google.golang.org/cloud\n-  version: 872c736f496c2ba12786bedbb8325576bbdb33cf\n+  repo: https://github.com/GoogleCloudPlatform/gcloud-golang.git\n+  vcs: git\n+  version: eb47ba841d53d93506cfbfbc03927daf9cc48f88\n   subpackages:\n   - compute/metadata\n   - internal\nI think the version can probably still be 6a436539be38c296a8075a871cc536686b458371, but I set it to 872c736f496c2ba12786bedbb8325576bbdb33cf since thats what the k8s package is using.\n. ",
    "bh016088": "Any word on documentation?  I would like to use this and work through the kube api but even the help description of \"Contact the Kubernetes API for subnet assignement instead of etcd or flannel-server.\" doesn't tell me what to actually put as the argument for -kube-subnet-mgr.. ",
    "timchenxiaoyu": "why we need  kubernetes apiserver  backed?. ths. after  I restart docker ,this problem disapper. ok,I have found the problem ,k8s kubelet config change --pod-cidr. I see, \n192.168.5.0/24 dev docker0  proto kernel  scope link  src 192.168.5.1 \nthis route  will lead package to docker0 ,then to container. you are right, is my err.I want to ask another question,about vxlan outerIP ,what's the different between use vxlan vtep  and use host eth?. i  get same promblem. check you etcd data and loacl  env file , flanneld will get preview ip  when startup . I meet same problem ? no change etcd data . I0816 14:15:12.460636  997012 network.go:160] Lease renewed, new expiration: 2018-08-17 06:15:12.397323577 +0000 UTC\nI0817 13:15:12.424162  997012 network.go:160] Lease renewed, new expiration: 2018-08-18 05:15:12.409554615 +0000 UTC\nI0818 12:15:12.772691  997012 network.go:160] Lease renewed, new expiration: 2018-08-19 04:15:12.425758602 +0000 UTC\nI0819 11:15:12.680788  997012 network.go:160] Lease renewed, new expiration: 2018-08-20 03:15:12.439583798 +0000 UTC\nI0820 10:15:12.522387  997012 network.go:160] Lease renewed, new expiration: 2018-08-21 02:15:12.456519505 +0000 UTC\nI0821 09:45:48.418973  997012 network.go:160] Lease renewed, new expiration: 2018-08-22 01:45:50.425406892 +0000 UTC\nW0822 09:48:14.516109  997012 network.go:170] Lease has been revoked. not renew lease  lead this problem. not solve yet\uff0cbut we set etcd ttl = 0  to avoid lost data in etcd. I have run  chrony in my cluster. I meet same problem  k8s 1.11.5 + flannel  daemonset pod hostport not mapping  \ntwo DNAT to same pod \n-A DOCKER ! -i docker0 -p tcp -m tcp --dport 10101 -j DNAT --to-destination 10.251.90.31:10101\n-A KUBE-SEP-SVXEETBGLRGRAETZ -p tcp -m comment --comment \"cfglyb/cfglyb-cyzhglptqd:port1\" -m tcp -j DNAT --to-destination 10.251.90.31:8080. It maybe docker bug , remove docker container not recycle DNAT rule\n-A DOCKER ! -i docker0 -p tcp -m tcp --dport 10101 -j DNAT --to-destination 10.251.90.31:10101\n-A DOCKER ! -i docker0 -p tcp -m tcp --dport 10101 -j DNAT --to-destination 10.251.90.7:10101. I also get this error. http://techblog.d2-si.eu/2017/08/20/deep-dive-into-docker-overlay-networks-part-3.html\nstandard vxlan use dist mac ,not vtep mac. any error logs?. ",
    "vdavidoff": "If I am parsing this correctly, this only works if flannel is run inside kubernetes - is that right? Or will there be a mode where you can point it at a kube-apiserver rather than it discovering incluster config things?\nDerp, sorry, I see from the --help output that all the options are there to do this outside a pod.. ",
    "sskorgal": "+1\n. ",
    "stevedbashton": "Hi Jon,\nI'd be happy to test, as it fits a use case I have. \nCheers, steve \n. ",
    "MikeSpreitzer": "Are you suggesting that --persist-network would make the assignments of subnets to hosts persist forever, no timeouts?  I presume there is a need to handle the case of a host that leaves the system permanently.  How would that be handled?\n. Yeah, I just wasted some time on this too.  The documentation for how this works on kube should not be closeted in the troubleshooting section.. BTW, it is also unclear to the non-experts why flanneld has both --public-ip and --iface.  And, BTW, on my first read of the doc I got the idea I only need to specify --public-ip.  WRONG.  Specifying both works.\nIn a related vein, the exhibited log messages from the troubleshooter ---\nI0629 14:28:35.866987    5522 main.go:399] Using interface with name enp62s0u1u2 and address 172.24.17.174\nI0629 14:28:35.867000    5522 main.go:412] Using 10.10.10.10 as external address\n--- do not make it clear what purposes each address will be used for.  That first message could be parsed as \"Using (interface with name enp62s0u1u2) and (address 172.24.17.174)\".  If the address in that message is only there to help the reader identify the interface then the wording should be clearer.  For example: \"Using interface with name enp62s0u1u2, one of whose addresses is 172.24.17.174\".. I probably was led to thinking about --public-ip by the comment at https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md#annotations about an annotation for public IP.  I suppose that is also obsolete?. ",
    "chenzhiwei": "@MikeSpreitzer Users should manually remove the network from Etcd.\nAnd maybe a simple tool to scan and clean the network which will be run by a user manually. This tool is very similar to Docker registry to clean the unused layers.\n. @tomdee Thanks, I think I can close this issue now.. ",
    "alexei-led": "Output of systemctl cat flanneld\n```\n/usr/lib64/systemd/system/flanneld.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nAfter=etcd.service etcd2.service\nBefore=docker.service\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=\"TMPDIR=/var/tmp/\"\nEnvironment=\"FLANNEL_VER=0.5.5\"\nEnvironment=\"FLANNEL_IMG=quay.io/coreos/flannel\"\nEnvironment=\"ETCD_SSL_DIR=/etc/ssl/etcd\"\nEnvironmentFile=-/run/flannel/options.env\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStart=/usr/bin/rkt run --net=host \\\n   --stage1-path=/usr/lib/rkt/stage1-images/stage1-fly.aci \\\n   --insecure-options=image \\\n   --set-env=NOTIFY_SOCKET=/run/systemd/notify \\\n   --inherit-env=true \\\n   --volume runsystemd,kind=host,source=/run/systemd,readOnly=false \\\n   --volume runflannel,kind=host,source=/run/flannel,readOnly=false \\\n   --volume ssl,kind=host,source=${ETCD_SSL_DIR},readOnly=true \\\n   --volume certs,kind=host,source=/usr/share/ca-certificates,readOnly=true \\\n   --volume resolv,kind=host,source=/etc/resolv.conf,readOnly=true \\\n   --mount volume=runsystemd,target=/run/systemd \\\n   --mount volume=runflannel,target=/run/flannel \\\n   --mount volume=ssl,target=${ETCD_SSL_DIR} \\\n   --mount volume=certs,target=/etc/ssl/certs \\\n   --mount volume=resolv,target=/etc/resolv.conf \\\n   ${FLANNEL_IMG}:${FLANNEL_VER} \\\n   --exec /opt/bin/flanneld \\\n   -- --ip-masq=true\nUpdate docker options\nExecStartPost=/usr/bin/rkt run --net=host \\\n   --stage1-path=/usr/lib/rkt/stage1-images/stage1-fly.aci \\\n   --insecure-options=image \\\n   --volume runvol,kind=host,source=/run,readOnly=false \\\n   --mount volume=runvol,target=/run \\\n   ${FLANNEL_IMG}:${FLANNEL_VER} \\\n   --exec /opt/bin/mk-docker-opts.sh -- -d /run/flannel_docker_opts.env -i\nExecStopPost=/usr/bin/rkt gc --mark-only\n[Install]\nWantedBy=multi-user.target\n/etc/systemd/system/flanneld.service.d/50-network-config.conf\n[Unit]\nAfter=etcd2.service\nRequires=etcd2.service\n[Service]\nExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ \"Network\": \"192.168.0.0/16\", \"Backend\": {\"Type\": \"aws-vpc\"} }'\n``\n. It works just fine with CoreOS fromstablechannel; only with latestbetaproblem begins\n. How to upgrade to flannel 0.6.0? I'm on CoreOS stable channel and 1122 is current version there\n. I've also updated toflannel v0.6.1. It still fails to run. \nRunningjournalctl -e -u flanneld` prints following lines:\n```\nSep 14 09:49:35 ip-10-10-3-182.ec2.internal systemd[1]: Starting Network fabric for containers...\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal etcdctl[11224]: { \"Network\": \"192.168.0.0/16\", \"Backend\": {\"Type\": \"aws-vpc\"} }\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal rkt[11235]: image: using image from file /usr/lib/rkt/stage1-images/stage1-fly.aci\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal rkt[11235]: image: using image from local store for image name quay.io/coreos/flannel:v0.6.1\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal rkt[11235]: I0914 09:49:36.721169 11235 manager.go:99] Register: alloc\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal rkt[11235]: log: exiting because of error: log: cannot create log: open /var/tmp/flanneld.ip-10-10-3-182.root.log.INFO.20160914-094936.11235: no such file or directory\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal systemd[1]: flanneld.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal rkt[11265]: gc: moving pod \"4b6179e7-247c-47c9-88fd-4b544fe871c3\" to garbage\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal systemd[1]: Failed to start Network fabric for containers.\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal systemd[1]: flanneld.service: Unit entered failed state.\nSep 14 09:49:36 ip-10-10-3-182.ec2.internal systemd[1]: flanneld.service: Failed with result 'exit-code'.\nSep 14 09:49:41 ip-10-10-3-182.ec2.internal systemd[1]: flanneld.service: Service hold-off time over, scheduling restart.\nSep 14 09:49:41 ip-10-10-3-182.ec2.internal systemd[1]: Stopped Network fabric for containers.\n...\nthese lines are repeated in loop\n``\n. And here is myflannedservice with drop-ins (I've updatedflanneld` image in drop-in):\n```\n/usr/lib64/systemd/system/flanneld.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nAfter=etcd.service etcd2.service\nBefore=docker.service\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nEnvironment=\"TMPDIR=/var/tmp/\"\nEnvironment=\"FLANNEL_VER=0.5.5\"\nEnvironment=\"FLANNEL_IMG=quay.io/coreos/flannel\"\nEnvironment=\"ETCD_SSL_DIR=/etc/ssl/etcd\"\nEnvironmentFile=-/run/flannel/options.env\nLimitNOFILE=40000\nLimitNPROC=1048576\nExecStartPre=/sbin/modprobe ip_tables\nExecStartPre=/usr/bin/mkdir -p /run/flannel\nExecStartPre=/usr/bin/mkdir -p ${ETCD_SSL_DIR}\nExecStart=/usr/bin/rkt run --net=host \\\n   --stage1-path=/usr/lib/rkt/stage1-images/stage1-fly.aci \\\n   --insecure-options=image \\\n   --set-env=NOTIFY_SOCKET=/run/systemd/notify \\\n   --inherit-env=true \\\n   --volume runsystemd,kind=host,source=/run/systemd,readOnly=false \\\n   --volume runflannel,kind=host,source=/run/flannel,readOnly=false \\\n   --volume ssl,kind=host,source=${ETCD_SSL_DIR},readOnly=true \\\n   --volume certs,kind=host,source=/usr/share/ca-certificates,readOnly=true \\\n   --volume resolv,kind=host,source=/etc/resolv.conf,readOnly=true \\\n   --mount volume=runsystemd,target=/run/systemd \\\n   --mount volume=runflannel,target=/run/flannel \\\n   --mount volume=ssl,target=${ETCD_SSL_DIR} \\\n   --mount volume=certs,target=/etc/ssl/certs \\\n   --mount volume=resolv,target=/etc/resolv.conf \\\n   ${FLANNEL_IMG}:${FLANNEL_VER} \\\n   --exec /opt/bin/flanneld \\\n   -- --ip-masq=true\nUpdate docker options\nExecStartPost=/usr/bin/rkt run --net=host \\\n   --stage1-path=/usr/lib/rkt/stage1-images/stage1-fly.aci \\\n   --insecure-options=image \\\n   --volume runvol,kind=host,source=/run,readOnly=false \\\n   --mount volume=runvol,target=/run \\\n   ${FLANNEL_IMG}:${FLANNEL_VER} \\\n   --exec /opt/bin/mk-docker-opts.sh -- -d /run/flannel_docker_opts.env -i\nExecStopPost=/usr/bin/rkt gc --mark-only\n[Install]\nWantedBy=multi-user.target\n```\n```\n/etc/systemd/system/flanneld.service.d/50-network-config.conf\n[Unit]\nAfter=etcd2.service\nRequires=etcd2.service\n[Service]\nExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{ \"Network\": \"192.168.0.0/16\", \"Backend\": {\"Type\": \"aws-vpc\"} }'\n```\n```\n/etc/systemd/system/flanneld.service.d/99-fix-flannel.conf\n[Service]\nEnvironment=\"FLANNEL_VER=v0.6.1\"\n``\n. I've also modifiedTMPDIRto point totmp. Nowflanneld` does not complain on log file creation (while log is not created), but fails with other error:\nep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]: I0914 10:21:54.224063 20786 local_manager.go:134] Found lease (192.168.98.0/24) for current IP (10.10.3.182), reusing\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]: I0914 10:21:54.257426 20786 awsvpc.go:96] Warning- disabling source destination check failed: UnauthorizedOperation: You are not authorized to perform this operation.\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]:         status code: 403, request id:\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]: I0914 10:21:54.257470 20786 awsvpc.go:100] RouteTableID not passed as config parameter, detecting ...\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]: I0914 10:21:54.346821 20786 awsvpc.go:212] Subnet-ID: subnet-069adf70\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]: I0914 10:21:54.346854 20786 awsvpc.go:213] VPC-ID: vpc-47190123\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]: E0914 10:21:54.363720 20786 network.go:106] failed to register network: error describing routeTables for subnetID subnet-069adf70: UnauthorizedOperation: You are not authorized to perform this operation.\nSep 14 10:21:54 ip-10-10-3-182.ec2.internal rkt[20786]:         status code: 403, request id:\nWhy UnauthorizedOperation? I've gave all required permissions: created IAM role for EC2 instance with following policies:\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:CreateRoute\",\n                \"ec2:DeleteRoute\",\n                \"ec2:ReplaceRoute\"\n            ],\n            \"Resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:DescribeRouteTables\",\n                \"ec2:DescribeInstances\",\n                \"ec2:ModifyInstanceAttribute\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\nI suppose it should be enough, right?\n. And I  can run\naws ec2 describe-route-tables --filter \"Name=vpc-id,Values=vpc-47190123\" --filter \"Name=association.subnet-id,Values=subnet-069adf70\"\nfrom other EC2 instance with same IAM policy, getting a valid result back.\n. OK, seems we found the problem.\nIn CoreOS 1122 version (and may be earlier) flannel is running inside rkt container and not in a docker container as in previous version.  \nflannel uses golang AWS SDK to access AWS API. This SDK first checks if there are environment variables defined for AWS AWS_ACCESS_KEY and AWS_SECRET_KEY. If these variables are defined, then associated AWS identity is used and IAM policy assigned to EC2 instance are ignored.\nNow, when flannel was executed by Docker, these environment variables were ignored and AWS SDK used EC2 level defined IAM policy. rkt respect these variables and pass them to flannel and then EC2 policy is ignored.\nIt happens that our EC2 machines have both IAM policy defined and associated with role assigned to EC2 machine and also use AWS identity keys (as env variables) for a Docker volume management plugin (that require permissions for S3 but does not need access to Route API).\nNow we need to find a way to ignore these env variables for rkt and flannel and use them only for Docker volume plugin.\nQ: Is there a way to do so, i.e. ignore AWS_ACCESS_KEY and AWS_SECRET_KEY env variables in rkt?\n. ",
    "monder": "You can always replace or patch service files as specified in Override the whole unit file\n. @alexei-led how does these docker and rkt variables collide? Are these global exported for root user somehow? Or it is the same service file that require both docker s3 and flannel? \ud83c\udfb1\nThe best scenario is that every services has variables defined only for itself and not expose it to other services to avoid such artifacts. Why did the docker ignore these variables?\nI think you can just zero out envs like Environment=\"AWS_ACCESS_KEY=\"\n. It seems it's not due to the blackhole. Could you please check if your route has any vpc-endpoints and PR (#523)\nIt is kinda a rare case since vpc-endpoints are usually at the end of the array, and the function returns before them, but sometimes when you launch/terminate machines frequently you may end up with situation where the new lease you got (172.37.0.4) is actually a blackhole from the instance long time ago.\n. ",
    "elsobrino": "+1\n. ",
    "alexpsi": "+1\n. ",
    "wenerme": "Same error started by kube-deploy (I fix the version format.)\n/opt/bin/flanneld: error while loading shared libraries: libpthread.so.0: cannot open shared object file: No such file or directory\n/opt/bin/flanneld: error while loading shared libraries: libpthread.so.0: cannot open shared object file: No such file or directory\nRelate to this https://github.com/kubernetes/kube-deploy/pull/200\n. ",
    "zhangyongsu": "I think it is for lost build file in 6.x version . I add it and ok now .\n. ",
    "jellonek": "What about other tools mentioned in Dockerfile?\n. You written that \"To build flannel in a container run make dist/flanneld-amd64\" but tbh container could be build with make flannel-git which also requires other tools, dist/iptables-amd64 and dist/libpthread.so.0-amd64.\nThis seems to me a bit imprecise but still, much better than actual version of docs.\n. ",
    "jcderr": "Nevermind. I realize now that this is the etcdctl call to set the subnet, not the actual flanneld invocation.\n. ",
    "TaiSHiNet": "I was starting flannel with cloud-config, network setup and lease were acquired (I had container-to-container connectivity), yet that message kept popping up. \nAs I said, didn't have any negative effect on the networking functions but it was worrisome to see on the journal.\n. ",
    "gl328518397": "i am sorry ,i forget reply.  i have fix it ,thank you .\n. hello ,  -iface=\"eth0\"  just try\u3002\n. ",
    "AirLu": "HI @gl328518397 ,I met the same issue  , what was your solution?\n. ",
    "mlbiam": "@luxas i'm getting this same error on arm with the flannel yaml.  I tried bumping to quay.io/coreos/flannel:v0.6.2-arm but it doesn't like -kube-subnet-mgr\n. I tried rebuilding my cluster to make sure it was fresh.  Here's the logs I get from the flannel docker image with debugging enabled:\nI1007 10:32:30.975393       1 round_trippers.go:299] curl -k -v -XGET  -H \"User-Agent: flanneld/v1.4.0 (linux/arm) kubernetes/$Format\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLTM4MmgyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIxZmNiYzgwNC04Yzc4LTExZTYtOTBiOS1iODI3ZWI2NDUzNzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.PpEau6WesQfvLhQ560ioXsiYqmtE-oH9smCbMeX8kO9bi2M_nKlUxSmgbLCk1-jh0FhrpK1n_qDSTqdk1UGmNM8DB1CdJwRH1QaiVmwo9k0yn4tU-I10MB7jQHzLJAlKKCoFR0nwzxPTTdRdcHvW3sz_0h8GhLOF4tdWOv70qO_UfqGjDCQrHR04RScn-pa4R1tUiisnPpFkI3H0l5k2qJHpoWC4xzGA83H-flLzScHpIIdCN0o6armiSNplF17LOPvK6XW5Z6PSaZBGnI5-zb3GJMK7JIJHArE7Bs_EZqCoz-kKryUIpVeL8i84Y0XVcGK3rg-4ogaHPYYx1d70EA\" -H \"Accept: application/json, */*\" https://100.64.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-2rs5n\nI1007 10:32:31.157899       1 round_trippers.go:318] GET https://100.64.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-2rs5n 200 OK in 182 milliseconds\nI1007 10:32:31.158467       1 round_trippers.go:324] Response Headers:\nI1007 10:32:31.158591       1 round_trippers.go:327]     Content-Type: application/json\nI1007 10:32:31.158682       1 round_trippers.go:327]     Date: Fri, 07 Oct 2016 10:32:31 GMT\nI1007 10:32:31.159978       1 request.go:908] Response Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kube-flannel-ds-2rs5n\",\"generateName\":\"kube-flannel-ds-\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/pods/kube-flannel-ds-2rs5n\",\"uid\":\"5913a5b6-8c79-11e6-90b9-b827eb645374\",\"resourceVersion\":\"1882\",\"creationTimestamp\":\"2016-10-07T10:32:29Z\",\"labels\":{\"app\":\"flannel\",\"tier\":\"node\"},\"annotations\":{\"kubernetes.io/created-by\":\"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"DaemonSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"kube-flannel-ds\\\",\\\"uid\\\":\\\"590d2ce9-8c79-11e6-90b9-b827eb645374\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"1878\\\"}}\\n\"}},\"spec\":{\"volumes\":[{\"name\":\"run\",\"hostPath\":{\"path\":\"/run\"}},{\"name\":\"cni\",\"hostPath\":{\"path\":\"/etc/cni/net.d\"}},{\"name\":\"flannel-cfg\",\"configMap\":{\"name\":\"kube-flannel-cfg\",\"defaultMode\":420}},{\"name\":\"default-token-382h2\",\"secret\":{\"secretName\":\"default-token-382h2\",\"defaultMode\":420}}],\"containers\":[{\"name\":\"kube-flannel\",\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"command\":[\"/opt/bin/flanneld\",\"--ip-masq\",\"--kube-subnet-mgr\",\"--v=11\"],\"env\":[{\"name\":\"POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.name\"}}},{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}}],\"resources\":{},\"volumeMounts\":[{\"name\":\"run\",\"mountPath\":\"/run\"},{\"name\":\"flannel-cfg\",\"mountPath\":\"/etc/kube-flannel/\"},{\"name\":\"default-token-382h2\",\"readOnly\":true,\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}},{\"name\":\"install-cni\",\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"command\":[\"/bin/sh\",\"-c\",\"set -e -x; cp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; while true; do sleep 3600; done\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"cni\",\"mountPath\":\"/etc/cni/net.d\"},{\"name\":\"flannel-cfg\",\"mountPath\":\"/etc/kube-flannel/\"},{\"name\":\"default-token-382h2\",\"readOnly\":true,\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeSelector\":{\"beta.kubernetes.io/arch\":\"arm\"},\"serviceAccountName\":\"default\",\"serviceAccount\":\"default\",\"nodeName\":\"k8s-pi-master\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{\"phase\":\"Pending\",\"conditions\":[{\"type\":\"Initialized\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2016-10-07T10:32:29Z\"},{\"type\":\"Ready\",\"status\":\"False\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2016-10-07T10:32:29Z\",\"reason\":\"ContainersNotReady\",\"message\":\"containers with unready status: [kube-flannel install-cni]\"}],\"hostIP\":\"192.168.3.50\",\"podIP\":\"192.168.3.50\",\"startTime\":\"2016-10-07T10:32:29Z\",\"containerStatuses\":[{\"name\":\"install-cni\",\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}},\"lastState\":{},\"ready\":false,\"restartCount\":0,\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"imageID\":\"\"},{\"name\":\"kube-flannel\",\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}},\"lastState\":{},\"ready\":false,\"restartCount\":0,\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"imageID\":\"\"}]}}\nI1007 10:32:31.164071       1 main.go:132] Installing signal handlers\nI1007 10:32:31.164442       1 kube.go:233] starting kube subnet manager\nI1007 10:32:31.164773       1 reflector.go:211] Starting reflector *api.Node (5m0s) from github.com/coreos/flannel/subnet/kube/kube.go:234\nI1007 10:32:31.164883       1 manager.go:133] Determining IP address of default interface\nI1007 10:32:31.164889       1 reflector.go:249] Listing and watching *api.Node from github.com/coreos/flannel/subnet/kube/kube.go:234\nI1007 10:32:31.166174       1 manager.go:163] Using 192.168.3.50 as external interface\nI1007 10:32:31.166266       1 round_trippers.go:299] curl -k -v -XGET  -H \"User-Agent: flanneld/v1.4.0 (linux/arm) kubernetes/$Format\" -H \"Accept: application/json, */*\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLTM4MmgyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIxZmNiYzgwNC04Yzc4LTExZTYtOTBiOS1iODI3ZWI2NDUzNzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.PpEau6WesQfvLhQ560ioXsiYqmtE-oH9smCbMeX8kO9bi2M_nKlUxSmgbLCk1-jh0FhrpK1n_qDSTqdk1UGmNM8DB1CdJwRH1QaiVmwo9k0yn4tU-I10MB7jQHzLJAlKKCoFR0nwzxPTTdRdcHvW3sz_0h8GhLOF4tdWOv70qO_UfqGjDCQrHR04RScn-pa4R1tUiisnPpFkI3H0l5k2qJHpoWC4xzGA83H-flLzScHpIIdCN0o6armiSNplF17LOPvK6XW5Z6PSaZBGnI5-zb3GJMK7JIJHArE7Bs_EZqCoz-kKryUIpVeL8i84Y0XVcGK3rg-4ogaHPYYx1d70EA\" https://100.64.0.1:443/api/v1/nodes?resourceVersion=0\nI1007 10:32:31.166274       1 manager.go:164] Using 192.168.3.50 as external endpoint\nE1007 10:32:31.170991       1 network.go:106] failed to register network: operation not supported\nI1007 10:32:31.179923       1 round_trippers.go:318] GET https://100.64.0.1:443/api/v1/nodes?resourceVersion=0 200 OK in 13 milliseconds\nI1007 10:32:31.180030       1 round_trippers.go:324] Response Headers:\nI1007 10:32:31.180074       1 round_trippers.go:327]     Content-Type: application/json\nI1007 10:32:31.180111       1 round_trippers.go:327]     Date: Fri, 07 Oct 2016 10:32:31 GMT\nI1007 10:32:31.180603       1 request.go:908] Response Body: {\"kind\":\"NodeList\",\"apiVersion\":\"v1\",\"metadata\":{\"selfLink\":\"/api/v1/nodes\",\"resourceVersion\":\"1873\"},\"items\":[{\"metadata\":{\"name\":\"k8s-pi-master\",\"selfLink\":\"/api/v1/nodes/k8s-pi-master\",\"uid\":\"216836f7-8c78-11e6-90b9-b827eb645374\",\"resourceVersion\":\"1873\",\"creationTimestamp\":\"2016-10-07T10:23:46Z\",\"labels\":{\"beta.kubernetes.io/arch\":\"arm\",\"beta.kubernetes.io/os\":\"linux\",\"kubeadm.alpha.kubernetes.io/role\":\"master\",\"kubernetes.io/hostname\":\"k8s-pi-master\"},\"annotations\":{\"scheduler.alpha.kubernetes.io/taints\":\"[{\\\"key\\\":\\\"dedicated\\\",\\\"value\\\":\\\"master\\\",\\\"effect\\\":\\\"NoSchedule\\\"}]\",\"volumes.kubernetes.io/controller-managed-attach-detach\":\"true\"}},\"spec\":{\"externalID\":\"k8s-pi-master\"},\"status\":{\"capacity\":{\"alpha.kubernetes.io/nvidia-gpu\":\"0\",\"cpu\":\"4\",\"memory\":\"948012Ki\",\"pods\":\"110\"},\"allocatable\":{\"alpha.kubernetes.io/nvidia-gpu\":\"0\",\"cpu\":\"4\",\"memory\":\"948012Ki\",\"pods\":\"110\"},\"conditions\":[{\"type\":\"OutOfDisk\",\"status\":\"False\",\"lastHeartbeatTime\":\"2016-10-07T10:32:27Z\",\"lastTransitionTime\":\"2016-10-07T10:23:46Z\",\"reason\":\"KubeletHasSufficientDisk\",\"message\":\"kubelet has sufficient disk space available\"},{\"type\":\"MemoryPressure\",\"status\":\"False\",\"lastHeartbeatTime\":\"2016-10-07T10:32:27Z\",\"lastTransitionTime\":\"2016-10-07T10:23:46Z\",\"reason\":\"KubeletHasSufficientMemory\",\"message\":\"kubelet has sufficient memory available\"},{\"type\":\"DiskPressure\",\"status\":\"False\",\"lastHeartbeatTime\":\"2016-10-07T10:32:27Z\",\"lastTransitionTime\":\"2016-10-07T10:23:46Z\",\"reason\":\"KubeletHasNoDiskPressure\",\"message\":\"kubelet has no disk pressure\"},{\"type\":\"Ready\",\"status\":\"True\",\"lastHeartbeatTime\":\"2016-10-07T10:32:27Z\",\"lastTransitionTime\":\"2016-10-07T10:23:46Z\",\"reason\":\"KubeletReady\",\"message\":\"kubelet is posting ready status. WARNING: CPU hardcapping unsupported\"}],\"addresses\":[{\"type\":\"LegacyHostIP\",\"address\":\"192.168.3.50\"},{\"type\":\"InternalIP\",\"address\":\"192.168.3.50\"}],\"daemonEndpoints\":{\"kubeletEndpoint\":{\"Port\":10250}},\"nodeInfo\":{\"machineID\":\"f010896c4d7443088901ba263eceffee\",\"systemUUID\":\"f010896c4d7443088901ba263eceffee\",\"bootID\":\"112ec821-98e2-4938-a6d0-cfdb2b04ceb7\",\"kernelVersion\":\"4.1.19-v7+\",\"osImage\":\"Ubuntu 16.04 LTS\",\"containerRuntimeVersion\":\"docker://1.11.2\",\"kubeletVersion\":\"v1.4.0\",\"kubeProxyVersion\":\"v1.4.0\",\"operatingSystem\":\"linux\",\"architecture\":\"arm\"},\"images\":[{\"names\":[\"gcr.io/google_containers/kube-proxy-arm:v1.4.0\"],\"sizeBytes\":177088263},{\"names\":[\"gcr.io/google_containers/kube-discovery-arm:1.0\"],\"sizeBytes\":122116943},{\"names\":[\"gcr.io/google_containers/kube-apiserver-arm:v1.4.0\"],\"sizeBytes\":116704924},{\"names\":[\"\\u003cnone\\u003e:\\u003cnone\\u003e\",\"\\u003cnone\\u003e@\\u003cnone\\u003e\"],\"sizeBytes\":116704924},{\"names\":[\"gcr.io/google_containers/kube-controller-manager-arm:v1.4.0\"],\"sizeBytes\":106962874},{\"names\":[\"gcr.io/google_containers/kube-scheduler-arm:v1.4.0\"],\"sizeBytes\":63165102},{\"names\":[\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\"],\"sizeBytes\":59864639},{\"names\":[\"gcr.io/google_containers/etcd-arm:2.2.5\"],\"sizeBytes\":24905124},{\"names\":[\"armel/busybox:latest\"],\"sizeBytes\":3410796},{\"names\":[\"gcr.io/google_containers/pause-arm:3.0\"],\"sizeBytes\":506244}]}}]}\nI1007 10:32:31.184692       1 round_trippers.go:299] curl -k -v -XGET  -H \"Accept: application/json, */*\" -H \"User-Agent: flanneld/v1.4.0 (linux/arm) kubernetes/$Format\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLTM4MmgyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIxZmNiYzgwNC04Yzc4LTExZTYtOTBiOS1iODI3ZWI2NDUzNzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.PpEau6WesQfvLhQ560ioXsiYqmtE-oH9smCbMeX8kO9bi2M_nKlUxSmgbLCk1-jh0FhrpK1n_qDSTqdk1UGmNM8DB1CdJwRH1QaiVmwo9k0yn4tU-I10MB7jQHzLJAlKKCoFR0nwzxPTTdRdcHvW3sz_0h8GhLOF4tdWOv70qO_UfqGjDCQrHR04RScn-pa4R1tUiisnPpFkI3H0l5k2qJHpoWC4xzGA83H-flLzScHpIIdCN0o6armiSNplF17LOPvK6XW5Z6PSaZBGnI5-zb3GJMK7JIJHArE7Bs_EZqCoz-kKryUIpVeL8i84Y0XVcGK3rg-4ogaHPYYx1d70EA\" https://100.64.0.1:443/api/v1/watch/nodes?resourceVersion=1873&timeoutSeconds=481\nI1007 10:32:31.194832       1 round_trippers.go:318] GET https://100.64.0.1:443/api/v1/watch/nodes?resourceVersion=1873&timeoutSeconds=481 200 OK in 10 milliseconds\nI1007 10:32:31.197110       1 round_trippers.go:324] Response Headers:\nI1007 10:32:31.197222       1 round_trippers.go:327]     Content-Type: application/json\nI1007 10:32:31.197267       1 round_trippers.go:327]     Date: Fri, 07 Oct 2016 10:32:31 GMT\nE1007 10:32:32.180297       1 network.go:106] failed to register network: operation not supported\nE1007 10:32:33.185074       1 network.go:106] failed to register network: operation not supported\n. I actually did try udp with another error. Will try this afternoon to get\nyou the error\nOn Oct 8, 2016 10:51 AM, \"Lucas K\u00e4ldstr\u00f6m\" notifications@github.com wrote:\n\n@tomdee https://github.com/tomdee @mlbiam https://github.com/mlbiam\nCan you verify CONFIG_VXLAN is enabled when running\ncurl -sSL https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash\nI think the problem is that vxlan support isn't present\nAlso, try going down to udp and see if it works\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/512#issuecomment-252428662, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AH3fw10Nn1EJ-uGCQIl5zr9idGdnBlX9ks5qx64EgaJpZM4KDZME\n.\n. @luxas the script didn't seem to work:\n\nI changed the backend to udp and now am getting:\nroot@k8s-pi-master:~# curl -sSL https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash\nwarning: /proc/config.gz does not exist, searching other paths for kernel config ...\nerror: cannot find kernel config\n  try running this script again, specifying the kernel config:\n    CONFIG=/path/to/kernel/.config bash or bash /path/to/kernel/.config\nroot@k8s-pi-master:~# kubectl logs kube-flannel-ds-4f95s kube-flannel  --namespace=kube-system\nI1008 19:38:47.932625       1 round_trippers.go:299] curl -k -v -XGET  -H \"Accept: application/json, */*\" -H \"User-Agent: flanneld/v1.4.0 (linux/arm) kubernetes/$Format\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWdyOHUwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjMjZjZTE0Ni04YzkyLTExZTYtYmIxZC1iODI3ZWI2NDUzNzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.ZYNQMgq7rR0uQUf2Mo_pEkWAlbN0rF047-y_S_dXfdZlV76yfDRfd3xJoygVBb3HMXvtJVcM4P12h9G5uJ6oIWnJPQ_rd6SwUJtznkqwlPSyjKuZMPhe7COow3i0Kh7T1WQd_MffUm6MuxGMrp9SnnvQmt14n3C8uB3tq3yqQoRdoMjWzRe33iJKYhBXsiczeG14b82-LMn-qyMRuMVYFZ8aG3J-wGL70VBTbJWEZCbrQtC03aUzxJF2WEcpej4P9ck1oYtM9Ed3uxNI2FFlpa0vSfjIcV6hxdlZrcVRqMfae4_W2uPU737NUyPqEpcSmU8oj9Mj9X27EtvnuqaDBQ\" https://100.64.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-4f95s\nI1008 19:38:48.350511       1 round_trippers.go:318] GET https://100.64.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-4f95s 200 OK in 416 milliseconds\nI1008 19:38:48.350779       1 round_trippers.go:324] Response Headers:\nI1008 19:38:48.350820       1 round_trippers.go:327]     Content-Type: application/json\nI1008 19:38:48.351051       1 round_trippers.go:327]     Date: Sat, 08 Oct 2016 19:38:48 GMT\nI1008 19:38:48.353068       1 request.go:908] Response Body: {\"kind\":\"Pod\",\"apiVersion\":\"v1\",\"metadata\":{\"name\":\"kube-flannel-ds-4f95s\",\"generateName\":\"kube-flannel-ds-\",\"namespace\":\"kube-system\",\"selfLink\":\"/api/v1/namespaces/kube-system/pods/kube-flannel-ds-4f95s\",\"uid\":\"d37e6451-8d8e-11e6-bb1d-b827eb645374\",\"resourceVersion\":\"168311\",\"creationTimestamp\":\"2016-10-08T19:38:45Z\",\"labels\":{\"app\":\"flannel\",\"tier\":\"node\"},\"annotations\":{\"kubernetes.io/created-by\":\"{\\\"kind\\\":\\\"SerializedReference\\\",\\\"apiVersion\\\":\\\"v1\\\",\\\"reference\\\":{\\\"kind\\\":\\\"DaemonSet\\\",\\\"namespace\\\":\\\"kube-system\\\",\\\"name\\\":\\\"kube-flannel-ds\\\",\\\"uid\\\":\\\"d3731096-8d8e-11e6-bb1d-b827eb645374\\\",\\\"apiVersion\\\":\\\"extensions\\\",\\\"resourceVersion\\\":\\\"168296\\\"}}\\n\"}},\"spec\":{\"volumes\":[{\"name\":\"run\",\"hostPath\":{\"path\":\"/run\"}},{\"name\":\"cni\",\"hostPath\":{\"path\":\"/etc/cni/net.d\"}},{\"name\":\"flannel-cfg\",\"configMap\":{\"name\":\"kube-flannel-cfg\",\"defaultMode\":420}},{\"name\":\"default-token-gr8u0\",\"secret\":{\"secretName\":\"default-token-gr8u0\",\"defaultMode\":420}}],\"containers\":[{\"name\":\"kube-flannel\",\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"command\":[\"/opt/bin/flanneld\",\"--ip-masq\",\"--kube-subnet-mgr\",\"--v=11\"],\"env\":[{\"name\":\"POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.name\"}}},{\"name\":\"POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}}],\"resources\":{},\"volumeMounts\":[{\"name\":\"run\",\"mountPath\":\"/run\"},{\"name\":\"flannel-cfg\",\"mountPath\":\"/etc/kube-flannel/\"},{\"name\":\"default-token-gr8u0\",\"readOnly\":true,\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\",\"securityContext\":{\"privileged\":true}},{\"name\":\"install-cni\",\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"command\":[\"/bin/sh\",\"-c\",\"set -e -x; cp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; while true; do sleep 3600; done\"],\"resources\":{},\"volumeMounts\":[{\"name\":\"cni\",\"mountPath\":\"/etc/cni/net.d\"},{\"name\":\"flannel-cfg\",\"mountPath\":\"/etc/kube-flannel/\"},{\"name\":\"default-token-gr8u0\",\"readOnly\":true,\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\"}],\"terminationMessagePath\":\"/dev/termination-log\",\"imagePullPolicy\":\"IfNotPresent\"}],\"restartPolicy\":\"Always\",\"terminationGracePeriodSeconds\":30,\"dnsPolicy\":\"ClusterFirst\",\"nodeSelector\":{\"beta.kubernetes.io/arch\":\"arm\"},\"serviceAccountName\":\"default\",\"serviceAccount\":\"default\",\"nodeName\":\"k8s-pi-master\",\"hostNetwork\":true,\"securityContext\":{}},\"status\":{\"phase\":\"Running\",\"conditions\":[{\"type\":\"Initialized\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2016-10-08T19:38:45Z\"},{\"type\":\"Ready\",\"status\":\"True\",\"lastProbeTime\":null,\"lastTransitionTime\":\"2016-10-08T19:38:47Z\"}],\"hostIP\":\"192.168.3.50\",\"podIP\":\"192.168.3.50\",\"startTime\":\"2016-10-08T19:38:45Z\",\"containerStatuses\":[{\"name\":\"install-cni\",\"state\":{\"running\":{\"startedAt\":\"2016-10-08T19:38:47Z\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"imageID\":\"docker://sha256:fc79374db247e8c3fbc9b70dd9eade992ac1794edf5f021db5251180fe6e7451\",\"containerID\":\"docker://342e37a90ed1d12ccd50a5561f936c076a524d92794a92126882304165ea0e6d\"},{\"name\":\"kube-flannel\",\"state\":{\"running\":{\"startedAt\":\"2016-10-08T19:38:46Z\"}},\"lastState\":{},\"ready\":true,\"restartCount\":0,\"image\":\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\",\"imageID\":\"docker://sha256:fc79374db247e8c3fbc9b70dd9eade992ac1794edf5f021db5251180fe6e7451\",\"containerID\":\"docker://7731e94c3eb63d76fb63b8468474217eaf9d9f16356d4f01ff829eefc8100f1e\"}]}}\nI1008 19:38:48.390595       1 main.go:132] Installing signal handlers\nI1008 19:38:48.390882       1 kube.go:233] starting kube subnet manager\nI1008 19:38:48.391704       1 manager.go:133] Determining IP address of default interface\nI1008 19:38:48.399537       1 manager.go:163] Using 192.168.3.50 as external interface\nI1008 19:38:48.399633       1 manager.go:164] Using 192.168.3.50 as external endpoint\nE1008 19:38:48.400105       1 network.go:106] failed to register network: failed to acquire lease: node \"k8s-pi-master\" not found\nI1008 19:38:48.392403       1 reflector.go:211] Starting reflector *api.Node (5m0s) from github.com/coreos/flannel/subnet/kube/kube.go:234\nI1008 19:38:48.400626       1 reflector.go:249] Listing and watching *api.Node from github.com/coreos/flannel/subnet/kube/kube.go:234\nI1008 19:38:48.413962       1 round_trippers.go:299] curl -k -v -XGET  -H \"User-Agent: flanneld/v1.4.0 (linux/arm) kubernetes/$Format\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWdyOHUwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjMjZjZTE0Ni04YzkyLTExZTYtYmIxZC1iODI3ZWI2NDUzNzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.ZYNQMgq7rR0uQUf2Mo_pEkWAlbN0rF047-y_S_dXfdZlV76yfDRfd3xJoygVBb3HMXvtJVcM4P12h9G5uJ6oIWnJPQ_rd6SwUJtznkqwlPSyjKuZMPhe7COow3i0Kh7T1WQd_MffUm6MuxGMrp9SnnvQmt14n3C8uB3tq3yqQoRdoMjWzRe33iJKYhBXsiczeG14b82-LMn-qyMRuMVYFZ8aG3J-wGL70VBTbJWEZCbrQtC03aUzxJF2WEcpej4P9ck1oYtM9Ed3uxNI2FFlpa0vSfjIcV6hxdlZrcVRqMfae4_W2uPU737NUyPqEpcSmU8oj9Mj9X27EtvnuqaDBQ\" -H \"Accept: application/json, */*\" https://100.64.0.1:443/api/v1/nodes?resourceVersion=0\nI1008 19:38:48.423491       1 round_trippers.go:318] GET https://100.64.0.1:443/api/v1/nodes?resourceVersion=0 200 OK in 9 milliseconds\nI1008 19:38:48.423609       1 round_trippers.go:324] Response Headers:\nI1008 19:38:48.423643       1 round_trippers.go:327]     Content-Type: application/json\nI1008 19:38:48.423677       1 round_trippers.go:327]     Date: Sat, 08 Oct 2016 19:38:48 GMT\nI1008 19:38:48.424067       1 request.go:908] Response Body: {\"kind\":\"NodeList\",\"apiVersion\":\"v1\",\"metadata\":{\"selfLink\":\"/api/v1/nodes\",\"resourceVersion\":\"168302\"},\"items\":[{\"metadata\":{\"name\":\"k8s-pi-master\",\"selfLink\":\"/api/v1/nodes/k8s-pi-master\",\"uid\":\"c1ea32bf-8c92-11e6-bb1d-b827eb645374\",\"resourceVersion\":\"168302\",\"creationTimestamp\":\"2016-10-07T13:34:22Z\",\"labels\":{\"beta.kubernetes.io/arch\":\"arm\",\"beta.kubernetes.io/os\":\"linux\",\"kubeadm.alpha.kubernetes.io/role\":\"master\",\"kubernetes.io/hostname\":\"k8s-pi-master\"},\"annotations\":{\"scheduler.alpha.kubernetes.io/taints\":\"[{\\\"key\\\":\\\"dedicated\\\",\\\"value\\\":\\\"master\\\",\\\"effect\\\":\\\"NoSchedule\\\"}]\",\"volumes.kubernetes.io/controller-managed-attach-detach\":\"true\"}},\"spec\":{\"externalID\":\"k8s-pi-master\"},\"status\":{\"capacity\":{\"alpha.kubernetes.io/nvidia-gpu\":\"0\",\"cpu\":\"4\",\"memory\":\"948012Ki\",\"pods\":\"110\"},\"allocatable\":{\"alpha.kubernetes.io/nvidia-gpu\":\"0\",\"cpu\":\"4\",\"memory\":\"948012Ki\",\"pods\":\"110\"},\"conditions\":[{\"type\":\"OutOfDisk\",\"status\":\"False\",\"lastHeartbeatTime\":\"2016-10-08T19:38:46Z\",\"lastTransitionTime\":\"2016-10-07T13:34:22Z\",\"reason\":\"KubeletHasSufficientDisk\",\"message\":\"kubelet has sufficient disk space available\"},{\"type\":\"MemoryPressure\",\"status\":\"False\",\"lastHeartbeatTime\":\"2016-10-08T19:38:46Z\",\"lastTransitionTime\":\"2016-10-07T13:34:22Z\",\"reason\":\"KubeletHasSufficientMemory\",\"message\":\"kubelet has sufficient memory available\"},{\"type\":\"DiskPressure\",\"status\":\"False\",\"lastHeartbeatTime\":\"2016-10-08T19:38:46Z\",\"lastTransitionTime\":\"2016-10-07T13:34:22Z\",\"reason\":\"KubeletHasNoDiskPressure\",\"message\":\"kubelet has no disk pressure\"},{\"type\":\"Ready\",\"status\":\"True\",\"lastHeartbeatTime\":\"2016-10-08T19:38:46Z\",\"lastTransitionTime\":\"2016-10-07T13:34:22Z\",\"reason\":\"KubeletReady\",\"message\":\"kubelet is posting ready status. WARNING: CPU hardcapping unsupported\"}],\"addresses\":[{\"type\":\"LegacyHostIP\",\"address\":\"192.168.3.50\"},{\"type\":\"InternalIP\",\"address\":\"192.168.3.50\"}],\"daemonEndpoints\":{\"kubeletEndpoint\":{\"Port\":10250}},\"nodeInfo\":{\"machineID\":\"f996bd096b8f418da602c292906fa243\",\"systemUUID\":\"f996bd096b8f418da602c292906fa243\",\"bootID\":\"c9375c4c-2480-41ba-ab38-b47e53ce6d1b\",\"kernelVersion\":\"4.1.19-v7+\",\"osImage\":\"Ubuntu 16.04 LTS\",\"containerRuntimeVersion\":\"docker://1.11.2\",\"kubeletVersion\":\"v1.4.0\",\"kubeProxyVersion\":\"v1.4.0\",\"operatingSystem\":\"linux\",\"architecture\":\"arm\"},\"images\":[{\"names\":[\"gcr.io/google_containers/kube-proxy-arm:v1.4.0\"],\"sizeBytes\":177088263},{\"names\":[\"gcr.io/google_containers/kube-discovery-arm:1.0\"],\"sizeBytes\":122116943},{\"names\":[\"gcr.io/google_containers/kube-apiserver-arm:v1.4.0\"],\"sizeBytes\":116704924},{\"names\":[\"gcr.io/google_containers/kube-controller-manager-arm:v1.4.0\"],\"sizeBytes\":106962874},{\"names\":[\"gcr.io/google_containers/kube-scheduler-arm:v1.4.0\"],\"sizeBytes\":63165102},{\"names\":[\"quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-arm\"],\"sizeBytes\":59864639},{\"names\":[\"gcr.io/google_containers/etcd-arm:2.2.5\"],\"sizeBytes\":24905124},{\"names\":[\"armel/busybox:latest\"],\"sizeBytes\":3410796},{\"names\":[\"gcr.io/google_containers/pause-arm:3.0\"],\"sizeBytes\":506244}]}}]}\nI1008 19:38:48.432691       1 round_trippers.go:299] curl -k -v -XGET  -H \"Accept: application/json, */*\" -H \"User-Agent: flanneld/v1.4.0 (linux/arm) kubernetes/$Format\" -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWdyOHUwIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjMjZjZTE0Ni04YzkyLTExZTYtYmIxZC1iODI3ZWI2NDUzNzQiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.ZYNQMgq7rR0uQUf2Mo_pEkWAlbN0rF047-y_S_dXfdZlV76yfDRfd3xJoygVBb3HMXvtJVcM4P12h9G5uJ6oIWnJPQ_rd6SwUJtznkqwlPSyjKuZMPhe7COow3i0Kh7T1WQd_MffUm6MuxGMrp9SnnvQmt14n3C8uB3tq3yqQoRdoMjWzRe33iJKYhBXsiczeG14b82-LMn-qyMRuMVYFZ8aG3J-wGL70VBTbJWEZCbrQtC03aUzxJF2WEcpej4P9ck1oYtM9Ed3uxNI2FFlpa0vSfjIcV6hxdlZrcVRqMfae4_W2uPU737NUyPqEpcSmU8oj9Mj9X27EtvnuqaDBQ\" https://100.64.0.1:443/api/v1/watch/nodes?resourceVersion=168302&timeoutSeconds=481\nI1008 19:38:48.439050       1 round_trippers.go:318] GET https://100.64.0.1:443/api/v1/watch/nodes?resourceVersion=168302&timeoutSeconds=481 200 OK in 6 milliseconds\nI1008 19:38:48.439245       1 round_trippers.go:324] Response Headers:\nI1008 19:38:48.439282       1 round_trippers.go:327]     Content-Type: application/json\nI1008 19:38:48.439337       1 round_trippers.go:327]     Date: Sat, 08 Oct 2016 19:38:48 GMT\nE1008 19:38:49.400929       1 network.go:106] failed to register network: failed to acquire lease: node \"k8s-pi-master\" pod cidr not assigned\n. @luxas @tomdee \nI think I got this working!  I added \"--pod-network-cidr\" to kubeadm:\n$ kubeadm init --pod-network-cidr 10.244.0.0/16\nI also replaced vxlan with udp.  Now all my pods are working (even past a reboot).  on to getting a minion running!\n. I certainly learned more about how Kubernetes networking works!\n. ",
    "autostatic": "Exact same issue here. Unfortunately I still get the message below on my nodes (minions) despite having changed to UDP and setting --pod-network-cidr with kubeadm init:\nfailed to register network: failed to acquire lease: node \"some.k8s.node\" pod cidr not assigned\n. @TamalSaha, tried your fix but still the pods can't communicate properly with each other:\nE1115 12:31:12.646494       1 reflector.go:214] pkg/dns/dns.go:155: Failed to list *api.Endpoints: Get https://10.96.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.96.0.1:443: connect: network is unreachable\n. I tested this on a small bare metal Ubuntu 16.04 cluster on OpenStack. One master, two nodes, K8s 1.4.6. I used kubeadm to deploy this cluster so as long as there is no pod network the kube-dns pod will not start up completely. I then deployed the kube-flannel.yml from https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml with the necessary modifications using a Flannel Docker image with your patch. After deploying the kube-dns pod still reports the errors I posted above.\nIf there are better ways to test this then I'd love to know. My main goal is to run plain Flannel as an add-on. I could use Canal but for some setups I'd prefer plain Flannel, I don't always need Calico.\nHere's the kube-flannel.yml I'm using:\n``` ---\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"flannelnet\",\n      \"type\": \"flannel\",\n      \"delegate\": {\n        \"isGateway\": true\n      }\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/22\",\n      \"SubnetLen\": 24,\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      imagePullSecrets:\n        - name: flanneld-registry\n      containers:\n      - name: kube-flannel\nimage: quay.io/coreos/flannel-git:latest\n    image: my.private.gitlab.registry/autostatic/flanneld:20161114\n    command: [ \"/opt/bin/flanneld\" ]\n    args: [ \"-ip-masq\", \"-kube-subnet-mgr\" ]\n    securityContext:\n      privileged: true\n    env:\n    - name: POD_NAME\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.name\n    - name: POD_NAMESPACE\n      valueFrom:\n        fieldRef:\n          fieldPath: metadata.namespace\n    volumeMounts:\n    - name: run\n      mountPath: /run\n    - name: hosts\n      mountPath: /etc/hosts\n    - name: flannel-cfg\n      mountPath: /etc/kube-flannel/\n  - name: install-cni\n    image: busybox\n    command: [ \"/bin/sh\", \"-c\", \"set -e -x; TMP=/etc/cni/net.d/.tmp-flannel-cfg; cp /etc/kube-flannel/cni-conf.json ${TMP}; mv ${TMP} /etc/cni/net.d/10-flannel.conf; while :; do sleep 3600; done\" ]\n    volumeMounts:\n    - name: cni\n      mountPath: /etc/cni/net.d\n    - name: flannel-cfg\n      mountPath: /etc/kube-flannel/\n  volumes:\n    - name: run\n      hostPath:\n        path: /run\n    - name: hosts\n      hostPath:\n        path: /etc/hosts\n    - name: cni\n      hostPath:\n        path: /etc/cni/net.d\n    - name: flannel-cfg\n      configMap:\n        name: kube-flannel-cfg\n\n``\n. Hello @TamalSaha, thanks for the feedback. I made the changes to the CNI config and then Flannel came up successfully, DNS started working and I could deploy a working Dashboard. I don't have a Flannel bridge on my master though, that could be related to the Hairpin setting?\nSo it indeed looks like I was facing a different issue, much thanks for the pointers in the right direction!\nFwiw, anifconfig` of one of the nodes now looks like this:\n``````\ncbr0      Link encap:Ethernet  HWaddr 0a:58:0a:f4:03:01\n          inet addr:10.244.3.1  Bcast:0.0.0.0  Mask:255.255.255.0\n          inet6 addr: fe80::7413:5eff:fec0:2743/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:39641 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:41818 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:7529077 (7.5 MB)  TX bytes:4309168 (4.3 MB)\ndocker0   Link encap:Ethernet  HWaddr 02:42:8c:ed:2b:2a\n          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\n          UP BROADCAST MULTICAST  MTU:1500  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\nens3      Link encap:Ethernet  HWaddr fa:16:3e:d3:c3:67\n          inet addr:172.16.172.101  Bcast:172.16.172.255  Mask:255.255.255.0\n          inet6 addr: fe80::f816:3eff:fed3:c367/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n          RX packets:522692 errors:0 dropped:0 overruns:0 frame:0```\n          TX packets:562404 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1000 \n          RX bytes:776695900 (776.6 MB)  TX bytes:132399137 (132.3 MB)\nflannel.1 Link encap:Ethernet  HWaddr 16:bd:ac:f0:fb:59\n          inet addr:10.244.3.0  Bcast:0.0.0.0  Mask:255.255.252.0\n          inet6 addr: fe80::14bd:acff:fef0:fb59/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:10212 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:6787 errors:0 dropped:8 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:429606 (429.6 KB)  TX bytes:1063174 (1.0 MB)\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          inet6 addr: ::1/128 Scope:Host\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:162 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:162 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1 \n          RX bytes:13460 (13.4 KB)  TX bytes:13460 (13.4 KB)\nveth673f5af5 Link encap:Ethernet  HWaddr aa:7e:f3:33:8b:1b\n          inet6 addr: fe80::a87e:f3ff:fe33:8b1b/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:39562 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:41781 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:8072264 (8.0 MB)  TX bytes:4297709 (4.2 MB)\nvetha2245abc Link encap:Ethernet  HWaddr de:35:55:89:4c:e1\n          inet6 addr: fe80::dc35:55ff:fe89:4ce1/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:3 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:258 (258.0 B)  TX bytes:858 (858.0 B)\nvetha99a743a Link encap:Ethernet  HWaddr 56:56:b5:b2:55:00\n          inet6 addr: fe80::5456:b5ff:feb2:5500/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:3 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:15 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:258 (258.0 B)  TX bytes:1158 (1.1 KB)\n``````\n. Hi @TamalSaha I did some more tests including a couple of fresh deployments and without your patch the cluster is not functional, I can't ping the other nodes from the master. If I do a deployment with a patched Flannel the cluster comes up properly.\n. #560 fixes my issues.. The kube-flannel.yml in the Documentation directory does not work with Kubernetes 1.4. Check out #535 \nYou will need the following set for CNI:\ncni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"type\": \"flannel\",\n      \"delegate\": {\n        \"bridge\": \"cbr0\",\n        \"forceAddress\": true,\n        \"isDefaultGateway\": true\n      }\n    }\nAnd keep in mind that the CIDR you use for your pod network is large enough, a three node cluster needs at least a /22 for instance as each node gets assigned a /24.\n. @garryknox Those options are in /etc/kubernetes/manifests/kube-controller-manager.json on the K8s master. I think you can set those when using the --pod-network-cidr option with kubeadm init or by modifying the manifest afterwards. And the Flannel DaemonSet needs etcd as Flannel is started with the -kube-subnet-mgr option. This is a call to the kube-apiserver which in its turn gets its info from etcd.. @garryknox Yes that's possible. You have to update the net-conf.json config in the kube-flannel.yaml file:\nnet-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/22\",\n      \"SubnetLen\": 24,\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\nAnd you also have to the tell the running etcd service that it has to use this Flannel network and subnet. You can do this with a curl command that reads the values from a JSON file. The contents of the JSON file should look like this:\n{\n  \"Network\": \"10.244.0.0/22\",\n  \"SubnetLen\": 24,\n  \"Backend\": {\"Type\": \"vxlan\"\n  }\n}\nSave this as flannel-config.json and load it into etcd with:\ncurl -L http://127.0.0.1:2379/v2/keys/coreos.com/network/config -XPUT --data-urlencode value@/root/kubernetes/flannel-config.json\nMake sure the Flannel network you're using is the same as the network you've set with --pod-network-cidr and that Flannel subnets can't be smaller than /24 with this method! If you want smaller subnets you'll have to resort to using Canal.. @abessifi \n\nKubernetes 1.4.4, but it should work with any 1.4 release\nDocker 1.12.1, but I don't think this is relevant\nComplete content of kube-flannel.yaml: https://gist.github.com/autostatic/6dcb0f28ac7da2eb8ae9d4b553479228\n\nI do use a patched Flannel otherwise I can't get the K8s master to talk to pods on other nodes. Drawback of this patch is that the first available Flannel subnet gets skipped, so in this specific case 10.244.0.0/24 gets skipped and 10.244.1.0/24 gets assigned first.\nEdit: using a patched Flannel is not necessary anymore, a build from git now works with the big plus that the first subnet is being used and not skipped.. That's not necessary when using Flannel as a CNI add-on as it doesn't depend on /run/flannel/subnet.env.. @xiaoping378 You might need a more recent Flannel Docker image, I think the current flannel-git doesn't have the fix yet.. In my case it's OK. I'm using a self built custom image: https://hub.docker.com/r/autostatic/flannel-git/\nIt contains this commit: https://github.com/coreos/flannel/commit/b5ea6d3a6b42c30c5af447e2c47278c74bb6c8b9\nI think you need at least a Flannel image that has that specific commit.. ",
    "mischief": "fixed by #514\n. fixes #513\n. ",
    "aquavitae": "Any progress on this?\n. ",
    "outworlder": "I have done that by using a variation of method 2. As in, change the Backend type in etcd, then stop and restart members. I guess it works either way, as long as the members are restarted after the change is applied.\nZero downtime for such a change is unlikely, but I'll defer to the experts.. I can't believe it.\nI have wasted more than one week on that too. More like a couple of months on and off. \nI do not understand why this happens though. Can someone shed some light? By the way, I experienced it in GCE, with connections from K8s pods to a VM-based RMQ node.. ",
    "fxpester": "changed backend from udp to vxlan in etcd, restarted all servers with full reboot\nk8s pods are not accessible now\nflannel spam logs with L3 miss but route for 10.233.95.4 not found\nin tcpdump I can see:\n08:20:38.026513 IP 10.111.0.20.44541 > 10.111.0.21.otv: OTV, flags [I] (0x08), overlay 0, instance 1\nIP 10.233.88.0 > 10.233.108.0: ICMP host 10.233.88.7 unreachable, length 60. ",
    "discostur": "Is there anything new on this? What is the best way to change the Network Backend Type?. ",
    "TattiQ": "Guys here   https://github.com/kubernetes/kops/pull/3190   mentioned \nkops edit cluster <- change udp for vxlan\nkubectl edit configmap -n kube-system kube-flannel-cfg <- change udp for vxlan\nkubectl delete pod -n kube-system -l app=flannel   \nbut none of it made it to the official docs . \nThe coreos docs mention a limitation for making changes at runtime - The backend type cannot be changed. (It can be changed if you stop all workloads and restart all flannel daemons.)  https://coreos.com/flannel/docs/latest/running.html .\n. Not clear what they mean by stopping all workloads though.  . ",
    "jonaz": "Do we have a config example of this? . ",
    "arthur-c": "That's exactly my case:\nthe last entry in my table:\n{\n                    \"GatewayId\": \"vpce-xx\", \n                    \"Origin\": \"CreateRoute\", \n                    \"State\": \"active\", \n                    \"DestinationPrefixListId\": \"pl-xxx\"\n                }\nI'll wait for the patch release, thanks.\n. ",
    "salamaniibm": "Facing same issue, built flannel on s390x (IBM zSystem) machine.\nModified the ip.NativelyLittle function to return true unconditionally, flannel worked correctly.\nAny further update on this.\nIs endian-type check is needed?\nRegards,\nSalamani\n. Hi tomdee,\nThe above issue is due to invalid path of LIB_DIR in Makefile for s390x.\nThe LIB_DIR for s390x has to be changed to /usr/s390x-linux-gnu/lib.\nI will add the PR for it.. \nKUBE_CROSS:1.7.4-1 has go which supports all the architectures including s390x(The curent version of KUBE_CROSS has go which does not support s390x). Also this new image uses glibc version 2.23.\nBut the busybox uses the glibc version 2.19 which is not compatible with flannel binary generated.To handle this situation for s390x, I am pulling libc.so.6 and ld64.so.1 from kube-cross to busybox.\nIf we upgrade the KUBE_CROSS version then either we need to change glibc version used in busybox or like s390x we will have to pull libc.so.6 and ld64.so.1 from kube-cross and use in busybox for all the architectures.. Did I answer your question or let me know if there is any comment\n. ",
    "kiich": "Issue https://github.com/coreos/etcd/issues/6046 is very similar as well but was fixed by CoreOS 1109.1.0 - we are using 1122.2.0 though so looks like new issue.\n. ",
    "chulkilee": "I'm hitting the same problem\n- etcdctl cluster-info seems fine\n- flannel connects to etcd when started - created key on etcd\nIf I change FLANNELD_ETCD_ENDPOINTS to use the ip address of etcd master, then this error message does not appear.\nversions\n- flannel: v0.6.2\n- etcd: 3.0.13\n- elb: forward tcp 2379\nconfiguration for single etcd master\n```\netcd.env\nETCD_NAME=etcd0\nETCD_DATA_DIR=/var/lib/etcd\nETCD_LISTEN_PEER_URLS=http://10.0.0.115:2380\nETCD_LISTEN_CLIENT_URLS=http://10.0.0.115:2379,http://127.0.0.1:2379\nclustering flags\nETCD_INITIAL_ADVERTISE_PEER_URLS=https://10.0.0.115:2380\nETCD_INITIAL_CLUSTER=etcd0=https://10.0.0.115:2380\nETCD_INITIAL_CLUSTER_STATE=new\nETCD_INITIAL_CLUSTER_TOKEN=foo\nETCD_ADVERTISE_CLIENT_URLS=http://10.0.0.115:2379\n```\nflanneld logs\nI1101 17:56:32.129243 08212 manager.go:133] Determining IP address of default interface\n I1101 17:56:32.129564 08212 manager.go:163] Using 10.0.26.204 as external interface\n I1101 17:56:32.129737 08212 manager.go:164] Using 10.0.26.204 as external endpoint\n I1101 17:56:32.139427 08212 local_manager.go:134] Found lease (172.17.56.0/24) for current IP (10.0.26.204), reusing\n I1101 17:56:32.141566 08212 manager.go:246] Lease acquired: 172.17.56.0/24\n I1101 17:56:32.141968 08212 network.go:58] Watching for L3 misses\n I1101 17:56:32.142175 08212 network.go:66] Watching for new subnet leases\n I1101 17:56:32.145097 08212 network.go:153] Handling initial subnet events\n I1101 17:56:32.145323 08212 device.go:163] calling GetL2List() dev.link.Index: 3\n I1101 17:56:32.145572 08212 network.go:160] fdb already populated with: 10.0.25.20 46:6b:54:e7:d7:65\n E1101 17:57:31.394033 08212 watch.go:272] Subnet watch failed: client: etcd cluster is unavailable or misconfigured\n E1101 17:57:31.394653 08212 watch.go:43] Watch subnets: client: etcd cluster is unavailable or misconfigured\n E1101 17:58:32.393741 08212 watch.go:272] Subnet watch failed: client: etcd cluster is unavailable or misconfigured\n E1101 17:58:32.393834 08212 watch.go:43] Watch subnets: client: etcd cluster is unavailable or misconfigured\n E1101 17:59:33.393401 08212 watch.go:272] Subnet watch failed: client: etcd cluster is unavailable or misconfigured\n E1101 17:59:33.393728 08212 watch.go:43] Watch subnets: client: etcd cluster is unavailable or misconfigured\n E1101 18:00:34.393061 08212 watch.go:272] Subnet watch failed: client: etcd cluster is unavailable or misconfigured\n E1101 18:00:34.393154 08212 watch.go:43] Watch subnets: client: etcd cluster is unavailable or misconfigured\n. @eghobo you're correct! I just created an issue on etcd - coreos/etcd#6810\n. I hit the same problem on manual installation - I made it work again by 1) stopping flannel and docker 2) remove stale docker_opts.env 3) start flannel and then docker.\n. @TehesFR as far as I remember I only had to run it on worker nodes (running docker), but not sure.. Okay, @tomdee closed #542 mentioning this, so I guess this will be a placeholder for using etcv3.\nAlthough flannel is working with etcv3, I see lots of warning when using etcd behind Amazon ELB since etcdv2 API does not send heart beat (while etcdv3 does). Although it does not break anything (I guess flannel will just reconnect), but it's annoying :)\n. ",
    "swqmaven": "@pires i had resolved this error thanks for you reply\n. in my environment\nnode1:192.168.1.41\nnode2:192.168.1.42\nnode1 ping node2 docker0 ok\nnode2 ping node1 containers ok\nbut node1 ping node2 other containers not ok!!\nIt's means the flannel is ok?\nI check nodes route and flannel config is ok.\nwhy this happened? i think the problem exits in node2. what is it?\nthanks for help!\n. ",
    "luohoufu": "tks,i see. maybe can customer setting flannel iface name will a new requirement.\n. i get it.\n. ",
    "bison": "I looked at this for a bit this afternoon. When Flannel is picking subnets, it skips the first, i.e. the network address: config.go. The allocator in Kubernetes doesn't: cidr_set.go.\nUsing that first subnet may still work with the routes set up correctly, but Flannel only adds a route with the UNIVERSE scope if the default link-scoped route wasn't added automatically: device.go.\nI have clusters up and running fine with the default link-scoped routing because the network address isn't being used as a subnet somewhere. Anyway, it seems like either Flannel needs to ensure the routing is definitely set up correctly, or Kubernetes needs to not allocate that first subnet.\n. RouteList() only lists the main table. I just tested, and this does work if you delete the routes from the local table as well:\n```go\nmainFilter := &netlink.Route{\n        LinkIndex: dev.link.Attrs().Index,\n        Table:     syscall.RT_TABLE_MAIN,\n}\nlocalFilter := &netlink.Route{\n        LinkIndex: dev.link.Attrs().Index,\n        Table:     syscall.RT_TABLE_LOCAL,\n}\nmainRoutes, err := netlink.RouteListFiltered(netlink.FAMILY_ALL, mainFilter, netlink.RT_FILTER_OIF|netlink.RT_FILTER_TABLE)\nif err != nil {\n        return fmt.Errorf(\"Failed to list routes: %v\", err)\n}\nlocalRoutes, err := netlink.RouteListFiltered(netlink.FAMILY_ALL, localFilter, netlink.RT_FILTER_OIF|netlink.RT_FILTER_TABLE)\nif err != nil {\n        return fmt.Errorf(\"Failed to list routes: %v\", err)\n}\nfor _, er := range append(mainRoutes, localRoutes...) {\n        log.Infof(\"Removing route: %s\", er.String())\n        if err := netlink.RouteDel(&er); err != nil {\n                return fmt.Errorf(\"Failed to delete route: %v\", err)\n        }\n}\nif err := netlink.RouteAdd(&route); err != nil {\n        return fmt.Errorf(\"Failed to add route: %v\", err)\n}\n```. It might also be an option to have the netlink library support some way of adding the link without the routes. Haven't looked to see if that's a thing that would be doable / desirable.. That's true. I'm still not sure what the deal is there. If you delete the daemonset, you can see that the device and routes are still there. If you recreate it, when Flannel comes back, it adds the correct routes. No idea why.. ",
    "aoxn": "@tomdee @philips  Any update? I came across the same issue.  And I find simply replace the flannel.1 device with another regular IP address(rather than a network address) works for me. Like replace 172.16.0.0/16 with 172.16.0.1/16. . @aaronlevy  That works for me , very much thanks!!. @tomdee @evanfarrar Could you review this? Tks.. @tomdee Tks,   Yes,We are happy to help out with supporting! Please do not hesitate to contact us.. @tomdee Add a AliCloud VPC flannel  Guide at Documentation directory. Pls review it .. updated.  @tomdee .  make sure your ECS security group has open to your ContainerNetwork.  And also check to make sure vxlan port is open.\nand i suggest you to provide a more specific description on your problem.  have you capture package flow use tcpdump\uff1f. Please make  sure your ECS is under VPC network environment ,this backend is only suitable for VPC network.@liyehaha. You might need to fix your local dns lookup configuration. i suppose. Because this driver is intended to be used on alibaba cloud env,  and mostly for china users , BUT quay.io/coreos is behind china's great firewall which we can not access directly, So we need to mirror quay.io ourselves. On the other hand, we can add another yaml file call kube-flannel-alivpc-quay.yml for out of china purpose.  and keep the old one.     Or do you have any other suggestion?  @tomdee Thanks!. ",
    "tamalsaha": "@cristifalcas, the PR is ok. Travis build is working. I need to add dependency on k8s.io/kubernetes/pkg/client/unversioned/clientcmd\n. @tomdee , I have made the changes as suggested. PTAL.\n. @tomdee, can this pull request be merged?. @aaronlevy do you mind taking a look at this pr?. @aaronlevy, we are using Flannel to setup networking for our kube cluster in Linode. Linode assigns 2 IPs (one public and one private) to the same eth0 interface. We would like to route all cluster pod traffic via the private IPs. So, we run flanneld using static pods and set the -publicIP=<instance_private_ip>. In that scenario, we need to pass a kubeconfig to flanneld so that it can connect to kube master. We can't use Daemonset, since the -publicIP needs to set properly for each instance of the cluster.. I have already set --advertise-address=<private-ip-of-master>.\nFrom the flannel docs, I read:\n--public-ip=\"\": IP accessible by other nodes for inter-host communication. Defaults to the IP of the interface being used for communication.\n--iface=\"\": interface to use (IP or name) for inter-host communication. Defaults to the interface for the default route on the machine.\nMy understanding was that this --public-ip is the IP used by flannel to route traffic.\n\nAlso, to address the concern about daemonsets, you can use the downward api in the pod template to extract the local node's podIP. An example of this is in bootkube:\n\nI think I see your point. Since the Daemonset has to be HostNetwork: true, PodIP = Instance's IP. If I configure Kubelet to use the  as nodeIP, then PodIP of flannel daemon will be . I think this will work. Did I understand you correctly?\n. @aaronlevy Thanks a lot for your help! I will test with DaemonSet and reopen if the problem exists.. Things seem to be working after applying https://github.com/appscode/flannel/commit/b083788405ce2bf3c34b9d4df7b5d77afc865b4e\n. @tomdee, I was pinging from master to a pod running on a different node.\n. @tomdee I just ran a nginx pod, then tried to wget from the master host directly.\n```\nkubectl run my-nginx --image=nginx --replicas=2 --port=80\nkubectl expose deployment my-nginx --port=80 \nwget http://:80\n```\n. @autostatic , can you explain your test case bit more so that I can recreate it?\n. @autostatic , I am not sure that you are having the same issue as I was. The issue I was facing was that  pods running on master (with hostNetwork:true in my case) could not connect to pods on regular nodes using Pod IP.\nFrom your log, it seems that DNS pod running on  regular node can't connect to kube apiserver (https://10.96.0.1:443). So, If I were you, I would first confirm that the flannel network is actually working as intended. One way to check that is to see if you can ping the IP address of the flannel bridge on master from the node running DNS pod.\nFYI, I also had to make some changes to the cni-conf.json. You can see my changes here: https://github.com/appscode/kubernetes/commit/ee660dc997f7ae5042033f226b4416d4513b5422 . The important thing here was, ensuring Kubernetes was using the bridge created by flannel. Without that, pods will be disconnected from the flannel overlay network. It will be helpful to see the result of ifconfig from  one of your regular nodes.\nIf you are unfamiliar with the cni conf option, you will find these docs handy:\n- https://github.com/containernetworking/cni/blob/master/Documentation/flannel.md\n- https://github.com/containernetworking/cni/blob/master/Documentation/bridge.md\n. @autostatic I am glad that your cluster is working. The flannel brdige gets created the first time CNI plguin is called. Since kubernetes does not run regular pod on master, cbr0 bridge has not been created yet.\nIt also seems that you don't need my patch. I needed this patch because we run a HAproxy based ingress controller on the master that load balances across pods on regular nodes. So, I needed Haproxy on master to be able to connect to pods on regular nodes.\n. Yes, if you want to ping regular nodes from master, you need this patch.\n. @mattenklicker, which version are you using? https://github.com/coreos/flannel/releases/tag/v0.7.0 is supposed to fix this issue.. This issue is fixed for me with v7.0 .. ",
    "mattenklicker": "I have the same problem: First kubernetes node gets the net address 10.244.0.0 from network 10.244.0.0/16 assigned. Therefore this node is not reachable from other nodes. NodePort services, that I want to reach  via the first node are unreachable, when the service itself runs on another node. I can see leaving packets from 10.244.0.0 to other nodes, but I can't see returning packets because they are not routable.\nThe above patch (https://github.com/coreos/flannel/issues/535#issuecomment-255516812) skips the net address, but networking didn't work for me after that. At least in an existing cluster. And there is no check for duplicate addresses.\nPerhaps a explicit route like 10.244.0.0/32 dev flannel.1 on all other nodes would work, but I did not test that and it doesn't look nice when an a route 10.244.0.0/16 dev flannel.1 exists.. @TamalSaha v0.7.0. ",
    "samarjit": "Update: Created a github project to create environment - https://github.com/samarjit/vagrant-kubeadm\nI am using v0.7.0 too. But having same issue master to slave node communication failure.\n[root@kmaster ~]# kubectl get pods -o wide\nNAME                                READY     STATUS    RESTARTS   AGE       IP              NODE\nhello-deployment-1725651635-1nnnx   1/1       Running   0          10m       10.244.1.4      kslave\nhello-deployment-1725651635-dh3r6   1/1       Running   0          10m       10.244.1.3      kslave\nhello-deployment-1725651635-smtx8   1/1       Running   0          10m       10.244.0.2      kmaster\nkube-flannel-ds-bklmr               2/2       Running   0          43m       192.168.33.10   kmaster\nkube-flannel-ds-m0lbd               2/2       Running   2          35m       192.168.33.11   kslave\n[root@kmaster ~]#\nPing 10.244.0.2 -> 10.244.1.4 master to slave does not work.\nIn master node, if I query dns it seems to work fine:\n[root@kmaster ~]# dig +short  @10.96.0.10 _http._tcp.hello-service.default.svc.cluster.local SRV\n;; connection timed out; no servers could be reached\n[root@kmaster ~]#\n[root@kmaster ~]#  tcpdump -e -i flannel.1 -n arp\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on flannel.1, link-type EN10MB (Ethernet), capture size 65535 bytes\n06:23:47.701820 0e:94:71:89:36:90 > 96:5a:33:93:7c:6f, ethertype ARP (0x0806), length 42: Request who-has 10.244.1.2 tell 10.244.0.0, length 28\n06:23:47.701832 0e:94:71:89:36:90 > 96:5a:33:93:7c:6f, ethertype ARP (0x0806), length 42: Request who-has 10.244.1.2 tell 10.244.0.0, length 28\n06:23:48.703932 0e:94:71:89:36:90 > 96:5a:33:93:7c:6f, ethertype ARP (0x0806), length 42: Request who-has 10.244.1.2 tell 10.244.0.0, length 28\nIf I try tcpdump in slave node no packets are received.\nI followed testing DNS as described in https://kubernetes.io/docs/admin/dns/. It works!\n```\n[root@kmaster ~]# kubectl exec -ti busybox -- nslookup kubernetes.default\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\nName:      kubernetes.default\nAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local\n[root@kmaster ~]#\n```\nI am starting kubeadm using the following script.\n```\nkubeadm init --api-advertise-addresses=192.168.33.10 --token=7baee4.d576223cb4884c9b --pod-network-cidr=\"10.244.0.0/16\"\njq \\\n   '.spec.containers[0].command |= .+ [\"--advertise-address=192.168.33.10\"]' \\\n   /etc/kubernetes/manifests/kube-apiserver.json > /tmp/kube-apiserver.json\nmv /tmp/kube-apiserver.json /etc/kubernetes/manifests/kube-apiserver.json\nkubectl -n kube-system get ds -l 'component=kube-proxy' -o json \\\n  | jq '.items[0].spec.template.spec.containers[0].command |= .+ [\"--proxy-mode=userspace\",\"--cluster-cidr=10.244.0.0/16\"]' \\\n  |   kubectl apply -f - && kubectl -n kube-system delete pods -l 'component=kube-proxy'\n  cp /etc/kubernetes/admin.conf /vagrant\n```\nkube-flanel.yml. \ncni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"type\": \"flannel\",\n      \"delegate\": {\n        \"isDefaultGateway\": true\n      }\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n[root@kmaster ~]# ip route\ndefault via 10.0.2.2 dev enp0s3  proto static  metric 100\n10.0.2.0/24 dev enp0s3  proto kernel  scope link  src 10.0.2.15  metric 100\n10.244.0.0/24 dev cni0  proto kernel  scope link  src 10.244.0.1\n10.244.0.0/16 dev flannel.1\n169.254.0.0/16 dev enp0s8  scope link  metric 1003\n172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1\n192.168.33.0/24 dev enp0s8  proto kernel  scope link  src 192.168.33.10\n[root@kmaster ~]#\n[root@kslave ~]# ip route\ndefault via 10.0.2.2 dev enp0s3  proto static  metric 100\n10.0.2.0/24 dev enp0s3  proto kernel  scope link  src 10.0.2.15  metric 100\n10.244.0.0/16 dev flannel.1\n10.244.1.0/24 dev cni0  proto kernel  scope link  src 10.244.1.1\n169.254.0.0/16 dev enp0s8  scope link  metric 1003\n172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1\n192.168.33.0/24 dev enp0s8  proto kernel  scope link  src 192.168.33.11\n[root@kslave ~]#\n[root@kmaster ~]# ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 08:00:27:5a:e9:e7 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3\n       valid_lft 82857sec preferred_lft 82857sec\n    inet6 fe80::a00:27ff:fe5a:e9e7/64 scope link\n       valid_lft forever preferred_lft forever\n3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 08:00:27:9b:03:a6 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.33.10/24 brd 192.168.33.255 scope global enp0s8\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe9b:3a6/64 scope link tentative dadfailed\n       valid_lft forever preferred_lft forever\n4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN\n    link/ether 02:42:4d:51:23:5b brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN\n    link/ether 0e:94:71:89:36:90 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::c94:71ff:fe89:3690/64 scope link\n       valid_lft forever preferred_lft forever\n6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP\n    link/ether 0a:58:0a:f4:00:01 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.1/24 scope global cni0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::780b:a4ff:fe46:ab02/64 scope link\n       valid_lft forever preferred_lft forever\n7: veth481ad07c@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP\n    link/ether 7a:0b:a4:46:ab:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet6 fe80::780b:a4ff:fe46:ab02/64 scope link\n       valid_lft forever preferred_lft forever\n[root@kmaster ~]#\n[root@kslave ~]# ip a\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 08:00:27:5a:e9:e7 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic enp0s3\n       valid_lft 82847sec preferred_lft 82847sec\n    inet6 fe80::a00:27ff:fe5a:e9e7/64 scope link\n       valid_lft forever preferred_lft forever\n3: enp0s8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000\n    link/ether 08:00:27:45:d8:7e brd ff:ff:ff:ff:ff:ff\n    inet 192.168.33.11/24 brd 192.168.33.255 scope global enp0s8\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a00:27ff:fe45:d87e/64 scope link tentative dadfailed\n       valid_lft forever preferred_lft forever\n4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN\n    link/ether 02:42:c3:53:19:21 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n5: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN\n    link/ether 96:5a:33:93:7c:6f brd ff:ff:ff:ff:ff:ff\n    inet 10.244.1.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::945a:33ff:fe93:7c6f/64 scope link\n       valid_lft forever preferred_lft forever\n6: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP\n    link/ether 0a:58:0a:f4:01:01 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.1.1/24 scope global cni0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::3052:1cff:fe84:193b/64 scope link\n       valid_lft forever preferred_lft forever\n7: veth33545403@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP\n    link/ether 32:52:1c:84:19:3b brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet6 fe80::3052:1cff:fe84:193b/64 scope link\n       valid_lft forever preferred_lft forever\n8: vethd5892a87@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP\n    link/ether 22:e8:13:6f:fe:ae brd ff:ff:ff:ff:ff:ff link-netnsid 1\n    inet6 fe80::20e8:13ff:fe6f:feae/64 scope link\n       valid_lft forever preferred_lft forever\n9: vethaf799bc1@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP\n    link/ether 4a:27:e1:5a:41:39 brd ff:ff:ff:ff:ff:ff link-netnsid 2\n    inet6 fe80::4827:e1ff:fe5a:4139/64 scope link\n       valid_lft forever preferred_lft forever\n10: veth84875acc@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP\n    link/ether 96:5c:ac:de:82:bb brd ff:ff:ff:ff:ff:ff link-netnsid 3\n    inet6 fe80::945c:acff:fede:82bb/64 scope link\n       valid_lft forever preferred_lft forever\n[root@kslave ~]#\n. My issue was solved. Its vagrant environment specific issue. \nVagrant assign 10.0.2.15 IP to each machine which flannel was using as key, so it was creating only one subnet, ideally there should be two subnets for each of the nodes. Solution was to provide --iface=eth1 while launching flanneld. I noticed this after deploying etcd and flannel natively on clean VMs. \nSame logic was applied in startup command of flannel in kubernetes.\nKube-Flannel yaml:\n```\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"type\": \"flannel\",\n      \"delegate\": {\n        \"isDefaultGateway\": true\n      }\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: amd64\n      serviceAccountName: flannel\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.7.0\n        command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" , \"--iface=enp0s8\"]\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.7.0\n        command: [ \"/bin/sh\", \"-c\", \"set -e -x; cp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf; while true; do sleep 3600; done\" ]\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n```\nNote: --iface=.\n[root@kmaster ~]#\n[root@kmaster ~]# kubectl describe hello-service\nthe server doesn't have a resource type \"hello-service\"\n[root@kmaster ~]# kubectl describe service hello-service\nName:                   hello-service\nNamespace:              default\nLabels:                 <none>\nSelector:               app=hello\nType:                   ClusterIP\nIP:                     10.104.194.162\nPort:                   http    80/TCP\nEndpoints:              10.244.0.2:8080,10.244.1.3:8080,10.244.1.4:8080\nSession Affinity:       None\nNo events.\n[root@kmaster ~]#\nShows DNS resolution.\n[root@kmaster ~]# dig +short  @10.96.0.10 _http._tcp.hello-service.default.svc.cluster.local SRV\n10 100 80 hello-service.default.svc.cluster.local.\n[root@kmaster ~]# dig +short  @10.96.0.10 hello-service.default.svc.cluster.local.\n10.104.194.162\nThe service is reachable.\n[root@kmaster ~]# curl http://10.104.194.162:80\nHello, \"/\"\nHOST: hello-deployment-1725651635-pb9mv\nADDRESSES:\n    127.0.0.1/8\n    10.244.1.4/24\n    ::1/128\n    fe80::f067:16ff:fe96:7295/64\n[root@kmaster ~]#\n[root@kmaster ~]#\n[root@kmaster ~]# curl http://10.104.194.162:80\nHello, \"/\"\nHOST: hello-deployment-1725651635-0t8xx\nADDRESSES:\n    127.0.0.1/8\n    10.244.1.3/24\n    ::1/128\n    fe80::c59:b2ff:fe82:ee1a/64\n[root@kmaster ~]#\n[root@kmaster ~]# curl http://10.104.194.162:80\nHello, \"/\"\nHOST: hello-deployment-1725651635-51df9\nADDRESSES:\n    127.0.0.1/8\n    10.244.0.2/24\n    ::1/128\n    fe80::c4a1:84ff:fe82:ec83/64\n[root@kmaster ~]#. ",
    "rastislavszabo": "@samarjit Thanks, I run into the same issue, specifying --iface= works for me too.. The issue is https://github.com/coreos/flannel/issues/535\nClosing this as a duplicate.. ",
    "linericyang": "@samarjit ran into the same issue in vagrant environment, specifying --iface= in flannel daemon works for me. Thanks.. ",
    "Ahrotahntee": "It is a problem (though admittedly not a very high priority one.) \nI had figured that since flannel is the one injecting the parameters that it should be the one to accommodate.\nI will also raise an issue on the docker tracker for the log-opts not terminating correctly.. ",
    "nkwangleiGIT": "We met with the same issue recently, any update about this issue? thanks!\n. ",
    "magicwang-cn": "+1\n. ",
    "TehesFR": "@chulkilee thanks I will test your workaround this week. Have you done this on all nodes ?. ",
    "charon-kyan": "i++. ",
    "KDF5000": "@TehesFR I met the same issue. Have you found any solution ?. @berlinsaint @defcyy my linux kernel version is the same as yours. I added  the forward rule \"iptables -P FORWARD ACCEPT\" ,but  it still did not work. The flannel version you use is 0.7.1? . @berlinsaint yes, it is 3.13.0-24. Did you upgrade to 16.04 or rebuild a new kernel?. @berlinsaint @defcyy After I upgrade the kernel to 4.2.0-42, it works! Thanks!! @berlinsaint @defcyy\nThere are some related issues which will be helpful:\nhttps://github.com/moby/moby/pull/28257\nSupport VXLAN on Kernels <= 3.15\nps: @berlinsaint \u4f60\u7684qq\u9700\u8981\u9a8c\u8bc1[\u6342\u8138]. I use \"ip link del flannel.1\" to delete the device on ubuntu. I guess that it will work too.. ",
    "chrislovecnm": "@luxas I applied that yaml and it did not appear to install the flannel binaries.  Is this the expected behavior?\n. > The manifest will use the flannel binary in /opt/cni/bin and put a file in /etc/cni/net.d.\n\nWould love to have a doc of this as well.\n\nThe binary did not get installed by the ds\n. @luxas which tar ball are you referring to ... can you ping me on slack?\n. Also is flannel compatible with etcd 3 yet? Off topic, let me know if I need to file another issue\n. I am unsubscribing from this because I never got any answer, and this issue is now on another topic . ",
    "garryknox": "@luxas  you wrote\n\nYou need to have set --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin and have the cni bins in /opt/cni/bin\nYou should set --allocate-node-cidrs=true and --cluster-cidr as well in order for it to work.\n\nWhere do you set these values?\nThanks.. I can answer the first part of my question. I see the cni parameters are part of the KUBELET_NETWORK_ARGS environment variables in my 10-kubeadm.conf file -- which is part of the kubeadm package install I used. I'm still tracking down --allocate-node-cidrs=true and --cluster-cidr .\n2nd question. Does the Flannel daemonset use etcd still? . @autostatic Thanks. \nRegarding etcd. I see the etcd urls I passed into kubeadm init are in the kube-apiserver conf file. My pod network seems to work fine but I don't see flannel subnets in my etcd db. Usually with flanneld you can set the  etch-prefix -- is there a way to do this with kubeadm+flannel daemonset? \n. ",
    "CaesarC": "@garryknox i have the same problem too.\ni have to build a machine whit flanned alone. but i can join the kube cluster network.\nin kube 1.2 i usually start flanned with etcd with key prefix /coreos.com/network\nE1205 17:13:22.380516 08865 network.go:106] failed to retrieve network config: 100: Key not found (/coreos.com) [10267]. ",
    "abessifi": "@autostatic I still not able to run flannel across my cluster. could you please tell us:\n- what is the version of kubernetes you are using ?\n- the docker version ?\n- what is the complete content of the kube-flannel.yaml file ?. @xiaoping378 \nCould you please point out your complete kube-flannel.yaml file ?. ",
    "xiaoping378": "should i modify manually docker.service ?\ndue to it depends the /run/flannel/subnet.env. Thanks, i got it.\ni missing --pod-network-cidr=10.244.0.0/16. But the the kube-dns always output\nGet https://10.96.0.1:443/api/v1/endpoints?resourceVersion=0: dial tcp 10.96.0.1:443: getsockopt: no route to host\nmy host route:\n[root@node0 ~]# ip route\ndefault via 192.168.31.1 dev enp0s8  proto static  metric 100 \n10.244.0.0/24 dev cni0  proto kernel  scope link  src 10.244.0.1 \n10.244.0.0/16 dev flannel.1  proto kernel  scope link  src 10.244.0.0 \n172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 \n192.168.31.0/24 dev enp0s8  proto kernel  scope link  src 192.168.31.102  metric 100 \n192.168.56.0/24 dev enp0s3  proto kernel  scope link  src 192.168.56.110  metric 100\nthe iptables is also OK.\nDon't understand this error, @autostatic  help.. OK, worked as below:\n1. disable selinux,\n2.  change cni-conf.json\n\n. Hi @abessifi \nafter i run kubeadm reset, and init again, the issue still exists.\nso ... debuging. using the real latest image,  dns can't route to 10.96.0.1:443\n@autostatic is it OK in your case ? can you tell me your image id of flannel-git ?. now in my case(the latest image), i type manually the below, it will be OK. \niptables -A IN_public_allow -p tcp -m tcp --dport 6443 -m conntrack --ctstate NEW -j ACCEPT. i found the code \uff0c so can't make number of pods larger then /24 ? \ngolang\nif cfg.Network.PrefixLen < 24 {\n    cfg.SubnetLen = 24\n} else {\n    cfg.SubnetLen = cfg.Network.PrefixLen + 1\n}. @rosenhouse i am using flannel-v0.7.0, it's configration is net-conf.json, i have set the SubnetLen as the first picture,  but it didn't work. \ncan you type your coreos.com/network/config ?. ",
    "HerrmannHinz": "u can go get the missing dependencies one after the other until u have all packages.\njust doing this  - this is of course just a temporary fix.. oh wait but it fails on this one, no step further possible:\nroot@localhost:/opt/src/flannel# go get k8s.io/kubernetes/pkg/controller/framework\npackage k8s.io/kubernetes/pkg/controller/framework\n    imports k8s.io/kubernetes/pkg/controller/framework\n    imports k8s.io/kubernetes/pkg/controller/framework: cannot find package \"k8s.io/kubernetes/pkg/controller/framework\" in any of:\n    /usr/lib/go/src/pkg/k8s.io/kubernetes/pkg/controller/framework (from $GOROOT)\n    /root/go/src/k8s.io/kubernetes/pkg/controller/framework (from $GOPATH). @fabianromerorcc no not yet. haven't tried anymore. . ",
    "fabianromerorcc": "Hi HerrmannHinz! Did you figure out how to resolve that package?\nI'll appreciate any help\nCheers!. ",
    "besnik": "Hi all,\nfollowing has worked for me (I don't know go so I apologize if there is better solution). You need to specify GOPATH env variable so go can find packages of dependencies that are located in vendor folder. The trick is that you need to clone flannel project into specific path (src/github.com/coreos) and set GOPATH to the directory where src subdirectory is, see below:\nAssuming you are in your home directory:\ncd ~\nexport GOPATH=$(pwd)\nmkdir -p src/github.com/coreos\ncd src/github.com/coreos\ngit clone https://github.com/coreos/flannel.git\ncd flannel\nmake dist/flanneld\nIt would be great if go or flannel team could review this as suggest better solution. Ideally vendor folder with dependencies should be located with respect to current directory of calling make (flannel folder). I dont know if this is problem of Go or could be enhanced in Makefile or somewhere. . Hi, I replied with a solution that worked for me in #552 \nBasically:\ncd ~\nexport GOPATH=$(pwd)\nmkdir -p src/github.com/coreos\ncd src/github.com/coreos\ngit clone https://github.com/coreos/flannel.git\ncd flannel\nmake dist/flanneld. ",
    "mgoodness": "This is a somewhat pressing question with etcd v3 becoming the default store in Kubernetes 1.6. This blog post seems to indicate that the v2 API will keep working after the data has been migrated to v3, but confirmation would be nice.. ",
    "nesc58": "Is anyone working on this issue? There's huge issue to create backups for v2 (flannel) and v3 (e.g. kubernetes). The clientv3 for etcd is in the official repo. Are there unsolved issues or is this a personal/timing problem to solve this issue in one of the next versions?\nAnd it's right. Flannel still works with etcdv3.\n. Is there a plan to merge this?\n@mkutsevol are you using your implementation in a productive environment or still testing? I am thinking about to fork the current release and merge your commits and try to test it in our testing environment. IPSec or encryption in general is a so necessary point in an infrastructure.. @mkutsevol thank you. I changed the flannel image and changed the backend type to ipsec. It works. But in combination with kubernetes it doesn't work. The configurations are the same. Nothing changed. \nE.g. I cannot access the kubernetes dashboard using the URL <apiserver>/ui. Is there something to change? Kube-proxy configuration or something else? \nIt is configured with proxy mode iptables. I tried to switch between activating and deactivating the masquerading all. Nothing changed.\nPods with a NodePort service can accessed from outside the cluster by using the host ip of the server where the pod is running. This is fine. \nThe internal routing between an ingress haproxy with ssl termination and an application behind also works.\nInternal, e.g. The dashboard is accessible e.g. From a busybox pod in another namespace. The routing seems to be okay. I can access it with the URL kubernetes-dashboard.kube-system. So, the dns addon (kube dns) also works.\nCan you help me?\nTomorrow I will test the cluster internal communication. \n. @mkutsevol thanks. I will test it soon. And again, thanks for this implementation. . ",
    "andyxning": "@nesc58 Seems like flannel has updated the etcd dep version to 3.1.x. @tomdee could you please confirm this?\nSee #759 for more info.. Any plan to upgrade etcd support to etcd v3? @tomdee . Any updates on this?. @genevievelesperance Why you close this PR? Is healthcheck functionality available already?. @eyakubovich @tomdee @zbwright Please take a look at the CI error. It seems nothing is related with the change. Seems there are more than one flannel instance is running simultaneous in ./dist/functional-test.sh.. @tomdee Currently, flannel listen on udp port 8285 by default, we have no methods to do a health check about the status of flanneld daemon, i.e., whether it is running or stopped accidentally. We have encountered a situation where flanneld has been stopped accidentally for more than two days without any notice, and when we found this and restart it, the newly allocated subnet is different from the original one cause we have not renew the lease about the original subnet. This makes it possible that we have two nodes with the same subnet which is not acceptable.\nSo, the original thought is we need a way to check whether flanneld is running or not. No much other logic. :)\nMaybe you want to add more real checks for the health check endpoint, such as where flanneld can connect to etcd successfully. This is more good. But, for now, we just want to check whether flanneld daemon is running or stopped.. /ping @tomdee . @tomdee Will Do.. @tomdee Done. PTAL.. /ping @tomdee . Friendly ping @tomdee . @tomdee  This health check is disable by default. I have not test whether it can pass all the tests when enabled by default. :). Friendly Ping @tomdee :). @tomdee Many thanks. \ud83d\udc4f . Will do.. Nope. panic because this method is called mustRunHealthz. According to golang convention, we should panic when we can not do some thing that must be done.\nThe reason for why we use mustRunHealthz is because is think we should must start the health check server once we enable it. If not, we should panic or stop flannel to give user more clear result. \n@tomdee WDYT. . ",
    "alapidas": "@tomdee Would there be interest in accepting a contribution that added this feature?. ",
    "dxdeidara": "@tomdee I'd like to know if the latest flannel version v0.9.0 supports etcd v3 now?. ",
    "372046933": "Still not supportted?. At the end of day, what's the exact config of host-gw backend? Can anyone give an example?. The problem seems to be how to process the network record saved in etcd. ",
    "calvix": "Are there any plans for adding support for etcdv3 ever?. > The network stack is left up on purpose, in order to facilitate zero-downtime restart/upgrade\nThis creates a problem when we delete flannel configuration  with specific VNI and later want to use the same VNI with a different network. flannel.x will still have the old network configuration and this broke flannel functionality\nIs there a way how to force remove it?. ",
    "fabifrank": "+1. ",
    "robinsondan87": "Kubectl 1.13 will be dropping any support for the v2 api so we need flannel to support the v3 api, are there any plans to introduce this soon as 1.13 is currently in alpha status with a RC due soon.. ",
    "unteem": "I'm a bit confused here, can someone confirm that compatibility between flannel and kubernetes 1.13 is broken or it sill works? Thanks . ",
    "dannyk81": "@unteem @robinsondan87 FYI --> https://github.com/kubernetes/kubernetes/issues/57354#issuecomment-451679688. ",
    "gufranmmu": "@djsly. ",
    "caseydavenport": "@SydOps You can get network policies with flannel today if you use Canal!\nCheck it out - https://github.com/tigera/canal\nIt runs Calico alongside Flannel for network policy.. This part of the manifest is not supported prior to v1.6 (tolerations were implemented as annotations previously) \ntolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule. This looks good to me.\n@ironlung975 what sort of testing have you done on this?. > usign the toleration without a key worked for me. Would this be the solution?\nThat sounds fine to me - flannel should probably tolerate all NoSchedule taints, since it's a critical piece of infrastructure. \nAnyone want to submit a PR?. ",
    "ozbillwang": "@caseydavenport \nThanks, will take a look. . ",
    "kryzthov": "Hey Tom! Thanks for the feedback. I added a line for the new flag and a small section on the subnet lease duration and timing. Let me know if you think it should be expanded.. ",
    "lxpollitt": "cc @tomdee . ",
    "pingles": "@vaijab: nope. we have our VPC networking configured with a private + public subnet per-AZ, for example: eu-west-1a-public, eu-west-1a-private etc. The public subnets have routes to the internet gateway, the private networks have a 0.0.0.0 route pointing to nat gateways in the equivalent public subnet.\nSo, for Flannel we configure it to manage routes in the private route tables.\nHope that helps? I'm not really an AWS networking guru so I may have messed up some terminology.. Yep, let me take a look now.. So it looks a little more involved than a few straight fixes- it's pretty late here now so I'll take a look at work tomorrow and push an update.. Really sorry but I've not had much time to do this today. I'm also away for the next week on holiday so I probably won't look at it until I get back. If someone else wants to give it a try please do- otherwise I'll do it asap.. Thanks for doing that @t0mmyt! @tomdee any suggestions on how we could improve to be merge-worthy? :). ",
    "t0mmyt": "To expand, if you want to have non internet facing subnets (in an AWS VPC) that have internet access, each subnet requires its own NAT gateway for HA.  This means that each subnet must have its own route table (as each has a different default route) but each route table still needs the full set of routes set by flannel.  Therefore the fix is to support propagating to multiple route tables.. Rebase @ #717 . We've been running this (albeit against an older release) for quite a few months now without any noticeable issue.. f6d7239  on Nov 9 2016.  We've had the current build (prior to the log changes) running on one cluster since this pull request. Done. If you want, I can deploy this build tomorrow morning (it's late here now) and confirm all still works.. Cleaned up and pushed. Cleaned up and pushed. Cleaned up and pushed. Changing to a slice would be a breaking change for existing configurations.  This seemed less disruptive to the users but added some complexity.  I don't mind either way.. I did initially, I caught it and squashed in to the previous commit.  Looks ok on the remote branch now.. ",
    "gunjan5": "@pingles @t0mmyt I've added review comments over at #717  . #717 is merged, so closing this one. Thanks @pingles and @t0mmyt! . @pingles @t0mmyt Curious to know if you have done any level of manual testing of this PR? I'd like to get this PR merged soon, so won't hold it up too long for not having the tests (nice to have) if there is some level of manual testing done. . @t0mmyt which release/commit?. can you squash both commits? . LGTM. @willise I think you're getting that BadRequest error because you haven't specified the container in the pod. you should add the flag -c  kube-flannel to specify the container in the pod (since there are two containers in the pod)\nAlternatively, you can also do a kubectl describe pod <pod-name> -n kube-system to see what's going on in the pod. Thanks @oilbeater! . @tomdee looks mostly good code wise, my only concern is what happens to the subnet entry in etcd if someone upgrades flannel?. @ayaz I plan to review it in the next few days, then it will need some testing before it's merged into master. @tomdee LGTM! I haven't had a chance to manually test this. I'm assuming you have done some testing? lmk if you want me to do some manual testing before the merge. I will likely not have any time this week tho. @Unb0rn seems like it's fixed in this commit https://github.com/coreos/coreos-overlay/commit/e9eeca8d90023441703b18c49e29769299a86348\nand flannel-wrapper is in a different repo :). Thanks, @sakeven for your contribution!. Thanks, @vissible for your contribution! . Thanks, @charsyam for the PR!. close and reopen to trigger the CI. Thanks, @mousavian. Thanks, @phantooom. this comment needs an update. Remove the Run() mention. Thinking out loud here: shouldn't we be able to just remove ctx field from manager struct? Seems like we're not using it anywhere, just closing it here.. wondering if we can just simplify the interface so that RouteTableID is always a slice, even if it has just one route table ID, that way we won't have to do this interface stuff. WDYT? . nit: can you use log.Infof() instead of Sprintf-ing the string? . nit: we can use log.Infof() here. same here, use Infof :). doh! Right, for some reason I thought this was a new field. . Did you forget this one in main.go? https://github.com/coreos/flannel/pull/717/files#diff-7ddfb3e035b42cd70649cc33393fe32cR147. weird, I still see the old log format on your remote branch: https://github.com/t0mmyt/flannel/blob/awsvpc-multiaz-rebase/main.go#L147. before -> after. do you mean subnet of instead of subnet on?. on -> of?. shouldn't the license headers be 2017? . this should be post startup, not pre. Adding a short comment here explaining why we don't monitor lease for kubeSubnetMgr would be useful. WDYT?. Just thinking out loud here, would there be an impact of not specifying the time here? if you leave it blank it will take the zero-value for time.Time which is 0001-01-01 00:00:00 +0000 UTC . any reason why we're using 1.6.1 instead of 1.6.6 which is the latest stable version?. nit: unless this is for consistency, we might want to keep the v in version string as part of K8S_VERSION. do you mean \"Currently this...\" here?. nit: it's a golang best practice to keep all the acronym characters same case in a variable name, but not a big deal . nit: I see flag help strings have mixed capitalization. Some start uppercase some don't. I notice we are not validating the URL/File path before passing it to the subnet mgr, might want to add a quick validation using url.ParseRequestURI and os.Stat so the request fails quickly on the client side instead of going higher up the stack . We might not need to check if the flags are passed or do an InCluster if they're not, BuildConfigFromFlags takes care of that:\n```go\nfunc BuildConfigFromFlags(masterUrl, kubeconfigPath string) (*restclient.Config, error) {\n    if kubeconfigPath == \"\" && masterUrl == \"\" {\n        glog.Warningf(\"Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.\")\n        kubeconfig, err := restclient.InClusterConfig()\n        if err == nil {\n            return kubeconfig, nil\n        }\n        glog.Warning(\"error creating inClusterConfig, falling back to default config: \", err)\n    }\n    return NewNonInteractiveDeferredLoadingClientConfig(\n        &ClientConfigLoadingRules{ExplicitPath: kubeconfigPath},\n        &ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: masterUrl}}).ClientConfig()\n}\n```\nSo the whole else block can also go I think. I might be misunderstanding this, but isn't the whole point of this so flannel can run outside the cluster? How would it get POD_NAME and POD_NAMESPACE env var populated if it's not running in a k8s pod? Or should these be manually set while starting flannel outside a k8s pod? I think Line 93/94 could use a short comment explaining this. why is the threshold set to FATAL? Why not ERROR?. actually k8s v1.7.0 was just released if you want to use that instead :). besides what zbwright said, this whole command should be in code format, not just the URL. > The network in the\nI think it should be Network in code format since it's a keyword. do you think mentioning the container names would help? I see a lot of people trying to do kubectl logs ... without realizing the container names. WDYT?. this comment should be moved to where the imports are. shouldn't you be passing &wg to MonitorLease if you're passing the reference? . same here, I think it should be wg *sync.WaitGroup . entries is linear. what do you mean by a \"decent address\" here? . wonder if we should return here when this fails?. and return here with cleanup from whatever we did in the previous step line 110? wdyt?. I think a better name would be AddFDB since FDB is an acronym\nSee: https://github.com/golang/go/wiki/CodeReviewComments#initialisms (also applies to a bunch of other method names in this file.). Same here, a better name would be DelFDB. a better name would be AddARP . any reason why you have some error logs with log.Error and uppercase error message vs log.Errorf and a lowercase message?. wonder if you can use any of netlink.Route's attributes to determine if it's a directly connected?. how would this condition set directRoutingOK to true for subnet.EventRemoved event? maybe I'm missing something, but when we get the EventAdded we program the directRoute which has Gw:  attrs.PublicIP.ToIP(), then we get EventRemoved for the same one and if we check this condition where we see if Gw==nil, so the clean up in L173 won't happen? . that would be nice but not necessary since your comments are pretty clear. yeah that makes sense, I misunderstood the direct link checking logic and thought directRoutingOk flag will stay false for delete, but it doesn't, so all good now :) . what if we do a latest tag here instead?. how about instead of all these sleep 2, we put the pings in a loop so it retries for 5-10 times to see if pings work, so the tests don't become flakey . or we could put this in some sort of retry block, maybe. btw, there's a cool project garethr/kubeval that can validate the k8s manifests. Not suggesting we should or shouldn't use it, but just throwing it out there. curious, why is this sleep 1 vs 2 in the other test?. i'd still advocate for some sort of ping retry instead of sleep . nit: 2015?. nit: MAINTAINER has been deprecated, we should probably switch to LABEL. any reason we're not using Go 1.9.2? . 2017. context has been move to \"context\" as part of the std lib since Go 1.7. shouldn't this be <= according to the comment?. it should be set to a node's. ",
    "Nick777818": "Any feedback on the following?\nThanks. ",
    "ashishth09": "You should specify the GOPATH. If you already have one then you can git clone so that\nflannel is at $GOAPTH/src/github.com/coreos/flannel\nRun go  get ./... after cd flannel to install all the required packages. Well you can skip go get ./... as the project already has dependencies in the vendor folder which will be made available to go compiler >= 1.6 :). https://blog.gopheracademy.com/advent-2015/vendor-folder/. You should specify the GOPATH.\nYou can run go get ./...  after cd flannel to install all the required packages. Well you can skip go get ./... as the project already has dependencies in the vendor folder which will be made available to go compiler :). https://blog.gopheracademy.com/advent-2015/vendor-folder/. If you already have the GOPATH then the following will make sense\nmkdir -p $GOPATH/src/github.com/coreos\ncd $GOPATH/src/github.com/coreos\ngit clone https://github.com/coreos/flannel.git\ncd flannel; make dist/flanneld\nProbably you cloned at some other path which was not in the GOPATH .\nI think the documentation needs to be more clear on that.. ",
    "mbruzek": "I have confirmed that removing the -it flags allows the flannel build to be run non-interactively.. Thanks @tomdee !. ",
    "abdoo": "I tried from a container to check other nodes gateway and it could be reached\nroot@ingress-nginx-2015555637-t7ln5:/# traceroute 172.16.54.1\ntraceroute to 172.16.54.1 (172.16.54.1), 30 hops max, 60 byte packets\n 1  172.16.55.1 (172.16.55.1)  0.107 ms  0.029 ms  0.018 ms\n 2  172.16.54.1 (172.16.54.1)  0.913 ms  0.783 ms  0.664 ms\nroot@ingress-nginx-2015555637-t7ln5:/# traceroute 172.16.34.1\ntraceroute to 172.16.34.1 (172.16.34.1), 30 hops max, 60 byte packets\n 1  172.16.55.1 (172.16.55.1)  0.047 ms  0.014 ms  0.010 ms\n 2  172.16.34.1 (172.16.34.1)  0.697 ms  0.694 ms  0.648 ms\nroot@ingress-nginx-2015555637-t7ln5:/#. @tomdee simply pods on on other nodes can't be reached. there is any specific requirements for NIC cards?. hi Tom,\nas I mentioned. I have built the cluster on premises. so I need to know if\nthere is any pre-requisites to Flannel to work. and it is okey to use type\nvxlan or I can use host-gw.\nThanks,\nbest regards,\nOn Wed, Jan 11, 2017 at 7:37 PM, Tom Denham notifications@github.com\nwrote:\n\nNot that I'm aware of.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/582#issuecomment-272041204, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAMhz5eaiBb6tujlKmzqlXf3xHFPRk3Uks5rRXXDgaJpZM4LTXA3\n.\n\n\n-- \nThanks\nBest Regards,\nAbdelFattah Mahran\nhttp://www.linkedin.com/in/abdoomahran\n. ",
    "drishticode": "Manually importing dependencies fixed the errors.\ngo get \"github.com/coreos/flannel/backend/alloc\"\ngo get \"github.com/coreos/flannel/backend/awsvpc\"\ngo get \"github.com/coreos/flannel/backend/gce\"\ngo get \"github.com/coreos/flannel/backend/hostgw\"\ngo get \"github.com/coreos/flannel/backend/udp\"\ngo get \"github.com/coreos/flannel/backend/vxlan\"\ngo get \"github.com/coreos/flannel/network\"\ngo get \"github.com/coreos/flannel/remote\"\ngo get \"github.com/coreos/flannel/subnet\"\ngo get \"github.com/coreos/flannel/subnet/kube\"\ngo get \"github.com/coreos/flannel/version\"\ngo get \"github.com/coreos/pkg/flagutil\"\ngo get \"github.com/golang/glog\"\ngo get \"golang.org/x/net/context\"\ngo build -o dist/flanneld \\\n  -ldflags \"-X github.com/coreos/flannel/version.Version=v0.6.1-62-g6d631ba\". @ashishth09 , guess I am still missing something.\nDo you mean specify the GOPATH in make command ? I already have GOPATH in my PATH. Seems to me the problem occurs when everything is installed fresh and GOPATH has nothing to begin with. Shouldn't the build scripts pull these dependencies  automatically ?. Ah I see. You are right about \"Probably you cloned at some other path which was not in the GOPATH .\" . I will test it in a couple of days and update how it goes. Thanks @ashishth09 !. @ashishth09 that worked. I created a symlink inside $GOPATH/src/github.com/coreos to my git clone folder /foo/flannel. \nhttps://github.com/coreos/flannel#building-flannel needs to be updated if this step is expected.. Refer issue #583 \nI cleared everything under $GOPATH, created a symlink inside $GOPATH/src/github.com/coreos to my git clone folder /foo/flannel and then ran make dist/flanneld. Now it runs fine.\n. @nigeldaniels \ncd /foo\ngit clone https://github.com/coreos/flannel.git\nmkdir -p $GOPATH/src/github.com/coreos\nln -s /foo/flannel $GOPATH/src/github.com/coreos/flannel. ",
    "nigeldaniels": "@drishticode what is the destination of the symlink in /foo/flannel ? . ",
    "madhanrm": "https://github.com/kubernetes/kubernetes/issues/35096. ping @tomdee . FYI @nagiesek . @tomdee Integration test is pending. Can we follow that up in a separate PR & get this one reviewed and merged?\n@nagiesek can take care of the review comments.\n. @Tomdee, there also seems to be another PR : https://github.com/coreos/flannel/pull/1042 which is doing similar to this PR. Can we get this PR merged & then work on top if it. The binaries out of this PR has been tested for over an year now. @tomdee Integration test is pending. Can we follow that up in a separate PR & get this one reviewed and merged?\n@ksubrmnn  can follow up to fix the review comments.. @tomdee, there also seems to be another PR : #1042 which is doing similar to this PR. Can we get this PR merged & then work on top if it. The binaries out of this PR has been tested for over an year now. ping @tomdee. @madhanrm. ping @philips. Thanks @philips . @thxCode Please include the contributors of  #921 and #922 in your commit message as co-authors.. /lgtm. /lgtm. Resolved.. @alekssaul  Yes, you are correct. At the exit of the for loop, can you add a check for the availability of providerAddress & remoteDrMac. Fail, with a meaningful error if you do not have them, since you cannot proceed.. ",
    "wksw": "etcd network config is '{\"Network\":\"172.16.0.0/16\", \"Backend\":{\"Type\":\"vxlan\"}}'\nthe normal flannel start option \u2019--etcd-endpoints=\"http://etcd_host_ip:4003\" --iface=\"eth0\"\u2018\nthe unnormal flannel start option  '--etcd-endpoints=\"http://etcd_host_ip:4003\" --public-ip=\"node_ip\"'\nfail log is 'vxlan.go:349] Route for 172.16.88.4 not found'. @tomdee issue solved,Time synchronization problem.. ",
    "jstoja": "@tomdee Would something using kubeadm + the requirements for Flannel + the creation of resources from the latest version of the flannel.yml be good for you?. ",
    "MathiasRenner": "Yes, it would be great if this is merged soon, thanks!. ",
    "kanor1306": "Thanks @jeremyeder! good to know it!. ",
    "andrewrynhard": "@liggitt updated. Is this what you had in mind?. Docs on this taint seem to be non-existant. I believe due to the fact that it is an alpha feature? I have been informed that https://github.com/kubernetes/kubeadm/issues/150 should change this and move to beta.. Ahh yes. Will fix now.. yep, fixing now. ",
    "liggitt": "Looks good to me. rather than hardcoding the kube-system namespace and duplicating the config/pod spec from the main example, I would include the service account in the core kubernetes yml example (without the namespace), make the core example use a bespoke service account, limit this file to the clusterrole with the required permissions, and document the kubectl command to grant the clusterrole to the flannel service account in the desired namespace. serviceAccountName is the non-deprecated field. might want to switch this to v1beta1 for longevity. ",
    "bretagne-peiqi": "@tomdee @YungSang . With smaller cluster, it should be ok. The problem is we have deployed it\nin production env and drop packets from time to time (about 1 days per\nweek), and I think we need to find the exact cause.\n2017-01-20 2:21 GMT+08:00 Tom Denham notifications@github.com:\n\n@peiqi-caicloud https://github.com/peiqi-caicloud are you able to\nreproduce this on a smaller cluster or with the latest release of flannel?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/592#issuecomment-273855762, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFXIItLXTLQXavk23iz1v8w9o_qJr_2vks5rT6m8gaJpZM4Lnzgi\n.\n. flannel is not planning to support multi-network any more ?\n. \n",
    "feisan": "@peiqi-caicloud \nDo you have any new clues?. ",
    "jeremyd": "bueller?. Ok great thanks!. ",
    "lipixun": "I met exactly the same issue, why this issue is closed without any further information or comments? thx.. After I downgrade docker to 1.12.0 (and reboot the system), things go right. I have no idea why 1.13.0 doesn't work.. got it, thanks @adaiguoguo @bobonovski . ",
    "bobonovski": "@lipixun Sorry for closing this issue, because I found out later that it was not the problem of flannel 0.7.0. Currently, I can use flannel 0.7.0 with docker 1.13.0. I fixed the issue like this:\n1. stop docker service\n2. reset iptables\n3. delete docker0 network\n4. reboot\n5. start flannel 0.7.0\n6. start docker 1.13.0 with DOCKER_OPTS pointed to flannel bridge net\nthen it worked!\nso I guess it's all because of the weird behavior of iptables. ",
    "adaiguoguo": "@lipixun \nsudo iptables -P FORWARD ACCEPT fix this.\nDocker change log . sudo iptables -P FORWARD ACCEPT fix this.\nDocker change log . ",
    "daniilyar": "Fix from @adaiguoguo worker for me on K8 1.5.4, Flannel 0.6.2 and Docker 1.13.1. ",
    "squeed": "+1 LGTM. Oh, it's so much worse than that. It depends on the underlying HyperV OS; the networking driver for some versions of Windows support checksum offloading only for TCP (hence breaking vxlan), while newer ones support both TCP and UDP.\nGiven that it's not really a flannel problem, this can probably just be closed.. I suggest also bundling the portmap plugin with the example configuration, and switching to an appropriate .conflist with it enabled.. @osoriano not exactly - Kubernetes prefers conflists over confs, regardless of the sorting.. @osoriano argh, you're completely right.. And I even wrote that code. I was thinking of a different CNI loading shim I wrote for rkt :-).. @klausenbusk The Docker networking code does nothing (noops) if the sysctl net.ipv4.ip_forward is already 1. Users of CoreOS Container Linux (including Tectonic) have this sysctl at boot (before Docker), so it also skips changing the iptables settings.. Switch to nftables :-) ?. ",
    "sugun999": "Hi,\nAfter disabling ipv6, it works now fast.\nHence closing this ticket. ",
    "intsilence": "Same problem here, worked with @adaiguoguo , thanks very much!. ",
    "lklkxcxc": "\u6211\u4e5f\u78b0\u5230\u4e86\u539f\u6765\u662f\u8fd9\u4e2a\u95ee\u9898\u5df2\u89e3\u51b3\u3002. ",
    "cmingxu": "works for me,  should save my hours if saw this earlier, great!. ",
    "uplogin": "Thank you very much @adaiguoguo \nIt works . ",
    "PengTaoWW": "i found miss iptables setting , need add iptable forward table . ",
    "jsravn": "@tomdee Wonder why you closed this? It's a pretty big gap. We recently had an outage because of this and there is no way to monitor flanneld health properly at the moment, as far as I can tell.. @guanjunding did you find anything? I don't think it's a flannel issue btw, most likely something in the vxlan driver drops packets occasionally.. ",
    "fulltopic": "I got the same issue:  fedora 25 KVM/flannel 0.7.0. \nMore information\n\n[root@fed-master ~]# etcdctl get /coreos.com/network/config\n{\n    \"Network\": \"10.16.0.0/16\",\n    \"SubnetLen\": 24,\n    \"Backend\": {\n        \"Type\": \"vxlan\",\n        \"VNI\": 1\n     }\n}\n[root@fed-master ~]# etcdctl get /coreos.com/network/subnets/10.16.28.0-24\n{\"PublicIP\":\"192.168.122.238\",\"BackendType\":\"vxlan\",\"BackendData\":{\"VtepMAC\":\"8e:b1:87:76:66:7d\"}}\nFeb 28 03:24:48 fed-node1 systemd-udevd: Could not generate persistent MAC address for flannel.1: No such file or directory\nFeb 28 03:24:48 fed-node1 systemd: Started Flanneld overlay address etcd agent.\nFeb 28 03:24:48 fed-node1 NetworkManager[19821]:  [1488277488.7594] device (flannel.1): Generated connection does not verify: vxlan.remote: property is missing\nFeb 28 03:24:48 fed-node1 NetworkManager[19821]:  [1488277488.7603] device (flannel.1): Generated connection does not verify: vxlan.remote: property is missing\nFeb 28 03:24:48 fed-node1 NetworkManager[19821]:   [1488277488.7605] device (flannel.1): state change: unavailable -> disconnected (reason 'none') [20 30 0]\nFeb 28 03:24:48 fed-node1 NetworkManager[19821]:  [1488277488.7626] device (flannel.1): Generated connection does not verify: vxlan.remote: property is missing\n[root@fed-node1 ~]# nmcli d\nDEVICE     TYPE      STATE         CONNECTION  \ncni0       bridge    connected     cni0             \nens3       ethernet  connected     Wired connection 1 \nflannel.1  vxlan     disconnected  --               \nlo         loopback  unmanaged     --       \n\nA similar issue with the same error output: flannel container not working w/ vxlan backend\nWhile this issue is about flannel container case.\nBTW: How vxlan device operation is related to /proc/fs and /proc/sys?. ",
    "aveshagarwal": "Not sure which release this change targeted to, but in 1.6, tolerations are going to be api fields https://github.com/kubernetes/kubernetes/issues/38957, and the pr is already merged. . ",
    "davidopp": "Moving taints and tolerations from annotations to fields was announced in these messages on kubernetes-dev\nhttps://groups.google.com/d/msg/kubernetes-dev/-fwfHQtm9GE/H1TU4yqiBAAJ\nhttps://groups.google.com/d/msg/kubernetes-dev/Xq7MI0CvRlc/-4sM_VKtAAAJ\n(and there were identical corresponding messages on kubernetes-users).\n. The fact that some deployment environments automatically add a taint to the master node (and therefore pods that want to run on the master node need to tolerate the taint) is outside the scope of the feature itself but I can see how it might be confusing people.. So IIUC the situation is that flannel is effectively a 1.5 client, running against a 1.6 master, that reads the Node object, makes some changes in memory, and then writes it back (does the Update() function you linked to use PATCH or PUT/POST? May not matter though) and the taints field gets lost because the field exists in 1.6 but not 1.5.\nI think what you described is definitely possible. I'm actually not sure how to fix it (other than upgrading the client, of course).\ncc/ @lavalamp or @xuchao to verify and hopefully provide some suggestion.\n. Here's an example of using Patch() instead of Put():\nhttps://github.com/kubernetes/kubernetes/blob/3b1d2343a84d3b8e3fd21554568e06b8594cdf26/pkg/controller/controller_utils.go#L935\nBut it looks like the diff is done on the client (I was hoping it was done on the server), so I don't think it would solve this problem.\n. And I wonder if 1.6 master with 1.5 kubelet has the same problem when Kubelet updates the Node object to update status (even though taints are in spec, not status, the granularity is the whole object).\n. Great - thanks @wojtek-t !\n. I believe the idea is that you can stick to the 1.5 client if you switch it to use patch, OR you can switch to the 1.6 client. But yeah, you should definitely switch to using patch before 1.7 either way.\n. ",
    "niyanchun": "I have found the problem, iptables drop packet from flannel0 to docker0.. ",
    "RaasAhsan": "Sorry I didn't specify in the original issue, but I'm using etcd as the backend. So, since then, I found the flannel reservations features in 0.7.0, so I've upgraded to that and just set the TTL of the subnet key to 0, and I haven't had any problems since then. Haven't tried out the auto-renew mode on 0.7.0 yet to see if it works, but let me know if you want me to dig into this more.. ",
    "Aleishus": "I met the same problem by using the host-gw backend.I think your guesses is right and fix it soon! @tomdee . ",
    "pgburt": "Added some content and a link to the bottom of the README @philips . ",
    "Grvmm": "Well thanks for explaining that and for the quick reply.  I'll stop by work tomorrow to check it out more but what I noticed on boot right after I log in it gives me something about a service failed to start like\n\nfailed to start -1 : flannel-docker.opts.service \n\nSo when I tried checking the logs and everything seemed to be alright up to the point when flannel starts.   It seemed to me that the first thing it does is try to start the opts service and it fails giving the error along with a long string of stuff I can't remember right now; but at the end of it all it said flannel failed to launch due to dependencies failed to load flannel-docker.opts.service.  Then after that the docker module wont load since flannel isn't running.\nI'll get the exact information later.  I'll see about trying to get the logs off the machines but it's on a enclosed network so it's a bit a pain.. So when I was pulling the logs I can't believe I managed to somehow not see this one section (on the stable build, the alpha which I'm been working with most doesn't have this) it seem to be requesting access to  \"public.update.core-os.net\"(at the very end of log) when it was trying to launch the flannel-docker-opts.service I think.\nI'm not sure what this flannel-docker-opts does but i guess it might require internet connection?  It seemed to me that coreos wouldn't need an internet connection to run?  Reminder I don't have a real internet connection, I could probably setup a proxy if I need it for a one time thing.  Maybe I can just remove that update function from the code?  idk\n``\nflanneld.service - flannel - Network fabric for containers (System Application Container)\n   Loaded: loaded (/usr/lib/systemd/system/flanneld.service; disabled; vendor preset: disabled)\n  Drop-In: /etc/systemd/system/flanneld.service.d\n           +-50-network-config.conf\n   Active: activating (auto-restart) (Result: exit-code) since Sat 2017-03-04 18:56:25 UTC; 7s ago\n     Docs: https://github.com/coreos/flannel\n  Process: 2708 ExecStop=/usr/bin/rkt stop --uuid-file=/var/lib/coreos/flannel-wrapper.uuid (code=exited, status=1/FAILURE)\n  Process: 2680 ExecStart=/usr/lib/coreos/flannel-wrapper $FLANNEL_OPTS (code=exited, status=254)\n  Process: 2672 ExecStartPre=/usr/bin/etcdctl --endpoints http://127.0.0.1:2379 \\ --ca-file /etc/ssl/etcd/ca.pem --cert-file /etc/ssl/etcd/client.pem --key-file /etc/ssl/etcd/client-key.pem \\ set /coreos.com/network/config { \"Network\": \"\n  Process: 2652 ExecStartPre=/usr/bin/rkt rm --uuid-file=/var/lib/coreos/flannel-wrapper.uuid (code=exited, status=254)\n  Process: 2649 ExecStartPre=/usr/bin/mkdir --parents /var/lib/coreos /run/flannel (code=exited, status=0/SUCCESS)\n  Process: 2647 ExecStartPre=/sbin/modprobe ip_tables (code=exited, status=0/SUCCESS)\n Main PID: 2680 (code=exited, status=254)\n    Tasks: 0\n   Memory: 0B\n      CPU: 0\n   CGroup: /system.slice/flanneld.service\n`Mar 04 18:52:53 Kube-MST1 systemd[1]: flanneld.service: Unit entered failed state.\nMar 04 18:52:53 Kube-MST1 systemd[1]: flanneld.service: Failed with result 'exit-code'.\nMar 04 18:52:53 Kube-MST1 systemd[1]: Starting flannel docker export service - Network fabric for containers (System Application Container)...\n-- Subject: Unit flannel-docker-opts.service has begun start-up\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flannel-docker-opts.service has begun starting up.\nMar 04 18:52:53 Kube-MST1 rkt[2377]: rm: unable to resolve UUID from file: open /var/lib/coreos/flannel-wrapper2.uuid: no such file or directory\nMar 04 18:52:53 Kube-MST1 rkt[2377]: rm: failed to remove one or more pods\nMar 04 18:52:53 Kube-MST1 sudo[2393]:     core : TTY=pts/0 ; PWD=/home/core ; USER=root ; COMMAND=/bin/systemctl start docker\nMar 04 18:52:53 Kube-MST1 sudo[2393]: pam_unix(sudo:session): session opened for user root by core(uid=0)\nMar 04 18:52:53 Kube-MST1 sudo[2393]: pam_systemd(sudo:session): Cannot create session: Already running in a session\nMar 04 18:52:53 Kube-MST1 flannel-wrapper[2396]: + exec /usr/bin/rkt run --uuid-file-save=/var/lib/coreos/flannel-wrapper2.uuid --trust-keys-from-https --volume ssl,kind=host,source=/etc/ssl/etcd,readOnly=true --mount volume=ssl,target=/\nMar 04 18:53:03 Kube-MST1 systemd[1]: flanneld.service: Service hold-off time over, scheduling restart.\nMar 04 18:53:03 Kube-MST1 systemd[1]: Stopped flannel - Network fabric for containers (System Application Container).\n-- Subject: Unit flanneld.service has finished shutting down\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flanneld.service has finished shutting down.\nMar 04 18:53:03 Kube-MST1 systemd[1]: Starting flannel - Network fabric for containers (System Application Container)...\n-- Subject: Unit flanneld.service has begun start-up\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flanneld.service has begun starting up.\nMar 04 18:53:03 Kube-MST1 rkt[2435]: rm: unable to resolve UUID from file: open /var/lib/coreos/flannel-wrapper.uuid: no such file or directory\nMar 04 18:53:03 Kube-MST1 rkt[2435]: rm: failed to remove one or more pods\nMar 04 18:53:03 Kube-MST1 etcdctl[2447]: No help topic for '\\ --ca-file'\nMar 04 18:53:03 Kube-MST1 flannel-wrapper[2455]: + exec /usr/bin/rkt run --uuid-file-save=/var/lib/coreos/flannel-wrapper.uuid --trust-keys-from-https --volume ssl,kind=host,source=/etc/ssl/etcd,readOnly=true --mount volume=ssl,target=/e\nMar 04 18:53:54 Kube-MST1 systemd[1]: flannel-docker-opts.service: Start operation timed out. Terminating.\nMar 04 18:53:54 Kube-MST1 systemd[1]: Failed to start flannel docker export service - Network fabric for containers (System Application Container).\n-- Subject: Unit flannel-docker-opts.service has failed\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flannel-docker-opts.service has failed.\n--\n-- The result is failed.\nMar 04 18:53:54 Kube-MST1 sudo[2393]: pam_unix(sudo:session): session closed for user root\nMar 04 18:53:54 Kube-MST1 systemd[1]: Dependency failed for flannel - Network fabric for containers (System Application Container).\n-- Subject: Unit flanneld.service has failed\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flanneld.service has failed.\n--\n-- The result is dependency.\nMar 04 18:53:54 Kube-MST1 systemd[1]: Dependency failed for Docker Application Container Engine.\n-- Subject: Unit docker.service has failed\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit docker.service has failed.\n--\n-- The result is dependency.\nMar 04 18:53:54 Kube-MST1 systemd[1]: docker.service: Job docker.service/start failed with result 'dependency'.\nMar 04 18:53:54 Kube-MST1 systemd[1]: flanneld.service: Job flanneld.service/start failed with result 'dependency'.\nMar 04 18:53:54 Kube-MST1 systemd[1]: flannel-docker-opts.service: Unit entered failed state.\nMar 04 18:53:54 Kube-MST1 systemd[1]: flannel-docker-opts.service: Failed with result 'timeout'.\n-- Unit flanneld.service has begun starting up.\nMar 04 18:59:17 Kube-MST1 rkt[3053]: rm: unable to resolve UUID from file: open /var/lib/coreos/flannel-wrapper.uuid: no such file or directory\nMar 04 18:59:17 Kube-MST1 rkt[3053]: rm: failed to remove one or more pods\nMar 04 18:59:18 Kube-MST1 etcdctl[3065]: No help topic for '\\ --ca-file'\nMar 04 18:59:18 Kube-MST1 flannel-wrapper[3072]: + exec /usr/bin/rkt run --uuid-file-save=/var/lib/coreos/flannel-wrapper.uuid --trust-keys-from-https --volume ssl,kind=host,source=/etc/ssl/etcd,readOnly=true --mount volume=ssl,target=/e\nMar 04 18:59:26 Kube-MST1 update_engine[1219]: I0304 18:59:26.637755  1219 libcurl_http_fetcher.cc:48] Starting/Resuming transfer\nMar 04 18:59:26 Kube-MST1 update_engine[1219]: I0304 18:59:26.637941  1219 libcurl_http_fetcher.cc:164] Setting up curl options for HTTPS\nMar 04 18:59:46 Kube-MST1 systemd[1]: flannel-docker-opts.service: Start operation timed out. Terminating.\nMar 04 18:59:46 Kube-MST1 systemd[1]: Failed to start flannel docker export service - Network fabric for containers (System Application Container).\n-- Subject: Unit flannel-docker-opts.service has failed\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flannel-docker-opts.service has failed.\n--\n-- The result is failed.\nMar 04 18:59:46 Kube-MST1 systemd[1]: Dependency failed for flannel - Network fabric for containers (System Application Container).\n-- Subject: Unit flanneld.service has failed\n-- Defined-By: systemd\n-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel\n--\n-- Unit flanneld.service has failed.\n-- The result is dependency.\nMar 04 18:59:46 Kube-MST1 sudo[3038]: pam_unix(sudo:session): session closed for user root\nMar 04 18:59:46 Kube-MST1 systemd[1]: flanneld.service: Job flanneld.service/start failed with result 'dependency'.\nMar 04 18:59:46 Kube-MST1 systemd[1]: flannel-docker-opts.service: Unit entered failed state.\nMar 04 18:59:46 Kube-MST1 systemd[1]: flannel-docker-opts.service: Failed with result 'timeout'.\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: E0304 18:59:54.554065  1219 libcurl_http_fetcher.cc:243] Unable to get http response code: Couldn't resolve host 'public.update.core-os.net'\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554150  1219 libcurl_http_fetcher.cc:274] Transfer resulted in an error (0), 0 bytes downloaded\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554159  1219 omaha_request_action.cc:592] Omaha request response:\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: E0304 18:59:54.554164  1219 omaha_request_action.cc:606] Omaha request network transfer failed.\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554209  1219 action_processor.cc:68] ActionProcessor::ActionComplete: OmahaRequestAction action failed. Aborting processing.\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554217  1219 action_processor.cc:73] ActionProcessor::ActionComplete: finished last action of type OmahaRequestAction\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554225  1219 update_attempter.cc:290] Processing Done.\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: E0304 18:59:54.554252  1219 update_attempter.cc:603] Update failed.\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554265  1219 utils.cc:599] Converting error code 2000 to kActionCodeOmahaErrorInHTTPResponse\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554276  1219 payload_state.cc:97] Updating payload state for error code: 37 (kActionCodeOmahaErrorInHTTPResponse)\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554285  1219 payload_state.cc:103] Ignoring failures until we get a valid Omaha response.\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554399  1219 action_processor.cc:36] ActionProcessor::StartProcessing: OmahaRequestAction\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554437  1219 omaha_request_action.cc:245] Posting an Omaha request to https://public.update.core-os.net/v1/update/\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554445  1219 omaha_request_action.cc:246] Request: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: \nMar 04 18:59:54 Kube-MST1 update_engine[1219]:     \nMar 04 18:59:54 Kube-MST1 update_engine[1219]:     \nMar 04 18:59:54 Kube-MST1 update_engine[1219]:     \nMar 04 18:59:54 Kube-MST1 update_engine[1219]: \nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554451  1219 libcurl_http_fetcher.cc:48] Starting/Resuming transfer\nMar 04 18:59:54 Kube-MST1 update_engine[1219]: I0304 18:59:54.554512  1219 libcurl_http_fetcher.cc:164] Setting up curl options for HTTPS\nMar 04 18:59:54 Kube-MST1 locksmithd[1944]: LastCheckedTime=0 Progress=0 CurrentOperation=\"UPDATE_STATUS_\n```. Just an update, I opened up the firewall for that site and it didn't fix anything.... ",
    "genevieve": "@rosenhouse ^^^. ",
    "Dieken": "bad network from container on node01 to container on node04, iptables trace in /var/log/syslog on node04:\n```\nMar 12 17:21:14 node04 kernel: [ 3452.909716] TRACE: raw:PREROUTING:policy:2 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909724] TRACE: nat:PREROUTING:rule:1 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909732] TRACE: nat:KUBE-SERVICES:return:6 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909736] TRACE: nat:PREROUTING:policy:3 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909742] TRACE: filter:FORWARD:rule:1 IN=flannel.1 OUT=cni0 MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909746] TRACE: filter:DOCKER-ISOLATION:return:1 IN=flannel.1 OUT=cni0 MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909750] TRACE: filter:FORWARD:policy:6 IN=flannel.1 OUT=cni0 MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909753] TRACE: nat:POSTROUTING:rule:1 IN= OUT=cni0 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909756] TRACE: nat:KUBE-POSTROUTING:return:2 IN= OUT=cni0 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909759] TRACE: nat:POSTROUTING:rule:3 IN= OUT=cni0 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909761] TRACE: nat:POSTROUTING:policy:6 IN= OUT=cni0 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=62 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909799] TRACE: raw:PREROUTING:policy:2 IN=cni0 OUT= PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909808] TRACE: filter:FORWARD:rule:1 IN=cni0 OUT=flannel.1 PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909812] TRACE: filter:DOCKER-ISOLATION:return:1 IN=cni0 OUT=flannel.1 PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node04 kernel: [ 3452.909816] TRACE: filter:FORWARD:policy:6 IN=cni0 OUT=flannel.1 PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1\nMar 12 17:21:20 node04 kernel: [ 3458.914902] TRACE: raw:OUTPUT:policy:2 IN= OUT=cni0 SRC=172.16.3.1 DST=172.16.3.12 LEN=112 TOS=0x00 PREC=0xC0 TTL=64 ID=11995 PROTO=ICMP TYPE=3 CODE=1 [SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1 ]\nMar 12 17:21:20 node04 kernel: [ 3458.914925] TRACE: filter:OUTPUT:rule:1 IN= OUT=cni0 SRC=172.16.3.1 DST=172.16.3.12 LEN=112 TOS=0x00 PREC=0xC0 TTL=64 ID=11995 PROTO=ICMP TYPE=3 CODE=1 [SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1 ]\nMar 12 17:21:20 node04 kernel: [ 3458.914934] TRACE: filter:KUBE-SERVICES:return:1 IN= OUT=cni0 SRC=172.16.3.1 DST=172.16.3.12 LEN=112 TOS=0x00 PREC=0xC0 TTL=64 ID=11995 PROTO=ICMP TYPE=3 CODE=1 [SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1 ]\nMar 12 17:21:20 node04 kernel: [ 3458.914940] TRACE: filter:OUTPUT:rule:2 IN= OUT=cni0 SRC=172.16.3.1 DST=172.16.3.12 LEN=112 TOS=0x00 PREC=0xC0 TTL=64 ID=11995 PROTO=ICMP TYPE=3 CODE=1 [SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1 ]\nMar 12 17:21:20 node04 kernel: [ 3458.914947] TRACE: filter:KUBE-FIREWALL:return:2 IN= OUT=cni0 SRC=172.16.3.1 DST=172.16.3.12 LEN=112 TOS=0x00 PREC=0xC0 TTL=64 ID=11995 PROTO=ICMP TYPE=3 CODE=1 [SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1 ]\nMar 12 17:21:20 node04 kernel: [ 3458.914954] TRACE: filter:OUTPUT:policy:3 IN= OUT=cni0 SRC=172.16.3.1 DST=172.16.3.12 LEN=112 TOS=0x00 PREC=0xC0 TTL=64 ID=11995 PROTO=ICMP TYPE=3 CODE=1 [SRC=172.16.3.12 DST=172.16.0.7 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=27538 PROTO=ICMP TYPE=0 CODE=0 ID=492 SEQ=1 ]\n```\ntrace on node01,  no ICMP reply:\n```\nMar 12 17:21:14 node01 kernel: [ 3453.096969] TRACE: raw:PREROUTING:policy:2 IN=cni0 OUT= PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.096976] TRACE: nat:PREROUTING:rule:1 IN=cni0 OUT= PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.096985] TRACE: nat:KUBE-SERVICES:return:6 IN=cni0 OUT= PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.096989] TRACE: nat:PREROUTING:policy:3 IN=cni0 OUT= PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097006] TRACE: filter:FORWARD:rule:1 IN=cni0 OUT=flannel.1 PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097011] TRACE: filter:DOCKER-ISOLATION:return:1 IN=cni0 OUT=flannel.1 PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097014] TRACE: filter:FORWARD:policy:6 IN=cni0 OUT=flannel.1 PHYSIN=vethf912d941 MAC=0a:58:ac:10:00:01:0a:58:ac:10:00:07:08:00 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097017] TRACE: nat:POSTROUTING:rule:2 IN= OUT=flannel.1 PHYSIN=vethf912d941 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097021] TRACE: nat:KUBE-POSTROUTING:return:2 IN= OUT=flannel.1 PHYSIN=vethf912d941 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097023] TRACE: nat:POSTROUTING:rule:3 IN= OUT=flannel.1 PHYSIN=vethf912d941 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\nMar 12 17:21:14 node01 kernel: [ 3453.097026] TRACE: nat:POSTROUTING:policy:6 IN= OUT=flannel.1 PHYSIN=vethf912d941 SRC=172.16.0.7 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=56818 DF PROTO=ICMP TYPE=8 CODE=0 ID=492 SEQ=1\n```\ngood network from node01 host to container on node04, iptables trace in /var/log/system on node04:\nMar 12 17:26:36 node04 kernel: [ 3774.709092] TRACE: raw:PREROUTING:policy:2 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709099] TRACE: nat:PREROUTING:rule:1 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709107] TRACE: nat:KUBE-SERVICES:return:6 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709154] TRACE: nat:PREROUTING:policy:3 IN=flannel.1 OUT= MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709162] TRACE: filter:FORWARD:rule:1 IN=flannel.1 OUT=cni0 MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709166] TRACE: filter:DOCKER-ISOLATION:return:1 IN=flannel.1 OUT=cni0 MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709169] TRACE: filter:FORWARD:policy:6 IN=flannel.1 OUT=cni0 MAC=fe:bf:08:13:d7:4a:ea:f6:36:35:fd:f9:08:00 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709172] TRACE: nat:POSTROUTING:rule:1 IN= OUT=cni0 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709176] TRACE: nat:KUBE-POSTROUTING:return:2 IN= OUT=cni0 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709178] TRACE: nat:POSTROUTING:rule:3 IN= OUT=cni0 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709181] TRACE: nat:POSTROUTING:policy:6 IN= OUT=cni0 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709218] TRACE: raw:PREROUTING:policy:2 IN=cni0 OUT= PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709226] TRACE: filter:FORWARD:rule:1 IN=cni0 OUT=flannel.1 PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709230] TRACE: filter:DOCKER-ISOLATION:return:1 IN=cni0 OUT=flannel.1 PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node04 kernel: [ 3774.709234] TRACE: filter:FORWARD:policy:6 IN=cni0 OUT=flannel.1 PHYSIN=veth63e31496 MAC=0a:58:ac:10:03:01:0a:58:ac:10:03:0c:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\ntrace on node01, got ICMP reply:\nMar 12 17:26:36 node01 kernel: [ 3774.885906] TRACE: raw:OUTPUT:policy:2 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885914] TRACE: nat:OUTPUT:rule:1 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885919] TRACE: nat:KUBE-SERVICES:return:6 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885922] TRACE: nat:OUTPUT:policy:3 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885925] TRACE: filter:OUTPUT:rule:1 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885928] TRACE: filter:KUBE-SERVICES:return:1 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885931] TRACE: filter:OUTPUT:rule:2 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885934] TRACE: filter:KUBE-FIREWALL:return:2 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885936] TRACE: filter:OUTPUT:policy:3 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885939] TRACE: nat:POSTROUTING:rule:2 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885942] TRACE: nat:KUBE-POSTROUTING:return:2 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885945] TRACE: nat:POSTROUTING:rule:3 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.885948] TRACE: nat:POSTROUTING:policy:6 IN= OUT=flannel.1 SRC=172.16.0.0 DST=172.16.3.12 LEN=84 TOS=0x00 PREC=0x00 TTL=64 ID=40486 DF PROTO=ICMP TYPE=8 CODE=0 ID=27983 SEQ=1 UID=0 GID=0\nMar 12 17:26:36 node01 kernel: [ 3774.897055] TRACE: raw:PREROUTING:policy:2 IN=flannel.1 OUT= MAC=ea:f6:36:35:fd:f9:fe:bf:08:13:d7:4a:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node01 kernel: [ 3774.897063] TRACE: filter:INPUT:rule:1 IN=flannel.1 OUT= MAC=ea:f6:36:35:fd:f9:fe:bf:08:13:d7:4a:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node01 kernel: [ 3774.897067] TRACE: filter:KUBE-FIREWALL:return:2 IN=flannel.1 OUT= MAC=ea:f6:36:35:fd:f9:fe:bf:08:13:d7:4a:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1\nMar 12 17:26:36 node01 kernel: [ 3774.897070] TRACE: filter:INPUT:policy:2 IN=flannel.1 OUT= MAC=ea:f6:36:35:fd:f9:fe:bf:08:13:d7:4a:08:00 SRC=172.16.3.12 DST=172.16.0.0 LEN=84 TOS=0x00 PREC=0x00 TTL=63 ID=31680 PROTO=ICMP TYPE=0 CODE=0 ID=27983 SEQ=1. The network topology, probably not very accurate, especially the relationship between veth and cni0, but it should be enough to understand the network data flow.\n\n. I got it. When flanneld on node04 stopped and couldn't start because kubernetes apiserver couldn't work without Etcd up,  there was nobody(it was flanneld on node04) to automatically inject ARP table of flannel vxlan interface on node04 with node01's POD IPs to node01's flannel vxlan interface's MAC. So all PODs on nodes except node04 couldn't be reached from node04 due to ARP miss.  This can be confirmed by this command on node04:\nsudo arp -i flannel.1 -s 172.16.0.3  MAC-of-flannel.1-on-node01\nThen ping from 172.16.0.3 to 172.16.3.10 works.. I feel it's better flanneld checks bridge fdb and subnet lease before it exits due to broken k8s apiserver.  If the fdb and subnet lease are valid, flanneld can do its best to keep injecting ARP table.. @tomdee \nThank you very much!!! That's so awesome!!! I just verified,  flanneld now injects permanet ARP table entries for each pod subnets of other nodes, so exit of flanneld won't affect the communication among pods any more.\n\nI have 8 nodes, the picture was captured from a node with pod subnet 172.29.2.0/24.\nroot@k8s-dev-a04:~# uname -a\nLinux k8s-dev-a04 4.4.0-98-generic #121-Ubuntu SMP Tue Oct 10 14:24:03 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nroot@k8s-dev-a04:~# lsb_release -a\nLSB Version:    core-9.20160110ubuntu0.2-amd64:core-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-amd64:security-9.20160110ubuntu0.2-noarch\nDistributor ID: Ubuntu\nDescription:    Ubuntu 16.04.3 LTS\nRelease:    16.04\nCodename:   xenial. @tomdee   sure, https://github.com/coreos/flannel/pull/849\n. @tomdee ,  since flanneld is running per node, and most clusters don't frequently create and destroy PODs, I guess 10m cpu and 50Mi memory  are enough for small scale(~10 nodes) and medium scale(~100 nodes) Kubernetes clusters.  I believe the administrators of large scale cluster are able to optimize these parameters :-D\nMaybe CoreOS has some test clusters to confirm the  appropriate resource limits,  or survey in some flannel mailing list.\nBTW, I did encounter Flanneld being OOM-killed by Linux kernel although it doesn't consume too much memory.. Maybe 50m cpu and 100Mi memory is safer, I'm not sure about the best numbers, need some benchmark and survey.. @tomdee  Thank you very much for your testing,  I just updated my PR and changed all the yamls in Documentation/.. In commit 014b2d52dfdc5008f5671c9aed4c5c9ebc34db80, @osoriano changed this line in Documentation/kube-flannel.yml:\n@@ -100,7 +110,7 @@ spec:\n         args:\n         - -f\n         - /etc/kube-flannel/cni-conf.json\n-        - /etc/cni/net.d/10-flannel.conf\n+        - /etc/cni/net.d/10-flannel.conflist\n         volumeMounts:\n         - name: cni\n           mountPath: /etc/cni/net.d\nIs this a mistake?\n. @osoriano Sorry I didn\u2019t know the difference between .conf and .conflist, I thought it was a typo, hadn\u2019t upgraded to the latest kube-flannel.yml.\nBTW, is that fix required to other yaml files in Documentation/ ? And maybe the initContainer need delete 10-flannel.conf for a smooth upgrade.. @tomdee could you review the PR and merge it?  The discussion in the last three comments are irrelated.. @ttarczynski  I guess recent releases of flannel consume more memory,  flannel-0.10 changed a lot on vxlan backend.  I also got this OOMKilled issue and I increased its memory limit from 50Mi to 100Mi.  You are free to increase the memory limit, the original purpose in #855 is to explicitly set resource requests and limits to get QoS class \"Guaranteed\".  But better not increase too much, it's fine to oom-kill flannel if it's abnormal,  network connectivity won't be affected if flanneld restarts quickly.\n@tomdee  maybe it's time to increase the default memory limit? or better to profile why it consumes so much memory?  My cluster has less than ten k8s worker nodes.. @ttarczynski  I double checked my clusters, luckily flannel pods didn't restart due to OOMKilled, rarely they restarted with error code 255, possibly due to some kube-apiserver error.\nBecause we both have small scale cluster,  I doubt flanneld-0.10 introduced some defect.\nBTW, you may try old version, I use k8s-1.8.13 + flannel-0.9.0 and k8s-1.9.8 + flannel-0.9.1, both with 100Mi memory limit for flannel, and actual memory usage below 30Mi when I just checked.. ",
    "roffe": "Looking forward to this hitting upstream and have started testing the custom images discussed here in my dev env. will report back if i run into any issues.\nThis is awesome btw!. correct me if i'm wrong, but isn't the idea you mount the CA's from HOST into flannel so you don't have to build them into the image.\nlike the flannel-wrapper does in coreOS\ntext\n        --volume usr-share-certs,kind=host,source=/usr/share/ca-certificates,readOnly=true \\\n        --mount volume=usr-share-certs,target=/usr/share/ca-certificates \\. https://github.com/containernetworking/plugins/pull/75 originates from https://github.com/kubernetes/kubernetes/issues/40182 I belive. This sounds like a xtables lock is not being released. ",
    "klausenbusk": "What exactly is the status of this? Are we just waiting on ?. > But templating dockerfiles seems like an overkill. What's you opinion?\nWith a new enough version of Docker, we can create multiple images from a single Dockerfile. Maybe worth a shot?\nSee: https://adilsoncarvalho.com/creating-multiple-images-from-a-single-dockerfile-3f69254b6137. > But it looks like it produces one image and let's us checkpoint state during one build, so it won't be useful for this particular situation.\nYou can use the --target option to specific which stage to build.\nSomething like this, could probably work:\n```\nFROM frolvlad/alpine-glibc AS base\nMAINTAINER Tom Denham tom@tigera.io\nENV FLANNEL_ARCH=amd64\nRUN apk add --no-cache iproute2 net-tools ca-certificates && update-ca-certificates\nFlannel\nFROM base AS flannel\nCOPY dist/flanneld-$FLANNEL_ARCH /opt/bin/flanneld\nCOPY dist/iptables-$FLANNEL_ARCH /usr/local/bin/iptables\nCOPY dist/mk-docker-opts.sh /opt/bin/\nENTRYPOINT [\"/opt/bin/flanneld\"]\nipsec\nFROM base AS ipsec\nRUN mkdir /var/run || echo /var/run exists\nCOPY dist/strongswan-$FLANNEL_ARCH  /usr/local/\nCOPY dist/libatomic.so.1-$FLANNEL_ARCH /lib/libatomic.so.1\nCOPY dist/libdl.so.2-$FLANNEL_ARCH /lib/libdl.so.2\nENTRYPOINT [\"/opt/flannel/libexec/ipsec/charon\"]\nand build like so:\ndocker build --target flannel --name flannel\ndocker build --target ipsec --name ipsec\n```. I'm wondering if this will be superseded by a Wireguard backend at some point. Wireguard isn't upstreamed yet, but at lot can happen in 6-18 months.\nWireguard seems way easier to work with than IPSEC (and all the packing logic) , and someone could probably tweak the ipip backend to use Wireguard.\nJust my two cents... > Wireguard seems way easier to work with than IPSEC (and all the packing logic)\nI think implementing could be as simple as (pseudo code):\n```\nWG_PRIVATE_KEY_PATH=\"/foo/wg.key\"\nif not exist $WG_PRIVATE_KEY_PATH; then\n    wg genkey > $WG_PRIVATE_KEY_PATH\n    set flannel.alpha.coreos.com/wg-pubkey $(wg pubkey < $WG_PRIVATE_KEY_PATH)\nfi\nip link add dev wg0 type wireguard\nip address add dev wg0  10.2.0.1)>\nfor externalIP, node.podCIDR, flannel.alpha.coreos.com/wg-pubkey in ; do\n    wg set wg0 peer  allowed-ips  endpoint \n    ip route add  dev wg0\ndone\n```. > Indeed doing this with WireGuard means it's a tiny shell script, as opposed to a massive cludge.\nBut at least all the kernel modules is already in place. Compiling and loading the WG kernel module on CoreOS isn't complicated [1] [2], but how do you automate the compiling? A shell script which run on every boot seems to be the only solution, but that feels a bit hackish, and it would need to block kubelet/flannel from starting until the kernel module is ready. Another solution is torcx, but that isn't ready yet. > Ensure it's shipped for the kernel with the package manager\nCoreOS contains no package manager.\n\nensure it's built into the kernel,\n\nCoreOS does not ship out-of-tree modules. See https://github.com/coreos/bugs/issues/2225\n\nor just use dkms\n\ndkms isn't available in CoreOS..\nSo.. Limited by the OS.. > Interesting idea, but this is not currently supported.\nIt seems to work:\nOn the outgoing node:\nip addr add 10.2.5.1 dev flannel.1\nOn the rest of the nodes:\n```\nip addr add 10.2.0.0/16 dev flannel.1\nip route add 216.239.32.21 via 10.2.5.1 # I have also tested with \niptables -t nat -I POSTROUTING -j MASQUERADE\ncurl 216.239.32.21 -H \"Host: ipinfo.io\"\nTraffic is routed through outgoing node.\n```\nI'm wondering if I can do this in a less hackish-way. I need to add 10.2.0.0/16 to all flannel.1 interface, and need some logic to route the traffic over another node if 10.2.5.1 goes down.\n@tomdee Do you have any ideas? . Are you using flannel v0.8.0? It contain this fix: https://github.com/coreos/flannel/issues/719 which seems related.. Just a question, but do you know how Tectonic works around this? Do they just add the iptables rule from some script?. > @klausenbusk The Docker networking code does nothing (noops) if the sysctl net.ipv4.ip_forward is already 1. Users of CoreOS Container Linux (including Tectonic) have this sysctl at boot (before Docker), so it also skips changing the iptables settings.\nThanks, I didn't knew that. Then I'm not worried about upgrading to docker 17.x.. > maybe https://github.com/coreos/flannel-cni could be updated to install the portmap plugin\nhttps://github.com/coreos/flannel-cni already install the portmap plugin and is used by bootkube where the portmap plugin is enabled.. > I was checking with the netstat -lptn command and it was not showing me open ports, I think because they are in the different network namespace. Accessing the host on the hostPort from the outside works as expected.\nportmap works by creating a iptables rule.. Check with iptables-save | grep <port> or something like that.. Nice work :) A bit sad though that installing the Wireguard kernel module on CoreOS is a mess.. Why port 8285 and not 51820 (which is the default WG port (?))?. ",
    "dcowden": "FWIW, we need this functionality. We use weave because it supports encryption ( a requirement for us). We think flannel would be much more stable, and we'd love to switch.. @mkutsevol  that's awesome! we'll be happy to be testers when you need that!. ",
    "RRAlex": "Is there any chance Wireguard could be used / integrated as a swappable lower layer of encryption for the overlay?\nSeems to be moving forward as a simple, fast and audited (secure) VPN solutions.\nSome people seems to be starting to think about it:\n\nhttps://github.com/hobby-kube/guide#wireguard-setup\nhttps://github.com/linuxkit/linuxkit/blob/master/docs/wireguard.md\n...\nIn the long term I think a common solution to this problem would make a lot of sense, especially because we're talking about crypto primitives here, and Wireguard seems to have nice properties (fast, noise protocol, modern algorithms, ...).. \n",
    "JasonGiedymin": "Keep up the good work. Either impl is desirable over nothing.. ",
    "zx2c4": "Indeed doing this with WireGuard means it's a tiny shell script, as opposed to a massive cludge. Let me know if you guys need help doing this.  . > but how do you automate the compiling? \nEnsure it's shipped for the kernel with the package manager, ensure it's built into the kernel, or just use dkms, which is what's done on ubuntu/debian/arch/fedora/etc.. >>ensure it's built into the kernel,\n\nCoreOS does not ship out-of-tree modules. See coreos/bugs#2225\n\nShucks. I guess you could make a \"policy exception\" if you wanted. I'll reiterate that over in the bug you linked, though, so as not to clutter this one.\nSo, I guess you can rig up a wireguard-autobuilder shell script then, like you originally suggested? Hate to come to that, but if that's our only solution.... Great to hear you all are adding WireGuard support to CoreOS. Let me know if I can be of any assistance.. Indeed 51820 is preferred.. ",
    "danehans": "@euank thanks for your help. Looks like it was a network config issue.. @dimhoLt I'm sorry, but I can't recall the details of my network issue.. Thanks for the information. I am using the v3.1.0 etcd3 release. I'll try v3.1.5. I am using flannel v0.6.2. Let me know if I need to upgrade flannel as well.\nI am trying to use the matchbox examples to back a Kubernetes/Flannel cluster with etcd3.\ncc/ @dghubble. I found my issue to be flanneld's interface discovery mechanism:\nflannel-wrapper[2672]: I0411 20:01:55.284739 02672 manager.go:163] Using x.x.x.x as external interface\nflannel-wrapper[2672]: I0411 20:01:55.284766 02672 manager.go:164] Using x.x.x.x as external endpoint\nMy k8s nodes have multiple interfaces. Flannel binds to the interface associated to the host's default route. In my instance, a node's public interface has the default route and was being used by flanneld instead of the private interface used for node<>node communication (i.e. pod networking). \nI fixed the issue by specifying the --FLANNELD_IFACE= private interface in flanenl.env.\nThanks for your help.. ",
    "dimhoLt": "@danehans Could you elaborate on the network issue? I'm having problems creating the second EC2 instance. I'm using kube-aws, but if I have more than one etcd, the first one will create successfully and the second one will fail, similarly to yours. If I have only one etcd instance, the first controller will fail to create.. ",
    "tamilmani1989": "Found out the reason for this issue https://github.com/coreos/flannel/issues/434. ",
    "cmluciano": "cc @tomdee . I think the current deploy file also needs updated to run in the kube-system namespace. If this should run on every node, I would also need to add the taint bypass lines so that it will be deployed to the Kube controller.. Not yet, I added the namespace to the relevant sections in both, but I\u2019m getting errors about the cni plugin not updating the pod network correctly. ",
    "ryzasss": "got same problem. ",
    "guirish": "As s390x uses Big endian format, I saw all routing table IP's in reverse order due to changes in byte alignment in C code. I am able to pass the test cases by removing networkOrder function calls in cproxy.go.\nI am currently looking for a fix either by conditionally checking endianness.. ",
    "teemow": "I've been able to reproduce the error. It has been related to the experimental server/client model. This has been removed in master already. So I guess this can be closed now. Let's go with the kube subnet manager :tada: . ",
    "geerlingguy": "Another +1 here\u2014it was totally non-obvious why applying the default kube-flannel.yml wasn't working on my Raspberry Pis, until a bunch of searching got me to this issue (all I had to go on was the errors below from kubelet).\nMay 24 21:34:19 kube1.pidramble.com kubelet[10218]: W0524 21:34:19.036909   10218 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d\nMay 24 21:34:19 kube1.pidramble.com kubelet[10218]: E0524 21:34:19.037327   10218 kubelet.go:2125] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized. I can confirm as well\u2014on 1.11.3 the configuration applies correctly. On 1.12.0 it does not.. ",
    "wojtek-t": "I think that using PATCH would solve the problem.\nThe diff on the client-side is done to compute the actual patch (that is the send to apiserver). If the client doesn't understand some field, it shouldn't have it neither in the \"original\" object not in \"patched\" one. So the patch should actually be correct.\nAnd when it is applied in apiserver, apiserver understands all fields, so the patched object should be exactly as expected.\nThat should also answer the question that 1.5 kubelet + 1.6 master should work (because kubelet is using PATCH).\nSo I guess the medium term solution is to migrate flannel (and canal whatever that is) to use PATCH. Though I don't have any good solution for what to do now (other than upgrading it to 1.6 too).. ",
    "lavalamp": "This is a longstanding problem in our client, that we delete fields the we don't understand. (Although even if we fix it by default there, users will still be able to get it wrong. But we're currently broken by default.)\nUsing PATCH is a pretty thorough fix.. ",
    "diegs": "I believe this is fixed by https://github.com/coreos/flannel/pull/681.\nThere's also https://github.com/coreos/flannel/pull/677 to move to client-go.\n(Big ups to @mikedanese for putting both of those together). I built flannel using this PR and did a fair bit of testing, both in host-pod and host-host networking as well as making sure that flannel correctly preserved the taints. Everything LGTM.. Also hand-tested this one: launched a node using --register-with-taints=key=value:PreferNoSchedule. Verified that after flannel configured the node the taint was still present. Did a bunch of host-pod and pod-pod network checks and all looked good.. ",
    "ajayapte": "Am having same issue.\nClient Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.1\", GitCommit:\"b0b7a323cc5a4a2019b2e9520c21c7830b7f708e\", GitTreeState:\"clean\", BuildDate:\"2017-04-03T20:44:38Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"6\", GitVersion:\"v1.6.0\", GitCommit:\"fff5156092b56e6bd60fff75aad4dc9de6b6ef37\", GitTreeState:\"clean\", BuildDate:\"2017-03-28T16:24:30Z\", GoVersion:\"go1.7.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nI am using flannel.  My first issue was /run/flannel/subnet.env not found which is reported in other issue. This stopped dns pods from coming up. I manually created one like following from older 1.5 working environment - \nFLANNEL_NETWORK=172.10.0.0/16\nFLANNEL_SUBNET=172.10.0.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=true\nThat got past dns pod issue and dns pods are running.  Now the flannel pod is in error with same message as this mentioned in this issue - \nroot@2kub-0:/tmp# kubectl logs kube-flannel-ds-gc73b -c kube-flannel --namespace kube-system --tail=20\nE0421 21:14:14.129572       1 main.go:127] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-gc73b': the server does not allow access to the requested resource (get pods kube-flannel-ds-gc73b)\n. And yes I did specify necessary inputs on my kubeadm init like this -\nkubeadm init --pod-network-cidr 172.10.0.0/16 --service-cidr 172.16.0.0/12. ",
    "igortin": "I had the same problem~\nSolution:\nkubectl apply -f https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel-rbac.yml\nkubectl apply -f https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yaml\n. please point out the correct namespace  \n. Are you using RBAC?\nHow to check:\n[kube@kube-master ~]$ kubectl exec kube-apiserver-kube-master.example.com -c kube-apiserver -n kube-system ps                                \nIn the output find: \n --authorization-mode=RBAC\nIf RBAC enabled:\n1) check sa\n[kube@kube-master ~]$ kubectl  get sa flannel -n kube-system            \nNAME      SECRETS   AGE\nflannel   1         5d\n2) check cluster role\nkubectl get clusterrole flannel -n kube-system\n3) check \nkubectl get ClusterRoleBinding flannel\nIf not exeist 1,2,3 \ncreate ServiceAccount flannel and exec yamls\n. https://github.com/coreos/flannel/blob/master/Documentation/k8s-manifests/kube-flannel-rbac.yml\n\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043e \u0441 iPhone\n\n27 \u043e\u043a\u0442. 2017 \u0433., \u0432 19:08, Sanda notifications@github.com \u043d\u0430\u043f\u0438\u0441\u0430\u043b(\u0430):\nThis link is not available : https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "ylhyh": "same issue here.\nebtables 2.0.10.4-3\nkubernetes-cni 0.5.1-00\nsocat 1.7.3.1-1\nkubelet 1.6.2-00\nkubectl 1.6.2-00\nkubeadm 1.6.2-00\nflannel 0.7.1-amd64\nSee @igortin  's solution. @igortin , the link to yml files should change to https://raw.githubusercontent.com/...\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml. @zakiournani please use thr \"raw\" link instead of github link in kubectl create command. ",
    "vnalla": "Hi,\nRunning in to the same issues on Ubuntu 16.04, Kubernetes via hyperkube v1.6.8_coreos.0 and flannel v0.7.1 & v0.8.0-amd64. appreciate your help and time.\n{\"log\":\"E0822 14:22:39.379922       1 config.go:265] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\\n\",\"stream\":\"stderr\",\"time\":\"2017-08-22T14:22:39.380327935Z\"}\n{\"log\":\"E0822 14:22:39.405061       1 main.go:127] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-14xmd': Get https://192.168.56.20:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-14xmd: x509: failed to load system roots and no roots provided\\n\",\"stream\":\"stderr\",\"time\":\"2017-08-22T14:22:39.405436407Z\"}\nthanks,\nVenkat\n. flannel daemon set is failing. appreciate your help. thanks\n{\"log\":\"E0822 00:28:17.613136       1 config.go:265] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory\\n\",\"stream\":\"stderr\",\"time\":\"2017-08-22T00:28:17.613518972Z\"}\n{\"log\":\"E0822 00:28:47.615504       1 main.go:127] Failed to create SubnetManager: error retrieving pod spec for 'kube-system/kube-flannel-ds-hrnm2': Get https://10.254.0.1:443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-hrnm2: dial tcp 10.254.0.1:443: i/o timeout\\n\",\"stream\":\"stderr\",\"time\":\"2017-08-22T00:28:47.615694906Z\"}\n. @gwind Thanks for the information. Yes able to deploy flannel and it starts cni0 upon deploying the first pod. But the pods are not talking to the pod network at all. \nkubedns  and dashboard are in endless crash loop because they can not talk to apiserver. Tried the trouble shooting as suggested in https://github.com/kubernetes/dashboard/blob/master/docs/user-guide/troubleshooting.md, the test pod give no route to host error, however curl from host machine is able to connect.\nFailedSync Error syncing pod, skipping: failed to \"CreatePodSandbox\" for \"test-1741613794-lvj32_default(2919a917-995e-11e7-83ab-080027caa9d5)\" with CreatePodSandboxError: \"CreatePodSandbox for pod \\\"test-1741613794-lvj32_default(2919a917-995e-11e7-83ab-080027caa9d5)\\\" failed: rpc error: code = 2 desc = NetworkPlugin cni failed to set up pod \\\"test-1741613794-lvj32_default\\\" network: open /run/flannel/subnet.env: no such file or directory\"\nkubectl exec test-1741613794-lvj32 -- curl -k https://10.253.0.1\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (7) Failed to connect to 10.253.0.1 port 443: No route to host\nkubectl get services\nNAME         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   10.253.0.1   <none>        443/TCP   21h\ncurl -k https://10.253.0.1\nUnauthorized\n`\nTOKEN_VALUE=$(kubectl exec test-1741613794-lvj32 -- cat /var/run/secrets/kubernetes.io/serviceaccount/token)\ncurl --cacert /srv/kubernetes/ca.crt -H  \"Authorization: Bearer $TOKEN_VALUE\" https://10.253.0.1\n{\n  \"paths\": [\n    \"/api\",\n    \"/api/v1\",\n    \"/apis\",\n    \"/apis/apps\",\n    \"/apis/apps/v1beta1\",\n    \"/apis/authentication.k8s.io\",\n    \"/apis/authentication.k8s.io/v1\",\n    \"/apis/authentication.k8s.io/v1beta1\",\n    \"/apis/authorization.k8s.io\",\n    \"/apis/authorization.k8s.io/v1\",\n    \"/apis/authorization.k8s.io/v1beta1\",\n    \"/apis/autoscaling\",\n    \"/apis/autoscaling/v1\",\n    \"/apis/autoscaling/v2alpha1\",\n    \"/apis/batch\",\n    \"/apis/batch/v1\",\n    \"/apis/batch/v2alpha1\",\n    \"/apis/certificates.k8s.io\",\n    \"/apis/certificates.k8s.io/v1beta1\",\n    \"/apis/extensions\",\n    \"/apis/extensions/v1beta1\",\n    \"/apis/policy\",\n    \"/apis/policy/v1beta1\",\n    \"/apis/rbac.authorization.k8s.io\",\n    \"/apis/rbac.authorization.k8s.io/v1alpha1\",\n    \"/apis/rbac.authorization.k8s.io/v1beta1\",\n    \"/apis/settings.k8s.io\",\n    \"/apis/settings.k8s.io/v1alpha1\",\n    \"/apis/storage.k8s.io\",\n    \"/apis/storage.k8s.io/v1\",\n    \"/apis/storage.k8s.io/v1beta1\",\n    \"/healthz\",\n    \"/healthz/ping\",\n    \"/healthz/poststarthook/bootstrap-controller\",\n    \"/healthz/poststarthook/ca-registration\",\n    \"/healthz/poststarthook/extensions/third-party-resources\",\n    \"/logs\",\n    \"/metrics\",\n    \"/swaggerapi/\",\n    \"/ui/\",\n    \"/version\"\n  ]\n}\n`\n. ",
    "spopoiu1": "This link is not available : https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml. ",
    "zakiournani": "I have a similaire problem:\nget pods --all-namespaces\nkube-system   kube-flannel-ds-bdcxj                                 0/1       CrashLoopBackOff    6          10m\nkube-system   kube-flannel-ds-llstg                                 0/1       CrashLoopBackOff      6            10m\nkubectl exec kube-apiserver-master.domain.local -c kube-apiserver -n kube-system ps\n--authorization-mode=Node,RBAC\nkubectl get sa flannel -n kube-system\nNAME      SECRETS   AGE\nflannel   1         19m\nwhen i try : kubectl create -f https://github.com/coreos/flannel/blob/master/Documentation/k8s-manifests/kube-flannel-rbac.yml\nerror: error converting YAML to JSON: yaml: line 376: mapping values are not allowed in this context\nand my kubectl logs:\nError adding network: open /run/flannel/subnet.env: no such file or directory\nError while adding to cni network: open /run/flannel/subnet.env: no such file or directory\n. Solved my issue, it was related to a mistake in interface name when configuring flannel.\nBTW i think raw link ain't working (404).\nCheers. ",
    "TracyBin": "@tomdee  hi, this is  logs of the flannel container\nlog\n[root@slave1 containers]# tail -f /var/log/pods/fbc13661-20bd-11e7-b15d-000c293e1335/kube-flannel_3.log\n{\"log\":\"E0414 02:56:51.415549       1 main.go:127] Failed to create SubnetManager: error retrieving pod spec for 'default/kube-flannel-ds-kz6jl': the server does not allow access to the requested resource (get pods kube-flannel-ds-kz6jl)\\n\",\"stream\":\"stderr\",\"time\":\"2017-04-14T02:56:51.418396354Z\"}. This look like a rbac permission error.New feature in k8s 1.6.0.\nissues\n. ",
    "singhsankalp": "Is there a workaround of this issue right now?. ",
    "aitorhh": "@singhsankalp The solution with Kubernetes v1.6.1 is to create the cluster role and role bindings first, as described in the header here.\nOr just execute:\ncurl -sSL https://rawgit.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml |  kubectl create -f -\ncurl -sSL https://rawgit.com/coreos/flannel/master/Documentation/kube-flannel.yml |  kubectl create -f -\n. ",
    "heyitsanthony": "@tomdee I don't think it needs a bleeding edge version of glide; I suspect the makefile will fail if it's not up to date. I added comments about where to get glide and glide vc so there's a hint for resolving any breakage from not having it.\netcd's deps script goes so far as to force an exact repo revision, but that seemed excessive for what this PR is trying to do.. ",
    "strigona-worksight": "I just got this working myself.\nDownload kube-flannel.yml and update references to the flannel image from 0.7.0 to 0.7.1.\nRe-run the configuration from scratch, but use\nkubectl apply -f kube-flannel.yml\n (your edited file). This should be fixed now #690 . ",
    "vjdhama": "@strigona-worksight Thank you very much. Changing the flannel image to v0.7.1 works.\n. ",
    "ganlaksh": "@euank I am using flannel with udp and not vxlan. Do you think it could still be relevant ? I will give it a try anyway. thanks! . @euank Tried with 0.7.1 and i see the same issue after node reboot and same workaround works as well . . @tomdee We havent looked into the performance aspect yet and we were just going with the defaults as of now. . ",
    "sd1587745": "SW versions:\nKUBE_VERSION=1.3.10\nFLANNEL_VERSION=0.5.5\nETCD_VERSION=3.1.6\nand using flannel vxlan mode.\nroot@pcnbj-sto024:/etc/default# etcdctl get /coreos.com/network/subnets/172.31.15.0-24\n{\"PublicIP\":\"10.140.140.29\",\"BackendType\":\"vxlan\",\"BackendData\":{\"VtepMAC\":\"16:f3:7d:8c:04:ab\"}}\n. when I use flannel 0.7.1 and a bug as this link occurred: [https://github.com/coreos/flannel/issues/196]\nmy etcd has new subnet profile for these hosts, docker0 port is set to correct subnet, but flannel.1 port remains old config.\n```\nroot@pcnbj-sto024:/var/run# etcdctl get /coreos.com/network/subnets/172.31.92.0-24\n{\"PublicIP\":\"10.148.13.236\",\"BackendType\":\"vxlan\",\"BackendData\":{\"VtepMAC\":\"e6:e0:c3:ee:7a:ac\"}}\ndocker0   Link encap:Ethernet  HWaddr 02:42:f9:8f:51:2b\n          inet addr:172.31.92.1  Bcast:0.0.0.0  Mask:255.255.255.0\n          inet6 addr: fe80::42:f9ff:fe8f:512b/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:8 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:8 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:536 (536.0 B)  TX bytes:648 (648.0 B)\nroot@pcnbj-sto024:/var/run# ifconfig flannel.1\nflannel.1 Link encap:Ethernet  HWaddr e6:e0:c3:ee:7a:ac\n          inet addr:172.31.11.0  Bcast:0.0.0.0  Mask:255.255.0.0\n          inet6 addr: fe80::e4e0:c3ff:feee:7aac/64 Scope:Link\n          UP BROADCAST RUNNING MULTICAST  MTU:1450  Metric:1\n          RX packets:7941 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:145 errors:0 dropped:27 overruns:0 carrier:0\n          collisions:0 txqueuelen:0 \n          RX bytes:632524 (632.5 KB)  TX bytes:14266 (14.2 KB)\n```\nacturally subnet.env is correct for flannel, but I restart flanneld many times it still keeps older IP subnet:\nroot@pcnbj-sto024:/var/run# cat /run/flannel/subnet.env\nFLANNEL_NETWORK=172.31.0.0/16\nFLANNEL_SUBNET=172.31.92.1/24\nFLANNEL_MTU=1450\nFLANNEL_IPMASQ=true\nhow to fix it.. also pid and log file doesn't created, I'm using ubuntu14.04.3.\nconfig file in /etc/init.d/flanneld\n```\nFLANNEL_PIDFILE=/var/run/$BASE.pid\nFLANNEL_LOGFILE=/var/log/$BASE.log\nFLANNEL_START=\"start-stop-daemon \\\n--start \\\n--background \\\n--quiet \\\n--exec $FLANNEL \\\n--make-pidfile --pidfile $FLANNEL_PIDFILE \\\n-- $FLANNEL_OPTS \\\n\n\n$FLANNEL_LOGFILE 2>&1\"\n```\nno files created:\n\n\nroot@pcnbj-sto024:~# cat /var/run/flanneld.pid\ncat: /var/run/flanneld.pid: No such file or directory\nroot@pcnbj-sto024:~# cat /var/log/flanneld.log\ncat: /var/log/flanneld.log: No such file or directory\n. anyway, when change back to flannel version 0.5.5, flannel.1 auto updated, not version 0.7.1.\npid and log files still doesn't exist.. ",
    "Sherler": "same issue, can ping the crossed node docker0, but not worked when pinging crossed node pods. ",
    "jnickc": "sd1587745, did you solved this issue? I have exactly the same problem. ",
    "phhutter": "@sd1587745 did you finally solve it ?. ",
    "azalio": "Today I have faced this problem and have solved it by running the command:\n# iptables --policy FORWARD ACCEPT. ",
    "lvthillo": "same issue, iptables does not seem to help.. @PatrickKutch Are you also using ubuntu 18? I managed to make it work by adding the network interface to the flannel.yml file:\n$ curl -o kube-flannel.yml https://raw.githubusercontent.com/coreos/flannel/v0.11.0/Documentation/kube-flannel.yml\n$ sed -i.bak 's|        - --ip-masq|        - --ip-masq\\n        - --iface=enp0s8|' kube-flannel.yml\n$ kubectl create -f kube-flannel.yml\nThe sed command is adding --iface=enp0s8 to the flannel args:\n...\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --iface=enp0s8\n        - --kube-subnet-mgr\n...\nHope this solves it for you too.. @PatrickKutch  Aren't it all Ubuntu 18 nodes with the enp0s8 network? (\"the eth0 of Ubuntu 18\").. ",
    "insoz": "setenforce 0 resolved my problem. . Sloved, The port 8472 have to accept with iptables . . ",
    "ntquyen": "Thanks @tomdee I'll post this issue there. ",
    "pmcgrath": "If I want to update from 0.9.1 to 0.10.0 today, how do I do so ?\nOn a cluster installed with kubeadm, I ran kubectl applying the manifest for 0.9.1 at the time\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml\nAs Tom pointed out the existing update strategy is OnDelete and since this impacts networking I need to be cautious\nIf I do kubectl apply the 0.10.0 manifest the flannel daemonset will not update ? \nIf I delete the 0.9.1 daemonset before applying the 0.10.0 manifest, will I lose network access to the cluster and networking within the cluster ?\nCouldn't find any guidance when searching, so some guidance would be appreciated ?\nThanks\nPat\n. @tomdee Do you have any info at this time ? A link to what someone else did ?\n. ",
    "aba182": "+1 here, we are looking to do a flannel upgrade and we want to make sure we are following best practices since this is a critical piece of our infrastructure. . When we attempted to deploy v0.10.0 we had two different /etc/cni/net.d/10-flannel.conf* files one was .conf and the other was .conflist. \n```\nls -l /etc/cni/net.d/10-flannel.conf*\n-rw-r--r-- 1 root root 117 Sep 20 16:05 /etc/cni/net.d/10-flannel.conf\n-rw-r--r-- 1 root root 267 Sep 20 16:07 /etc/cni/net.d/10-flannel.conflist\n```\nThe .conflist had our desired config but it wasn't applied. We discovered this when we attempted to leverage the hairpin system. \nTo fix this we move from .conflist -> .conf and our issue was resolved. . ",
    "jansmets": "any pointers?. I'm having the same issue.. ",
    "rafagsiqueira": "Selinux was enabled.. ",
    "katanyoussef": "did not work on mine thanks. ",
    "maxx": "I'm working on a fix for this which will check to see if the public-ip node annotation already exists before overwriting it.. My fix could potentially cause a problem if the internal IP of the VM changes on reboot.  Perhaps a second annotation to indicate that the IP is overridden would be good.. or perhaps a kube node label would be more appropriate for this sort of configuration data.. @tomdee In my situation, the external IP is stored in openstack metadata.  It's not something that could be expressed in a regex and it's not assigned to any interface within the node.  Flannel has to read this data from somewhere on startup.   A node annotation is a good place (it obviously stores it there already).  We just need another annotation which is an override which flannel won't overwrite with any presumed IP.. ",
    "tz-lom": "Any luck with that problem? I like \"public-ip-override\" sollution, it usable not only with OpenStack - my hosting provides eth0 with local ip, and public ip can't be easy acquired on the machine. ",
    "martynd": "If you are using --hostname-override on the kubelet, have you tried making sure that its value has an entry in the host machines /etc/hosts file or that its able to be resolved correctly by your primary resolver?\nWhen it isn't defined ive experienced the inconsistent behavior described, however once it is, ive not experienced any issues, even with the daemonset method.\nIf you have your kubelet sandboxed using the kubelet-wrapper and you're still having issues, try making sure the rkt args include --hosts-entry=host to have it use the nodes hosts file to test it.\nSome providers use public to mean the world accessible IP rather than the private ip which is what flannel wants which seems to be where this comes about.. ",
    "tommyknows": "Hi,\nI tried to apply your patch - i took the standard daemon-set, replaced the docker image with one from quay.io/coreos/flannel-git and set the node annotations with\nkubectl annotate node $name flannel.alpha.coreos.com/public-ip-overwrite=$IP\nYour fix works - it sets flannel.alpha.coreos.com/public-ip to the IP I provided.\nHowever, when checking the flannel logs:\n{\"log\":\"I1110 13:57:46.705442       1 main.go:469] Determining IP address of default interface\\n\",\"stream\":\"stderr\",\"time\":\"2017-11-10T13:57:46.706125283Z\"}\n{\"log\":\"I1110 13:57:46.706949       1 main.go:482] Using interface with name eth0 and address 10.94.41.81\\n\",\"stream\":\"stderr\",\"time\":\"2017-11-10T13:57:46.707759148Z\"}\n{\"log\":\"I1110 13:57:46.706969       1 main.go:499] Defaulting external address to interface address (10.94.41.81)\\n\",\"stream\":\"stderr\",\"time\":\"2017-11-10T13:57:46.707780337Z\"}\nFlannel still defaults to the standard interface and IP. Do I have to change any of the parameters in the daemonset or configmap?. You're right - it's just the log. It didn't work right after that because I had to add some iptables rules to the host system. Thank you!. ",
    "alvaroaleman": "Uhm. I guess only the logging is wrong there. Did you try if you can reach pods that are placed on the node you annotated?. Added docs. Writing a test seems to be rather complicated, because there are none for the kube subnet manager yet.. Thanks for the hint @tomdee, I added a test.\nI also moved the check for ls /run/flannel/subnet.env into the start_flannel function, since it is a generic test if flannel startup was finished and not specific to the other ping tests in the suite.\n. Recreated the PR because travis was failing after having issues pulling the etcd image.\nThe e2e tests run twice btw, because they are called in the travis file and from within the test maketarget. Updated the explanation and squashed my commits.\nAs far as I can tell, the other annotations are not supposed to be altered by users thus it doesn't make sense to document them or am I wrong there?. Added the missing commit back.. ",
    "jaytaylor": "This is still causing me problems in environments where the public IP is different the the private IP.\nIs there any way to prevent flannel from always overwriting the flannel.alpha.coreos.com/public-ip key when it already exists?  I've had to implement a rather ugly hack to check every few minutes for mismatches between alpha.kubernetes.io/provided-node-ip and when one is found, set flannel.alpha.coreos.com/public-ip accordingly.\nReally wishing there were a --use-ip=w.x.y.z flag... Fantastic information, thank you @wakawaka54!. ",
    "wakawaka54": "I'm working on the same issue right now. It looks like there is an override flag you can set as an annotation on the node: https://coreos.com/flannel/docs/latest/kubernetes.html\nThe flag is this flannel.alpha.coreos.com/public-ip-overwrite . I tried adding this annotation to my master node but upon restarting flannel, it looks like it resets the public-ip annotation to the incorrect ip address again.\nUpdate: I bet this is because I am using Flannel v0.9.1 which is in the kubeadm docs but the latest is v0.10.0 which seems to add the flag. I will check on this and then follow up.. ",
    "timorjim": "Looks like we are seeing the same behaviour in v0.10.0. Set the IP with:\n\nkubectl annotate node  flannel.alpha.coreos.com/public-ip= --overwrite\n\nand it is reset to the default on reboot.. ",
    "vasu-dasari": "Yes. I am also seeing this issue.\n```\nSet annotation\nkubectl annotate node seeweed-vm-06 flannel.alpha.coreos.com/public-ip-overwrite=10.200.1.6 --overwrite\nRestart flannel\nkubectl get pod kube-flannel-ds-49shf -o yaml -n kube-system  | kubectl replace --force -f -\n```\nNow I see that packet is supposed using the right interface eth1.31, but, the source IP address that is used in outer IP header is not right. It should have been 10.200.1.6\n10:40:43.052861 00:0c:29:8c:82:93 > 00:0c:29:39:02:0c, ethertype 802.1Q (0x8100), length 152: vlan 31, p 0, ethertype IPv4, 172.17.4.197.33434 > 10.200.1.5.8472: OTV, flags [I] (0x08), overlay 0, instance 1\n    ce:08:6c:7e:3a:ea > d2:39:c2:72:cb:3c, ethertype IPv4 (0x0800), length 98: 10.244.1.12 > 10.244.5.6: ICMP echo request, id 273, seq 587, length 64\nBefore makikng annotation changes, VxLAN packets used leave from eth0 whose IP address used to be 172.17.4.197.\nAnd I see that the flannel that is being used is: quay.io/coreos/flannel:v0.10.0-amd64\n. ",
    "aisensiy": "@vasu-dasari I meet the same problem. Trying to change the annotation but it come back quickly.. ",
    "tuyuri2201": "sorry team , that 're my problems , sometimes docker daemons cannot restart appropriately , i have to kill docker process and every thing work well . ",
    "raeesbhatti": "The parameters to mount certificates are same as they are for other services (etcd, kube-apiserver, etc.). The cert files are there because if they were not, it would throw an error about not being able to open the file. As I said before, I tried the same ca.pem with CURL and it works alright.. ",
    "glennmcallister": "I'm observing similar behaviour with a new cluster I'm trying to deploy, although the versions are slightly different:\n\nFlannel version: v0.7.0\nBackend used (e.g. vxlan or udp): vxlan\nEtcd version: v3.1.6\nOperating System and version: Container Linux by CoreOS stable (1353.7.0)\n\nsudo etcdctl --endpoints=https://<ip1>:2379,https://<ip2>:2379,https://<ip3>:2379 --cert-file=/etc/kubernetes/ssl/apiserver.pem --key-file=/etc/kubernetes/ssl/apiserver-key.pem cluster-health\nmember 2b83d802c287600 is healthy: got healthy result from https://<ip1>:2379\nmember a2f15b71c0182d77 is healthy: got healthy result from https://<ip2>:2379\nmember c40650a32d25891b is healthy: got healthy result from https://<ip3>:2379\ncluster is healthy\nI've tried various ways of providing the certificate configuration to flanneld, last one looks like the following values in /etc/flannel/options.env:\nFLANNELD_IFACE=<vmip>\nETCD_SSL_DIR=/etc/kubernetes/ssl\nFLANNEL_OPTS=\"-v=9 --etcd-certfile=/etc/kubernetes/ssl/apiserver.pem --etcd-keyfile=/etc/kubernetes/ssl/apiserver-key.pem --etcd-endpoints=https://<ip1>:2379,https://<ip2>:2379,https://<ip3>:2379\"\nI've also used the FLANNELD_* environment variables, same results:\nMay 23 14:20:34 <vm-name> flannel-wrapper[4617]: E0523 14:20:34.434201    4617 network.go:102] failed to retrieve network config: client: etcd cluster is unavailable or misconfigured\nMay 23 14:20:35 <vm-name> flannel-wrapper[4617]: E0523 14:20:35.447655    4617 network.go:102] failed to retrieve network config: client: etcd cluster is unavailable or misconfigured\nMay 23 14:20:36 <vm-name> flannel-wrapper[4617]: E0523 14:20:36.462808    4617 network.go:102] failed to retrieve network config: client: etcd cluster is unavailable or misconfigured \nAnd before anyone chimes in with my missing the CA file, it's already been included in /etc/ssl/certs and sudo update-ca-certificates run during an earlier provisioning step.. So, I managed to get this to work. The TL;DR is that /etc/ssl/certs is not in the set of directory mounts that flannel-wrapper provides by default. \nThe quick workaround was for me to copy my cert chain into /etc/kubernetes/ssl as bcl-ca.pem, and voila, it all worked:\nMay 23 15:30:57 <vmname> flannel-wrapper[8314]: + exec /usr/bin/rkt run --uuid-file-save=/var/lib/coreos/flannel-wrapper.uuid --trust-keys-from-https --volume ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=true --mount volume=ssl,target=/etc/kubernetes/ssl --mount volume=notify,target=/run/systemd/notify --volume notify,kind=host,source=/run/systemd/notify --set-env=NOTIFY_SOCKET=/run/systemd/notify --net=host --volume run-flannel,kind=host,source=/run/flannel,readOnly=false --volume etc-ssl-certs,kind=host,source=/usr/share/ca-certificates,readOnly=true --volume usr-share-certs,kind=host,source=/usr/share/ca-certificates,readOnly=true --volume etc-hosts,kind=host,source=/etc/hosts,readOnly=true --volume etc-resolv,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=run-flannel,target=/run/flannel --mount volume=etc-ssl-certs,target=/etc/ssl/certs --mount volume=usr-share-certs,target=/usr/share/ca-certificates --mount volume=etc-hosts,target=/etc/hosts --mount volume=etc-resolv,target=/etc/resolv.conf --inherit-env --stage1-from-dir=stage1-fly.aci quay.io/coreos/flannel:v0.7.0 -- -v=9 --etcd-certfile=/etc/kubernetes/ssl/apiserver.pem --etcd-keyfile=/etc/kubernetes/ssl/apiserver-key.pem --etcd-endpoints=https://<etcd-ip1>:2379,https://<etcd-ip2>:2379,https://<etcd-ip3>:2379 --etcd-cafile=/etc/kubernetes/ssl/bcl-ca.pem\nMay 23 15:30:57 <vmname> flannel-wrapper[8314]: I0523 15:30:57.924696    8314 main.go:132] Installing signal handlers\nMay 23 15:30:57 <vmname> flannel-wrapper[8314]: I0523 15:30:57.925035    8314 manager.go:124] Searching for interface using <vmip>\nMay 23 15:30:57 <vmname> flannel-wrapper[8314]: I0523 15:30:57.925331    8314 manager.go:149] Using interface with name ens192 and address <vmip>\nMay 23 15:30:57 <vmname> flannel-wrapper[8314]: I0523 15:30:57.925486    8314 manager.go:166] Defaulting external address to interface address (<vmip>)\nMay 23 15:30:57 <vmname> flannel-wrapper[8314]: I0523 15:30:57.996613    8314 local_manager.go:179] Picking subnet in range 10.2.1.0 ... 10.2.255.0\nMay 23 15:30:58 <vmname> flannel-wrapper[8314]: I0523 15:30:58.002767    8314 manager.go:250] Lease acquired: 10.2.60.0/24\nMay 23 15:30:58 <vmname> flannel-wrapper[8314]: I0523 15:30:58.003039    8314 network.go:58] Watching for L3 misses\nMay 23 15:30:58 <vmname> flannel-wrapper[8314]: I0523 15:30:58.003049    8314 network.go:66] Watching for new subnet leases\nMay 23 15:30:58 <vmname> systemd[1]: Started flannel - Network fabric for containers (System Application Container).\nSo, what I'm confused about is the following:\n--volume etc-ssl-certs,kind=host,source=/usr/share/ca-certificates,readOnly=true \n--volume usr-share-certs,kind=host,source=/usr/share/ca-certificates,readOnly=true\nAnd later:\n--mount volume=etc-ssl-certs,target=/etc/ssl/certs \n--mount volume=usr-share-certs,target=/usr/share/ca-certificates\nWe have two volume mounts pointing at the same directory. Why?  Why doesn't the first actually point at /etc/ssl/certs as expected? If it did, I wouldn't be broken.\n. So, in all fairness, what I'm observing isn't a flannel bug, it's a Container Linux bug; any advice as to the best place to write this up?. ",
    "Capitrium": "I'm running into the same issue - kube-flannel seems to hang on new nodes once the cluster has reached 100 running nodes. The kube-flannel logs shows \"Waiting 10m0s for node controller to sync\", but that timeout never seems to expire. I don't see any red flags in the logs myself, but I've included them below.\nLogs from a broken kube-flannel pod, which has been running for ~30 minutes now:\n$ kubectl logs -n kube-system kube-flannel-ds-0763k -f -c kube-flannel\nI0518 20:21:27.924866       1 kube.go:111] Waiting 10m0s for node controller to sync\nI0518 20:21:27.924939       1 kube.go:315] starting kube subnet manager\nAPI server logs:\n$ kubectl logs -n kube-system kube-apiserver-ip-10-0-15-238.ec2.internal -f\nI0518 19:41:16.120565       1 aws.go:762] Building AWS cloudprovider\nI0518 19:41:16.120654       1 aws.go:725] Zone not specified in configuration file; querying AWS metadata service\nI0518 19:41:16.365252       1 tags.go:76] AWS cloud filtering on ClusterID: dev-cluster\nE0518 19:41:16.840996       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *api.LimitRange: Get https://localhost:443/api/v1/limitranges?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841107       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *api.Secret: Get https://localhost:443/api/v1/secrets?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841199       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *rbac.Role: Get https://localhost:443/apis/rbac.authorization.k8s.io/v1beta1/roles?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841308       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *rbac.RoleBinding: Get https://localhost:443/apis/rbac.authorization.k8s.io/v1beta1/rolebindings?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841308       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *rbac.ClusterRole: Get https://localhost:443/apis/rbac.authorization.k8s.io/v1beta1/clusterroles?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841451       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *rbac.ClusterRoleBinding: Get https://localhost:443/apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841489       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *api.Namespace: Get https://localhost:443/api/v1/namespaces?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841519       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *api.ResourceQuota: Get https://localhost:443/api/v1/resourcequotas?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\nE0518 19:41:16.841592       1 reflector.go:201] k8s.io/kubernetes/pkg/client/informers/informers_generated/internalversion/factory.go:70: Failed to list *api.ServiceAccount: Get https://localhost:443/api/v1/serviceaccounts?resourceVersion=0: dial tcp [::1]:443: getsockopt: connection refused\n[restful] 2017/05/18 19:41:16 log.go:30: [restful/swagger] listing is available at https://204.236.223.202/swaggerapi/\n[restful] 2017/05/18 19:41:16 log.go:30: [restful/swagger] https://204.236.223.202/swaggerui/ is mapped to folder /swagger-ui/\nI0518 19:41:16.922771       1 serve.go:79] Serving securely on 0.0.0.0:443\nI0518 19:41:16.922907       1 serve.go:94] Serving insecurely on 127.0.0.1:8080\nI0518 19:41:17.863623       1 trace.go:61] Trace \"Create /api/v1/namespaces/kube-system/serviceaccounts\" (started 2017-05-18 19:41:17.058770376 +0000 UTC):\n[39.912\u00b5s] [39.912\u00b5s] About to convert to expected version\n[97.171\u00b5s] [57.259\u00b5s] Conversion done\n[801.502671ms] [801.4055ms] About to store object in database\n[804.695623ms] [3.192952ms] Object stored in database\n[804.698656ms] [3.033\u00b5s] Self-link added\n\"Create /api/v1/namespaces/kube-system/serviceaccounts\" [804.801614ms] [102.958\u00b5s] END\nI0518 19:41:17.877272       1 trace.go:61] Trace \"Create /api/v1/namespaces/default/services\" (started 2017-05-18 19:41:16.958558439 +0000 UTC):\n[37.004\u00b5s] [37.004\u00b5s] About to convert to expected version\n[104.49\u00b5s] [67.486\u00b5s] Conversion done\n[905.520287ms] [905.415797ms] About to store object in database\n[918.659621ms] [13.139334ms] Object stored in database\n[918.664693ms] [5.072\u00b5s] Self-link added\n\"Create /api/v1/namespaces/default/services\" [918.690463ms] [25.77\u00b5s] END\nI0518 19:41:17.937227       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/cluster-admin\nI0518 19:41:17.943815       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:discovery\nI0518 19:41:17.943925       1 trace.go:61] Trace \"Create /api/v1/namespaces/kube-system/configmaps\" (started 2017-05-18 19:41:16.938161583 +0000 UTC):\n[25.066\u00b5s] [25.066\u00b5s] About to convert to expected version\n[89.957\u00b5s] [64.891\u00b5s] Conversion done\n[1.001978266s] [1.001888309s] About to store object in database\n[1.005687379s] [3.709113ms] Object stored in database\n[1.005692485s] [5.106\u00b5s] Self-link added\n\"Create /api/v1/namespaces/kube-system/configmaps\" [1.00572877s] [36.285\u00b5s] END\nI0518 19:41:17.950363       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:basic-user\nI0518 19:41:17.964645       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/admin\nI0518 19:41:17.970782       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/edit\nI0518 19:41:17.977394       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/view\nI0518 19:41:17.991109       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:heapster\nI0518 19:41:17.996917       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:node\nI0518 19:41:18.002949       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:node-problem-detector\nI0518 19:41:18.011007       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:node-proxier\nI0518 19:41:18.017204       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:node-bootstrapper\nI0518 19:41:18.023128       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:auth-delegator\nI0518 19:41:18.029558       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:kube-aggregator\nI0518 19:41:18.035836       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:kube-controller-manager\nI0518 19:41:18.042544       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:kube-scheduler\nI0518 19:41:18.048470       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:kube-dns\nI0518 19:41:18.054258       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:persistent-volume-provisioner\nI0518 19:41:18.060100       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:attachdetach-controller\nI0518 19:41:18.066027       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:cronjob-controller\nI0518 19:41:18.072763       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:daemon-set-controller\nI0518 19:41:18.078582       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:deployment-controller\nI0518 19:41:18.084329       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:disruption-controller\nI0518 19:41:18.094122       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:endpoint-controller\nI0518 19:41:18.101085       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:generic-garbage-collector\nI0518 19:41:18.107027       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:horizontal-pod-autoscaler\nI0518 19:41:18.113418       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:job-controller\nI0518 19:41:18.119160       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:namespace-controller\nI0518 19:41:18.125149       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:node-controller\nI0518 19:41:18.130977       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:persistent-volume-binder\nI0518 19:41:18.136699       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:pod-garbage-collector\nI0518 19:41:18.142724       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:replicaset-controller\nI0518 19:41:18.148499       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:replication-controller\nI0518 19:41:18.154469       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:resourcequota-controller\nI0518 19:41:18.160132       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:route-controller\nI0518 19:41:18.165782       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:service-account-controller\nI0518 19:41:18.171829       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:service-controller\nI0518 19:41:18.179999       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:statefulset-controller\nI0518 19:41:18.185971       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:ttl-controller\nI0518 19:41:18.192247       1 storage_rbac.go:168] created clusterrole.rbac.authorization.k8s.io/system:controller:certificate-controller\nI0518 19:41:18.198330       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/cluster-admin\nI0518 19:41:18.204286       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:discovery\nI0518 19:41:18.214992       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:basic-user\nI0518 19:41:18.221462       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:node\nI0518 19:41:18.227672       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:node-proxier\nI0518 19:41:18.233582       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:kube-controller-manager\nI0518 19:41:18.239339       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns\nI0518 19:41:18.245567       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:kube-scheduler\nI0518 19:41:18.251685       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:attachdetach-controller\nI0518 19:41:18.262448       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:cronjob-controller\nI0518 19:41:18.271834       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:daemon-set-controller\nI0518 19:41:18.278787       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:deployment-controller\nI0518 19:41:18.284864       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:disruption-controller\nI0518 19:41:18.290780       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:endpoint-controller\nI0518 19:41:18.296790       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:generic-garbage-collector\nI0518 19:41:18.303179       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:horizontal-pod-autoscaler\nI0518 19:41:18.308858       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:job-controller\nI0518 19:41:18.314994       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:namespace-controller\nI0518 19:41:18.328195       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:node-controller\nI0518 19:41:18.333903       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:persistent-volume-binder\nI0518 19:41:18.367297       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:pod-garbage-collector\nI0518 19:41:18.407192       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:replicaset-controller\nI0518 19:41:18.448378       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:replication-controller\nI0518 19:41:18.487373       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:resourcequota-controller\nI0518 19:41:18.528101       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:route-controller\nI0518 19:41:18.567229       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:service-account-controller\nI0518 19:41:18.607665       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:service-controller\nI0518 19:41:18.647557       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:statefulset-controller\nI0518 19:41:18.687547       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:ttl-controller\nI0518 19:41:18.727327       1 storage_rbac.go:196] created clusterrolebinding.rbac.authorization.k8s.io/system:controller:certificate-controller\nI0518 19:41:18.771112       1 storage_rbac.go:227] created role.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-public \nI0518 19:41:18.807424       1 storage_rbac.go:227] created role.rbac.authorization.k8s.io/extension-apiserver-authentication-reader in kube-system \nI0518 19:41:18.847425       1 storage_rbac.go:227] created role.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-system \nI0518 19:41:18.887679       1 storage_rbac.go:227] created role.rbac.authorization.k8s.io/system:controller:token-cleaner in kube-system \nI0518 19:41:18.927976       1 storage_rbac.go:257] created rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-system\nI0518 19:41:18.967479       1 storage_rbac.go:257] created rolebinding.rbac.authorization.k8s.io/system:controller:token-cleaner in kube-system\nI0518 19:41:19.007231       1 storage_rbac.go:257] created rolebinding.rbac.authorization.k8s.io/system:controller:bootstrap-signer in kube-public\nI0518 19:51:16.638058       1 compact.go:159] etcd: compacted rev (175), endpoints ([https://etcd-events.k8s:2379])\nI0518 19:51:16.647254       1 compact.go:159] etcd: compacted rev (421), endpoints ([https://etcd.k8s:2379])\nI0518 19:56:16.642789       1 compact.go:159] etcd: compacted rev (272), endpoints ([https://etcd-events.k8s:2379])\nI0518 19:56:16.663078       1 compact.go:159] etcd: compacted rev (813), endpoints ([https://etcd.k8s:2379])\nE0518 19:58:00.454192       1 watcher.go:188] watch chan error: etcdserver: mvcc: required revision has been compacted\nI0518 20:01:16.660297       1 compact.go:159] etcd: compacted rev (344), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:01:16.673474       1 compact.go:159] etcd: compacted rev (1233), endpoints ([https://etcd.k8s:2379])\nI0518 20:06:16.671250       1 compact.go:159] etcd: compacted rev (6057), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:06:16.680355       1 compact.go:159] etcd: compacted rev (4858), endpoints ([https://etcd.k8s:2379])\nI0518 20:11:16.678306       1 compact.go:159] etcd: compacted rev (9157), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:11:16.687403       1 compact.go:159] etcd: compacted rev (9120), endpoints ([https://etcd.k8s:2379])\nE0518 20:13:23.553856       1 watcher.go:188] watch chan error: etcdserver: mvcc: required revision has been compacted\nI0518 20:16:16.693806       1 compact.go:159] etcd: compacted rev (9158), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:16:16.695677       1 compact.go:159] etcd: compacted rev (12387), endpoints ([https://etcd.k8s:2379])\nI0518 20:21:16.705018       1 compact.go:159] etcd: compacted rev (11916), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:21:16.713789       1 compact.go:159] etcd: compacted rev (16688), endpoints ([https://etcd.k8s:2379])\nI0518 20:26:16.715901       1 compact.go:159] etcd: compacted rev (25686), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:26:16.722670       1 compact.go:159] etcd: compacted rev (22112), endpoints ([https://etcd.k8s:2379])\nE0518 20:27:09.623635       1 watcher.go:188] watch chan error: etcdserver: mvcc: required revision has been compacted\nI0518 20:31:16.725031       1 compact.go:159] etcd: compacted rev (42694), endpoints ([https://etcd-events.k8s:2379])\nI0518 20:31:16.730585       1 compact.go:159] etcd: compacted rev (27279), endpoints ([https://etcd.k8s:2379]). ",
    "gtirloni": "I'm facing the same issue with Kubernetes 1.6.4 (kubeadm). Applied the files below:\nhttps://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml\nhttps://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n```\nkubectl -n kube-system logs -c kube-flannel kube-flannel-ds-zj00j\nI0526 22:48:02.412713       1 kube.go:111] Waiting 10m0s for node controller to sync\nI0526 22:48:02.412795       1 kube.go:315] starting kube subnet manager\nI0526 22:48:03.413179       1 kube.go:118] Node controller sync successful\nI0526 22:48:03.413240       1 main.go:132] Installing signal handlers\nI0526 22:48:03.413374       1 manager.go:136] Determining IP address of default interface\nI0526 22:48:03.414338       1 manager.go:149] Using interface with name eth0 and address 10.10.10.10\nI0526 22:48:03.414379       1 manager.go:166] Defaulting external address to interface address (10.10.10.10)\nE0526 22:48:03.469315       1 network.go:102] failed to register network: failed to acquire lease: node \"host.example.com\" pod cidr not assigned\n```. ",
    "wkjun": "try the blow,may be useful\nedit  /etc/kubernetes/manifests/kube-controller-manager.yaml \nat command ,add \n    --allocate-node-cidrs=true\n    --cluster-cidr=10.244.0.0/16\nthen,reload kubelet \nmy situation is ,update kubernete 1.7.1 to 1.7.4,and  /etc/kubernetes/manifests cidr paramers  was  lost.. ",
    "wutianya": "\ntry the blow,may be useful\nedit /etc/kubernetes/manifests/kube-controller-manager.yaml\nat command ,add\n--allocate-node-cidrs=true\n--cluster-cidr=10.244.0.0/16\nthen,reload kubelet\nmy situation is ,update kubernete 1.7.1 to 1.7.4,and /etc/kubernetes/manifests cidr paramers was lost.\n\nthanks. ",
    "berlinsaint": "i meet the same problems wit you. can't forward packets to docker0,my issue\uff1ahttps://github.com/kubernetes/kubernetes/issues/46077\nmy master is ubunu 16.04\nand all the nodes is 14.04\nnode's docker0 can ping success but the pod some time ok,some time bad.\ni see that you use 14.04 too, my 14.04 node kernel version is Linux i-40C8C8F5 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\nhow about you?. @defcyy  i don't know how @KDF5000 did, i did the  sudo iptables -P FORWARD ACCEPT ,\nbut not use, i make success sometimes , but when i delete the pods ,and after the pods rebuilt, the same issues became.. i guess it's the ubuntu 14.04 kernel's problem;\nLinux i-40C8C8F5 3.13.0-24-generic #46-Ubuntu SMP Thu Apr 10 19:11:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux\ni take tests for many times; sometimes ok ,but usually has ping problem between the different node's pods. @KDF5000 071 too, i guess your kernel version is 3.13 right? i update my kernel ,the problem solved.\ni update to Linux i-40C8C8F5 4.2.0-42-generic #49~14.04.1-Ubuntu SMP Wed Jun 29 20:22:11 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux then every thing becomes ok.\n. nop,  just upgrade the kernel. use\nsudo apt-get install linux-generic-lts-wily\n. \u9760\uff0c\u90fd\u662f\u4e2d\u56fd\u4eba\uff0cQQ 66375364  \u4e00\u8d77\u4ea4\u6d41. \u4f60\u8981\u662f \u5347\u7ea7\u5185\u6838\u540e\u4e5f\u89e3\u51b3\u95ee\u9898\u4e86\uff0c\u6211\u5efa\u8bae\u6211\u4eec\u63d0\u4e00\u4e2aissue\uff0c\u5b98\u65b9flannel\u8c8c\u4f3c\u6ca1\u6709\u8bf4 \u6709\u5185\u6838\u9650\u5236\u5427\uff1f\u6211\u597d\u50cf\u6ca1\u770b\u5230\u3002. ## \u6211\u53d1\u90ae\u4ef6\u7ed9\u4f60\uff0c \u4f60\u56de\u590d\u6211\u4e0b @KDF5000 . ",
    "defcyy": "@KDF5000 the problem seems to be the same as #609 ,  sudo iptables -P FORWARD ACCEPT works for me.. ",
    "gojoy": "Got it. Thanks. ",
    "manuperera": "Thanks for the workaround. \nDo you know if the next version of flannel (0.8) will fix this problem?. I've tested the possible solution mentioned in https://github.com/coreos/flannel/blob/master/network/ipmasq.go#L36, but not working.\nI will write again in this issue the tests details by request of @MrHohn. https://github.com/kubernetes/kubernetes/issues/52096 .\nI've installed the latest stable release of CoreOS (1548.0.0). This release contains Flanneld 0.9.0.\ncoreos002 ~ # more /etc/lsb-release \nDISTRIB_ID=\"Container Linux by CoreOS\"\nDISTRIB_RELEASE=1548.0.0\nDISTRIB_CODENAME=\"Ladybug\"\nDISTRIB_DESCRIPTION=\"Container Linux by CoreOS 1548.0.0 (Ladybug)\"\ncoreos002 ~ # /var/lib/rkt/cas/tree/deps-sha512-d422e9e7efc0d089bfcf581cac189d39513a6fd421269aa31f863c44ef96d9ff/rootfs/opt/bin/flanneld --version\nv0.9.0\nI've repeated tests over connector container and results are the same. Packets captured in Kubernetes's master nodo have correct IP address.\nlogs\n13:34:00.340753 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x48 length: 166\n13:34:00.340880 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x49 length: 151\n13:34:01.241995 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x4a length: 166\n13:34:01.242135 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x4b length: 151\n13:34:02.142696 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x4c length: 166\n13:34:02.142817 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x4d length: 151\n13:34:02.538837 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x4e length: 151\n13:34:03.440217 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x4f length: 151\n13:34:04.341705 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x50 length: 151\n13:34:05.243232 IP 172.19.18.53.35686 > coreos002.amplia.int.radius-acct: RADIUS, Accounting-Request (4), id: 0x51 length: 151\nBut, packets captured in Docker container (Connector) don,t have client IP address.\n13:34:00.343773 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x48 length: 166\n13:34:00.343857 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x49 length: 151\n13:34:01.245124 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x4a length: 166\n13:34:01.245862 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x4b length: 151\n13:34:02.145736 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x4c length: 166\n13:34:02.146381 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x4d length: 151\n13:34:02.541613 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x4e length: 151\n13:34:03.443608 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x4f length: 151\n13:34:04.344522 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x50 length: 151\n13:34:05.246380 IP 10.1.7.0.35686 > perfconnector-ttqx8.radius-acct: RADIUS, Accounting-Request (4), id: 0x51 length: 151\nI hope that somebody can I help me with this situation.\nRegards.. ",
    "somejfn": "Workaround works for me too. I have great interest in this as we can make some real use of advanced policies and CIDR selectors in an ingress rule.    . ",
    "willise": "@gunjan5 But where to add the flag -c kube-flannel? command like kubectl apply -f kube-flannel.yml -c kube-flannel makes errors.. @ericchiang Got it and thanks.. ",
    "ericchiang": "@willise the error is telling you to provide an argument like kube-flannel to your kubectl log command. That error isn't from flannel.\nError from server (BadRequest): a container name must be specified for pod kube-flannel-ds-hdqb4, choose one of: [kube-flannel install-cni]. ",
    "AlawnWong": "How to deal with this problem?  @willise \nI have the same problem.. ",
    "vglisin": "Starting kubernetes and having flannel working is really a master move. I have everything done by a book for CentOS 7 but flannel gets \"CrashLoopBackOff\". Can you make this working? Is it possible?. ",
    "alexey-medvedchikov": "@tomdee yeah, sure. I've just expected you to check out code for now.\nWe're using flannel in our proprietary K8S bundle with multiAZ AWS setup. We don't have direct communication between terraform and provisioner, so it's impossible to get RT IDs directly. Other option is to use awscli and pass RT IDs into flanneld configuration, but it requires awscli to be installed and some bash glue. From our perspective it's much easier to do this job inside flannel, since it introduces one simple function.\nI've made simple tests with filters configured, checked connectivity. I think AWS e2e tests will be awesome addition.\nI don't see any risk for people not using this feature, since missing sections in config not introducing ones and it's explicitly coded \"if no filters set do the usual way\".. ",
    "FarhadF": "Just to confirm @tomdee , I performed this benchmark:\nOn a virtualized host (VMware ESXi) I have 2 VM's running debian stretch. On both flannel is configured to use vxlan. Using flanneld v0.9.1 for this. VMs on the same host using virtual network and thats why it's pretty fast.\nTest Results:\n1. Using the VM's network interface (VM to VM traffic on same host benchmark):\n```\niperf3 -s\n\nServer listening on 5201\nAccepted connection from 192.168.161.75, port 44782\n[  5] local 192.168.161.219 port 5201 connected to 192.168.161.75 port 44784\n[ ID] Interval           Transfer     Bandwidth\n[  5]   0.00-1.00   sec   765 MBytes  6.41 Gbits/sec                \n[  5]   1.00-2.00   sec   809 MBytes  6.79 Gbits/sec                \n[  5]   2.00-3.00   sec   828 MBytes  6.94 Gbits/sec                \n[  5]   3.00-4.00   sec  1022 MBytes  8.57 Gbits/sec                \n[  5]   4.00-5.00   sec   686 MBytes  5.75 Gbits/sec                \n[  5]   5.00-6.00   sec   671 MBytes  5.63 Gbits/sec                \n[  5]   6.00-7.00   sec   823 MBytes  6.90 Gbits/sec                \n[  5]   7.00-8.00   sec   798 MBytes  6.70 Gbits/sec                \n[  5]   8.00-9.00   sec   747 MBytes  6.26 Gbits/sec                \n[  5]   9.00-10.00  sec   796 MBytes  6.67 Gbits/sec                \n[  5]  10.00-11.00  sec   879 MBytes  7.37 Gbits/sec                \n[  5]  11.00-12.00  sec   792 MBytes  6.65 Gbits/sec                \n[  5]  12.00-13.00  sec   767 MBytes  6.43 Gbits/sec                \n[  5]  13.00-14.00  sec   934 MBytes  7.84 Gbits/sec                \n[  5]  14.00-15.00  sec   798 MBytes  6.70 Gbits/sec                \n[  5]  15.00-16.00  sec   708 MBytes  5.94 Gbits/sec                \n[  5]  16.00-17.00  sec   776 MBytes  6.51 Gbits/sec                \n[  5]  17.00-18.00  sec   731 MBytes  6.13 Gbits/sec                \n[  5]  18.00-19.00  sec   833 MBytes  6.99 Gbits/sec                \n[  5]  19.00-20.00  sec   788 MBytes  6.61 Gbits/sec                \n[  5]  20.00-21.00  sec   759 MBytes  6.37 Gbits/sec                \n[  5]  21.00-22.00  sec   820 MBytes  6.88 Gbits/sec                \n[  5]  22.00-23.00  sec   774 MBytes  6.49 Gbits/sec                \n[  5]  23.00-24.00  sec   771 MBytes  6.47 Gbits/sec                \n[  5]  24.00-25.00  sec   817 MBytes  6.85 Gbits/sec                \n[  5]  25.00-26.00  sec   813 MBytes  6.82 Gbits/sec                \n[  5]  26.00-27.00  sec   753 MBytes  6.32 Gbits/sec                \n[  5]  27.00-28.00  sec   838 MBytes  7.03 Gbits/sec                \n[  5]  28.00-29.00  sec   798 MBytes  6.70 Gbits/sec                \n[  5]  29.00-30.00  sec   821 MBytes  6.88 Gbits/sec                \n[  5]  30.00-30.04  sec  25.9 MBytes  5.50 Gbits/sec                  \n\n[ ID] Interval           Transfer     Bandwidth\n[  5]   0.00-30.04  sec  0.00 Bytes  0.00 bits/sec                  sender\n[  5]   0.00-30.04  sec  23.4 GBytes  6.68 Gbits/sec                  receiver\n```\nSo 23.4GBytes Total data transfered in 30 secs with average 6.68Gbit/sec bandwidth.\n\nUsing flanneld vxlan ip:\n```\nAccepted connection from 10.201.51.0, port 41426\n[  5] local 10.201.8.1 port 5201 connected to 10.201.51.0 port 41428\n[ ID] Interval           Transfer     Bandwidth\n[  5]   0.00-1.00   sec   543 MBytes  4.55 Gbits/sec                \n[  5]   1.00-2.00   sec   374 MBytes  3.14 Gbits/sec                \n[  5]   2.00-3.00   sec   411 MBytes  3.45 Gbits/sec                \n[  5]   3.00-4.00   sec   367 MBytes  3.08 Gbits/sec                \n[  5]   4.00-5.00   sec   383 MBytes  3.22 Gbits/sec                \n[  5]   5.00-6.00   sec   339 MBytes  2.84 Gbits/sec                \n[  5]   6.00-7.00   sec   350 MBytes  2.93 Gbits/sec                \n[  5]   7.00-8.00   sec   361 MBytes  3.03 Gbits/sec                \n[  5]   8.00-9.00   sec   402 MBytes  3.37 Gbits/sec                \n[  5]   9.00-10.00  sec   364 MBytes  3.06 Gbits/sec                \n[  5]  10.00-11.00  sec   406 MBytes  3.41 Gbits/sec                \n[  5]  11.00-12.00  sec   349 MBytes  2.92 Gbits/sec                \n[  5]  12.00-13.00  sec   322 MBytes  2.70 Gbits/sec                \n[  5]  13.00-14.00  sec   371 MBytes  3.11 Gbits/sec                \n[  5]  14.00-15.00  sec   367 MBytes  3.08 Gbits/sec                \n[  5]  15.00-16.00  sec   367 MBytes  3.08 Gbits/sec                \n[  5]  16.00-17.00  sec   398 MBytes  3.34 Gbits/sec                \n[  5]  17.00-18.00  sec   322 MBytes  2.70 Gbits/sec                \n[  5]  18.00-19.00  sec   367 MBytes  3.07 Gbits/sec                \n[  5]  19.00-20.00  sec   339 MBytes  2.84 Gbits/sec                \n[  5]  20.00-21.00  sec   360 MBytes  3.02 Gbits/sec                \n[  5]  21.00-22.00  sec   356 MBytes  2.99 Gbits/sec                \n[  5]  22.00-23.00  sec   359 MBytes  3.01 Gbits/sec                \n[  5]  23.00-24.00  sec   375 MBytes  3.15 Gbits/sec                \n[  5]  24.00-25.00  sec   336 MBytes  2.81 Gbits/sec                \n[  5]  25.00-26.00  sec   338 MBytes  2.83 Gbits/sec                \n[  5]  26.00-27.00  sec   483 MBytes  4.05 Gbits/sec                \n[  5]  27.00-28.00  sec   496 MBytes  4.16 Gbits/sec                \n[  5]  28.00-29.00  sec   359 MBytes  3.01 Gbits/sec                \n[  5]  29.00-30.00  sec   355 MBytes  2.98 Gbits/sec                \n[  5]  30.00-30.04  sec  23.7 MBytes  4.70 Gbits/sec                  \n\n\n[ ID] Interval           Transfer     Bandwidth\n[  5]   0.00-30.04  sec  0.00 Bytes  0.00 bits/sec                  sender\n[  5]   0.00-30.04  sec  11.1 GBytes  3.17 Gbits/sec                  receiver\n```\nSo 11.1GBytes Total data transfered in 30 secs with average 3.17Gbit/sec bandwidth.\nI'm seeing about 53% degrade in performance.. ",
    "xh3b4sd": "\nI don't fully understand your problem, so maybe you can add a little more detail.\n\nYou may want to ask a specific question. I was under the impression to provide a detailed description of the problem. I cannot be more specific out of thin air. \n\nThe filesystem state is only a single file.\nThe network stack is left up on purpose, in order to facilitate zero-downtime restart/upgrade.\n\nEven though this is only a single file it would be better to not \"pollute\" the server host. I fear to reuse certain configuration by accident which is not supposed to be reused because another bridge was created for the same VNI. The bottom line is, when something creates state on demand, it should be able to remove this state on demand. Keeping something across restarts/upgrades is fine. What is missing is a teardown/cleanup mechanism which an administrator can invoke via some API, whatever that API might be when talking about Flannel. \n\nI'd love to hear more about the flannel-operator that you're writing but your link above doesn't work.\n\nI just realized the project is still private. We will make it public at some point. . Hey man. Thanks for your efforts here and considering my input. Sounds like I can dream on a green field here. :) \nIn my issue earlier I questioned flannel's interface. My current understanding is that etcd is the interface and the configuration you provide describes the desired state. What I would find nice would be to decide on an interface and then use it consistently. Assuming etcd is the interface then desired state and status updates should be readable/writable through this interface. One cool thing would be to do the custom resource thing when being on Kubernetes. The interface would then be a CRD/CRO. Uncertain how you want to roll with this. \nFrom my point of view only making flannel cleaning up when it is started with a specific flag is quite bad when you want to automate the process. You would need to schedule it once to setup and later schedule it again, but differently to cleanup. Would be cool if you schedule it and use some interface where you can set the desired state and check for some status. So it would be cool to have the operator approach to reconcile state gradually. \nHave a nice weekend. . ",
    "tadas-subonis": "@ecsumed Did you find a solution for this?. @ecsumed I had a problem with this because iptables where disabled on Docker.\nRemoving --iptables=false fixed it.. Also, there is no cni0 interface on master node where one is present on the \"worker\" node.. It looks like the issue was the option \"--iptabales=false\" on docker.service. After I've removed that, it started working properly.. @acomisario \nBasically, I was configuring a manual deployment using ansible.\n\nYes\nYes\nNot really, here is the config:\n```\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket\nWants=docker.socket\n\n[Service]\nEnvironment=GOTRACEBACK=crash\nType=notify\nExecStart=/usr/bin/dockerd \\\n    {% for d in docker_dns_servers %}--dns {{ d }} {% endfor %} \\\n    {% for d in docker_dns_search_domains %}--dns-search {{ d }} {% endfor %} \\\n    {% for o in docker_dns_options %}--dns-opt {{ o }} {% endfor %} \\\n    --iptables=false \\\n    --insecure-registry={{service_cluster_ip_range}} \\\n    --log-opt max-size=50m \\\n    --log-opt max-file=5\nExecReload=/bin/kill -s HUP $MAINPID\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\nTasksMax=infinity\nTimeoutStartSec=0\nTimeoutStartSec=1min\nRestart=on-abnormal\nDelegate=yes\nKillMode=process\n[Install]\nWantedBy=multi-user.target\n```\n4. There is one worker and it has all the mentioned interfaces. The master node is/was missing flannel.1.\nBasically, I was configuring a manual deployment using ansible.. ",
    "deimosfr": "@ecsumed have the same issue. Any solution ?. I didn't have --iptables=false so it's not the problem. Any other idea ?. ",
    "ecsumed": "@tadas-subonis @deimosfr Nope. I retried the guide multiple times and made sure I ran it down to the dot. After about a week I moved on to kubespray which worked without any hitch.\nI can't recall if I tried removing iptables=false, but good to know.. ",
    "pmagos00": "10.3.0.1 must be the kubernetes svc.\nHaving the same issue, digging if is something related to RBAC ... \nFor me, this issue is happening after the Node evicted all the kube-system Pods after a DiskPressure condition ... . ",
    "mgleung": "@caseydavenport Just ran the go tests and ran this locally in flannel.  I'm currently fixing some issues with my kubernetes test setup but that's the last thing I have planned for testing.. @caseydavenport got my tests working in kubernetes.  Looks like this should be good to go from what I can tell.. Added some documentation.  Let me know if it is clear enough.. That seems to be unexpected behavior since I would have expected your docker0 bridge to have the unencapsulated packets.  Can you make sure that your docker daemon and your flannel config have the ip-masq option set to false and try restarting docker to see if that works?  I noticed that the ip-masq option for the docker daemon defaults to true so that might possibly be the issue.. Can you post the output of iptables-save? I also vaguely recall a similar issue to this for flannel on kubernetes that was resolved which I thought had something to do with the iptables rules. Can you also try enabling the flannel ip-masq option? I noticed that its enabled as a part of a default kubernetes install using the example manifest files, and I was wondering if that will make a difference.. godotenv only reads from env files sadly. I wanted to avoid changing the AcquireLease function to accept additional arguments since that would require the kube manager to also be updated when it does not need a subnet ip (since it is not affected by this).  I could change it but I wanted to avoid changing the interface signatures.. Since I was adding a variable to the local manager, I thought that since it was currently being built according to the values passed in from this EtcdConfig that this would be the best place to pass in that value.  Looking back at it, this config really only applies to values used to set up a connection to etcd so the value should be moved out for organizational reasons.  Downside of moving it out is it clutters up the NewLocalManager constructor a little.. ",
    "fatg1988": "@spacexnice \u4f60\u597d \u8fd9\u662f\u963f\u91cc\u4e91\u4e24\u4e2a\u8d26\u6237\u4e0b\u9762\u7684\u4e24\u53f0ecs \uff0c\u662f\u4e0d\u662f\u7528ali-vpc\u8fd9\u4e2a\u7c7b\u578b \u5c31\u53ef\u4ee5\u4e86~~~. ",
    "answer1991": "Hi @tomdee , thanks for your reply.\nI found the bad route is created by the vm clone template. I re-cloned the machine and try to reproduce this issue, the ip route command result : \n$ ip route\ndefault via 10.244.31.247 dev eth0\n10.0.0.0/8 via 10.244.31.247 dev eth0\n10.244.16.0/20 dev eth0  proto kernel  scope link  src 10.244.25.48\n11.0.0.0/8 via 10.244.31.247 dev eth0\n30.0.0.0/8 via 10.244.31.247 dev eth0\n100.64.0.0/10 via 10.244.31.247 dev eth0\n169.254.0.0/16 dev eth0  scope link  metric 1002\n172.16.0.0/12 via 10.244.31.247 dev eth0\n192.168.0.0/16 via 10.244.31.247 dev eth0\nAfter del 192.168.0.0/16 via 10.244.31.247 dev eth0 rule, and create the network, then the correct route rule generated.\nThanks for your reply. I will close this issue.. ",
    "yurchenkosv": "@tomdee Thanks for your answer. Actualy, I've tried many ways to get it run. If I've specify host network it's run as I expecting, but question is no one pod can communicate with internal k8s services. And no one pod can resolve dns name of another pod because of no traffic can reach pods via pod network. This is very sad, because I've no ability to use internal k8s services and all connections with pods can only be established via host network.\nIt seems, that something wrong with iptables or flannel configuration, but I can't locate the problem.. Here is my configs, if it's might be helpfull\ncat /etc/systemd/system/flanneld.service.d/50-network-config.conf\n[Service]\nEnvironment=\"FLANNELD_ETCD_PREFIX=/coreos.com/network/\"\nEnvironment=\"FLANNELD_IFACE=172.16.16.87\"\nEnvironment=\"FLANNELD_ETCD_ENDPOINTS=http://kube-test-node1:2379\"\nExecStartPre=\"/bin/echo -e  'FLANNELD_IFACE=172.16.16.87 \\nFLANNELD_ETCD_ENDPOINTS=http://kube-test-node1:2379 \\nFLANNELD_ETCD_PREFIX=/coreos.com/network' > /etc/flannel/options.env\"\nExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env\ncat /etc/flannel/options.env\nFLANNELD_IFACE=172.16.16.87\nFLANNELD_ETCD_ENDPOINTS=http://kube-test-node1:2379\ncat /etc/systemd/system/docker.service.d/40-flannel.conf\n[Unit]\nRequires=flanneld.service\nAfter=flanneld.service\n[Service]\nEnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env\ncat /etc/kubernetes/cni/docker_opts_cni.env\nDOCKER_OPT_BIP=\"\"\nDOCKER_OPT_IPMASQ=\"\"\ncat /etc/kubernetes/cni/net.d/10-flannel.conf\n{\n    \"name\": \"podnet\",\n    \"type\": \"flannel\",\n    \"delegate\": {\n        \"isDefaultGateway\": true\n    }\n}. ",
    "wjp719": "@tomdee Hi, in my environment, flannel cannot get the network config from etcdv3 with v2 api, does it need additional configuration in etcd or flannel, thanks . ",
    "lghinet": "same problem here . ",
    "justdigitalfilm": "Did anyone ever resolve this issue?  It seems to still be effecting Flannel 0.7.1 >> etcd 3.3.11. ",
    "zq-david-wang": "@tomdee  Glad to hear that~!. #765 #519 . The log indicates that flanneld is being killed/stopped somehow, which would cause some goroutines return something strange. I do not think it is about iptables. Most likely, your pod was killed during startup\n\nI0424 21:18:12.079484       1 main.go:322] Waiting for all goroutines to exit\nI0424 21:18:12.079516       1 vxlan_network.go:60] watching for new subnet leases\nE0424 21:47:24.568958       1 iptables.go:97] Failed to ensure iptables rules: Error checking rule existence: failed to check rule existence: running [/sbin/iptables -t nat -C POSTROUTING -s 172.20.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE --wait]: exit status 4: iptables: Resource temporarily unavailable.\n\nBase on the timestamps, the error message showed up about 30min after  flanneld started to shutdown,  in my experience this could indicates resource shortage on your system. (e.g. fork bomb). @dtshepherd sorry about my unthoughtful statements above. I run flannel via systemd, and the log \"Waiting for all goroutines to exit\" would not be printed out until  flanneld break out its loop, and if flanneld is killed right after it started, I always notice error logs about 'iptables'.  I rushed to a conclusion that what I experienced is similar to yours. But after checking with code, it turned out that when --kube-subnet-mgr is used, the code for lease monitoring loop is skipped and that line of log would always printed out....\nChecking with flannel code, it seems that it would retry every \"iptables-resync\" seconds, defult is 5s\nhttps://github.com/coreos/flannel/blob/8a083a890a4820fe97fa315dc1ecaa739c1d14db/network/iptables.go#L94-L100\n```\n    for {\n        // Ensure that all the iptables rules exist every 5 seconds\n        if err := ensureIPTables(ipt, rules); err != nil {\n            log.Errorf(\"Failed to ensure iptables rules: %v\", err)\n        }\n    time.Sleep(time.Duration(resyncPeriod) * time.Second)\n\n```\nSorry again... Just tried to help..... ",
    "IanLuites": "Thanks for the response!\nI started with replacing them with environment variables when I noticed the difference in the arguments.\nI've used single - in stead of --. Funny that it parsed the arguments with single dashes when I manually entered them.\n```\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nAfter=etcd.service\nBefore=docker.service\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nExecStart=/usr/local/bin/flanneld \\\n  --iface=10.132.57.158 \\\n  --etcd-endpoints=\"https://10.132.57.158:2379\" \\\n  --etcd-prefix=/kubernetes/network \\\n  --etcd-cafile=/lib/kubernetes/cert/ca.pem \\\n  --etcd-certfile=/lib/kubernetes/cert/kubernetes-etcd.pem \\\n  --etcd-keyfile=/lib/kubernetes/cert/kubernetes-etcd-key.pem \\\n  --public-ip=10.132.57.158\n[Install]\nWantedBy=multi-user.target\n```\nIs now working! Sorry for the trouble, I'll close it.. ",
    "doronoffir": "Hi Tom,\nThank you for your answer, I have reached KOPS guys as well.\nI have another question regarding flannel performance I hope you could\nprovide me with an insight. In this setup we are running also Elasticsearch\ncluster that are been feed from out scrappers, and we experience high\nlatency.\nCould it be relevant in any way to Flannel or should we look in another\ndirection?\nThanks again and all of the very best,\n-Doron\nOn 14 July 2017 at 01:04, Tom Denham notifications@github.com wrote:\n\nHi @Doron-offir https://github.com/doron-offir, sorry you hit these\nissues. I suspect that you were hitting the problem where flannel wouldn't\nstart with kube-subnet-mgr and >100 nodes. #719\nhttps://github.com/coreos/flannel/issues/719 This is fixed in v0.8.0 (\nhttps://github.com/coreos/flannel/releases/tag/v0.8.0) so I suggest you\nraise an issue with Kops to get it updated to the new flannel release.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/770#issuecomment-315214891, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGpQ4u282_VZ9VbV1wevSWb_GiMMODcQks5sNpRRgaJpZM4OWtho\n.\n. \n",
    "cwalker67": "@tomdee try running the arm version\n$ docker run quay.io/coreos/flannel:v0.8.0-arm\nstandard_init_linux.go:178: exec user process caused \"no such file or directory\". @tomdee \nScaleway's ARMv8 64 bit servers don't equate to a rpi3 ;)\n$ docker run quay.io/coreos/flannel:v0.8.0-arm64\nstandard_init_linux.go:178: exec user process caused \"exec format error\"\n. in examining the images, i've found the the libc.so.6 is substantially smaller than the one it replaces in the copy\n * new libc.so.6 is 894700\n * old libc.so.6  is 1275540\nThis may have nothing to do with it but I do notice that the sizes being copied for the arm64 version is larger 1259184\n. ",
    "cdickson-sum": "@tomdee thank you for starting to look at this, but the issue is with the 32 bit ARM version.. ",
    "jimmycuadra": "Same problem here. Brand new Kubernetes cluster using kubeadm, both v1.7.1. flannel v0.8.0-amd. Raspberry Pi 3.\n$ sudo docker images | grep flannel\nquay.io/coreos/flannel                                 v0.8.0-arm          450a18ed92ec        8 days ago          35.14 MB\n$ sudo docker ps\nCONTAINER ID        IMAGE                                                                                                                          COMMAND                  CREATED              STATUS                  PORTS               NAMES\n208d94700252        quay.io/coreos/flannel@sha256:be0d1e3cc5f7dbde66243d3b5e99a7d194755416dcf365a155304193fa6aca7e                                 \"/bin/sh -c 'set -e -\"   2 seconds ago        Up Less than a second                       k8s_install-cni_kube-flannel-ds-6x7tq_kube-system_1800fd69-6d1c-11e7-b2b2-b827ebfa0d18_3\n$ sudo journalctl CONTAINER_ID=208d94700252\n-- Logs begin at Thu 2017-07-20 00:05:25 PDT, end at Thu 2017-07-20 00:23:36 PDT. --\nJul 20 00:23:14 redacted dockerd[759]: /bin/sh: error while loading shared libraries: libc.so.6: cannot open shared object file: No such file or directory\n$ uname -a\nLinux redacted 4.9.35-v7+ #1014 SMP Fri Jun 30 14:47:43 BST 2017 armv7l GNU/Linux\n$ cat /etc/os-release\nPRETTY_NAME=\"Raspbian GNU/Linux 8 (jessie)\"\nNAME=\"Raspbian GNU/Linux\"\nVERSION_ID=\"8\"\nVERSION=\"8 (jessie)\"\nID=raspbian\nID_LIKE=debian\nHOME_URL=\"http://www.raspbian.org/\"\nSUPPORT_URL=\"http://www.raspbian.org/RaspbianForums\"\nBUG_REPORT_URL=\"http://www.raspbian.org/RaspbianBugs\". I just switched from flannel to weave and it's been working fine.. @asaaki It was a new cluster. Can't comment about switching from one to another, as I haven't tried that, but I don't see why it wouldn't work in theory. Perhaps if you share specifics of what went wrong, someone can help figure out your problem.. ",
    "stevesloka": "Hey, @tomdee I'm happy to test out any builds. I'm seeing the same error mentioned above on my arm cluster. . ",
    "ikester": "Is there a workaround for using Flannel with K8s v1.7.x on ARM? Using v0.7?\nEdit: I was able to deploy v0.7.1 of Flannel with a K8s v1.7.3 cluster. Looking forward to seeing v0.8 working on ARM.. ",
    "swestcott": "I also had an issue with flannel 0.8 on my pi3,\npi@pi3:~ $ kubectl logs kube-flannel-ds-qnjc3 -n kube-system -c kube-flannel\nstandard_init_linux.go:178: exec user process caused \"no such file or directory\"\nFlannel 0.7.1 is working ok. ",
    "asaaki": "@jimmycuadra Did you set up a fresh cluster or switched within the running one? The latter didn't really work for me. :-/. ",
    "bitvector2": "How can I help with this?  I have a cluster of RPIs just waiting to help?\n. ",
    "ecliptik": "@tomdee I haven't deployed Flannel v0.9.0-rc1 to my k8s Raspberry Pi 3 cluster, but the image does run now and no longer gives a standard_init_linux.go:178: exec user process caused \"no such file or directory that I was seeing with v0.8.0-arm.\n```\nHypriotOS/armv7: pirate@navi in /var/tmp\n$ uname -a\nLinux navi 4.4.50-hypriotos-v7+ #1 SMP PREEMPT Sun Mar 19 14:11:54 UTC 2017 armv7l GNU/Linux\nHypriotOS/armv7: pirate@navi in /var/tmp\n$ docker run -it --rm quay.io/coreos/flannel:v0.8.0-arm\nstandard_init_linux.go:178: exec user process caused \"no such file or directory\"\nHypriotOS/armv7: pirate@navi in /var/tmp\n$ docker run -it --rm quay.io/coreos/flannel:v0.9.0-rc1-arm\nI0919 03:43:26.072450       1 main.go:479] Determining IP address of default interface\nI0919 03:43:26.074220       1 main.go:492] Using interface with name eth0 and address 172.17.0.2\nI0919 03:43:26.074385       1 main.go:509] Defaulting external address to interface address (172.17.0.2)\nI0919 03:43:26.075041       1 main.go:234] Created subnet manager: Etcd Local Manager with Previous Subnet: 0.0.0.0/0\nI0919 03:43:26.075134       1 main.go:237] Installing signal handlers\nE0919 03:43:26.081125       1 main.go:353] Couldn't fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused\n; error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused\ntimed out\nE0919 03:43:27.083944       1 main.go:353] Couldn't fetch network config: client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused\n; error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused\n^CI0919 03:43:27.674273       1 main.go:341] shutdownHandler sent cancel signal...\n```. ",
    "danmikita": "@tomdee I tried the v0.9.0-rc1 image and it fixed the problem. Everything is working perfect! . ",
    "rickypai": "thanks for merging this. sorry i sort of forgot about this PR. I can update https://github.com/coreos/flannel/blob/master/Documentation/Kubernetes.md to indicate installation requirements. ",
    "rakelkar": "Aargh - did not mean to raise a PR on coreos just yet!. Thanks for taking a look Tom. Your comments are very reasonable. I'm going\nto update the PR with changes I've made for VXLAN and address the easy\nfixes like removing logrus and adding some unit tests. Will get back to you\non how best to build/integration test after I get a chance to huddle with\nothers on the Windows networking team. Just letting you know so you're not\nsurprised to see the PR increase in feature scope before your comments are\naddressed - this will allow me to avoid merge madness.\nOn Fri, Oct 13, 2017 at 4:28 PM, Tom Denham notifications@github.com\nwrote:\n\n@tomdee requested changes on this pull request.\nThanks for this PR. I've taken a look and it's looking broadly OK, though\nI do have a few questions/comments.\n\nCommon code\n   What are your thoughts on all the duplicate code in hostgw_*.go? The\n   Windows and Linux versions share quite a bit of code, do you think it\n   should be pulled out?\nTesting\n   How will the flannel.exe binary be automatically tested? TravisCI\n   doesn't support windows so we need some alternative.\n   You'll need to add some windows specific tests before this can be\n   merged.\nMaintenance\n   Who is going to be responsible for ongoing maintenance for Windows\n   support in flannel? I am likely going to need help in this area since I\n   don't have a Windows computer. I'm thinking of things like Windows specific\n   test failing, reviewing Windows specific PR and providing Windows support.\nDocumentation\n   This PR will need to include some Windows specific documentation\n\nAlso, you shouldn't have added logrus as a submodule in vendor. If you\nneed it to be updated you should do it through glide.\n\nIn backend/hostgw/hostgw_network_windows.go\nhttps://github.com/coreos/flannel/pull/832#discussion_r144674198:\n\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package hostgw\n+\n+import (\n+ \"sync\"\n+ \"time\"\n+\n+ log \"github.com/golang/glog\"\n+ \"golang.org/x/net/context\"\n+\n+ \"github.com/coreos/flannel/backend\"\n+ \"github.com/coreos/flannel/subnet\"\n+\n+ netroute \"github.com/rakelkar/gonetsh/netroute\"\n\nThis will need to be added to the glide.* files\nIn backend/hostgw/hostgw_windows.go\nhttps://github.com/coreos/flannel/pull/832#discussion_r144674481:\n\n+// You may obtain a copy of the License at\n+//\n+//     http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package hostgw\n+\n+import (\n+ \"fmt\"\n+\n+ \"github.com/Microsoft/hcsshim\"\n\nthis will also need to be added to glide\nIn backend/vxlan/vxlan_windows.go\nhttps://github.com/coreos/flannel/pull/832#discussion_r144674732:\n\n@@ -0,0 +1,25 @@\n+// +build windows\n\nDo you need this file since vxlan isn't currently supported on Windows?\nIn network/ipmasq_windows.go\nhttps://github.com/coreos/flannel/pull/832#discussion_r144674790:\n\n@@ -0,0 +1,34 @@\n+// +build windows\n\nWhy have a windows version of this file?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/pull/832#pullrequestreview-69370341,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADQphVIlA5QEGEzKQh_Nbv4QL5NTfcA2ks5sr_IhgaJpZM4PtGcf\n.\n\n\n-- \nRakesh Kelkar\n. Snuff out logrus (it snuck in). Can do..\nOn Oct 20, 2017 9:14 AM, \"Tom Denham\" notifications@github.com wrote:\n\n@rakelkar https://github.com/rakelkar If it's possible to submit\nsmaller PRs then that's helpful for me. e.g. moving the ipmasq code out of\nmain might be a generally applicable change which doesn't depend on the\nWindows changes.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/pull/832#issuecomment-338249932, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADQpheSFml_G2st5OIjYHffbkBQptlZBks5suMSTgaJpZM4PtGcf\n.\n. @tomdee We are planning to break this PR up into the following new PRs:\nPR1: ipmasq to build for windows - this is a just refactor, hopefully can be approved without too much controversy :)\nPR2: build for windows, appveyor, windows iface code+gonetsh via glide\nThis PR will focus on getting Flannel to build on windows, with a little bit of functionality to determine the default interface. \nPR3a: hostgw\nThis PR will add _windows files for hostgw functionality\nPR3b: vxlan\nThis PR will add _windows files for vxlan functionality\n\n. Maintenance: The team that own the Windows Host Networking Services has agreed to own maintenance of these features. This is ofcourse dependent on automated testing, still poking about to figure out best to automate.. containernetworking/plugins#77. containernetworking/plugins#76. Yes we are working towards making this available... Would you like to try\nearly bits to see if it works for you?\nOn Dec 6, 2017 1:58 AM, \"Konstantin Pozdniakov\" notifications@github.com\nwrote:\n\nHi!\nAre there any plans when this would be available for use on Windows K8S\nsystems?\nThanks!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/833#issuecomment-349590897, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ADQphXAyihTEssi399tHY5veDl_Y-kkgks5s9mVXgaJpZM4PtGg_\n.\n. @cspwizard @rjmorse \nI've posted some slides that describe how flannel works on windows and how to set it up on the k8s sig-windows slack channel... have a look and give it a shot. We're working with the Flannel maintainers to get this merged step by step :) currently stuck at #891  . @kdomanski for the l2bridge issue - this is a known issue... @jroggeman  made a fix for this in the underlying gonetsh library, but we didn;t update the PR (since it was closed). I need to create a new PR that includes this fix.. For overlay - when did you build the overlay plugin? @banatm made a fix in https://github.com/rakelkar/plugins/tree/windowsCni - can you make sure you have the fix?. For L2Bridge try #921 that should have the fix for:\n\"Error registering network: unable to enable forwarding on [vEthernet (Ethernet)], error: failed to enable forwarding on [vEthernet (Ethernet)], error: exit status 1. cmd: int ipv4 set int \"vEthernet (Ethernet)\" for=en. stdout: The parameter is incorrect.\". Here is how I build CNI (not a GO expert so feel free to critique my method).\n(on a windows machine...)\ncd $GOPATH\\github.com\\\nmkdir containernetworking\ncd containernetworking\ngit clone https://github.com/rakelkar/plugins\ngo build -o overlay.exe  github.com\\containernetworking\\plugins\\plugins\\main\\windows\\overlay. Figured out forwarding issue and crash issue.. raised pull request in underlying lib for the forwarding issue: https://github.com/rakelkar/gonetsh/pull/6. @dmitryzykov I've updated #921 with fixes - but to use it you still need to update gonetsh (see comment on the PR). @madhanrm the host-gw fails for me with HNS errors that I dont understand. We've made fixes for netsh etc. to work that @dmitryzykov saw - but unable to figure out HNS errors. Can you give it a try when you get a chance? To build you need #921 but also need latest from https://github.com/rakelkar/gonetsh (to do that just delete the vendored gonetsh - and go will use the copy you cloned locally). #833 . @tomdee \n\nWe've added an appveyor build YML to enable CI - however to work you need enable it for the repo in repo settings. Its free for open source projects and is what was just done to get the CNI repo to do CI for windows, see https://github.com/containernetworking/plugins/pull/84\nWe also preferred adding !windows instead of naming the files _linux.. this leaves them free to compile on *nix and only windows code will be _windows (from subsequent commits) - this feels cleaner.... @tomdee Thanks for reviewing and merging. Can enable AppVeyor CI to make sure it keeps building?. #833 . #833 . @tomdee I am having trouble updating just the gonetsh vendor version. What is the correct way to do this - issues online appear to suggest editing the lock file manually. Do you have thoughts on how to do? I want to update to https://github.com/rakelkar/gonetsh/commit/758b1f7c9d1ca5acaca0b909f06ff9ed9fe35de1 . @anikundesu nice find! Could open a PR into the source branch with your fix?. @madhanrm to review. Merged VXLAN support. Had to move some IPMASQ functions out of main - see updated file.. ",
    "delfer": "With host-gw and fix from https://github.com/kubernetes-incubator/kubespray/blob/master/docs/openstack.md\n\ncore@test-k8s-node-nf-2 ~ $ docker run -it --rm networkstatic/iperf3 -c 10.233.77.4\nConnecting to host 10.233.77.4, port 5201\n[  4] local 10.233.100.3 port 36420 connected to 10.233.77.4 port 5201\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  98.3 MBytes   825 Mbits/sec    4    376 KBytes\n[  4]   1.00-2.00   sec   106 MBytes   886 Mbits/sec   33    537 KBytes\n[  4]   2.00-3.00   sec   104 MBytes   873 Mbits/sec    0    660 KBytes\n[  4]   3.00-4.00   sec   105 MBytes   882 Mbits/sec    0    767 KBytes\n[  4]   4.00-5.00   sec   108 MBytes   904 Mbits/sec    0    862 KBytes\n[  4]   5.00-6.00   sec   107 MBytes   898 Mbits/sec    0    946 KBytes\n[  4]   6.00-7.00   sec   106 MBytes   891 Mbits/sec    0   1.00 MBytes\n[  4]   7.00-8.00   sec   107 MBytes   901 Mbits/sec  158   1.07 MBytes\n[  4]   8.00-9.00   sec   108 MBytes   902 Mbits/sec    0   1.14 MBytes\n[  4]   9.00-10.00  sec   109 MBytes   912 Mbits/sec    0   1.20 MBytes\n\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-10.00  sec  1.03 GBytes   887 Mbits/sec  195             sender\n[  4]   0.00-10.00  sec  1.03 GBytes   885 Mbits/sec                  receiver\niperf Done.\n. With udp\ncore@testing-k8s-node-nf-2 ~ $ docker run -it --rm networkstatic/iperf3 -c 10.233.76.4\nConnecting to host 10.233.76.4, port 5201\n[  4] local 10.233.65.3 port 56772 connected to 10.233.76.4 port 5201\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  9.97 MBytes  83.6 Mbits/sec    9    139 KBytes    \n[  4]   1.00-2.00   sec  7.31 MBytes  61.3 Mbits/sec   23   77.6 KBytes    \n[  4]   2.00-3.00   sec  9.33 MBytes  78.3 Mbits/sec   12    106 KBytes    \n[  4]   3.00-4.00   sec  10.4 MBytes  87.5 Mbits/sec   15    103 KBytes    \n[  4]   4.00-5.00   sec  7.80 MBytes  65.3 Mbits/sec   14   87.0 KBytes    \n[  4]   5.00-6.00   sec  9.46 MBytes  79.4 Mbits/sec    7   82.9 KBytes    \n[  4]   6.00-7.00   sec  8.90 MBytes  74.7 Mbits/sec    5    108 KBytes    \n[  4]   7.00-8.00   sec  8.68 MBytes  72.8 Mbits/sec   15   88.3 KBytes    \n[  4]   8.00-9.00   sec  8.97 MBytes  75.2 Mbits/sec    3   89.6 KBytes    \n[  4]   9.00-10.00  sec  9.52 MBytes  79.8 Mbits/sec    6   91.0 KBytes      \n\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-10.00  sec  90.4 MBytes  75.8 Mbits/sec  109             sender\n[  4]   0.00-10.00  sec  90.1 MBytes  75.6 Mbits/sec                  receiver\niperf Done.. With vxlan and flannel v0.8.0\ncore@testing-k8s-node-nf-2 ~ $ docker run -it --rm networkstatic/iperf3 -c 10.233.126.4\nConnecting to host 10.233.126.4, port 5201\n[  4] local 10.233.89.3 port 51086 connected to 10.233.126.4 port 5201\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  52.7 KBytes   430 Kbits/sec   17   2.63 KBytes\n[  4]   1.00-2.00   sec   411 KBytes  3.37 Mbits/sec  154   2.63 KBytes\n[  4]   2.00-3.00   sec   432 KBytes  3.54 Mbits/sec  154   2.63 KBytes\n[  4]   3.00-4.00   sec   424 KBytes  3.47 Mbits/sec  158   2.63 KBytes\n[  4]   4.00-5.00   sec   421 KBytes  3.45 Mbits/sec  160   2.63 KBytes\n[  4]   5.00-6.00   sec   427 KBytes  3.49 Mbits/sec  162   2.63 KBytes\n[  4]   6.00-7.00   sec   469 KBytes  3.84 Mbits/sec  162   2.63 KBytes\n[  4]   7.00-8.00   sec   424 KBytes  3.47 Mbits/sec  160   2.63 KBytes\n[  4]   8.00-9.00   sec   432 KBytes  3.54 Mbits/sec  164   2.63 KBytes\n[  4]   9.00-10.00  sec   429 KBytes  3.52 Mbits/sec  162   2.63 KBytes\n\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-10.00  sec  3.83 MBytes  3.21 Mbits/sec  1453             sender\n[  4]   0.00-10.00  sec  3.79 MBytes  3.18 Mbits/sec                  receiver\niperf Done.. @philips My environment is OpenStack with vxlan Neutron. I will check ethtool.. \n",
    "ayaz": "@tomdee @gunjan5 When can we hope to have this available officially? I have just had this bug hit me on a perfectly running cluster. . ",
    "s-urbaniak": "Question: Although inefficient and prohibiting static binaries, the UDP backend turned out to be an escape hatch in some environments, where VXLAN or other methods of network encapsulation are unsupported/broken/restricted. It is a bit of a pity that this is simply removed. Can this backend at least still live in a separate build/repo?\n/cc @crawford @squeed @sym3tri . ",
    "sym3tri": "@s-urbaniak yes that was a handy escape hatch to have.\n@aaronlevy do you have an opinion on this?. ",
    "squat": "We had to write a service to disable tx checksum offloading for Tectonic clusters on Azure [0]; it would be much nicer and more maintainable to do this in flannel configuration in a configmap than using systemd units.\n[0] https://github.com/coreos/tectonic-installer/pull/1586\nBelow are some iperf benchmarks we ran to compare how different MTU and tx checksum offloading combinations would perform. We found that disabling TX checksum offloading and keeping the default MTU does not result in a major performance penalty. By only disabling tx checksum offloading we can take advantage of any future changes in Azure network MTU changes without having to modify existing deployments. This will be the most maintainable workaround going forward.\n\niperf.yaml:\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: iperf\n  namespace: default\n  labels:\n    app: iperf\nspec:\n  replicas: 1\n  template:\n    metadata:\n      name: iperf\n      labels:\n        app: iperf\n    spec:\n      containers:\n      - name: iperf-server\n        image: networkstatic/iperf3\n        args:\n        - -s\n        ports:\n        - containerPort: 5201\n          protocol: TCP\n      nodeSelector:\n        node-role.kubernetes.io/node: \"\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: iperf\n  namespace: default\n  labels:\n    app: iperf\nspec:\n  type: NodePort\n  selector:\n    app: iperf\n  ports:\n  - name: iperf\n    protocol: TCP\n    port: 5201\n    nodePort: 31211\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: iperf\n  namespace: default\n  labels:\n    app: iperf\nspec:\n  template:\n    metadata:\n      name: iperf\n      labels:\n        app: iperf\n    spec:\n      hostNetwork: true\n      containers:\n      - name: iperf\n        image: networkstatic/iperf3\n        args: [\"-c\", \"green.westcentralus.cloudapp.azure.com\", \"-p\", \"31211\", \"-t\", \"30\", \"-V\"]\n      restartPolicy: Never\n```\n\nI got the following results:\n\nMTU 1350 tx on\n```\nTime: Thu, 03 Aug 2017 23:21:12 GMT\nConnecting to host green.westcentralus.cloudapp.azure.com, port 31211\n      Cookie: green-worker-0.1501802472.198990.573\n      TCP MSS: 1228 (default)\n[  4] local 10.0.16.4 port 39306 connected to 52.161.110.8 port 31211\nStarting Test: protocol: TCP, 1 streams, 131072 byte blocks, omitting 0 seconds, 30 second test\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  88.3 MBytes   741 Mbits/sec  802    191 KBytes       \n[  4]   1.00-2.00   sec  82.3 MBytes   690 Mbits/sec  292    170 KBytes       \n[  4]   2.00-3.00   sec  73.1 MBytes   613 Mbits/sec  337    154 KBytes       \n[  4]   3.00-4.00   sec  81.7 MBytes   686 Mbits/sec  258    171 KBytes       \n[  4]   4.00-5.00   sec  84.1 MBytes   706 Mbits/sec  283    229 KBytes       \n[  4]   5.00-6.00   sec  83.1 MBytes   698 Mbits/sec  355    216 KBytes       \n[  4]   6.00-7.00   sec  82.6 MBytes   693 Mbits/sec  318    175 KBytes       \n[  4]   7.00-8.00   sec  72.8 MBytes   611 Mbits/sec  427    109 KBytes       \n[  4]   8.00-9.00   sec  81.3 MBytes   682 Mbits/sec  300    246 KBytes       \n[  4]   9.00-10.00  sec  83.9 MBytes   704 Mbits/sec  355    199 KBytes       \n[  4]  10.00-11.00  sec  83.2 MBytes   698 Mbits/sec  442    199 KBytes       \n[  4]  11.00-12.00  sec  83.0 MBytes   697 Mbits/sec  290    228 KBytes       \n[  4]  12.00-13.00  sec  73.3 MBytes   614 Mbits/sec  341    173 KBytes       \n[  4]  13.00-14.00  sec  81.5 MBytes   684 Mbits/sec  296    216 KBytes       \n[  4]  14.00-15.00  sec  84.1 MBytes   705 Mbits/sec  332    143 KBytes       \n[  4]  15.00-16.00  sec  83.7 MBytes   702 Mbits/sec  191    218 KBytes       \n[  4]  16.00-17.00  sec  83.1 MBytes   697 Mbits/sec  394    162 KBytes       \n[  4]  17.00-18.00  sec  72.7 MBytes   610 Mbits/sec  282    174 KBytes       \n[  4]  18.00-19.00  sec  82.0 MBytes   688 Mbits/sec  399    180 KBytes       \n[  4]  19.00-20.00  sec  83.3 MBytes   698 Mbits/sec  378    140 KBytes       \n[  4]  20.00-21.00  sec  83.4 MBytes   700 Mbits/sec  298    156 KBytes       \n[  4]  21.00-22.00  sec  84.0 MBytes   705 Mbits/sec  351    155 KBytes       \n[  4]  22.00-23.00  sec  72.6 MBytes   608 Mbits/sec  264    215 KBytes       \n[  4]  23.00-24.00  sec  82.1 MBytes   689 Mbits/sec  567    156 KBytes       \n[  4]  24.00-25.00  sec  82.6 MBytes   692 Mbits/sec  363    154 KBytes       \n[  4]  25.00-26.00  sec  83.1 MBytes   698 Mbits/sec  281    176 KBytes       \n[  4]  26.00-27.00  sec  84.1 MBytes   705 Mbits/sec  206    229 KBytes       \n[  4]  27.00-28.00  sec  71.6 MBytes   600 Mbits/sec  436    150 KBytes       \n[  4]  28.00-29.00  sec  83.3 MBytes   699 Mbits/sec  327    189 KBytes       \n[  4]  29.00-30.00  sec  83.1 MBytes   697 Mbits/sec  352    197 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\nTest Complete. Summary Results:\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-30.00  sec  2.38 GBytes   680 Mbits/sec  10517             sender\n[  4]   0.00-30.00  sec  2.37 GBytes   680 Mbits/sec                  receiver\nCPU Utilization: local/sender 1.1% (0.0%u/1.1%s), remote/receiver 0.1% (0.0%u/0.1%s)\n```\nMTU 1350 tx off:\n```\nTime: Thu, 03 Aug 2017 23:43:17 GMT\nConnecting to host green.westcentralus.cloudapp.azure.com, port 31211\n      Cookie: green-worker-0.1501803797.590523.57c\n      TCP MSS: 1228 (default)\n[  4] local 10.0.16.4 port 37416 connected to 52.161.110.8 port 31211\nStarting Test: protocol: TCP, 1 streams, 131072 byte blocks, omitting 0 seconds, 30 second test\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  87.7 MBytes   736 Mbits/sec  889    211 KBytes       \n[  4]   1.00-2.00   sec  70.9 MBytes   592 Mbits/sec  256    204 KBytes       \n[  4]   2.00-3.00   sec  82.2 MBytes   692 Mbits/sec  449    195 KBytes       \n[  4]   3.00-4.00   sec  84.8 MBytes   711 Mbits/sec  137    198 KBytes       \n[  4]   4.00-5.00   sec  83.6 MBytes   701 Mbits/sec  205    212 KBytes       \n[  4]   5.00-6.01   sec  84.3 MBytes   702 Mbits/sec  266    243 KBytes       \n[  4]   6.01-7.01   sec  69.8 MBytes   583 Mbits/sec  250    149 KBytes       \n[  4]   7.01-8.00   sec  83.2 MBytes   705 Mbits/sec  360    161 KBytes       \n[  4]   8.00-9.00   sec  84.6 MBytes   709 Mbits/sec  286    175 KBytes       \n[  4]   9.00-10.00  sec  82.9 MBytes   696 Mbits/sec  457    140 KBytes       \n[  4]  10.00-11.00  sec  82.9 MBytes   695 Mbits/sec  207    270 KBytes       \n[  4]  11.00-12.00  sec  70.6 MBytes   592 Mbits/sec  267    163 KBytes       \n[  4]  12.00-13.00  sec  83.6 MBytes   701 Mbits/sec  255    125 KBytes       \n[  4]  13.00-14.00  sec  84.3 MBytes   707 Mbits/sec  324    118 KBytes       \n[  4]  14.00-15.00  sec  84.4 MBytes   708 Mbits/sec  205    205 KBytes       \n[  4]  15.00-16.00  sec  82.4 MBytes   691 Mbits/sec  317    154 KBytes       \n[  4]  16.00-17.00  sec  70.4 MBytes   590 Mbits/sec  144    211 KBytes       \n[  4]  17.00-18.00  sec  84.1 MBytes   705 Mbits/sec  301    162 KBytes       \n[  4]  18.00-19.00  sec  83.9 MBytes   703 Mbits/sec  292    164 KBytes       \n[  4]  19.00-20.00  sec  83.4 MBytes   699 Mbits/sec  178    151 KBytes       \n[  4]  20.00-21.00  sec  83.5 MBytes   700 Mbits/sec  115    205 KBytes       \n[  4]  21.00-22.00  sec  70.8 MBytes   594 Mbits/sec   90    223 KBytes       \n[  4]  22.00-23.00  sec  84.6 MBytes   710 Mbits/sec  350    235 KBytes       \n[  4]  23.00-24.00  sec  82.6 MBytes   693 Mbits/sec  350    231 KBytes       \n[  4]  24.00-25.00  sec  84.0 MBytes   704 Mbits/sec  403    223 KBytes       \n[  4]  25.00-26.00  sec  82.3 MBytes   690 Mbits/sec  275    229 KBytes       \n[  4]  26.00-27.00  sec  70.6 MBytes   592 Mbits/sec  202    317 KBytes       \n[  4]  27.00-28.00  sec  84.7 MBytes   710 Mbits/sec  602    228 KBytes       \n[  4]  28.00-29.00  sec  82.7 MBytes   694 Mbits/sec  232    228 KBytes       \n[  4]  29.00-30.00  sec  84.3 MBytes   707 Mbits/sec  176    233 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\nTest Complete. Summary Results:\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-30.00  sec  2.38 GBytes   680 Mbits/sec  8840             sender\n[  4]   0.00-30.00  sec  2.37 GBytes   680 Mbits/sec                  receiver\nCPU Utilization: local/sender 1.0% (0.0%u/1.0%s), remote/receiver 0.5% (0.0%u/0.5%s)\n```\n\nMTU 1500 tx off:\n```\nTime: Thu, 03 Aug 2017 23:31:42 GMT\nConnecting to host green.westcentralus.cloudapp.azure.com, port 31211\n      Cookie: green-worker-0.1501803101.996116.571\n      TCP MSS: 1378 (default)\n[  4] local 10.0.16.4 port 53694 connected to 52.161.110.8 port 31211\nStarting Test: protocol: TCP, 1 streams, 131072 byte blocks, omitting 0 seconds, 30 second test\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  85.6 MBytes   718 Mbits/sec  1115    219 KBytes       \n[  4]   1.00-2.00   sec  69.2 MBytes   581 Mbits/sec  305    209 KBytes       \n[  4]   2.00-3.01   sec  82.9 MBytes   692 Mbits/sec  350    237 KBytes       \n[  4]   3.01-4.00   sec  83.3 MBytes   702 Mbits/sec  510    178 KBytes       \n[  4]   4.00-5.00   sec  82.2 MBytes   690 Mbits/sec  306    240 KBytes       \n[  4]   5.00-6.00   sec  80.5 MBytes   676 Mbits/sec  470    175 KBytes       \n[  4]   6.00-7.00   sec  70.4 MBytes   590 Mbits/sec  276    261 KBytes       \n[  4]   7.00-8.00   sec  82.4 MBytes   691 Mbits/sec  643    238 KBytes       \n[  4]   8.00-9.00   sec  82.5 MBytes   692 Mbits/sec  325    214 KBytes       \n[  4]   9.00-10.00  sec  82.1 MBytes   689 Mbits/sec  292    277 KBytes       \n[  4]  10.00-11.00  sec  81.7 MBytes   686 Mbits/sec  286    188 KBytes       \n[  4]  11.00-12.00  sec  69.7 MBytes   585 Mbits/sec  104    203 KBytes       \n[  4]  12.00-13.00  sec  83.7 MBytes   702 Mbits/sec  312    164 KBytes       \n[  4]  13.00-14.00  sec  80.2 MBytes   673 Mbits/sec  245    195 KBytes       \n[  4]  14.00-15.00  sec  83.9 MBytes   703 Mbits/sec  241    184 KBytes       \n[  4]  15.00-16.00  sec  82.1 MBytes   686 Mbits/sec  365    211 KBytes       \n[  4]  16.00-17.00  sec  70.2 MBytes   591 Mbits/sec  561    209 KBytes       \n[  4]  17.00-18.00  sec  82.6 MBytes   693 Mbits/sec  350    223 KBytes       \n[  4]  18.00-19.00  sec  83.1 MBytes   697 Mbits/sec  251    254 KBytes       \n[  4]  19.00-20.00  sec  81.1 MBytes   680 Mbits/sec  487    219 KBytes       \n[  4]  20.00-21.00  sec  82.0 MBytes   688 Mbits/sec  199    226 KBytes       \n[  4]  21.00-22.00  sec  69.5 MBytes   583 Mbits/sec  249    280 KBytes       \n[  4]  22.00-23.00  sec  83.0 MBytes   697 Mbits/sec  798    246 KBytes       \n[  4]  23.00-24.00  sec  82.0 MBytes   688 Mbits/sec  460    252 KBytes       \n[  4]  24.00-25.00  sec  80.4 MBytes   674 Mbits/sec  306    155 KBytes       \n[  4]  25.00-26.00  sec  83.4 MBytes   700 Mbits/sec  267    246 KBytes       \n[  4]  26.00-27.00  sec  70.6 MBytes   592 Mbits/sec  234    217 KBytes       \n[  4]  27.00-28.00  sec  82.7 MBytes   694 Mbits/sec  212    201 KBytes       \n[  4]  28.00-29.00  sec  80.7 MBytes   677 Mbits/sec  289    270 KBytes       \n[  4]  29.00-30.00  sec  84.2 MBytes   706 Mbits/sec  442    225 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\nTest Complete. Summary Results:\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-30.00  sec  2.34 GBytes   671 Mbits/sec  11250             sender\n[  4]   0.00-30.00  sec  2.34 GBytes   670 Mbits/sec                  receiver\nCPU Utilization: local/sender 0.8% (0.0%u/0.8%s), remote/receiver 0.5% (0.0%u/0.4%s)\n```\n\nMTU 2000 tx off:\n```\nTime: Thu, 03 Aug 2017 23:52:56 GMT\nConnecting to host green.westcentralus.cloudapp.azure.com, port 31211\n      Cookie: green-worker-0.1501804376.270876.56c\n      TCP MSS: 1878 (default)\n[  4] local 10.0.16.4 port 33618 connected to 52.161.110.8 port 31211\nStarting Test: protocol: TCP, 1 streams, 131072 byte blocks, omitting 0 seconds, 30 second test\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  77.3 MBytes   649 Mbits/sec  978    226 KBytes       \n[  4]   1.00-2.00   sec  85.1 MBytes   714 Mbits/sec  391    207 KBytes       \n[  4]   2.00-3.00   sec  85.1 MBytes   714 Mbits/sec  395    244 KBytes       \n[  4]   3.00-4.00   sec  85.2 MBytes   715 Mbits/sec  168    262 KBytes       \n[  4]   4.00-5.00   sec  85.0 MBytes   713 Mbits/sec   81    328 KBytes       \n[  4]   5.00-6.00   sec  72.8 MBytes   610 Mbits/sec  271    264 KBytes       \n[  4]   6.00-7.00   sec  85.9 MBytes   720 Mbits/sec  305    242 KBytes       \n[  4]   7.00-8.00   sec  86.6 MBytes   726 Mbits/sec  195    282 KBytes       \n[  4]   8.00-9.00   sec  84.2 MBytes   706 Mbits/sec  378    271 KBytes       \n[  4]   9.00-10.00  sec  84.7 MBytes   711 Mbits/sec  316    303 KBytes       \n[  4]  10.00-11.00  sec  71.9 MBytes   603 Mbits/sec  465    161 KBytes       \n[  4]  11.00-12.00  sec  86.9 MBytes   729 Mbits/sec  441    248 KBytes       \n[  4]  12.00-13.00  sec  85.3 MBytes   715 Mbits/sec  259    255 KBytes       \n[  4]  13.00-14.00  sec  85.4 MBytes   716 Mbits/sec  334    194 KBytes       \n[  4]  14.00-15.00  sec  85.9 MBytes   721 Mbits/sec  311    216 KBytes       \n[  4]  15.00-16.00  sec  72.3 MBytes   606 Mbits/sec  223    213 KBytes       \n[  4]  16.00-17.00  sec  85.8 MBytes   719 Mbits/sec  371    224 KBytes       \n[  4]  17.00-18.00  sec  86.3 MBytes   724 Mbits/sec   88    297 KBytes       \n[  4]  18.00-19.00  sec  84.9 MBytes   713 Mbits/sec  440    171 KBytes       \n[  4]  19.00-20.00  sec  84.3 MBytes   708 Mbits/sec  318   91.7 KBytes       \n[  4]  20.00-21.00  sec  72.5 MBytes   608 Mbits/sec  327    260 KBytes       \n[  4]  21.00-22.00  sec  85.6 MBytes   718 Mbits/sec  361    242 KBytes       \n[  4]  22.00-23.00  sec  85.6 MBytes   717 Mbits/sec  391    215 KBytes       \n[  4]  23.00-24.00  sec  85.4 MBytes   717 Mbits/sec  423    242 KBytes       \n[  4]  24.00-25.00  sec  84.3 MBytes   707 Mbits/sec  705    196 KBytes       \n[  4]  25.00-26.00  sec  71.9 MBytes   603 Mbits/sec  342    229 KBytes       \n[  4]  26.00-27.00  sec  85.3 MBytes   716 Mbits/sec  210    343 KBytes       \n[  4]  27.00-28.00  sec  85.9 MBytes   720 Mbits/sec  317    185 KBytes       \n[  4]  28.00-29.00  sec  84.8 MBytes   711 Mbits/sec  304    257 KBytes       \n[  4]  29.00-30.00  sec  85.0 MBytes   713 Mbits/sec  450    204 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\nTest Complete. Summary Results:\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-30.00  sec  2.43 GBytes   695 Mbits/sec  10558             sender\n[  4]   0.00-30.00  sec  2.43 GBytes   695 Mbits/sec                  receiver\nCPU Utilization: local/sender 0.7% (0.0%u/0.7%s), remote/receiver 0.4% (0.0%u/0.3%s)\n```\n\nNative host networking, MTU 1500 tx on:\n```\nTime: Thu, 03 Aug 2017 23:57:16 GMT\nConnecting to host green.westcentralus.cloudapp.azure.com, port 31211\n      Cookie: b59be62b4f3d.1501804635.978869.6b579\n      TCP MSS: 1428 (default)\n[  4] local 172.17.0.3 port 47012 connected to 52.161.110.8 port 31211\nStarting Test: protocol: TCP, 1 streams, 131072 byte blocks, omitting 0 seconds, 30 second test\n[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n[  4]   0.00-1.00   sec  94.8 MBytes   795 Mbits/sec  379    531 KBytes\n[  4]   1.00-2.00   sec  87.9 MBytes   737 Mbits/sec  715    335 KBytes\n[  4]   2.00-3.00   sec  82.3 MBytes   691 Mbits/sec  157    321 KBytes\n[  4]   3.00-4.00   sec  89.4 MBytes   750 Mbits/sec  513    331 KBytes\n[  4]   4.00-5.00   sec  88.6 MBytes   743 Mbits/sec  651    350 KBytes\n[  4]   5.00-6.00   sec  90.9 MBytes   762 Mbits/sec    0    509 KBytes\n[  4]   6.00-7.00   sec  88.2 MBytes   740 Mbits/sec  563    342 KBytes\n[  4]   7.00-8.00   sec  82.4 MBytes   691 Mbits/sec    0    491 KBytes\n[  4]   8.00-9.00   sec  88.9 MBytes   745 Mbits/sec  520    409 KBytes\n[  4]   9.00-10.00  sec  90.1 MBytes   756 Mbits/sec  254    407 KBytes\n[  4]  10.00-11.00  sec  89.4 MBytes   750 Mbits/sec  386    409 KBytes\n[  4]  11.00-12.00  sec  88.9 MBytes   746 Mbits/sec  446    541 KBytes\n[  4]  12.00-13.00  sec  81.5 MBytes   684 Mbits/sec   49    633 KBytes\n[  4]  13.00-14.00  sec  89.8 MBytes   754 Mbits/sec  677    195 KBytes\n[  4]  14.00-15.00  sec  89.1 MBytes   747 Mbits/sec   96    575 KBytes\n[  4]  15.00-16.00  sec  89.3 MBytes   749 Mbits/sec  680    360 KBytes\n[  4]  16.00-17.00  sec  88.9 MBytes   745 Mbits/sec  228    368 KBytes\n[  4]  17.00-18.00  sec  81.2 MBytes   682 Mbits/sec  303    300 KBytes\n[  4]  18.00-19.00  sec  89.8 MBytes   754 Mbits/sec   46    477 KBytes\n[  4]  19.00-20.00  sec  90.6 MBytes   760 Mbits/sec  270    446 KBytes\n[  4]  20.00-21.00  sec  89.4 MBytes   750 Mbits/sec  472    396 KBytes\n[  4]  21.00-22.00  sec  88.0 MBytes   738 Mbits/sec  584    489 KBytes\n[  4]  22.00-23.00  sec  82.2 MBytes   689 Mbits/sec    0    491 KBytes\n[  4]  23.00-24.00  sec  89.5 MBytes   751 Mbits/sec  300    570 KBytes\n[  4]  24.00-25.00  sec  88.9 MBytes   745 Mbits/sec  477    339 KBytes\n[  4]  25.00-26.00  sec  89.3 MBytes   749 Mbits/sec  485    386 KBytes\n[  4]  26.00-27.00  sec  90.4 MBytes   758 Mbits/sec  120    435 KBytes\n[  4]  27.00-28.00  sec  81.0 MBytes   680 Mbits/sec  261    368 KBytes\n[  4]  28.00-29.00  sec  90.8 MBytes   762 Mbits/sec    6    519 KBytes\n[  4]  29.00-30.00  sec  89.2 MBytes   748 Mbits/sec  310    404 KBytes\n- - - - - - - - - - - - - - - - - - - - - - - - -\nTest Complete. Summary Results:\n[ ID] Interval           Transfer     Bandwidth       Retr\n[  4]   0.00-30.00  sec  2.58 GBytes   738 Mbits/sec  9948             sender\n[  4]   0.00-30.00  sec  2.58 GBytes   738 Mbits/sec                  receiver\nCPU Utilization: local/sender 0.8% (0.0%u/0.8%s), remote/receiver 1.4% (0.1%u/1.3%s)\n```.",
    "minhdanh": "Thank you. ",
    "little-dude": "\nWhen running as a pod it can be given permission to access those details. I did add some experimental support for running outside a pod, but it's not currently documented (and may never be)\n\n@tomdee so running flannel as a systemd service is not supported? Or is it just --kube-subnet-mgr that is not supported when running flannel as a systemd service?\n. Alright that seems to work. For posterity, here is my flannel service file:\n```ini\n/etc/systemd/system/flannel.service\n[Unit]\nDescription=Network fabric for containers\nDocumentation=https://github.com/coreos/flannel\nAfter=network.target\nAfter=network-online.target\nWants=network-online.target\n[Service]\nType=notify\nRestart=always\nRestartSec=5\nThis is needed because of this: https://github.com/coreos/flannel/issues/792\nKubernetes knows the nodes by their FQDN so we have to use the FQDN\nEnvironment=NODE_NAME=my-node.foo.bar.com\nNote that we don't specify any etcd option. This is because we want to talk\nto the apiserver instead. The apiserver then talks to etcd on flannel's\nbehalf.\nExecStart=/opt/bin/flanneld \\\n  --kube-subnet-mgr=true \\\n  --kubeconfig-file=/etc/kube-flannel/kube.config.yml \\\n  --v=1 \\\n  --ip-masq=true \\\n  --iface=eth0\n```. @zhangguanzhang yes see my service file above.. ",
    "johanot": "@little-dude @tomdee Had I similar issue yesterday which I fixed by setting NODE_NAME, but this leads to a follow-up question: Is flannel restricted to run only on nodes which has a kubelet running? \nI now get: Error registering network: failed to acquire lease: node \"master\" not found\nBefore switching from etcd to kubernetes configuration backend, I had flannel running on my k8s master node, which haven't got an active kubelet service on it - i.e. the master node is not a cluster node as such.. @zhangguanzhang I did. Setting NODE_NAME solved the issues I had. But no-one responded to my followup question :-) I just ended up running a kubelet on all nodes, and tainted it Unschedulable when I didn't want any pods.. ",
    "zhangguanzhang": "@minhdanh  @johanot @little-dude I also want to run the flanneld with Kubernetes API server, I want to know if you have succeeded.. @johanot @minhdanh I see there has files net-conf.json and the cni-conf.json at the kube-flannel.yml, should I touch the file and write the same word in the two files when I use the systemd?. I have run successfully. Thanks. $  /opt/bin/flanneld --help |& grep log_\n  -log_backtrace_at value\n  -log_dir string\n$ /opt/bin/flanneld --version\n0.7.1\n. ",
    "simepo": "Hi, thanks for responding.\nThe RPMs all come from Red Hat, they are included in the RHEL 7.2 extras/optional channels.\nI guess the summary is:\nVNI not in etcd - new node will start / existing node will not start\nVNI included in etcd - new node will not start / existing node will start\nHope that helps, please let me know what other relevant details I can provide.. ",
    "clchandan": "Hi Tom,\nthanks for looking at it and reply. But not sure why it was closed as in our case we did start the flanneld daemon and that is when we tried to Ping the flannel0 and also flannel1(in an AWS environment which has vxlan) IP. I understand and see in Doc that UDP is not recommended. But is there a better way to confirm it is actually UDP other than from the Interface name.\nAlso it would be good to know who/what uses that IP to check what affect it will have on our system.. Any update on this or at least can somebody tell what/who needs to ping the flannel0,1 IP so we get an idea on what side affect to look for.. ",
    "oilbeater": "@roffe that means flannel containers rely on host settings, I don't think it's a good practice. \nflannel:v0.6 image does have CAs in image and aws/ali-vpc backend works fine by official kubernetes daemonset yaml, but v0.8 is broken. That's why I commit this change.. ",
    "gwind": "@vnalla I've install flannel daemonset successful, you can saw my steps in\nhttps://github.com/kubernetes/kubernetes/issues/51881 . ",
    "cdyue": "@limited Thanks!It works!\n\nflannel version: v0.8.0. I have the same issue.But i think  it's not a bug of FLANNEL.\nMy env:\ncentos 7.2\nflannel: v0.8.0\ndocker: 17.06.0-ce\nkubernetes:v1.7.4\n\nIt seems docker version >=1.13 will add iptables rule like below,and it make  this issue happen:\niptables -P FORWARD DROP\nAll you need to do is add a rule below:\niptables -P FORWARD ACCEPT. ",
    "domino14": "I see this issue too. How do we make sure those iptables rules run on reboot?. ",
    "limited": "I'm using Docker 1.12, so I think the behavior must start in an earlier version. Also, I don't think its an acceptable solution to change the default behavior for an IPTables rules. My two rules are a more precise fix. . Thanks will give the ip-masq a shot. ",
    "jeffmhastings": "This seems related: https://github.com/containernetworking/plugins/pull/75. ",
    "GheRivero": "I can confirm this issue with flannel 0.9.0 (both vxlan & host-gw), k8s 1.8.2, docker 17.05\nApplying the iptables rules solves the problems. . ",
    "balbalas": "@tomdee \nDo you know which version flannel has the fix? We are seeing it with 0.10.0.\n[bbalasubram@cirrus-vm1 Demo]$ docker version\nClient:\n Version:   17.12.1-ce\n API version:   1.35\n Go version:    go1.9.4\n Git commit:    7390fc6\n Built: Tue Feb 27 22:15:20 2018\n OS/Arch:   linux/amd64\nServer:\n Engine:\n  Version:  17.12.1-ce\n  API version:  1.35 (minimum version 1.12)\n  Go version:   go1.9.4\n  Git commit:   7390fc6\n  Built:    Tue Feb 27 22:17:54 2018\n  OS/Arch:  linux/amd64\n  Experimental: false\n[bbalasubram@cirrus-vm1 Demo]$ kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.4\", GitCommit:\"bee2d1505c4fe820744d26d41ecd3fdd4a3d6546\", GitTreeState:\"clean\", BuildDate:\"2018-03-12T16:29:47Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.4\", GitCommit:\"bee2d1505c4fe820744d26d41ecd3fdd4a3d6546\", GitTreeState:\"clean\", BuildDate:\"2018-03-12T16:21:35Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n[bbalasubram@cirrus-vm1 Demo]$. ",
    "hectorqin": "I see it with 0.10.0 too. And it dosen't work after i apply those iptables rules. \n```\nroot@XMT01-VIDEO01:~# docker version\nClient:\n Version:      1.13.1\n API version:  1.26\n Go version:   go1.6.2\n Git commit:   092cba3\n Built:        Thu Nov  2 20:40:23 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      1.13.1\n API version:  1.26 (minimum version 1.12)\n Go version:   go1.6.2\n Git commit:   092cba3\n Built:        Thu Nov  2 20:40:23 2017\n OS/Arch:      linux/amd64\n Experimental: false\nroot@XMT01-VIDEO01:~# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.2\", GitCommit:\"81753b10df112992bf51bbc2c2f85208aad78335\", GitTreeState:\"clean\", BuildDate:\"2018-04-27T09:22:21Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"10\", GitVersion:\"v1.10.2\", GitCommit:\"81753b10df112992bf51bbc2c2f85208aad78335\", GitTreeState:\"clean\", BuildDate:\"2018-04-27T09:10:24Z\", GoVersion:\"go1.9.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nroot@XMT01-VIDEO01:~# kubectl get po -o wide\nNAME                                   READY     STATUS    RESTARTS   AGE       IP            NODE\nkubernetes-bootcamp-7799cbcb86-cdsdx   1/1       Running   0          2d        10.244.3.9    xmt01-middleware01\nkubernetes-bootcamp-7799cbcb86-wxglw   1/1       Running   0          2d        10.244.5.25   xmt01-web02\nroot@XMT01-VIDEO01:~# kubectl describe pod/kubernetes-bootcamp-7799cbcb86-cdsdx\nName:           kubernetes-bootcamp-7799cbcb86-cdsdx\nNamespace:      default\nNode:           xmt01-middleware01/192.168.82.113\nStart Time:     Tue, 15 May 2018 10:40:38 +0800\nLabels:         pod-template-hash=3355767642\n                run=kubernetes-bootcamp\nAnnotations:    \nStatus:         Running\nIP:             10.244.3.9\nControlled By:  ReplicaSet/kubernetes-bootcamp-7799cbcb86\nContainers:\n  kubernetes-bootcamp:\n    Container ID:   docker://f09a0af14d43335a9982ab991dcde30ca75491a879c7ca6acaed27c98370452a\n    Image:          jocatalin/kubernetes-bootcamp:v2\n    Image ID:       docker-pullable://jocatalin/kubernetes-bootcamp@sha256:fb1a3ced00cecfc1f83f18ab5cd14199e30adc1b49aa4244f5d65ad3f5feb2a5\n    Port:           8080/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 15 May 2018 10:40:41 +0800\n    Ready:          True\n    Restart Count:  0\n    Environment:    \n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-6x9qk (ro)\nConditions:\n  Type           Status\n  Initialized    True \n  Ready          True \n  PodScheduled   True \nVolumes:\n  default-token-6x9qk:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-6x9qk\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  \nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:          \nroot@XMT01-VIDEO01:~# curl 10.244.3.9:8080\ncurl: (7) Failed to connect to 10.244.3.9 port 8080: Connection timed out\n\nroot@XMT01-VIDEO01:~# iptables -nL\nChain INPUT (policy DROP)\ntarget     prot opt source               destination       \nKUBE-EXTERNAL-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW / kubernetes externally-visible service portals /\nKUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     112  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            state RELATED,ESTABLISHED\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:22\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:80\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:21\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:20000:30000\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:443\nACCEPT     tcp  --  192.168.82.0/24      0.0.0.0/0            tcp dpt:111\nACCEPT     udp  --  192.168.82.0/24      0.0.0.0/0            udp dpt:111\nACCEPT     tcp  --  192.168.82.0/24      0.0.0.0/0            tcp dpt:2049\nACCEPT     udp  --  192.168.82.0/24      0.0.0.0/0            udp dpt:2049\nACCEPT     tcp  --  192.168.82.0/24      0.0.0.0/0            tcp dpts:30001:30004\nACCEPT     udp  --  192.168.82.0/24      0.0.0.0/0            udp dpts:30001:30004\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:6443\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:10250:10252\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:10255\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:30000:32767\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:2379:2380\nACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0            limit: avg 1/sec burst 10\nACCEPT     all  -f  0.0.0.0/0            0.0.0.0/0            limit: avg 100/sec burst 100\nsyn-flood  tcp  --  0.0.0.0/0            0.0.0.0/0            tcp flags:0x17/0x02\nREJECT     all  --  0.0.0.0/0            0.0.0.0/0            reject-with icmp-host-prohibited\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination       \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nKUBE-FORWARD  all  --  0.0.0.0/0            0.0.0.0/0            / kubernetes forwarding rules /\nDOCKER-ISOLATION  all  --  0.0.0.0/0            0.0.0.0/0         \nDOCKER     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  10.244.0.0/16        0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            10.244.0.0/16       \nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination       \nKUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW / kubernetes service portals /\nKUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0           \nChain DOCKER (1 references)\ntarget     prot opt source               destination         \nChain DOCKER-ISOLATION (1 references)\ntarget     prot opt source               destination       \nRETURN     all  --  0.0.0.0/0            0.0.0.0/0           \nChain KUBE-EXTERNAL-SERVICES (1 references)\ntarget     prot opt source               destination         \nChain KUBE-FIREWALL (2 references)\ntarget     prot opt source               destination       \nDROP       all  --  0.0.0.0/0            0.0.0.0/0            / kubernetes firewall for dropping marked packets / mark match 0x8000/0x8000\nChain KUBE-FORWARD (1 references)\ntarget     prot opt source               destination       \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / kubernetes forwarding rules / mark match 0x4000/0x4000\nACCEPT     all  --  10.244.0.0/16        0.0.0.0/0            / kubernetes forwarding conntrack pod source rule / ctstate RELATED,ESTABLISHED\nACCEPT     all  --  0.0.0.0/0            10.244.0.0/16        / kubernetes forwarding conntrack pod destination rule / ctstate RELATED,ESTABLISHED\nChain KUBE-SERVICES (1 references)\ntarget     prot opt source               destination         \nChain syn-flood (1 references)\ntarget     prot opt source               destination       \nRETURN     tcp  --  0.0.0.0/0            0.0.0.0/0            limit: avg 3/sec burst 6\nREJECT     all  --  0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable\nroot@XMT01-VIDEO01:~# \n\nroot@XMT01-MIDDLEWARE01:~# iptables -nL\nChain INPUT (policy DROP)\ntarget     prot opt source               destination       \nKUBE-EXTERNAL-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW / kubernetes externally-visible service portals /\nKUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            state RELATED,ESTABLISHED\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:22\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:80\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:443\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:4869 / zimg server /\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:8000:8100 / proxy server /\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:6443\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:10250:10252\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpt:10255\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:30000:32767\nACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0            state NEW tcp dpts:2379:2380\nACCEPT     icmp --  0.0.0.0/0            0.0.0.0/0            limit: avg 1/sec burst 10\nACCEPT     all  -f  0.0.0.0/0            0.0.0.0/0            limit: avg 100/sec burst 100\nsyn-flood  tcp  --  0.0.0.0/0            0.0.0.0/0            tcp flags:0x17/0x02\nREJECT     all  --  0.0.0.0/0            0.0.0.0/0            reject-with icmp-host-prohibited\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination       \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / flannel subnet /\nKUBE-FORWARD  all  --  0.0.0.0/0            0.0.0.0/0            / kubernetes forwarding rules /\nDOCKER-ISOLATION  all  --  0.0.0.0/0            0.0.0.0/0         \nDOCKER     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED\nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0         \nACCEPT     all  --  10.244.0.0/16        0.0.0.0/0         \nACCEPT     all  --  0.0.0.0/0            10.244.0.0/16       \nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination       \nKUBE-SERVICES  all  --  0.0.0.0/0            0.0.0.0/0            ctstate NEW / kubernetes service portals /\nKUBE-FIREWALL  all  --  0.0.0.0/0            0.0.0.0/0           \nChain DOCKER (1 references)\ntarget     prot opt source               destination         \nChain DOCKER-ISOLATION (1 references)\ntarget     prot opt source               destination       \nRETURN     all  --  0.0.0.0/0            0.0.0.0/0           \nChain KUBE-EXTERNAL-SERVICES (1 references)\ntarget     prot opt source               destination         \nChain KUBE-FIREWALL (2 references)\ntarget     prot opt source               destination       \nDROP       all  --  0.0.0.0/0            0.0.0.0/0            / kubernetes firewall for dropping marked packets / mark match 0x8000/0x8000\nChain KUBE-FORWARD (1 references)\ntarget     prot opt source               destination       \nACCEPT     all  --  0.0.0.0/0            0.0.0.0/0            / kubernetes forwarding rules / mark match 0x4000/0x4000\nACCEPT     all  --  10.244.0.0/16        0.0.0.0/0            / kubernetes forwarding conntrack pod source rule / ctstate RELATED,ESTABLISHED\nACCEPT     all  --  0.0.0.0/0            10.244.0.0/16        / kubernetes forwarding conntrack pod destination rule / ctstate RELATED,ESTABLISHED\nChain KUBE-SERVICES (1 references)\ntarget     prot opt source               destination         \nChain syn-flood (1 references)\ntarget     prot opt source               destination       \nRETURN     tcp  --  0.0.0.0/0            0.0.0.0/0            limit: avg 3/sec burst 6\nREJECT     all  --  0.0.0.0/0            0.0.0.0/0            reject-with icmp-port-unreachable\n```. ",
    "strigazi": "I think this issue need to be re-opened. WIth [0], I still need to apply iptables -P FORWARD ACCEPT\n[0] quay.io/coreos/flannel:v0.10.0-amd64\ncc @tomdee . ",
    "FengyunPan2": "I see it with 0.10.0 too.\n/reopen. met this issue too, any thoughts?. ",
    "ajay-alef": "I was also facing the same, until I allowed \"All Traffic\" in aws security group.. ",
    "rajeshmuraleedharan": "Flushed all my firewalls with iptables --flush and iptables -tnat --flush then restart docker fixed it\ncheck this link. ",
    "julia-stripe": "Found some more information! \nI straced flannel, and these are the messages it's sending to the netlink socket:\nhttps://gist.github.com/julia-stripe/c2a4aafbccf3533d738be1e665a79eb8\nI parsed them all (using pyroute2: http://docs.pyroute2.org/debug.html\nand got this resuilt for the failed message:\n```\n{'attrs': [('RTA_DST', '10.32.5.0'),\n           ('RTA_GATEWAY', '10.68.28.131'),\n           ('RTA_OIF', 0)],\n 'dst_len': 24,\n 'family': 2,\n 'flags': 0,\n 'header': {'flags': 5,\n            'length': 52,\n            'pid': 0,\n            'sequence_number': 4,\n            'type': 25},\n 'proto': 0,\n 'scope': 0,\n 'src_len': 0,\n 'table': 254,\n 'tos': 0,\n 'type': 0}\n```\nSo basically what's happening is that Flannel sets RTA_OIF (the interface ID for the network interface) to 0 when it should be 2 (on our machines). This value (the interface id) comes from the linkIndex struct member, which appears to be unset. So it seems like the linkIndex struct member being 0 (instead of the right interface id) is the culprit.. > I see one key problem with this PR and then a couple of minor concerns (below) - the main concern is that if a single rule (of the four that flannel installs) is removed, then flannel just restores the one missing rule, which means it might end up being out of order! Since the ordering of the rules matters, I think this defeats the purpose of this PR a bit.\nthis is a great point. Changed the PR so that it instead:\n\nchecks if any rules are missing\nif so, deletes all the rules and recreates them in order.\n\nI also added a mock IPTables struct & some tests to give us a little more confidence that this code actually works as intended. \nThis should be reviewed commit-by-commit -- the first commit just adds tests, and the second actually changes functionality.. Squashed.\n\nI also think it's desirable that the rules aren't cleaned up (so the dataplane keeps working during an upgrade). \n\nThis makes sense to me.. not totally sure that this return is the right thing, it may be better to continue retrying instead. ",
    "CallMeFoxie": "Sorry my bad, found the issue somewhere completely else on my end :). It's not 2K node cluster, it's only several hundred, only 2K is the allocated limit. And we have two clusters. And the node was in the wrong cluster for whatever reason :).... Coincidentally on the same flannel IP as another node in the other cluster which made me go WTF until I noticed wrong config file.\nSorry!. ",
    "fasaxc": "On (1), looks like the code checks for the existence of the rule first, so it's 4 reads per second.  That  could be moderately expensive if there are a lot of rules (since most iptables operations do a full table load from the kernel to userspace, even to check one rule).\nOn (2), as long as the other process isn't deleting these rules actively then we should be OK (and if it was actively trying to delete those rules then there's not much we can do.\nLonger term, you may want to migrate to having only a single rule inserted into the root chain and then jumping fro there to a flannel-owned chain.  That's the approach that we have in Calico; it reduces cloberring since there's only one rule and it avoids reordering issues.  (And, since we own the calico chain, we can just flush and rewrite the whole chain at will if anyone ever tampers with it.). @thxCode is this based on the work in #921 and/or #922?  Your commit history doesn't include either of those PRs. Typo. ",
    "mkumatag": "/cc @tomdee . @tomdee can you release a minor release with this fix if you don't mind, something like - v0.8.1. /assign. /cc @tomdee . Sure will rebase it.... submitted via different PR - https://github.com/coreos/flannel/pull/845. /cc @tomdee . /cc @tomdee . Thanks @tomdee . /cc @tomdee . @tomdee . @tomdee any idea what is happening? how do I debug why I'm not able to ping other host in IPsec mode.?. It would be appreciated if you could help me @tomdee.. The difference between these 2 targets are:\ndist/flanneld-$(TAG)-$(ARCH).docker uses gcr.io/google_containers/kube-cross which is not a multiarch image and can be run only on amd64 platform. And if I run this docker image on other than amd64 platform I'll hit exec format error.\nand dist/flanneld-e2e-$(TAG)-$(ARCH).docker target uses golang:1.8.3 docker image which is multiarch image which will load the respective docker image while running on different architecture.. ",
    "devent": "Update: If I restart the nodes, a new subnet is picked. Shouldn't it be done without a restart?\nnode-0\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.474993     833 main.go:229] Installing signal handlers\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.552696     833 main.go:330] Found network config - Backend type: vxlan\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.578382     833 local_manager.go:234] Picking subnet in range 10.2.1.0 ... 10.2.255.0\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.584280     833 local_manager.go:220] Allocated lease (10.2.24.0/24) to current node (192.168.56.200)\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.585051     833 main.go:279] Wrote subnet file to /run/flannel/subnet.env\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.585072     833 main.go:284] Finished starting backend.\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.586597     833 vxlan_network.go:56] Watching for L3 misses\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.586615     833 vxlan_network.go:64] Watching for new subnet leases\nSep 14 13:29:44 robobee-test.test systemd[1]: Started Flanneld overlay address etcd agent.\nSep 14 13:29:44 robobee-test.test flanneld[833]: I0914 13:29:44.611990     833 main.go:367] Waiting for 22h59m59.967070833s to renew lease\nnode-1\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.261677     776 main.go:226] Created subnet manager: Etcd Local Manager with Previous Subnet: 0.0.0.0/0\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.261869     776 main.go:229] Installing signal handlers\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.351221     776 main.go:330] Found network config - Backend type: vxlan\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.378529     776 local_manager.go:147] Found lease (10.2.43.0/24) for current IP (192.168.56.201), reusing\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.405887     776 main.go:279] Wrote subnet file to /run/flannel/subnet.env\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.405911     776 main.go:284] Finished starting backend.\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.406324     776 vxlan_network.go:56] Watching for L3 misses\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.406339     776 vxlan_network.go:64] Watching for new subnet leases\nSep 14 13:29:45 robobee-1-test.test systemd[1]: Started Flanneld overlay address etcd agent.\nSep 14 13:29:45 robobee-1-test.test flanneld[776]: I0914 13:29:45.637436     776 main.go:367] Waiting for 22h59m59.741953496s to renew lease. I forgot to write, I had flanneld already installed on the node-0 and node-1, but the etcd cluster wasn't setup yet between the nodes. But I deleted the virtual network flannel.1 before I reinstalled flanneld.\nip link delete flanneld.1\nMaybe the reason was that the old network interfaces flanneld.1 was still present on the nodes?. To clarify. The whole process was as following.\n1. One Virtualbox Debian 9, lets say node-0, hostname node-0, address 192.168.56.200\n2. Installed etcd, flanneld and docker on node-0, everything runs fine.\n3. make a clone of node-0, with new mac address, node-1.\n4. change the IP and hostname of node-1 to node-1 and 192.168.56.201, respectively.\n5. stop etcd on the nodes, delete old etcd data directory on node-0 and node-1.\n6. reinstall etcd, flanneld and docker on node-0 and node-1, with a new etcd cluster made of node-0 and -1.\n7. flanneld on the nodes is now using the newly installed etcd cluster.\n8. subnet on node-0 and node-1 is the same, with either flanneld-0 or flanneld-1 overriding the subnet configuration on the etcd-cluster, depending which node was started last.\n...\n9. restart both nodes, flanneld now works as expected.\nMy expectation was that no restart should be needed, because flanneld-0 and flanneld-1 would see the subnet in the etcd-cluster. Since I deleted the old etcd data directories on both nodes and started a fresh new etcd cluster, there should be no old configuration left.. ",
    "szuecs": "@tomdee we updated also production to version 0.9.0. It fixed our problem.. @discordianfish do you deployed a successful fix?\n@maxlaverse we also have problems with that and run coreos + flannel in 85 clusters. Monitoring shows regular spikes of DNS request timeouts. \nWe do not see many failures a day, but it's measurable.\nSome inserts fail, so this might show the problems we can measure.\n~# conntrack -S\ncpu=0           found=63 invalid=129 ignore=1314771 insert=0 insert_failed=11 drop=11 early_drop=0 error=8 search_restart=27531 \ncpu=1           found=57 invalid=102 ignore=1303273 insert=0 insert_failed=11 drop=11 early_drop=0 error=2 search_restart=37546 \ncpu=2           found=48 invalid=109 ignore=1276723 insert=0 insert_failed=6 drop=6 early_drop=0 error=5 search_restart=36513 \ncpu=3           found=77 invalid=138 ignore=1272906 insert=0 insert_failed=16 drop=16 early_drop=0 error=5 search_restart=35515\nWe also increased node_nf_conntrack_entries_limit and alert on node_nf_conntrack_entries, we do not see any issues there. \nCoreOS release\ncat /etc/lsb-release \nDISTRIB_ID=\"Container Linux by CoreOS\"\nDISTRIB_RELEASE=1688.5.3\nDISTRIB_CODENAME=\"Rhyolite\"\nDISTRIB_DESCRIPTION=\"Container Linux by CoreOS 1688.5.3 (Rhyolite)\". @maxlaverse thanks for the response. I think also that DNAT will be the problem. I ported basically the code  by @Quentin-M to flannel (we use 53 as destination so also change 5353 to 53, but anything else is pretty similar).\nI added this as sidecar to flannel running as daemonset and added:\n  - args:\n    - -c\n    - /tc-flannel.sh\n    command:\n    - /bin/bash\n    image: registry.opensource.zalan.do/teapot/flannel-tc:7d93e04\n    imagePullPolicy: IfNotPresent\n    name: flannel-tc\n    resources:\n      requests:\n        cpu: 25m\n        memory: 25Mi\n    securityContext:\n      privileged: true\n    stdin: true\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /run\n      name: run\n    - mountPath: /lib/tc\n      name: lib-tc\n\nI published the code repository at https://github.com/szuecs/flannel-tc\nI started the test at 6PM on 7/9 and it looks better than before (so the race condition is less likely to happen than before). We have to wait before to claim a success:\n\n. ",
    "erez-rabih": "Hi thanks for the tip, upgrading did solve the problem. ",
    "gensmusic": "Found a way see https://stackoverflow.com/questions/46276796/kubenetes-cannot-cleanup-flannel/46438072#46438072. ",
    "slawekgh": "I got similar problem on centos 7 with k8s 1,7 , take a look at https://github.com/coreos/flannel/issues/823\n. i tried , no success \n. tested once again - now it helps . type=vxlan\n[root@cent501 ~]# cat /tmp/kube-flannel.yml | grep -i type\n      \"type\": \"flannel\",\n        \"Type\": \"vxlan\". I installed k8s via kubeadm and after that apply 2 flannel yml files:\nkubeadm init --pod-network-cidr=10.244.0.0/16\nkubectl apply -f /tmp/kube-flannel.yml\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel-rbac.yml\nthe only difference between my local kube-flannel.yml and \"official\" file:\nhttps://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\nthe only diff is:\n```\n59c59\n<         command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\" ]\n\n\n    command: [ \"/opt/bin/flanneld\", \"--ip-masq\", \"--kube-subnet-mgr\", \"--iface=enp0s3\" ]\n\n```\nI made this change according to: \nhttps://github.com/coreos/flannel/issues/535\n\nwithout or without these small change I still have problems . I also tested solution from other issues:\nip route add 10.244.0.0/16 dev enp0s3\nwith no success :-( \n. Finally I found important checkpoint during installation:\nkubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'\nif it is not ok (empty output) my problem still persists, if its ok (output with 10.244.1.0/24) there is no problem any more - and I have correct inter-node communication between pods \n. and 2nd milestone - after reboot of kubernetes slave node we must execute \niptables -P FORWARD ACCEPT. still (sometimes) have to use \"iptables -P FORWARD ACCEPT\"\nimho it's docker oriented problem , it is not a bug in k8s/Flannel \nwe can close the issue or leave it open for future users  . ",
    "ggaaooppeenngg": "@slawekgh  try iptables -P FORWARD ACCEPT, maybe your docker version is latest and from docker 1.13, it disable FORWARD chain in iptables.. @slawekgh good news, maybe you didn't clean your interface settings (cni0 and flannel.1) before you reset your cluster.. ",
    "acomisario": "news? @slawekgh . @tadas-subonis hi! im interested on knowing how did you deployed that, can you clarify me this ?\n\nare you using CNI with flannel plugin on the kubelet service configuration ?\nafter that, you are deploying the kube-flannel.yml so that it provides what cni needs ?\ndocker is configured as is ? (since i see you removed --iptables=false that is not the default)\nso on you workers you have a docker0 and cni0 bridges, and flannel.1 interface right ?\n\nafter this clarifications, i think i can ask you more things.\nthanks for you help. i will close the ticket, the issue was with the VXLAN tunnels, im using AWS for the environment, and i only alowed all TCP between nodes, not UDP, so communication between workers using VXLAN was not working.\nAfter updating the secgroup, everything is working as expected.. ",
    "camflan": "I've run iptables -P FORWARD ACCEPT on all nodes and pods still can't communicate across hosts. One of the effects of this is that I can't get cluster DNS working (they can't reach the dns pods)\n```\nGenerated by iptables-save v1.6.0 on Thu Nov 16 02:50:17 2017\n*nat\n:PREROUTING ACCEPT [0:0]\n:INPUT ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n:POSTROUTING ACCEPT [0:0]\n:DOCKER - [0:0]\n:KUBE-MARK-DROP - [0:0]\n:KUBE-MARK-MASQ - [0:0]\n:KUBE-NODEPORTS - [0:0]\n:KUBE-POSTROUTING - [0:0]\n:KUBE-SEP-IT2ZTR26TO4XFPTO - [0:0]\n:KUBE-SEP-S5EQFLAXMXOTYOQY - [0:0]\n:KUBE-SEP-YIL6JZP7A3QYXJU2 - [0:0]\n:KUBE-SERVICES - [0:0]\n:KUBE-SVC-ERIFXISQEP7F7OF4 - [0:0]\n:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]\n:KUBE-SVC-TCOU7JCQXEZGVUNU - [0:0]\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\n-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n-A POSTROUTING -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN\n-A POSTROUTING -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE\n-A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN\n-A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE\n-A DOCKER -i docker0 -j RETURN\n-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000\n-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE\n-A KUBE-SEP-IT2ZTR26TO4XFPTO -s 10.244.0.2/32 -m comment --comment \"kube-system/kube-dns:dns-tcp\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-IT2ZTR26TO4XFPTO -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp\" -m tcp -j DNAT --to-destination 10.244.0.2:53\n-A KUBE-SEP-S5EQFLAXMXOTYOQY -s 10.0.1.2/32 -m comment --comment \"default/kubernetes:https\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-S5EQFLAXMXOTYOQY -p tcp -m comment --comment \"default/kubernetes:https\" -m recent --set --name KUBE-SEP-S5EQFLAXMXOTYOQY --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.0.1.2:6443\n-A KUBE-SEP-YIL6JZP7A3QYXJU2 -s 10.244.0.2/32 -m comment --comment \"kube-system/kube-dns:dns\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-YIL6JZP7A3QYXJU2 -p udp -m comment --comment \"kube-system/kube-dns:dns\" -m udp -j DNAT --to-destination 10.244.0.2:53\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.10/32 -p udp -m comment --comment \"kube-system/kube-dns:dns cluster IP\" -m udp --dport 53 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \"kube-system/kube-dns:dns cluster IP\" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.10/32 -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp cluster IP\" -m tcp --dport 53 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp cluster IP\" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.1/32 -p tcp -m comment --comment \"default/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment \"default/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y\n-A KUBE-SERVICES -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS\n-A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment \"kube-system/kube-dns:dns-tcp\" -j KUBE-SEP-IT2ZTR26TO4XFPTO\n-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment \"default/kubernetes:https\" -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-S5EQFLAXMXOTYOQY --mask 255.255.255.255 --rsource -j KUBE-SEP-S5EQFLAXMXOTYOQY\n-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment \"default/kubernetes:https\" -j KUBE-SEP-S5EQFLAXMXOTYOQY\n-A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \"kube-system/kube-dns:dns\" -j KUBE-SEP-YIL6JZP7A3QYXJU2\nCOMMIT\nCompleted on Thu Nov 16 02:50:17 2017\nGenerated by iptables-save v1.6.0 on Thu Nov 16 02:50:17 2017\n*filter\n:INPUT DROP [0:0]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [1302:324500]\n:DOCKER - [0:0]\n:DOCKER-ISOLATION - [0:0]\n:KUBE-FIREWALL - [0:0]\n:KUBE-SERVICES - [0:0]\n-A INPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A INPUT -j KUBE-FIREWALL\n-A INPUT -s 192.168.168.141/32 -p tcp -j ACCEPT\n-A INPUT -s 192.168.168.141/32 -p udp -j ACCEPT\n-A INPUT -s 192.168.151.236/32 -p tcp -j ACCEPT\n-A INPUT -s 192.168.151.236/32 -p udp -j ACCEPT\n-A INPUT -i wg0 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT\n-A INPUT -p udp -m udp --dport 80 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 443 -j ACCEPT\n-A INPUT -p udp -m udp --dport 443 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 6443 -j ACCEPT\n-A INPUT -p udp -m udp --dport 6443 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT\n-A INPUT -p udp -m udp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT\n-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A INPUT -i lo -m comment --comment \"Allow all loopback traffic\" -j ACCEPT\n-A INPUT -d 127.0.0.0/8 ! -i lo -m comment --comment \"Drop all traffic to 127 that doesn\\'t use lo\" -j REJECT --reject-with icmp-port-unreachable\n-A FORWARD -j DOCKER-ISOLATION\n-A FORWARD -o docker0 -j DOCKER\n-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A FORWARD -i docker0 ! -o docker0 -j ACCEPT\n-A FORWARD -i docker0 -o docker0 -j ACCEPT\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT -j KUBE-FIREWALL\n-A DOCKER-ISOLATION -j RETURN\n-A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP\nCOMMIT\nCompleted on Thu Nov 16 02:50:17 2017\n```. I have a similar issue, same versions of flannel, k8s. Using vxlan, flannel is up and running, no errors in the logs (not even the error above).\nkubeadm 1.8.1\nk8s 1.8.0\nflannel 0.9\nubuntu 16.04\ndocker 17.03ce\nI've tried combinations of k8s as far back as 1.6 and flannel as far back as 0.8, all with the same results.\nI'm able to connect pod <-> pod and host <-> pod as long as the pods are on that host. All hosts can communicate with each other without issues. I've spent almost a month fiddling with iptables, routes, etc and cannot figure this out. I'm seeing traffic via tcpdump on the cni0 bridge, but my pods aren't getting it. IIRC, last night I was using iptstate and was seeing udp traffic on the bridge when I expected tcp. Maybe this is the issue? It's also possible I was seeing something else...\nShould I open another ticket, or piggy back on this one?. Tom, I'll recreate the cluster tonight and give you those details\n\nCamron\n\n\nOn Dec 12, 2017, at 7:14 PM, Tom Denham notifications@github.com wrote:\nCan you give me some ip -d route output showing the bad routing rules when using vxlan?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @tomdee here's ip -d route on the master node\n\nroot@node-1:~# ip -d route\nunicast default via 45.56.69.1 dev eth0  proto boot  scope global onlink\nunicast 10.0.1.3 dev wg0  proto boot  scope link\nunicast 10.0.1.4 dev wg0  proto boot  scope link\nunicast 10.0.1.5 dev wg0  proto boot  scope link\nunicast 10.244.1.0/24 via 10.244.1.0 dev flannel.1  proto boot  scope global onlink\nunicast 10.244.2.0/24 via 10.244.2.0 dev flannel.1  proto boot  scope global onlink\nunicast 10.244.3.0/24 via 10.244.3.0 dev flannel.1  proto boot  scope global onlink\nunicast 45.56.69.0/24 dev eth0  proto kernel  scope link  src 45.56.69.90\nunicast 172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown\nunicast 192.168.128.0/17 dev eth0  proto kernel  scope link  src 192.168.168.99\nand from node-2\nroot@node-2:~# ip -d route\nunicast default via 45.56.75.1 dev eth0  proto boot  scope global onlink \nunicast 10.0.1.2 dev wg0  proto boot  scope link \nunicast 10.0.1.4 dev wg0  proto boot  scope link \nunicast 10.0.1.5 dev wg0  proto boot  scope link \nunicast 10.244.0.0/24 via 10.244.0.0 dev flannel.1  proto boot  scope global onlink \nunicast 10.244.1.0/24 via 10.244.1.0 dev flannel.1  proto boot  scope global onlink \nunicast 10.244.3.0/24 via 10.244.3.0 dev flannel.1  proto boot  scope global onlink \nunicast 45.56.75.0/24 dev eth0  proto kernel  scope link  src 45.56.75.158 \nunicast 172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown \nunicast 192.168.128.0/17 dev eth0  proto kernel  scope link  src 192.168.146.246\nnow I'm trying to remember how I was showing that the packets had incorrect routes. I'm pretty sure I was logging iptables filter table to disk, I'll report back. I haven't been able to see the wrong src IP on packets like I was the last 20 times I built this cluster. I'm sure I discovered it via iptables TRACE. \ud83e\udd14 \nPods from different hosts still can't communicate using VXLAN, but another new cluster using UDP is working fine. This is the only difference between the 2.\nThe best I've found is that it's attempting to use eth0 to communicate across flannel instead of wg0, but the src IP looks right so far in my flailing around.\nHere's iptables from node-2 on my VXLAN cluster. If you see anything here, or have a better idea on how to properly trace iptables/snat, please let me know.\n```\nGenerated by iptables-save v1.6.0 on Wed Dec 13 05:25:32 2017\n*raw\n:PREROUTING ACCEPT [2515:1745077]\n:OUTPUT ACCEPT [1899:268869]\n-A PREROUTING -d 10.244.0.0/16 -p tcp -j TRACE\n-A OUTPUT -d 10.244.0.0/16 -p tcp -j TRACE\n-A OUTPUT -s 10.244.0.0/16 -p tcp -j TRACE\nCOMMIT\nCompleted on Wed Dec 13 05:25:32 2017\nGenerated by iptables-save v1.6.0 on Wed Dec 13 05:25:32 2017\n*nat\n:PREROUTING ACCEPT [0:0]\n:INPUT ACCEPT [0:0]\n:OUTPUT ACCEPT [0:0]\n:POSTROUTING ACCEPT [0:0]\n:DOCKER - [0:0]\n:KUBE-MARK-DROP - [0:0]\n:KUBE-MARK-MASQ - [0:0]\n:KUBE-NODEPORTS - [0:0]\n:KUBE-POSTROUTING - [0:0]\n:KUBE-SEP-25M36I63GRYNYR7P - [0:0]\n:KUBE-SEP-2ZJ4BX7VSUVLTWRU - [0:0]\n:KUBE-SEP-E5XKBYQJWVLMMMDO - [0:0]\n:KUBE-SEP-EXMTHT3LYJSGRPTI - [0:0]\n:KUBE-SEP-GNSJTLV2J42BEG4Q - [0:0]\n:KUBE-SEP-MRGDWWZY4KIHEUKF - [0:0]\n:KUBE-SEP-ORBMK5GMWA2QWT4C - [0:0]\n:KUBE-SEP-S5EQFLAXMXOTYOQY - [0:0]\n:KUBE-SERVICES - [0:0]\n:KUBE-SVC-ERIFXISQEP7F7OF4 - [0:0]\n:KUBE-SVC-GF3EYLEDKOFUSEN2 - [0:0]\n:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]\n:KUBE-SVC-TCOU7JCQXEZGVUNU - [0:0]\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\n-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE\n-A POSTROUTING -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN\n-A POSTROUTING -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE\n-A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.2.0/24 -j RETURN\n-A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE\n-A DOCKER -i docker0 -j RETURN\n-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000\n-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE\n-A KUBE-SEP-25M36I63GRYNYR7P -s 10.244.1.7/32 -m comment --comment \"kube-system/kube-dns:dns\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-25M36I63GRYNYR7P -p udp -m comment --comment \"kube-system/kube-dns:dns\" -m udp -j DNAT --to-destination 10.244.1.7:53\n-A KUBE-SEP-2ZJ4BX7VSUVLTWRU -s 10.244.3.2/32 -m comment --comment \"default/microbot-service:\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-2ZJ4BX7VSUVLTWRU -p tcp -m comment --comment \"default/microbot-service:\" -m tcp -j DNAT --to-destination 10.244.3.2:80\n-A KUBE-SEP-E5XKBYQJWVLMMMDO -s 10.244.1.3/32 -m comment --comment \"default/microbot-service:\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-E5XKBYQJWVLMMMDO -p tcp -m comment --comment \"default/microbot-service:\" -m tcp -j DNAT --to-destination 10.244.1.3:80\n-A KUBE-SEP-EXMTHT3LYJSGRPTI -s 10.244.3.3/32 -m comment --comment \"default/microbot-service:\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-EXMTHT3LYJSGRPTI -p tcp -m comment --comment \"default/microbot-service:\" -m tcp -j DNAT --to-destination 10.244.3.3:80\n-A KUBE-SEP-GNSJTLV2J42BEG4Q -s 10.244.2.2/32 -m comment --comment \"default/microbot-service:\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-GNSJTLV2J42BEG4Q -p tcp -m comment --comment \"default/microbot-service:\" -m tcp -j DNAT --to-destination 10.244.2.2:80\n-A KUBE-SEP-MRGDWWZY4KIHEUKF -s 10.244.2.3/32 -m comment --comment \"default/microbot-service:\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-MRGDWWZY4KIHEUKF -p tcp -m comment --comment \"default/microbot-service:\" -m tcp -j DNAT --to-destination 10.244.2.3:80\n-A KUBE-SEP-ORBMK5GMWA2QWT4C -s 10.244.1.7/32 -m comment --comment \"kube-system/kube-dns:dns-tcp\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-ORBMK5GMWA2QWT4C -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp\" -m tcp -j DNAT --to-destination 10.244.1.7:53\n-A KUBE-SEP-S5EQFLAXMXOTYOQY -s 10.0.1.2/32 -m comment --comment \"default/kubernetes:https\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-S5EQFLAXMXOTYOQY -p tcp -m comment --comment \"default/kubernetes:https\" -m recent --set --name KUBE-SEP-S5EQFLAXMXOTYOQY --mask 255.255.255.255 --rsource -m tcp -j DNAT --to-destination 10.0.1.2:6443\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.1/32 -p tcp -m comment --comment \"default/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment \"default/kubernetes:https cluster IP\" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.10/32 -p udp -m comment --comment \"kube-system/kube-dns:dns cluster IP\" -m udp --dport 53 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \"kube-system/kube-dns:dns cluster IP\" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.10/32 -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp cluster IP\" -m tcp --dport 53 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp cluster IP\" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.109.12.4/32 -p tcp -m comment --comment \"default/microbot-service: cluster IP\" -m tcp --dport 80 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.109.12.4/32 -p tcp -m comment --comment \"default/microbot-service: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-GF3EYLEDKOFUSEN2\n-A KUBE-SERVICES -m comment --comment \"kubernetes service nodeports; NOTE: this must be the last rule in this chain\" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS\n-A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment \"kube-system/kube-dns:dns-tcp\" -j KUBE-SEP-ORBMK5GMWA2QWT4C\n-A KUBE-SVC-GF3EYLEDKOFUSEN2 -m comment --comment \"default/microbot-service:\" -m statistic --mode random --probability 0.20000000019 -j KUBE-SEP-E5XKBYQJWVLMMMDO\n-A KUBE-SVC-GF3EYLEDKOFUSEN2 -m comment --comment \"default/microbot-service:\" -m statistic --mode random --probability 0.25000000000 -j KUBE-SEP-GNSJTLV2J42BEG4Q\n-A KUBE-SVC-GF3EYLEDKOFUSEN2 -m comment --comment \"default/microbot-service:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-MRGDWWZY4KIHEUKF\n-A KUBE-SVC-GF3EYLEDKOFUSEN2 -m comment --comment \"default/microbot-service:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-2ZJ4BX7VSUVLTWRU\n-A KUBE-SVC-GF3EYLEDKOFUSEN2 -m comment --comment \"default/microbot-service:\" -j KUBE-SEP-EXMTHT3LYJSGRPTI\n-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment \"default/kubernetes:https\" -m recent --rcheck --seconds 10800 --reap --name KUBE-SEP-S5EQFLAXMXOTYOQY --mask 255.255.255.255 --rsource -j KUBE-SEP-S5EQFLAXMXOTYOQY\n-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment \"default/kubernetes:https\" -j KUBE-SEP-S5EQFLAXMXOTYOQY\n-A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \"kube-system/kube-dns:dns\" -j KUBE-SEP-25M36I63GRYNYR7P\nCOMMIT\nCompleted on Wed Dec 13 05:25:32 2017\nGenerated by iptables-save v1.6.0 on Wed Dec 13 05:25:32 2017\n*filter\n:INPUT DROP [1:40]\n:FORWARD ACCEPT [0:0]\n:OUTPUT ACCEPT [81:8669]\n:DOCKER - [0:0]\n:DOCKER-ISOLATION - [0:0]\n:KUBE-FIREWALL - [0:0]\n:KUBE-SERVICES - [0:0]\n-A INPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A INPUT -j KUBE-FIREWALL\n-A INPUT -s 192.168.168.99/32 -p tcp -j ACCEPT\n-A INPUT -s 192.168.168.99/32 -p udp -j ACCEPT\n-A INPUT -s 192.168.146.246/32 -p tcp -j ACCEPT\n-A INPUT -s 192.168.146.246/32 -p udp -j ACCEPT\n-A INPUT -s 192.168.175.52/32 -p tcp -j ACCEPT\n-A INPUT -s 192.168.175.52/32 -p udp -j ACCEPT\n-A INPUT -s 192.168.187.140/32 -p tcp -j ACCEPT\n-A INPUT -s 192.168.187.140/32 -p udp -j ACCEPT\n-A INPUT -i wg0 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT\n-A INPUT -p udp -m udp --dport 80 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 443 -j ACCEPT\n-A INPUT -p udp -m udp --dport 443 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 6443 -j ACCEPT\n-A INPUT -p udp -m udp --dport 6443 -j ACCEPT\n-A INPUT -p tcp -m tcp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT\n-A INPUT -p udp -m udp --dport 22 -m conntrack --ctstate NEW,ESTABLISHED -j ACCEPT\n-A INPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A INPUT -i lo -m comment --comment \"Allow all loopback traffic\" -j ACCEPT\n-A INPUT -d 127.0.0.0/8 ! -i lo -m comment --comment \"Drop all traffic to 127 that doesn\\'t use lo\" -j REJECT --reject-with icmp-port-unreachable\n-A FORWARD -j DOCKER-ISOLATION\n-A FORWARD -o docker0 -j DOCKER\n-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n-A FORWARD -i docker0 ! -o docker0 -j ACCEPT\n-A FORWARD -i docker0 -o docker0 -j ACCEPT\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT -j KUBE-FIREWALL\n-A DOCKER-ISOLATION -j RETURN\n-A KUBE-FIREWALL -m comment --comment \"kubernetes firewall for dropping marked packets\" -m mark --mark 0x8000/0x8000 -j DROP\nCOMMIT\nCompleted on Wed Dec 13 05:25:32 2017\n```. ",
    "manojmeda": "I have a similar issue.\nMy setup:\nk8s version: v1.8.5\nI have two nodes (Oracle Linux 7.4)\nNetwork plugin: flannel\nnode1 -> pod1 (nginx)\nnode2 -> pod2 (nginx)\nnode1 to pod1 - works\nnode2 to pod2 - works\nnode1 to pod2 -> ping works. curl does not work.\nnode2 to pod1 -> ping works. curl does not work.\nI have added the iptables rules to FORWARD chain. Still does not work. I am really not sure why ping works but not http. Any help is much appreciated.. Using tcpdump, I can see that the traffic reaches the pod but the I do not see anything going out from the pod.. ",
    "daveamit": "Having the same issue but with weave. (5 Nodes CoreOS)\nI have to agree with @slawekgh ... maybe its linked to docker somehow.\nIf I ping from other nodes, it works. If I curl from other nodes, itdoes not work.\nIf I ping from the slave node, it works. if I curl from the slave node (where that specific pod is running), it works.. ",
    "jverger": "Hello,\ni've the same problem in my cluster...\nDid someone resolved this issue ?\nRegards \nJulien. Hello,\nthis post helped me to solve my issue: \nIt seems that some forwarding rules needs to be set manualy since docker v1.13 \nMaybe earlier version of flanneld can solve this issue (actually can't have a more recent version than 0.7.0)\nRegards. https://github.com/coreos/flannel/issues/799. ",
    "RahulG115": "facing the same for weave as well \nhelpp??\n. ",
    "bogd": "I am seeing the exact same symptoms as @manojmeda and @daveamit :\n\nping to pods running on other nodes works, but curl does not\ncurl to a pod only works from the node running the pod\nwith a tcpdump running within a pod, I can see traffic coming in (TCP SYNs), but no reply traffic going out\n\niptables -P FORWARD ACCEPT is set.\nDid anyone manage to get to the bottom of this issue?. I am seeing a very similar issue in my lab environment (k8s bootstrapped using kubeadm, flannel installed with kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ).\nStrangely enough, in my case I can ping from a node to a pod running on another node, but a curl to it fails. Using tcpdump on the destination pod, I can see the initial packet (TCP SYN) on the pod's network interface, but no response ever goes out.\nI did configure iptables -P FORWARD ACCEPT - same result.\nSwitching to weave does solve the issue, but I would still like to get flannel to work.... As far as I can tell (but don't take my word for it), they both use VXLAN behind the scenes by default. So it is very strange that one works and another one does not. I tested them on exactly the same cloud infrastructure, provisioned by exactly the same scripts - so I can guarantee that the VMs are absolutely identical!. ",
    "trunet": "We had a dependency between flannel and docker on systemd. Everytime that flannel was restarted, docker was being restarted as well. I removed the RequiredBy from docker unit to fix this behaviour.\nSo, there's no impact on doing the full restart. flannel restart is part of the certification rotation script that run once a week.. ",
    "aksgithub": "I am curious to know latest on this issue. i am experiencing same problem since, flannel has to be restarted to reload the certificates which causes docker to restart due to docker depending on flannel for ips. . ",
    "Dao007forever": "I guess it's the kubelet?. ",
    "michmike": "cc: @alinbalutoiu. ",
    "cspwizard": "Hi!\nAre there any plans when this would be available for use on Windows K8S systems?\nThanks!. @rakelkar yes, sure!. ",
    "rjmorse": "Count me in as a tester of early bits as well please. . ",
    "dmitryzykov": "Hello @rakelkar\nI've tried to test host-gw and VXLAN modes for Windows 1709 nodes, but unfortunately, it doesn't work at the end. \nWhat I did after reading slack talks, watching your demo video and presentation from slack:\nran kubernetes v1.9.0 cluster with flannel in VXLAN and then in host-gw mode.\ncompiled all required windows binaries from: https://github.com/containernetworking/plugins/pull/85 and https://github.com/coreos/flannel/pull/832 and v1.9.0 kubernetes repo\n\u251c\u2500\u2500 admin.conf\n\u251c\u2500\u2500 cni\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 cni-conf.json\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 flannel.exe\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host-local.exe\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 win-l2bridge.exe\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 win-overlay.exe\n\u251c\u2500\u2500 etc\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 kube-flannel\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 net-conf.json\n\u251c\u2500\u2500 flanneld.exe\n\u251c\u2500\u2500 kubectl.exe\n\u251c\u2500\u2500 kubelet.exe\n\u2514\u2500\u2500 kube-proxy.exe\nstarted binaries\nc:\\k\\kubelet.exe --hostname-override=windows1 --v=6 `\n    --pod-infra-container-image=kubeletwin/pause --resolv-conf=\"\" `\n    --allow-privileged=true --enable-debugging-handlers `\n    --cluster-dns=10.233.0.3 --cluster-domain=cluster.local `\n    --kubeconfig=c:\\k\\admin.conf --hairpin-mode=promiscuous-bridge `\n    --image-pull-progress-deadline=20m --cgroups-per-qos=false `\n    --enforce-node-allocatable=\"\" `\n    --network-plugin=cni --cni-bin-dir=\"c:\\k\\cni\" --cni-conf-dir \"c:\\k\\cni\\config\"\n$env:NODE_NAME = \"windows1\"\n.\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr=1\nwindows node sucessfully joined cluster:\nNAME       STATUS    ROLES     AGE       VERSION\nlinux4     Ready     node      19m       v1.9.0+coreos.0\nlinux5     Ready     node      19m       v1.9.0+coreos.0\nlinux6     Ready     node      18m       v1.9.0+coreos.0\nmaster1    Ready     master    19m       v1.9.0+coreos.0\nmaster2    Ready     master    19m       v1.9.0+coreos.0\nwindows1   Ready     <none>    4m        v1.9.0-dirty\ncreated POD, which stuck in ContainerCreating status\nkubectl get pods\nNAME                             READY     STATUS              RESTARTS   AGE\nwin-webserver-6f5c5c676d-8mt77   0/1       ContainerCreating   0          10m\nin pod events i found that:\nWarning  FailedCreatePodSandBox  <invalid> (x6 over 25s)  kubelet, windows1  Failed create pod sandbox.\n  Normal   SandboxChanged          <invalid> (x6 over 25s)  kubelet, windows1  Pod sandbox changed, it will be killed and re-created.\nIn kubelet windows logs was error:\nE0115 08:42:00.545300   56084 kuberuntime_manager.go:647] createPodSandbox for pod \"win-webserver-6f5c5c676d-8mt77_default(e5deb513-fa12-11e7-a02b-00155d010712)\" failed: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod \"win-webserver-6f5c5c676d-8mt77_default\" network: HNS failed with error : The parameter is incorrect.\nE0115 08:42:00.546236   56084 pod_workers.go:186] Error syncing pod e5deb513-fa12-11e7-a02b-00155d010712 (\"win-webserver-6f5c5c676d-8mt77_default(e5deb513-fa12-11e7-a02b-00155d010712)\"), skipping: failed to \"CreatePodSandbox\" for \"win-webserver-6f5c5c676d-8mt77_default(e5deb513-fa12-11e7-a02b-00155d010712)\" with CreatePodSandboxError: \"CreatePodSandbox for pod \\\"win-webserver-6f5c5c676d-8mt77_default(e5deb513-fa12-11e7-a02b-00155d010712)\\\" failed: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod \\\"win-webserver-6f5c5c676d-8mt77_default\\\" network: HNS failed with error : The parameter is incorrect. \"\nI0115 08:42:00.546236   56084 server.go:231] Event(v1.ObjectReference{Kind:\"Pod\", Namespace:\"default\", Name:\"win-webserver-6f5c5c676d-8mt77\", UID:\"e5deb513-fa12-11e7-a02b-00155d010712\", APIVersion:\"v1\", ResourceVersion:\"2640\", FieldPath:\"\"}): type: 'Warning' reason: 'FailedCreatePodSandBox' Failed create pod sandbox.\nThis error was in VXLAN mode.\nmy CNI configs:\nbash-4.2# cat cni-conf.json\n{\n   \"name\": \"vxlan0\",\n   \"type\": \"flannel\",\n   \"delegate\": {\n      \"type\": \"win-overlay\"\n   }\n}\nbash-4.2# cat net-conf.json\n{\n  \"Network\": \"10.244.0.0/16\",\n  \"Backend\": {\n    \"name\": \"vxlan0\",\n    \"type\": \"vxlan\",\n    \"vni\": 4096\n  }\n}\nand flannel output:\n```\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0115 08:37:11.927665   57700 main.go:411] Searching for interface using 10.2.51.61\nI0115 08:37:12.198777   57700 main.go:482] Using interface with name Ethernet and address 10.2.51.61\nI0115 08:37:12.198777   57700 main.go:499] Defaulting external address to interface address (10.2.51.61)\nI0115 08:37:12.209776   57700 kube.go:130] Waiting 10m0s for node controller to sync\nI0115 08:37:12.209776   57700 kube.go:283] Starting kube subnet manager\nI0115 08:37:13.212567   57700 kube.go:137] Node controller sync successful\nI0115 08:37:13.212567   57700 main.go:234] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0115 08:37:13.214566   57700 main.go:237] Installing signal handlers\nI0115 08:37:13.215568   57700 main.go:347] Found network config - Backend type: vxlan\nI0115 08:37:13.215568   57700 vxlan_windows.go:111] VXLAN config: {name:vxlan0 macPrefix:0E-2A VNI:4096 Port:4789 GBP:false DirectRouting:false}\nI0115 08:37:13.269572   57700 vxlan_windows.go:181] Attempting to create HNS network, request: {\"Name\":\"vxlan0\",\"Subnets\":[{\"AddressPrefix\":\"10.244.0.0/16\",\"GatewayAddress\":\"10.244.0.1\",\"Policies\":[{\"Type\":\"VSID\",\"VSID\":4096}]}],\"Type\":\"Overlay\"}\nE0115 08:37:14.385312   57700 streamwatcher.go:109] Unable to decode an event from the watch stream: read tcp 10.2.51.61:51724->10.2.51.51:6443: wsarecv: An established connection was aborted by the software in your host machine.\nE0115 08:37:14.388313   57700 reflector.go:304] github.com/coreos/flannel/subnet/kube/kube.go:284: Failed to watch v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=2227&timeoutSeconds=582&watch=true: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable network.\nI0115 08:37:15.358324   57700 vxlan_windows.go:186] Created HNS network [vxlan0] as &{Id:ab461c56-1327-44bf-aa7e-1a985c84b76e Name:vxlan0 Type:Overlay NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-15-5D-10-A0-00 EndMacAddress:00-15-5D-10-AF-FF}] Subnets:[{AddressPrefix:10.244.0.0/16 GatewayAddress:10.244.0.1 Policies:[[123 32 34 84 121 112 101 34 32 58 32 34 86 83 73 68 34 44 32 34 86 83 73 68 34 32 58 32 52 48 57 54 32 125]]}] DNSSuffix: DNSServerList: DNSServerCompartment:4 ManagementIP:10.2.51.61 AutomaticDNS:false}\nI0115 08:37:15.364326   57700 main.go:294] Wrote subnet file to /run/flannel/subnet.env\nI0115 08:37:15.364326   57700 main.go:298] Running backend.\nI0115 08:37:15.364326   57700 main.go:316] Waiting for all goroutines to exit\nI0115 08:37:15.364326   57700 vxlan_network_windows.go:56] Watching for new subnet leases\nE0115 08:37:15.393328   57700 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:284: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nE0115 08:37:16.396338   57700 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:284: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nE0115 08:37:17.398268   57700 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:284: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nI0115 08:37:44.830206   57700 vxlan_network_windows.go:123] Subnet added: 10.233.64.0/24 [29464486400 ns]\nI0115 08:38:35.936004   57700 vxlan_network_windows.go:123] Subnet added: 10.233.65.0/24 [51103984000 ns]\nI0115 08:39:50.522490   57700 vxlan_network_windows.go:123] Subnet added: 10.233.66.0/24 [74584420300 ns]\nI0115 08:41:26.992390   57700 vxlan_network_windows.go:123] Subnet added: 10.233.67.0/24 [96467647200 ns]\nI0115 08:43:12.912303   57700 vxlan_network_windows.go:123] Subnet added: 10.233.68.0/24 [105917484400 ns]\nand network config:\nPS C:\\k> Get-HNSNetwork\n\n\nActivityId             : ba2b92fe-f41b-450d-9a36-7700d9e02265\nCurrentEndpointCount   : 0\nDNSServerCompartment   : 4\nDrMacAddress           : 00-15-5D-7B-F9-50\nExtensions             : {@{Id=e7c3b2f0-f3c5-48df-af2b-10fed6d72e7a; IsEnabled=False},\n                         @{Id=e9b59cfa-2be1-4b21-828f-b6fbdbddc017; IsEnabled=True},\n                         @{Id=ea24cd6c-d17a-4348-9190-09f0d5be83dd; IsEnabled=False}}\nID                     : ab461c56-1327-44bf-aa7e-1a985c84b76e\nLayeredOn              : 60f6e4a4-bf78-479e-b987-c0c9ec4040a2\nMacPools               : {@{EndMacAddress=00-15-5D-10-AF-FF; StartMacAddress=00-15-5D-10-A0-00}}\nManagementIP           : 10.2.51.61\nMaxConcurrentEndpoints : 0\nName                   : vxlan0\nPolicies               : {}\nResources              : @{AllocationOrder=1; Allocators=System.Object[];\n                         ID=ba2b92fe-f41b-450d-9a36-7700d9e02265; PortOperationTime=0; State=1;\n                         SwitchOperationTime=0; VfpOperationTime=0;\n                         parentId=d2471075-c1e1-4dc4-bb0d-e0134efd004c}\nState                  : 1\nSubnets                : {@{AddressPrefix=10.244.0.0/16; GatewayAddress=10.244.0.1; Policies=System.Object[]}}\nTotalEndpoints         : 0\nType                   : Overlay\nVersion                : 21474836481\n``\nSo it stuck with kubelet errornetwork: HNS failed with error : The parameter is incorrect.` and i couldn't solve this.\nAfter that, I've reinstalled cluster in host-gw mode\nmy CNI configs (i've tried both br0 and cbr0 names) :\nbash-4.2# cat cni-conf.json\n{\n   \"name\": \"cbr0\",\n   \"type\": \"flannel\",\n   \"delegate\": {\n      \"type\": \"win-l2bridge\"\n   }\n}\nbash-4.2# cat net-conf.json\n{\n  \"Network\": \"10.244.0.0/16\",\n  \"Backend\": {\n    \"name\": \"cbr0\",\n    \"type\": \"host-gw\",\n    \"vni\": 4096\n  }\n}\nand flannel is failed to run:\nwhen I started it for the first time:\n```\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0114 12:10:07.263566   51624 main.go:411] Searching for interface using 10.2.51.61\nI0114 12:10:07.709211   51624 main.go:482] Using interface with name Ethernet and address 10.2.51.61\nI0114 12:10:07.709211   51624 main.go:499] Defaulting external address to interface address (10.2.51.61)\nI0114 12:10:07.717207   51624 kube.go:130] Waiting 10m0s for node controller to sync\nI0114 12:10:07.717207   51624 kube.go:283] Starting kube subnet manager\nI0114 12:10:08.717246   51624 kube.go:137] Node controller sync successful\nI0114 12:10:08.717246   51624 main.go:234] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0114 12:10:08.720248   51624 main.go:237] Installing signal handlers\nI0114 12:10:08.720248   51624 main.go:347] Found network config - Backend type: host-gw\nI0114 12:10:08.744258   51624 hostgw_windows.go:147] Attempting to create HNS network, request: {\"DNSServerList\n\":\"\",\"Name\":\"cbr0\",\"Subnets\":[{\"AddressPrefix\":\"10.233.69.0/24\",\"GatewayAddress\":\"10.233.69.1\"}],\"Type\":\"l2brid\nge\"}\nE0114 12:10:09.426250   51624 streamwatcher.go:109] Unable to decode an event from the watch stream: read tcp 1\n0.2.51.61:51473->10.2.51.51:6443: wsarecv: An established connection was aborted by the software in your host m\nachine.\nE0114 12:10:09.428263   51624 reflector.go:304] github.com/coreos/flannel/subnet/kube/kube.go:284: Failed to wa\ntch *v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=13593&timeoutSeconds=582&watch=true: dia\nl tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable network.\nI0114 12:10:10.058264   51624 hostgw_windows.go:153] Created HNS network [cbr0] as &{Id:a92467b9-7e34-44bf-ae5e\n-07ca4703b45e Name:cbr0 Type:l2bridge NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-\n15-5D-CD-90-00 EndMacAddress:00-15-5D-CD-9F-FF}] Subnets:[{AddressPrefix:10.233.69.0/24 GatewayAddress:10.233.6\n9.1 Policies:[]}] DNSSuffix: DNSServerList: DNSServerCompartment:0 ManagementIP: AutomaticDNS:false}\nI0114 12:10:10.065256   51624 hostgw_windows.go:182] Attempting to create HNS endpoint [&{Id: Name:cbr0_ep Virt\nualNetwork:a92467b9-7e34-44bf-ae5e-07ca4703b45e VirtualNetworkName: Policies:[] MacAddress: IPAddress:10.233.69\n.2 DNSSuffix: DNSServerList: GatewayAddress: EnableInternalDNS:false DisableICC:false PrefixLength:0 IsRemoteEn\ndpoint:false}]\nI0114 12:10:10.104265   51624 hostgw_windows.go:188] Created bridge endpoint [cbr0_ep] as &{Id:b4aaf069-e5f9-40\n5f-96de-191b625650a4 Name:cbr0_ep VirtualNetwork:a92467b9-7e34-44bf-ae5e-07ca4703b45e VirtualNetworkName:cbr0 P\nolicies:[[123 32 34 84 121 112 101 34 32 58 32 34 76 50 68 114 105 118 101 114 34 32 125]] MacAddress:00-15-5D-\nCD-98-11 IPAddress:10.233.69.2 DNSSuffix: DNSServerList: GatewayAddress:10.233.69.1 EnableInternalDNS:false Dis\nableICC:false PrefixLength:24 IsRemoteEndpoint:false}\nI0114 12:10:10.335260   51624 hostgw_windows.go:194] Attached bridge endpoint [cbr0_ep] to host\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xc0000005 code=0x0 addr=0xc0 pc=0x1177fcc]\n\n\ngoroutine 1 [running]:\ngithub.com/coreos/flannel/backend/hostgw.(*HostgwBackend).RegisterNetwork(0xc042176500, 0x6e70068, 0xc0423b4600\n, 0xc042142910, 0xc042176500, 0x0, 0x0, 0xa0042599d01)\n        /root/gopath/src/github.com/coreos/flannel/backend/hostgw/hostgw_windows.go:198 +0x171c\nmain.main()\n        /root/gopath/src/github.com/coreos/flannel/main.go:277 +0x7a4\nand for all the next:\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0114 12:12:25.132420   51192 main.go:411] Searching for interface using 10.2.51.61\nI0114 12:12:25.383257   51192 main.go:482] Using interface with name vEthernet (Ethernet) and address 10.2.51.6\n1\nI0114 12:12:25.383257   51192 main.go:499] Defaulting external address to interface address (10.2.51.61)\nI0114 12:12:25.393256   51192 kube.go:130] Waiting 10m0s for node controller to sync\nI0114 12:12:25.393256   51192 kube.go:283] Starting kube subnet manager\nI0114 12:12:26.393277   51192 kube.go:137] Node controller sync successful\nI0114 12:12:26.393277   51192 main.go:234] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0114 12:12:26.395298   51192 main.go:237] Installing signal handlers\nI0114 12:12:26.396283   51192 main.go:347] Found network config - Backend type: host-gw\nI0114 12:12:26.419268   51192 hostgw_windows.go:114] Found existing HNS network [&{Id:a92467b9-7e34-44bf-ae5e-0\n7ca4703b45e Name:cbr0 Type:l2bridge NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-15\n-5D-CD-90-00 EndMacAddress:00-15-5D-CD-9F-FF}] Subnets:[{AddressPrefix:10.233.69.0/24 GatewayAddress:10.233.69.\n1 Policies:[]}] DNSSuffix: DNSServerList: DNSServerCompartment:0 ManagementIP:10.2.51.61 AutomaticDNS:false}]\nI0114 12:12:26.428269   51192 hostgw_windows.go:162] Found existing HNS bridge endpoint [&{Id:b4aaf069-e5f9-405\nf-96de-191b625650a4 Name:cbr0_ep VirtualNetwork:a92467b9-7e34-44bf-ae5e-07ca4703b45e VirtualNetworkName:cbr0 Po\nlicies:[[123 32 34 84 121 112 101 34 32 58 32 34 76 50 68 114 105 118 101 114 34 32 125]] MacAddress:00-15-5D-C\nD-98-11 IPAddress:10.233.69.2 DNSSuffix: DNSServerList: GatewayAddress:10.233.69.1 EnableInternalDNS:false Disa\nbleICC:false PrefixLength:24 IsRemoteEndpoint:false}]\nI0114 12:12:26.449273   51192 hostgw_windows.go:194] Attached bridge endpoint [cbr0_ep] to host\nE0114 12:12:26.810274   51192 main.go:279] Error registering network: unable to enable forwarding on [vEthernet\n (Ethernet)], error: failed to enable forwarding on [vEthernet (Ethernet)], error: exit status 1. cmd: int ipv4\n set int \"vEthernet (Ethernet)\" for=en. stdout: The parameter is incorrect.\n\n\nI0114 12:12:26.810274   51192 main.go:327] Stopping shutdownHandler...\n```\nI appreciate any help to run it. Also, I can provide any details of my setup. Thank you.. @rakelkar \nI'm trying to build it now from https://github.com/rakelkar/plugins/tree/windowsCni, but I can't find valid deps:\n-bash-4.2# go get -d github.com/rakelkar/plugins/\npackage github.com/rakelkar/plugins: no Go files in /root/gopath/src/github.com/rakelkar/plugins\n-bash-4.2# cd $GOPATH/src/github.com/rakelkar/plugins/\n-bash-4.2# git checkout windowsCni\nBranch windowsCni set up to track remote branch windowsCni from origin.\nSwitched to a new branch 'windowsCni'\n-bash-4.2# export GOOS=windows GOARCH=amd64\n-bash-4.2# go get ./...\npackage github.com/containernetworking/plugins/pkg/hns: cannot find package \"github.com/containernetworking/plugins/pkg/hns\" in any of:\n    /root/go/src/github.com/containernetworking/plugins/pkg/hns (from $GOROOT)\n    /root/gopath/src/github.com/containernetworking/plugins/pkg/hns (from $GOPATH)\n-bash-4.2# go build -o /root/windows/cni/win-overlay.exe -pkgdir \"$GOPATH/pkg\" ./plugins/main/windows/overlay\nplugins/main/windows/overlay/overlay_windows.go:30:2: cannot find package \"github.com/containernetworking/plugins/pkg/hns\" in any of:\n    /root/gopath/src/github.com/rakelkar/plugins/vendor/github.com/containernetworking/plugins/pkg/hns (vendor tree)\n    /root/go/src/github.com/containernetworking/plugins/pkg/hns (from $GOROOT)\n    /root/gopath/src/github.com/containernetworking/plugins/pkg/hns (from $GOPATH)\nIf I get hns from https://github.com/containernetworking/plugins/pull/85 it doesn't compile well\n```\n-bash-4.2# go build -o /root/windows/cni/win-overlay.exe -pkgdir \"$GOPATH/pkg\" ./plugins/main/windows/overlay\ngithub.com/rakelkar/plugins/plugins/main/windows/overlay\nplugins/main/windows/overlay/overlay_windows.go:87:85: cannot use func literal (type func() (\"github.com/rakelkar/plugins/vendor/github.com/Microsoft/hcsshim\".HNSEndpoint, error)) as type hns.EndpointMakerFunc in argument to hns.ProvisionEndpoint\nplugins/main/windows/overlay/overlay_windows.go:95:45: cannot use r (type \"github.com/containernetworking/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result) as type \"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result in argument to \"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types/current\".NewResultFromResult:\n    \"github.com/containernetworking/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result does not implement \"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result (wrong type for GetAsVersion method)\n        have GetAsVersion(string) (\"github.com/containernetworking/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result, error)\n        want GetAsVersion(string) (\"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result, error)\nplugins/main/windows/overlay/overlay_windows.go:132:36: cannot use hnsNetwork (type \"github.com/rakelkar/plugins/vendor/github.com/Microsoft/hcsshim\".HNSNetwork) as type \"github.com/containernetworking/plugins/vendor/github.com/Microsoft/hcsshim\".HNSNetwork in argument to hns.ConstructResult\nplugins/main/windows/overlay/overlay_windows.go:137:26: cannot use result (type \"github.com/containernetworking/plugins/vendor/github.com/containernetworking/cni/pkg/types/current\".Result) as type \"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result in argument to \"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".PrintResult:\n    *\"github.com/containernetworking/plugins/vendor/github.com/containernetworking/cni/pkg/types/current\".Result does not implement \"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result (wrong type for GetAsVersion method)\n        have GetAsVersion(string) (\"github.com/containernetworking/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result, error)\n        want GetAsVersion(string) (\"github.com/rakelkar/plugins/vendor/github.com/containernetworking/cni/pkg/types\".Result, error)\nso where is proper location for hns?. @rakelkar\nI've tried host-gw from #921\nbash-4.2# go get -d github.com/coreos/flannel\nbash-4.2# cd $GOPATH/src/github.com/coreos/flannel/\nbash-4.2#\nbash-4.2# git fetch origin pull/921/head:rakelkar/windowsHostGw180115\nremote: Counting objects: 6, done.\nremote: Compressing objects: 100% (4/4), done.\nremote: Total 6 (delta 2), reused 5 (delta 2), pack-reused 0\nUnpacking objects: 100% (6/6), done.\nFrom https://github.com/coreos/flannel\n * [new ref]         refs/pull/921/head -> rakelkar/windowsHostGw180115\nbash-4.2# git checkout rakelkar/windowsHostGw180115\nSwitched to branch 'rakelkar/windowsHostGw180115'\nbash-4.2#\nbash-4.2# go get -d github.com/Microsoft/hcsshim\nbash-4.2# go get -d github.com/rakelkar/gonetsh/netroute\nbash-4.2# go get -d github.com/rakelkar/gonetsh/netsh\nbash-4.2#\nbash-4.2# curl https://raw.githubusercontent.com/coreos/flannel/master/Makefile > Makefile\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 10820  100 10820    0     0  46837      0 --:--:-- --:--:-- --:--:-- 47043\nbash-4.2# make dist/flanneld.exe\nGOOS=windows go build -o dist/flanneld.exe \\\n  -ldflags '-s -w -X github.com/coreos/flannel/version.Version=v0.9.0-84-g1cb9412 -extldflags \"-static\"'\nbut I have the same error:\nAt the first run:\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0117 10:27:15.872672   61908 main.go:417] Searching for interface using 10.2.51.61\nI0117 10:27:16.123660   61908 main.go:488] Using interface with name Ethernet and address 10.2.51.61\nI0117 10:27:16.123660   61908 main.go:505] Defaulting external address to interface address (10.2.51.61)\nI0117 10:27:16.136660   61908 kube.go:131] Waiting 10m0s for node controller to sync\nI0117 10:27:16.136660   61908 kube.go:294] Starting kube subnet manager\nI0117 10:27:17.137377   61908 kube.go:138] Node controller sync successful\nI0117 10:27:17.137377   61908 main.go:235] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0117 10:27:17.138381   61908 main.go:238] Installing signal handlers\nI0117 10:27:17.138381   61908 main.go:353] Found network config - Backend type: host-gw\nI0117 10:27:17.162379   61908 hostgw_windows.go:147] Attempting to create HNS network, request: {\"DNSServerList\n\":\"\",\"Name\":\"cbr0\",\"Subnets\":[{\"AddressPrefix\":\"10.233.69.0/24\",\"GatewayAddress\":\"10.233.69.1\"}],\"Type\":\"l2brid\nge\"}\nE0117 10:27:17.985358   61908 streamwatcher.go:109] Unable to decode an event from the watch stream: read tcp 1\n0.2.51.61:61216->10.2.51.51:6443: wsarecv: An established connection was aborted by the software in your host m\nachine.\nE0117 10:27:17.987375   61908 reflector.go:304] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to wa\ntch v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=6487&timeoutSeconds=582&watch=true: dial\n tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable network.\nI0117 10:27:18.669142   61908 hostgw_windows.go:153] Created HNS network [cbr0] as &{Id:ea7c6139-2b72-449c-afd5\n-4b804860dd83 Name:cbr0 Type:l2bridge NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-\n15-5D-5F-10-00 EndMacAddress:00-15-5D-5F-1F-FF}] Subnets:[{AddressPrefix:10.233.69.0/24 GatewayAddress:10.233.6\n9.1 Policies:[]}] DNSSuffix: DNSServerList: DNSServerCompartment:0 ManagementIP: AutomaticDNS:false}\nI0117 10:27:18.676144   61908 hostgw_windows.go:182] Attempting to create HNS endpoint [&{Id: Name:cbr0_ep Virt\nualNetwork:ea7c6139-2b72-449c-afd5-4b804860dd83 VirtualNetworkName: Policies:[] MacAddress: IPAddress:10.233.69\n.2 DNSSuffix: DNSServerList: GatewayAddress: EnableInternalDNS:false DisableICC:false PrefixLength:0 IsRemoteEn\ndpoint:false}]\nI0117 10:27:18.724142   61908 hostgw_windows.go:188] Created bridge endpoint [cbr0_ep] as &{Id:23d91650-0e11-43\n08-8377-c06ae4554533 Name:cbr0_ep VirtualNetwork:ea7c6139-2b72-449c-afd5-4b804860dd83 VirtualNetworkName:cbr0 P\nolicies:[[123 32 34 84 121 112 101 34 32 58 32 34 76 50 68 114 105 118 101 114 34 32 125]] MacAddress:00-15-5D-\n5F-11-0E IPAddress:10.233.69.2 DNSSuffix: DNSServerList: GatewayAddress:10.233.69.1 EnableInternalDNS:false Dis\nableICC:false PrefixLength:24 IsRemoteEndpoint:false}\nE0117 10:27:18.994143   61908 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to li\nst v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A\n socket operation was attempted to an unreachable host.\nI0117 10:27:19.022139   61908 hostgw_windows.go:194] Attached bridge endpoint [cbr0_ep] to host\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xc0000005 code=0x0 addr=0xc0 pc=0x117892c]\n\n\ngoroutine 1 [running]:\ngithub.com/coreos/flannel/backend/hostgw.(*HostgwBackend).RegisterNetwork(0xc042674160, 0x71b0000, 0xc04203f900\n, 0xc042131090, 0xc042674160, 0x0, 0x0, 0xa004259bd01)\n        /root/gopath/src/github.com/coreos/flannel/backend/hostgw/hostgw_windows.go:198 +0x171c\nmain.main()\n        /root/gopath/src/github.com/coreos/flannel/main.go:278 +0x7a7\nThe second run:\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0117 10:29:16.131574   56212 main.go:417] Searching for interface using 10.2.51.61\nI0117 10:29:16.385577   56212 main.go:488] Using interface with name vEthernet (Ethernet) and address 10.2.51.61\nI0117 10:29:16.386578   56212 main.go:505] Defaulting external address to interface address (10.2.51.61)\nI0117 10:29:16.395587   56212 kube.go:131] Waiting 10m0s for node controller to sync\nI0117 10:29:16.395587   56212 kube.go:294] Starting kube subnet manager\nI0117 10:29:17.397165   56212 kube.go:138] Node controller sync successful\nI0117 10:29:17.397165   56212 main.go:235] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0117 10:29:17.399167   56212 main.go:238] Installing signal handlers\nI0117 10:29:17.399167   56212 main.go:353] Found network config - Backend type: host-gw\nI0117 10:29:17.421162   56212 hostgw_windows.go:114] Found existing HNS network [&{Id:ea7c6139-2b72-449c-afd5-4b804860dd83 Name:cbr0 Type:l2bridge NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-15-5D-5F-10-00 EndMacAddress:00-15-5D-5F-1F-FF}] Subnets:[{AddressPrefix:10.233.69.0/24 GatewayAddress:10.233.69.1 Policies:[]}] DNSSuffix: DNSServerList: DNSServerCompartment:0 ManagementIP:10.2.51.61 AutomaticDNS:false}]\nI0117 10:29:17.429174   56212 hostgw_windows.go:162] Found existing HNS bridge endpoint [&{Id:23d91650-0e11-4308-8377-c06ae4554533 Name:cbr0_ep VirtualNetwork:ea7c6139-2b72-449c-afd5-4b804860dd83 VirtualNetworkName:cbr0 Policies:[[123 32 34 84 121 112 101 34 32 58 32 34 76 50 68 114 105 118 101 114 34 32 125]] MacAddress:00-15-5D-5F-11-0E IPAddress:10.233.69.2 DNSSuffix: DNSServerList: GatewayAddress:10.233.69.1 EnableInternalDNS:false DisableICC:false PrefixLength:24 IsRemoteEndpoint:false}]\nI0117 10:29:17.436165   56212 hostgw_windows.go:194] Attached bridge endpoint [cbr0_ep] to host\nE0117 10:29:17.793563   56212 main.go:280] Error registering network: unable to enable forwarding on [vEthernet (Ethernet)], error: failed to enable forwarding on [vEthernet (Ethernet)], error: exit status 1. cmd: int ipv4 set int \"vEthernet (Ethernet)\" for=en. stdout: The parameter is incorrect.\n\n\nI0117 10:29:17.793563   56212 main.go:333] Stopping shutdownHandler...\n. @rakelkar I've tried vxlan (CNI from latest https://github.com/rakelkar/plugins/ windowsCni branch and flannel from #922 ) and I got this errors in kubelet at windows node:\nE0120 11:20:50.232260   12812 cni.go:259] Error adding network: Error while ProvisionEndpoint(fbe0d3bfae648e2d715f3b2d8a04a355ff11c767ae301b19fe847a69a78de379_vxlan0,774d8ed7-7299-4665-9141-2a9135933c92,fbe0d3bfae648e2d715f3b2d8a04a355ff11c767ae301b19fe847a69a78de379) :Error while ipam.ExecAdd: failed to allocate for range 0: no IP addresses available in range set: 10.233.69.1-10.233.69.254\n```\nkubelet at linux master node started with --service-cluster-ip-range=10.233.0.0/18\nflanneld output:\n```\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0120 11:17:31.026000    6608 main.go:417] Searching for interface using 10.2.51.61\nI0120 11:17:31.283997    6608 main.go:488] Using interface with name Ethernet and address 10.2.51.61\nI0120 11:17:31.283997    6608 main.go:505] Defaulting external address to interface address (10.2.51.61)\nI0120 11:17:31.295000    6608 kube.go:131] Waiting 10m0s for node controller to sync\nI0120 11:17:31.295000    6608 kube.go:294] Starting kube subnet manager\nI0120 11:17:32.295801    6608 kube.go:138] Node controller sync successful\nI0120 11:17:32.295801    6608 main.go:235] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0120 11:17:32.299810    6608 main.go:238] Installing signal handlers\nI0120 11:17:32.300803    6608 main.go:353] Found network config - Backend type: vxlan\nI0120 11:17:32.301805    6608 vxlan_windows.go:111] VXLAN config: {name:vxlan0 macPrefix:0E-2A VNI:4096 Port:4789 GBP:false DirectRouting:false}\nI0120 11:17:32.331802    6608 vxlan_windows.go:181] Attempting to create HNS network, request: {\"Name\":\"vxlan0\",\"Subnets\":[{\"AddressPrefix\":\"10.233.64.0/18\",\"GatewayAddress\":\"10.233.64.1\",\"Policies\":[{\"Type\":\"VSID\",\"VSID\":4096}]}],\"Type\":\"Overlay\"}\nE0120 11:17:33.806779    6608 streamwatcher.go:109] Unable to decode an event from the watch stream: read tcp 10.2.51.61:49922->10.2.51.51:6443: wsarecv: An established connection was aborted by the software in your host machine.\nE0120 11:17:33.810778    6608 reflector.go:304] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to watch v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=269379&timeoutSeconds=582&watch=true: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable network.\nI0120 11:17:34.739796    6608 vxlan_windows.go:186] Created HNS network [vxlan0] as &{Id:774d8ed7-7299-4665-9141-2a9135933c92 Name:vxlan0 Type:Overlay NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-15-5D-C0-A0-00 EndMacAddress:00-15-5D-C0-AF-FF}] Subnets:[{AddressPrefix:10.233.64.0/18 GatewayAddress:10.233.64.1 Policies:[[123 32 34 84 121 112 101 34 32 58 32 34 86 83 73 68 34 44 32 34 86 83 73 68 34 32 58 32 52 48 57 54 32 125]]}] DNSSuffix: DNSServerList: DNSServerCompartment:3 ManagementIP:10.2.51.61 AutomaticDNS:false}\nI0120 11:17:34.752798    6608 main.go:300] Wrote subnet file to /run/flannel/subnet.env\nI0120 11:17:34.752798    6608 main.go:304] Running backend.\nI0120 11:17:34.753799    6608 main.go:322] Waiting for all goroutines to exit\nI0120 11:17:34.753799    6608 vxlan_network_windows.go:56] Watching for new subnet leases\nE0120 11:17:34.852820    6608 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nE0120 11:17:35.854811    6608 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nE0120 11:17:36.855828    6608 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nI0120 11:17:59.823878    6608 vxlan_network_windows.go:123] Subnet added: 10.233.66.0/24 [25067727500 ns]\nI0120 11:18:48.529974    6608 vxlan_network_windows.go:123] Subnet added: 10.233.67.0/24 [48704449400 ns]\nI0120 11:19:55.874976    6608 vxlan_network_windows.go:123] Subnet added: 10.233.68.0/24 [67344095300 ns]\nI0120 11:21:21.508992    6608 vxlan_network_windows.go:123] Subnet added: 10.233.64.0/24 [85630857700 ns]\n```\n\n\nnet-conf.json\n{\n  \"Network\": \"10.233.64.0/18\",\n  \"Backend\": {\n    \"name\": \"vxlan0\",\n    \"type\": \"vxlan\",\n    \"vni\": 4096\n  }\n}\nP.S. I'll test host-gw soon.. @rakelkar I tried host-gw (CNI from latest https://github.com/rakelkar/plugins/ windowsCni branch, flannel from #921 and latest gonetsh from https://github.com/rakelkar/gonetsh) and I got this errors in kubelet at windows node:\nE0121 05:33:43.353929    1504 cni.go:259] Error adding network: network cbr0 is of an unexpected type: l2bridge\nI0121 05:33:43.356931    1504 cni_windows.go:47] GetPodNetworkStatus result <nil>\nE0121 05:33:43.356931    1504 cni_windows.go:49] error while adding to cni network: network cbr0 is of an unexpected type: l2bridge\nW0121 05:33:43.356931    1504 docker_sandbox.go:340] failed to read pod IP from plugin/docker: NetworkPlugin cni failed on the status hook for pod \"win-webserver-64bf4ffc9d-fn9pn_default\": network cbr0 is of an unexpected type: l2bridge\nI0121 05:33:43.356931    1504 plugins.go:426] Calling network plugin cni to tear down pod \"win-webserver-64bf4ffc9d-fn9pn_default\"\nflanneld output:\n```\nPS C:\\k> .\\flanneld.exe  --kubeconfig-file=\"c:\\k\\admin.conf\" --iface=\"10.2.51.61\" --ip-masq=1 --kube-subnet-mgr\n=1\n\n\nI0121 05:25:19.341026    5708 main.go:417] Searching for interface using 10.2.51.61\nI0121 05:25:24.353253    5708 main.go:488] Using interface with name Ethernet and address 10.2.51.61\nI0121 05:25:24.353253    5708 main.go:505] Defaulting external address to interface address (10.2.51.61)\nI0121 05:25:24.385279    5708 kube.go:131] Waiting 10m0s for node controller to sync\nI0121 05:25:24.386279    5708 kube.go:294] Starting kube subnet manager\nI0121 05:25:25.386575    5708 kube.go:138] Node controller sync successful\nI0121 05:25:25.386575    5708 main.go:235] Created subnet manager: Kubernetes Subnet Manager - windows1\nI0121 05:25:25.386575    5708 main.go:238] Installing signal handlers\nI0121 05:25:25.387576    5708 main.go:353] Found network config - Backend type: host-gw\nI0121 05:25:25.693582    5708 hostgw_windows.go:148] Attempting to create HNS network, request: {\"DNSServerList\":\"\",\"Name\":\"cbr0\",\"Subnets\":[{\"AddressPrefix\":\"10.233.69.0/24\",\"GatewayAddress\":\"10.233.69.1\"}],\"Type\":\"l2bridge\"}\nE0121 05:25:27.397324    5708 streamwatcher.go:109] Unable to decode an event from the watch stream: read tcp 10.2.51.61:49731->10.2.51.51:6443: wsarecv: An established connection was aborted by the software in your host machine.\nE0121 05:25:27.401242    5708 reflector.go:304] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to watch v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=2231&timeoutSeconds=582&watch=true: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable network.\nE0121 05:25:28.402557    5708 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nI0121 05:25:28.547573    5708 hostgw_windows.go:156] Created HNS network [cbr0] as &{Id:0391977a-dc64-4c66-8c2f-3b27f06c2ac4 Name:cbr0 Type:l2bridge NetworkAdapterName: SourceMac: Policies:[] MacPools:[{StartMacAddress:00-15-5D-E2-D0-00 EndMacAddress:00-15-5D-E2-DF-FF}] Subnets:[{AddressPrefix:10.233.69.0/24 GatewayAddress:10.233.69.1 Policies:[]}] DNSSuffix: DNSServerList: DNSServerCompartment:0 ManagementIP: AutomaticDNS:false}\nI0121 05:25:28.557564    5708 hostgw_windows.go:185] Attempting to create HNS endpoint [&{Id: Name:cbr0_ep VirtualNetwork:0391977a-dc64-4c66-8c2f-3b27f06c2ac4 VirtualNetworkName: Policies:[] MacAddress: IPAddress:10.233.69.2 DNSSuffix: DNSServerList: GatewayAddress: EnableInternalDNS:false DisableICC:false PrefixLength:0 IsRemoteEndpoint:false}]\nI0121 05:25:28.566559    5708 hostgw_windows.go:191] Created bridge endpoint [cbr0_ep] as &{Id:aab73359-c58e-4738-be4e-c9f90d41bdcb Name:cbr0_ep VirtualNetwork:0391977a-dc64-4c66-8c2f-3b27f06c2ac4 VirtualNetworkName:cbr0 Policies:[[123 32 34 84 121 112 101 34 32 58 32 34 76 50 68 114 105 118 101 114 34 32 125]] MacAddress:00-15-5D-E2-DF-4C IPAddress:10.233.69.2 DNSSuffix: DNSServerList: GatewayAddress:10.233.69.1 EnableInternalDNS:false DisableICC:false PrefixLength:24 IsRemoteEndpoint:false}\nI0121 05:25:28.782560    5708 hostgw_windows.go:196] Attached bridge endpoint [cbr0_ep] to host\nI0121 05:25:29.198563    5708 hostgw_windows.go:210] Enabled forwarding on [vEthernet (Ethernet)] index [3]\nE0121 05:25:29.418568    5708 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A socket operation was attempted to an unreachable host.\nI0121 05:25:29.816580    5708 hostgw_windows.go:210] Enabled forwarding on [vEthernet (cbr0_ep)] index [13]\nI0121 05:25:29.821574    5708 main.go:300] Wrote subnet file to /run/flannel/subnet.env\nI0121 05:25:29.821574    5708 main.go:304] Running backend.\nI0121 05:25:29.823576    5708 main.go:322] Waiting for all goroutines to exit\nI0121 05:25:29.823576    5708 hostgw_network_windows.go:52] Watching for new subnet leases\nI0121 05:25:29.872577    5708 hostgw_network_windows.go:86] Subnet added: 10.233.66.0/24 via 10.2.51.66\nW0121 05:25:29.872577    5708 hostgw_network_windows.go:89] Ignoring non-host-gw subnet: type=vxlan\nI0121 05:25:31.476818    5708 hostgw_network_windows.go:86] Subnet added: 10.233.67.0/24 via 10.2.51.51\nW0121 05:25:31.476818    5708 hostgw_network_windows.go:89] Ignoring non-host-gw subnet: type=vxlan\nI0121 05:25:32.513120    5708 hostgw_network_windows.go:86] Subnet added: 10.233.68.0/24 via 10.2.51.65\nW0121 05:25:32.513120    5708 hostgw_network_windows.go:89] Ignoring non-host-gw subnet: type=vxlan\nI0121 05:25:33.310125    5708 hostgw_network_windows.go:86] Subnet added: 10.233.64.0/24 via 10.2.51.52\nW0121 05:25:33.311125    5708 hostgw_network_windows.go:89] Ignoring non-host-gw subnet: type=vxlan\nI0121 05:25:34.094133    5708 hostgw_network_windows.go:86] Subnet added: 10.233.65.0/24 via 10.2.51.64\nW0121 05:25:34.094133    5708 hostgw_network_windows.go:89] Ignoring non-host-gw subnet: type=vxlan\nE0121 05:25:51.422050    5708 reflector.go:201] github.com/coreos/flannel/subnet/kube/kube.go:295: Failed to list v1.Node: Get https://10.2.51.51:6443/api/v1/nodes?resourceVersion=0: dial tcp 10.2.51.51:6443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\nmy configs:\n-bash-4.2# cat cni-conf.json\n{\n   \"name\": \"cbr0\",\n   \"type\": \"flannel\",\n   \"delegate\": {\n      \"type\": \"win-l2bridge\"\n   }\n}\n-bash-4.2# cat net-conf.json\n{\n  \"Network\": \"10.233.64.0/18\",\n  \"Backend\": {\n    \"name\": \"cbr0\",\n    \"type\": \"host-gw\",\n    \"vni\": 4096\n  }\n}\n===\nThen for test I've renamed `cbr0` to `br0` and `win-l2bridge` to `l2bridge` in confs, removed cbr0 by `Get-HNSNetwork|Remove-HNSNetwork` and for sure rebooted windows node\nbut flannel creates cbr0, not br0 as expected and kubelet is trying to reach br0 and can't find it.\nI0121 06:17:55.575883    6420 main.go:353] Found network config - Backend type: host-gw\nI0121 06:17:55.616888    6420 hostgw_windows.go:148] Attempting to create HNS network, request: {\"DNSServerList\":\"\",\"Name\":\"cbr0\",\"Subnets\":[{\"AddressPrefix\":\"10.233.69.0/24\",\"GatewayAddress\":\"10.233.69.1\"}],\"Type\":\"l2bridge\"}\n\nActivityId             : 715b7752-48e6-40da-9f7f-7121df2553b7\nCurrentEndpointCount   : 1\nDNSServerList          :\nExtensions             : {@{Id=e7c3b2f0-f3c5-48df-af2b-10fed6d72e7a; IsEnabled=False},\n                         @{Id=e9b59cfa-2be1-4b21-828f-b6fbdbddc017; IsEnabled=True},\n                         @{Id=ea24cd6c-d17a-4348-9190-09f0d5be83dd; IsEnabled=False}}\nID                     : 312e9973-24a3-4b54-8c32-2efddc3fbe52\nLayeredOn              : 7ff7d9b1-35f8-4352-92c4-256869f173ce\nMacPools               : {@{EndMacAddress=00-15-5D-69-3F-FF; StartMacAddress=00-15-5D-69-30-00}}\nManagementIP           : 10.2.51.61\nMaxConcurrentEndpoints : 1\nName                   : cbr0\nPolicies               : {}\nResources              : @{AllocationOrder=0; ID=715b7752-48e6-40da-9f7f-7121df2553b7; PortOperationTime=0; State=1;\n                         SwitchOperationTime=0; VfpOperationTime=0; parentId=d1d08645-78ca-4b7b-a653-6bab06aab2a2}\nState                  : 1\nSubnets                : {@{AddressPrefix=10.233.69.0/24; GatewayAddress=10.233.69.1}}\nTotalEndpoints         : 1\nType                   : l2bridge\nVersion                : 21474836481\n```\n\n\n===\nthen I renamed back br0 to cbr0 and keep l2bridge without win- prefix, reset networks.\nAnd i have the same error like at the beginning:\nI0121 06:43:17.360215    6944 cni.go:284] Got netns path none\nI0121 06:43:17.360215    6944 cni.go:285] Using podns path default\nI0121 06:43:17.360215    6944 cni.go:274] About to del CNI network cbr0 (type=flannel)\nI0121 06:43:17.370230    6944 kuberuntime_manager.go:641] Creating sandbox for pod \"kube-flannel-5rlm9_kube-system(71b3b82d-feae-11e7-92e6-00155d01070e)\"\nI0121 06:43:17.377221    6944 docker_service.go:441] Setting cgroup parent to: \"\"\nE0121 06:43:17.502213    6944 cni.go:259] Error adding network: network cbr0 is of an unexpected type: l2bridge\nI0121 06:43:17.502213    6944 cni_windows.go:47] GetPodNetworkStatus result <nil>\nE0121 06:43:17.504217    6944 cni_windows.go:49] error while adding to cni network: network cbr0 is of an unexpected type: l2bridge\nW0121 06:43:17.505221    6944 docker_sandbox.go:340] failed to read pod IP from plugin/docker: NetworkPlugin cni failed on the status hook for pod \"win-webserver-64bf4ffc9d-wlt8n_default\": network cbr0 is of an unexpected type: l2bridge\nI0121 06:43:17.505221    6944 plugins.go:426] Calling network plugin cni to tear down pod \"win-webserver-64bf4ffc9d-wlt8n_default\"\n. ",
    "nlowe": "Is there more up-to-date documentation for testing this? I was able to join a windows node to my cluster in VXLAN mode and even deploy a windows pod to it. The pod starts up but has no network connectivity. I can't ping the gateway, cluster DNS, or any internal / external services, but I can get pod logs and an exec shell from my workstation or one of my nodes via kubectl. Linux pods successfully resolve the IP of the service exposing the windows pods but can't interact with them.. ",
    "jinxiao": "@nlowe  i think i have the same problem, and the kubelet logs showed that : \n\nCalling network plugin cni to set up pod \"win-webserver-7b87fb99b4-q9nld_default\"\nI0515 15:12:30.174829    9952 cni.go:284] Got netns path none\nI0515 15:12:30.174829    9952 cni.go:285] Using podns path default\nI0515 15:12:30.175692    9952 cni.go:256] About to add CNI network cbr0 (type=flannel)\nE0515 15:12:30.327684    9952 cni.go:259] Error adding network: unsupported platform request\nE0515 15:12:30.327684    9952 cni.go:227] Error while adding to cni network: unsupported platform request\nI0515 15:12:30.369685    9952 kubelet.go:2102] Container runtime status: Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=true reason: message:\n. \n",
    "seanknox": "\nIs there more up-to-date documentation for testing this? I was able to join a windows node to my cluster in VXLAN mode and even deploy a windows pod to it. The pod starts up but has no network connectivity. I can't ping the gateway, cluster DNS, or any internal / external services, but I can get pod logs and an exec shell from my workstation or one of my nodes via kubectl. Linux pods successfully resolve the IP of the service exposing the windows pods but can't interact with them.\n\nIs IP forwarding enabled on the nodes and masters?. ",
    "atomaras": "We are having problems with flannel on windows. Flannel leaks IPs and I am forced to cleanup the cni folder and rejoin my windows nodes. We are also getting a bunch of CNI errors about not being able to remove endpoints and finally service discovery stops working.. ",
    "ylfforme": "after I run this \uff1aiptables -I -s 172.16.0.0/16 FORWARD -j ACCEPT\uff0cIt work fine. ",
    "piontec": "@mgleung Thanks for the answer, but if that's unexpected, it means 2 things to me:\n1. flannel should be a perfect solution to my problems\n2. I'm totally lost and got no idea why I can't make it work :(\nCan I ask for help here?\nLet me give you some more data now:\n1. nodes config\n  * wn2: docker0: 10.5.89.1/24, flannel.1: 10.5.89.0/32\n  * wn3: docker0: 10.5.47.1/24, flannel.1: 10.5.47.0/32\n  * etcd: v3.2.8, the simplest setup that was good enough for testing, 1 node, no security\n  * flanneld: I upgraded to 0.9.0 today, but no change :(\n  * docker running with\n\"userland-proxy\": false,\n    \"iptables\": false,\n    \"ip-forward\": true,\n    \"ip-masq\": false\n  * I checked iptables\n    * for table filter - no rules, all policies to ACCEPT\n    * for table nat some stuff set by flannel to masq traffic going out; just to be sure it's not messing up anything, I Flushed the whole POSTROUTING chain; end result: all chains empty and set to ACCEPT\n  * flanneld start command ExecStart=/usr/local/bin/flanneld -v 4 -iface eth0 -etcd-endpoints 'http://10.237.33.235:2379' --healthz-port=8090 on both nodes\n\nTest results\non wn3 I'm starting tcpdump in a container, it gets 10.5.47.3\n```\n\ndocker run -it --name tcpd --rm  corfr/tcpdump -n -i eth0\n* on `wn2`, I'm pinging tcpdump container on `wn3` from aother container\ndocker run -it --rm alpine ip a && ping 10.5.47.3\n1: lo:  mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n196: eth0@if197:  mtu 8951 qdisc noqueue state UP \n    link/ether 02:42:0a:05:59:02 brd ff:ff:ff:ff:ff:ff\n    inet 10.5.89.2/24 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:aff:fe05:5902/64 scope link tentative \n       valid_lft forever preferred_lft forever\nPING 10.5.47.3 (10.5.47.3) 56(84) bytes of data.\n64 bytes from 10.5.47.3: icmp_seq=7 ttl=63 time=0.589 ms\n64 bytes from 10.5.47.3: icmp_seq=8 ttl=63 time=0.610 ms\n```\nand the tcpdump is\n```\ndocker run -it --name tcpd --rm  corfr/tcpdump -n -i eth0\ntcpdump: verbose output suppressed, use -v or -vv for full protocol decode\nlistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes\n12:10:14.131073 ARP, Request who-has 10.5.47.3 tell 10.5.47.1, length 28\n12:10:14.131090 ARP, Reply 10.5.47.3 is-at 02:42:0a:05:2f:03, length 28\n12:10:14.168889 IP 10.5.89.0 > 10.5.47.3: ICMP echo request, id 25685, seq 7, length 64\n12:10:14.168913 IP 10.5.47.3 > 10.5.89.0: ICMP echo reply, id 25685, seq 7, length 64\n...\n```\nConclusions: definitely not working, src IP is masqueraded, but no MASQ done in iptables. What am I doing wrong?. Afterall, it doesn't feel unexpected, please check dumps showed on this page - they exactly confirm my findings:\nhttp://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-flannel.html. I can't really recreate the testing environment now, so also no iptables-save. But please note I did check iptables in the previous one:\n\nI checked iptables\nfor table filter - no rules, all policies to ACCEPT\nfor table nat some stuff set by flannel to masq traffic going out; just to be sure it's not messing up  anything, I Flushed the whole POSTROUTING chain; end result: all chains empty and set to ACCEPT\n\nSo I'm pretty sure it's not iptables - I flushed everything manually. Can you give me a setup/config that is expected to work? Maybe I can recreate it and then verify again?. ",
    "harryge00": "I have encountered similar issue:\nI have two kube-nodes:\n node1: \n6: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default \n    link/ether 02:42:61:51:7d:e5 brd ff:ff:ff:ff:ff:ff\n    inet 172.16.43.1/25 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:61ff:fe51:7de5/64 scope link \n       valid_lft forever preferred_lft forever\n7: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default \n    link/ether 0e:47:17:48:a4:27 brd ff:ff:ff:ff:ff:ff\n    inet 172.16.43.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::c47:17ff:fe48:a427/64 scope link \n       valid_lft forever preferred_lft forever\n node2:\n6: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default \n    link/ether 82:94:82:80:b8:ad brd ff:ff:ff:ff:ff:ff\n    inet 172.16.5.128/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::8094:82ff:fe80:b8ad/64 scope link \n       valid_lft forever preferred_lft forever\n8: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default \n    link/ether 02:42:a4:90:57:1a brd ff:ff:ff:ff:ff:ff\n    inet 172.16.5.129/25 scope global docker0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::42:a4ff:fe90:571a/64 scope link \n       valid_lft forever preferred_lft forever\nAnd I have two pods running on each nodes. \nNAME             READY     STATUS    RESTARTS   AGE       IP             NODE\nnettools-9nv2p   1/1       Running   0          14m       172.16.43.3        node1\nnettools-9x4qx   1/1       Running   0          14m       172.16.5.135      node2\nWhen I run curl or ping from nettools-9nv2p, the source ip of packet received by nettools-9x4qx  is always 172.16.43.0 instead of 172.16.43.3\nroot@debian:~# flanneld --version\nv0.10.0\nroot@debian:~# kubelet --version\nKubernetes v1.6.7\nroot@debian:~# kube-proxy --version\nKubernetes v1.5.3+029c3a4. node1 ip-tables\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING\n-A POSTROUTING -s 172.16.43.0/25 ! -o docker0 -j MASQUERADE\n-A DOCKER -i docker0 -j RETURN\n-A KUBE-MARK-DROP -j MARK --set-xmark 0x8000/0x8000\n-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000\n-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE\nnode2\n-A PREROUTING -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER\n-A OUTPUT -m comment --comment \"kubernetes service portals\" -j KUBE-SERVICES\n-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER\n-A POSTROUTING -s 172.16.5.128/25 ! -o docker0 -j MASQUERADE\n-A POSTROUTING -m comment --comment \"kubernetes postrouting rules\" -j KUBE-POSTROUTING. Solved after clearing old iptable rules. And add --iptables=false --ip-masq=false to dockerd args. @tomdee \nI still got rules like -A POSTROUTING ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE on my kube-node. I don't know whethter this rule is added by docker or flanneld. I have added -ip-masq=false to both flannel and docker.\nDetailed can be found https://github.com/coreos/flannel/issues/835\n. Solved after cleaning old iptables rules.. ",
    "apple0407": "\nSolved after clearing old iptable rules. And add --iptables=false --ip-masq=false to dockerd args\n\n@harryge00 \nyes, by this way i can get the real ip. But i get another problem, with --ip-masq=false , within container, i can't connect to the internet, or any  other hosts in the host's network.  \nBut within the container, i can still connect to  containers on other nodes via flannel\ni just can't figure out how this happen. . ",
    "sgarg1": "We upgraded to the Alpha version CoreOS 1562.1.0 with Flannel 0.9 and the problem still persists.. ",
    "mikkeloscar": "I don't see anywhere in the vxlan backend that would actually change the MTU it just sets up the device which gets the default MTU of 1500 afaict.\n@tomdee Did you use vxlan in the example above? I see that the udp backend as code for setting the MTU, maybe this is used? (https://github.com/coreos/flannel/blob/master/backend/udp/udp_network.go#L120)\nWhether it makes sense to change the MTU when using vxlan I don't know?\n. We run 100s of Container Linux stable nodes on AWS and we see the same 1500 MTU value for all interfaces (except eth0 which is 9001). If I change the default MTU with a /etc/systemd/network/99-default.link file then I get that instead of 1500 but not -20.\nThis is our node configuration fwiw: https://github.com/zalando-incubator/kubernetes-on-aws/blob/dev/cluster/userdata-worker.yaml\n. > Does this resolve to an IP address on an interface that has the 9000 MTU?\nI'm sorry, I don't fully understand what you mean? Do you mean if flannel uses the eth0 interface? It always does, example from one flannel log:\n1031 main.go:470] Determining IP address of default interface\n1109 15:31:22.125676    1031 main.go:483] Using interface with name eth0 and address 172.31.9.74\n1109 15:31:22.125695    1031 main.go:500] Defaulting external address to interface address (172.31.9.74)\n1109 15:31:22.125785    1031 main.go:235] Created subnet manager: Etcd Local Manager with Previous Subnet: 0.0.0.0/0\n1109 15:31:22.125796    1031 main.go:238] Installing signal handlers\n1109 15:31:22.144787    1031 main.go:348] Found network config - Backend type: vxlan\n1109 15:31:22.144839    1031 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false\n1109 15:31:22.224149    1031 local_manager.go:234] Picking subnet in range 10.2.1.0 ... 10.2.255.0\n1109 15:31:22.227551    1031 local_manager.go:220] Allocated lease (10.2.108.0/24) to current node (172.31.9.74)\n1109 15:31:22.248275    1031 main.go:295] Wrote subnet file to /run/flannel/subnet.env\n1109 15:31:22.248290    1031 main.go:299] Running backend.\nnel - Network fabric for containers (System Application Container).\n1109 15:31:22.254644    1031 ipmasq.go:75] Some iptables rules are missing; deleting and recreating rules\n1109 15:31:22.254670    1031 ipmasq.go:97] Deleting iptables rule: -s 10.2.0.0/16 -d 10.2.0.0/16 -j RETURN\n1109 15:31:22.255818    1031 ipmasq.go:97] Deleting iptables rule: -s 10.2.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE\n1109 15:31:22.257049    1031 ipmasq.go:97] Deleting iptables rule: ! -s 10.2.0.0/16 -d 10.2.108.0/24 -j RETURN\n1109 15:31:22.263636    1031 ipmasq.go:97] Deleting iptables rule: ! -s 10.2.0.0/16 -d 10.2.0.0/16 -j MASQUERADE\n1109 15:31:22.264832    1031 ipmasq.go:85] Adding iptables rule: -s 10.2.0.0/16 -d 10.2.0.0/16 -j RETURN\n1109 15:31:22.269451    1031 ipmasq.go:85] Adding iptables rule: -s 10.2.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE\n1109 15:31:22.284971    1031 vxlan_network.go:56] watching for new subnet leases\n1109 15:31:22.296313    1031 main.go:391] Waiting for 22h59m59.927033448s to renew lease\n1109 15:31:22.296397    1031 ipmasq.go:85] Adding iptables rule: ! -s 10.2.0.0/16 -d 10.2.108.0/24 -j RETURN\n1109 15:31:22.298935    1031 ipmasq.go:85] Adding iptables rule: ! -s 10.2.0.0/16 -d 10.2.0.0/16 -j MASQUERADE\nAnd the interfaces on this host:\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000\n    link/ether 06:2f:ce:c7:aa:3a brd ff:ff:ff:ff:ff:ff\n    inet 172.31.9.74/21 brd 172.31.15.255 scope global dynamic eth0\n       valid_lft 1883sec preferred_lft 1883sec\n    inet6 fe80::42f:ceff:fec7:aa3a/64 scope link \n       valid_lft forever preferred_lft forever\n3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default \n    link/ether ca:d3:ce:dd:25:31 brd ff:ff:ff:ff:ff:ff\n    inet 10.2.108.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::c8d3:ceff:fedd:2531/64 scope link \n       valid_lft forever preferred_lft forever\n4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n    link/ether 02:42:95:95:86:5d brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 scope global docker0\n       valid_lft forever preferred_lft forever\n5: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 0a:58:0a:02:6c:01 brd ff:ff:ff:ff:ff:ff\n    inet 10.2.108.1/24 scope global cni0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::60cf:e9ff:fed0:aab5/64 scope link \n       valid_lft forever preferred_lft forever\n$ ip -d link show dev flannel.1\n3: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default \n    link/ether ca:d3:ce:dd:25:31 brd ff:ff:ff:ff:ff:ff promiscuity 0 \n    vxlan id 1 local 172.31.9.74 dev eth0 srcport 0 0 dstport 8472 nolearning ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535. ",
    "chenchun": "@tomdee PTAL . > If I understand your comment (#842 (comment)) and the code correctly then you're not setting a \"remote\" but you are setting the \"local\" address to the public IP. Can you confirm what effects this has? Which address will be used as the source IP address for packets that originate from the host network namespace? And what if the public IP isn't the same as an IP address on the host, then you can't actually use it?\nMy mistake. I originally had a check if PublicIP is the same as IfaceAddr and I assumed they are equal. Now that I removed this check, I will change the \"local\" address to the IfaceAddr.\nif !extIface.ExtAddr.Equal(extIface.IfaceAddr) {\n        return nil, fmt.Errorf(\"your PublicIP differs from interface IP, meaning that probably you're on a NAT, which is not supported by host-gw backend\")\n    }\n\nmaybe you should be getting the MTU value from that device? WDYT?\n\nI agree.\n\nWhen I run your code I'm seeing two IPIP devices, is that expected?\n\nYes, this is expected. tunl0 is created by the ipip kernel module itself and it is not used by this ipip backend. . The last update removed MTU param from backend configuration. We may not need it since we calculate it based on --iface device's MTU.. Ausome! Thanks for making this happen.. Found this bug through reading the code. I've added an unit test. PTAL. Previous tests failed with\n--- FAIL: TestRouteCache (0.00s)\n    hostgw_network_test.go:43: operation not permitted\nUpdated Makefile to makes use of docker run to give go test NET_ADMIN capability.. +1 I'm trying to implement network policy in my CNI plugin and having the same problem.. Can anyone have a look? . Instead of using tunl0 device, created a new flanneld.ipip ipip device. As a effect of this we should set flanneld.ipip\u2018s local ip to public ip, or kernel will forward packets to tunl0.. Made 1480 as a default MTU and made MTU configurable.. I don't have an environment to test this and currently removed this check.. tunl0 is an namespace default device with IPIP attributes local=any and remote=any.\nIf we create another IPIP device flannel.ipip with local=any and remote=any. It seems when receiving IPIP protocol packets, kernel will forward them to tunl0 as a fallback device instead of flannel.ipip. See https://github.com/torvalds/linux/blob/master/net/ipv4/ip_tunnel.c#L85-L96 .\nSo we have two options here, either rename tunl0 to flannel.ipip or setting local attribute of flannel.ipip to distinguish these two devices. Considering tunl0 might be used by user, so I choose the later option.. Fixed. Fixed. I couldn't find any reference that saying setting down the link first. I think changing MTU on the fly is ok. \nFrom http://linux-ip.net/html/tools-ip-link.html \n\nB.3.5. Using ip link set to change the MTU\nThe remaining options to the ip link command cannot be used while the interface is in an UP state.\n. Fixed. Fixed. I'll try to do it. Added RouteNetwork in common.go. . \n",
    "senwangrockets": "What I think is interesting is \"\nE1018 17:58:54.573780 1 vxlan_network.go:158] failed to add vxlanRoute (172.16.0.0/24 -> 172.16.0.0): invalid argument\n\". ",
    "jhorwit2": "I'm running into the same issue it seems.\nI1026 22:38:06.797811     208 vxlan_network.go:56] watching for new subnet leases\nI1026 22:38:06.800429     208 ipmasq.go:75] Some iptables rules are missing; deleting and recreating rules\nI1026 22:38:06.800450     208 ipmasq.go:97] Deleting iptables rule: -s 172.17.0.0/16 -d 172.17.0.0/16 -j RETURN\nI1026 22:38:06.801507     208 ipmasq.go:97] Deleting iptables rule: -s 172.17.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE\nI1026 22:38:06.802527     208 ipmasq.go:97] Deleting iptables rule: ! -s 172.17.0.0/16 -d 172.17.9.0/24 -j RETURN\nI1026 22:38:06.803535     208 ipmasq.go:97] Deleting iptables rule: ! -s 172.17.0.0/16 -d 172.17.0.0/16 -j MASQUERADE\nI1026 22:38:06.804543     208 ipmasq.go:85] Adding iptables rule: -s 172.17.0.0/16 -d 172.17.0.0/16 -j RETURN\nI1026 22:38:06.806706     208 ipmasq.go:85] Adding iptables rule: -s 172.17.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE\nI1026 22:38:06.808932     208 ipmasq.go:85] Adding iptables rule: ! -s 172.17.0.0/16 -d 172.17.9.0/24 -j RETURN\nI1026 22:38:06.811148     208 ipmasq.go:85] Adding iptables rule: ! -s 172.17.0.0/16 -d 172.17.0.0/16 -j MASQUERADE\nE1026 22:38:11.064786     208 vxlan_network.go:158] failed to add vxlanRoute (172.17.0.0/24 -> 172.17.0.0): invalid argument\nE1027 02:51:24.265565     208 vxlan_network.go:158] failed to add vxlanRoute (172.17.0.0/24 -> 172.17.0.0): invalid argument\n@tomdee none of my nodes have that as the public ip annotation (they're all correct). . I don't see a route for 172.17.0.0/24 on any of my hosts. \n$ ip route\n172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1\n172.17.1.0/24 via 172.17.1.0 dev flannel.1 onlink\n172.17.2.0/24 via 172.17.2.0 dev flannel.1 onlink\n172.17.3.0/24 via 172.17.3.0 dev flannel.1 onlink\n172.17.4.0/24 via 172.17.4.0 dev flannel.1 onlink\n172.17.5.0/24 via 172.17.5.0 dev flannel.1 onlink\n172.17.6.0/24 via 172.17.6.0 dev flannel.1 onlink\n172.17.7.0/24 via 172.17.7.0 dev flannel.1 onlink\n172.17.8.0/24 via 172.17.8.0 dev flannel.1 onlink\n172.17.9.2 dev cali299270d87b6 scope link\n172.17.9.3 dev calib63aee49779 scope link\n172.17.9.4 dev cali12d4a061371 scope link\n$ arp -a\n...\n? (172.17.0.0) at <incomplete> on flannel.1\n...\nFlannel logs\nI1027 12:53:29.439503     166 vxlan_network.go:138] adding subnet: 172.17.0.0/24 PublicIP: 10.65.27.18 VtepMAC: 46:ee:d0:82:55:a4\nI1027 12:53:29.439524     166 device.go:179] calling AddARP: 172.17.0.0, 46:ee:d0:82:55:a4\nI1027 12:53:29.439591     166 device.go:156] calling AddFDB: <hostip>, 46:ee:d0:82:55:a4\nE1027 12:53:29.439668     166 vxlan_network.go:158] failed to add vxlanRoute (172.17.0.0/24 -> 172.17.0.0): invalid argument\nI1027 12:53:29.439706     166 device.go:190] calling DelARP: 172.17.0.0, 46:ee:d0:82:55:a4\nI1027 12:53:29.439751     166 device.go:168] calling DelFDB: <hostip>, 46:ee:d0:82:55:a4. @tomdee that was my issue. Sorry I forgot to post after I realized that.. ",
    "DominicDV": "I had this error too when transitioning from 1.7.5 to 1.8.2.\nA reboot solved this error for me.\n(for completenes: prior to this I deleted the fstab swap entry because kubelet requires that the system doesnt swap. Not sure If this is related). ",
    "kumarganesh2814": "@tomdee \nHi Tom,\nI initialised my cluster with same kubeadm command \nkubeadm init --pod-network-cidr 10.244.0.0/16\nBut Still in Flannel pods I see errors \nE1210 07:10:45.198903       1 vxlan_network.go:158] failed to add vxlanRoute (10.244.2.0/24 -> 10.244.2.0): invalid argument\nI have 4 host cluster 2 of them works fine but other 2 fails to schedule container \nAlways in state of \"ContainerCreating\"\nErrors which I see is \nDec 10 01:39:14 kongapi-poc-db1 kubelet: E1210 01:39:14.554032   58034 cni.go:250] Error while adding to cni network: \"cni0\" already has an IP address different from 10.244.3.1/24\nDec 10 01:39:14 kongapi-poc-db1 kernel: cni0: port 1(veth7b12c96f) entered disabled state\nDec 10 01:39:14 kongapi-poc-db1 kernel: device veth7b12c96f left promiscuous mode\nDec 10 01:39:14 kongapi-poc-db1 kernel: cni0: port 1(veth7b12c96f) entered disabled state\nDec 10 01:39:14 kongapi-poc-db1 NetworkManager[702]: <info>  [1512898754.6477] device (veth7b12c96f): released from master device cni0\nDec 10 01:39:14 kongapi-poc-db1 kubelet: E1210 01:39:14.655974   58034 remote_runtime.go:92] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod \"tomcat-d6b5b9647-prq9w_tomcat\" network: \"cni0\" already has an IP address different from 10.244.3.1/24. ",
    "eroji": "Having the same problem. 4 nodes, 2 masters and 2 workers. the .167 and .168 are the workers and .167 is the one that's having issues adding the route.\nOutput of: kubectl get nodes -o yaml |grep flannel.alpha\nflannel.alpha.coreos.com/backend-data: '{\"VtepMAC\":\"d2:28:18:cd:1d:82\"}'\n      flannel.alpha.coreos.com/backend-type: vxlan\n      flannel.alpha.coreos.com/kube-subnet-manager: \"true\"\n      flannel.alpha.coreos.com/public-ip: 10.1.130.165\n      flannel.alpha.coreos.com/backend-data: '{\"VtepMAC\":\"b6:67:12:1c:d9:c4\"}'\n      flannel.alpha.coreos.com/backend-type: vxlan\n      flannel.alpha.coreos.com/kube-subnet-manager: \"true\"\n      flannel.alpha.coreos.com/public-ip: 10.1.130.166\n      flannel.alpha.coreos.com/backend-data: '{\"VtepMAC\":\"aa:e0:31:6e:d1:ef\"}'\n      flannel.alpha.coreos.com/backend-type: vxlan\n      flannel.alpha.coreos.com/kube-subnet-manager: \"true\"\n      flannel.alpha.coreos.com/public-ip: 10.1.130.167\n      flannel.alpha.coreos.com/backend-data: '{\"VtepMAC\":\"16:13:d5:7c:c5:e2\"}'\n      flannel.alpha.coreos.com/backend-type: vxlan\n      flannel.alpha.coreos.com/kube-subnet-manager: \"true\"\n      flannel.alpha.coreos.com/public-ip: 10.1.130.168. ",
    "BSWANG": "Are the invalid gateway addresses treated as multicast address by linux?\nThe subnet allocation in flannel will skip the multicast addresses https://github.com/coreos/flannel/blob/master/subnet/config.go#L86-L93.  But using the podCidr allocated by \"controller manager\" not skip the first subnet.\n@tomdee . Pass the ACCESS_KEY_ID and ACCESS_KEY_SECRET by  config it on the flannel pod template's env  section; . ",
    "nabheet": "\n@tomdee\nHi Tom,\nI initialised my cluster with same kubeadm command\nkubeadm init --pod-network-cidr 10.244.0.0/16\nBut Still in Flannel pods I see errors\nE1210 07:10:45.198903 1 vxlan_network.go:158] failed to add vxlanRoute (10.244.2.0/24 -> 10.244.2.0): invalid argument\nI have 4 host cluster 2 of them works fine but other 2 fails to schedule container\nAlways in state of \"ContainerCreating\"\nErrors which I see is\nDec 10 01:39:14 kongapi-poc-db1 kubelet: E1210 01:39:14.554032   58034 cni.go:250] Error while adding to cni network: \"cni0\" already has an IP address different from 10.244.3.1/24\nDec 10 01:39:14 kongapi-poc-db1 kernel: cni0: port 1(veth7b12c96f) entered disabled state\nDec 10 01:39:14 kongapi-poc-db1 kernel: device veth7b12c96f left promiscuous mode\nDec 10 01:39:14 kongapi-poc-db1 kernel: cni0: port 1(veth7b12c96f) entered disabled state\nDec 10 01:39:14 kongapi-poc-db1 NetworkManager[702]: <info>  [1512898754.6477] device (veth7b12c96f): released from master device cni0\nDec 10 01:39:14 kongapi-poc-db1 kubelet: E1210 01:39:14.655974   58034 remote_runtime.go:92] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod \"tomcat-d6b5b9647-prq9w_tomcat\" network: \"cni0\" already has an IP address different from 10.244.3.1/24\n\nI am not sure if this will help, but you might want to delete all the network/bridge devices before initializing k8s again. I had similar issues but I destroyed and created new VMs which resolved my similar issue. However, the issues might not be the same.\nAfter reading flannel documentation, it was not obvious to me that flannel works one cidr only. But after the change things are much better, although with other issues.. ",
    "jroggeman": "@tomdee This addresses your comment from #832 . @tomdee updated review to only be the minor refactoring, can you take a look?. Squashed changes are pushed :). @tomdee Made changes, should be good to go. vendor files were whitespace changes from gofmt; I can revert them.. Two important notes:\n\nWe didn't install the gonetsh using glide; glide get also updates packages, and one of the vendored deps it pulled managed to include a file that can't be checked out on Windows due to the filename :) Not sure how to resolve that sort of issue.\nThis PR shouldn't be merged until appveyor is setup for Windows-based CI.. Fixed the CI issues; I'm no super familiar with glide and go vendoring but I think that AppVeyor is failing since we haven't vendored rakesh's package via glide.\n\nThe file that was pulled in while vendoring with glide:\nfatal: cannot create directory at 'vendor/github.com/docker/distribution/contrib/docker-integration/generated_certs.d/localregistry:5440': Invalid argument\nThis does appear to be a folder on linux, and colons are invalid as folder names on windows.. @tomdee We're still having issues with properly getting our library vendored.  When we run just a glide install to fetch deps, it brings in a huge number of files and dependencies (that aren't checked in to git) into the vendor folder.  Are these supposed to be gitignored?  Are they supposed to be vendored (and thus checked in) but aren't?  We're pretty confused at the moment with the dependency management, so any light you could shed on how dependencies are being managed would be quite helpful :). Even this doesn't seem to be working too well; even after installing, glide vc doesn't work at all, just exits.  Appears that glide list fails with versioning issues, not able to find pinned versions for any packages, even without explicitly installing the new package we need. Effectively I can't seem to do the specified steps without changing or adding lots of files to the vendor directory. removed the above one to remain consistent with style. ",
    "osoriano": "@Dieken 10-flannel.conflist should be read by CNI here \nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/network/cni/cni.go#L99\nhttps://github.com/containernetworking/cni/blob/master/libcni/conf.go#L177\nIs there some error you are seeing?. @Dieken, thanks for pointing that out. The .conflist files are parsed differently since they contain a list of plugins for plugin chaining. \nIf both 10-flannel.conf and 10-flannel.conflist are both present, only 10-flannel.conf will be used (first file in the sorted list). You're right, the old file should be removed in an upgrade. Although, it won't break the upgrade either.\nI only tested the change with kube-flannel.yml, but it seems like others could be updated as well.. @squeed Can you help me understand where this happens in the code?\nI'm not that familiar with CNI, but saw that the conf files are first sorted, then we return on the first valid conf.\nAlso, from the documentation:\nIf there are multiple CNI configuration files in the directory, the first one in lexicographic order of file name is used [1]\n[1] https://kubernetes.io/docs/concepts/cluster-administration/network-plugins/. The main kube-flannel.yml was updated to include RBAC resources. Would that work for you?\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml. Sounds like there are two workarounds.\n1. Use the 0.9.1 kube-flannel.yml\n2. Update the CNI plugin installation on the host\nAFAIK we don't package CNI plugins into the flannel image. Maybe we should update the docs? Sorry for the breakage, the portmap plugin is used for hostPort support. ",
    "tiayi": "I have got the same issue. \nBecause the dns container cannot access the apiserver, other hosts, internet, any clusterIP , they just can access the host IP and other conainers (can crosss host).\nI tried to debug them.\n Container A(10.200.0.180) runs on host (172.21.1.123),  then ping another host(172..21.1.122),\ntcpdmp on eth0,    can see 10.200.0.180 > 172.21.1.122  , but no response.\nThere is another way  can fix your DNS crush ,but pod still cannot access outside and clusterIP.\nhttps://github.com/kubernetes/kubernetes/issues/40761\n. I find the real reason , I used an error config.  \n--service-cluster-ip-range (apiserver) ,  --service-cluster-ip-range (controller-manager)  and  the config of flannel configmap (net-conf.json: netowrk) should be same..\n. ",
    "cheyang": "Is it introduced by backend/vxlan: simplify vxlan processing?\nI'm wondering why we need change one routing rule (172.28.0.0/16 dev flannel.1, which is using subnet 16)into so many rules (172.28.2.0/24 via 172.28.2.0 dev flannel.1 onlink, which are using subnet 24). There will be more rules in the routing table. will it introduce performance overhead?. Thanks.. ",
    "vincentbernat": "With one routing rule to a directly attached interface, you'll generate L2MISS events that need to be handled. From my understanding, the goal was to remove any netlink events as the key/value store already told us all the info we need beforehand. Requests for unknown subnets won't trigger a lookup, a scan of the subnet won't trigger a lot of load on the key/value store and the latency of first packets will be lower. This new approach seems more robust.\nThe routing overhead is pretty low. See this benchmark. The /24 case is similar to the /32 case of the benchmark. The additional 5 ns you'll spent in the routing lookup is quite small compared to the time you spend in Netfilter. Moreover, you'll have less ARP entries, so less time spent in managing the hash table associated.. ",
    "indiketa": "The complete loss of connectivity of one node caused by a net restart is an enhancement?. You are right: Sorry. \nI opened a PR with a backend health checker proposal. \nIn my case: flannel iface disappears not as a cause of a \"network restart\".  It's more related with #877, my master device it's a VPN:  On a short VPN outage master interface disappears and flannel stops working, the only solution for me es restarting the flannel pod on that node manually.. Oh no! Many errors: i don't know what happened to vendors, and travis reports errors. I've to learn more.. I thought that if I remove the branch after doing the PR nothing happened. Sorry.. ",
    "rushins": "we hit the same error kubernetes 1.8.4. can some one provide the RBAC flannel file.\n. i found this location but this only creates role-bindings not t flannel pod or overlay network \nhttps://gist.github.com/joejulian/a5f308f73bfee7444810870313cec251\ncheck the URL and download 2 files , try  executing one by one see if this helps. i tried but my cluster FLANNEL PODS still craching .\nhttps://gist.github.com/joejulian/\n[7:52] \nhowever our flannel pod is crashing. ",
    "cehoffman": "It looks like the --ip-forward flag got dropped from the changes. It is mentioned in the docs and existed on the replaced #862 PR.. @tomdee makes sense and I think dropping the flag is better. We saw similar problems with what I wouldn't consider a large cluster yet, but one that had fairly frequent pod activity changing the service ip landscape.\nIn our investigation, we found there were a few core kubernetes services for our cluster that had bad pod mount configuration, kube-proxy and calico, and a general incompatibility between the host iptables command and the one inside the flannel container.\nWe run Container Linux as our host OS. In the most recent version, 1967.3.0, the host iptables is very outdated. It has version 1.4.21 released in 2013 with the first iteration of locking that used unix domain sockets 1. Later in 2015 this locking mechanism changed to the current flock using /run/xtables.lock 2. This means if you have any host based iptables configuration, it cannot work reliably together with the iptables in the flannel, calico, or hyperkube base images because they all use the more recent flock mechanism.\nOur main issue came down to missing mounts for /run/xtables.lock in our kube-proxy deployment. This was the deployment configuration that came from Tectonic standard. We updated that and have not seen the referenced error from flannel.. Looks like --ip-forward should be dropped from documentation here.. ",
    "jbrissier": "\nTo resolve it upgrade to the lateset version of flannel.\n\nYou mean v0.9.1 from quay.io/repository/coreos/flannel ?\nThe current kube-flannel.yml refers the version v.0.9.0. ",
    "Bengrunt": "Thanks, I got stuck by this because I pulled the kube-flannel file 2 days ago.\nBad luck I guess.. ",
    "jethrogb": "Similar to #869. 10-flannel.conf sorts before 10-flannel.conflist, so if you update you configuration with this change, this change will not be picked up since 10-flannel.conf still exists.. ",
    "gnodli": "there is my master node log, this problem has repeated many tims in my environment.\nmaster node log.txt\nbelow file is my install script and configure file\ninstall script and config file.zip\nthx. ",
    "bugmind": "@gnodli  I met the same issue after rebooting a node, how did you bring back the inet property at last?. ",
    "mohammadobaid1": "Fix this issue by modifying iptables in ubuntu servers . Seems like firewall blocking data packet to transfer from flannel interface to docker interface. ",
    "iver3on": "flanneld process is running ,but ifconfig no flannel0  devicehow to solve. ",
    "outcoldman": "ping @tomdee . @caseydavenport I have submitted PR against master https://github.com/coreos/flannel/pull/1045/files\nBut it will be good to have the same fix for the tag v0.10.0, considering that in a lot of places there is a reference to this path https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml\nConsidering that this is just a configuration change, maybe make a release v0.10.1 and update the Kubernetes documentation? \n. @hegdedarsh possible that it is a different problem, but I would suggest using a released version https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml, modify the tolerations and give it a try. . ",
    "d11wtq": "Thoughts: is it possible flannel attempts to allocate a single IP, which works, but flannel believes it failed, so it attempts to allocate the IP again, which results in the two addresses?. Update: I fixed this by just ip link delete flannel.1 on the affected nodes. I'm not sure how it started, but flannel was than able to recreate the interface and work.. ",
    "xukunfeng": "we see the same issue  with flannel 0.7.1.\nshell\nip a \n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:a2:ab:59 brd ff:ff:ff:ff:ff:ff\n    inet 10.10.14.187/23 brd 10.10.15.255 scope global eth0\n       valid_lft forever preferred_lft forever\n3: docker0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP\n    link/ether 02:42:ad:b4:c5:7f brd ff:ff:ff:ff:ff:ff\n    inet 172.30.79.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\n46: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN\n    link/ether 62:d6:6c:70:2a:94 brd ff:ff:ff:ff:ff:ff\n    inet 172.30.73.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet 172.30.79.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n[cloud@psd011 ~]$ etcdctl get /k8s/network/subnets/172.30.73.0-24\nError:  100: Key not found (/k8s/network/subnets/172.30.73.0-24) [1959]\n[cloud@psd011 ~]$ etcdctl get /k8s/network/subnets/172.30.79.0-24\n{\"PublicIP\":\"10.10.14.187\",\"BackendType\":\"vxlan\",\"BackendData\":{\"VtepMAC\":\"76:ef:58:64:0d:91\"}}\nYour Environment\nFlannel version: 0.7.1\nBackend used (e.g. vxlan or udp): vxlan\nEtcd version:gcr.io/google_containers/etcd-arm:3.0.17 (?)\nKubernetes version (if used): 1.9.3\nOperating System and version: Centos 7.3 \nLink to your project (optional):. ",
    "adarsh1001": "The same issue occurs with Flannel v0.10.0.\n14: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default \nlink/ether a6:62:08:a6:c2:f0 brd ff:ff:ff:ff:ff:ff\n inet 10.244.1.0/32 scope global flannel.1\n    valid_lft forever preferred_lft forever\n    inet 169.254.46.120/16 brd 169.254.255.255 scope global flannel.1\n    valid_lft forever preferred_lft forever\n    inet6 fe80::a462:8ff:fea6:c2f0/64 scope link \n    valid_lft forever preferred_lft forever\nFlannel goes into CrashLoopBackOff. Deleting the flannel.1 link does solve the issue.\nOS: Raspbian Stretch Lite\nKubernetes version: 1.10.2. ",
    "wroney688": "Same under K8s 1.12.0, flannel v0.10.0; however, sudo ip link delete flannel.1 did allow it to come up without error.  (Hypriot 1.9.0 on ARM - Raspberry Pi 3 B+). ",
    "ljfranklin": "+1, same setup as @wroney688:\n-k8s 1.12.0\n- flannel v0.10.0\n- Hypriot 1.9.0 on ARM - Raspberry Pi 3 B+\nIn my case all the flannel pods initially come up successfully but after ~3 days the flannel pod gets stuck in CrashLoopBackOff with the following error (other 4 workers are fine):\nkubectl -n kube-system logs kube-flannel-ds-arm-fcm6r\nI1019 03:16:15.097562       1 main.go:475] Determining IP address of default interface\nI1019 03:16:15.099403       1 main.go:488] Using interface with name eth0 and address 192.168.1.246\nI1019 03:16:15.099582       1 main.go:505] Defaulting external address to interface address (192.168.1.246)\nI1019 03:16:15.734392       1 kube.go:131] Waiting 10m0s for node controller to sync\nI1019 03:16:15.794863       1 kube.go:294] Starting kube subnet manager\nI1019 03:16:16.995022       1 kube.go:138] Node controller sync successful\nI1019 03:16:16.995104       1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - k8s-worker4\nI1019 03:16:16.995131       1 main.go:238] Installing signal handlers\nI1019 03:16:16.995337       1 main.go:353] Found network config - Backend type: vxlan\nI1019 03:16:16.995492       1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false\nE1019 03:16:16.996653       1 main.go:280] Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. &netlink.Vxlan{LinkAttrs:netlink.LinkAttrs{Index:16, MTU:1450, TxQLen:0, Name:\"flannel.1\", HardwareAddr:net.HardwareAddr{0x96, 0x73, 0x59, 0x4a, 0xa2, 0xd6}, Flags:0x13, RawFlags:0x11043, ParentIndex:0, MasterIndex:0, Namespace:interface {}(nil), Alias:\"\", Statistics:(*netlink.LinkStatistics)(0x13f340e4), Promisc:0, Xdp:(*netlink.LinkXdp)(0x14027100), EncapType:\"ether\", Protinfo:(*netlink.Protinfo)(nil), OperState:0x0}, VxlanId:1, VtepDevIndex:2, SrcAddr:net.IP{0xc0, 0xa8, 0x1, 0xf6}, Group:net.IP(nil), TTL:0, TOS:0, Learning:false, Proxy:false, RSC:false, L2miss:false, L3miss:false, UDPCSum:true, NoAge:false, GBP:false, Age:300, Limit:0, Port:8472, PortLow:0, PortHigh:0}\nI1019 03:16:16.996768       1 main.go:333] Stopping shutdownHandler...\nHere's the interfaces on the worker with the failing flannel pod:\n$ ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether b8:27:eb:fa:0d:e4 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.246/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::a775:2ad:bcca:1d44/64 scope link\n       valid_lft forever preferred_lft forever\n3: wlan0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000\n    link/ether b8:27:eb:af:58:b1 brd ff:ff:ff:ff:ff:ff\n4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default\n    link/ether 02:42:fc:53:bc:ad brd ff:ff:ff:ff:ff:ff\n    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n       valid_lft forever preferred_lft forever\n16: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default\n    link/ether 96:73:59:4a:a2:d6 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.2.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet 169.254.47.220/16 brd 169.254.255.255 scope global flannel.1\n       valid_lft forever preferred_lft forever\n    inet6 fe80::697a:866d:c96e:5849/64 scope link\n       valid_lft forever preferred_lft forever\nAs with others, running sudo ip link delete flannel.1 resolves it at least temporarily.\nCan we re-open this issue? Any logs I can grab next time to help debug?. ",
    "Gacko": "Done. I did not change the legacy init containers in some manifests since it would make them hardly readable.. I removed the commit changing the version of the image from the aliyun registry.. I don't know. I thought you already did so. I'll revert this change.. ",
    "zcalusic": "I ended up with both 10-flannel.conf & 10-flannel.conflist with the exactly same contents, and lots of kubelet[29256]: W1227 18:53:27.409233   29256 cni.go:125] Error loading CNI config file /etc/cni/net.d/10-flannel.conf: no 'type'; perhaps this is a .conflist? log spam in journal. Every few seconds... :(\nAdmittedly, this was applied to 0.9.1 image so maybe some bits & pieces are still missing, possibly will be available in 0.9.2?\nFor now, I deleted 10-flannel.conf, it fixes the extreme log spam issue. Hopefully everything continues working ok.... ",
    "TLmaK0": "thanks @tomdee, you are right.. ",
    "magic7s": "I have this issue as well. \nOS: Ubuntu 16.04\nroot@ip-10-0-0-10:~# dpkg-query -L kubernetes-cni\n/.\n/opt\n/opt/cni\n/opt/cni/bin\n/opt/cni/bin/dhcp\n/opt/cni/bin/host-local\n/opt/cni/bin/bridge\n/opt/cni/bin/tuning\n/opt/cni/bin/macvlan\n/opt/cni/bin/flannel\n/opt/cni/bin/cnitool\n/opt/cni/bin/ptp\n/opt/cni/bin/loopback\n/opt/cni/bin/ipvlan\n/opt/cni/bin/noop\nFixed by downloading portmap to /opt/cni/bin\nhttps://github.com/projectcalico/cni-plugin/releases/download/v1.9.1/portmap. ",
    "cmoscardi": "+1, just ran into this. ",
    "fengyd2018": "The 0.9.1 kube-flannel.yml is used, but hostPort still cannot work.\nAny extra work is needed?\nEnviroment:\nLinux master 3.10.0-693.el7.x86_64 #1 SMP Tue Aug 22 21:09:27 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nCentOS Linux release 7.4.1708 (Core)\n[root@master ~]# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.2\", GitCommit:\"5fa2db2bd46ac79e5e00a4e6ed24191080aa463b\", GitTreeState:\"clean\", BuildDate:\"2018-01-18T10:09:24Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"9\", GitVersion:\"v1.9.3\", GitCommit:\"d2835416544f298c919e2ead3be3d0864b52323b\", GitTreeState:\"clean\", BuildDate:\"2018-02-07T11:55:20Z\", GoVersion:\"go1.9.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}. Thanks for your answer.\nI tried with portmappign enabled, but hostPort cannot work.\nhttps://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml \nI just reset the kubernetes cluster and install it again, and hostPort can work now.\n. I think the kubernetes docs should be updated\nhttps://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/\nIn (3/4) Installing a pod network, the flannel version is still v0.9.1.\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml. ",
    "wangjunwei87": "I encounter the same problem, any updates?. Download cni plugins binaries and put it in /opt/cni/bin solve my problem.. ",
    "aronica": "\nDownload cni plugins binaries and put it in /opt/cni/bin solve my problem.\n\nThis works for me with kubelet version v1.8.1. ",
    "eranreshef": "I have a question that might be a bit silly but I hope you'll forgive since I'm new in this:\nIf I deployed the latest flanneld image in my k8s cluster, is there something special I need to do in order to \"activate\" wireguard? If not, how can I tell if its running?\nThanks.. Well I'm not 100% know what I'm doing but I don't mind experimenting with this.\nI deployed the latest flannel image and installed the wireguard module on the host. When the flanneld is starting it prints the following lines and exit:\n```\nI0116 14:30:01.902096       1 main.go:353] Found network config - Backend type: extension-wireguard\nE0116 14:30:01.902115       1 main.go:272] Error fetching backend: unknown backend type: extension-wireguard\n```\nwhats wrong here?. Yes it does. Thanks!. some more info:\nAfter a deeper analysis, it seems that in v0.11.0 packets are coming out from the host with the caliXXX or flannel-wg interface ip address instead of the actual requesting pod's ip address (as in v0.10.0).\nIs this behaviour expected?. https://github.com/projectcalico/calico/issues/2400. ",
    "akshaymankar": "\nThe assumption is that flannel running with --kube-subnet-mgr is running under a kubelet.\n\nI guess I am looking for a reason for this assumption?\nI am asking because it forces me to start kubelet on my master VM, when all I want is an interface and a route into my container network from master. It seems theoretically possible to achieve that. Which brings us to the second question, would you be open to supporting this use case?. ",
    "Cougar": "It is the new and normal behavior. flanneld monitors etcd2 and inserts/removes node networks as they come and go. It is implemented in 5d3d6642 and described here. ",
    "w564791": "What should I do, T_T. the problem happened again\n```\nDec 18 01:31:46 ip-10-10-6-90.cn-north-1.compute.internal systemd[1]: Started Flanneld overlay address etcd agent.\nDec 19 09:31:47 ip-10-10-6-90.cn-north-1.compute.internal flanneld-start[1096]: E1219 09:31:47.078227    1096 main.go:398] Lease has been revoked. Shutting down daemon.\nDec 19 09:31:47 ip-10-10-6-90.cn-north-1.compute.internal flanneld-start[1096]: I1219 09:31:47.078257    1096 main.go:321] Waiting for all goroutines to exit\nDec 19 09:31:47 ip-10-10-6-90.cn-north-1.compute.internal flanneld-start[1096]: I1219 09:31:47.078317    1096 main.go:332] Stopping shutdownHandler...\nDec 19 09:31:47 ip-10-10-6-90.cn-north-1.compute.internal flanneld-start[1096]: I1219 09:31:47.078395    1096 main.go:324] Exiting cleanly...\nDec 19 09:33:05 ip-10-10-6-90.cn-north-1.compute.internal systemd[1]: Starting Flanneld overlay address etcd agent...\n```. Thanks Reply ! I checked all the server's clock, there is not much time between them.and no time to change when this problem occurs.This project is my personal maintenance, so I confirm that no one removed data from etcd.. ",
    "deepakprabhakara": "I am seeing this issue on our production sytem. I can confirm that no data is being removed from etcd. Only flannel on 1 node shuts down though, rest are fine. A restart of flannel and it continues working fine.. ",
    "hjydxy": "Same trouble, kubernetes 1.10  flannel 0.10.0\nI notice that there is no information under  /coreos.com/network/subnets/.\nThe subnets information is gone.\n. Do you solve it? @timchenxiaoyu . Do you see \"time difference is too high\" or sth in etcd  log. \nMaybe my trouble is caused by this . \nDo you run ntp or chrony on your cluster? @timchenxiaoyu . ",
    "xilu0": "update. You can't comment at this time \u2014 your comment cannot be blank. ",
    "dawidmalina": "@tomdee when v0.10.0 will be released?. ",
    "ptylenda": "FYI I am sharing my Ansible playbooks for deploying windows/ubuntu cluster which has flannel with host-gw backend. Someone may find this useful I guess, unfortunately I am not able to focus on developing thexe in next weeks: https://github.com/ptylenda/kubernetes-for-windows. ",
    "rocky0001": "got the error: HNS failed with error : An adapter was not found.\nI0711 13:57:07.141605    4872 main.go:422] Searching for interface using 10.126.79.218\n2018-07-11 13:57:07.182620 I | Interfaces : [{0 vEthernet (Ethernet 2) 25 true 10.126.79.218 24 0 10.126.79.1} {0 Loopback Pseudo-Interface 1 75 false 127.0.0.1 8 0 } {0 vEthernet (nat) 5000 true 172.29.160.1 20 0 }]\n2018-07-11 13:57:07.219603 I | Found Interface for 10.126.79.218 => {15 vEthernet (Ethernet 2) 25 true 10.126.79.218 24 0 10.126.79.1}\nI0711 13:57:07.221603    4872 main.go:499] Using interface with name vEthernet (Ethernet 2) and address 10.126.79.218\nI0711 13:57:07.234575    4872 main.go:516] Defaulting external address to interface address (10.126.79.218)\nI0711 13:57:07.237603    4872 kube.go:131] Waiting 10m0s for node controller to sync\nI0711 13:57:07.238604    4872 kube.go:294] Starting kube subnet manager\nI0711 13:57:08.239596    4872 kube.go:138] Node controller sync successful\nI0711 13:57:08.241573    4872 main.go:240] Created subnet manager: Kubernetes Subnet Manager - ip-10-126-79-218.us-west-2.compute.internal\nI0711 13:57:08.241573    4872 main.go:243] Installing signal handlers\nI0711 13:57:08.241573    4872 main.go:358] Found network config - Backend type: host-gw\nI0711 13:57:08.257572    4872 hostgw_windows.go:151] Attempting to create HNS network, request: {\"DNSServerList\":\"\",\"Name\":\"cbr0\",\"Subnets\":[{\"AddressPrefix\":\"192.168.3.0/24\",\"GatewayAddress\":\"192.168.3.1\"}],\"Type\":\"l2bridge\"}\nE0711 13:57:08.317554    4872 main.go:285] Error registering network: unable to create network [cbr0], error: HNS failed with error : An adapter was not found.\nI0711 13:57:08.320573    4872 main.go:338] Stopping shutdownHandler.... ",
    "pablodav": "What's the status of this possible support? would be great to have support for flannel plugin on windows.\nSame for #922 \n. Thaks for the update @daschott !  Sounds good to have some news and clear information where it is going!. got into this issue when trying to use nginx-ingress on-premises.\n. News: \nLooks like I was missing: \n\ncurl https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml \\\n    | kubectl apply -f -\n\nKubespray doesn't have option to deploy service-nodeport for nginx-ingress, so that should be done  manually. Now tested using calico but looks like I'm could try again with flannel.. I have modified some ansible playbook created and testing on:\nhttps://github.com/pablodav/kubernetes-for-windows\nhttps://github.com/pablodav/kubernetes-for-windows-quickstart\nIt is working for me\n. Yes the kubernetes-for-windows project commented above uses some patches version of flannel, patched by @ptylenda (Main author of kubernetes-for-windows).\nIt is working stable since 3 weeks by now with some test service (nothing in production yet, but approving now to go to next stage and to production in next months).\n\n. ",
    "daschott": "@pablodav Flannel in host-gw mode works today on Windows with pre-built binaries that include the open PR's, but it's not an ideal situation. Right now we are targeting this PR and working together with the maintainers to try and get basic Windows CNI plugins in which can be used by Flannel. Afterwards we will cleanup these plugins and work on tests with @madhanrm and @dineshgovindasamy as maintainers (this is a Windows only plugin, so it shouldn't break any existing CNI's for Linux). VxLAN is still under active development, including from platform perspective. . Indeed, this issue is reproducible on 1803. With the same configuration, it seems to work for 1809. We are going to investigate what the diff is and post back here.. ping @rajatchopra PTAL. @bclau this should be resolved new on newest win-overlay, can you try it out and confirm?. ",
    "astrieanna": "I have a flannel/hostgw cluster on vsphere, with a linux node and a windows node. The one thing that doesn't work is that the linux node (the host, not a container) can't connect to windows service clusterips. This breaks accessing windows services through the NodePort on the linux node.\nDoes accessing a windows service via a nodeport on a linux node work for you with this PR?. ",
    "dineshgovindasamy": "Lets abandon this PR. We are taking PR#1042 which is merged PR.\n@rakelkar , can u abandon this PR?. Lets abandon this PR. We are taking PR#1042 which is merged PR.\nRakesh, can u abandon this PR?. tagging @rakelkar . @benmoss yes we are working with @rajatchopra @tomdee  to merge this PR and there are some followup fixes we need to make to make this code work. Kalya can generate them as soon as this PR is merged.. Will fix this in a subsequent PR. . we will move it in the next PR.. ",
    "anikundesu": "@rakelkar\nThe reason why Travis CI build failed is lack of \"EoF\" at the last of backend/vxlan/vxlan_network_windows.go and backend/vxlan/vxlan_network_windows_test.go as linux-based text files.\nTo fix the failure, these two files should be opened on the linux-based system, add one line at the last of the file, and delete the last line. I used vi to do it and successfully executed \"make test\" on a linux-based system.\n\nTakashi Kanai\nMicrosoft MVP of Cloud and Datacenter Management. @rakelkar The fix I pointed out has already been commited by @madhanrm at dd0cd9e. I am awaiting for the release of PR in Microsoft/SDN. This PR will become the answer.\nhttps://github.com/Microsoft/SDN/pull/189. ",
    "ksubrmnn": "@ksubrmnn. @rajatchopra will update after validating. @rajatchopra LGTM. @rajatchopra Testing and documentation will probably take about 2 weeks. I think it's fine to merge this PR and follow up with testing/docs in a separate PR, if that's okay with you. This has been manually validated.. @rajatchopra let me do one final run in the morning for peace of mind, then we can merge. \nOnly upcoming changes I can foresee are removing hardcoding of VNI (necessary for now until I can add a small Linux change) and testing. Both of those will be separate PRs from this though.. @rajatchopra this looks good to merge.. @madhanrm . @rajatchopra this looks fine to me. I'm still looking at PR 1068, but this can be merged in the meantime. ping @madhanrm . @madhanrm . Do not merge this yet. Just creating this PR for now, I haven't super strictly validated it yet. @madhanrm @rajatchopra  this is ready to be reviewed and merged. @rajatchopra . Hi Ben, \nIn Overlay, the vxlan0 network is created by FlannelD  here\nHowever, you need to create a separate Overlay Network for vxlan0 to be layered on top of as is done here\nNote that this is also done for L2Bridge (this is so that the host won't lose connectivity as the cbr0 network is created/torn down). For L2Bridge, Flannel will still work properly without that External network.\nThe reason that you need the External for Overlay is because in Windows, the DR NIC is created when an Overlay network is present. This NIC's MAC address is needed in the subnet lease attributes. So we create an Overlay network called \"External\" to get that DR MAC. Then when we have the subnet lease, we can create the vxlan0 network.\nI agree with you that it is preferable to not have to depend on that external network.\nCan you edit this issue to reflect what I've explained above? Regarding the actual backend network, both implementations will either reuse an existing network or create a new one. The only difference is that Overlay has a hard dependency on that external network.\nIn the meantime, I'll look into changing this and will update this issue soon.. @madhanrm @daschott FYI. This should be len(newNetwork.ManagementIP) != 0 so that the loop ends. This should be len(newNetwork.ManagementIP) != 0 so that the loop ends. Address prefix should be lease.Subnet so that the network has the /24 subnet. . Should be:\nlastErr = existingBridgeEndpoint.HostAttach(1). This is not true. expectedBridgeEndpoint.Create() returns the result of the HNSEndpoint created by hcsshim which is then stored in existingBridgeEndpoint. However, expectedBridgeEndpoint remains the same object as it is initialized in line 141 (without an ID).\nexistingBridgeEndpoint is an HNSEndpoint with the ID of the endpoint that is created. You need that ID for HNS to do HostAttach. As this code is, it fails with an HNS error, but with the suggested fix, it attaches the endpoint properly.\nEdit: existingBridgeEndpoint is actually set to an endpoint without Id in line 206. existingBridgeEndpoint is now null since it is not in line 213. Previously there was a remote endpoint for each IP in the subnet on the host. For example if Host A has subnet 192168.1.0/24 and Host B has 192.168.2.0/24, then Host A had 250+ remote endpoints from 192.168.2.1-254. Vice versa for Host B\nThis was before HNS had the remote subnet feature. And before Remote Subnet, many things were not working properly like service vip, node port access, etc. With remote subnet feature, this RemoteSubnetPolicy replaces the tens/hundreds of endpoints you previously had to create per other node in the cluster. \nWith the previous remote endpoint strategy, it worked because flannel sets the mac by using a similar approach with mac prefix and then setting the mac based on IP. So we just used conjureMac in flanneld to replicate that logic. So it was a real mac, that we sort of figured out from the IP.. Can you add a nil check for newNetwork.ManagementIP here and see if that resolves the error? This may be a better solution. Also try increasing the timeout to 2 minutes with this change. Updated my previous response after consulting with Dinesh when I realized I misunderstood your original question. @song-jiang There is a known issue in RS4 that the ManagementIP may not be populated until the host receives an IP. This does not occur in RS5 onwards. Can you try delaying starting flannel until the host receives an IP? I think flannel may be starting too soon. Is it possible to get access to your set up? This issue should not happen and we have not been able to repro it.. Added below. Typo: manually. Do you mean flanneld.exe?. ",
    "drake7707": "The annotation is deprecated as the entire rescheduler will be removed starting from v1.12+. Flagging pods as critical is now done by applying a priorityclass, which is supported for out of resource eviction since v1.9, see the documentation over at https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n(In v1.11 at least) there are 2 priority classes set up by default:\n$ kubectl get priorityclass\nNAME                      CREATED AT\nsystem-cluster-critical   2018-07-04T10:05:08Z\nsystem-node-critical      2018-07-04T10:05:08Z\nWhich are defined as follows:\n$ kubectl describe priorityclass system-node-critical\nName:           system-node-critical\nValue:          2000001000\nGlobalDefault:  false\nDescription:    Used for system critical pods that must not be moved from their current node.\nAnnotations:    <none>\nEvents:         <none>\n$ kubectl describe priorityclass system-cluster-critical\nName:           system-cluster-critical\nValue:          2000000000\nGlobalDefault:  false\nDescription:    Used for system critical pods that must run in the cluster, but can be moved to another node if necessary.\nAnnotations:    <none>\nEvents:         <none>\nIt should suffice to add priorityClassName: system-node-critical in the pod spec to flag it as critical.\n. I've tested this on a v1.11 cluster with the master node being amd64 and a raspberry pi (being arm) and it works without a problem (as soon as I had done the same strategy for kube-proxy, which is required for actually setting up the network interfaces).\nOnly issue I encountered is that it refused to deploy at first because of DiskPressure (4gb sd card with a default raspbian install doesn't leave much free disk space), which can be solved by setting the critical priority class as per https://github.com/coreos/flannel/issues/925\n\n. ",
    "dlipovetsky": "\nI believe setting the priority class on kubernetes versions prior to 1.11 without setting --feature-gates=PodPriority=true in the kubelet will lead to a crash.\n\nI just patched the v0.10.0 kube-flannel.yml manifest, adding priorityClassName: system-node-critical to Pod spec nested in the DaemonSet spec. I then applied the manifest to a v1.10.4 cluster that does not  have the  PodPriority feature enabled.\nThe manifest was successfully applied and flannel is running as expected. The API server did remove the priorityClassName field from the Pod spec, but I think that is expected.\nThis leads me to conclude that adding the priorityClassname field to flannel Pod spec will not break backward compatibility with v1.10.x clusters, though testing against other versions may be warranted.. ",
    "Alexhha": "Any advice?. ",
    "jscottsf": "Would also like some advice on this.\nThere seems to be a lot of conflicting info on the web on how to exactly setup containerized Flannel via kubeadm. Mainly, do you need to further edit the docker.service to use any generated env vars (which would not be available until you first run Docker to get Flannel running in a container). Chicken and egg problem?\nLastly, I'm not getting DNS resolution working from within containers. Completely tangental to this issue.\n. ",
    "fntlnz": "Related to: https://github.com/kubernetes/kubernetes/issues/70202. ",
    "DiamondYuan": "close #927 . ",
    "rchernobelskiy": "I hit a similar issue - I had originally deployed flannel 0.9 which did not support portmap plugin and then removed it and added flannel 0.10 which does include portmap plugin.\nThe problem was that 10-flannel.conf from flannel 0.9 remained in /etc/cni/net.d\nOnce I removed it and restarted nodes, HostPort started working (it may have been enough to just restart kubelet though).\nPerhaps rm -f should be included in flannel 0.10 for other folks upgrading. ",
    "NickMRamirez": "@smorrisfv  Did you find a work-around for this? I am also seeing hostPort not creating a port mapping to the host when I use it in a Daemonset.\nI'm using https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml to install Flannel, which uses quay.io/coreos/flannel:v0.10.0-amd64. I shouldn't be running into the problem of having old 0.9 config files left over because this is on a fresh install.. ",
    "smorrisfv": "@NickMRamirez I never did find a workaround.  I worked at it for a week or so then moved on to other pressing tasks and haven't come back.  It is one of the last things blocking my k8s deployment.  It would just not create the port on the host, I should look to see what version I'm running, but I think it was 0.1.. ",
    "ut0mt8": "Hey exactly what I need. I should have read the code :) \nI think we can close this one. Thanks you.. ",
    "SleepyBrett": "I don't expect i need a test to test that time.Duration(int)*time.Second is functioning properly ;). ",
    "wujiandong": "+1. ",
    "wangyaliyali": "how to resolve this problem?. ",
    "KevinTHU": "it is a kernel problem, reference https://bugzilla.redhat.com/show_bug.cgi?id=1445054, and some comment say \n\nReproduced on 3.10.0-632.el7 kernel using script in comment 1. Adding ip6_disable=1 to kernel cmdline made vxlan not working.\nVerified on 3.10.0-668.el7 kernel. The vxlan works with or without ip6_disable=1 on kernel cmdline.. \n",
    "PaulFurtado": "@qrpike  This fix was merged in https://github.com/containernetworking/plugins/pull/62\nAnd it's in the v0.7.0 release: https://github.com/containernetworking/plugins/compare/v0.6.0...v0.7.0\n@KevinTHU That looks like a different bug than the CNI errors shown above. ",
    "KashifSaadat": "Any thoughts on this?. Fixed in Release v0.11.0\nSet FLANNELD_IPTABLES_FORWARD_RULES to false for those default ACCEPT rules to no longer be appended to the iptables FORWARD chain.. CC @caseydavenport @tomdee . ",
    "ydp": "@ImmanuelJDaniel  Did you solve this?. ",
    "audig": "Same issues, one pod can start, but 2 others failed. The node is never the same. ",
    "melvynpan": "Same issues\uff0canyone help?. ",
    "suylem": "any update about this issue ?  i also meet this problem.\nOS:  windows server 2016 core 1709 and window server 2016 version 1607\nflannel version:  v0.10.0. i  continued this error ,  after install English language on my server and set as default language. wait for docs     +1. i has try it, but flannel start failed\nerror \"Error fetching backend: unknown backend type: vxlan\"\nwhen run start.ps1 , the script while always \"Waiting for the Network to be created\".  i have try add etcd params  and change etcd network setting to host-gw , but this error no any change \ndetail flannel start log:\nPS C:\\flannel> [Environment]::SetEnvironmentVariable(\"NODE_NAME\", (hostname).ToLower())\nPS C:\\flannel> C:\\flannel\\flanneld.exe -iface=\"192.168.3.49\" --ip-masq=1 --kube-subnet-mgr=1 --kubeconfig-file=c:\\k\\kubelet.kubeconfig\nI0312 00:42:57.291499     684 main.go:417] Searching for interface using 192.168.3.49\nI0312 00:42:57.436510     684 main.go:488] Using interface with name vEthernet (Ethernet) and address 192.168.3.49\nI0312 00:42:57.437505     684 main.go:505] Defaulting external address to interface address (192.168.3.49)\nI0312 00:42:57.478521     684 kube.go:131] Waiting 10m0s for node controller to sync\nI0312 00:42:57.478521     684 kube.go:294] Starting kube subnet manager\nI0312 00:42:58.479671     684 kube.go:138] Node controller sync successful\nI0312 00:42:58.479671     684 main.go:235] Created subnet manager: Kubernetes Subnet Manager - t-shhq-docker-2\nI0312 00:42:58.483746     684 main.go:238] Installing signal handlers\nI0312 00:42:58.484704     684 main.go:353] Found network config - Backend type: vxlan\nE0312 00:42:58.485650     684 main.go:272] Error fetching backend: unknown backend type: vxlan\nI0312 00:42:58.486655     684 main.go:333] Stopping shutdownHandler.... @funky81 \nDo you have any windows image can replace this Linux image \"flannel:v0.10.0-amd64\" ?\nIn DaemonSet mode, Linux node can run kube-flannel pod, but the windows node was failed. the error is 'image operating system \"linux\" cannot be used on this platform'\n. ",
    "devenfan": "My windows 10 has been changed to English as below, but the error is still there with a small difference:\n\n\nflanneld  -etcd-endpoints \"http://127.0.0.1:2379\"  -etcd-prefix \"/test/network\"  -iface \"10.5.225.34\"\nI0225 17:44:29.893162    4932 main.go:417] Searching for interface using 10.5.225.34\nI0225 17:44:30.736004    4932 main.go:201] Could not find valid interface matching 10.5.225.34: error looking up interface 10.5.225.34: route ip+net: no such network interface\nE0225 17:44:30.736004    4932 main.go:225] Failed to find interface to use that matches the interfaces and/or regexes provided\n\nAs you can see above:\nerror looking up interface 10.5.225.34: no index found for interface\nchanged to:\nerror looking up interface 10.5.225.34: route ip+net: no such network interface\n. @guhuajun You have only one network interface right? . @guhuajun I think you may want to deploy Kubernetes Slaves on windows machines, and I suggest you use CentOS 7 instead.\nAnd my problem is, how to connect docker network on different Windows machines automatically.... ",
    "guhuajun": "Greetings,\nI have installed a new Windows Server 1709. Flanneld can find my interface.\n\n. @devenfan Yes, I do.\nhttps://kubernetes.io/docs/getting-started-guides/windows/ mentions that Flannel and Weavenet are not yet supported. Maybe we should give up at this moment?. ",
    "funky81": "Hi all, any guidelines to develop flannel in Windows node? Thanks . anyone? please help.... Finally Microsoft release flannel support. Check it here. \nBut apparently still in alpha / beta stage. I try to modify few things in my github\nAnyone have try it yet?. Have you install kube-flannel in your Linux Master Node (\nhttps://raw.githubusercontent.com/Microsoft/SDN/master/Kubernetes/flannel/overlay/kube-flannel-vxlan.yaml\n).\nYou should getting new messages \ud83d\ude0e\nOn Mon, Mar 12, 2018 at 2:54 PM, suylem notifications@github.com wrote:\n\ni has try it, but flannel start failed\nerror \"Error fetching backend: unknown backend type: vxlan\"\nwhen run start.ps1 , the script while always \"Waiting for the Network to\nbe created\". i have try add etcd params and change etcd network setting to\nhost-gw , but this error no any change\ndetail flannel start log:\nPS C:\\flannel> [Environment]::SetEnvironmentVariable(\"NODE_NAME\",\n(hostname).ToLower())\nPS C:\\flannel> C:\\flannel\\flanneld.exe -iface=\"192.168.3.49\" --ip-masq=1\n--kube-subnet-mgr=1 --kubeconfig-file=c:\\k\\kubelet.kubeconfig\nI0312 00:42:57.291499 684 main.go:417] Searching for interface using\n192.168.3.49\nI0312 00:42:57.436510 684 main.go:488] Using interface with name vEthernet\n(Ethernet) and address 10.110.3.49\nI0312 00:42:57.437505 684 main.go:505] Defaulting external address to\ninterface address (10.110.3.49)\nI0312 00:42:57.478521 684 kube.go:131] Waiting 10m0s for node controller\nto sync\nI0312 00:42:57.478521 684 kube.go:294] Starting kube subnet manager\nI0312 00:42:58.479671 684 kube.go:138] Node controller sync successful\nI0312 00:42:58.479671 684 main.go:235] Created subnet manager: Kubernetes\nSubnet Manager - t-shhq-docker-2\nI0312 00:42:58.483746 684 main.go:238] Installing signal handlers\nI0312 00:42:58.484704 684 main.go:353] Found network config - Backend\ntype: vxlan\nE0312 00:42:58.485650 684 main.go:272] Error fetching backend: unknown\nbackend type: vxlan\nI0312 00:42:58.486655 684 main.go:333] Stopping shutdownHandler...\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/957#issuecomment-372220651, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAGk7IhkOTdopys81CKI-aKyqI-zBXKXks5tdinJgaJpZM4SUr15\n.\n\n\n-- \nhttp://hobby.keluargareski.net\nSent from Sony VAIO notebook\n. ",
    "laoshancun": "\nHi all, any guidelines to develop flannel in Windows node? Thanks\n\nsame issue on 1803,any updates?. ",
    "kinglionsoft": "Same issue on Windows Server 2019.. @ekochen You save my many many days. Thank you very much.. same issue.. ",
    "ekochen": "solve this issue on my env by type chcp 65001  to ues utf8 ,then re run the flannel start command. encounter this \nwaiting for the Network to be created\nWaiting for the Network to be created\nI1127 10:24:28.829686 3668 kube.go:133] Node controller sync successful\nI1127 10:24:28.829686 3668 main.go:244] Created subnet manager: Kubernetes Subnet Manager - windows-kube-139\nI1127 10:24:28.832687 3668 main.go:247] Installing signal handlers\nI1127 10:24:28.832687 3668 main.go:386] Found network config - Backend type: host-gw\nI1127 10:24:28.833674 3668 hostgw_windows.go:73] HOST-GW config: {Name:cbr0 DNSServerList:}\nI1127 10:24:28.853822 3668 hostgw_windows.go:157] Attempting to create HNSNetwork {\"Name\":\"cbr0\",\"Type\":\"L2Bridge\",\"Subnets\":[{\"AddressPrefix\":\"10.20.4.0/24\",\"GatewayAddress\":\"10.20.4.1\"}]}\nI1127 10:24:28.859820 3668 hostgw_windows.go:164] Waiting to get ManagementIP from HNSNetwork cbr0\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xc0000005 code=0x0 addr=0x18 pc=0x10e43ea]\ngoroutine 1 [running]:\nmain.main()\n/home/kasubra/repo/gopath/src/github.com/coreos/flannel/main.go:297 +0xbfa\ni see this bug https://github.com/coreos/flannel/issues/1066\ndo you also have this issue?. aiting for the Network to be created\nWaiting for the Network to be created\nI1127 10:24:28.829686    3668 kube.go:133] Node controller sync successful\nI1127 10:24:28.829686    3668 main.go:244] Created subnet manager: Kubernetes Subnet Manager - windows-kube-139\nI1127 10:24:28.832687    3668 main.go:247] Installing signal handlers\nI1127 10:24:28.832687    3668 main.go:386] Found network config - Backend type: host-gw\nI1127 10:24:28.833674    3668 hostgw_windows.go:73] HOST-GW config: {Name:cbr0 DNSServerList:}\nI1127 10:24:28.853822    3668 hostgw_windows.go:157] Attempting to create HNSNetwork {\"Name\":\"cbr0\",\"Type\":\"L2Bridge\",\"Subnets\":[{\"AddressPrefix\":\"10.20.4.0/24\",\"GatewayAddress\":\"10.20.4.1\"}]}\nI1127 10:24:28.859820    3668 hostgw_windows.go:164] Waiting to get ManagementIP from HNSNetwork cbr0\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xc0000005 code=0x0 addr=0x18 pc=0x10e43ea]\ngoroutine 1 [running]:\nmain.main()\n        /home/kasubra/repo/gopath/src/github.com/coreos/flannel/main.go:297 +0xbfa\nalso encounter this issue , do we have workaround for this ?. ",
    "vincentmli": "I found the problem\nFeb 06 12:50:13 fed-master dockerd-current[2187]: cp: can't create '/etc/cni/net.d/10-flannel.conflist': Permission denied\nhttps://github.com/coreos/flannel/issues/709 resolved the issue, sorry for the noise\n. I found https://github.com/kubernetes/kubernetes/issues/51881, it mentions setup CNI plugin for each node, if I use kube-flannel, do I need to perform manual CNI plugin installation for each node?. after I installed the CNI plugin on the node as kubernetes/kubernetes#51881, the pod get ip from podCIDR now. \ncluster/kubectl.sh get po -o wide\nNAME          READY     STATUS    RESTARTS   AGE       IP           NODE\nnginx-c7rld   1/1       Running   0          11s       10.244.0.3   10.169.72.93\nnginx-wvbw9   1/1       Running   0          11s       10.244.0.4   10.169.72.93\n2: ens192:  mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:86:2f:17 brd ff:ff:ff:ff:ff:ff\n    inet 10.3.72.93/16 brd 10.3.255.255 scope global ens192\n       valid_lft forever preferred_lft forever\n    inet6 fe80::e58d:8440:74b9:6793/64 scope link\n       valid_lft forever preferred_lft forever\n3: ens224:  mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:86:1d:75 brd ff:ff:ff:ff:ff:ff\n    inet 10.169.72.93/16 brd 10.169.255.255 scope global ens224\n       valid_lft forever preferred_lft forever\n    inet6 fe80::8a32:a61:60bc:5e1b/64 scope link\n       valid_lft forever preferred_lft forever\n4: docker0:  mtu 1500 qdisc noqueue state DOWN\n    link/ether 02:42:c1:b3:6f:35 brd ff:ff:ff:ff:ff:ff\n    inet 172.16.46.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\n5: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN\n    link/ether 6a:ad:79:6e:3b:53 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n6: cni0:  mtu 1450 qdisc noqueue state UP qlen 1000\n    link/ether 5e:fb:84:b6:44:93 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.1/24 scope global cni0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::5cfb:84ff:feb6:4493/64 scope link\n       valid_lft forever preferred_lft forever\none more question:\nflannel.alpha.coreos.com/public-ip=10.3.72.93\n10.3.72.93 is my management ip on the node which has default mangaement route, I thought the public-ip is suppose to be the node ip 10.169.72.93, how can I let flannel choose 10.169.72.93 on interface ens224 as public-ip ?. used --iface=ens224 resolved it\n  containers:\n  - name: kube-flannel\n    image: quay.io/coreos/flannel:v0.10.0-amd64\n    command:\n    - /opt/bin/flanneld\n    args:\n    - --iface=ens224\n    - --ip-masq\n    - --kube-subnet-mgr\n\n. close the issue, paste below for the referrence\n[root@centos-k8s-master kubernetes]# cluster/kubectl.sh describe node 10.169.72.93\nName:               10.169.72.93\nRoles:              \nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/hostname=10.169.72.93\nAnnotations:        flannel.alpha.coreos.com/backend-data={\"VtepMAC\":\"f6:45:ee:df:8e:61\"}\n                    flannel.alpha.coreos.com/backend-type=vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager=true\n                    flannel.alpha.coreos.com/public-ip=10.169.72.93 <=====\n                    node.alpha.kubernetes.io/ttl=0\n                    volumes.kubernetes.io/controller-managed-attach-detach=true\n                    volumes.kubernetes.io/keep-terminated-pod-volumes=true\nTaints:             \nCreationTimestamp:  Wed, 07 Feb 2018 12:25:16 -0800\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  OutOfDisk        False   Wed, 07 Feb 2018 12:27:16 -0800   Wed, 07 Feb 2018 12:25:16 -0800   KubeletHasSufficientDisk     kubelet has sufficient disk space available\n  MemoryPressure   False   Wed, 07 Feb 2018 12:27:16 -0800   Wed, 07 Feb 2018 12:25:16 -0800   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 07 Feb 2018 12:27:16 -0800   Wed, 07 Feb 2018 12:25:16 -0800   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  Ready            True    Wed, 07 Feb 2018 12:27:16 -0800   Wed, 07 Feb 2018 12:25:26 -0800   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.169.72.93\n  Hostname:    10.169.72.93\nCapacity:\n cpu:            4\n hugepages-2Mi:  0\n memory:         3882052Ki\n pods:           110\nAllocatable:\n cpu:            4\n hugepages-2Mi:  0\n memory:         3779652Ki\n pods:           110\nSystem Info:\n Machine ID:                 4c195d2cc9bd441d8fd1cd7c4cfdf659\n System UUID:                4206EF49-4134-6579-4811-4B52DFD7151E\n Boot ID:                    0d414388-bbb9-441e-b190-1a57b5cfedda\n Kernel Version:             3.10.0-514.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.12.6\n Kubelet Version:            v1.10.0-alpha.3.14+bdde1961911138-dirty\n Kube-Proxy Version:         v1.10.0-alpha.3.14+bdde1961911138-dirty\nPodCIDR:                     10.244.0.0/24\nExternalID:                  10.169.72.93\nNon-terminated Pods:         (2 in total)\n  Namespace                  Name                         CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ---------                  ----                         ------------  ----------  ---------------  -------------\n  kube-system                kube-dns-754f9cd4f5-mr6cn    260m (6%)     0 (0%)      110Mi (2%)       170Mi (4%)\n  kube-system                kube-flannel-ds-jfjnd        100m (2%)     100m (2%)   50Mi (1%)        50Mi (1%)\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  CPU Requests  CPU Limits  Memory Requests  Memory Limits\n  ------------  ----------  ---------------  -------------\n  360m (9%)     100m (2%)   160Mi (4%)       220Mi (5%)\nEvents:\n  Type    Reason                   Age              From                      Message\n  ----    ------                   ----             ----                      -------\n  Normal  Starting                 2m               kube-proxy, 10.169.72.93  Starting kube-proxy.\n  Normal  Starting                 2m               kubelet, 10.169.72.93     Starting kubelet.\n  Normal  NodeHasSufficientDisk    2m (x2 over 2m)  kubelet, 10.169.72.93     Node 10.169.72.93 status is now: NodeHasSufficientDisk\n  Normal  NodeHasSufficientMemory  2m (x2 over 2m)  kubelet, 10.169.72.93     Node 10.169.72.93 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    2m (x2 over 2m)  kubelet, 10.169.72.93     Node 10.169.72.93 status is now: NodeHasNoDiskPressure\n  Normal  NodeAllocatableEnforced  2m               kubelet, 10.169.72.93     Updated Node Allocatable limit across pods\n  Normal  NodeReady                1m               kubelet, 10.169.72.93     Node 10.169.72.93 status is now: NodeReady\n3: ens224:  mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:86:1d:75 brd ff:ff:ff:ff:ff:ff\n    inet 10.169.72.93/16 brd 10.169.255.255 scope global ens224\n       valid_lft forever preferred_lft forever\n    inet6 fe80::8a32:a61:60bc:5e1b/64 scope link\n       valid_lft forever preferred_lft forever\n4: docker0:  mtu 1500 qdisc noqueue state DOWN\n    link/ether 02:42:37:8e:9d:e0 brd ff:ff:ff:ff:ff:ff\n    inet 172.16.46.1/24 scope global docker0\n       valid_lft forever preferred_lft forever\n5: dummy0:  mtu 1500 qdisc noop state DOWN qlen 1000\n    link/ether 96:bc:dd:ff:fb:b3 brd ff:ff:ff:ff:ff:ff\n6: kube-ipvs0:  mtu 1500 qdisc noop state DOWN\n    link/ether 46:43:7f:16:80:3f brd ff:ff:ff:ff:ff:ff\n    inet 10.0.0.1/32 brd 10.0.0.1 scope global kube-ipvs0\n       valid_lft forever preferred_lft forever\n    inet 10.0.0.10/32 brd 10.0.0.10 scope global kube-ipvs0\n       valid_lft forever preferred_lft forever\n7: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN\n    link/ether f6:45:ee:df:8e:61 brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.0/32 scope global flannel.1\n       valid_lft forever preferred_lft forever\n8: cni0:  mtu 1450 qdisc noqueue state UP qlen 1000\n    link/ether be:a9:70:f7:93:4a brd ff:ff:ff:ff:ff:ff\n    inet 10.244.0.1/24 scope global cni0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::bca9:70ff:fef7:934a/64 scope link\n       valid_lft forever preferred_lft forever\n. ",
    "christianhuening": "The error was, I was not mounting xtables altogether in the first place. /closed. ",
    "Cherishty": "@suylem\nYes it does occur error since the  kubelet can't create a pod for flannel. Even though, I still find  the windows node can run normally via flanneld.exe, maybe we can ignore the flannel image. ",
    "sulaweyo": "I wanted to ask if you got it really working or basically working. \nMy windows k8s host works and provisions pods without issues, the network though is not stable and sometimes stops to resolve anything or after the creation of x nodes i need to clean up C:\\var\\lib\\cni\\flannel manually as the files are not autocleaned and i don't get IPs anymore. . ",
    "jpiper": "Looks like this might be related to https://github.com/coreos/flannel/issues/779. ",
    "svenwltr": "We already tried the proposed solution, but it didn't work.\nIIUC correctly we have to change net.core.rmem_max and net.core.wmem_max. These are the values on a failed node:\n```\nsysctl -a | grep [wr]mem_max\nnet.core.rmem_max = 125829120\nnet.core.wmem_max = 125829120\n```. ",
    "slecrenski": "I don't see the flannel.1 link on the agent node.  Is that the issue?  Why is this link not being created?  Is it due to vxlan_network.go:158] failed to add vxlanRoute (10.244.0.0/24 -> 10.244.0.0): invalid argument?  Also if that's the case when why am I able to communicate with the kubernetes.default via 10.96.0.1 over 443?\nIf so what is causing this?\nAppreciate any help.. So if anyone is interested or has this issue I was able to get past this issue by kubectl delete node k8s-master and recreating it.  This allocated a different node cidr subnet 10.244.1.0/24 instead of 10.244.0.0/24 which appears to conflict.\nAll of the links were created but I am still having an issue with the service interface.\n```bash\n[root@k8s-agent2 ~]# nslookup kubernetes.default.svc.cluster.local 10.244.1.2\nServer:     10.244.1.2\nAddress:    10.244.1.2#53\nName:   kubernetes.default.svc.cluster.local\nAddress: 10.96.0.1\n[root@k8s-agent2 ~]# nslookup kubernetes.default.svc.cluster.local 10.96.0.10\n;; connection timed out; trying next origin\n;; connection timed out; no servers could be reached\n[root@k8s-master ~]# kubectl get svc -n kube-system\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)         AGE\nkube-dns   ClusterIP   10.96.0.10           53/UDP,53/TCP   2d\n[root@k8s-master ~]# kubectl get ep -n kube-system\nNAME                      ENDPOINTS                     AGE\nkube-controller-manager                           2d\nkube-dns                  10.244.1.2:53,10.244.1.2:53   2d\nkube-scheduler            \n[root@k8s-agent2 ~]# iptables-save | grep kube-dns\n-A KUBE-SEP-BWHGELGX6BITPZVO -s 10.244.1.2/32 -m comment --comment \"kube-system/kube-dns:dns-tcp\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-BWHGELGX6BITPZVO -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp\" -m tcp -j DNAT --to-destination 10.244.1.2:53\n-A KUBE-SEP-Z6M7ZHWCTBNMPLD7 -s 10.244.1.2/32 -m comment --comment \"kube-system/kube-dns:dns\" -j KUBE-MARK-MASQ\n-A KUBE-SEP-Z6M7ZHWCTBNMPLD7 -p udp -m comment --comment \"kube-system/kube-dns:dns\" -m udp -j DNAT --to-destination 10.244.1.2:53\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.10/32 -p udp -m comment --comment \"kube-system/kube-dns:dns cluster IP\" -m udp --dport 53 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.10/32 -p udp -m comment --comment \"kube-system/kube-dns:dns cluster IP\" -m udp --dport 53 -j KUBE-SVC-TCOU7JCQXEZGVUNU\n-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.96.0.10/32 -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp cluster IP\" -m tcp --dport 53 -j KUBE-MARK-MASQ\n-A KUBE-SERVICES -d 10.96.0.10/32 -p tcp -m comment --comment \"kube-system/kube-dns:dns-tcp cluster IP\" -m tcp --dport 53 -j KUBE-SVC-ERIFXISQEP7F7OF4\n-A KUBE-SVC-ERIFXISQEP7F7OF4 -m comment --comment \"kube-system/kube-dns:dns-tcp\" -j KUBE-SEP-BWHGELGX6BITPZVO\n-A KUBE-SVC-TCOU7JCQXEZGVUNU -m comment --comment \"kube-system/kube-dns:dns\" -j KUBE-SEP-Z6M7ZHWCTBNMPLD7\n[root@k8s-agent2 ~]# ip route\ndefault via 10.244.0.1 dev eth0 proto static metric 100 \n10.244.0.0/16 dev eth0 proto kernel scope link src 10.244.0.6 metric 100 \n10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink \n10.244.2.2 dev cali4b8ffe82a2b scope link \n10.244.2.4 dev cali10256f09271 scope link \n10.244.2.5 dev cali3ac1a873578 scope link \n10.244.3.0/24 via 10.244.3.0 dev flannel.1 onlink \n168.63.129.16 via 10.244.0.1 dev eth0 proto dhcp metric 100 \n169.254.169.254 via 10.244.0.1 dev eth0 proto dhcp metric 100 \n172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1\n```\nAnybody see anything wrong with my rules here?\n. I figured it out.. Figured it out.. The kube-proxy process handles K8s service iptables configuration.\nService IPs are not part of the flannel networking so I think they were going out via the default route eth0 which my network didn't know how to properly route and were likely dropped.\nSo as a solution for all service IPs I enabled SNAT ip masquerading via the kube-proxy config map.  You can also do this with parameters to the kube-proxy ds in older configurations.. ",
    "diwakar-s-maurya": "@slecrenski Can you please share what did you do solve this problem? I am having the same problem.. ",
    "RadixGG": "@slecrenski Yes. I get the same problem. Could you please share how to solve the problem?. ",
    "georgepapas": "Hi, what was the issue?. ",
    "SpComb": "For comparison, kube-router configures separate point-to-point IPIP tunnel interfaces, so it's not affected by this.\n7: tunl0@NONE: <NOARP> mtu 1480 qdisc noop state DOWN mode DEFAULT group default qlen 1\n    link/ipip 0.0.0.0 brd 0.0.0.0 promiscuity 0 \n    ipip remote any local any ttl inherit nopmtudisc \n8: tun-aaaaaaa@bond0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1460 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1\n    link/ipip 192.0.2.2 peer 192.0.2.1 promiscuity 0 \n    ipip remote 192.0.2.1 local 192.0.2.2 dev bond0 ttl inherit pmtudisc addrgenmode eui64 \n9: tun-bbbbbbb@bond0: <POINTOPOINT,NOARP,UP,LOWER_UP> mtu 1460 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1\n    link/ipip 192.0.2.2 peer 192.0.2.0 promiscuity 0 \n    ipip remote 192.0.2.0 local 192.0.2.2 dev bond0 ttl inherit pmtudisc addrgenmode eui64\nWith the tunl0 interface being down, the incoming IP-IP packets from unconfigured sources get rejected by the kernel with ICMP unreachable:\n15:18:11.214722 x.y.z.w > 192.0.2.2: x.y.z.w.52126 > 10.96.0.10.53: 5619+ [1au] A? kubernetes.default.svc.cluster.local. (65) (ipip-proto-4)\n15:18:11.214829 192.0.2.2 > x.y.z.w: ICMP 192.0.2.2 protocol 4 port 93 unreachable, length 121\nIf you bring the tunl0 interface up and disable RPF on it, then does accept the injected traffic.. ",
    "jbpin": "We have updated resources limits and there are no more restart. So the question is :\nWhat's the recommended memory limit for Flannel running in Kubernetes node's pod ?. ",
    "kamrar": "same here. ",
    "Tapolsky": "The same issue for me. Fixed by setting 256Mi memory limit in flannel daemonSet properties. ",
    "ttarczynski": "I'm observing a very similar issue.\n- flannel version: v0.10.0\n  - installed from https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml\n- kubernetes version: v1.11.3\n- OS: CentOS 7.3.1611\n- kernel: 3.10.0-862.11.6.el7.x86_64\n- I've observed flannel getting OOMKilled\n  - with original 50Mi memory limit\n  - then increased the limit to 100M but it get OOMKilled again\n- my cluster is only 10 nodes\nI didn't observe these issues with flannel v0.9.1 as it didn't have the memory limit set.\n- I've found the memory limit was introduced in #855\n  - and the limit value was discussed then by @tomdee and @Dieken \n@tomdee do you think this issue may be caused by the memory limit being too low? (as advised by @Tapolsky above) \n\nHere's the status of this OOMKilled pod:\n```\nkubectl get pod  | egrep -vi running\nNAME                                    READY     STATUS                   RESTARTS   AGE\nkube-flannel-ds-mcg7n                   0/1       Init:RunContainerError   1          7d\n\nName:               kube-flannel-ds-mcg7n\nNode:               node1/172.24.11.44\nStart Time:         Mon, 15 Oct 2018 07:51:22 +0200\nStatus:             Running\nControlled By:      DaemonSet/kube-flannel-ds\nInit Containers:\n  install-cni:\n    Container ID:\n    Image:         quay.io/coreos/flannel:v0.10.0-amd64\n    Image ID:    \n    Command:\n      cp\n    Args:\n      -f\n      /etc/kube-flannel/cni-conf.json\n      /etc/cni/net.d/10-flannel.conflist\n    State:          Waiting\n      Reason:       RunContainerError\n    Last State:     Terminated\n      Reason:       ContainerCannotRun\n      Message:      containerd: container not started\n      Exit Code:    128\n      Started:      Sun, 21 Oct 2018 02:45:27 +0200\n      Finished:     Sun, 21 Oct 2018 02:45:27 +0200\n    Ready:          False\n    Restart Count:  1\nContainers:\n  kube-flannel:\n    Container ID:  docker://be496e7701c9087386e38932b6962b4d50db697cfd3074257cd315fe63cce50b\n    Image:         quay.io/coreos/flannel:v0.10.0-amd64\n    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:88f2b4d96fae34bfff3d46293f7f18d1f9f3ca026b4a4d288f28347fcb6580ac\n    Command:\n      /opt/bin/flanneld\n    Args:\n      --ip-masq\n      --kube-subnet-mgr\n    State:          Terminated\n      Reason:       OOMKilled\n      Exit Code:    2\n      Started:      Sun, 21 Oct 2018 02:40:19 +0200\n      Finished:     Sun, 21 Oct 2018 02:40:19 +0200\n    Ready:          False\n    Restart Count:  13\n    Limits:\n      cpu:     100m\n      memory:  100Mi\n    Requests:\n      cpu:     100m\n      memory:  100Mi\nConditions:\n  Type              Status\n  Initialized       False \n  Ready             False \n  ContainersReady   False \n  PodScheduled      True \nEvents:\n  Type     Reason                  Age                  From                  Message\n  ----     ------                  ----                 ----                  -------\n  Normal   SandboxChanged          8m (x40462 over 5d)  kubelet, node1  Pod sandbox changed, it will be killed and re-created.\n  Warning  FailedCreatePodSandBox  3m (x39973 over 1d)  kubelet, node1  Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod \"kube-flannel-ds-mcg7n\": Error response from daemon: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"process_linux.go:286: decoding sync type from init pipe caused \\\\\"read parent: connection reset by peer\\\\\"\\\"\\n\"\n```\nAnd status on the node:\n- There seems to be 296 install-cni container instances and 14 kube-flannel instances:\n[2018-10-22 08:33:23] # ls /var/lib/kubelet/pods/588b1245-d03e-11e8-b788-02010035514b/containers/install-cni/ | wc -l \n296\n[2018-10-22 08:33:27] # ls /var/lib/kubelet/pods/588b1245-d03e-11e8-b788-02010035514b/containers/kube-flannel/ | wc -l \n14\n- oom-killer logs in dmesg:\nOct 22 08:29:07 node1 kernel: exe invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=-999\nOct 22 08:29:07 node1 kernel: Task in /kubepods/pod588b1245-d03e-11e8-b788-02010035514b/0673b08fed23d22ddb3e6eea674c9356371ca39f9abf1258c8bc7c032f49b50b killed as a result \nOct 22 08:29:07 node1 kernel: memory: usage 102400kB, limit 102400kB, failcnt 3794299\nOct 22 08:29:07 node1 kernel: memory+swap: usage 102400kB, limit 9007199254740988kB, failcnt 0\nOct 22 08:29:07 node1 kernel: kmem: usage 102396kB, limit 9007199254740988kB, failcnt 0. @Dieken In my case I've already increased the limit from 50Mi to 100Mi and still got it OOMKilled.\nBut what's worse, after the OOMKill it was not able to start again. It displayed ContainerCannotRun for the init container install-cni.\nI've observed this problem in my environment once per week and each time needed to delete flannel pods to fix this. This week I've increased the limit to 256Mi and will see if it helps.\nMy cluster is also only 10 nodes and I always see the memory/working_set metric values below 30 MiB.. I think the problems I've seen were related to the default kernel version in CentOS 7 (kernel-3.10.0-*.el7).\nIt seems kubernetes v1.9 or higher with CentOS kernels can result in a \"slab cache memory leak\":\n https://github.com/kubernetes/kubernetes/issues/61937\n https://github.com/moby/moby/issues/37722\nAnd probably this was causing flannel to be OOMKilled in my cluster.\nI have upgraded to a newer kernel version a month ago and since then haven't seen the flannel OOMKilled issue again.. I'm observing a similar issue after upgrading flannel from v0.9.1 to v0.10.0:\n```\nls -l /etc/cni/net.d/\ntotal 8\n-rw-r--r--. 1 root root  92 Oct  3 01:59 10-flannel.conf\n-rw-r--r--. 1 root root 267 Oct 12 03:16 10-flannel.conflist\n```\nI see that the config file name has changed from 10-flannel.conf to 10-flannel.conflist in: #888\nBut I believe there's no mechanism that would delete the old 10-flannel.conf file after the upgrade.\nAnd it seems that 10-flannel.conf still gets applied because it sorts first.. ",
    "chemist": "The same problem. Rollback to 0.9.1 fix it.. ",
    "shettyh": "I have upgraded the kernel to 4.4.176-1.el7.elrepo.x86_64 and upgraded docker to 18.09.3 with kubernetes 1.13 and flannel 0.11\nBut still i am facing OOMKilled with default memory limits.\nAny know fixes for this ? Does increasing the memory limit actually solves this issue ?\nInterestingly enough i am facing this issue only in server class machines!\nMore details:\nKubernetes details\n\nSystem logs\n\n. ",
    "mwmix": "So I got this to work. Basically, I had to do the following:\nfor each server in overlay network add their overlay ip to eth0:\nservera 172.16.1.0\nserverb 172.16.2.0\non servera:\nip addr add 172.16.1.0 dev eth0 \non serverb:\nip addr add 172.16.2.0 dev eth0\nI was then able to do:\non server a:\nping -I 172.16.1.0 172.16.2.0 and see a proper echo reply.\nIs there any reason why flannel, strongswan or charon are not already doing this for us? \nAlso I think it would be a good idea to mention that in order for \"ip xfrm state\" to work you need to edit:\n/etc/strongswan/strongswan.d/charon/kernel-libipsec.conf changing \"load=yes\" to \"load=no\". This apparently then lets kernel-netlink handle things at the kernel level as opposed to userland.. I also found a use case that apparently flannel can't handle right now at least regarding ipsec. In openstack servers are assigned ips that aren't routable across regions or projects. You assign a server a floating ip to get around this which is a publicly accessible ip. However, when you do this that server you assigned the floating ip to has no idea about that floating ip. It just knows about it's private ip address and openstack handles the natting of the traffic. So when you want to do cross region tunnels you run into an issue. The ipsec backend right now generates a policy that looks for a src address of either what is assigned to eth0 or what you specify with --public-ip . So if we tell flannel to use the floating ip via the --public-ip option then the ipsec backend generates policies that look for a src that matches the floating ip. This doesn't work in our openstack scenario since the local server has no concept of that floating ip and so no traffic will ever be sent out with a src ip == the server's floating ip. What we need in this case is a --private-ip option where we can have the ipsec backend use the private ip for the src from the server and then use the --public-ip of the other servers for the given destinations. \nHopefully the above makes sense. I am more than willing to elaborate on this issue. And I have confirmed strongswan with charon will handle this scenario if configured properly.. One final useful comment of note is perhaps considering adding a link in the troubleshooting the ipsec backend documentation to point users at the strongswan HelpRequests wiki page. Most notable it details a really good logging configuration that helps greatly in diagnosing most issues. https://wiki.strongswan.org/projects/strongswan/wiki/HelpRequests. ",
    "bchannak": "@mwmix I ran into a issue similar to your and filed this issue: https://github.com/coreos/flannel/issues/1027\nIs flannel.1 interface necessary when using IPSec backend?\n. @artheus: Sorry for the late response. Yes, you are right. \nWhen running on cloud service providers like AWS, it may make sense to encrypt pod to pod traffic (CPU utilization vs. security) since instances (EC2) are connected over an insecure network. For on-prem it may not be much of an issue since it may be well isolated but in case of running in cloud it cannot be assumed.. ",
    "ncabatoff": "I've run into this issue too.  I too don't have a flannel.1 interface, but having studied the code and compared it to the vxlan backend code, it's clear that's deliberate.  So I'm treating this issue as a generic \"flannel ipsec doesn't work out of the box with Kubernetes\" bug report, as per its title, which matches my experience.\nIn my case, everything comes up just fine and reports as healthy, but I can't connect to any services I deploy.  Pod-to-pod networking works just fine, but node-to-pod networking does not.  \nI didn't find adding overlay IPs (as proposed above) to help.  What worked for me was to add a route on masters:\nroute add -net 10.244.0.0 netmask 255.255.0.0 cni0\nThe resulting route table:\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         192.168.0.2     0.0.0.0         UG    0      0        0 enp0s3\n10.0.0.0        *               255.255.255.0   U     0      0        0 enp0s8\n10.244.0.0      *               255.255.255.0   U     0      0        0 cni0\n10.244.0.0      *               255.255.0.0     U     0      0        0 cni0\n172.17.0.0      *               255.255.0.0     U     0      0        0 docker0\n192.168.0.0     *               255.255.0.0     U     0      0        0 enp0s3\nSetup:\n- created a cluster via kubeadm init with the --pod-network-cidr=10.244.0.0/16 argument as per kubernetes docs for flannel\n- modified the kube-flannel.yaml manifest:\n  - added --iface=enp0s8 to the kube-flannel container arguments\n  - changed image to a locally built image since the quay.io image doesn't seem to have ipsec included\n  - changed the kube-flannel-cfg configmap (PSK generated using dd as descibed in backends):\nnet-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"ipsec\",\n        \"PSK\": \"ee9b0c24c5d015f4ed23e84fb1473314a8baa8bee65fcefd659981d59423c6ee718eee681f579cccf18fd5d7f759f736\"\n      }\n    }. ",
    "pms1969": "I've run into this problem just now.  The route fix specified by @ncabatoff did the trick, but no matter what I did with swanctl to reload setting to with the charon.conf: install_routes=yes line uncommented, didn't seem to work.  Am I missing something here?\nEither way, how do I make sure that route is added?  an init container?\n. It's worth noting that I opened the security groups so that it accepted traffic from everywhere, and the iam permissions were set to:\n\"autoscaling:DescribeAutoScalingGroups\",\n      \"autoscaling:DescribeAutoScalingInstances\",\n      \"autoscaling:DescribeLaunchConfigurations\",\n      \"autoscaling:DescribeTags\",\n      \"autoscaling:SetDesiredCapacity\",\n      \"autoscaling:TerminateInstanceInAutoScalingGroup\",\n      \"autoscaling:UpdateAutoScalingGroup\",\n      \"ec2:AttachVolume\",\n      \"ec2:AuthorizeSecurityGroupIngress\",\n      \"ec2:CreateRoute\",\n      \"ec2:CreateSecurityGroup\",\n      \"ec2:CreateTags\",\n      \"ec2:CreateVolume\",\n      \"ec2:DeleteRoute\",\n      \"ec2:DeleteSecurityGroup\",\n      \"ec2:DeleteVolume\",\n      \"ec2:DescribeAvailabilityZones\",\n      \"ec2:DescribeInstances\",\n      \"ec2:DescribeRegions\",\n      \"ec2:DescribeRouteTables\",\n      \"ec2:DescribeSecurityGroups\",\n      \"ec2:DescribeSubnets\",\n      \"ec2:DescribeSubnets\",\n      \"ec2:DescribeVolumes\",\n      \"ec2:DescribeVolumesModifications\",\n      \"ec2:DescribeVpcs\",\n      \"ec2:DetachVolume\",\n      \"ec2:ModifyInstanceAttribute\",\n      \"ec2:ModifyVolume\",\n      \"ec2:ReplaceRoute\",\n      \"ec2:RevokeSecurityGroupIngress\",\n      \"elasticloadbalancing:DescribeListeners\",\n      \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\n      \"elasticloadbalancing:DescribeLoadBalancerPolicies\",\n      \"elasticloadbalancing:DescribeLoadBalancers\",\n      \"elasticloadbalancing:DescribeTargetGroups\",\n      \"elasticloadbalancing:DescribeTargetHealth\",\n      \"iam:GetServerCertificate\",\n      \"iam:ListServerCertificates\",. Never mind.  Tracked the issue down to a network acl rule issue.. ",
    "c-rainstorm": "This problem is solved.\n\nFirst, modify the kube-flannel.yaml\uff0cadd this line to command /opt/bin/flanneld\n```yaml\n--iface=enp0s8       # I use virtualBox to create a  cluster. enp0s8 is your Host-Only adapter, default adapter is enp0s3 which is the NAT adapter\n```\n\nthen, restart your VMs.\n\n\nSetup your cluster again.\n. \n\n",
    "Pezhvak": "i have the same problem. ",
    "shsjshentao": "It looks like a bug of golang 1.9.2\nhttps://github.com/golang/go/issues/22724 . ",
    "ccollicutt": "I also face this problem. Packet.net is slightly unusual in how they manage their private networking, though in a good way :), but I'm sure there will be situations in which flannel needs to deal with an interface that has multiple IPs associated.\nFor now I've migrated the internal IP to a macvlan interface attached to the bond and it seems to be working, and then I tell flannel to use that interface instead.. ",
    "ivan-section-io": "I too faced this problem today. Also at Packet.net - worked around by removing+re-adding the public IP from the interface at boot (this made the existing private 10.n.n.n address the \"first\").\nipPublicCIDR=$(ip -o addr show  label bond0 | grep -Po 'inet \\K[\\d.]+/\\d\\d')\nip addr del ${ipPublicCIDR} dev bond0\nip addr add ${ipPublicCIDR} dev bond0\nWould have been nice if --iface-regex=^10\\. would have found it.. ",
    "lukaszpy": "Found the problem which exists in DNS (ip adresses works ok. Helped me config map for dns:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\ndata:\n  upstreamNameservers: |-\n    [\"8.8.8.8\", \"8.8.4.4\"]. ",
    "parvez84": "I am stuck at the same exact place ,  I tried giving google dns and corp dns it still fails. Any suggestions ?. ",
    "dperique": "This same issue happens to us with minikube 0.23.0, Kubernetes 1.7.5, and Ubuntu 18 (bionic).  DNS names don't resolve.  The workaround for putting upstreamNameservers in the configmap helped.    Thanks @lukaszpy !\nWe still have to figure out a way to tweak our minikube setups to apply this workaround until this gets fixed.. ",
    "hokiegeek2": "From inside busybox pod after applying upstreamServer config @lukaszpy mentioned above I get the following for external nslookups (minikube  v0.28.1 on ubuntu 18.04):\n/ # nslookup 8.8.8.8\nServer:     10.96.0.10\nAddress:    10.96.0.10:53\nNon-authoritative answer:\n8.8.8.8.in-addr.arpa    name = google-public-dns-a.google.com\n/ # nslookup google.com\nServer:     10.96.0.10\nAddress:    10.96.0.10:53\nNon-authoritative answer:\nName:   google.com\nAddress: 172.217.5.238\n*** Can't find google.com: No answer\n/ # \nWhen I attempt kubernetes.default, I get this:\n/ # nslookup kubernetes.default\nServer:     10.96.0.10\nAddress:    10.96.0.10:53\n** server can't find kubernetes.default: NXDOMAIN\n*** Can't find kubernetes.default: No answer\n/ # \n. From inside busybox pod after applying upstreamServer config @lukaszpy mentioned above I get the following for external nslookups (minikube  v0.28.1 on ubuntu 18.04):\n/ # nslookup 8.8.8.8\nServer:     10.96.0.10\nAddress:    10.96.0.10:53\nNon-authoritative answer:\n8.8.8.8.in-addr.arpa    name = google-public-dns-a.google.com\n/ # nslookup google.com\nServer:     10.96.0.10\nAddress:    10.96.0.10:53\nNon-authoritative answer:\nName:   google.com\nAddress: 172.217.5.238\n*** Can't find google.com: No answer\n/ # \nWhen I attempt kubernetes.default, I get this:\n/ # nslookup kubernetes.default\nServer:     10.96.0.10\nAddress:    10.96.0.10:53\n** server can't find kubernetes.default: NXDOMAIN\n*** Can't find kubernetes.default: No answer\n/ # \n. ",
    "kotnn": "+1 the same issue. ",
    "velmurugan-r": "+1 the same issue. I fixed the problem sorry for the confusion , forgot to copy the file to the kubeconfig export path, after copied to the path it works for me \nexport KUBECONFIG=$HOME/.kube/admin.conf. ",
    "CrisMon-01": "@velmurugan-r i have change with weave. it work better for my purpouse, check the ethernet. i don't remeber how i solved this problem because is too old. ",
    "mainred": "close this ticket for being addressed..  @caseydavenport @tomdee, please take a look. ",
    "dtshepherd": "There are no error messages in syslog indicating a resource shortage and everything else in the system seems to be functional.  The flannel daemonset pods are still running as expected.  Once that error shows up, flannel never recovers without bouncing the pods.  We have some other code that is managing iptables rules and suspect both it and flannel are trying to make changes at the same time (hence, why I pointed to #935 as a workaround).. No problem!   I thought it should retry as well, but the pod becomes hung and doesn't do anything.  I haven't had a chance to dig into the code as to why it isn't working.. @cehoffman Wow good catch, we are using hyperkube and I was just looking at our kube-proxy manifest and it does not have the /run/xtables.lock mount point.  When I find some time, I'll try that configuration to see if it fixes the problem.  For now, we've been running with weave.... ",
    "omkensey": "A few things occur to me right off:\n\n172.17.*.* looks like the default Docker bridge network to me.  If your etcd containers are running on the host-only Docker bridge network they won't be accessible outside that host by default.\nYour endpoints list only has two endpoints.  Ideally you should have an odd number -- 1, 3, 5 or 7.\nDo the etcd SSL cert/key/etc. exist on the worker nodes in the same directories and file names as on the master?. \n",
    "Dileep2016": "@omkensey SSl  keys are same place in worker nodes..\netcd running on ubuntu machine .. not on docker host.. for testing purpose i spun up 2 maste and 2 nodes\n enpoints i will add 1 more etcd .. may i know the reason behind odd number. @omkensey  etcd.service  file is correct right is there anything to add...\ninterface = eth0 or eth1 do i need to add this one. ",
    "depperson": "What was the problem?. ",
    "mendoncaangelo": "Apparently K8s 1.10 was causing the issue. I downgraded to 1.9 and it was fine.. ",
    "benoitheinrich": "Hi guys, I've got the same issue as documented here, but I can't find how to downgrade my kubernetes version from 1.10 to 1.9.\nWhen running the sudo kubeadm upgrade apply v1.9.0 --force command it fails with the following error: \n```\n[upgrade/version] FATAL: The --version argument is invalid due to these fatal errors:\n- Specified version to upgrade to \"v1.9.0\" is equal to or lower than the minimum supported version \"v1.10.4\". Please specify a higher version to upgrade to\n- Kubeadm version v1.10.4 can only be used to upgrade to Kubernetes versions 1.10\n\nPlease fix the misalignments highlighted above and try upgrading again\n```\nCan you please let me know how to downgrade my kubernetes version, and also, do you know if that issue will be fixed for version 1.10 of kubernetes ?\nThanks.. ",
    "tan-tan-kanarek": "I'm having the same issue with version 1.11.3, I assume it's not solved yet.\nShould I downgrade the server version or the kubectl client version?. ",
    "makeloffve": "Check if there are multiple default routes in your ip route, make sure there is only one default route.. ",
    "kopiczko": "@tomdee is there any way I can move it forward?. @tomdee PTALA. I added a comment. LMK if you'd like to further improve it. The pattern is well tested in annotations_test.go.. Done. Done. My idea was to add a trailing slash if the annotation doesn't contain any. Please check test cases I copied here. They show how the flag value is transformed to the final annotation name.\n+       {\n+           prefix:              \"flannel.alpha.coreos.com\",\n+           expectedBackendType: \"flannel.alpha.coreos.com/backend-type\",\n+       },\n+       {\n+           prefix:              \"flannel.alpha.coreos.com/\",\n+           expectedBackendType: \"flannel.alpha.coreos.com/backend-type\",\n+       },\n+       {\n+           prefix:              \"flannel.alpha.coreos.com/prefix\",\n+           expectedBackendType: \"flannel.alpha.coreos.com/prefix-backend-type\",\n+       },\n+       {\n+           prefix:              \"flannel.alpha.coreos.com/prefix-\",\n+           expectedBackendType: \"flannel.alpha.coreos.com/prefix-backend-type\",\n+       },\nThis would be useful for us. We'd like to to have prefixes like flannel.alpha.coreos.com/some-id-. I'm fine with changing that. Let me know what is your preference.. ",
    "discordianfish": "Honestly, I'm not sure this helps. I've just tested this on a cluster here and I don't see the conntrack insert failures going down at all... \n\nIn my case it got worse, so probably don't merge this.. There is a underlying issue somewhere though.. @maxlaverse Oh Hi! :) No, conntrack table isn't full. There are at least ~100k entries available on all my nodes:\n\nbottomk(3, node_nf_conntrack_entries_limit - node_nf_conntrack_entries)\n{instance=\"172.20.163.110:9100\",job=\"kubernetes-service-endpoints\",kubernetes_name=\"node-exporter\",kubernetes_namespace=\"prometheus\"} | 99950\n{instance=\"172.20.160.36:9100\",job=\"kubernetes-service-endpoints\",kubernetes_name=\"node-exporter\",kubernetes_namespace=\"prometheus\"} | 100297\n{instance=\"172.20.164.169:9100\",job=\"kubernetes-service-endpoints\",kubernetes_name=\"node-exporter\",kubernetes_namespace=\"prometheus\"} | 127662\n\n. ",
    "maxlaverse": "Hi @discordianfish \nCould it be that you've exhausted the triplet local_ip:nameserver_ip:nameserver_port ? How full is your conntrack table ? . As discussed together, I don't know what the issue could be here nor why the --random-fully makes the situation worse :/\nBut if this was going to be merged, I think I would make it configurable. There is also a very interesting hack in the link you posted, using tc to randomly slow some packets down. Maybe worse a try.\nOur plan is still to get rid of NAT completely . I read a few issues about those DNS timeouts. As pointed out in https://github.com/weaveworks/weave/issues/3287#issuecomment-387178077, the problem in that case might be with the DNAT rules that transform the Service IP into a PodIP in the requests for nameserver resolution. It looks like there is the same delay as with SNAT, between the allocation of a conntrack record and its insertion in the table. This can lead some collisation and ultimately in packet drops.\nApparently, IPVS uses its own connection tracking module (http://kb.linuxvirtualserver.org/wiki/Performance_and_Tuning). If you have a network that supports routable PodIPs and switch kube-proxy to ipvs, it might help with your issue (if it's really about DNAT in conntrack).\nJust read there are different issues with ipvs too: https://github.com/kubernetes/kubernetes/issues/57841. Hi @szuecs \nMost of the DNS requests timeout issues I read about are located between the Pods and the Cluster DNS. This is likely caused by a race condition the DNAT path, not the SNAT (which this PR is about). In that case it's not the Network Plugin's fault but kube-proxy's and its implementation of the ClusterIP load-balancing.\n@Quentin-M wrote an article about it that might interest you, with a workaround: https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/\nI hope it will help. ",
    "IvanovOleg": "I have the same issue with Kubernetes 1.10.4 + Azure CNI 1.0.6. ",
    "Quentin-M": "I just posted a little write-up about our journey troubleshooting the issue, and how we are worked around it in production: https://blog.quentin-machu.fr/2018/06/24/5-15s-dns-lookups-on-kubernetes/.\nImplementing NF_NAT_RANGE_PROTO_RANDOM_FULLY may not be enough to fix the problem, as it only addresses the SNAT race in my understand, whereas the race also exist with DNAT.. ",
    "sdedwards": "I had the same symptoms and it was caused by my machine not having a default route set. You can see if you have a default route by running:\nroute\nadd checking that default or 0.0.0.0 is present. If not run:\nroute add default gw <gateway ip> netmask 0.0.0.0\nI don't understand why I had to do this because I thought flannel was supposed to configure the routing table. Can anyone explain?. ",
    "randyrue": "It turned out I was trying to use kubectl apply to make changes to the flannel install but was failing to overwrite an earlier wrong subnet entry. Ripped out flannel and reinstalled with the correct entry and made progress.\nStill not there yet, however. All appears to be well, and traffic is routing among pods for the pod subnet and the real world. But the pods and nodes don't have any routing entries for the service subnet, nodes don't have any virtual IPs on the service subnet, and pods can't reach things like kube-dns.. I've made some more progress but it's clear there are things about kubernetes networking I don't understand, and I'm still not working 100%.\nFrom a busybox pod I was unable to use nslookup to resolve any hostnames from my on-premise DNS or the outside world. Instead it would return a cluster.local name for the kube-dns pod and a 10.96 IP. But I eventually realized if it was reporting a PTR lookup for the nameserver it had to be reaching a nameserver somehow. The problem was that nameserver wasn't forwarding lookups for anything outside cluster.local.\nI added a configmap:\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kube-dns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\ndata:\n  upstreamNameservers: |-\n    [\"x.x.x.x\", \"y.y.y.y\", \"z.z.z.z\"]\nwith the IPs of my on-premise DNS servers. Now nslookup on my pod resolves hostnames in my own zones and outside names like google.com.\nBut for every lookup there's almost exactly a 20s pause, 20 plus a few milliseconds. For google.com I get both an IPv4 and IPv6 address returned, and it takes 40s.\nThis smells like kube-dns is timing out and then forwarding the lookup. Where is that 20s timeout defined?\nAnd a separate question, how is traffic reaching the 10.96 service subnet and kube-dns with no interfaces or routing entries anywhere?. I'm having the same problem with kubernetes 1.10.5 and Ubuntu 18.04 LTS. I'm new to Kubernetes and am probably misunderstanding how this all works. Would appreciate any guidance.\nIf I go to the shell on a busybox pod I can't resolve kubernetes.default or kubernetes.x where x is one of three namespaces I've created.\nBut I'm getting slightly farther than the OP. When I run nslookup I get:\n```\n/ # nslookup kubernetes.default\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\nnslookup: can't resolve 'kubernetes.default'\n/ #\nThis suggests that the nameserver IP is successfully resolving to a PTR record of some sort? Sure enough, testing nslookup against the above name of the nameserver works:\n/ # nslookup kube-dns.kube-system.svc.cluster.local\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\nName:      kube-dns.kube-system.svc.cluster.local\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n/ #\n```\nSo nslookup appears to be able to reach a 10.96 IP even though there are no interfaces, IPs or routing entries on my nodes or pods related to any 10.xxx network.\nAnd it appears that kube-dns is not forwarding other requests to the node's nameserver entries, which are present in /etc/netplan/bond0.yaml.\nWhat am I missing?\nHow is traffic reaching kube-dns at a 10.96 IP?\nWhere/how should kube-dns know to forward lookups to the node's nameserver setting?\nHope to hear from you.... OK, found this other earlier issue: https://github.com/coreos/flannel/issues/983\nWhich is much like mine, they're also getting the reverse lookup for the kube-dns pod\nAdded the configmap for kube-dns with the upstreamNameserver setting pointed at my on-premise nameservers and I can now nslookup both outside and on-premise hostnames but it takes just about exactly 20s to return an address. In fact, for google.com it takes 20s to return an IPv6 address and then another 20s to return IPv4.\nThis smells like a timeout setting that needs to be changed.. OK, it's getting weirder.\nAfter deleting and relaunching the busybox pod, nslookup now immediately returns a \"non-authoritative\" answer. But then it hangs for 5s exactly and returns an error \"*** Can't find s---.org: No answer\"\nHowever, calls like ping resolve and reply right away. I think this is an nslookup quirk, and I'm calling it good for now.. ",
    "pytomtoto": "    command:\n    - /opt/bin/flanneld\n    args:\n    - --ip-masq\n    - --kube-subnet-mgr\n    - --iface=eth0\n\nyour yaml file try to set this        - --iface=eth0. kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml\nyou can try this.. ###flannel pod logs\n\n[root@master001 flannel]# kubectl  -n kube-system logs kube-flannel-ds-2rmhd -c kube-flannel\nI0719 09:16:15.342703       1 main.go:475] Determining IP address of default interface\nI0719 09:16:15.342939       1 main.go:488] Using interface with name eth0 and address 10.222.35.100\nI0719 09:16:15.342957       1 main.go:505] Defaulting external address to interface address (10.222.35.100)\nI0719 09:16:15.441906       1 kube.go:131] Waiting 10m0s for node controller to sync\nI0719 09:16:15.442790       1 kube.go:294] Starting kube subnet manager\nI0719 09:16:16.443405       1 kube.go:138] Node controller sync successful\nI0719 09:16:16.443427       1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - node001.demo.com\nI0719 09:16:16.443438       1 main.go:238] Installing signal handlers\nI0719 09:16:16.443612       1 main.go:353] Found network config - Backend type: vxlan\nI0719 09:16:16.443660       1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false\nI0719 09:16:16.444012       1 main.go:300] Wrote subnet file to /run/flannel/subnet.env\nI0719 09:16:16.444026       1 main.go:304] Running backend.\nI0719 09:16:16.444032       1 main.go:322] Waiting for all goroutines to exit\nI0719 09:16:16.444081       1 vxlan_network.go:60] watching for new subnet leases\n[root@master001 flannel]# kubectl  -n kube-system logs kube-flannel-ds-2rmhd -c install-cni\ncp -f /etc/kube-flannel/cni-conf.json /etc/cni/net.d/10-flannel.conf\ntrue\nsleep 3600\n[root@master001 flannel]#\nJul 19 17:50:19 localhost rz[6470]: [root] kube-flannel.yml/ZMODEM: 3294 Bytes, 99091 BPS\nJul 19 17:50:33 localhost kubelet: I0719 17:50:33.150217   28278 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"cni\" (UniqueName: \"kubernetes.io/host-path/2db5e36e-8b39-11e8-a393-52560ade2364-cni\") pod \"kube-flannel-ds-kqwzn\" (UID: \"2db5e36e-8b39-11e8-a393-52560ade2364\")\nJul 19 17:50:33 localhost kubelet: I0719 17:50:33.150328   28278 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"flannel-token-lfn64\" (UniqueName: \"kubernetes.io/secret/2db5e36e-8b39-11e8-a393-52560ade2364-flannel-token-lfn64\") pod \"kube-flannel-ds-kqwzn\" (UID: \"2db5e36e-8b39-11e8-a393-52560ade2364\")\nJul 19 17:50:33 localhost kubelet: I0719 17:50:33.150399   28278 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"flannel-cfg\" (UniqueName: \"kubernetes.io/configmap/2db5e36e-8b39-11e8-a393-52560ade2364-flannel-cfg\") pod \"kube-flannel-ds-kqwzn\" (UID: \"2db5e36e-8b39-11e8-a393-52560ade2364\")\nJul 19 17:50:33 localhost kubelet: I0719 17:50:33.150444   28278 reconciler.go:207] operationExecutor.VerifyControllerAttachedVolume started for volume \"run\" (UniqueName: \"kubernetes.io/host-path/2db5e36e-8b39-11e8-a393-52560ade2364-run\") pod \"kube-flannel-ds-kqwzn\" (UID: \"2db5e36e-8b39-11e8-a393-52560ade2364\")\nJul 19 17:50:33 localhost systemd: Started Kubernetes transient mount for /var/lib/kubelet/pods/2db5e36e-8b39-11e8-a393-52560ade2364/volumes/kubernetes.io~secret/flannel-token-lfn64.\nJul 19 17:50:33 localhost systemd: Starting Kubernetes transient mount for /var/lib/kubelet/pods/2db5e36e-8b39-11e8-a393-52560ade2364/volumes/kubernetes.io~secret/flannel-token-lfn64.\nJul 19 17:50:33 localhost dockerd: time=\"2018-07-19T17:50:33.829102856+08:00\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container c797157e345da04944109615ce2c547d32df4d1da97e7f1eae27a00d74f94538\"\nJul 19 17:50:34 localhost kubelet: I0719 17:50:34.482980   28278 kuberuntime_manager.go:513] Container {Name:kube-flannel Image:registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 Command:[/opt/bin/flanneld] Args:[--ip-masq --kube-subnet-mgr] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[cpu:{i:{value:100 scale:-3} d:{Dec:} s:100m Format:DecimalSI} memory:{i:{value:52428800 scale:0} d:{Dec:} s:50Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:} s:100m Format:DecimalSI} memory:{i:{value:52428800 scale:0} d:{Dec:} s:50Mi Format:BinarySI}]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:} {Name:flannel-token-lfn64 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.\nJul 19 17:50:34 localhost dockerd: time=\"2018-07-19T17:50:34.714153357+08:00\" level=warning msg=\"Unknown healthcheck type 'NONE' (expected 'CMD') in container 7d98da8d931230449bc7151758bc587bcfed4d8dbc4fbf7c41155989f7562a40\"\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): new Vxlan device (carrier: OFF, driver: 'vxlan', ifindex: 5)\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): link connected\nJul 19 17:50:37 localhost NetworkManager[442]:   keyfile: add connection in-memory (318d30b3-a68e-45b1-90ac-ec11b0486d00,\"flannel.1\")\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: unmanaged -> unavailable (reason 'connection-assumed') [10 20 41]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: unavailable -> disconnected (reason 'connection-assumed') [20 30 41]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): Activation: starting connection 'flannel.1' (318d30b3-a68e-45b1-90ac-ec11b0486d00)\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: disconnected -> prepare (reason 'none') [30 40 0]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: prepare -> config (reason 'none') [40 50 0]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: config -> ip-config (reason 'none') [50 70 0]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: ip-config -> ip-check (reason 'none') [70 80 0]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: ip-check -> secondaries (reason 'none') [80 90 0]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): device state change: secondaries -> activated (reason 'none') [90 100 0]\nJul 19 17:50:37 localhost NetworkManager[442]:   (flannel.1): Activation: successful, device activated.\nJul 19 17:50:37 localhost dbus[410]: [system] Activating via systemd: service name='org.freedesktop.nm_dispatcher' unit='dbus-org.freedesktop.nm-dispatcher.service'\nJul 19 17:50:37 localhost dbus-daemon: dbus[410]: [system] Activating via systemd: service name='org.freedesktop.nm_dispatcher' unit='dbus-org.freedesktop.nm-dispatcher.service'\nJul 19 17:50:37 localhost systemd: Starting Network Manager Script Dispatcher Service...\nJul 19 17:50:37 localhost dbus[410]: [system] Successfully activated service 'org.freedesktop.nm_dispatcher'\nJul 19 17:50:37 localhost dbus-daemon: dbus[410]: [system] Successfully activated service 'org.freedesktop.nm_dispatcher'\nJul 19 17:50:37 localhost systemd: Started Network Manager Script Dispatcher Service.\nJul 19 17:50:37 localhost nm-dispatcher: Dispatching action 'up' for flannel.1. /close. \n",
    "andyzasl": "Have same issue on debian 9 with kernel 4.9.110-3+deb9u5\n. ",
    "shineygreen": "It looks like the pod is running out of cgroup memory and the system is killing it. Is something causing it to grow too large, or has the memory limit been set too low?\n[16218.363226] Task in /kubepods/pod3142ce18-765e-11e8-9a6d-0cc47ac764d4/01fcf9fd104573d838cf2182552c0fdbdc56f472792410f2ef7eb1010a19b2bc killed as a result of limit of /kubepods/pod3142ce18-765e-11e8-9a6d-0cc47ac764d4\n[16218.363237] memory: usage 51200kB, limit 51200kB, failcnt 5974389\n[16218.363240] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0\n[16218.363242] kmem: usage 36932kB, limit 9007199254740988kB, failcnt 0. I could not get the Flannel pods to stop restarting, so I punted on Flannel and switched to Weave. That is stable. . ",
    "royxue": "@shineygreen \n+1 this problem.\nIs switching to Weave need kubeadm reset first?. @prateek2408 as I tried before, it doesnt. ",
    "prateek2408": "got the same error, does switching to weave need a kubeadm reset ?. ",
    "jorgonzalez": "+1 to this problem. Is there going to be any investigation or a new flannel image anytime soon?\nThanks.. ",
    "notxcain": "I think this should be reopened to appear on flannel devs radar. ",
    "ujnzxw": "I encountered same issue with you. How did you solve this?. ",
    "hrj713": "security group configuration required :\n\n. ",
    "beebird": "@hrj713 Great! I've been in pain these days for the same issue.\nI was trying to remove iptables rule: POSTROUTING -s 172.16.0.0/16 -d 172.16.0.0/16 -j RETURN to allow dynamic SNAT (send through node ip), while flannel keeps reseting my manual change every a few seconds...\nThanks again, apparently your solution is correct, and traffic doesn't have to be NATed.. ",
    "bladerunner512": "Already have rbac applied. This error only occurs when using separate secure etcd cluster, when etcd is on master node Flannel works. Etcd also works fine outside of cluster and inside, i.e. multiple masters come up (apiserver, controller, proxy and scheduler) with no errors.\nkubelet log has apiserver unauthorized error:\nkubelet[60238]: E0719 10:15:34.603650   60238 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list v1.Pod: Unauthorized\nkubelet[60238]: E0719 10:15:34.604275   60238 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:451: Failed to list v1.Service: Unauthorized\nkubelet[60238]: E0719 10:15:34.605166   60238 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:460: Failed to list *v1.Node: Unauthorized\n\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"plugins\": [\n        {\n          \"type\": \"flannel\",\n          \"delegate\": {\n            \"hairpinMode\": true,\n            \"isDefaultGateway\": true\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"capabilities\": {\n            \"portMappings\": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: amd64\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: quay.io/coreos/flannel:v0.10.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: quay.io/coreos/flannel:v0.10.0-amd64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --iface=eth0\n        - --ip-masq\n        - --kube-subnet-mgr\n        - --etcd-endpoints=https://127.0.0.1:4242\n        - --etcd-keyfile=/etc/kubernetes/pki/etcd/client-key.pem\n        - --etcd-certfile=/etc/kubernetes/pki/etcd/client.pem\n        - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem   \n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg. ",
    "echokk11": "but it works in another node.\n```\n./flanneld --etcd-endpoints=https://172.16.1.38:2379,https://172.16.2.82:2379 \\\n\n--etcd-cafile=/var/lib/kubernetes/ca.pem \\\n--etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\n--etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem\nI0726 11:00:40.366100   10791 main.go:475] Determining IP address of default interface\nI0726 11:00:40.366407   10791 main.go:488] Using interface with name eth0 and address 172.16.2.82\nI0726 11:00:40.366423   10791 main.go:505] Defaulting external address to interface address (172.16.2.82)\n2018-07-26 11:00:40.367120 I | warning: ignoring ServerName for user-provided CA for backwards compatibility is deprecated\nI0726 11:00:40.367175   10791 main.go:235] Created subnet manager: Etcd Local Manager with Previous Subnet: 10.200.30.0/24\nI0726 11:00:40.367182   10791 main.go:238] Installing signal handlers\nI0726 11:00:40.391856   10791 main.go:353] Found network config - Backend type: ali-vpc\nI0726 11:00:40.391918   10791 alivpc.go:63] Unmarshal Configure : { }\nI0726 11:00:40.405122   10791 local_manager.go:147] Found lease (10.200.30.0/24) for current IP (172.16.2.82), reusing\nI0726 11:00:41.018413   10791 alivpc.go:164] Keep target entry: rtableid=vtb-23przw1rz, CIDR=10.200.30.0/24, NextHop=i-bp1ggd58nxvl4mqlqisl\nI0726 11:00:41.018440   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=192.168.0.0/16, NextHop=i-bp1aslokq5ljn2cpyzv8\nI0726 11:00:41.018449   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=172.26.0.0/24, NextHop=VCON-0lb1ut1q3\nI0726 11:00:41.018475   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=10.128.0.0/9, NextHop=VCON-0lb1ut1q3\nI0726 11:00:41.018484   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=10.0.0.0/9, NextHop=VCON-0lb1ut1q3\nI0726 11:00:41.018492   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=172.16.1.0/24, NextHop=\nI0726 11:00:41.018499   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=172.16.2.0/24, NextHop=\nI0726 11:00:41.018507   10791 alivpc.go:187] Keep route entry: rtableid=vtb-23przw1rz, CIDR=100.64.0.0/10, NextHop=\nI0726 11:00:41.200829   10791 main.go:300] Wrote subnet file to /run/flannel/subnet.env\nI0726 11:00:41.200856   10791 main.go:304] Running backend.\nI0726 11:00:41.207778   10791 main.go:396] Waiting for 22h59m59.199404331s to renew lease\n```\n\nthe 2 alicloud is the same.. ",
    "tuxknight": "@echokk11 \nDid you fix this problem ?\nI found a solution. You could set an environment varaible with  export GODEBUG=netdns=go .\nIt is an option in /etc/resovel.conf  (single-request-reopen) that cause thepanic. So you can also delete this option to avoid this bug.\nHope this will be a help.. ",
    "marksugar": "After running for 34 minutes, I realized that he had a problem.\nkubectl get pods --all-namespaces \nNAMESPACE     NAME                                           READY     STATUS              RESTARTS   AGE\nkube-system   coredns-78fcdf6894-gxzvk                       0/1       ContainerCreating   0          34m\nkube-system   coredns-78fcdf6894-qt2zq                       0/1       ContainerCreating   0          34m\nBut after I opened docker ps -a, I found that this container has been Exited (0) 26 minutes ago\nF4410baaf5c3 f0fad859c909 \"cp -f /etc/kube-f...\" 26 minutes ago Exited (0) 26 minutes ago k8s_install-cni_kube-flannel-ds-amd64-7xphb_kube-system_b9181d5d-a396-11e8-aaf5-88882f0da0b5_0\nAnd, what is he constantly trying?\nfc01139716cf        k8s.gcr.io/pause:3.1   \"/pause\"                 Less than a second ago   Up Less than a second                                   k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1520\n0be80323c554        k8s.gcr.io/pause:3.1   \"/pause\"                 Less than a second ago   Up Less than a second                                   k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1515\n8035bc0ff9eb        k8s.gcr.io/pause:3.1   \"/pause\"                 1 second ago             Exited (0) Less than a second ago                       k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1514\n1ded83fd1bcc        k8s.gcr.io/pause:3.1   \"/pause\"                 1 second ago             Exited (0) 1 second ago                                 k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1519\nbe4936c45c05        k8s.gcr.io/pause:3.1   \"/pause\"                 2 seconds ago            Exited (0) 1 second ago                                 k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1513\nc6b37e314c0b        k8s.gcr.io/pause:3.1   \"/pause\"                 3 seconds ago            Exited (0) 2 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1518\nd5ebfcd313a8        k8s.gcr.io/pause:3.1   \"/pause\"                 4 seconds ago            Exited (0) 3 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1512\n206a64561310        k8s.gcr.io/pause:3.1   \"/pause\"                 5 seconds ago            Exited (0) 4 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1511\nbdeda43c4d6d        k8s.gcr.io/pause:3.1   \"/pause\"                 5 seconds ago            Exited (0) 4 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1517\n0dcecd20317f        k8s.gcr.io/pause:3.1   \"/pause\"                 6 seconds ago            Exited (0) 5 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1510\n88eafefb9665        k8s.gcr.io/pause:3.1   \"/pause\"                 6 seconds ago            Exited (0) 5 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1516\n56077c076d57        k8s.gcr.io/pause:3.1   \"/pause\"                 7 seconds ago            Exited (0) 6 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1509\n5e403d699ea6        k8s.gcr.io/pause:3.1   \"/pause\"                 7 seconds ago            Exited (0) 6 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1515\nfb8cad37eb9c        k8s.gcr.io/pause:3.1   \"/pause\"                 8 seconds ago            Exited (0) 7 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1508\n633cdd7266a7        k8s.gcr.io/pause:3.1   \"/pause\"                 8 seconds ago            Exited (0) 7 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1514\nf2a20ec7588c        k8s.gcr.io/pause:3.1   \"/pause\"                 9 seconds ago            Exited (0) 8 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1513\nf185cae55cfb        k8s.gcr.io/pause:3.1   \"/pause\"                 9 seconds ago            Exited (0) 8 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1507\na088f481902a        k8s.gcr.io/pause:3.1   \"/pause\"                 10 seconds ago           Exited (0) 9 seconds ago                                k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1512\n682c09da3e80        k8s.gcr.io/pause:3.1   \"/pause\"                 10 seconds ago           Exited (0) 9 seconds ago                                k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1506\n6f7420ec4d56        k8s.gcr.io/pause:3.1   \"/pause\"                 11 seconds ago           Exited (0) 10 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1505\nfd27fb711987        k8s.gcr.io/pause:3.1   \"/pause\"                 11 seconds ago           Exited (0) 10 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1511\n0512624609b2        k8s.gcr.io/pause:3.1   \"/pause\"                 12 seconds ago           Exited (0) 11 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1504\n474e5ecad1d2        k8s.gcr.io/pause:3.1   \"/pause\"                 12 seconds ago           Exited (0) 11 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1510\n7a9a981b9351        k8s.gcr.io/pause:3.1   \"/pause\"                 13 seconds ago           Exited (0) 12 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1503\ncaff8bc0d65c        k8s.gcr.io/pause:3.1   \"/pause\"                 13 seconds ago           Exited (0) 13 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1509\na2201e43c624        k8s.gcr.io/pause:3.1   \"/pause\"                 14 seconds ago           Exited (0) 13 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1502\n34219b8788cb        k8s.gcr.io/pause:3.1   \"/pause\"                 14 seconds ago           Exited (0) 14 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1508\ndac2e43f193d        k8s.gcr.io/pause:3.1   \"/pause\"                 15 seconds ago           Exited (0) 14 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1507\n4a3eb2e8d96b        k8s.gcr.io/pause:3.1   \"/pause\"                 16 seconds ago           Exited (0) 15 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1501\na2df34006889        k8s.gcr.io/pause:3.1   \"/pause\"                 17 seconds ago           Exited (0) 16 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1506\n48dd35cfa0b8        k8s.gcr.io/pause:3.1   \"/pause\"                 17 seconds ago           Exited (0) 16 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1500\ndb0e2f608438        k8s.gcr.io/pause:3.1   \"/pause\"                 18 seconds ago           Exited (0) 17 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1505\ne7d5b5b357ee        k8s.gcr.io/pause:3.1   \"/pause\"                 18 seconds ago           Exited (0) 17 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1499\nef98353f9e2c        k8s.gcr.io/pause:3.1   \"/pause\"                 19 seconds ago           Exited (0) 18 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1504\na68ee413a143        k8s.gcr.io/pause:3.1   \"/pause\"                 19 seconds ago           Exited (0) 18 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1498\nd8ac70534d20        k8s.gcr.io/pause:3.1   \"/pause\"                 20 seconds ago           Exited (0) 19 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1497\na2175e909e69        k8s.gcr.io/pause:3.1   \"/pause\"                 20 seconds ago           Exited (0) 19 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1503\n3e11faa198f6        k8s.gcr.io/pause:3.1   \"/pause\"                 21 seconds ago           Exited (0) 20 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1502\naff6aafe64c3        k8s.gcr.io/pause:3.1   \"/pause\"                 22 seconds ago           Exited (0) 20 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1496\n5355ed246d45        k8s.gcr.io/pause:3.1   \"/pause\"                 23 seconds ago           Exited (0) 22 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1495\na89ea7a1268b        k8s.gcr.io/pause:3.1   \"/pause\"                 23 seconds ago           Exited (0) 21 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1501\n4820f8760791        k8s.gcr.io/pause:3.1   \"/pause\"                 24 seconds ago           Exited (0) 23 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1494\ne75ebfbfc557        k8s.gcr.io/pause:3.1   \"/pause\"                 24 seconds ago           Exited (0) 23 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1500\nac9753b04b28        k8s.gcr.io/pause:3.1   \"/pause\"                 25 seconds ago           Exited (0) 24 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1493\nf22928ff2541        k8s.gcr.io/pause:3.1   \"/pause\"                 26 seconds ago           Exited (0) 25 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1492\n46fa07b02398        k8s.gcr.io/pause:3.1   \"/pause\"                 26 seconds ago           Exited (0) 25 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1499\n218ac00a8f0d        k8s.gcr.io/pause:3.1   \"/pause\"                 27 seconds ago           Exited (0) 26 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1491\n41c16f2d4574        k8s.gcr.io/pause:3.1   \"/pause\"                 27 seconds ago           Exited (0) 26 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1498\n609f046d265d        k8s.gcr.io/pause:3.1   \"/pause\"                 28 seconds ago           Exited (0) 27 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1490\n86723f547525        k8s.gcr.io/pause:3.1   \"/pause\"                 29 seconds ago           Exited (0) 28 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1497\ne142d90a5400        k8s.gcr.io/pause:3.1   \"/pause\"                 29 seconds ago           Exited (0) 28 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1489\na608b882c687        k8s.gcr.io/pause:3.1   \"/pause\"                 30 seconds ago           Exited (0) 29 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1496\nd8babda49694        k8s.gcr.io/pause:3.1   \"/pause\"                 31 seconds ago           Exited (0) 30 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1488\n989b36513067        k8s.gcr.io/pause:3.1   \"/pause\"                 31 seconds ago           Exited (0) 30 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1495\naee63f3263d9        k8s.gcr.io/pause:3.1   \"/pause\"                 32 seconds ago           Exited (0) 32 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1494\n65a1ee0840d3        k8s.gcr.io/pause:3.1   \"/pause\"                 32 seconds ago           Exited (0) 32 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1487\ne447512fb15f        k8s.gcr.io/pause:3.1   \"/pause\"                 33 seconds ago           Exited (0) 33 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1493\n64292796e7b9        k8s.gcr.io/pause:3.1   \"/pause\"                 34 seconds ago           Exited (0) 33 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1492\n04d356fa1c6d        k8s.gcr.io/pause:3.1   \"/pause\"                 35 seconds ago           Exited (0) 34 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1486\nb18d3d152075        k8s.gcr.io/pause:3.1   \"/pause\"                 36 seconds ago           Exited (0) 35 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1491\n12a791b9ac7b        k8s.gcr.io/pause:3.1   \"/pause\"                 37 seconds ago           Exited (0) 36 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1490\n15f6b7cc3b5d        k8s.gcr.io/pause:3.1   \"/pause\"                 37 seconds ago           Exited (0) 36 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1485\ne4ee063afd29        k8s.gcr.io/pause:3.1   \"/pause\"                 38 seconds ago           Exited (0) 37 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1489\n7fe83f18f95e        k8s.gcr.io/pause:3.1   \"/pause\"                 38 seconds ago           Exited (0) 37 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1484\nafba15ff242a        k8s.gcr.io/pause:3.1   \"/pause\"                 39 seconds ago           Exited (0) 38 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1488\nd20927867473        k8s.gcr.io/pause:3.1   \"/pause\"                 40 seconds ago           Exited (0) 39 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1487\n8ad5794df76d        k8s.gcr.io/pause:3.1   \"/pause\"                 40 seconds ago           Exited (0) 39 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1483\n4e8ea53690c3        k8s.gcr.io/pause:3.1   \"/pause\"                 41 seconds ago           Exited (0) 40 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1486\nc81c3465736e        k8s.gcr.io/pause:3.1   \"/pause\"                 42 seconds ago           Exited (0) 41 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1485\n38d3b22275f5        k8s.gcr.io/pause:3.1   \"/pause\"                 43 seconds ago           Exited (0) 42 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1482\n9f62fc818e94        k8s.gcr.io/pause:3.1   \"/pause\"                 44 seconds ago           Exited (0) 43 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1484\na1ba719ea71b        k8s.gcr.io/pause:3.1   \"/pause\"                 45 seconds ago           Exited (0) 44 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1481\n1d9e60422c2e        k8s.gcr.io/pause:3.1   \"/pause\"                 46 seconds ago           Exited (0) 45 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1483\nbf9c1461cbb3        k8s.gcr.io/pause:3.1   \"/pause\"                 47 seconds ago           Exited (0) 46 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1480\n8bdc53364dea        k8s.gcr.io/pause:3.1   \"/pause\"                 48 seconds ago           Exited (0) 47 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1482\n5f1fefb15e91        k8s.gcr.io/pause:3.1   \"/pause\"                 49 seconds ago           Exited (0) 48 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1479\n6f787d13c236        k8s.gcr.io/pause:3.1   \"/pause\"                 50 seconds ago           Exited (0) 49 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1481\nc083d6b66ace        k8s.gcr.io/pause:3.1   \"/pause\"                 51 seconds ago           Exited (0) 50 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1478\n5847f2d5cfca        k8s.gcr.io/pause:3.1   \"/pause\"                 52 seconds ago           Exited (0) 51 seconds ago                               k8s_POD_coredns-78fcdf6894-gxzvk_kube-system_60f23ec4-a396-11e8-aaf5-88882f0da0b5_1477\n4ce826d4506e        k8s.gcr.io/pause:3.1   \"/pause\"                 52 seconds ago           Exited (0) 51 seconds ago                               k8s_POD_coredns-78fcdf6894-qt2zq_kube-system_60f2d08e-a396-11e8-aaf5-88882f0da0b5_1480\nf4410baaf5c3        f0fad859c909           \"cp -f /etc/kube-f...\"   26 minutes ago           Exited (0) 26 minutes ago                               k8s_install-cni_kube-flannel-ds-amd64-7xphb_kube-system_b9181d5d-a396-11e8-aaf5-88882f0da0b5_0\n1da7df55b781        f0fad859c909           \"/opt/bin/flanneld...\"   32 minutes ago           Up 32 minutes                                           k8s_kube-flannel_kube-flannel-ds-amd64-7xphb_kube-system_b9181d5d-a396-11e8-aaf5-88882f0da0b5_0\n. ",
    "sebt3": "Confirmed, this issue is because the kernel module \"vxlan\" is missing. Compile and load it to have flannel working. ",
    "adtrytech": "Hi Lazywhite,\nI got information from command \"kubectl describe pod\" and there is a IP filed point to the cluster IP address. So, I think that it uses the same physical interface of the cluster IP.\nIf I were wrong, please correct me. Thank you!\nRegards,\nEric. ",
    "atooki": "@SyCode7 are you logged in as root or a regular user? You will get that problem if logged in as root. (Well I did anyway). ",
    "tejp1002": "I am trying to set up a single node Kubernetes Cluster from https://ninetaillabs.com/setting-up-a-single-node-kubernetes-cluster/ and getting error dial tcp 127.0.0.1:6443: connect: connection refused\nwhen kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml is executed.\nI have tried different yml files \nhttps://raw.githubusercontent.com/coreos/flannel/master/Documentation/k8s-manifests/kube-flannel-rbac.yml\nhttps://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\nbut every time it is dial tcp 127.0.0.1:6443: connect: connection refused.\nThanks\n. ",
    "vnlinuser": "My problem has been solved: because I dropped the udp protocol in the firewall policy. ",
    "perrymana": "Looks like we are experiencing exactly the same issue with Flannel and CentOS. Manually re-ordering the rules so those 2 ACCEPT rules are above the REJECT rule (or turning the firewall off!) resolves the problem.\nDid you find a workaround? I was about to see if I could manually declare some firewall rules to resolve it.. ",
    "benjackwhite": "We are also experiencing the exact same issue with almost identical configuration values. Any luck finding a solution @luca-esse ? Is it an actual issue or was there simply something wrong with the config?. ",
    "luca-esse": "Hi @benjackwhite, unfortunatly i didn't found any solution or the cause of the problem. ",
    "sgmacdougall": "I'd like to check etcd to see what the podCIDRs look like in there, but I don't know how to do that or if its even possible.. ",
    "MrEcco": "I do not know why this is happening, but I was able to reproduce this bug. Experimentally, I discovered that there is no configuration /etc/cni/net.d/ on the bad nodes.\nMay be solution:\nCopy /etc/cni/net.d/ from master and manualy paste to bad nodes. Configs applyed immediately and you can test intercluster network.. Every time what flannel not work on node I use this (on node):\n```bash\nmkdir -p /etc/cni/net.d\ncd /etc/cni/net.d\nThis is zipped cni config which must be deployed by flannel-pod, but\nwasnt deployed by unknow reason\ncat << EOF | openssl base64 -d | xz -d > 10-flannel.conflist\n/Td6WFoAAATm1rRGAgAhARwAAAAQz1jM4AEKAJFdAD2CgBccLouJnyT/6A8zPtZS\nxLRFcjIbx3pn6UV/UpoPAEjPLRmPz8u5fwxtKGvSxeMWHNVeyJ2Vpb491DXaBjHk\nhP/DcMJyv+4mJL330vZDjgFq9OUqbVG0Nx6n6BAMRfhEYAqrhEcyjIQJVsTAgWVi\nODNmTWnAm3vdSjAtesWbiM+PR2FP/IK0cGdsy1VvzDQAAAAAXN3PLZF7zbAAAa0B\niwIAAAkaa2KxxGf7AgAAAAAEWVo=\nEOF\nAfter this i see flannel-pods in Creating statusbash\nwatch -n1 kubectl get pods --all-namespaces\n. Should.\nI have this in work cluster:bash\nroot@kube-master:/var/lib/cni# find .\n./flannel\n./flannel/<64_hex_symbols>\n./flannel/\n./networks\n./networks/cbr0\n./networks/cbr0/10.244.0.4\n./networks/cbr0/last_reserved_ip.0\n./networks/cbr0/10.244.0.5\n```\nAre you sure you turned off selinux? May be you use custom iptables policyes? Or this is problem with connection between datacenters? After https://github.com/coreos/flannel/issues/1039#issuecomment-435896167 are you see flannel pods in kube-system namespace? Nodes is resolvable by their hostnames? . ",
    "lanoxx": "I have one master and one worker node. For me, the cni0 interface is missing on the master node, while it is being created on the worker node. Flannel is running on both nodes and reports no errors but I cannot get any network traffic across the nodes using the overlay IPs because of the missing cni0 interface on the master node.. @MrEcco This configuration is present under /etc/cni/net.d/10-flannel.conflist on my master node but still there is no cni0 interface.. I just noticed that /var/lib/cni/ does not exist on my master node. Shouldn't that be created by flannel?. I am running this on Ubuntu 18.04 which does not have selinux installed or enabled by default. I also did not add any iptables policies by my self.\nI can see that for each node a flannel pod is running:\nubuntu@ip-172-33-1-142:~$ kubectl get pods -n kube-system -o wide\nNAME                                  READY   STATUS     RESTARTS   IP             NODE\nkube-flannel-ds-amd64-knnmh           1/1     Running    0          172.33.1.142   ip-172-33-1-142   \nkube-flannel-ds-amd64-vqp2v           1/1     Running    0          172.33.1.188   ip-172-33-1-188   \nkube-flannel-ds-msgdj                 1/1     Running    0          172.33.1.188   ip-172-33-1-188   \nkube-flannel-ds-xhjwk                 1/1     Running    0          172.33.1.142   ip-172-33-1-142\n\nThe master (.142) and worker (.188) nodes can ping each other by IP and also by hostname.\nOn the master node there is no cni folder under /var/lib:\n# on master node:\nubuntu@ip-172-33-1-142:~$ cd /var/lib/cni\n-bash: cd: /var/lib/cni: No such file or directory\n\nOn the worker node the folder exists and has the flannel and network subfolders as in your find . output.. I made some progress on this today. I had only one pod running on the master and it was configured with hostNetwork: true. As soon as I set this to hostNetwork: false and redeployed the pod, flannel started to create the cni0 interface.\nNow I have a cni0 interface on my master node, but I am unable to communicate across nodes using the overlay network.\nMy master has 10.244.0.0/24 while my worker node has 10.244.1.0/24. I can ping pods from my master node using the masters' overlay subnet (e.g. 10.244.0.x) and I can ping pods from my worker node using the worker node's overlay subnet (e.g. 10.244.1.x). But I cannot get any traffic (e.g pings or even HTTP) across the overlay network. So I cannot reach a pod's http server on the worker node from my master node using the overlay IP of the pod.. Solved that final issue too, the port 8472 was not open in my AWS security group which is needed for VXLAN.. ",
    "vainu-arto": "Apparently there was already PR #1001, which I hadn't noticed. That one has the options hard-coded, this version detects support in the underlying iptables binary. Otherwise the results are the same.. ",
    "thxCode": "@rakelkar \n\nrebase to master\nupdate deps\n. @rajatchopra , addressed. @madhanrm , Added.. @ksubrmnn , expectedBridgeEndpoint is valued by existingBridgeEndpoint on top.. Got, I will replace expectedBridgeEndpoint after expectedBridgeEndpoint.Create() in line 213 to satisfy that.. @rajatchopra , @ksubrmnn  Addressed.. \n",
    "song-jiang": "@thxCode @rakelkar I was testing my cluster setup ( AWS, Windows_Server-1803-English-Core-Containers, flannel host-gw) with PR https://github.com/coreos/flannel/pull/921 . I found management IP will not show up for newly created subnet unless X.X.X.2 endpoint been created and attached to host. I made some changes here https://github.com/song-jiang/flannel/commit/d8c0eacd6d73f1a2837262fced847aace7eee800 with which the management ip issue has been fixed.  \nSo my question is \n- Is my fix reasonable?\n- Does this PR address such issue?\nThanks.. Hmm... The patch works for me. Not sure waiting for 5 seconds would be enough or not. If you get more results, please let me know.. @thxCode @rajatchopra Could you take a look? Thanks.. @ksubrmnn Thanks for your suggestion. However, with my setup (AWS Windows server 1803 with Containers), after waiting for two minutes, newNetwork.ManagementIP still has length 0. \nIf I create and attach endpoint to cbr0 network before checking management ip, everything start working. . @ksubrmnn I can confirm the host did receive an IP before starting flannel process. I used powershell script similar to https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/start.ps1 with small modifications. \nThe flanneld process is started after\n- flanneld.exe binary downloaded from internet.\n- Docker images pulled.\n- windows node registered to kube-apiserver. \n- vSwitch created.. @ksubrmnn I will try to reproduce and come back to you. Could you email your contact information to song@tigera.io for me to send you access credentials? Thanks.. ",
    "benmoss": "\n@thxCode is this based on the work in #921 and/or #922? Your commit history doesn't include either of those PRs\n\nYeah, I am fairly certain this is the same code just with some of the feedback from that and this thread addressed. If people care about keeping those original commits in the history I'd be happy to send another PR, but if it's all the same I'd just move ahead with this one.. Yeah, I think the ipsec backend just barely missed getting merged before the 0.10.0 release was cut. @philips / @tomdee  any plans for a new release? The new Windows backends also haven't seen an official release yet.. i'd recommend using kubectl port-forward to achieve things like this. i wouldn't recommend trying to run Flannel on your dev machine, it messes with a lot of system networking settings that you probably don't want changed.. if all the PRs are as useful as the earlier one, i'd vote no on this. Thanks Kalya, I started digging in more and realized it is the external network and not the container subnet as it is on L2Bridge. I updated my issue to try to describe the problem better.. ",
    "Balonno": "usign the toleration without a key worked for me. Would this be the solution?. ",
    "jmyung": "thanks @outcoldman. it helps :). ",
    "adhipati-blambangan": "thanks @outcoldman ! it works like a charm. ;). ",
    "mauilion": "Flannel should probably set\ntolerations:\n        - operator: Exists\nas the default tolerations set. This will ensure that the flannel ds tolerates all taints.\n. ",
    "ReSearchITEng": "For anyone willing to test the flannel fix for 1.12 , \nkubectl -n kube-system apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml. Sorry, it was a typo, it's kubectl.\nFor those interested, k8s 1.12 deployment with all the goodies (ingress, dashboard, optional vsphere*, etc) automated with ansible and maintained here: github.com/ReSearchITEng/kubeadm-playbook/\nThe above has been scripted there as well.. I confirm that such a change fixes the issue.\n@mauilion : is there anything else we can do to see this merged ?\n. For anyone willing to test the flannel fix for 1.12 further, \nkubeadm -n kube-system apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml. ",
    "NerdyShawn": "\nFor anyone willing to test the flannel fix for 1.12 ,\nkubeadm -n kube-system apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\n\ntrying on a pi2 b+ master\n`HypriotOS/armv7: root@piNode01 in ~\nkubeadm -n kube-system apply -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\nError: unknown command \"apply\" for \"kubeadm\"\nRun 'kubeadm --help' for usage.\nerror: unknown command \"apply\" for \"kubeadm\"\n`. So that got me closer, but still no dice, here is the docker output the apiserver container seems no bueno. Sorry I'm struggling with text formatting so here is the screenshot.\n\n. ",
    "cablespaghetti": "Try with kubectl rather than kubeadm. I think that was a typo.\nUnfortunately I don't have a 1.12 cluster at the moment. I'll test this\nwhen I do though.\nOn Fri, 5 Oct 2018, 16:25 NerdyShawn, notifications@github.com wrote:\n\nFor anyone willing to test the flannel fix for 1.12 ,\nkubeadm -n kube-system apply -f\nhttps://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\ntrying on a pi2 b+ master\n`HypriotOS/armv7: root@piNode01 in ~\nkubeadm -n kube-system apply -f\nhttps://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\nError: unknown command \"apply\" for \"kubeadm\"\nRun 'kubeadm --help' for usage.\nerror: unknown command \"apply\" for \"kubeadm\"\n`\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/1044#issuecomment-427403833,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKoi5ounjV64Fz2gZD0SGvXbTrK-4JHxks5uh3nagaJpZM4W9xBu\n.\n. Hi @NerdyShawn,\n\nI don't think you've got your kubectl configured correctly to connect to your cluster. As it seems like @rberg2 has managed to get this working, maybe it would be good to continue this on one of the support channels like slack rather than this issue.. What's currently on master works just fine. But yes a release would be\ngood. :)\nOn Sat, 26 Jan 2019, 01:27 vmendi <notifications@github.com wrote:\n\nThanks!\nWasn't it fix here? 13a990b\nhttps://github.com/coreos/flannel/commit/13a990bb716c82a118b8e825b78189dcfbfb2f1e\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/coreos/flannel/issues/1044#issuecomment-457788178,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AKoi5lm86HZYEaJtGADmS0Ry7cd2179aks5vG67zgaJpZM4W9xBu\n.\n. \n",
    "rberg2": "Hello,\nI can confirm this fixes flannel on my 1.12 test cluster. https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\n. ",
    "tallaxes": "@ReSearchITEng, confirm works (1.12.1). \nThe link to ansible playbook is broken.. ",
    "hegdedarsh": "Hello,\nEven with the tolerations, it still says , i used the below link to run the flannel\nhttps://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml\nPlease find the output of the pods:-\n[user@darshan-p-hegde-89ca8c531 ~]$ kubectl get pods -n kube-system\nNAME                                                                READY   STATUS              RESTARTS   AGE\ncoredns-576cbf47c7-9r27x                                            0/1     ContainerCreating   0          6m\ncoredns-576cbf47c7-qc4tm                                            0/1     ContainerCreating   0          6m\netcd-darshan-p-hegde-89ca8c531.mylabserver.com                      1/1     Running             0          4m54s\nkube-apiserver-darshan-p-hegde-89ca8c531.mylabserver.com            1/1     Running             0          5m2s\nkube-controller-manager-darshan-p-hegde-89ca8c531.mylabserver.com   1/1     Running             0          5m2s\nkube-flannel-ds-amd64-gm5z7                                         0/1     CrashLoopBackOff    5          4m56s\nkube-proxy-mbtcj                                                    1/1     Running             0          6m\nkube-scheduler-darshan-p-hegde-89ca8c531.mylabserver.com            1/1     Running             0          5m13s\nI have described the flannel pod and and the output is below:-\nName:               kube-flannel-ds-amd64-gm5z7\nNamespace:          kube-system\nPriority:           0\nPriorityClassName:  \nNode:               darshan-p-hegde-89ca8c531.mylabserver.com/172.31.42.12\nStart Time:         Sun, 07 Oct 2018 06:37:31 +0000\nLabels:             app=flannel\n                    controller-revision-hash=6697bf5fc6\n                    pod-template-generation=1\n                    tier=node\nAnnotations:        \nStatus:             Running\nIP:                 172.31.42.12\nControlled By:      DaemonSet/kube-flannel-ds-amd64\nInit Containers:\n  install-cni:\n    Container ID:  docker://b085e4a7d80b26730dc795d4a72b8a278ddc4ba71e5c463bfcd0172b793de349\n    Image:         quay.io/coreos/flannel:v0.10.0-amd64\n    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:88f2b4d96fae34bfff3d46293f7f18d1f9f3ca026b4a4d288f28347fcb6580ac\n    Port:          \n    Host Port:     \n    Command:\n      cp\n    Args:\n      -f\n      /etc/kube-flannel/cni-conf.json\n      /etc/cni/net.d/10-flannel.conflist\n    State:          Terminated\n      Reason:       Completed\n      Exit Code:    0\n      Started:      Sun, 07 Oct 2018 06:37:33 +0000\n      Finished:     Sun, 07 Oct 2018 06:37:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    \n    Mounts:\n      /etc/cni/net.d from cni (rw)\n      /etc/kube-flannel/ from flannel-cfg (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-llwn4 (ro)\nContainers:\n  kube-flannel:\n    Container ID:  docker://a8096a56009a0566b53e4b0aac09430b75120979e63dbe32eb8ed91053666a77\n    Image:         quay.io/coreos/flannel:v0.10.0-amd64\n    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:88f2b4d96fae34bfff3d46293f7f18d1f9f3ca026b4a4d288f28347fcb6580ac\n    Port:          \n    Host Port:     \n    Command:\n      /opt/bin/flanneld\n    Args:\n      --ip-masq\n      --kube-subnet-mgr\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    1\n      Started:      Sun, 07 Oct 2018 06:43:46 +0000\n      Finished:     Sun, 07 Oct 2018 06:43:48 +0000\n    Ready:          False\n    Restart Count:  6\n    Limits:\n      cpu:     100m\n      memory:  50Mi\n    Requests:\n      cpu:     100m\n      memory:  50Mi\n    Environment:\n      POD_NAME:       kube-flannel-ds-amd64-gm5z7 (v1:metadata.name)\n      POD_NAMESPACE:  kube-system (v1:metadata.namespace)\n    Mounts:\n      /etc/kube-flannel/ from flannel-cfg (rw)\n      /run from run (rw)\n      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-llwn4 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             False \n  ContainersReady   False \n  PodScheduled      True \nVolumes:\n  run:\n    Type:          HostPath (bare host directory volume)\n    Path:          /run\n    HostPathType:\n  cni:\n    Type:          HostPath (bare host directory volume)\n    Path:          /etc/cni/net.d\n    HostPathType:\n  flannel-cfg:\n    Type:      ConfigMap (a volume populated by a ConfigMap)\n    Name:      kube-flannel-cfg\n    Optional:  false\n  flannel-token-llwn4:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  flannel-token-llwn4\n    Optional:    false\nQoS Class:       Guaranteed\nNode-Selectors:  beta.kubernetes.io/arch=amd64\nTolerations:     :NoSchedule\n                 node.kubernetes.io/disk-pressure:NoSchedule\n                 node.kubernetes.io/memory-pressure:NoSchedule\n                 node.kubernetes.io/network-unavailable:NoSchedule\n                 node.kubernetes.io/not-ready:NoExecute\n                 node.kubernetes.io/unreachable:NoExecute\n                 node.kubernetes.io/unschedulable:NoSchedule\nEvents:\n  Type     Reason     Age                    From                                                Message\n  ----     ------     ----                   ----                                                -------\n  Normal   Scheduled  6m57s                  default-scheduler                                   Successfully assigned kube-system/kube-flannel-ds-amd64-gm5z7 to darshan-p-hegde-89ca8c531.mylabserver.com\n  Normal   Pulling    6m57s                  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  pulling image \"quay.io/coreos/flannel:v0.10.0-amd64\"\n  Normal   Pulled     6m55s                  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Successfully pulled image \"quay.io/coreos/flannel:v0.10.0-amd64\"\n  Normal   Created    6m55s                  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Created container\n  Normal   Started    6m55s                  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Started container\n  Normal   Started    6m5s (x4 over 6m53s)   kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Started container\n  Normal   Pulled     5m11s (x5 over 6m54s)  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Container image \"quay.io/coreos/flannel:v0.10.0-amd64\" already present on machine\n  Normal   Created    5m11s (x5 over 6m53s)  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Created container\n  Warning  BackOff    105s (x23 over 6m48s)  kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Back-off restarting failed container\nPlease find the output of the coreos pods:-\nWarning  FailedCreatePodSandBox  7m50s                    kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container \"5f6770d9dfcb53738a0dd428b86e815d4d85e9b71a76d17b10b1f764f102fb61\" network for pod \"coredns-576cbf47c7-9r27x\": NetworkPlugin cni failed to set up pod \"coredns-576cbf47c7-9r27x_kube-system\" network: open /run/flannel/subnet.env: no such file or directory\n  Warning  FailedCreatePodSandBox  7m49s                    kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container \"009e9e0099f993086300649a89995a28a0fdf1a128863f7a71e3ff1973788c26\" network for pod \"coredns-576cbf47c7-9r27x\": NetworkPlugin cni failed to set up pod \"coredns-576cbf47c7-9r27x_kube-system\" network: open /run/flannel/subnet.env: no such file or directory\n  Warning  FailedCreatePodSandBox  7m48s                    kubelet, darshan-p-hegde-89ca8c531.mylabserver.com  Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container \"ea0ddaf5c411dd026cfd23366e49424526b7cc547652ca262a346f4c800f0c04\" network for pod \"coredns-576cbf47c7-9r27x\": NetworkPlugin cni failed to set up pod \"coredns-576cbf47c7-9r27x_kube-system\" network: open /run/flannel/subnet.env: no such file or directory\n. ",
    "pkeuter": "This fixes the issue for me. Thanks for the PR!. ",
    "telecodani": "Adding the toleration in the Flannel yaml works for me also. Tested on v1.12.1 Kubernetes. Thanks. . ",
    "benn0r": "I am using the yaml file recommended in this issue. But for me nodePort and \"externalIps\" doesn't work anymore unless its from the same node that the pods are located on. If i try to telnet via the master ip i get a timeout. \nthis is since the upgrade to kubernetes 1.12.\nIs this a problem with flannel?. ",
    "sarlacpit": "I am on a fresh install of k8s 1.12 and have just tried downloading v0.10 and the tolerations seem to exist already. So I applied the yml \ntolerations:\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\nIt tried creating the flannel pod but came up with 'Error' and eventually \"CrashLoopBackOff\".\nStill very new to k8s. Any debug I can provide let me know.\n. ",
    "bitva77": "just here to say that using https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml with the toleration's set as the below works on Kubernetes 1.12.3 with kubeadm install:\ntolerations:\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n        effect: NoSchedule\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n        effect: NoSchedule. ",
    "sa9226": "Thanks . it worked for me after applying above changes to flannel config to v1.12.3. . ",
    "vmendi": "There hasn't been a release of flannel for a year and we need to upgrade to Kubernetes 1.12.\nAre there plans to have a new release anytime soon? If not, it's not a problem, we can always branch and fix it ourselves.\nThanks. Thanks!\nWasn't it fixed here? https://github.com/coreos/flannel/commit/13a990bb716c82a118b8e825b78189dcfbfb2f1e. I can certify that with the latest release v0.11.0, flannel works with kubernetes 1.12.5 out of the box :). ",
    "neolit123": "@tomdee @philips this is resulting in k8s reports. could you please take a look. :+1: . ",
    "girikuncoro": "I'm also facing this issue, at the moment which flannel image to use if I want to enable ipsec backend? I tried both quay.io/coreos/flannel:v0.10.0-amd64 and quay.io/coreos/flannel:v0.9.1-amd64, same error\nEdit: I think it's only available on master branch at the moment, I was checking the latest tag branch, it's not there.. ",
    "andreek": "charon.conf: install_routes=yes solves the problem.. ",
    "scm-ns": "It works ! . ",
    "zenglian": "Everyting works fine now,  after applying the above steps plus a reboot.\nReboot should not be a must, maybe restart network would do the trick.\n```\n$ k get po -o=wide\nNAME                         READY     STATUS    RESTARTS   AGE       IP             NODE\nh2-75cb7756c6-r4gkj          1/1       Running   0          5m        192.168.1.14   slave1\nh2-75cb7756c6-xfstk          1/1       Running   0          16m       192.168.0.5    master\njobserver-58bf6985f9-77mdd   1/1       Running   0          16m       192.168.0.6    master\njobserver-58bf6985f9-h9hlx   1/1       Running   0          5m        192.168.1.15   slave1\nping pod on slave\n$ ping 192.168.1.14\nPING 192.168.1.14 (192.168.1.14) 56(84) bytes of data.\n64 bytes from 192.168.1.14: icmp_seq=1 ttl=63 time=0.454 ms\nping pod on master\n$ ping 192.168.0.5\nPING 192.168.0.5 (192.168.0.5) 56(84) bytes of data.\n64 bytes from 192.168.0.5: icmp_seq=1 ttl=64 time=0.143 ms\nping docker container on the same node\n$ ping 172.18.0.2  \nPING 172.18.0.2 (172.18.0.2): 56 data bytes\n64 bytes from 172.18.0.2: seq=0 ttl=241 time=21.580 ms\n. ",
    "halfcrazy": "Currently flannel does not support etcd v3 API, use \"--kube-subnet-mgr\" instead.. ",
    "b3nw": "Flannel version: v0.10.0\nBackend used (e.g. vxlan or udp): vxlan\nEtcd version: 3.2.24\nKubernetes version (if used): 1.13.1\nOperating System and version: Debian 9.6 on Raspberry Pi 3 B. ",
    "jmeridth": "same setup as @b3nw.  Only difference is I'm using HypriotOS 1.9.0 on Raspberry Pi 3 B+. ",
    "mr-sour": "Got this same thing on hypriotos 1.10.0-rc2 on a raspberry Pi 3 B+. ",
    "bobmhong": "\nGot this same thing on hypriotos 1.10.0-rc2 on a raspberry Pi 3 B+\n\n@mr-sour My config is same as yours and have same the results.  I'm glad at least that sudo ip link delete flannel.1 on the failing node allows the pod to recreate successfully after deleting the failing pod.. ",
    "toothbrush": "Hi again!  Just asking for your advice, not wanting to badger you.\nSince we're experiencing this issue in our production systems, there's some pressure from rest of our team to move ahead before a real flannel release.  Our two options are:\n\nrun master (which i lean towards), or\ncherry-pick the fix onto the release tag we're currently using in production, v0.9.1.\n\nCan you see anything catastrophically bad about either of those options?  Either one sound preferable?  I don't want to be the guy who's nagging volunteer developers for free stuff, so, um, sorry if it sounds that way!. Cool, thanks for the progress report :). ",
    "JohPa8696": "Getting the same issue deploying kubernetes 1.12.2. It was working 1 week ago.\n```\nWaiting for the Network to be created\nI1129 16:07:23.313913    3220 main.go:450] Searching for interface using 10.204.35.207\nWaiting for the Network to be created\nI1129 16:07:24.439939    3220 main.go:527] Using interface with name vEthernet (Ethernet) and address 10.204.35.207\nI1129 16:07:24.443546    3220 main.go:544] Defaulting external address to interface address (10.204.35.207)\nI1129 16:07:24.455545    3220 kube.go:126] Waiting 10m0s for node controller to sync\nI1129 16:07:24.459540    3220 kube.go:309] Starting kube subnet manager\nWaiting for the Network to be created\nI1129 16:07:25.481545    3220 kube.go:133] Node controller sync successful\nI1129 16:07:25.488548    3220 main.go:244] Created subnet manager: Kubernetes Subnet Manager - kubewindowsminion1\nI1129 16:07:25.489547    3220 main.go:247] Installing signal handlers\nI1129 16:07:25.492547    3220 main.go:386] Found network config - Backend type: host-gw\nI1129 16:07:25.494540    3220 hostgw_windows.go:73] HOST-GW config: {Name:cbr0 DNSServerList:}\nI1129 16:07:25.640833    3220 hostgw_windows.go:157] Attempting to create HNSNetwork {\"Name\":\"cbr0\",\"Type\":\"L2Bridge\",\"Subnets\":[{\"AddressPrefix\":\"10.244.5.0/24\",\"GatewayAddress\":\"10.244.5.1\"}]}\nI1129 16:07:25.666035    3220 hostgw_windows.go:164] Waiting to get ManagementIP from HNSNetwork cbr0\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xc0000005 code=0x0 addr=0x18 pc=0x10e43ea]\ngoroutine 1 [running]:\nmain.main()\n        /home/kasubra/repo/gopath/src/github.com/coreos/flannel/main.go:297 +0xbfa\n```. ",
    "titilambert": "Hello,\nI applied this patch but I still have an issue. The network interface doesn't get an IP.\nShould I force it ?\nHere the output of flannel:\n...\nI1130 20:11:28.512420    3760 kube.go:133] Node controller sync successful                                 \nI1130 20:11:28.512420    3760 main.go:244] Created subnet manager: Kubernetes Subnet Manager - dmn0sueik6l2\n4d6                                                                                                        \nI1130 20:11:28.513406    3760 main.go:247] Installing signal handlers                                      \nI1130 20:11:28.513406    3760 main.go:386] Found network config - Backend type: host-gw                    \nI1130 20:11:28.513406    3760 hostgw_windows.go:73] HOST-GW config: {Name:cbr0 DNSServerList:}             \nI1130 20:11:28.529410    3760 hostgw_windows.go:157] Attempting to create HNSNetwork {\"Name\":\"cbr0\",\"Type\":\n\"L2Bridge\",\"Subnets\":[{\"AddressPrefix\":\"10.201.18.0/24\",\"GatewayAddress\":\"10.201.18.1\"}]}                  \nI1130 20:11:28.535418    3760 hostgw_windows.go:163] Created HNSNetwork cbr0                               \nI1130 20:11:28.539408    3760 hostgw_windows.go:192] Attempting to create bridge HNSEndpoint &{Id: Name:cbr\n0_ep VirtualNetwork:0dfc5298-3449-4843-975c-4dfccdcb537e VirtualNetworkName: Policies:[] MacAddress: IPAddr\ness:10.201.18.2 DNSSuffix: DNSServerList: GatewayAddress: EnableInternalDNS:false DisableICC:false PrefixLe\nngth:0 IsRemoteEndpoint:false Namespace:<nil>}                                                             \nI1130 20:11:28.545420    3760 hostgw_windows.go:197] Created bridge HNSEndpoint cbr0_ep                    \nI1130 20:11:28.545420    3760 hostgw_windows.go:201] Waiting to attach bridge endpoint cbr0_ep to host     \nI1130 20:11:29.321437    3760 hostgw_windows.go:209] Attached bridge endpoint cbr0_ep to host successfully \nI1130 20:11:29.336431    3760 hostgw_windows.go:212] Waiting to get ManagementIP from HNSNetwork cbr0      \nE1130 20:11:34.356677    3760 main.go:289] Error registering network: timeout, failed to get management IP \nfrom HNSNetwork cbr0: timed out waiting for the condition                                                  \nI1130 20:11:34.357665    3760 main.go:366] Stopping shutdownHandler.... I confirm 5 seconds is too low, I put 25 seconds and It's working. I'm working on Azure, maybe that explains the delay.... ",
    "barnettZQG": "microsoft doc show:\nl2bridge:\n      Requires: When this mode is used in a virtualization scenario (container host is a VM) MAC address spoofing is required.\nMy environment is a VM, I tried to change the waiting time to one minute, but the system still could not allocate IP. Whether it is related to the appeal description or not is not supported.. If the IP is not allocated after a certain period of time, it is absolutely useful to restart the machine.. ",
    "davemeier": "Is the flanneld.exe updated anywhere for the master flannel code, or do I need to build it myself?\n[Edit] I have flanneld.exe building successfully now - https://github.com/coreos/flannel/issues/1081. @JohnJAS - I get the same problem in my environment. I'm working in AWS and my windows node is an 1803 instance. I had to modify the Get-MgmtSubnet function in SDN start-kubelet.ps1 as well, as that code was failing. I am still getting the \"Error registering network: timeout, failed to get management IP from HNSNetwork cbr0: timed out waiting for the condition\" problem.\nThere is no \"ManagementIP\" element showing when I look at cbr0:\nPS C:\\k> Get-HnsNetwork -Id 207cd47b-66af-419f-98c2-c0699927f9b5\nActivityId             : 16605ab1-ba02-4009-a984-375bded9b3cd\nCurrentEndpointCount   : 2\nExtensions             : {@{Id=e7c3b2f0-f3c5-48df-af2b-10fed6d72e7a; IsEnabled=False; Name=Microsoft Windows Filtering Platform}, @{Id=e9b59cfa-2be1-4b21-828f-b6fbdbddc017; IsEnabled=True; Name=Microsoft Azure VFP Switch\n                         Extension}, @{Id=ea24cd6c-d17a-4348-9190-09f0d5be83dd; IsEnabled=False; Name=Microsoft NDIS Capture}}\nID                     : 207cd47b-66af-419f-98c2-c0699927f9b5\nLayerResources         : @{AllocationOrder=3; Allocators=System.Object[]; ID=146d1cbc-74d4-4437-ac1e-8cee81dff07b; PortOperationTime=0; State=1; SwitchOperationTime=0; VfpOperationTime=0;\n                         parentId=00000000-0000-0000-0000-000000000000}\nLayeredOn              : 7383cb85-bde7-452f-acdb-0e728689a8d5\nMacPools               : {@{EndMacAddress=00-15-5D-F5-DF-FF; StartMacAddress=00-15-5D-F5-D0-00}}\nMaxConcurrentEndpoints : 2\nName                   : cbr0\nPolicies               : {}\nResources              : @{AllocationOrder=0; ID=16605ab1-ba02-4009-a984-375bded9b3cd; PortOperationTime=0; State=1; SwitchOperationTime=0; VfpOperationTime=0; parentId=146d1cbc-74d4-4437-ac1e-8cee81dff07b}\nState                  : 1\nSubnets                : {@{AddressPrefix=10.244.1.0/24; GatewayAddress=10.244.1.1}}\nTotalEndpoints         : 2\nType                   : L2Bridge\nVersion                : 30064771074. After moving to Windows 1809 on AWS, I no longer have any problems with flannel or with the cbr0 network. In addition to changing platforms, I also have made sure that my linux and windows EC2 instances are on the same subnet, plus I have disabled source/destination checks. I found these suggestions on the following web page: https://rancher.com/docs/rancher/v2.x/en/cluster-provisioning/rke-clusters/windows-clusters/\nMy windows pods are now working correctly and can be accessed successfully through the service.. After going through the building.md doc I realized I had installed the wrong go package, gccgo-go. After doing what the doc said (sudo apt-get install linux-libc-dev golang gcc) I was able to build the exe successfully.. Ran collectlogs.ps1 and attached here:\napp27_cbr0_wrong_diskette_logs_2.zip\n. ",
    "JohnJAS": "I haven't see the patch for flanneld.exe. And I failed to build a flannel.exe according to the building.md.\nIs there any walkround for this issue? Or is there anywhere I can get the patched flanneld.exe?\n[Edit] I created a PR to enhance the document about how to build the flanned.exe. https://github.com/coreos/flannel/pull/1089. > Hello,\n\nI applied this patch but I still have an issue. The network interface doesn't get an IP.\nShould I force it ?\nHere the output of flannel:\n...\nI1130 20:11:28.512420    3760 kube.go:133] Node controller sync successful                                 \nI1130 20:11:28.512420    3760 main.go:244] Created subnet manager: Kubernetes Subnet Manager - dmn0sueik6l2\n4d6                                                                                                        \nI1130 20:11:28.513406    3760 main.go:247] Installing signal handlers                                      \nI1130 20:11:28.513406    3760 main.go:386] Found network config - Backend type: host-gw                    \nI1130 20:11:28.513406    3760 hostgw_windows.go:73] HOST-GW config: {Name:cbr0 DNSServerList:}             \nI1130 20:11:28.529410    3760 hostgw_windows.go:157] Attempting to create HNSNetwork {\"Name\":\"cbr0\",\"Type\":\n\"L2Bridge\",\"Subnets\":[{\"AddressPrefix\":\"10.201.18.0/24\",\"GatewayAddress\":\"10.201.18.1\"}]}                  \nI1130 20:11:28.535418    3760 hostgw_windows.go:163] Created HNSNetwork cbr0                               \nI1130 20:11:28.539408    3760 hostgw_windows.go:192] Attempting to create bridge HNSEndpoint &{Id: Name:cbr\n0_ep VirtualNetwork:0dfc5298-3449-4843-975c-4dfccdcb537e VirtualNetworkName: Policies:[] MacAddress: IPAddr\ness:10.201.18.2 DNSSuffix: DNSServerList: GatewayAddress: EnableInternalDNS:false DisableICC:false PrefixLe\nngth:0 IsRemoteEndpoint:false Namespace:<nil>}                                                             \nI1130 20:11:28.545420    3760 hostgw_windows.go:197] Created bridge HNSEndpoint cbr0_ep                    \nI1130 20:11:28.545420    3760 hostgw_windows.go:201] Waiting to attach bridge endpoint cbr0_ep to host     \nI1130 20:11:29.321437    3760 hostgw_windows.go:209] Attached bridge endpoint cbr0_ep to host successfully \nI1130 20:11:29.336431    3760 hostgw_windows.go:212] Waiting to get ManagementIP from HNSNetwork cbr0      \nE1130 20:11:34.356677    3760 main.go:289] Error registering network: timeout, failed to get management IP \nfrom HNSNetwork cbr0: timed out waiting for the condition                                                  \nI1130 20:11:34.357665    3760 main.go:366] Stopping shutdownHandler...\n\n+1 Same issue after using the patched flanneld.exe.. > After going through the building.md doc I realized I had installed the wrong go package, gccgo-go. After doing what the doc said (sudo apt-get install linux-libc-dev golang gcc) I was able to build the exe successfully.\nHi, I can't compile the fluentd.exe in Ubuntu either. I found that the golang version is to low to compile.\nIt throws out the following error.\n'vendor/k8s.io/utils/exec/exec.go:20:2: cannot find package \"context\" in any of:\n        /root/go/src/github.com/coreos/flannel/vendor/context (vendor tree)\n        /usr/lib/go-1.6/src/context (from $GOROOT)\n        /root/go/src/context (from $GOPATH)\n'\nI wonder how do you solve this compile problem if you have met this issue? . @rajatchopra Sure, I just create a PR for it. Maybe it can be some help for you.\nRefer to: https://github.com/coreos/flannel/pull/1089. Yeah, it's a typo error. \nMeanwhile, for Fedora/Redhat, I think the mingw-w64 is also needed, but I only test on Ubuntu. Maybe someone who is in charge of the document can improve this section too. . Yeah. You can just edit it or another PR from me?. ",
    "daddylhz": "Flannel doesn't monitor the state of host network. You should restart flannel if you restart network. \u597d\u50cf\u6ca1\u505a\u8fd9\u4e2a. ",
    "heidini": "I also got the same same issue for coredns in CrashLoopBackOff state:\nosboxes@kubemaster:~$ kubectl get pods -n kube-system\nNAME                                 READY   STATUS             RESTARTS   AGE\ncoredns-86c58d9df4-jrtdv             0/1     CrashLoopBackOff   2          17h\ncoredns-86c58d9df4-zfwn2             0/1     CrashLoopBackOff   3          17h\netcd-kubemaster                      1/1     Running            0          17h\nkube-apiserver-kubemaster            1/1     Running            0          17h\nkube-controller-manager-kubemaster   1/1     Running            0          17h\nkube-flannel-ds-amd64-s7g8r          1/1     Running            0          77s\nkube-proxy-rgp9p                     1/1     Running            0          17h\nkube-scheduler-kubemaster            1/1     Running            0          17h\nMy environment:\nFlannel: 0.10.0\nKubernetes version:   Client Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.1\", GitCommit:\"eec55b9ba98609a46fee712359c7b5b365bdd920\", GitTreeState:\"clean\", BuildDate:\"2018-12-13T10:39:04Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.1\", GitCommit:\"eec55b9ba98609a46fee712359c7b5b365bdd920\", GitTreeState:\"clean\", BuildDate:\"2018-12-13T10:31:33Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nOperating System and version: Ubuntu 16.04 LTS ppc64le\n. I also tried to apply the v0.10.0 yaml, but got pending state:\nosboxes@kubemaster:~$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml\nclusterrole.rbac.authorization.k8s.io/flannel created\nclusterrolebinding.rbac.authorization.k8s.io/flannel created\nserviceaccount/flannel created\nconfigmap/kube-flannel-cfg created\ndaemonset.extensions/kube-flannel-ds created\nosboxes@kubemaster:~$ kubectl get pods --all-namespaces\nNAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE\nkube-system   coredns-86c58d9df4-jrtdv             0/1     Pending   0          17h\nkube-system   coredns-86c58d9df4-zfwn2             0/1     Pending   0          17h\nkube-system   etcd-kubemaster                      1/1     Running   0          17h\nkube-system   kube-apiserver-kubemaster            1/1     Running   0          17h\nkube-system   kube-controller-manager-kubemaster   1/1     Running   0          17h\nkube-system   kube-proxy-rgp9p                     1/1     Running   0          17h\nkube-system   kube-scheduler-kubemaster            1/1     Running   0          17h. ",
    "langerkunal": "/sig. ",
    "armanriazi": "I applied it again with this config but anything didn't change :\n```\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - pods\n    verbs:\n      - get\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes/status\n    verbs:\n      - patch\n\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: flannel\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: flannel\nsubjects:\n- kind: ServiceAccount\n  name: flannel\n  namespace: kube-system\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: flannel\n  namespace: kube-system\n\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: kube-flannel-cfg\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\ndata:\n  cni-conf.json: |\n    {\n      \"name\": \"cbr0\",\n      \"plugins\": [\n        {\n          \"type\": \"flannel\",\n          \"delegate\": {\n            \"hairpinMode\": true,\n            \"isDefaultGateway\": true\n          }\n        },\n        {\n          \"type\": \"portmap\",\n          \"capabilities\": {\n            \"portMappings\": true\n          }\n        }\n      ]\n    }\n  net-conf.json: |\n    {\n      \"Network\": \"10.244.0.0/16\",\n      \"Backend\": {\n        \"Type\": \"vxlan\"\n      }\n    }\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: amd64\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-amd64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-amd64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-arm64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: arm64\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-arm64\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-arm64\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-arm\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: arm\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-arm\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-arm\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-ppc64le\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: ppc64le\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-ppc64le\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-ppc64le\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n\napiVersion: extensions/v1beta1\nkind: DaemonSet\nmetadata:\n  name: kube-flannel-ds-s390x\n  namespace: kube-system\n  labels:\n    tier: node\n    app: flannel\nspec:\n  template:\n    metadata:\n      labels:\n        tier: node\n        app: flannel\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        beta.kubernetes.io/arch: s390x\n      tolerations:\n      - operator: Exists\n        effect: NoSchedule\n      serviceAccountName: flannel\n      initContainers:\n      - name: install-cni\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-s390x\n        command:\n        - cp\n        args:\n        - -f\n        - /etc/kube-flannel/cni-conf.json\n        - /etc/cni/net.d/10-flannel.conflist\n        volumeMounts:\n        - name: cni\n          mountPath: /etc/cni/net.d\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      containers:\n      - name: kube-flannel\n        image: 172.18.3.9:5000/coreos/flannel:v0.10.0-s390x\n        command:\n        - /opt/bin/flanneld\n        args:\n        - --ip-masq\n        - --kube-subnet-mgr\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n          limits:\n            cpu: \"100m\"\n            memory: \"50Mi\"\n        securityContext:\n          privileged: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: run\n          mountPath: /run\n        - name: flannel-cfg\n          mountPath: /etc/kube-flannel/\n      volumes:\n        - name: run\n          hostPath:\n            path: /run\n        - name: cni\n          hostPath:\n            path: /etc/cni/net.d\n        - name: flannel-cfg\n          configMap:\n            name: kube-flannel-cfg\n```. ",
    "michaelfig": "I had a similar problem that was caused by updating kubelets to 1.13 without properly running kubeadm upgrade plan and kubeadm upgrade apply....\nReverting to 1.12, deleting the flannel daemonset and then doing the upgrade and applying the flannel yaml made it all work again.\nHope this helps someone,\nMichael.. ",
    "vroomvroomvroom": "appears accurate .. this fixed me up:    sudo ln -s /etc/cni/net.d/10-flannel.conf /etc/cni/net.d/10-flannel.conflist. ",
    "pain400": "@bogd Are there many differences between flannel and weave? I remember that they all run in layer2 mode.. ",
    "bitbucket90": "I believe this could be a repo issue; however canal does (I am not versed in canal, do they include the drivers(bins) in their image or pull from their own site. We are using custom repos. . Confirm you have the cni drivers in /opt/cni if not, download them from their git and untar. If that is your issue. It seems like that may have been a flimsy install thus not a real issue, just something on the host side typically the cni plugin should install its drivers. Still couldn't get flannel to on any host of cent7 still worth looking into I guess. . ",
    "hussainaphroj": "Hi,\nI am getting similar issue on CentOS7. Could you please guide me resolve this.\nkubelet.go:2192] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: netwo\nFeb 05 06:25:10 master kubelet[12435]: E0205 06:25:10.239839   12435 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup\nFeb 05 06:25:10 master kubelet[12435]: E0205 06:25:10.240756   12435 summary_sys_containers.go:47] Failed to get system container stats for \"/system.slice/kubelet.service\": failed to get cgrou\nFeb 05 06:25:12 master kubelet[12435]: E0205 06:25:12.990881   12435 kuberuntime_manager.go:719] init container start failed: ImagePullBackOff: Back-off pulling image \"quay.io/coreos/flannel:v\nFeb 05 06:25:12 master kubelet[12435]: E0205 06:25:12.990908   12435 pod_workers.go:190] Error syncing pod 9503841c-290a-11e9-991b-52540075dc3d (\"kube-flannel-ds-amd64-s95dn_kube-system(950384\nFeb 05 06:25:13 master kubelet[12435]: W0205 06:25:13.653716   12435 cni.go:203] Unable to update cni config: No networks found in /etc/cni/net.d\nFeb 05 06:25:13 master kubelet[12435]: E0205 06:25:13.653840   12435 kubelet.go:2192] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: netwo\nAny help would be appreciated. ",
    "pierreozoux": "Ok, I understand now it was a RedHat statement and not necessarily a community statement :)\nGreat, I trust you and your enthusiasm :) do you come to fosdem or know anyone going?\nCheers!. ",
    "mosaicwang": "additional:\nI searched, other people also met this question, and someone says change flannel to calico, then solve the problem.so this question is only about flannel.\nhttps://github.com/kubernetes/kubeadm/issues/193. I install flannel step:\n1.download flannel_v0.10.0-amd64.tar and kube-flannel.yam\n2. docker load --input flannel_v0.10.0-amd64.tar (because of GFW block )\n3. in k8s master node : kubectl apply -f kube-flannel.yam\n. ",
    "drpaneas": "Is there anything else to do? Can we merge this?. > > Is there anything else to do? Can we merge this?\n\n@drpaneas I think we should maintain a kube-flannel.yaml for older versions. Do you mind doing that within this PR? Call the old one old-kube-flannel.yaml or pre198-kube-flannel.yaml or something, I will update the Documentation indicating on the file to be used for older deployments. Thanks.\n\nsure, no problem. done :) . Removing the beta I end up with:\nError from server (BadRequest): error when creating \"STDIN\": PodSecurityPolicy in version \"v1\" cannot be handled as a PodSecurityPolicy: no kind \"PodSecurityPolicy\" is registered for version \"extensions/v1\"\nI've also noticed that you are using a really old kubernetes version when it comes to testing:\nK8S_VERSION=\"${K8S_VERSION:-1.7.6} ---> ./dist/functional-test-k8s.sh\nAs a result, the TravisCI was failing due to missing features related to PSP. I had to bump the version in the e2e tests to the latest stable one (1.13.2). However this up to you, I don't want to be too intrusive with this PR, just raise the security concerns.\n. I've also noticed in the logs of the TravisCI:\n```\n--cap-add=NET_ADMIN --cap-add=SYS_ADMIN\nNET_ADMIN capacity is required to do some network operation\nSYS_ADMIN capacity is required to create network namespace\n```\nGiven that hostNetwork: true do think that SYS_ADMIN is still needed? I could bootstrap a cluster, run some basic tests and deploy some apps -- doing all those without using SYS_ADMIN capability. However, if you think this needs to be part of the YAML and the PSP, please clarify so I can modify the PR.. No the SYS_ADMIN is not needed to write stuff into /run/flannel (I have already changed the path in this PR) as it can RW already without any problem. I have tested it multiple times to make sure of it.\n```\nFrom the container (/run)\nbash-4.3# ls -l /run/\ntotal 4\ndrwxr-xr-x 2 root root 60 Feb  4 13:14 flannel\ndrwxr-xr-x 4 root root 80 Feb  4 12:56 secrets\n-rw-r--r-- 1 root root 96 Feb  4 13:07 subnet.env\nFrom the host (/run/flannel)\nmaster-0:/run/flannel # ls -l\ntotal 4\ndrwxr-xr-x 2 root root 60 Feb  4 12:57 flannel\ndrwxr-xr-x 4 root root 80 Feb  4 12:57 secrets\n-rw-r--r-- 1 root root 96 Feb  4 01:02 subnet.env\n```\nAs for the older version that can be supported will this YAML that is 1.9.8. I have modified the Makefile and the version for the tests accordingly in this PR.. ",
    "PatrickKutch": "I have the exact same issue, though I'm running 1.13.0.  Calico works fine, but wanted to use flannel.  just trying a simple pod to pod ping with no luck.. I am on Ubuntu 18.  I am glad you got it working for yourself, however since I've many nodes in my cluster, with different networking devices on them, I don't see how specifying a device will work for me :-(. Nope.  None of my 20 boxes have that.  Depends on what slot NIC sits and some other stuff.. ",
    "emilywwj": "I met the same problem here. Should we use this kube-flannel.yml file or not? How should we config the flannel.. > > I met the same problem here. Should we use this kube-flannel.yml file or not? How should we config the flannel.\n\nActually I have messed up with the weave and the flannel and reinstalled the flannel after completely removing the weavenet\nI got that working\n\nI fix it too. I figured that my pods used weavenet already but I didn't know. That's why I failed to install kube-flannel here.. ",
    "phanisowjanyavutukuri": "\nthe kube-flannel.yml from documentation? To deploy using daemonset?\n\nyes I have used the same from the documentation\n. > I met the same problem here. Should we use this kube-flannel.yml file or not? How should we config the flannel.\nActually I  have messed up with the weave and the flannel and reinstalled the flannel after completely removing the weavenet \nI got that working. ",
    "mullikine": "Hi coreos and flannel contributers,\nShane from CodeLingo here. This PR is testing out a new service. I'd love to hear your feedback or answer your questions. We're keen to learn how we can best help dev teams without getting in your way.\nShane & the CodeLingo Team. ",
    "hellobinge": "@rajatchopra thanks rajat, I will do some more research. ",
    "xuefengedu": "it seems a problem of RBAC which seems not setting in my cluster of 3 boxes.\nneed to investigate more, don't know the reason now.. Fixed using kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml, just share it.. latest kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml seems has problem, I got above command from https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ , and it works fine.\nthere are many difference changes: https://www.diffchecker.com/Fr9rkyny. I also don't know the real reason, but it seems there is some setting outside my cluster affected the resource RBAC.. mine works fine now:kubernetes-admin@kubernetes, but it seems @unixetisalat  still has issue.. ",
    "unixetisalat": "I am also getting the same error, is there any solution ?\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterroles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRole\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"rules\":[map[\"verbs\":[\"get\"] \"apiGroups\":[\"\"] \"resources\":[\"pods\"]] map[\"apiGroups\":[\"\"] \"resources\":[\"nodes\"] \"verbs\":[\"list\" \"watch\"]] map[\"resources\":[\"nodes/status\"] \"verbs\":[\"patch\"] \"apiGroups\":[\"\"]]] \"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRole\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": clusterroles.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRoleBinding\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"roleRef\":map[\"apiGroup\":\"rbac.authorization.k8s.io\" \"kind\":\"ClusterRole\" \"name\":\"flannel\"] \"subjects\":[map[\"kind\":\"ServiceAccount\" \"name\":\"flannel\" \"namespace\":\"kube-system\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": clusterrolebindings.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"\nName: \"flannel\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"name\":\"flannel\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"apiVersion\":\"v1\" \"kind\":\"ServiceAccount\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": serviceaccounts \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"kube-system\": can only create tokens for individual service accounts\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=configmaps\", GroupVersionKind: \"/v1, Kind=ConfigMap\"\nName: \"kube-flannel-cfg\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"v1\" \"data\":map[\"cni-conf.json\":\"{\\n  \\\"name\\\": \\\"cbr0\\\",\\n  \\\"plugins\\\": [\\n    {\\n      \\\"type\\\": \\\"flannel\\\",\\n      \\\"delegate\\\": {\\n        \\\"hairpinMode\\\": true,\\n        \\\"isDefaultGateway\\\": true\\n      }\\n    },\\n    {\\n      \\\"type\\\": \\\"portmap\\\",\\n      \\\"capabilities\\\": {\\n        \\\"portMappings\\\": true\\n      }\\n    }\\n  ]\\n}\\n\" \"net-conf.json\":\"{\\n  \\\"Network\\\": \\\"10.244.0.0/16\\\",\\n  \\\"Backend\\\": {\\n    \\\"Type\\\": \\\"vxlan\\\"\\n  }\\n}\\n\"] \"kind\":\"ConfigMap\" \"metadata\":map[\"name\":\"kube-flannel-cfg\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": configmaps \"kube-flannel-cfg\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no path found to object\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-amd64\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-amd64\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"amd64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]] \"name\":\"POD_NAMESPACE\"]] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-amd64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm64\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm64\" \"namespace\":\"kube-system\"] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"memory\":\"50Mi\" \"cpu\":\"100m\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm\", Namespace: \"kube-system\"\nObject: &{map[\"kind\":\"DaemonSet\" \"metadata\":map[\"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm\" \"namespace\":\"kube-system\"] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"name\":\"flannel-cfg\" \"configMap\":map[\"name\":\"kube-flannel-cfg\"]]] \"containers\":[map[\"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]] \"name\":\"POD_NAME\"] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]]]]] \"apiVersion\":\"extensions/v1beta1\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-ppc64le\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"name\":\"kube-flannel-ds-ppc64le\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"template\":map[\"spec\":map[\"containers\":[map[\"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"install-cni\"]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"ppc64le\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]]] \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]] \"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-ppc64le\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-s390x\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-s390x\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"name\":\"cni\" \"hostPath\":map[\"path\":\"/etc/cni/net.d\"]] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"install-cni\"]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"s390x\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-s390x\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\n[root@k8s-master bin]#\n[root@k8s-master bin]# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:08:12Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n[root@k8s-master bin]#\n[root@k8s-master bin]#\n[root@k8s-master bin]# uname -a\nLinux k8s-master 3.10.0-862.14.4.el7.x86_64 #1 SMP Fri Sep 21 09:07:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n[root@k8s-master bin]#\n. Tried both , but still getting the error.\n[root@k8s-master rsd]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterroles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRole\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRole\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"rules\":[map[\"apiGroups\":[\"\"] \"resources\":[\"pods\"] \"verbs\":[\"get\"]] map[\"apiGroups\":[\"\"] \"resources\":[\"nodes\"] \"verbs\":[\"list\" \"watch\"]] map[\"apiGroups\":[\"\"] \"resources\":[\"nodes/status\"] \"verbs\":[\"patch\"]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": clusterroles.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"roleRef\":map[\"kind\":\"ClusterRole\" \"name\":\"flannel\" \"apiGroup\":\"rbac.authorization.k8s.io\"] \"subjects\":[map[\"kind\":\"ServiceAccount\" \"name\":\"flannel\" \"namespace\":\"kube-system\"]] \"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRoleBinding\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": clusterrolebindings.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"\nName: \"flannel\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"v1\" \"kind\":\"ServiceAccount\" \"metadata\":map[\"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"name\":\"flannel\" \"namespace\":\"kube-system\"]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": serviceaccounts \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"kube-system\": can only create tokens for individual service accounts\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=configmaps\", GroupVersionKind: \"/v1, Kind=ConfigMap\"\nName: \"kube-flannel-cfg\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"labels\":map[\"tier\":\"node\" \"app\":\"flannel\"] \"name\":\"kube-flannel-cfg\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"apiVersion\":\"v1\" \"data\":map[\"cni-conf.json\":\"{\\n  \\\"name\\\": \\\"cbr0\\\",\\n  \\\"plugins\\\": [\\n    {\\n      \\\"type\\\": \\\"flannel\\\",\\n      \\\"delegate\\\": {\\n        \\\"hairpinMode\\\": true,\\n        \\\"isDefaultGateway\\\": true\\n      }\\n    },\\n    {\\n      \\\"type\\\": \\\"portmap\\\",\\n      \\\"capabilities\\\": {\\n        \\\"portMappings\\\": true\\n      }\\n    }\\n  ]\\n}\\n\" \"net-conf.json\":\"{\\n  \\\"Network\\\": \\\"10.244.0.0/16\\\",\\n  \\\"Backend\\\": {\\n    \\\"Type\\\": \\\"vxlan\\\"\\n  }\\n}\\n\"] \"kind\":\"ConfigMap\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": configmaps \"kube-flannel-cfg\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no path found to object\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-amd64\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-amd64\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"initContainers\":[map[\"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"amd64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"operator\":\"Exists\" \"effect\":\"NoSchedule\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"memory\":\"50Mi\" \"cpu\":\"100m\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]]]] \"hostNetwork\":%!q(bool=true)]]] \"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-amd64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm64\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm64\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm64\"] \"serviceAccountName\":\"flannel\"]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"containers\":[map[\"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"kube-flannel\" \"resources\":map[\"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-ppc64le\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-ppc64le\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"containers\":[map[\"resources\":map[\"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"name\":\"flannel-cfg\" \"mountPath\":\"/etc/kube-flannel/\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"kube-flannel\"]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"ppc64le\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-ppc64le\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-s390x\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-s390x\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"containers\":[map[\"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\"]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"s390x\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"name\":\"run\" \"hostPath\":map[\"path\":\"/run\"]] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-s390x\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\n[root@k8s-master rsd]#\n[root@k8s-master rsd]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=podsecuritypolicies\", GroupVersionKind: \"extensions/v1beta1, Kind=PodSecurityPolicy\"\nName: \"psp.flannel.unprivileged\", Namespace: \"\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"PodSecurityPolicy\" \"metadata\":map[\"annotations\":map[\"apparmor.security.beta.kubernetes.io/allowedProfileNames\":\"runtime/default\" \"apparmor.security.beta.kubernetes.io/defaultProfileName\":\"runtime/default\" \"seccomp.security.alpha.kubernetes.io/allowedProfileNames\":\"docker/default\" \"seccomp.security.alpha.kubernetes.io/defaultProfileName\":\"docker/default\" \"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"name\":\"psp.flannel.unprivileged\"] \"spec\":map[\"hostNetwork\":%!q(bool=true) \"privileged\":%!q(bool=false) \"readOnlyRootFilesystem\":%!q(bool=false) \"allowPrivilegeEscalation\":%!q(bool=false) \"defaultAddCapabilities\":[] \"defaultAllowPrivilegeEscalation\":%!q(bool=false) \"fsGroup\":map[\"rule\":\"RunAsAny\"] \"hostIPC\":%!q(bool=false) \"hostPorts\":[map[\"max\":'\\uffff' \"min\":'\\x00']] \"hostPID\":%!q(bool=false) \"requiredDropCapabilities\":[] \"supplementalGroups\":map[\"rule\":\"RunAsAny\"] \"seLinux\":map[\"rule\":\"RunAsAny\"] \"volumes\":[\"configMap\" \"secret\" \"emptyDir\" \"hostPath\"] \"allowedCapabilities\":[\"NET_ADMIN\"] \"allowedHostPaths\":[map[\"pathPrefix\":\"/etc/cni/net.d\"] map[\"pathPrefix\":\"/etc/kube-flannel\"] map[\"pathPrefix\":\"/run/flannel\"]] \"runAsUser\":map[\"rule\":\"RunAsAny\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": podsecuritypolicies.extensions \"psp.flannel.unprivileged\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"podsecuritypolicies\" in API group \"extensions\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterroles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRole\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"kind\":\"ClusterRole\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"rules\":[map[\"verbs\":[\"use\"] \"apiGroups\":[\"extensions\"] \"resourceNames\":[\"psp.flannel.unprivileged\"] \"resources\":[\"podsecuritypolicies\"]] map[\"apiGroups\":[\"\"] \"resources\":[\"pods\"] \"verbs\":[\"get\"]] map[\"resources\":[\"nodes\"] \"verbs\":[\"list\" \"watch\"] \"apiGroups\":[\"\"]] map[\"verbs\":[\"patch\"] \"apiGroups\":[\"\"] \"resources\":[\"nodes/status\"]]] \"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": clusterroles.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRoleBinding\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"roleRef\":map[\"apiGroup\":\"rbac.authorization.k8s.io\" \"kind\":\"ClusterRole\" \"name\":\"flannel\"] \"subjects\":[map[\"kind\":\"ServiceAccount\" \"name\":\"flannel\" \"namespace\":\"kube-system\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": clusterrolebindings.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"\nName: \"flannel\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"name\":\"flannel\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"apiVersion\":\"v1\" \"kind\":\"ServiceAccount\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": serviceaccounts \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"kube-system\": can only create tokens for individual service accounts\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=configmaps\", GroupVersionKind: \"/v1, Kind=ConfigMap\"\nName: \"kube-flannel-cfg\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"v1\" \"data\":map[\"cni-conf.json\":\"{\\n  \\\"name\\\": \\\"cbr0\\\",\\n  \\\"plugins\\\": [\\n    {\\n      \\\"type\\\": \\\"flannel\\\",\\n      \\\"delegate\\\": {\\n        \\\"hairpinMode\\\": true,\\n        \\\"isDefaultGateway\\\": true\\n      }\\n    },\\n    {\\n      \\\"type\\\": \\\"portmap\\\",\\n      \\\"capabilities\\\": {\\n        \\\"portMappings\\\": true\\n      }\\n    }\\n  ]\\n}\\n\" \"net-conf.json\":\"{\\n  \\\"Network\\\": \\\"10.244.0.0/16\\\",\\n  \\\"Backend\\\": {\\n    \\\"Type\\\": \\\"vxlan\\\"\\n  }\\n}\\n\"] \"kind\":\"ConfigMap\" \"metadata\":map[\"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-cfg\" \"namespace\":\"kube-system\"]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": configmaps \"kube-flannel-cfg\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no path found to object\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-amd64\", Namespace: \"kube-system\"\nObject: &{map[\"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-amd64\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"volumes\":[map[\"hostPath\":map[\"path\":\"/run/flannel\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"volumeMounts\":[map[\"mountPath\":\"/run/flannel\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"capabilities\":map[\"add\":[\"NET_ADMIN\"]] \"privileged\":%!q(bool=false)]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"name\":\"flannel-cfg\" \"mountPath\":\"/etc/kube-flannel/\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"amd64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]]]]] \"apiVersion\":\"extensions/v1beta1\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-amd64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm64\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm64\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"spec\":map[\"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run/flannel\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"capabilities\":map[\"add\":[\"NET_ADMIN\"]] \"privileged\":%!q(bool=false)] \"volumeMounts\":[map[\"mountPath\":\"/run/flannel\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"hostNetwork\":%!q(bool=true)] \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"spec\":map[\"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run/flannel\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"capabilities\":map[\"add\":[\"NET_ADMIN\"]] \"privileged\":%!q(bool=false)] \"volumeMounts\":[map[\"mountPath\":\"/run/flannel\" \"name\":\"run\"] map[\"name\":\"flannel-cfg\" \"mountPath\":\"/etc/kube-flannel/\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm\"]] \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-ppc64le\", Namespace: \"kube-system\"\nObject: &{map[\"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"capabilities\":map[\"add\":[\"NET_ADMIN\"]] \"privileged\":%!q(bool=false)] \"volumeMounts\":[map[\"mountPath\":\"/run/flannel\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"ppc64le\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"operator\":\"Exists\" \"effect\":\"NoSchedule\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run/flannel\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]]]]] \"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-ppc64le\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-ppc64le\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-s390x\", Namespace: \"kube-system\"\nObject: &{map[\"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"volumes\":[map[\"hostPath\":map[\"path\":\"/run/flannel\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"memory\":\"50Mi\" \"cpu\":\"100m\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"capabilities\":map[\"add\":[\"NET_ADMIN\"]] \"privileged\":%!q(bool=false)] \"volumeMounts\":[map[\"name\":\"run\" \"mountPath\":\"/run/flannel\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"s390x\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]]]]] \"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"name\":\"kube-flannel-ds-s390x\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-s390x\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\n[root@k8s-master rsd]#\n. @rajatchopra  iam using 1.13.3. \n[root@k8master ~]# kubectl config current-context\nsystem:node:k8master@kubernetes\n[root@k8master ~]#\n[root@k8master ~]# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.0\", GitCommit:\"ddf47ac13c1a9483ea035a79cd7c10005ff21a6d\", GitTreeState:\"clean\", BuildDate:\"2018-12-03T21:04:45Z\", GoVersion:\"go1.11.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n[root@k8master ~]#\n. \n@rajatchopra \nGetting below error as well.\n[root@k8master ~]# kubectl taint nodes --all node-role.kubernetes.io/master-\nError from server (Forbidden): nodes \"k8master\" is forbidden: node \"k8master\" cannot modify taints\n[root@k8master ~]#. Installation of pod network add-on getting failed with below error.\n[root@k8s-master bin]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterroles\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRole\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"rules\":[map[\"verbs\":[\"get\"] \"apiGroups\":[\"\"] \"resources\":[\"pods\"]] map[\"apiGroups\":[\"\"] \"resources\":[\"nodes\"] \"verbs\":[\"list\" \"watch\"]] map[\"resources\":[\"nodes/status\"] \"verbs\":[\"patch\"] \"apiGroups\":[\"\"]]] \"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRole\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": clusterroles.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterroles\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"rbac.authorization.k8s.io/v1beta1, Resource=clusterrolebindings\", GroupVersionKind: \"rbac.authorization.k8s.io/v1beta1, Kind=ClusterRoleBinding\"\nName: \"flannel\", Namespace: \"\"\nObject: &{map[\"apiVersion\":\"rbac.authorization.k8s.io/v1beta1\" \"kind\":\"ClusterRoleBinding\" \"metadata\":map[\"name\":\"flannel\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"roleRef\":map[\"apiGroup\":\"rbac.authorization.k8s.io\" \"kind\":\"ClusterRole\" \"name\":\"flannel\"] \"subjects\":[map[\"kind\":\"ServiceAccount\" \"name\":\"flannel\" \"namespace\":\"kube-system\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": clusterrolebindings.rbac.authorization.k8s.io \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=serviceaccounts\", GroupVersionKind: \"/v1, Kind=ServiceAccount\"\nName: \"flannel\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"name\":\"flannel\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"apiVersion\":\"v1\" \"kind\":\"ServiceAccount\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": serviceaccounts \"flannel\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"serviceaccounts\" in API group \"\" in the namespace \"kube-system\": can only create tokens for individual service accounts\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"/v1, Resource=configmaps\", GroupVersionKind: \"/v1, Kind=ConfigMap\"\nName: \"kube-flannel-cfg\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"v1\" \"data\":map[\"cni-conf.json\":\"{\\n  \\\"name\\\": \\\"cbr0\\\",\\n  \\\"plugins\\\": [\\n    {\\n      \\\"type\\\": \\\"flannel\\\",\\n      \\\"delegate\\\": {\\n        \\\"hairpinMode\\\": true,\\n        \\\"isDefaultGateway\\\": true\\n      }\\n    },\\n    {\\n      \\\"type\\\": \\\"portmap\\\",\\n      \\\"capabilities\\\": {\\n        \\\"portMappings\\\": true\\n      }\\n    }\\n  ]\\n}\\n\" \"net-conf.json\":\"{\\n  \\\"Network\\\": \\\"10.244.0.0/16\\\",\\n  \\\"Backend\\\": {\\n    \\\"Type\\\": \\\"vxlan\\\"\\n  }\\n}\\n\"] \"kind\":\"ConfigMap\" \"metadata\":map[\"name\":\"kube-flannel-cfg\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": configmaps \"kube-flannel-cfg\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no path found to object\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-amd64\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-amd64\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"amd64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]] \"name\":\"POD_NAMESPACE\"]] \"image\":\"quay.io/coreos/flannel:v0.11.0-amd64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-amd64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm64\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm64\" \"namespace\":\"kube-system\"] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm64\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"memory\":\"50Mi\" \"cpu\":\"100m\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm64\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm64\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-arm\", Namespace: \"kube-system\"\nObject: &{map[\"kind\":\"DaemonSet\" \"metadata\":map[\"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-arm\" \"namespace\":\"kube-system\"] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"name\":\"flannel-cfg\" \"configMap\":map[\"name\":\"kube-flannel-cfg\"]]] \"containers\":[map[\"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]] \"name\":\"POD_NAME\"] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-arm\" \"name\":\"install-cni\" \"volumeMounts\":[map[\"mountPath\":\"/etc/cni/net.d\" \"name\":\"cni\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"arm\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]]]]] \"apiVersion\":\"extensions/v1beta1\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-arm\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-ppc64le\", Namespace: \"kube-system\"\nObject: &{map[\"metadata\":map[\"name\":\"kube-flannel-ds-ppc64le\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"] \"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"template\":map[\"spec\":map[\"containers\":[map[\"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-ppc64le\" \"name\":\"install-cni\"]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"ppc64le\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]] \"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"hostPath\":map[\"path\":\"/etc/cni/net.d\"] \"name\":\"cni\"] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]]] \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]]]] \"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\"]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-ppc64le\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\nError from server (Forbidden): error when retrieving current configuration of:\nResource: \"extensions/v1beta1, Resource=daemonsets\", GroupVersionKind: \"extensions/v1beta1, Kind=DaemonSet\"\nName: \"kube-flannel-ds-s390x\", Namespace: \"kube-system\"\nObject: &{map[\"apiVersion\":\"extensions/v1beta1\" \"kind\":\"DaemonSet\" \"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"] \"name\":\"kube-flannel-ds-s390x\" \"namespace\":\"kube-system\" \"annotations\":map[\"kubectl.kubernetes.io/last-applied-configuration\":\"\"]] \"spec\":map[\"template\":map[\"metadata\":map[\"labels\":map[\"app\":\"flannel\" \"tier\":\"node\"]] \"spec\":map[\"volumes\":[map[\"hostPath\":map[\"path\":\"/run\"] \"name\":\"run\"] map[\"name\":\"cni\" \"hostPath\":map[\"path\":\"/etc/cni/net.d\"]] map[\"configMap\":map[\"name\":\"kube-flannel-cfg\"] \"name\":\"flannel-cfg\"]] \"containers\":[map[\"args\":[\"--ip-masq\" \"--kube-subnet-mgr\"] \"command\":[\"/opt/bin/flanneld\"] \"env\":[map[\"name\":\"POD_NAME\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.name\"]]] map[\"name\":\"POD_NAMESPACE\" \"valueFrom\":map[\"fieldRef\":map[\"fieldPath\":\"metadata.namespace\"]]]] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"kube-flannel\" \"resources\":map[\"limits\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"] \"requests\":map[\"cpu\":\"100m\" \"memory\":\"50Mi\"]] \"securityContext\":map[\"privileged\":%!q(bool=true)] \"volumeMounts\":[map[\"mountPath\":\"/run\" \"name\":\"run\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]]]] \"hostNetwork\":%!q(bool=true) \"initContainers\":[map[\"volumeMounts\":[map[\"name\":\"cni\" \"mountPath\":\"/etc/cni/net.d\"] map[\"mountPath\":\"/etc/kube-flannel/\" \"name\":\"flannel-cfg\"]] \"args\":[\"-f\" \"/etc/kube-flannel/cni-conf.json\" \"/etc/cni/net.d/10-flannel.conflist\"] \"command\":[\"cp\"] \"image\":\"quay.io/coreos/flannel:v0.11.0-s390x\" \"name\":\"install-cni\"]] \"nodeSelector\":map[\"beta.kubernetes.io/arch\":\"s390x\"] \"serviceAccountName\":\"flannel\" \"tolerations\":[map[\"effect\":\"NoSchedule\" \"operator\":\"Exists\"]]]]]]}\nfrom server for: \"https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml\": daemonsets.extensions \"kube-flannel-ds-s390x\" is forbidden: User \"system:node:k8s-master\" cannot get resource \"daemonsets\" in API group \"extensions\" in the namespace \"kube-system\"\n[root@k8s-master bin]#\nKubernete/OS -version\n[root@k8s-master bin]# kubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:08:12Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"13\", GitVersion:\"v1.13.3\", GitCommit:\"721bfa751924da8d1680787490c54b9179b1fed0\", GitTreeState:\"clean\", BuildDate:\"2019-02-01T20:00:57Z\", GoVersion:\"go1.11.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n[root@k8s-master bin]#\n[root@k8s-master bin]# uname -a\nLinux k8s-master 3.10.0-862.14.4.el7.x86_64 #1 SMP Fri Sep 21 09:07:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n[root@k8s-master bin]#\nRegards\nRashid. ",
    "IceMimosa": "From @xuefengedu , i add sudo and the error is gone:\nsudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml. ",
    "segator": "same issue here, I tried with exactly same versions you describe and happens the same, 0.9.1 works but not others,\nTried 0.10 in other hosts and works.\nOnly a single host have problems.\nWhat info you need about the machine?. ",
    "florath": "Same here - with slightly different stack trace (see below).\nUbuntu 18.04.2 LTS\nKernel: 4.15.0-46-generic\nlibc6: 2.27-3ubuntu1\nFails: 0.11.0 and 0.10.0\nWorks: 0.9.1\n```\nI0310 15:19:18.959145   24981 main.go:475] Determining IP address of default interface\nI0310 15:19:18.959431   24981 main.go:488] Using interface with name enp1s0 and address 192.168.122.75\nI0310 15:19:18.959495   24981 main.go:505] Defaulting external address to interface address (192.168.122.75)\nI0310 15:19:18.959642   24981 main.go:235] Created subnet manager: Etcd Local Manager with Previous Subnet: None\nI0310 15:19:18.959710   24981 main.go:238] Installing signal handlers\nfatal error: unexpected signal during runtime execution\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x63 pc=0x7f2d8cc20448]\nruntime stack:\nruntime.throw(0x1a7f7cf, 0x2a)\n    /usr/local/go/src/runtime/panic.go:605 +0x95\nruntime.sigpanic()\n    /usr/local/go/src/runtime/signal_unix.go:351 +0x2b8\ngoroutine 10 [syscall, locked to thread]:\nruntime.cgocall(0x15273b0, 0xc42002bde8, 0x1a7d72e)\n    /usr/local/go/src/runtime/cgocall.go:132 +0xe4 fp=0xc42002bda8 sp=0xc42002bd68 pc=0x402514\nnet._C2func_getaddrinfo(0x7f2d800008c0, 0x0, 0xc4203760f0, 0xc420482080, 0x0, 0x0, 0x0)\n    net/_obj/_cgo_gotypes.go:86 +0x5f fp=0xc42002bde8 sp=0xc42002bda8 pc=0x52081f\nnet.cgoLookupIPCNAME.func2(0x7f2d800008c0, 0x0, 0xc4203760f0, 0xc420482080, 0xc42005cc60, 0xc420349920, 0x10)\n    /usr/local/go/src/net/cgo_unix.go:151 +0x13f fp=0xc42002be40 sp=0xc42002bde8 pc=0x527dbf\nnet.cgoLookupIPCNAME(0xc420349920, 0x10, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:151 +0x175 fp=0xc42002bf38 sp=0xc42002be40 pc=0x522075\nnet.cgoIPLookup(0xc42005cde0, 0xc420349920, 0x10)\n    /usr/local/go/src/net/cgo_unix.go:203 +0x4d fp=0xc42002bfc8 sp=0xc42002bf38 pc=0x5227bd\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2337 +0x1 fp=0xc42002bfd0 sp=0xc42002bfc8 pc=0x4598f1\ncreated by net.cgoLookupIP\n    /usr/local/go/src/net/cgo_unix.go:213 +0xaf\ngoroutine 1 [select]:\ngithub.com/coreos/flannel/vendor/github.com/coreos/etcd/client.(simpleHTTPClient).Do(0xc4203cfae0, 0x7f2d8c272a08, 0xc4201d6840, 0x2838420, 0xc42033d1a0, 0x0, 0x0, 0x0, 0x0, 0x0, ...)\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/client.go:531 +0x31d\ngithub.com/coreos/flannel/vendor/github.com/coreos/etcd/client.(redirectFollowingHTTPClient).Do(0xc4201bf2e0, 0x7f2d8c272a08, 0xc4201d6840, 0x2838420, 0xc42033d1a0, 0x7ffcf7291755, 0x15, 0x0, 0x0, 0x0, ...)\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/client.go:603 +0xb2\ngithub.com/coreos/flannel/vendor/github.com/coreos/etcd/client.(httpClusterClient).Do(0xc42005c780, 0x7f2d8c272a08, 0xc4201d6840, 0x2838420, 0xc42033d1a0, 0xc420601a88, 0xc420601aa8, 0x4119c8, 0x3, 0x18b1f20, ...)\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/client.go:360 +0x36c\ngithub.com/coreos/flannel/vendor/github.com/coreos/etcd/client.(httpKeysAPI).Get(0xc4201bf240, 0x7f2d8c272a08, 0xc4201d6840, 0xc4203498e0, 0x1a, 0xc4204471c9, 0xc420404401, 0x3, 0x4)\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/keys.go:422 +0xe1\ngithub.com/coreos/flannel/subnet/etcdv2.(etcdSubnetRegistry).getNetworkConfig(0xc42033d050, 0x7f2d8c272a08, 0xc4201d6840, 0x7f2d8c272a08, 0x453cd0, 0xc420601b68, 0x1859480)\n    /go/src/github.com/coreos/flannel/subnet/etcdv2/registry.go:117 +0x125\ngithub.com/coreos/flannel/subnet/etcdv2.(LocalManager).GetNetworkConfig(0xc4201bf280, 0x7f2d8c272a08, 0xc4201d6840, 0x1, 0x28d52e0, 0x7f2d8c272a08)\n    /go/src/github.com/coreos/flannel/subnet/etcdv2/local_manager.go:88 +0x4b\nmain.getConfig(0x7f2d8c272a08, 0xc4201d6840, 0x284ffc0, 0xc4201bf280, 0xc42005c7e0, 0xc420404470, 0xc420447210)\n    /go/src/github.com/coreos/flannel/main.go:347 +0xb2\nmain.main()\n    /go/src/github.com/coreos/flannel/main.go:262 +0x595\ngoroutine 19 [syscall]:\nos/signal.signal_recv(0x0)\n    /usr/local/go/src/runtime/sigqueue.go:131 +0xa6\nos/signal.loop()\n    /usr/local/go/src/os/signal/signal_unix.go:22 +0x22\ncreated by os/signal.init.0\n    /usr/local/go/src/os/signal/signal_unix.go:28 +0x41\ngoroutine 20 [chan receive]:\ngithub.com/coreos/flannel/vendor/github.com/golang/glog.(*loggingT).flushDaemon(0x28a94c0)\n    /go/src/github.com/coreos/flannel/vendor/github.com/golang/glog/glog.go:879 +0x9f\ncreated by github.com/coreos/flannel/vendor/github.com/golang/glog.init.0\n    /go/src/github.com/coreos/flannel/vendor/github.com/golang/glog/glog.go:410 +0x203\ngoroutine 5 [select, locked to thread]:\nruntime.gopark(0x1ad26a0, 0x0, 0x1a58035, 0x6, 0x18, 0x1)\n    /usr/local/go/src/runtime/proc.go:287 +0x12c\nruntime.selectgo(0xc420027f50, 0xc4200662a0)\n    /usr/local/go/src/runtime/select.go:395 +0x1149\nruntime.ensureSigM.func1()\n    /usr/local/go/src/runtime/signal_unix.go:511 +0x220\nruntime.goexit()\n    /usr/local/go/src/runtime/asm_amd64.s:2337 +0x1\ngoroutine 6 [runnable]:\nmain.main.func1(0x7f2d8c272a08, 0xc4201d6840, 0xc42005c7e0, 0xc420404470, 0xc420447210)\n    /go/src/github.com/coreos/flannel/main.go:251\ncreated by main.main\n    /go/src/github.com/coreos/flannel/main.go:251 +0x553\ngoroutine 7 [select]:\nnet/http.(Transport).getConn(0xc420346000, 0xc42033d2f0, 0x0, 0xc4201e42a0, 0x4, 0xc420349920, 0x15, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:948 +0x5bf\nnet/http.(Transport).RoundTrip(0xc420346000, 0xc42046c400, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/http/transport.go:400 +0x6a6\ngithub.com/coreos/flannel/vendor/github.com/coreos/etcd/client.(simpleHTTPClient).Do.func1(0xc4203cfae0, 0xc42046c400, 0xc42005c8a0)\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/client.go:523 +0x41\ncreated by github.com/coreos/flannel/vendor/github.com/coreos/etcd/client.(simpleHTTPClient).Do\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/client.go:522 +0x200\ngoroutine 8 [select]:\nnet.(Resolver).LookupIPAddr(0x28a7580, 0x284d920, 0xc42005cc60, 0xc420349920, 0x10, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/lookup.go:196 +0x52b\nnet.(Resolver).internetAddrList(0x28a7580, 0x284d920, 0xc42005cc60, 0x1a55550, 0x3, 0xc420349920, 0x15, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/ipsock.go:293 +0x644\nnet.(Resolver).resolveAddrList(0x28a7580, 0x284d920, 0xc42005cc60, 0x1a55bae, 0x4, 0x1a55550, 0x3, 0xc420349920, 0x15, 0x0, ...)\n    /usr/local/go/src/net/dial.go:193 +0x594\nnet.(Dialer).DialContext(0xc42005c6c0, 0x284d8e0, 0xc420072010, 0x1a55550, 0x3, 0xc420349920, 0x15, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/dial.go:375 +0x248\nnet.(Dialer).Dial(0xc42005c6c0, 0x1a55550, 0x3, 0xc420349920, 0x15, 0x240020066660, 0x110, 0x110, 0xc420266120)\n    /usr/local/go/src/net/dial.go:320 +0x75\nnet.(Dialer).Dial-fm(0x1a55550, 0x3, 0xc420349920, 0x15, 0xc420404550, 0xc42003d998, 0x404409, 0x60)\n    /go/src/github.com/coreos/flannel/vendor/github.com/coreos/etcd/client/client.go:52 +0x52\nnet/http.(Transport).dial(0xc420346000, 0x284d8e0, 0xc420072010, 0x1a55550, 0x3, 0xc420349920, 0x15, 0x0, 0x0, 0x0, ...)\n    /usr/local/go/src/net/http/transport.go:887 +0x7b\nnet/http.(Transport).dialConn(0xc420346000, 0x284d8e0, 0xc420072010, 0x0, 0xc4201e42a0, 0x4, 0xc420349920, 0x15, 0xc420346000, 0xc42046c400, ...)\n    /usr/local/go/src/net/http/transport.go:1060 +0x1d62\nnet/http.(Transport).getConn.func4(0xc420346000, 0x284d8e0, 0xc420072010, 0xc42033d320, 0xc420066540)\n    /usr/local/go/src/net/http/transport.go:943 +0x78\ncreated by net/http.(Transport).getConn\n    /usr/local/go/src/net/http/transport.go:942 +0x393\ngoroutine 9 [select]:\nnet.cgoLookupIP(0x284d920, 0xc42005cc60, 0xc420349920, 0x10, 0xc42033d2f0, 0x0, 0xc4201e42a0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/cgo_unix.go:214 +0x1b0\nnet.(Resolver).lookupIP(0x28a7580, 0x284d920, 0xc42005cc60, 0xc420349920, 0x10, 0x0, 0x17ebf00, 0xc42033d0b0, 0x0, 0x0)\n    /usr/local/go/src/net/lookup_unix.go:95 +0x12d\nnet.(Resolver).(net.lookupIP)-fm(0x284d920, 0xc42005cc60, 0xc420349920, 0x10, 0x0, 0x0, 0x6a1d50, 0xc420346000, 0x0)\n    /usr/local/go/src/net/lookup.go:187 +0x56\nnet.glob..func10(0x284d920, 0xc42005cc60, 0xc420404590, 0xc420349920, 0x10, 0x0, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/go/src/net/hook.go:19 +0x52\nnet.(Resolver).LookupIPAddr.func1(0xc420346000, 0x284d8e0, 0xc420072010, 0x0)\n    /usr/local/go/src/net/lookup.go:193 +0x5c\ninternal/singleflight.(Group).doCall(0x28a7570, 0xc420078dc0, 0xc420349920, 0x10, 0xc42033d440)\n    /usr/local/go/src/internal/singleflight/singleflight.go:93 +0x2e\ncreated by internal/singleflight.(*Group).DoChan\n    /usr/local/go/src/internal/singleflight/singleflight.go:86 +0x31f\n```. ",
    "junhuihuang": "Thanks for your reply. This is the corresponding cpu consumption.\n\n. ",
    "bhperry": "Quick update: Found out the OSSEC paging was over a \\n that was in the initial file but gets dropped by the init container we use to cp the /etc/cni/net.d/10-flannel.conflist file. So that bit is resolved, but still very interested to figure out why flannel might be restarting itself.. ",
    "ducttapecoder-vt": "I didn't read the requirements closely enough. Apparently Flannel VXLAN on windows isn't supported until Kubernetes 1.14. Will try again after March 25.\n```\nOverlay networking: use Flannel in vxlan mode to configure a virtual overlay network\n\nrequires either Windows Server 2019 with KB4482887 installed or Windows Server vNext Insider Preview Build 18317+\nrequires Kubernetes v1.14 (or above) with WinOverlay feature gate enabled\nrequires Flannel v0.11.0 (or above)\n``. @celestialorb Interesting. Did you enable the WinOverlay feature gate?\nI also had a similar issue with switching from overlay to host-gw because the script didn't clean up after itself. Ripping all the scripts and files out and runningGet-HNSNetwork | Remove-HNSNetwork` helped.. \n",
    "celestialorb": "I'm running into this as well with kubernetes 1.14.0-beta.2 so I'm not sure it's related to the version.\nSeems to be related to this: https://github.com/coreos/flannel/blob/e4deb05e97ceb68d43df022fb76cd505a38a6973/backend/vxlan/vxlan_windows.go#L136-L138\nSince I'm not seeing Checking HNS network for DR MAC... my guess is that this line:\nhttps://github.com/coreos/flannel/blob/e4deb05e97ceb68d43df022fb76cd505a38a6973/backend/vxlan/vxlan_windows.go#L125\nis returning an empty set of HNS networks.. If I recall correctly that was the issue. I had a typo in my script that caused the KubeletFeatureGates parameter to be left out of the start-up script.. @Larswa Syntax looks correct to me. Are you positive the update took? You can check to see if it exists with Get-Hotfix in Powershell. You probably have to restart the node after applying it?. I was able to get the error to disappear once installing this update (KB4482887): http://download.windowsupdate.com/c/msdownload/update/software/updt/2019/02/windows10.0-kb4482887-x64_826158e9ebfcabe08b425bf2cb160cd5bc1401da.msu. Also make sure to restart, it wouldn't work for me until I restarted the machine after applying the update.. ",
    "Larswa": "I tried with the kubernetes 1.14.0-beta.2 executables on windows (1.13.4 on master), wanting to get it working with VXLAN mode for flannel. \nI also updated win2019 with the kb mentioned and added this parameter to start.ps1  -KubeletFeatureGates \"WinOverlay=true\" but got the same errors as @ducttapecoder-vt  Not sure if I got the syntax right for the featuregate?\n. @celestialorb I'm fairly positive.  Windows update presented me with this update https://support.microsoft.com/en-us/help/4489899 that bumped me to windows 1809 build 17763.379     And the earlier KB4482887 is build 17763.348 So I ought to have it.  \nI am working on a vagrant setup that can wipe and restore the cluster so I'll try again ... I had some manual not scripted things I tried, so I may have fat fingered something along the way .... waiting for my vagrant up now ... . I'll try with just THAT update on my next round ... thanks @celestialorb. Stay tuned. \n. ",
    "dnhodgson": "My mistake, this is actually an issue with kube-proxy.. ",
    "matthazinski": "Is there an advantage to using IKEv1 here? IKEv2 seems to be simpler and more reliable.. ",
    "joshix": "Last sentence: \"Packets are forwarded through one of several [backend mechanisms][backends].\". s/really//\nsecond sentence: \"Flannel can use the Kubernetes API as its backing store, meaning there's no need to deploy a discrete etcd cluster for flannel.\". s/Kubernetres/Kubernetes/\nsecond sentence: \"It's simplest to add flannel before any pods using the pod network have been started.\". Unpack sentence one into 2 or 3?\nCan one discover whether the pod CIDR has been set by \"some other means\"?. a little more explanation hint, e.g., \"add flannel to the cluster by applying the manifest file...\". last sentence: remove from parens. \"This is the default for clusters created by kubeadm.\". the manifest defines objects; kubectl apply creates them. This example seems incomplete. Shouldn't it show starting 2 pods, perhaps the second one interactively, so that you fall into its shell and from there ping the first pod?. strike second sentence\n. I'd say this right after Getting Started, instead. Maybe:\n\"The easiest way to use flannel with Kubernetes is with one of the several deployment tools and distributions that network clusters with flannel by default. CoreOS's [Tectonic][coreos.com/tectonic/] sets up flannel in the Kubernetes clusters it creates with the open source [Tectonic Installer][installer github] that drives the setup process.\". by way of explanation, @zbwright, etcd clients like flannel can use the etcd v3 or v2 apis when communicating with an etcd v3 service. So and etcd v3 binary can serve either the v2 http api and the v3 grpc api to a client. The two apis cannot be used interchangeably and they have separate data stores.\n(No change required for this comment). prefer https://github.com/coreos/etcd/blob/master/Documentation/dev-guide/local_cluster.md. ",
    "alekssaul": "is this not supposed to be\naddressPrefix := l.Subnet\n    networkGatewayAddress := l.Subnet.IP + 1\nlease should have the address and gateway information instead of the subnet.config If I understand correctly. Based on my testing config.Network gets the whole clusterCIDR (/8), where as l.Subnet gets the actual network (/24) that's assigned from etcd. "
}
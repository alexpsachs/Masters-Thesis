{
    "coveralls": "\nCoverage increased (+0.25%) when pulling 529aac33a2d09ae1ba1a7d3b1b10682f4b596c48 on waffle-iron:master into ce21ed48daa1e7f125cafaa1e15ac529c5146525 on ben-manes:master.\n. \nCoverage decreased (-0.04%) to 93.42% when pulling 3b051ff519c04c7df450e4ea86eaf82d1dfa3c9b on wburns:ispn_changes into 75e625f54cdd63818989dcbff1aa5849d06b296c on ben-manes:master.\n. \nCoverage remained the same at 93.78% when pulling 31bcb8beed68bf92a35484767bb16a73a45331d4 on cache2k into 4d636ef807ab9265e66576ec063179e557eafcce on master.\n. \nCoverage decreased (-0.44%) to 93.34% when pulling 1b3e8b6dd600bb937013ebd1094727c9200d9d93 on cache2k into 4d636ef807ab9265e66576ec063179e557eafcce on master.\n. \nCoverage decreased (-0.38%) to 93.4% when pulling d67a752bc59f06677972dc0ddd9242ffd01ba1b7 on cruftex:cache2k into 4d636ef807ab9265e66576ec063179e557eafcce on ben-manes:master.\n. \nCoverage decreased (-0.04%) to 93.73% when pulling 60330abd0c0b5eaa9550d470ed8c8c818e11ede6 on cruftex:cache2k into 4d636ef807ab9265e66576ec063179e557eafcce on ben-manes:master.\n. \nCoverage decreased (-0.35%) to 93.42% when pulling 0994fa39eb51cf0c7d382f34f234d7035ed6db21 on cruftex:cache2k into 4d636ef807ab9265e66576ec063179e557eafcce on ben-manes:master.\n. \nCoverage increased (+0.07%) to 94.494% when pulling 443886f3d9a8f192d55993ecb9dc6c30a57b4ffc on vlsi:travis_jdk9 into 7b5b753505895c18ca39fd62e47eb1fb91dead6e on ben-manes:master.\n. \nCoverage increased (+0.05%) to 94.476% when pulling 443886f3d9a8f192d55993ecb9dc6c30a57b4ffc on vlsi:travis_jdk9 into 7b5b753505895c18ca39fd62e47eb1fb91dead6e on ben-manes:master.\n. \nCoverage increased (+0.05%) to 94.476% when pulling 54d1aa3b2dd8629ba2ec3fc14c05cfcd1ca68e79 on vlsi:travis_jdk9 into 7b5b753505895c18ca39fd62e47eb1fb91dead6e on ben-manes:master.\n. \nCoverage increased (+0.04%) to 94.458% when pulling 54d1aa3b2dd8629ba2ec3fc14c05cfcd1ca68e79 on vlsi:travis_jdk9 into 7b5b753505895c18ca39fd62e47eb1fb91dead6e on ben-manes:master.\n. \nCoverage decreased (-0.02%) to 94.458% when pulling 3713aa8aa6e6c4eb3a5832154f4e1f55106fd088 on Dirk-c-Walter:patch-1 into 00bc8959d1ddbaebdab27a93779c1d7ebb45b402 on ben-manes:master.\n. \n\nCoverage increased (+0.03%) to 93.658% when pulling d8f0aedc9e129bf3c67f0e31732cd182089c393f on trauzti:2q-hashmap-insert into 322c2298a4d58bbbc8a7e1edb6f2fe0b7577ad06 on ben-manes:master.\n. \n\nCoverage decreased (-0.1%) to 93.757% when pulling f2dd9f024de1394b3aac0a069b0158dce96e35c0 on solr into c5cc2d4f04a317111705108a06b7babe3ce9dc6f on master.\n. \n\nCoverage remained the same at 93.281% when pulling 6a6a9b1130dfe35ba891c04e314028de30f1123e on nmihajlovski:master into 56aa693b03ac343df02bc084e4fb259b68377dc4 on ben-manes:master.\n. \n\nCoverage increased (+0.08%) to 93.187% when pulling 6e6541bab80931a3305d613b258a3835caaa4ce8 on ohadeytan:master into f9e3e6240464d97183a73040bf82fb88d64abb2c on ben-manes:master.\n. \n\nCoverage decreased (-0.02%) to 93.849% when pulling 4e954ddaa8075c2c153543406657d2c705100a1d on Harshrajsinh:hotfix/readme-update into 89e6487686eb3bc0b43dae5b0151db034cc6a1a3 on ben-manes:master.\n. \n\nCoverage increased (+0.001%) to 93.85% when pulling 4119f9c026d8ebb5bf59d9005b4c08428b0e9189 on nickrobison:listener_fix into 3f8054d16842478de3139780bda339e04e5ed770 on ben-manes:master.\n. \n\nCoverage increased (+0.02%) to 94.198% when pulling cb87b0f2bdc32a80559b9940d35cffbbaf130952 on kdombeck:jcache-unwrap into 582d3a00d2a90ebf01165bc665d68cb3ec68e0d8 on ben-manes:master.\n. \n\nCoverage decreased (-0.3%) to 93.765% when pulling e84866fd6ad5bf9ab8c5a798df63b6f31f5e684a on j-baker:patch-1 into 7c0e23508c66bbf376892a0945decd8ee8813862 on ben-manes:master.\n. \n\nCoverage decreased (-0.02%) to 94.105% when pulling 91ac4e091cd337041e43cca054670319e8561bf1 on leventov:loading-stats-spec into 916407ea56bbb19567640da7b79ac27104bd45be on ben-manes:master.\n. \n\nCoverage decreased (-0.2%) to 93.644% when pulling 64a08058ad1fbb051503c9abea372bf3aa3cec68 on corda:christians/configure-window into 29389bef6351bad4b60e8d432a5d1cb208d2db52 on ben-manes:master.\n. \n\nCoverage increased (+0.05%) to 93.801% when pulling c105cb719518f9fdf824e1c532ab1807dc9cc780 on ohadeytan:master into f107fbc8b93883ec5513916c6c7fb5a819e6b498 on ben-manes:master.\n. \n\nCoverage decreased (-0.2%) to 93.656% when pulling d360356c0b847a95364673fe0bd1c1378ae6ffc0 on imasahiro:work into b4f195b3307acaa49e1a6d14e2c150e06756c3ea on ben-manes:master.\n. \n\nCoverage decreased (-0.08%) to 93.815% when pulling 4a8c9690e56e24f24c194ffe9bafa60171994146 on burka:patch-1 into 2721dbfdd1f5d43add2afb858e156e3b3d6ce4e8 on ben-manes:master.\n. ",
    "ben-manes": "Added in 7772145\n. Thanks!\n. Thanks!\n. I haven't followed jsr307 since its reboot, when I had to decline Greg Luck's invitation to join the JSR due to the Oracle/Google lawsuit. At the time it was simply an extraction of Ehcache's APIs, though they appear to have resolved a lot of my original concerns since. \nSurprisingly it seems that they entirely ignored Java 8, neither adopting j.u.c, lambdas, nor new JDK naming conventions in favor of their own versions (e.g. CompletionListenerFuture vs CompletableFuture). Their API seems to be more focused on distributed caching than in-memory cache performance, so some of the design decisions are a little quirky to promote the vender implementations users are expected to adopt.\nMy understanding is that this JSR is implemented at the container level, not the library. Spring's implementation supports a Guava backend. I imagine that there's CDI and Guice extensions somewhere too.\nIf there's a base substrate that the library should provide to make it easier to integrate with JCache, then I'd add that as an optional extension. I'd need to learn more about the whole situation and look into existing integrations.\nIf you have a better understanding if what I can/should do to aid JCache usage, that would be helpful in getting support added. Otherwise it will probably be backlogged for the time being.\n. Thanks for the clarification. Looking into the other providers I see that you're right. They implement adapters and annotation extensions leverage the ServiceLocator to load the implementation.\nI'm not a fan of the JCache spec to use in my work, but I understand the motivation to allow others to use it if they prefer. It won't be my top priority since there's a lot to do before I'm happy with the implementation. I agree that it should be an optional dependency just like the Guava adapters and may be in the first release or shortly after. I'll probably work on it when I'm done with the core and am iterating on feedback from anyone I can convince to give a review.\nThe relationship with Guava is unknown. While some of my former colleagues are supportive, the core team has been quiet. This may be because they have not communicated what Guava will look like in a Java 8 world, so there are many open questions. While I'd love to get this back under the Google brand and their involvement, it may simply be an alternative to promote as the Guava cache languishes.\n. I started flushing out the basics.\nI plan on using Typesafe Config for the configuration file unless there is a reasonable concern.\nOne difficulty that may delay this feature is figuring out how to run the TCK as part of the build. It is intended to be used manually by updating its configuration files and running at the command-line or in IntelliJ. It appears to be a suite of JUnit tests, some using the JVM and others calling to a local cache server of its own design. It doesn't appear designed for easy integration, except for the foresight of being published on Maven Central. Hazelcast manually extends every test class so that their build picks it up with their customizations. I think with some proper Gradle magic this can be integrated in a cleaner fashion, but that will take some time to work out.\nIf I can get the TCK integrated, that should make it implementing the adapters much easier since development will be driven by tests.\n. I spoke too soon. The TCK is now integrated into the build.\n. Okay, so after spending the weekend on this roughly 80% (381) tests pass. The failures are due to not yet implementing adapters for all of the JCache features (listeners, expiration, writer, and statistics). I suspect that these are probably very easy to add, but I haven't investigated yet. We also need to flush out configuration through an external file, which is just scaffolding at the moment.\nWhen the module is complete, you're welcome to port it over to Guava. There isn't a lot of Caffeine-specific code, though it will be a little more work if you want to backport from Java 8. The only challenge might be Cache.invoke where I use asMap's compute method to provide the per-entry atomicity expected by EntryProcessor. If you implement it racy instead, that may be spec-compatible due to the JavaDoc saying that consistency is defined by the cache (even though it contradicts itself by saying the entry is operated on atomicly). None of the TCK tests actually test any concurrency expectations.\n. Listeners are now working. Their design requires in-order event processing and listeners may be configured to force the caller to block. This differs from Guava where events are asynchronous and unordered. I resolved this conflict by treating JCache listeners as actors using CompletableFuture chaining to construct a dispatch queue and CHM.compute for scheduling. The RI, Ehcache, and Hazelcache versions all appear buggy and wrong if going by the JavaDoc. The TCK, as a whole, is a pitiful test suite that doesn't actually enforce most of the spec.\nExpiration appears messy because they continue to redefine terms commonly accepted in the JDK. LinkedHashMap's access-order means read or write, whereas JCache does not consider a write as accessing an entry. There are further redefinitions in creation and update expiration policies that are inconsistent. So expiration needs to be implemented from scratch.\nAs listeners cannot accept null key or values, JCache does not support reference collection (weak, soft). This would probably not be available anyways because I'll have to add expiration wrappers.\nThe spec authors originally designed everything to match Ehcache, as they drove it. Instead of understanding algorithmic choices, all implementations are expected to mirror it. Now that the lead is at Hazelcache they've migrated that implementation to be JCache compatible by converting algorithms. He's made the recent v1.1 API incompatible with v1.0 by adding non-default methods to the interfaces that match Hazelcache's extensions.\nHopefully when JEE8 is released the JCP will provide adult supervision. Until then the JCache spec is not reliable and grossly under tested.\n. Expiration is the remaining feature left to implement, followed by external configuration. However I need your guidance on what you expect from expiration support.\nAll of the implementation available use lazy expiration, due to Ehcache being implemented that way and dictating the spec. The Ehcache documentation says,\n\nIf an entry expires but is not accessed, and no resource constraints force eviction,\nthen the expired entry remains in place.\n\nThis approach is trivial to implement and fast, but error prone. Its an excellent choice for large caches with a size bound like memcached, but poor as a standalone policy due to a memory leak if uncapped. The common active approach is a O(lg n) priority queue, which has numerous drawbacks. Instead by modeling expiration as doubly linked lists, we adapted CLHM's O(1) concurrent approach to be time bound instead of size bounded. We use share an access-order list (read or write) for maximum size and time-to-idle policies. A write-order list (insert / update) is shared by a time-to-live policy and the refresh-after-write feature.\nUnfortunately JCache's expiration policies do not match 1-to-1 with Caffeine / Guava's. I do not want to add their variants into Caffeine's core as each requires their own list structure that cannot be reused, thereby being wasteful memory-wise. I would also not want to expose it in a public API, so that would need some reflection hacks to configure. Simply not worth doing.\n1. Do you want lazy expiration evaluated on entry access? (matches all others)\n2. Do you want periodic expiration? If so, which are acceptable?\n   1. O(1) periodic cleanup on user thread\n   2. O(n) background sweep by evaluating a watermark (ForkJoinPool.commonPool() thread)\n   3. O(lg n) per call on user-thread (hint, not acceptable to me)\n   4. Lazy with per-thread ops counter to perform a random sampling to discover expired entries\n   5. Other ideas?\nMy ideal is (2.1), but I don't want to invest into re-implementing Caffeine's core in the JCache module. So I'd lean towards (2.2) as the simplest as the cost is not user-facing. If (1) is acceptable, then (2.4) is a simple addition that help alleviate fears unbounded growth though does not prevent it.\nP.S. Sorry for these excessively long ramblings =)\n. Now passes TCK with lazy expiration support. There's probably a lot of clean-up work and so on, which can be resolved in future iterations.\n. The API for this improvement is not very clear. Even if a batch refresh may predominantly delegate to loadAll(keys), the method should try to be symmetric with the reload(key, @Nonnull value). What would the batch version of that be?\nA batch refresh using refreshAfterWrite and triggered by getAll would maps directly to reloadAll(Map<K, V> entries). However, LoadingCache.refresh(key) will reload if the entry is present and load if it is not. How should refreshAll(keys) behave when some of the entries are present and some are not? Should it split the operation into two calls, reloadAll(present) and loadAll(missing), or should it delegate the decision to the user, e.g. reloadAll(Map<K, V> entries, Iterable<K> remainingKeys)? The latter seems wrong from an API consistency perspective, but correct from a performance one.\nI'm favoring the reloadAll(Map) approach because it is more consistent and we can optimize the majority of the use-cases. Here's my rational.\n- A partially present batch is not the common case (explicit refresh). Most usages will be triggered by using refreshAfterWrite and a batch getAll.\n- Most users do not use the old values when computing the new one, relying on the default implementation (delegate to load).\n- Multiple operation are only required if the user overrides the default implementation (delegate to loadAll). This can be detected using reflection (at cache construction), allowing us to perform a single call instead of two since that will have the same user visible behavior.\n- A refresh is performed asynchronously, so 2 is a better worse case than N.\n- Like getAll, we cannot ensure that parallel batch operations do not overlap and may load the same entries. In the common case of refreshAfterWrite this can be optimized to not occur, though.\nOn reflection, I like CacheLoader.reloadAll(Map) as providing the best power-to-weight ratio.\n@abatkin @cjohnson2\n. The default interface implementation of reloadAll(Map) would be to throw an UnsupportedOperationException. That would make it symmetric with loadAll where the \"exception\" is to indicate to fallback to the iterative loop. But similarly we don't actually have to throw and catch an exception, since by reflection we know ahead of time.\nI think its best to layout each scenario separately based on which methods are implemented.\n| load | loadAll | reload | reloadAll | description |\n| --- | --- | --- | --- | --- |\n| X |  | ? |  | N calls (L + R) |\n| X | X |  |  | 1 call directly to loadAll |\n| X | X | X |  | 1 call to loadAll, R calls to reload |\n| X | X | ? | X | R = 1: 1 call to loadAll, 1 call to reload |\n| X | X | ? | X | R > 1: 1 call to loadAll, 1 call to reloadAll |\n| X |  | ? | X | R = 1: L calls to load, 1 call to reload |\n| X |  | ? | X | R > 1: L calls to load, 1 call to reloadAll |\nThe assumption is that most users only ever implement load and maybe loadAll. Very few will ever want to do anything smart with the old value. Those who want to batch reloadAll should trust us to do it for them and not need to delegate it themselves.\nDoes that sound right?\n. Only if reload was not implemented, which would have delegated to n calls to load. So they should be pleased because we make a single call to loadAll, which is what @cjohnson2 had expected in his Guava ticket.\n. Currently refreshing is performed on individual keys and calls CacheLoader.reload(key, oldValue). Ideally it would take advantage of your bulk loader, but neither Caffeine nor Guava do this. We would need to add reloadAll to delegate to and the intelligent handling to support this scenario.. Hi Nitsan,\nIt would be nice to bounce ideas, but I think the projects have different intents. I've read your blog and more recently used JCTools comparatively to check for performance lapses in my algorithms. I may have misread your project's goals, as I took the lack of documentation and minimal test suite as indicating it is a library for experimenting and demonstrating ideas.\nThis project is an outgrowth of my previous caching libraries (Guava, CLHM). The broadening to include collections is generalizing algorithms that I had explored for cache internals. For example 3.5yrs ago I wrote the original version of SingleConsumerQueue when exploring ways to improve write performance and talking with @plokhotnyuk who was trying to improve Akka's mailbox implementation. I wrote the EliminationStack as a favor to @jbellis, who wanted alternative ideas for solving a Cassandra performance bug. Internally the cache uses a ring buffer with an adaption of Striped64 resizing, and I haven't decided if any of that should be made reusable.\nMy experience working on Guava taught me to be very API conservative. I think JCTools will have a broader bag of data structures to choose from, whereas Caffeine will focus more on maximizing the power-to-weight ratio. Caffeine will be much more narrow like how Guava originally was, perhaps not venturing very far from its caching roots.\nI see our two projects as very different, but a great opportunity explore ideas together.\n. Great, also feel free to email me: ben.manes@gmail.com. I'm in the bay area and haven't been able to sleep the last few nights (~4:30am), so I won't skype you tonight as I head to bed.\nIn short, I would use JCTools in an application but prefer minimizing dependencies in any library that I publish. So I'd probably borrow ideas from JCTools and embed them internally, at least for this project.\n. Good point. It seems like there are two obvious choices, either fix isEmpty() or poll(). I think either of the following snippets would work, with the first being my preference for performance reasons. Thoughts?\njava\n  @Override\n  public boolean isEmpty() {\n    return (head.getNextRelaxed() == null);\n  }\njava\n  public E poll() {\n    if (isEmpty()) {\n      return null;\n    }\n    for (;;) {\n      Node<E> next = head.getNextRelaxed();\n      if ((next == null) && ((next = head.next) == null)) {\n        continue;\n      }\n      lazySetHead(next);\n      E e = next.value;\n      next.value = null;\n      return e;\n    }\n  }\n. If the state hasn't been fully materialized, why is it incorrect to use head.getNextRelaxed() == null until the elements become visible to the consumer?\n. Oh, of course. Sorry, I'm working on unrelated stuff so I keep switching contexts. =)\nGood catch. I'll make the fix shortly. Feel free to borrow any of the arena code if that interests you.\n. All JDK queues, and in general throughout the collections framework, do not handle sizes larger than Integer.MAX_VALUE. Two examples are LinkedList and LinkedBlockingQueue. And of course array-based versions will crash due to the length limit of an array. Because this has not been an issue in practice, it is generally acceptable to not follow the contract strictly and leave it unhandled until deemed necessary.\nThe only JDK collections that I'm aware of that does handle the size threshold is ConcurrentHashMap and ConcurrentSkipList[Set, Map].\nI think this is unlikely enough to not be covered.\n. Actually I might as well fix this, so I'll add it with #9's changes.\n. I'm going to fix all cases as part of #9. All traversals need to be fixed, so it impacts a lot of methods.\n. Isn't that how it is implemented? The head and tail are read once. Traversal uses a relaxed read, falling back to a volatile read spin wait until the next field is visible. The current version is,\njava\n  public int size() {\n    Node<E> cursor = head;\n    Node<E> t = tail;\n    int size = 0;\n    while ((cursor != t) && (size != Integer.MAX_VALUE)) {\n      Node<E> next = cursor.getNextRelaxed();\n      if (next == null) {\n        while ((next = cursor.next) == null) {}\n      }\n      size++;\n      cursor = next;\n    }\n    return size;\n  }\n. Thanks, though! Grab a redbull =)\n. fyi,\nI rewrote my EliminationStack to include combining properties. The resulting ConcurrentLinkedStack performs quite well in my benchmarks. You may find it interesting to glance over.\n. I had forgotten about that detail myself when reading the Guava bug. There are two options I can think of.\n1. Use AsyncLoadingCache. The load from the map's perspective is immediate since it stores a CompleteableFuture as the value. An invalidation will not block and the listener will be invoked after the future's computation has completed.\n2. Store the loading keys in a auxiliary set. This could be done internally using the compute methods or externally with CacheLoader. It may be too error prone to store the full key set because then we'd have to ensure that compute is used to mimic and instrument puts, removes, etc.\nYou might be able to get away with #1 or implement #2 as a decorator. I'm hesitant to implement #2 internally, at least until its proven problematic for enough people.\nThoughts?\n. For the decorator approach, I was thinking something like,\n``` java\nSet loadingKeys = ConcurrentHashMap.newKeySet();\ninvalidateAll() {\n  delegate.invalidateAll();\n  for (Iterator it = loadingKeys.iterator(); it.hasNext()) {\n    delegate.invalidate(it.next()); // blocks\n    it.remove();\n  }\n}\n// CacheLoader or other computes (atomic)\nloadingKeys.add(key)\nV value = // compute\nloadingKeys.remove(key);\nreturn value\n```\nIn regards to the async cache, it has many nice properties that could make this simpler. If you don't need to block until each pending load completes, just call invalidate through the LoadingCache view. If you need to wait, then that would require additional methods to the AsyncLoadingCache view, which can be added.\n. 1. The key set avoid latency fluctuations, more evident with a fair r/w lock, where new loads wait until the writer's turn to acquire exclusive access.\n2. Yes, for a particular cache instance. This is ensured by ConcurrentHashMap. Unfortunately this can't be guaranteed by ConcurrentMap which is one reason why the map is not configurable.\n3. You're right, I forgot that even another delegate.invalidateAll() to catch the race wouldn't work. Moving the key removal outside of the loader is the only safe approach.\nI had to do some similar hacks for the JCache adapter (see CacheProxy) due to requiring that listeners receive events in-order for an entry. The spec was vague regarding if a removal followed by insertion of a key had to be in-order, though, so for simplicity I didn't enforce that. The other JCache implementations use fat locks around the cache and invoke the listeners synchronously (whereas I have dispatch queues via EventDispatcher). I can't tell whether any of that is similar to how your internal system behaves, but may be worth a quick glance.\n. Makes sense. Sorry that I should have been more careful when writing the loadingKeys idea. There are many subtle details to work around, like if the remove is stalled long enough for an evict/load cycle (which I think is what you are trying to show). For your case a r/w lock is simplest. I think that if I was pushed to handle this internally, I'd have to fully explore the key approach to see if it made more sense.\nI'm glad you have a working solution.\n. Yep, I saw that too. Sorry I was throwing the idea out, not an implementation, so it was off-the-cuff rather than a validated alternative.\n. I started to look into this and think about how CacheWriter would interact with this improvement. Unfortunately all of ConcurrentHashMap's internal iterator methods, like removeIf and forEachKey, skip over computing entries.\nI think this can only be done safely using a secondary set of all present keys. This is just the loadingKeys discussion without the race because the keys are not actively discarded.\nThe key set is only modified within a compute block, so all insertions and removals must be done through a compute method. This is to avoid the races mentioned earlier by always relying on the cache's hash table locks. The key is added to the set at the beginning of a load and, if that load failure, removes the key before completing.\nAn invalidateAll() walks our key set instead of data.keySet() using a compute-based removal. If there are races where the entry no longer exists then nothing extra is needed. Otherwise a removal notification and CacheWriter.delete() is triggered.\nThe cost of this approach is a slight overhead for writes and the extra memory used to retain an additional ConcurrentHashMap-based set. Neither of those are prohibitive, though it seems unfortunate to need an extra map just to cover a race condition. However I think that's better than the alternatives.\n. When I caught up with Doug last year his response to this was,\n\nThe desire to do this is racy, even using clear, which non-atomically\ntraverses. So there's only weak consistency wrt arriving/ongoing/completed\ncomputeIfAbsent's etc. In principle, CHM could offer a method with similar\nproperties, but it still wouldn't provide a simple-to-state guarantee.\nThe only solution I know is for users themselves to arrange quiescent\npoints if they need a strict demarcation. (And I wonder if this desire\nabout invalidateAll obscures assumptions they might have about consistency.)\n\nGiven his hesitation and my own regarding strengthening invalidateAll, I think the current behavior is the best approach and should be better documented. Since this can be adequately worked around in user code, as you've demonstrated. I'll close this when I've improved the documentation.\n. Let me first say that I am open to API changes, as getting the right API that uses new language features is hard. I have a minor fixes planned for v2 and had hoped to hear more concerns like this one before I bit the bullet and made API updates.\nIf CompletionStages had all of the read side properties I would have used it. It is actually only the fluent chaining capabilities and lacks the ability to inspect the result (isDone, getNow, etc). At first this seems fine from an interface-impl theology by viewing the toCompletableFuture() bridge as an escape hatch. Unfortunately that cannot be relied upon due to being optional (@throws UnsupportedOperationException if this implementation does not interoperate with CompletableFuture).\nThis is not just annoying for users who want to bridge frameworks. Its also a problem for the LoadingCache view, which provides the convenience of a synchronous api and avoids duplicating various methods (size, invalidation, etc). It did not seem to be a better API choice to have the view break if a stage couldn't be converted to a future. If that was allowed, it may also be surprising for users who are combining libraries and don't realize the incompatibility choice they made.\nThe concurrency-interest threads on why CompletionStage was introduced does not provide a lot of guidance. Prior to its introduction, CompletableFuture was said to be a low-level class that others should build abstractions above that match their policies. A subclass is allowed to disable features and JDK9 introduces a copy() method for safe publication to avoid obtruding the original's value. There is very little said about how to use CF/CS from in api design.\nThe interop approach you mentioned is ugly, but an explicit bypass in user code so they know that obtruding the result (e.g. cancel) won't work. It doesn't break the principle of least astonishment and lets the APIs work as expected. The bridge code is short and isolated, and the cache itself can be hidden behind a custom facade to avoid leaking CompletableFuture if desired. An alternative is to accept CompletionStage, produce CompletableFuture, and try to perform the bridge code internally. That would have some surprising side effects too, so I don't think it would be an ideal API either.\nGiven that rational, what do you think would be the best choice going forward?\n. @dpratt \nI'm ready to focus on revamping the APIs for v2 and fix any mistakes or gaps. Please advise on this specific issue and any other commentary on the other APIs.\n. That would be great, thanks!\nI'm slowly flushing out the simulator to explore alternative eviction policies. That will take some time and, other than api improvements, nothing else is currently planned for v2.\n. I am going to begin the code phase for v2 this week. I spent a lot of time researching eviction policies, eventually designing a new one that has an optimal hit rate, low footprint, is O(1), and is fairly simple. The integration work should be relatively straightforward. At that point I'll be pretty excited to get it out the door, so having a checklist of what else needs improvements would help round out that release.\n. Thanks for the thoughts Jens.\nWhile its unfortunate that CF is not an interface, the impact is thankfully not high. For testing, such as mocks, its very easy to control a CF by either supplying an executor or using the constructor and obtruding the value. \nI think that issue between CF, CS, etc. are about integration between different async operators. A proliferation of CS variants requiring integration between each type might be more cumbersome than standardizing on CF, which all have to integrate with regardless. The slow adoption of Java 8, wide number of ad hoc async operators (Rx, Guava, Twitter's, etc), CS compatible futures (Scala's), and lack of a hint by the JSR makes this all very confusing.\nBy itself CS is not enough for the cache, because it might accept a future rather than wrap synchronous code. For example the value might be the result of composing a chain of async operators across different web services. If the cache was only accepting synchronous computations and exposing a Future, then your idea of a custom CS might work out well. However I think computing futures is a valuable use case as more frameworks adopt reactive APIs. At that point CF appears to be the best common future all need to interact with, hence the current AsyncLoadingCache interface.\nI don't think composing multiple cache operations like your example shows will be a common pattern. I think that case might begin to lead into transactions, which becomes a messy discussion. I suspect the usage of being part of an Observable style chain might be more prevalent.\nI should note that the compute time of a future is not incurred by the cache, like it is for a synchronous compute. The value in the cache is a future, potentially in-flight on an Executor, so the cache operations themselves are low overhead. The details comes into handling errors, disabling size/expiration for in-flight futures, weighted size, removal listeners (discarding while in-flight), statistics, and bulk computation. The interface provides a convenient and correct abstraction rather than doing this ad hoc by caching futures manually.\n. The scala-java8-compat project recently added support for toCompletableFuture(). The future-converter project supports Spring, Guava, and RxJava conversion. This removes the integration concern for the most popular alternative future libraries.\nI think the remaining issue is how can the API be improved? What functionality is missing?\n. Closing due to lack of actionable feedback. Please reopen, or open a new issue, if you'd like to continue the discussion.\n. This is documented under How to Contribute. The jmh.gradle file includes a link to the plugin where the configuration options are explained.\nYou'd want to run,\nbash\n./gradlew jmh -PincludePattern=GetPutBenchmark\n. > I think it should be cache.peek()\ndone\n\nOne of the aim in cache2k is to provide a universally good cache replacement\nalgorithm (which is not LRU), so, consequently there should be no option to change\nthe cache implementation on the API level. I don't want the normal user to bother \nwith this.\n\nThis is also why Charles and I tried to keep LRU language out of the Guava api. Charles had prototypes of LIRS and ClockPro, with the LIRS code adapted by Infinispan in their version. My work showed how LRU could be made fast, and I had bad experience with other policies due to them degrading in non-obvious ways. I made attempts to augment LIRS, but being so poorly tested by the authors and subsequent implementations all having memory leaks I never trusted it enough.\nFor Guava we were able to adapt LRU handling for expiration and reference collection, and so being a known quantity with solid data we never revisited it. I'm not of the opinion that LRU is a bad policy, but that alternatives need more rigorous evaluation than merely reading the source material. This time around I added tracing and a primitive simulator from the start, an idea we wanted in Guava but had too much on our plate so it was backlogged indefinitely. There are many potential paths, such as TinyLFU augmenting the policy rather than abandon it.\n\nIf you do not use CacheBuilder.implementation() there is no need to depend on the > cache2k-core module. \n\nThe core module would be a dependency, just runtime instead of compile time makes no difference for a benchmark module. I do like being explicit about what the implementation is, for now, until the library provides a canonical policy.\n\nOption 2: Expose the different implementations and run benchmarks on it, like: \nCache2k_LRU, Cache2k_ClockPro or even Cache2k_Random?\n\nI'd go with this option for now because benchmarks need to be explicit in what they are testing, where the policy is one design tradeoff. Otherwise all libraries caches might as well be random, which can be made effectively free performance-wise.\n. Most likely IntelliJ doesn't include the jmh configuration (and its sourceSet) automatically. This was the case for Eclipse, which the jmh.gradle customizes.\nIts been a while since I've had to customize Gradle for IntelliJ. The documentation makes me think you need to add something like,\n``` gradle\napply plugin: 'idea'\nidea.module {\n  scopes.PROVIDED.plus += [ configurations.jmh ]\n}\n```\n. Please sign the CLA.\nYou can delete the cleanUp method since the default method is being overridden, which is a no-op.\n. There should be additional benchmarks to provide very different information. It would have to be constructed carefully to determine exactly what it is proving.\nThe benchmarks clearly state that they show the read/write throughput under concurrency. This is most meaningful when measuring the hit penalty, which is the most expensive scenario and ideal case for users. It would be fair to say that the miss penalty could be judged as well (100% miss rate), but that is typically not very interesting for caches.\nIf a mixed hit/miss benchmark was shown then they can highly skew the results. For example Infinispan incorrectly believe their rewrite is faster due to the high miss rate in their benchmark, which on a Java 8 ConcurrentHashMap is very fast. This would mean that the worse the eviction policy is, the faster the cumulative cache operations might appear. It becomes very problematic to construct a concurrent benchmark with less than a 100% hit rate across different implementations that provides meaningful insight.\nReducing the hit rate is more appropriate in a few scenarios,\n- For efficiency benchmarks, which is where a simulator with policies and traces comes in. It would help show what the expected hit rate of the cache is under different loads. This is traditional approach to caching, which is useful but often highly irrelevant for Java applications where the hit rate is often in the 9x%.\n- To show the eviction penalty due to overflowing the cache. Different eviction policies and concurrency schemes have different penalties. This requires care because some policies degrade differently based on the trace data. For example a classic SecondChance CLOCK cache would require a full scan if all the entries were marked.\n- To show the overhead of the cache loader calls, which is a user function. That would be to demonstrate how much concurrency is supported for handling computations in flight. Other than a single exclusive lock approach, this is typically not a concern for users.\nFor microbenchmarks the current scheme is reasonable, given that all use LRU. When a different policy is in the mix then the results are skewed in that policy's favor. A long running application may have many hiccups due to the cost of eviction, as mentioned above, whereas LRU is O(1) in all scenarios. This is similar to the misunderstanding that many had regarding soft references, which appears to be great in a microbenchmark and horrible in practice. When evaluating eviction policies that are not concurrently O(1) the benchmark author needs to be very careful to communicate accurate and meaningful results.\n. > In most real world workloads modern eviction strategies outperform LRU in terms of achieved hitrate. So additional costs for eviction, might get amortized. As soon as there is some real cost for generating the cache value (CPU usage or latency) it will get amortized.\nYep, but unfortunately some policies have high eviction penalties that, if consuming a user thread, can add noticeable latencies. In particular those that are CLOCK-based often have O(n) worst case penalties which, when a cache is 1M+ entries, is unacceptable. How to benchmark to show the eviction penalty is non-trivial, since the that often requires initializing to a worse-case state for a single run.\nUnfortunately this is one of many areas glossed over by academics, so those implementing their algorithms may not notice the issue until they observe high production latencies. The simplest solution for an implementation is to be pragmatic by putting a cap on the eviction search space so that it never goes O(n) and chooses a good enough victim (but perhaps not the best).\n. The simulator is now pretty well flushed out now. It shows the eviction policy hit rate and actual time complexity (# of steps taken). The execution time is also shown, but that's unfair because the thread may be swapped out, GC, etc due to simulating in parallel. Its fairly trivial to write a JMH benchmark for the policies, though they were written for correctness and ease of understanding rather than raw performance.\nThe other categories (miss penalty, eviction penalty, compute penalty) are harder to benchmark correctly. Of them I think eviction penalty is the most interesting, but hardest to show accurately across implementations. If implemented as an infinite counter then its a subset of the 100% write benchmark (where all do poorly). It would accurately show the cost for policies that are O(1), like Ehcache's sampling, but not for those that are CLOCK-based. Those policies would mimic a FIFO and appear better than actual usage by never needing to scan. I don't know whether interleaving random reads would solve that, and if so it would probably need to be a single threaded benchmark.\nThe remaining class of benchmarks are variations of the GetPutBenchmark using different configurations, such as expiration. That's typically dominated by the slow native calls in System.nanoTime() or System.currentTimeMillis(), where the performance varies widely across operating systems. If one implementation resolves that with a coarser time resolution or background thread then I'm not sure if that remains a valid comparison.\nGiven the above, I'm inclined to close this task. If you have concrete benchmarks in mind then we can discuss them specifically.\n. Added Efficiency wiki page to compare hit rates. Otherwise I don't know what other benchmarks provide meaningful information and can be implemented reliably. Closing.\n. > Why does caffeine declare classes that overlap with tracing-api?\nThis is odd, as I don't know why there should be overlapping classes. Each artifact uses its own sub-packages under the caffeine package hierarchy and all classes have unique names. Even if you glob them to a single package there should be no conflicts.\n\nAre the two sets of classes equivalent? Said another way, do I need to be concerned with which versions of those classes that maven-shade-plugin chooses to place in the resulting .jar file?\n\nI assume so, since this shouldn't occur. If you provide a sample I can help you debug what is going on.\n\nBut I'm not actually using any tracing features (that I know of), so I don't know if I'm breaking Caffeine by doing that.\n\nUnfortunately it will, because the tracing api is tightly integrated, but the JIT should dead code eliminate it at runtime. The api is a stub that service loads and no-ops if no implementation is discovered.\n\nWhy doesn't it instead ... not depend on tracing-api?\n\nBecause instrumenting it in user-code is selective, error prone, and negates the limited value tracing might provide. The intent is that tracing can be enabled system wide and analyzed offline, but otherwise is a tiny jar. We had wanted this in Guava to answer some design questions, but without good data we abandoned our non-LRU policy prototypes. We also thought it might provide some value to recommend cache sizes, back when Googlers liked the simplicity of soft-references \"auto sizing\" their caches.\nThe tracing package is future proofing and shouldn't be causing any problems.\n. Ah, I see what's going on now. It seems that this is an unintentional effect of using the gradle-bundle-plugin to generate the OSGI manifest. For some reason it is packaging the dependency into the jar too, though it doesn't appear to to do that with other artifacts.\nI'm only using the bundle plugin because its a little more convenient to configure than Gradle's OSGI plugin. I'll see if switching over to that solves this for the next release.\n. So the problem was that I declared the bnd instruction Export-Package, com.github.benmanes.caffeine.* assuming that it would only inspect the classes in the module. Instead the bnd tool is also inspects the classes inside dependencies and, since the tracing package falls inside of the wildcard, it grabs them as well.\nBoth the bundle and official plugin require stating each packages explicitly for this scenario. Once I did that instead of the wildcard the output jar was correct.\nSorry about the inconvenience.\n. I just released v1.3.0 which includes this fix. Sorry that it took a while.\n. Sadly any bug is a feature (#22) to someone else =)\n. Unfortunately this is a limitation in Java's type inferencing. The problem is that the types are inferred for Caffeine, but due to the extra chaining it doesn't look at the LHS until the statement completes. It instead infers <Object, Object> as the most generic type.\nThe way to do this in one statement is to add a type hint,\njava\nMap<K, V> map = Caffeine.newBuilder()\n    .maximumSize(10)\n    .<K, V>build()\n    .asMap();\nThat can get pretty verbose when the key/value types are explicit, so your approach of using two statements is probably cleaner code most of the time.\nPast experience showed that using Map as a base for a cache wasn't the right model. The asMap view is an escape hatch to more explicit control, but most of the time a LoadingCache is more appropriate. That lets you avoid explicit get / put operations and self populate on a LoadingCache#get.\nThe only way to make this perhaps a little friendlier is to have newBuilder() be generic to move the type hint to the beginning of the build chain. Then it would look like,\njava\nMap<K, V> map = Caffeine.<K, V>newBuilder()\n    .maximumSize(10)\n    .build()\n    .asMap();\nI don't recall the rational for not doing that in Guava's CacheBuilder, but I'm sure there was one that @kevinb9n would remember. I know it doesn't provide much weight, makes reusing the builder more awkward, and promotes poor usage. Providing a buildMap() to support inference would be considered a poor choice for similar reasons.\n. Here's the Guava issue which confirms the above rational.\n. Feel free to reopen if you have any suggestions. Thanks.\n. tracing-api should be added, but automatically due to being a maven dependency. Previously the api classes were accidentally bundled into the jar by the OSGi bnd tool. \nIf that doesn't work can you provide me a sample project to investigate with? \n. Sorry I missed this comment, or I would have replied sooner.\nI wonder if this might be because Gradle generates the pom with tracing-api in both compile scope and test scope. Perhaps that's messing it up?\nI'll experiment with this on my way home (Caltrain), using Maven as a baseline. I'll make a new release if we can figure this out.\n. That's a weird convention. I checked mvn dependency:tree with your example and it shows that its not coming in.\nI think the only reason the test is used is due to a handy place to stash a reusable utility class, so I can probably fix that another way. I'm not sure how to prioritize the pom dependency order.\n. Yeah if I hack the pom in the .m2/repository I see that it can resolve iff the scope test is before compile. So Maven takes the last one defined, probably by abusing a  HashMap.\n. I'll release this tonight as 1.3.1. Thanks!\n. Released. I checked all the poms in the staging repository and they all looked good. The jars should sync to central within the next two hours.\n. FYi,\nRelease v2 which no longer includes the tracing packages. A new eviction policy was chosen so capturing traces is no useful enough to be part of the API.\n. Infinispan's version is based on Charles' prototype. That was cool to see as we instead focused on Guava Cache features, and never got back to it.\nIn CLHM I tried a few times with moderate success. Its easy to port LIRS if it is guarded by an exclusive segment lock, which I felt was a hack forced upon us due to legacy Guava code. To make it work without segments was harder due to weights. Entries are moved between the stack & queue under the eviction lock, but the weight may change independently. This led to a race and the fix was to have two weights: the user's and the policy's. When the buffered read/write events are replayed the two get in sync without exposing the race to the policy.\nFinally when it seemed close to being done, I didn't feel that confident in supporting the code without a lot of tests. The authors of the LIRS paper could only offer their version and traces to validate against. So I ended up shelving it until I spent the time to flush out enough unit tests. I saw subsequent implementations get burned due to the paper's vagueness result in memory leaks, and I also wondered if the average (not worst case) O(1) stack pruning could degrade rather poorly.\nSo now I'm just starting to on a simulator with traces and policies. I hope it will be very clear what the weaknesses are, the implementation complexity, and memory overhead. If one is chosen for adoption then tests can be written against the simulator's version. Finally we can attempt to integrate it. So a slow process, but I felt burned out from prior attempts.\nIf you want to help out or have trace data to offer, that would be awesome =)\n. Cool, I'll definitely take a look when I write a simple LIRS for the simulator. Thanks!\nIt seems feasible to do weights if you use a write buffer to replay the changes on a second weight field. I just don't know if that's worth the extra memory cost. There are also policies that take into account the weight, like Greedy Dual Size Frequency, though again I don't know if that's useful in practice.\n\nAlso I am curious as to the memory leaks you mentioned.\n\nClojure's leaks and @thomasmueller (H2) put in a workaround in his implementation. Both were due to unbounded non-resident entries in the stack, like you suspected.\n\nIt would be interesting to see on a real system if we had something like your trace/simulator.\n\nAll you need to do is log the keys accessing the cache. The data can be obfuscated, either by recording a hash of the key or post-processing to translate keys into a unique identifier. All that is needed is a logger, ideally separating the trace into its own log file. My tracing & simulator packages are decoupled, but admittedly more of a placeholder than mature libraries.\n\n[Simulator] Unfortunately it doesn't have a lot of details explaining it.\n\nIt is really primitive right now. It is driven by the configuration file, reads from an event stream, broadcasts events to each policy, and aggregates the statistics into a report. Each policy implements a minimal interface to record an access and return the statistics. Any contribution here would be great, since I only just started looking at it seriously.\n\nAre saying to replay back a set of cache accesses/modifications to see how it behaves?\n\nYep. All the research papers only present favorable information. It would be nice to see how a policy behaves in different situations. For example, from the few policies implemented I might be able to recommend TinyLFU+FIFO for a secondary disk cache. That pairing would have low I/O and a high hit rate. However TinyLFU does poorly on temporal locality, but this wouldn't be an issue due to the primary cache handling those requests. That's pretty cool and I wouldn't have thought of it otherwise.\n. That's pretty cool. I'll get your adaption into my simulator and seriously consider adopting it. I am really glad you have tests to peek at too! =)\nOut of curiosity, I quickly bootstrapped the Jackrabbit implementations into my benchmarks. On my laptop it does very well, so I'd expect similar behavior on a server-class machine. The reads are excellent and, not surprisingly, it degrades as the writes increase due to segments. The numbers are,\n- reads: 123M ops/s\n- read/write: 60M ops/s\n- write: 18M ops/s\nThe optimization to not reorder on every read makes a big difference. This would be huge for Infinispan's version because, in my benchmarks, the lock-free deque is a bottleneck due to mutation. For Caffeine this would be very helpful as well when expireAfterAccess / reference caching are not enabled. For those and LRU it records the read into a striped ring buffer, which is replayed in batches against the policy. If your adaption was used then the buffer would still be needed, but much less frequently for a maximum-size only configuration.\nThe reason why weight changes are problematic is only if the cache does not use per-segment locks. In that case the hash table and eviction policy are decoupled and can be operated on concurrently. If the entry's weight is modified by a put while the eviction policy is shuffling entries around then the data race can lead to inconsistencies because the policy does not handle the changing weights properly. The solution to this is to use a write queue to replay the weight change against the policy's representation. That way increasing or reducing the weight is properly reflected in the stack & queue. If you use segments, though, then this is all a moot point.\nAs an aside, do you think you might be able to capture some trace data as well?\n. @thomasmueller Can you check my configuration to see if I made a mistake? The hit rate is poor in all of the traces that I've tried. Below is the Wikipedia trace results as a real-world example.\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Policy                 \u2502 Hit rate \u2502 Requests   \u2502 Evictions \u2502 Time     \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 irr.JackrabbitLirs     \u2502 39.31 %  \u2502 14,974,511 \u2502 4,543,447 \u2502 4,970 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Clock           \u2502 47.80 %  \u2502 10,430,564 \u2502 5,444,026 \u2502 2,703 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Clock_TinyLfu   \u2502 57.04 %  \u2502 10,430,564 \u2502 4,479,984 \u2502 5,374 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Fifo            \u2502 41.87 %  \u2502 10,430,564 \u2502 6,062,391 \u2502 2,603 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Fifo_TinyLfu    \u2502 46.86 %  \u2502 10,430,564 \u2502 5,541,789 \u2502 5,369 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Lru             \u2502 46.62 %  \u2502 10,430,564 \u2502 5,567,011 \u2502 2,658 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Lru_TinyLfu     \u2502 57.04 %  \u2502 10,430,564 \u2502 4,480,699 \u2502 5,367 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Mru             \u2502 25.38 %  \u2502 10,430,564 \u2502 7,782,358 \u2502 2,789 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 linked.Mru_TinyLfu     \u2502 53.99 %  \u2502 10,430,564 \u2502 4,798,740 \u2502 5,523 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 opt.Unbounded          \u2502 85.90 %  \u2502 10,430,564 \u2502 0         \u2502 2,591 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Fifo           \u2502 41.87 %  \u2502 10,430,564 \u2502 6,062,273 \u2502 6,370 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Fifo_TinyLfu   \u2502 55.90 %  \u2502 10,430,564 \u2502 4,599,729 \u2502 8,343 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Lfu            \u2502 53.80 %  \u2502 10,430,564 \u2502 4,818,247 \u2502 5,903 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Lfu_TinyLfu    \u2502 57.07 %  \u2502 10,430,564 \u2502 4,477,320 \u2502 8,103 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Lru            \u2502 46.46 %  \u2502 10,430,564 \u2502 5,583,962 \u2502 6,219 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Lru_TinyLfu    \u2502 57.06 %  \u2502 10,430,564 \u2502 4,477,922 \u2502 8,260 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Mfu            \u2502 31.58 %  \u2502 10,430,564 \u2502 7,135,808 \u2502 6,566 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Mfu_TinyLfu    \u2502 55.01 %  \u2502 10,430,564 \u2502 4,691,757 \u2502 8,111 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Mru            \u2502 26.75 %  \u2502 10,430,564 \u2502 7,639,712 \u2502 7,076 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Mru_TinyLfu    \u2502 56.02 %  \u2502 10,430,564 \u2502 4,587,313 \u2502 8,023 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Random         \u2502 41.86 %  \u2502 10,430,564 \u2502 6,064,273 \u2502 4,243 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sampled.Random_TinyLfu \u2502 57.06 %  \u2502 10,430,564 \u2502 4,478,075 \u2502 6,732 ms \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n. I see, that's because on a get(key) you call get(Object key, int hash) multiple times and record the stats each time. For a miss that means its incremented twice.\n. Here's the results with a workaround, which is much more favorable.\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Policy             \u2502 Hit rate \u2502 Requests   \u2502 Evictions \u2502 Time     \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 irr.JackrabbitLirs \u2502 56.44 %  \u2502 10,430,564 \u2502 4,543,447 \u2502 3,179 ms \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n. I'm hoping to implement LIRS in the simulator this weekend to improve my familiarity with it. I included Jackrabbit's and Infinispan's versions for comparison. So far LIRS is always a top performer, usually on par with ARC. In the case of looping patterns (e.g. glimpse) ARC degrades to match an LRU while LIRS retains a high hit rate. So LIRS appears to be the more robust algorithm.\nIn a few cases Infinispan's implementation has a significantly better hit rate over Jackrabbit's. For example multi2 is 50.5% and 41% respectively. I haven't looked into why that's the case, but I suspect its due to the size of the non-resident queue. Perhaps using the simulator we can determine the optimal tuning parameters.\nOnce I my own adaption of LIRS in place, I should feel familiar enough with the policy to try to implement it in Caffeine.\n. That's a good point, but not the issue. Since this is for evaluating the hit rate and not concurrency the number of segments was set to 1 to avoid that issue. The related code is here.\n. If you can ping your network to capture trace data that would help. I'll do the same.\nI'd prefer not exposing the queue size parameter as an implementation leak, too expert level of a configuration, and no one will actually tune it. ARC uses a total of 2N size (N resident, N non-resident).\nThe degradation is only on a few traces and stays above LRU. I think the current parameters are acceptable, but tuning it based on data would be very cool.\n. Sorry I lost a little momentum on this as well.\nThe cache2k traces are probably similar to the ones checked in. The LIRS authors emailed me their trace files and C code in 2012. They also had I/O traces that I didn't save and the link expired, so those might have been bigger. The file sizes they describe in their paper are the source programs and not the resulting trace file, so its not clear if there is a larger variant. The authors were under NDA for the Facebook memcached traces that they were using, so they weren't available. The idea of each trace was to show different workload patterns that policies degrade with, so being small makes sense and does have some value.\n@gilga1983 is exploring using the simulator as an aid in his work for further improvements to TinyLfu. He pointed me to the wikibench traces (supported) and might still have access to the YouTube traces.\n. Oh and I found industry traces provided by Internet Traffic Archive and SNIA but didn't write parsers for their data.\n. Thanks! I added a parser for the ARC trace files.\n. Sorry that I haven't gotten much done lately.\nI have ~70% of the LIRS simulator's version in place. I just need to find the time to finish it and externalize the knobs. I think porting it into the cache won't be too difficult.\nI added ClockPro for completeness and a little more familiarity of IRR. A quick comparison to cache2k's non-adaptive version shows roughly 5-8% improvement, hopefully due to being adaptive rather than other implementation differences.\n. I have my simulated LIRS version working. It does not yet include @thomasmueller's improvements.\nInterestingly I found that all of the implementations I tested differ slightly from the author's reference version. Thomas' is different enough that I can't easily compare it. Charles' does not prune the stack after an eviction, and Infinispan's is probably adding prior to evicting resulting in an off-by-one issue. Those I can compare, mine included, evict from the LIR blocks in the opposite order from the author's. Instead of demoting from the bottom of the stack, they find the LIR block at closest to the top (roughly in the middle). This appears to make a negligible impact on the hit rate, does not follow the paper, and seems to be a coding mistake. It wouldn't hurt to debug this more to verify that initial analysis.\nThe author's version bounds the non-resident HIR blocks by having a maximum size of the S stack (S holds all types of entry types). Thomas bounds a separate non-resident queue. Both approaches have merit and their differences might have an impact on the hit rate, so its not clear which is preferable at the moment. Any preferences?\nI also need to add Thomas' fast path technique. Hopefully by playing we can tune the parameters based on the tracing data to find their optimal settings. At that point it shouldn't be too difficult to port this into the cache.\n. I created a new replacement algorithm that matches LIRS's efficiency, is much simpler, and does not rely on non-resident entries. In all of the traces it matches LIRS and ARC, except for an MRU-friendly workload. In that case LIRS better adapts to match MRU, with my policy coming in 3rd. All other policies (including ARC) do very poorly on that trace.\nThis new algorithm is tentatively named EdenQueue. It uses a sketch data structure to record the non-resident history with gradual aging. This increases the initial footprint of the cache for a smaller footprint when populated by not retaining the user's keys.\nI'd appreciate feedback on this new approach versus LIRS.\n. Good call, I verified with Gil that there are no patent issues. He's interested in writing a paper on this improvement, which makes TinyLfu more practical. The flaw in their paper, which the eden space resolves, is that entries with high temporal locality and low frequency (e.g. program address traces) had poor hit rates.\nIts too bad Java doesn't expose many SIMD operations. The LIRS stack pruning is technically unbounded and has poor cache locality. So there's potential that the counter reset operation will be comparable with improvements to the sketch data structure.\n. I ported the PyClockPro and did a rough check against the paper. So what causes the differences was a guess.\nI added a clockpro branch which has Cache2k wired up. You can run the simulator at the command line or in your IDE (via Simulator.java). You can change the trace file in reference.conf in the simulator's resource folder. \nChecking a few traces the difference is less noticeable, sometimes in each other's favor, so I might have had a mistake in my prior hack. For example multi2 results are,\n\u2551 irr.Cache2kClockPro         \u2502 49.49 %  \u2502 26,311   \u2502 12,789    \u2502 565.1 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 irr.ClockPro                \u2502 47.84 %  \u2502 26,311   \u2502 13,225    \u2502 258.0 ms \u2551\nAnd in multi1,\n\u2551 irr.Cache2kClockPro         \u2502 49.46 %  \u2502 15,858   \u2502 7,515     \u2502 545.6 ms \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 irr.ClockPro                \u2502 52.51 %  \u2502 15,858   \u2502 7,031     \u2502 212.9 ms \u2551\n. Oh, I know what I did to get 5-8% difference. I didn't set the implementation on the builder and it defaulted to LruCache. Then its 46.51% in multi1, which is expected.\n. @thomasmueller \nI added your bounding of the number of non-resident entries and stack move optimization as optional parameters. The LIRS reference implementation suggests the total size is 3x (1x for resident, 2x for non-resident). If set to a smaller value (e.g double like ARC) most hit rates stay the same except that multi3 drops from 44.5% to 40.5%. I set the stack move distance to 5% of the cache, simply because it seems reasonable.\n. I tweaked my TinyLfu-stlye policy so it now performs slightly better across the board and is near optimal for MRU workloads. I wrote a 4-bit CountMinSketch which reduces the overhead to an long[512] and a slight (0.5-1%) negative impact on the hit rate. \nI plan on using this policy instead of LIRS because it is much simpler to integrate, should have a smaller memory footprint, equivalent hit rate, and doesn't require exposing a tuning parameter. The 4-bit sketch is good enough to start with, but will later be replaced with @gilga1983's TinyTable which is more accurate and denser.\n. I fixed a bug in the CMS causing the top-most bits to be unused, so now its nearly identical hit rate\n. You're right, I was only paying attention to the hash for determining the array index. I hadn't thought about the low diffusion's impact on selecting the counter group (via start), so the supplemental hash is necessary. The impact on simulation runs is mixed, but regardless I think this is a good change. Thanks!\n. Sorry, on review I realized that I hadn't digested your last paragraph. Can you provide more depth to your suggestion for improving indexOf?\nThe constraint is to have four independent hash functions for CountMinSketch. The seeds were chosen naively by generating them randomly, performing a few simulations, and choosing the set with the highest hit rate. This is of course a far cry from your test program to find ideal constants.\n. Oh, since I linked to the first commit you are not seeing the changes on master. The current version is a little smarter. It includes the fix to start.\nThe output of your test case is against master,\n0 freq 2\n1 freq 1\n2 freq 4\n3 freq 1\n4 freq 6\n5 freq 0\n6 freq 7\n7 freq 1\n8 freq 9\n9 freq 1\nThe output of your test case with your indexOf applied to master,\n0 freq 2\n1 freq 2\n2 freq 2\n3 freq 2\n4 freq 5\n5 freq 1\n6 freq 7\n7 freq 1\n8 freq 9\n9 freq 3\nI don't have an intuition for what to expect, so naively the grouping of four 2s looks worse.\n. Right, I lost the efficiency of rotateLeft because the indexOf was changed to take the depth instead. So passing in the start + i as you intended the output is,\n0 freq 2\n1 freq 2\n2 freq 3\n3 freq 3\n4 freq 5\n5 freq 2\n6 freq 8\n7 freq 1\n8 freq 9\n9 freq 1\n. Yes, the original code wasn't very smart as I was just flushing out the overall picture. I had played with a different hashes, like HashMap's old spread function, but was only looking at changes to indexOf and hadn't thought about start. So your observation was really helpful, thanks!\nI improved the seeds by borrowing the constants from FNV, CityHash, and Murmur3. The impact was small since I had by luck found a good set, but it does seem a little more robust now.\nWhen I optimized using a JMH benchmark I saw that removing multiplication significantly helped increment (+35M ops/s, from a base of 100M). This didn't affect frequency (500M ops/s). I suspect that is due to data dependencies due to writes, so the slower operation became noticeable. However the alternatives that I've tried reduce the quality of the hash enough to impact the hit rates in the simulator. Since the increment is applied in batches by one non-blocking thread (default a ForkJoinPool task), there isn't much perceived user latency.\nJava 8 is much more tolerant to the hash attack by degrading to a red-black tree when the list chain exceeds a threshold. The sketch is vulnerable, but since its only for caching I'd expect it merely diminish the hit rate to around a normal LRU's. I agree we could use nanoTime() as a seed in the supplemental hash to combat this. Do you think that's necessary?\n. I'll put in the hash protection. I guess it wouldn't be an LRU. The admission policy would not admit new entries because both the candidate and victim had the same frequency, so the victim is retained (this was better in traces). So yeah, you scored another point :-)\n. I added an Efficiency wiki page that shows the hit rates of the most favorable polices. It includes 5 workloads (Wikipedia, Glimpse, Database, Search, and OLTP) which come from published sources.\nI left TinyLfu (no windowing) out of the results, but tracked it. The window usually didn't affect the hit rate (positive or negative), except in the OLTP trace where TinyLfu did very poorly. So that verifies my original analysis that the window corrects certain low-frequency workloads, but otherwise augments TinyLfu to provide the high hit rate at a low cost.\nW-TinyLfu was always competitive with LIRS. It seems that ARC varies widely with workload.\n. W-TinyLfu is fully integrated, with support for weighted values. All tests pass and LRU ones were migrated. The hit rate matches the simulator's (adjusting for the random hash). I need to ponder over whether there are good policy-specific tests cases that would be simple and worthwhile to add.\nWeights cause some rare quirks. \n- An entry with a weight of zero (never size evict) is retained in the eden queue and never moves to the main queue. Like in LRU these entries are skipped over.\n- To corrects for gaps the main queue is (max - edenSize) rather than a dedicated boundary. This avoids evicting an entry prematurely because there is enough total capacity free, but split between queues.\n- For simplicity we move eden entries to the front of the main queue, which assumes the LRU is the candidate. On a weight change with no candidates then it may evict the LRU entry. This can be worked around to evaluate the last two victims instead when this situation is detected, but seems benign for the time being given the rarity of weight changes in my experience.\n- The movement between queues (w/o segments), as we discussed with LIRS, is handled by keeping a policy-oriented view. When the weight is updated this view is changed during the policy's replay. The final removal of an entry still uses the node-oriented view because an update may be pending (replaying events out-of-order).\n- A newly added entry whose weight exceeds the eden queue's capacity may be rejected outright, resulting in a TinyLfu behavior (still has a good hit rate in general). This is not a concern because small caches with weights doesn't make much sense in practice.\nThese issues would be similar in LIRS or other multi-queue policies. The trade-off of this slight complexity versus LRU is worthwhile, though. The most common usage of weights is for off-heap caches (e.g. Cassandra's) where the higher hit rate has a pronounced effect due to the workload patterns.\n@gilga1983 is working on a TinyTable alternative to our use of CountMin sketch. That would reduce the footprint and replace the sampling period with an incremental random removal. It may improve the hit rate slightly, though I suspect a bigger gain would come from experimenting with LRU alternatives for the queues. This work will probably be considered for a future release, as I hope to be code complete is ~2 weeks.\nThe hit rate is competitive with LIRS, so I think this was a better approach. The half percentage point that LIRS sometimes achieves can be made up for by increasing the cache size (since we don't waste as much memory for non-resident entries).\nWhen comparing against products, I discovered that Ehcache 3 is horrible. They take a very poor sample, effectively the MRU entries, sort by Lru/Fifo/Lfu, and evict. The naive sampling results in a very low hit rate for small caches, and corrects to roughly random replacement for large caches. Except for MRU friendly workloads (loops) the cache is routinely 10% points below LRU and often below random replacement due to choosing a poor victim. This isn't a problem in Ehcache 2, which better mimics LRU.\nThe remaining task for this issue is to implement @thomasmueller's fast-path optimization. This is mostly in place but not yet enabled. I'll try to complete it tonight or tomorrow, and then close this ticket.\nI expect 2-3 weeks before the next release. This is to complete some API tasks and perform a fresh round of benchmarking.\n. Improved the weight handling and integrated in the fast path trick.\nThe read performance was only raised from 150M to 180M ops/s on my laptop, whereas an unbounded CHM is 335M. The culprit is scheduling the draining of the buffers, which should be less frequent with the fast path. If disabled the performance jumps to 285M, but if only parts are disabled it drops to 135M. This indicates to me that false sharing is the likely problem, so I need to figure out what fields to pad.\nSince everything discussed in this thread is done, closing.\n. Released v2.0 with the new policy.\nI had to disable the fast path trick because it reduced performance. I'm not sure why, but the results varied wildly and were always slower. This is a bit surprising since the fewer CAS operations should have helped, but I guess it caused unpredictable thrashing. The code is still present so the optimization can be evaluated again for a future release.\n. @wburns @thomasmueller @cruftex \nThe revised paper is now available as a technical report. This paper is being submitted to ACM ToS journal, which will go through the normal editor / peer review cycle before being published.\n. @Stephan202 yeah, Roy is going to push a revision tomorrow. He reformatted the ToS paper for Arxiv and messed it up!\n. @vlsi Yes, it definitely can be! I clarified the patent issue with Roy and @gilga1983 before adopting my variation. None of us want any patents on our work, as adoption would be far more rewarding.\n. @vlsi Also note that the simulator is provided by Caffeine (section 5.3), includes the 2Q policy (which I think Postgres uses), and it is easy to add new trace file formats. If adoption is considered, it should be easy to capture a trace of Postgres buffer accesses for different workloads and validate our results.\n. @vlsi There are a few different approaches to resolving the concurrency issue, with different degrees of impact to the existing code. @gilga1983 may have some suggestions here. Looking into PostgreSQL's buffer cache, I see now that it is a CLOCK algorithm which is a concurrent way to approximate LRU.\nCaffeine uses a non-blocking global lock. It doesn't use the doorkeeper optimization as I've been happy with the space/time and never experimented with it (I use a 4-bit CountMin Sketch). The approach taken is described in the design doc, which is an evolution of Guava's design and CLHM's design.\nCaffeine's approach borrows from database theory where a database's transaction log greatly improves write performance. In a cache every read is a write to update the usage history, but this information is not directly visible to the client. The reads are recorded in ring buffers and drained in batches under a non-blocking lock. Writes are also buffered but applied immediately, to avoid a writer blocking on the global lock (Guava instead blocks using per-segment locks). This design results in ~33% read throughput and ~90% write throughput of an unbounded ConcurrentHashMap on a 32-way Xeon.\nPostgres might prefer to retain CLOCK and use a concurrent counter. That could be by some form of lock striping (e.g. buckets, segments) or a CAS update loop around the per-counter location. I think both approaches would work effectively, be simple to implement, and provides the majority of the hit rate gains. This is the idea that I think @gilga1983 might have suggestions on.\n. Sorry, here's the correct link for CLHM's design.\n. You're right, CLOCK requires some mitigation like a maximum walking distance before a marked candidate is selected. While a FIFO should retain most of the gains, another idea might be to scan a small distance, select the victim that has the lowest sketch frequency, and then use TinyLFU's filtering. That could easily be added as another policy in the simulator to see if the effects are worth the complexity.\n. @vlsi\nIf it helps, you might find the go-tinylfu project a nice self-contained example. @dgryski was able to write it without much difficulty. It doesn't tackle the concurrency aspects, though. If you end up pushing forward with the Postgres experiments, let me know how I can help.\n. Its been a while, but the top of the stack should be the most recent and the bottom should be the least recent. A quick scan and it looks like I'm always pruning from the bottom, so I'd have to dig in deeper to verify fully.\nI debugged my version against Song Jiang's C version, printing out when they differed. That included finding an optimization they glossed over in the paper (consecutive accesses). I stopped when there was an off-by-one in a few traces, but effectively the same hit rate, so there could be a minor bug here.. head is a sentinel for a circular linked list. So the prev is the bottom and the next is the top.. Please format using Google Java style. You can import it into Eclipse to format your code. Also you should put the copyright header (just copy it from another class).\n. lgtm after you make the minor formatting changes.\nThe common approach is to either make changes as new commits and then squash them before merging, or amend the existing commit and force push. Most people end up doing the latter because its easier, but a little less safe if you want to revisit your old history.\nSo you would make the changes, git add ., git commit --amend, git push -f. Then write a comment to ping me that its ready to review. Or if I lgtm it you could merge into master directly, e.g. git checkout master git merge gilga, git push.\nOnce you have the hang of git, the project's conventions, etc. you don't need to send me pull requests unless you want feedback or want to propose a change outside of the simulator module.\n. Cleaned up the pull request and squashed the commits.\n. I cleaned up your branch. Can you please take a look at let me know if the changes are okay? If so then I can merge it in.\nNote that you didn't wire the new TinyCacheAdmission class for the simulator. I can do this if you tell me what the nrSets and itemsPerSet constructor values should be set to (e.g. config file defaults). Would you want your version and the CountMinSketch to both be evaluated with, or should the count-min version be removed?\nfyi I deleted the simulator/TinyCache/* files (e.g. NewTinyCache since that looked like a temporary directory used for porting. It wasn't part of the project so neither the build or Eclipse saw it.\n. Okay, do you want this merged in as is or do you plan on continuing to work on this pull request?\n. oh okay. In the future no need to send a pull request until you want it reviewed\n. I'm looking forward to playing with this. I think this is based on your tiny table, right? I'd like to see the performance gain of this scheme in your tinylfu (memory & cpu costs). If I shouldn't have deleted the TinyCache let me know and I can revert them back into the branch.\nAlso, let me know if you have ideas for a better name for the EdenQueue.\n. Closing. Please open a new pull request when a TinyTable version is ready. It sounds like you won't have time for now and the 4-bit CountMinSketch is pretty good for the time being.\n. No worries! I think we're in a pretty good spot right now, but it will be cool to try out your TinyTable when you have the bandwidth.\n. You're right! A million unit test executions and somehow I missed checking for this.\nThanks for the test case. I'll fix and release an update this evening.\n. Released v1.3.2 to Maven Central.\n. When this came up in Guava the reason we didn't do this was,\n\nThe stats that tend to matter most are things like \"stats over the last 2 minutes\" and \"stats over the last 15 minutes\" etc. If you reset, you'll be resetting constantly! And you will have to pick one increment or the other. If you reset every minute, then to get the stats for the last hour you have 60 things to sum up.\nIn contrast, just saving absolute snapshots and diffing any two endpoints you want to is a lot simpler and more flexible. \nGoogle's monitoring tools are designed so that components who want to report stats only have to worry about reporting monotonically increasing counts, and they handle the rest, which I think is a good approach that keeps the burden of things like \"resetting\" from being shoved onto every individual component that wants to report stats.\n\nYou can use a reporting library like Dropwizard Metrics or similar to get that behavior. This keeps the cache statistics simple, and those wanting to build a caching framework can handle the bridge code (cache manager, config file, reporting, etc).\n. It would get weird. An invalidateAll() resets the hit count, but invalidate(key) and invalidateAll(keys) does not? Its simpler to think of the statistics over the lifetime of the cache. The plus() and minus() methods on CacheStats makes it easy to take a snapshot prior to clearing the cache to report on the increment.\n. Closing, but feel free to reopen if you (or anyone else) has thoughts on this. I'm happy to entertain api changes for V2.\n. FYI,\nIn v2 you can supply your own StatsCounter implementation. That could provide a hook to manually reset the counters, if deemed necessary. The intent was to simplify integration with reporting tools, though.\n. You're right, the @Nonnegative should be removed as an invalid constraint and there are no usages depending on this assumption.\n. Sorry that I haven't used OSGi so it's not tested, but I thought the bundle instructions were correct. The tracing jar is built with, \ngradle\nbundle {\n  instruction 'Export-Package', 'com.github.benmanes.caffeine.cache.tracing.*'\n}\n. That makes sense that I missed the Import-Package. If there's an easy way to run an OSGi container in a unit test to verify compatibility, I'd appreciate some pointers. The fix should be available in the snapshot repository when the CI passes (~25 minutes) if you'd like to verify.\nThe tracing package will probably be removed in v2 since we collected enough data to simulate policies and choose an optimal one to replace LRU.\n. I probably need an import statement for the guava jar too...\n. Release v2.0 which fixes the OSGi manifests. It also removes the tracing packages, which are no longer needed due to choosing a new eviction policy. Sorry this took so long to release.\n. Thanks. I won't be able to take a look at this until tonight.\nMost of the unit tests use a Ticker to manipulate time. Can you verify that it fails with that, too? If not then it would indicate an unexpected visibility problem.\n. Oh, this is the async cache. Sorry, I need to play with the code to get a sense of what is going on. I just reproduced it failing and see it runs if I use a direct executor but fails on a single-thread executor. So definitely some race, but not sure what yet.\n. Nope, that was my misreading. Since it runs fine with a direct executor and fails with a single-threaded, its a race. I have to verify your code is valid because sleeping and waking are not exact time durations. The big question is whether the race is valid or not.\nSo my first step will be to strengthen the visibility to see if it passes. Right now reads and writes to the timestamp piggy back on other volatiles generating our memory barriers to avoid excessive cpu stalls.\n. I took a look and here's my current guess. I haven't been able to validate it yet, so hopefully I'm not too far off base and will have a patch released by the end of the weekend.\nWhen a write to key1 occurs, it triggers a cache maintenance cycle (on the executor) to evict/expire/reorder entries in the policy's linked lists. The expiration check is racy, as key2 could have expired but then be resurrected by a concurrent update. The atomic removal (via computeIfPresent) doesn't validate that the entry is still expired, though it probably should. This race was assumed to be rare, so one or two extra evictions wouldn't be noticeable. That may have been a bad assumption if this happened often enough for you to investigate and write a test that triggered the race reliably.\nI verified that the tests pass if the maintenance cycle is disabled. I haven't been able to get a hack that resolves the race to work reliably yet, which would let me then fix it properly. So I might either be tired or there is a different issue at play. Regardless I hope to figure it out, fix it, and have a release by the end of the weekend.\n. I was exhausted last night and flipped a gt / lt sign which is why my quick hack of a fix didn't work. It was Yom Kippur and I was out late when we broke the fast.\nSo the above rational is correct. If I hack BoundedLocalCache#evictEntry to use the following your tests pass. I should be able to clean this up, wrap your code into an integration test, and have a release out in the timeframe I promised.\nThanks for doing the detective work to isolate the bug and report it.\njava\ndata.computeIfPresent(node.getKeyReference(), (k, n) -> {\n  if (actualCause == RemovalCause.EXPIRED) {\n    long now = expirationTicker().read();\n    if (expiresAfterWrite()) {\n      long expirationTime = now - expiresAfterWriteNanos();\n      if (n.getWriteTime() >= expirationTime) {\n        resurrect[0] = true;\n        return n;\n      }\n    }\n  }\n  if (n == node) {\n    writer.delete(key, value, actualCause);\n    removed[0] = true;\n    return null;\n  }\n  return n;\n});\nif (resurrect[0]) {\n  return;\n}\n. I have the fix ready and refactored your tests a little. Because this relies on exact timings it can't run reliably as part of the build / CI yet. There is enough other work (concurrent tests, shared CI) that the timing thresholds can be exceeded. I'll fix this later by isolating the test in its own build step, though that will probably only run locally because the CI is too heavily loaded.\nI'll get the next release put together tonight.\n. v1.3.3 released\n. Oh, also note that the git history looks a little weird since I had to rebase master on top. That was to isolate it as a direct patch to the previous release. The current work has been on v2 which will introduce an new eviction policy. I didn't want that to go out prematurely\n. Maven Central sync is broken MVNCENTRAL-840...\n. great! Thanks for the update\n. @yurgis2 \nIf you're using size based eviction, too, it would be interesting to hear your feedback when upgrading to v2's W-TinyLfu policy.\n. I'm sorry to hear there was confusion. However I do think this is properly documented.\njava\n* If the specified key is not already associated with a value, attempts to compute its\n* value using the given mapping function and enters it into this cache unless {@code null}.\nThis follows the pattern set out by the Java Collections Framework, namely computeIfAbsent.\njava\n[Map]\n* If the function returns {@code null} no mapping is recorded.\n[ConcurrentMap]\n* This implementation assumes that the ConcurrentMap cannot contain null values\n* and {@code get()} returning null unambiguously means the key is absent.\n[ConcurrentHashMap]\n* If the specified key is not already associated with a value, attempts to compute its\n* value using the given mapping function and enters it into this map unless {@code null}.\nLike java.util.concurrent collections, the cache does not allow storing null values and a null is always an indicator that the mapping is absent. If your intent is to support negative caching (cache that the entry does not exist), then the pattern is to return an Optional value. This works well except in the case of weak/soft value reference caching.\nIn Guava we disallowed null computations, threw a checked InvalidCacheLoadException if a loader returned null, and LoadingCache.get(k) is checked. We had originally used nulls as above, but switched based on Josh Bloch's review where he preferred checked exceptions as better matching Java 5's coding style. Now that Java 8 makes null the preferred style (lambdas are hostile to checked exceptions, JCF's compute methods), we're following that idiom.\nDo you think the JavaDoc should be more explicit by discussing null in its own paragraph, with a little more warning in the writing? I'm a little hesitant only because I think once developers become more familiar with Java 8 and Map's compute methods this won't seem so surprising. It will instead seem natural and the warning verbose.\n. Thanks! I updated the Population wiki page.\n. That's Infinispan_Old_Lru which was written around the same time as Guava's Cache. They also ported Charles' prototype of LIRS, which we never integrated into Guava. So not surprisingly it has similar performance characteristics.\nInfinispan_New_Lru is a rewrite based on ConcurrentHashMapv8 and ConcurrentLinkedDeque. Its similar to my earliest prototypes, where a lock-free deque is used for the LRU queue. The problem is that every read is a write, so it thrashes worse than a lock (hence the poor performance). In its favor is that the LIRS implementation correctly handled non-resident entries, so it has a much better hit rate than Charles' prototype code.\nOLD would be much faster if it used a ring buffer, like CLHM does (and I keep trying to get Guava to). New could be a lot faster if it borrowed @thomasmueller fast-path trick to skip over hot reads.\nThe reason why New was adopted is their benchmarks incorporate a high miss rate. The new CHM is much faster, so misses skew the results dramatically. I intentionally avoid that as hits are the expensive and desirable case of a cache.\nSee CacheTypefor the factory of all the integrated implementations.\n. You'll probably be interested in Window TinyLfu. The integration is complete, I just haven't had the time to benchmark which is holding up the release.\n. If you want performance, then yes. Its used by Cassandra, Spray, Camel, Solr, etc, and was the first concurrent Java cache. Guava was the intended successor, but a lot of simple performance fixes weren't carried over. If you want features, which is the most common case, then Guava's is a good default.\n. I released v2.0. This would be a better fit for ratpack as both are Java 8 based, is significantly faster, and has a much higher hit rate.\n. I know, but saw you used BCHM in ratpack and Gradle is 1.6 :)\n. @alkemist You may enjoy reading our research paper.\n. Its generated via the Gradle build file.\nDo you have any recommendations for how to test OSGi? I've never used it and I seem to screw up the manifest in every release. Is PaxExam the right approach for compatibility testing?\n. I was partially successful and need your help to debug. I added a JUnit sanity test in the caffeine and guava modules, due to the TestNG integration eagerly initializing the container even if the test is skipped (broke tests). The caffeine sanity test passes after your fix.\nThe guava test fails with the exception below. The class should be exported and the manifest looks correct to me. Can you take a look at the osgi branch?\nYou can run using gradlew :guava:test.\ncom.github.benmanes.caffeine.guava.OSGiTest > sanity FAILED\n    java.lang.ClassNotFoundException: com.github.benmanes.caffeine.guava.CaffeinatedGuava not found by PAXEXAM-PROBE-bee238e9-41a5-4071-8e3e-776f37fe25be [16]\n        at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1574)\n        at org.apache.felix.framework.BundleWiringImpl.access$400(BundleWiringImpl.java:79)\n        at org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:2018)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at com.github.benmanes.caffeine.guava.OSGiTest.sanity(OSGiTest.java:59)\n. got it!\nThe error is due to using Guava 18.0 in test's mavenBundle, while the build uses 19.0-rc2. This caused the resolution to fail with a misleading error message. The reason why they differed is because I accidentally set the version to the full artifact coordinate, which it said wasn't on maven central, so I hard coded it to an earlier version. Fixing the system property and it passes. I guess I'm tired :)\nI'll try to finish the integration tests tomorrow and cut a patch release.\n. Any chance you're familiar with OSGi + ServiceLoaders? I tried adding the instructions described here, but no luck so far. Probably because tests are consumers and I'm not declaring that right.\ncom.github.benmanes.caffeine.jcache.OSGiTest > sanity FAILED\n    java.util.ServiceConfigurationError: javax.cache.spi.CachingProvider: Provider com.github.benmanes.caffeine.jcache.spi.CaffeineCachingProvider not a subtype\n        at java.util.ServiceLoader.fail(ServiceLoader.java:239)\n        at java.util.ServiceLoader.access$300(ServiceLoader.java:185)\n        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376)\n        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)\n        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)\n        at javax.cache.Caching$CachingProviderRegistry$1.run(Caching.java:438)\n        at javax.cache.Caching$CachingProviderRegistry$1.run(Caching.java:432)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.cache.Caching$CachingProviderRegistry.getCachingProviders(Caching.java:432)\n        at javax.cache.Caching$CachingProviderRegistry.getCachingProvider(Caching.java:370)\n        at javax.cache.Caching$CachingProviderRegistry.getCachingProvider(Caching.java:351)\n        at javax.cache.Caching.getCachingProvider(Caching.java:142)\n        at javax.cache.Caching.getCache(Caching.java:279)\n        at com.github.benmanes.caffeine.jcache.OSGiTest.sanity(OSGiTest.java:54)\n. Did you pull from upstream? Its pushed\n. This might also be due to JCache #326 or at least be a future blocker.\n. Thanks for trying. It looks like Ehcache3's tests bypass the service loader so I'll try that tonight. I gave it a quick shot and it reported another not found error, so at least its a path to debug down.\n. It turns out that the underling error message goes to standard error, which is available as a separate tab in the html report. The problem is due jsr107 not exporting its packages, as noted in the bug report.\norg.osgi.framework.BundleException: Unable to resolve com.github.ben-manes.caffeine.jcache [17](R 17.0): missing requirement [com.github.ben-manes.caffeine.jcache [17](R 17.0)] osgi.wiring.package; (osgi.wiring.package=javax.cache.configuration) Unresolved requirements: [[com.github.ben-manes.caffeine.jcache [17](R 17.0)] osgi.wiring.package; (osgi.wiring.package=javax.cache.configuration)]\n    at org.apache.felix.framework.Felix.resolveBundleRevision(Felix.java:4111)\n    at org.apache.felix.framework.Felix.startBundle(Felix.java:2117)\n    at org.apache.felix.framework.Felix.setActiveStartLevel(Felix.java:1371)\n    at org.apache.felix.framework.FrameworkStartLevelImpl.run(FrameworkStartLevelImpl.java:308)\n    at java.lang.Thread.run(Thread.java:745)\n. Can you try the snapshot jar and verify that it works for you? If so, I'll then release the update.\n. Awesome, thanks!\n. Released. Might take a hour or two before all the mirrors get in sync. Thanks for the help on this one.\n. Thanks!\n. That's an excellent point and thanks for the test case + analysis. I think your fix is correct, except most likely the added check should be only around afterWrite so that the method eagerly returns.\nI'll review to make sure there are tests for all cases when the CacheLoader (or mapping function) returns null (e.g. refresh) and probably have a patch release out tomorrow or the day after.\n. There's a test case for the scenario in a generalized fashion (so all configurations can be verified). It looks like I missed this because I disabled logging to avoid spamming the results as some tests assert the exceptional conditions. Since the internal data structures of the cache weren't corrupted (e.g. removal task had nothing to do) the validation aspect passes. So the issue is benign except for scary log messages.\nSince this is the executor failing but the logger swallowing the cases, I'll probably have to do something quirky like supply an executor that remembers if it failed. I think figuring out how to communicate this as a test failure (so other cases are caught) will be the tricky part of this bug fix. For example asMap().compute(key, (k, v) -> null) has the same problem with AddTask. Ideally the test executor should highlight that scenario as well.\n. @seidler2547 I agree cleanUp shouldn't be expected to throw an exception in general. Adding a guard makes it tricky to figure out a nice way to test since I can instrument the executor which delegates to cleanUp. Since a same-thread executor could be used, changing to an unsafe method is still leaky. If we assume my code won't throw because its fixed, then the executor and such things behave fine without a catch guard clause.\nA problem is that CacheWriter might throw on delete, which could be on an eviction. That cancels the mapping change and the outer operation, which might be a cleanUp. Caffeine.writer(...) is documented with a warning,\n\nWarning: any exception thrown by {@code writer} will be propagated to the {@code Cache} user.\n\nSo I think its okay to say that a CacheWriter.delete() can propagate to the cleanUp() caller, just like it might on any other write. The user has to be careful when writing a CacheWriter anyways since they are using an advanced integration option.\nI think if we can agree that's okay, then testing isn't hindered.\n. Okay, cleanup() now avoids propagating the exceptions as expected. It took me a little bit to get all the details sorted out I guess.\nI'll release 2.0.2 now with this fix. Thanks again.\n. This is now released. It took a little while to work out some test issues. Should sync from staging to central within 2 hours.\n. That is weird. Can you provide a test case and configuration so that we can try to debug it?\nNote that if you have a recursive computation that can spin indefinitely (see JDK-8062841).\n. let me know how things turn out. I'm guessing your computation is trying to modify the cache and that's a known issue with CHM. I worked with Doug on the fixes to fail fast, but its been a year and they're still not in a JDK8 release.\nIf things look messy I can probably meet up over the weekend. I'm right by downtown Mountain View, so pretty close.\n. Not sure if it will help, but you can use the existing testing infrastructure. The obvious recursive scenarios are disabled, e.g. AsMapTest#computeIfAbsent_recursive. Easiest is to run selectively in the IDE, which supply hundreds of permutations.\n. I wouldn't expect it to, as there is no weird ThreadLocals floating around in Caffeine or CHM. That's reasonable to expect to be safe.\n. Great! Hopefully Doug's fixes will get in and it will fail fast in a future JDK release.\n. Sure, that's a good idea to add to the wiki page. If you take a look at Copier and its associated implementation then you should have an idea of what is involved. I'll try to add an example this evening (PST time).\nI generally don't recommend JCache because I think it has numerous flaws. If you're using it for Spring, native support is planned.\n. Oh, and why do you need a custom serializer? That is only for store-by-value which copies entries entering and leaving the cache. The main use is for distributed caches and store-by-reference is the more appropriate (and default) behavior.\nSo perhaps first we should understand why store-by-value is a good use-case.\n. Seems to be planned as part of Spring Boot 1.4. I haven't used Spring since 2.0 so sorry if I'm a little behind on things. :)\nCan you use the Guava adapter with Spring's Guava cache extension? Then when 1.4 is out you can switch over and remove the indirection. I assume that is easier than adding JCache?\nI didn't know anyone outside of Google used CacheBuilderSpec! It was really useful for the internal flags, never noticed anyone using it otherwise. If it will simplify Spring integration then its worth adding.\n. JCache has a lot special configuration aspects so unfortunately that can't be shared. I also like the library over framework approach, so I prefer leaving external files to let frameworks (like Spring) decide what integration should look like. I'm hopeful that I won't need to provide a config and cache managers, and can instead provide the hooks like we did with Guava.\nSo I'm going to rename this task to be about adding a CaffeineSpec and supporting CacheBuilderSpec in the Guava module.\n. St\u00e9phane Nicoll indicated on the Spring task that a CaffeineSpec shouldn't be necessary and that he'll take care of doing it the right way for Boot. I'm eager to see what looks like, but leaving this open in case there's a misunderstanding and a spec is useful. If it doesn't aid Spring's integration then I'll close this until another user reopens for discussing their need.\n. Spring Boot #4903 provides integration. Now I understand why a CacheBuilderSpec wouldn't have been helpful to the integration, so I'm closing this.\n. Are you using Spring? If so then it sounds like you want to externalize the configuration parameters? The Spring Boot integration seems to imply that Java configuration is their new preference.\nIf you're not using Spring, can you describe how you are configuring your app? For example, what file format are you using? Is this needed for any other popular frameworks or internal conventions? Is this desirable for a Guava bridge (guava adapters) or general library use?\nCacheBuilderSpec was primarily for Google's internal Flag library so the single line CSV style made sense given how flags are parsed. I'd imagine most users would want the configuration broken into multiple entries and have to perform some munging between their format and the spec's. For example with TypesafeConfig (json-like) would require iterating over the sub paths, converting snake to camel case, and joining the strings. A spec would still be useful, but some custom code required.\nIf a CaffeineSpec is useful than I agree it should be added. Since it seemed to not be of interest to the Spring guys and originally for Google's internal use, I hadn't bothered.\nFeel free to reopen or start a new ticket.\n. Sounds reasonable. I'll take a look at offering a port of CacheBuilderSpec. Are there any aspects of Guava's spec that you'd want changed / reconsidered?\n. I really appreciate the feedback, @gokhanoner.\nA Guava-like spec is as far as I'd go for the core module. I'd like to keep it as a library, whereas a configuration file leads to cache managers, numerous dependencies, and an opinionated framework mindset. That is best left to integration frameworks (e.g. JEE, Spring, Dropwizard, Play, Grails) which each have their own file formats, life cycles, etc.\nI will provide integration modules when helpful like JCache for the JEE world, which in that case didn't have a format convention. Perhaps XML is the JEE norm due to legacy, but because there wasn't a expectation I used Typesafe Config (a powerful and friendly json dialect). Unfortunately JCache is a highly flawed standard (even the RI is warned to not be used in production), so applications have been very slow to adopt.\nWhen I used Spring in the 2.x days its convention was to use property files with EL in the XML wiring. I'm a little surprised that Spring didn't expand CacheBuilderSpec into a set of properties, which are then joined to feed into the spec. Perhaps by offering a spec they'll decide to offer that as an option, or a Spring module could be provided as an aid.\nI think being a library but trying to work with framework authors to make integration as painlessly as possible is the best choice. The idea of a CaffeineSpec has merit and helps in that goal, so I hope to flush that out shortly.\n. 2.1.0 is now released with CaffeineSpec. Thanks again for the feedback.\n. @gokhanoner @koen-serry \nSpring 4.3-RC1 was released. I assume the next Spring Boot RC will follow shortly. These have Caffeine (with CaffeineSpec) integration for Spring Cache. Hope that helps.\n. Oh, Spring Boot 1.4-M2 is already out. That's the version that added Caffeine support.\n. I'm not planning on providing a non-concurrent version, can you explain why that would be desirable?\nIdeally the single threaded performance shouldn't be that much worse than a non-concurrent version, despite the added complexity required. I quickly hacked the GetPutBenchmark to show the results below. The upper bound should be an unsynchronized LinkedHashMap and the performance loss is reasonable when compared to other concurrent caches. Unless configured to not use a ForkJoinPool.commonPool() thread for maintenance work, the impact is negligible.\nI'd argue that in a single-threaded case the cache probably won't be the bottleneck. I'd lean towards saying the addition isn't worth the conceptual weight of adding another configuration option. Probably users would be adequately served by a LinkedHashMap (in LRU mode) with the Java 8 compute methods. Extra features like expiration, weak/soft references, etc. likely aren't of much interest there.\nThe primary use case I know of for a single-threaded version is very Google specific. Guava provides a trimmed down cache that is compatible with GWT for transpiling into JavaScript. Similar might be offered for J2ObjC transpiling to Objective-C (iOS).\nNote that the concurrent cache is not lock-free. Rather it avoids lock contention and unnecessary memory barriers from delaying the cpu.\n| Policy | Read | Write |\n| --- | --- | --- |\n| LinkedHashMap_Lru (synchronized) | 24,585,737 | 25,302,704 |\n| LinkedHashMap_Lru (unsynchronized) | 27,493,881 | 27,553,367 |\n| Caffeine (default - FJP cleanup) | 27,947,147 | 12,990,017 |\n| Caffeine (same thread cleanup) | 11,626,684 | 8,832,595 |\n| Guava | 9,112.000 | 8,017,133 |\n| Ehcache2_Lru | 6,913,574 | 2,802,196 |\n| Ehcache3_Lru | 6,146,850 | 3,702,588 |\n. Its an n-way associative cache, which I think is a better way of describing it.\nI did try adding it to the simulator when he publicized it a few weeks ago (note that its a month old, not 1yr like copyright implies). I think it has similar LRU hit rates. At first I accidentally grabbed his IntLruCache which has uncharacteristically bad performance.\n. Oh right. My complaint is that his version does an O(n) move on every access. I'm not sure why he didn't use System.arrayCopy at least. It definitely isn't very performant.\nI'm not going to add Argona into the suite because its unfair to them. That benchmark requires concurrent access with Object keys which negates their advantage.\n| Benchmark | Read | Write |\n| --- | --- | --- |\n| int | 126,676,062 | 27,466,359 |\n| Integer | 113,786,703 | 26,011,716 |\n| Integer + synchronized | 44,033,221 | 25,560,991 |\nWhenever you can specialize a data structure for your domain its a great performance win. If you can specialize to integers then the best approach is probably an n-way cache with TinyLFU filter and a custom API. It looks like you could borrow his IntLruCache and my FrequencySketch to combine them easily enough.\nMy gut feeling is that its not appropriate for Caffeine to provide this all. But it is good to advocate the ideas so that others can write specialized versions or provide them in a fastutil style library.\n. Have a great weekend too! Thanks for the feedback, its always welcome. :)\n. I took another look at Int2ObjectCache and one thing to be aware of is that it uses a very poor quality hash function. This makes a lot of sense, as I saw similar performance and a drop when making the hash function for the FrequencySketch more robust.\nThey took the hash function from Java 8's ConcurrentHashMap, though removed the comments explaining why a poor spreader was not a concern (degrades into a tree). For an n-way cache the degredation is a higher miss rate with a higher throughput due to cpu cache efficiencies. The simpler function results in fewer data dependencies in the pipeline, so overall a greater throughput. The clustering effect is very noticeable in Ehcache 3 which is one reason why its random sampled eviction to be very slow.\nThe sketch uses a better hash function, thanks to advise from @thomasmueller. It also includes a random seed to protect against collision attacks. I suspect that in some traces Int2ObjectCache would have a poor hit rate but that could be compensated by a TinyLFU filter. Of course that should use a higher quality hash function, but the hash could be reused between the two structures.\nIn short, as an n-way associative table the performance is dominated by the hash function (just like my sketch). The lower quality the higher throughput, with a probably negative impact to the hit rate and resiliency to some workloads.\n. That's a fair critique and should be fixed.\nHowever, dead code elimination shouldn't affect the benchmarks. The reason is because it cannot eliminate code that has stateful side-effects, such as mutating a map. It can eliminate branches or computations that are never used. In all other cases those side-effects mean elimination can't occur, except in the one you pointed at (where it by chance doesn't appear to). I vaguely remember a discussion of if JMH tries to fix that in its byte-code rewriting, but regardless the style should be improved to minimize the chance.\n. Oops, I misspoke about that one case. You're definitely right :)\n| Benchmark | Throughput |\n| --- | --- |\n| freq (elimination) | 522,384,286 |\n| freq (no elimination | 62,048,286 |\n| increment | 48,950,376 |\nThat also shows that the gain of using Unsafe to avoid array overhead (object header, bounds checking) isn't adding much anymore as when removed the frequency result is 59,098,743. So I might revisit and simplify that code.\n. You can run the benchmark using,\n```\nterminal 1\n$ gradle jmh -PincludePattern=FrequencySketchBenchmark\nterminal 2\n$ tail -f caffeine/build/reports/jmh/human.txt\n```\n. Thanks for this. Do you want me to release v2.0.3 now with this fix?\nI hope to profile the JCache adapters over the holidays or in early January. The spec makes it hard to be performance centric, but I think I can at least get rid of some of the redundant native calls for the time. At the time I focused on the TCK and trying to design an event dispatcher that would be efficient.\n. @rogerkl I reviewed the time usages and fixed the statistics MXBean which was misreporting the average operation times. As you know it passes the TCK, but unfortunately that's not good at catching these silly mistakes. I can release whenever you want, e.g. soon or wait in case you find other issues we should fix.\n. Thanks again :-)\nPlease be aware that @CacheResult does not perform the computation atomically. This is a requirement in the spec, which dictates that a racy get then compute then put sequence must be performed (see annotation's JavaDoc). This means the usages are vulnerable to the thundering herds problem.\n. @rogerkl I profiled the adapter and fixed a lot of low hanging fruit. This raised read performance from 20M to 135M / s and writes from 9M to 35M / s. That's on a simple profiler hook benchmark, not JMH, so its observational only. I only focused on get & put, and applied the same fixed across other methods.\nThe main culprit was the slow calls to get the current time, which is most often unnecessary. That's a slow operation, especially when being thrashed. Its more an issue in micro-benchmarks than systems.\nThe writes are still slower than I'd like (should be closer to 65M), but it has to use a slower compute path. There may be some low hanging fruit I missed too. Since caches are read heavy its probably okay.\nI think a release is probably worth doing soon so that the JCache adapter is more production worthy.\n. Released 2.0.3\n. In Guava the processing is performed under the segment lock, thereby blocking future writes to that segment. The removal of segments means that there is a global lock, which is operated on asynchronously to not become a bottleneck. I'd expect you'd see a similar problem in Guava because RemovalListener is asynchronously processed too. So I'm a little surprised by this as it seems the problem could occur in either library.\nYou can specify the Executor which is where the maintenance work is performed. Perhaps that can be leveraged to throttle writes? It might be enough to put the penalty back on the calling thread (Runnable::run) or use a lock that blocks writers until complete. There could be a hack with CacheWriter.\nI'm not sure what makes sense to change at the library level, but I think we could figure out a way to tune it at your usage.\n. In Guava the RemoveListener is executed outside of any locks on a caller's thread. Most likely the writers are busy processing the removals, so it appears to throttle the rate (unintentionally). Technically the segments are free for other writers to busily insert entries while the listener is doing work.\nIt seems like you need a more explicit rate limiting scheme. If your writes are in a thread pool then you could have Caffeine use it, which would then throttle based on the number of threads in the pool. Another option might be to block inserts while expiration is being processed, perhaps with a lock. I'm not sure what to recommend so you might need to experiment a little to find the best match.\nHope that helped a little.\n. Sam,\nI sat on this for a while in case an idea struck me for how to map this to a generally useful feature. I think it could be adequately solved at a layer above the cache and is better suited to a customization in your code.\nBasically you're using the cache as a means to throttle the write rate. The fact that Guava delegates execution of the removal listener (outside of any lock) to the calling threads was sufficient. Guava optionally lets that be asynchronous to avoid high latencies and have a predictable response time. Now that Java comes with a globally shared executor, I think making it asynchronous is a better default choice.\nI see a few simple approaches that might work well for you.\n- Use a shared Semaphore within your RemovalListener and CacheLoader to limit the number of concurrent writes.\n- Use a Guava RateLimiter in your CacheLoader to reduce the pace of creation.\n- Mimic Guava by having removal listeners publish a task to a shared ConcurrentLinkedQueue. All cache accesses should poll the queue checking for pending work and running the consumed tasks.\n. Closing because I don't think there is anything actionable to be done within this project. Feel free to reopen to discuss or email me privately if I can be of help. Thanks!\n. Would CacheWriter work here? It is called synchronously with the removal.. If you are using a LoadingCache and removal is delayed until the next access due to races or inactivity, the writer is notified synchronously before the computer. So it could be the right approach.. nifty, thanks!\n. Thanks!\n. Thanks for the detailed and thoughtful request!\nPlease correct me, but I think that refreshAfterWrite provides this capability in a minimal form. An entry that is stale (but not expired) is automatically refreshed if accessed. This means that entries that are stale but not accessed will be eligible for eviction by the size/expiration constraints. The user guide includes a little more depth than the JavaDoc linked above.\nA limitation is that the refresh is not performed in bulk (#7) during a batch get. I believe this could be done without significant effort, but I have not put any effort towards that task yet. A LoadingCache would then have a refreshAll(keys) method to provide the optimal entry point for explicit refreshes. If coalescing one-off accesses into a batch refresh was desired, this could be done as a user hack by having the refresh simply enqueue the operation for a background thread to refresh every so often.\nAs with Guava and CLHM, I've tried to stick with the idea of not requiring my own threads, using O(1) data structures, and being library centric (to be customizable, rather than a dictatorial framework). That means preemptive notification without entry access is not directly available. However, hooks are provided to let users peek in to solve their custom needs. The Policy exposes enough information for your own periodic refresh thread (with a smell of reusing the Policy.Expiration interface). That would provide the ability to refresh entries regardless of being \"hot\", which would otherwise be done on-demand.\nI think that your needs are covered, but please poke holes in my argument.\n. Great! =)\nAs a notification you might use replace to avoid a refresh from stampeding over an explicit write (if that's used). The races on the client would be the same internally, unfortunately. We would need to lock all entries, batch load, populate, and unlock. We can't do that with synchronized except in raw byte code, so a getAll is racy. The AsyncLoadingCache is able to perform a blocking getAll by inserting future that proxy to the batch operation. I can't see how to reuse that trick for a batch refresh, though, so I think it would always be racy by allowing duplicate loads.\nIf you come across ways to improve the documentation please let me know.\n. You're right, as usual. :)\nI was mostly focusing on offer when I originally wrote the benchmarks. The case for the cache is lots of user threads writing and an async task draining to replay the events. So the draining part is only there for avoiding OOME in the queue, and to keep filling the buffer. The read buffers can be lossy, so its mostly about minimizing the cost when contended (which StripedBuffer helps alleviate). I wouldn't be surprised if my collection wiki pages have skewed numbers, as when generalizing I could have forgotten my original intents with the benchmarks.\nI am planning on gradually dropping the collections, since your jctools covers that more thoroughly and this project has matured to have more focus. The stack is marked @Beta so its not bound by semver rules and not used by the cache, so it will go in the next minor release. The queue is public, so that has to wait for a major release. The reason why I'd like to hide it is that I don't know how migrating to VarHandles will affect things, e.g. such as the inability to use the threadLocalRandomProbe trick.\nI would like to learn how to write better and more correct benchmarks. Do you have any good examples that I can borrow from and experiment with?\n. Thanks for the tip on @AuxCounters! I started playing with them and its really nice to see the data broken down.\n. I removed the stack, scheduled the queue for removal in v3, added @ AuxCounters to some benchmarks, and removed the collection wiki pages.\nWhen the SingleConsumerQueue is embedded I'll revisit what to do with it. Its usage as a write buffer never shows up in profiling, so the extra complexity of the arena isn't useful. The best options then is either to track JCTools' MpscLinkedQueue or use a growable mpsc array queue. If the queue is not a hotspot, then being GC hygienic seems like the right design preference. An implemented is available in Jetty so that should be easy to experiment with.\nSince the flawed benchmarks and associated classes were removed, I'll close this issue. Thanks for teaching me another neat JMH trick! =)\n. Oh perfect. Those are the slides, here is the paper. You might have read it because I mentioned them in jctools #59, but hadn't known of the Jetty implementation then.\n. Ideally you would use LoadingCache#get(key), which uses a CacheLoader to fetch the value if not present. This provides additional power and reduces the clutter in your code if you have multiple places where you lookup in the cache.\nThe Cache#get(key, mappingFunction) is the next best. This is the same as computeIfAbsent except domain oriented and guaranteed to be atomic.\nLastly Cache#asMap is an escape hatch, where the provided computeIfAbsent is atomic (the Map's contract doesn't require this!). The map lets you do things that would make less sense in the context of a cache (e.g. iterate over the keys), but trusts that users know why they need this without burdening the contextual API.\nDo you have any suggestions for helping to clarify this? The user guide and javadoc try to nudge the user in the above preferences.\n. This is a common misunderstanding, so advise on clarifying the documentation would be appreciated. The rational is discussed in the user guide and the builder mentions this in the class and expiration method JavaDoc.\nTo expire and notify promptly would require a background thread and a high penalty for every read and/or write. The common eager approach is a background thread, a O(lg k) priority queue, and weak reference tasks. This unfortunately becomes very expensive for large, high throughput caches. Because of that many caches leave expired entries in place, rely on size eviction, and do not promptly notify.\nOur approach is a middle ground which has worked well in practice. The order is maintained on O(1) priority queues and expired entries are removed + notified during the cache's maintenance phase. That amortizes the penalty of locks, etc. in a similar idea of how ArrayList amortizes the resize penalty when elements are added. The maintenance penalty and notification is performed using the executor to avoid incurring the latency on the calling thread.\nIf your cache is rarely read from or modified, then you can call cache.cleanUp() using a scheduled executor. That is usually unnecessary though, but a good hook to be aware of.\nI think that no matter what system you used, its behavior would be surprising. One that tried to promptly notify simply wouldn't scale, so it would only mask the problem during testing but failover in production. As a design trade off, I think the scheme described is the best we can manage.\n. I'm sorry there is confusion, but I am having trouble reproducing a problem that does not fit into the design described above.\nIn your test you do not insert any entries into the cache. In that case then no eviction will occur.\nIf I insert an entry (\"a\", \"b\") into the cache prior, an eviction will be delayed until the next maintenance cycle. That is triggered by activity (reads, writes, or an explicit cleanUp). When I add that into the loop then the listener prints a b EXPIRED and statistics show evictionCount=1.\nThis is why I concluded the misunderstanding above. There is no guarantee of when the listener is called when the cache is inactive.\nIf it helps clarify things, the test cases associated with this feature are ExpirationTest and ExpireAfterWriteTest.\n. Can you take another look at this?\n. Closing because I can't find a problem. Please reopen if you are able to put together a reproducible test case that violates the described design. Thanks!\n. I'm afraid that this might make the README too busy. It seems that people find their way to the issue tracker, email, or SO already so the hint may not be necessary. \n. cherry-picked onto master, thanks.\n. No, sorry. The library is intended for server-side Java 1.8+. The API and implementation take advantage of features that cannot be easily back ported (e.g. using retrolambda). I'd recommend,\n- If you are on Android then prefer LruCache.\n- If you want features, then use Guava's cache.\n- If performance matters most, then use ConcurrentLinkedHashMap.\n. You're welcome to send me a private email, or create / add onto an issue. As long as you get answers I don't mind what mechanism.\nI'm a little confused with your last statement about compute methods, which are Java 8 only. Or do you mean that you'd use ConcurrentHashMapV8 for a back port of that feature to Java 7?\n. Oh okay. Adding this to the Java 7 issue was confusing.\nCLHM doesn't handle computations natively, so you would fallback to the looping retrials from the default method. Or you could use a decorator, but more work than you'd prefer.\nCLHM is built on CHMv8 backport and Caffeine on CHM, so the raw hash table will always win. I included that as \"unbounded\" in my benchmark results. Note that CHM is pessimistic about computations, so I pre-screen to avoid unnecessary locking. That has a big benefit for fewer than 32-cores and is less pronounced on larger machines. Caffeine has an unbounded version that is CHM with prescreening, interface adaptions, and optional listener/writer/stats.\nI think the answer you're looking for is that CHM is fine to use and really great.\n. That is really weird, since it's failing on an enum field. Are you running anything like proguard to minify the jar?\n. By the way, for your use case you probably want Suppliers.memoize from Guava.\n. Guava's is an abstraction of the double-checked locking idiom. The caller or the delegate function may be recursive, but neither library supports recursive calls into its code (e.g. cache.get(a) --> cache.get(b)). I think for your case the answer should be yes given you don't need any input key.\n. I think that solution is broken as well. It may appear to work, except if the table resizes. The more verbose solution (get, compute, put) is the only safe one for a recursive map operation.\n. The fix was to throw an ConcurrentModificationException so it would force the recursive logic to fail. So the solution is broken, just fails fast instead of corrupting the hash table during a resize operation.\n. To catch up from the private email, where we worked out a few of the details before this ticket.\nCurrently the way to return a CompletableFuture directly is unpleasant by requiring that the asyncLoad (and maybe asyncLoadAll) are implemented by the CacheLoader. This still requires load be implemented, though it might do nothing if never called (only on reload). So while workable, the ideal would be to have an AsyncLoadingCache functional interface so that (key, executor) -> future lambda could be passed to the builder.\nTo our surprise, we think that this can be implemented elegantly without a breaking or invasive changes. It could be released in a minor version and be a natural extension to the existing code. The idea was that we can have CacheLoader extend AsyncCacheLoader to define / redefine the default methods of the asynchronous interface. Then buildAsync could accept either loader, the internal code would be easy to manage, and the flow would be understandable.\nA quick stub of the interfaces would look something like,\n``` java\n@FunctionalInterface\ninterface AsyncCacheLoader {\n  CompletableFuture asyncLoad(K key, Executor executor);\ndefault CompletableFuture> asyncLoadAll(Iterable keys, Executor executor) {\n    throw new UnsupportedOperationException();\n  }\ndefault CompletableFuture asyncReload(K key, V oldValue, Executor executor) { ... }\n}\n@FunctionalInterface\npublic interface CacheLoader extends AsyncCacheLoader {\n  V load(K key) throws Exception;\ndefault Map loadAll(Iterable keys) throws Exception { ... }\ndefault CompletableFuture asyncLoad(K key, Executor executor) {\n    // define\n  }\ndefault CompletableFuture> asyncLoadAll(Iterable keys, Executor executor) {\n    // redefine\n }\ndefault V reload(K key, V oldValue) throws Exception { ... }\ndefault CompletableFuture asyncReload(K key, V oldValue, Executor executor) {\n    // redefine\n  }\n}\n// both work!\nCaffeine.newBuilder().buildAsync(key -> value);\nCaffeine.newBuilder().buildAsync((key, executor) -> future);\n```\nThis introduces asyncReload for supporting refresh, which is asynchronous but wrapped in an executor in the cache's code. That method will be called by AsyncLoadingCache, while LoadingCache is free to call the reload directly. This is preferred so that the reload is atomic within a compute block.\nA quick pass to see how this might work is in the async-loader branch. That needs a lot more effort, but it feels correct so far.\n. A little more work and the existing tests pass, making this mostly done.\nThe parameterized test generator needs to be updated to create cache variants using an AsyncLoadingCache and a few new tests added. That will help in coverage and be an assurance, as I'm fairly confident due to CacheLoader extending the async version that the flows are working correctly. So if tests pass now the more limited API should just work. Then only JavaDoc updates will be necessary before this is mergeable.\n. @ondbar I think this is ready to merge. Would cutting a release (2.2.0) for this feature be helpful for you?\n. Released 2.2.0 with this API improvement.\n. The Argona Queue is based on Dmitry's algorithm as well. I use relaxed reads for the consumer and enhanced the queue with a backoff arena to handle producer contention. Their's also does not fully implement the Queue interface by throwing UnsupportedOperationException for most methods.\nI plan on removing the queue from the public api in 3.0. It is only used for write operations and the cache might switch to an array based approach to avoid garbage. That is because the queue doesn't appear to be a performance bottleneck for the write path. If you need a queue, then prefer jctools which is dedicated towards providing those collections.\n. Closing, but I'd be happy to discuss further (especially if you have any suggestions).\n. Writes seem to be throttled by the entry update time. A concern I have is if heavy writes causes excessive queuing up of the async tasks. I have an idea of how to reduce that with a smarter state machine, but it also might be a tad complex. Is that the problem you're observing?\n. Let me know if you see anything. The scheduling state machine is very simple (idle, required, processing) and ideal for read-centric workloads. A write forces the state to required, potentially causing a pile up. The idea is to break processing into two states indicating its transition point (processing_idle, processing_required) so that a write can safely skip scheduling if it sees that a maintenance task will be triggered promptly. This shouldn't be too hard, but I haven't prototyped the idea yet.\nAsynchronous weighing is already supported. The incomplete future is inserted with a weight of zero, meaning that it won't be evicted and takes up no capacity. When the future completes successfully it is re-inserted (replace(k, f, f)) to trigger a new weighing based on the materialized value. That may then trigger an eviction.\n. Oh! I see. By asynchronous weighing I meant for use by an AsyncLoadingCache which holds futures.\nThe weight has to be calculated up front and is computed outside of any atomic blocks. I think you could use a similar scheme based on the same principles (initially zero -> async ->  re-weigh & replace). Then you could use a ThreadLocal to observe whether the weigher was called by a user or async thread. I'm thinking something like,\n``` java\nclass AsyncWeigher implements Weigher {\n  static final ThreadLocal isAsyncThread = ...\nCache cache;\n  final Executor executor = ...\n  final Weigher delegate = ...\n@Override public int weigh(K key, V value) {\n    if (isAsyncThread.get()) {\n      return delegate.weigh(key, value);\n    }\n    executor.execute(() -> {\n      isAsyncThread.set(true);\n      cache.asMap().replace(key, value, value);\n      isAsyncThread.set(false);\n    });\n    return 0;\n  }\n}\nAsyncWeigher asyncWeigher = AsyncWeigher.of(expensiveWeigher, ...);\nCache cache = Caffeine.newBuilder()\n        .maximumWeight(MAX_WEIGHT)\n        .weigher(asyncWeigher)\n        .build();\nasyncWeigher.setCache(cache);\n```\n. There is a slight chance of a race where the async thread completes before the user's (e.g. same thread executor). Not sure how to handle that nicely.\nI didn't see a use-case for an AsyncCache, but that could be added if proven useful. An AsyncLoadingCache would probably solve your problem, but as a hack you would have the load return null indicating a miss if anyone does a loading get. You could probably avoid that from ever occurring by using the synchronous() view to access only with the Cache interface.\n. The initial suggestion could leave a zero-weight value in the cache, so unlikely but annoying if it occurred in production. A zero-weight is never evicted, so a potential memory leak worse case.\nThe second suggestion has a mistake where the completed future is weighed immediately (whenComplete instead of whenCompleteAsync). That could be changed, as I hadn't seen a good reason for not running it on synchronously on the completing thread (writer or async completer). For the current version you'd need a wrapper, but I'd be open to making the change if it is helpful to you.\n. I investigated whether heavy writes would cause any user-visible issue. My concerns of potential excessive scheduling was unfounded, as I had forgotten that the tryLock guard provides adequate protection. A prototype of perfect scheduling (larger state machine) didn't appear to make much of a difference other than be slightly slower to operate on.\nThe only issues I'm aware of could only occur in synthetic tests or a system already in a bad state. If writer threads spin performing no work but inserting unique keys (e.g. a counter with the full scheduling quantum), then the maintenance task cannot drain the queues and evict fast enough. If the writer threads do any other work, like Thread.yield(), then no issue occurs. If I switch to a growable array-based queue to avoid the GC of linked nodes this will throttle that case as a side effect. The bad system state is when the executor is clobbered so the guard's effectiveness is reduced and then there would be excessive scheduling. An excess is benign and not too costly so not an issue, as the real problem would be that the maintenance isn't being run promptly in general.\nA slight improvement to your async weigher idea to avoid the race is to synchronize on the value before inserting into the cache and within the executor's lambda. If we assume the executor will never run the task on the same thread (e.g. no CallerRunsPolicy) then this avoids the race condition entirely.\nI also strongly recommend using self populating caching whenever feasible. It often results in cleaner code and avoids racy get-compute-put by being atomic. Then your issue would be naturally solved using an AsyncLoadingCache because the future be inserted at weight zero, load asynchronously and be readable, and the async thread would re-weigh.\nLet me know how your prototyping works out :-)\n. I just realized there is an easy hack to make the AsyncWeigher detect the race. Instead of using a boolean for the ThreadLocal, store Thread.currentThread().getId() and compare.\njava\n@Override public int weigh(K key, V value) {\n  long threadId = Thread.currentThread().getId();\n  if (isAsyncThread.get() == threadId) {\n    return delegate.weigh(key, value);\n  }\n  executor.execute(() -> {\n    long asyncThreadId = Thread.currentThread().getId();\n    isAsyncThread.set(asyncThreadId);\n    if (threadId != asyncThreadId) {\n      synchronized (value) {\n        // If we assume caller of cache.put(k, v) also wraps in a synchronized block, then\n        // the async task is blocked from re-weighing until the insertion has completed.\n        cache.asMap().replace(key, value, value);\n        isAsyncThread.set(null);\n      }\n    }\n  });\n  if (isAsyncThread.get() == null) {\n    return 0;\n  }\n  isAsyncThread.set(null);\n  return delegate.weigh(key, value);\n}\njava\nsynchronized (value) {\n  cache.put(key, value);\n}\nI'm pretty sure with the threadId and synchronization hacks, there won't be any race conditions and you get the desired behavior. It is ugly, but cheap.\n. Yes, Caffeine's native expiration model is not compatible with the JSR's. Unfortunately that means there is no direct mapping, so additional configuration options have to be provided and the CacheProxy provides a JSR compatible version. Unfortunately the JSR's expiration approach is very error prone, a tad confusing, and relatively slow.\nThe JSR's configuration object is pretty limited and I don't think configuration should have been part of the spec. From the other implementations, the assumption appears to be that you either use a configuration file (e.g. application.conf) or the implementor's config object (e.g. CaffeineConfiguration).\nIf you want to use an application.conf defaults for an anonymous cache and are using annotations, you need to override the DefaultCacheResolverFactory binding.\nI'm not quite sure what else you're asking, so please clarify if I missed something.\n. If aCache is defined in application.conf then I'd expect it to resolved and configured. Otherwise the spec expects you to create the cache, so you'd have to use TypesafeConfigurator.defaults(ConfigFactory.load()) if you want to fallback. If you asking that an unknown cache is automatically created using the default values in your application.conf, then I think it will break in the TCK.\nOr is there a problem and application.conf is not being honored and the named cache isn't found?\n. Unfortunately I don't think automatically creating an anonymous cache would pass the TCK and seems to not be the expected behavior from what I can tell. I could try this evening (or you are welcome to give it a quick hack) and see if the TCK tests pass (gradle jcache:testCompatibilityKit). If they do, we'd probably want to ask on the JSR mailing list and review their Google Doc to see if that is a TCK bug or a viable implementation behavior. But, if it was then it wouldn't be generic all other implementors who don't do that so you're still kinda stuck...\n. I'm not a fan of JCache's design and APIs, so my default recommendation is to avoid it unless you do foresee using multiple implementations.\nIf you are on Spring (unlikely given Scala) then their cache is a better abstraction. If you're using JCache's annotations (also unlikely given Scala), then be careful because they prone to cache stampeding by using a racy get-compute-put instead of an atomic load.\nIt sounds like the experts group is expecting to make breaking changes in 2.0, if that ever happens. Its very much vendor-ware to push commercial products, rather than designed to make happy developers.\n. Fyi, I gave it a shot out of curiosity and as expected the TCK failed. Unfortunately your only option is to forego the generic configuration and rely on implementation-specific in general. A distributed cache will have its replication and other settings, so really the generic configuration API is for show and pretty much unusable (e.g. anonymous caches for JCache annotations causes memory leaks).\n``` gradle\norg.jsr107.tck.CacheManagerTest > getCache_Missing FAILED\n    java.lang.AssertionError: expected null, but was:com.github.benmanes.caffeine.jcache.CacheProxy@1296409e\n        at org.junit.Assert.fail(Assert.java:88)\n        at org.junit.Assert.failNotNull(Assert.java:755)\n        at org.junit.Assert.assertNull(Assert.java:737)\n        at org.junit.Assert.assertNull(Assert.java:747)\n        at org.jsr107.tck.CacheManagerTest.getCache_Missing(CacheManagerTest.java:395)\norg.jsr107.tck.CachingTest > cachingProviderGetNonExistentCache FAILED\n    java.lang.ClassCastException: Incompatible cache key types specified, expected class java.lang.Object but class java.lang.Long was specified\n        at com.github.benmanes.caffeine.jcache.CacheManagerImpl.getCache(CacheManagerImpl.java:120)\n        at javax.cache.Caching.getCache(Caching.java:279)\n        at org.jsr107.tck.CachingTest.cachingProviderGetNonExistentCache(CachingTest.java:167)\n``\n. This is progressing on the [refresh](https://github.com/ben-manes/caffeine/tree/refresh) branch. The changes match closely to the above, but further work is needed to complete. The existing tests cases pass. The pending tasks include,\n- Add test cases for whenRemovalCause.REFRESHEDis emitted\n  - Probably emitsREPLACEDon a success, but should now be the new cause\n- Adding tests forretainOnRefreshConflictreturningtrue- Adding JavaDoc for the API additions\n- Ignoring expireAfterAccess while the entry is being refreshed\n. I'm not sold on the method name either and am hoping to hear suggestions. I figured that I should try to get the rest of the code ready in the meantime, though.\n. The concern with that approach is if the resolved value is neither the current value nor the refreshed value (newValue). Then the rejected values would be sent to the removal listener and the cache updated to hold the resolved value. This user behavior doesn't make a lot of sense and defeats the intent of the method (to pick between the current or new value), but we'd have to account for it. I could see a user deciding to resolve by fetching from the database again to return that value, whereas preferably they might compare a modified timestamp on the values instead.\n. An enum indicates that the options might expand, which could only be to reload yet again. It also carries the weight of increasing the API further and dealing withnull` as an error. That begs the question of what to do, regardless of the method signature, if an exception is thrown.\nPerhaps this method doesn't carry its conceptual weight? I'm sure very few users will utilize the conflict handling and some resolution is needed. Guava's clobber/insert is simple and correct in most cases, though arguably could have (hopefully benign) races. Dropping would be safest and less efficient, but requires a new removal cause. If this method shouldn't be included, at least yet, then I'd be inclined to follow Guava's lead here. I don't recall the internal discussion for refresh and why conflicts were resolved that way.\n. It occurred to me that this same race occurs on a loadAll for a synchronous cache. That is because there is an inherent race because synchronized objects cannot be locked / unlocked in aggregate. This is not the case for an asynchronous cache where we insert incomplete futures, bulk load, and then populate those futures.\nLoadingCache#getAll(keys) is documented as allowing implementors to race with the behavior of,\n\nIn the case of overlapping non-blocking loads, the last load to complete will replace the existing entry.\n\nThat is slightly too narrowly worded because the last load will clobber an existing entry for any reason, including an explicit insert. This allows clobbering to insert stale data.\nIt seems better to generalize this enhancement to resolve conflicts for any type of load. That means the method would be called retainOnConflict,  the addition removal cause conflict would be added, and getAll documentation updated, and getAll updated to resolve conflicts. The default behavior would be pessimistic by dropping the newly loaded value.\n. I plan on cutting a minor release (2.3) soon and, given how close this is, it bugs me not to finish it off. I think my only mental blocker is the name.\n- retainOnConflict: a query, but it is not clear which value is being retained\n- resolveOnConflict: a action, that sounds heavy, but is really just a boolean response\n- preserveOnConflict: a query, indicates that the current value would be preserved\nThere's probably a few other good options on a better thesaurus. Ideally the method chosen sounds like a question and it is obvious what the reaction will be to the answer. I think \"preserve\" is the closest word I can find to describe intent and minimize confusion.\n. The good thing about \"resolve\" is that since you can't guess, you have to look it up :-)\nFor \"retain\" I don't know if I'm confusing myself and no one else would be. The caller just loaded a value and sees that a different one exists in the cache. From its perspective \"retain\" might be misread as not discarding it. By definition I think its \"continue to have; keep possession of\" would mean that the current value is kept and the refreshed value is discarded. I don't know if I'm confusing myself from all of the other details, or if others wouldn't immediately know what \"retain\" meant.\n\"preserve\" is a very strong word, \"maintain (something) in its original or existing state.\" That is extremely clear to me that the current value should be kept and a newly loaded value discarded. So I'm leaning towards that name, which I hadn't thought of using until today.\nDo you have a preference?\n. Thanks. I'm going to see if I can finish this task up in the next couple of days. I think that's mostly tests and documentation. Then I shouldn't have a good excuse for punting on #7, other than finding the time to focus on it.\n. I agree that \"retain\" and \"resolve\" feel more natural in the context of a cache. I think preserve is less ambiguous. I'm a little weary of an enum, but could be convinced.\nI'm unsure if I have created a more confusing mess by deviating from Guava's clobbering behavior, or at least not making it default. Take for example the timeline,\n- t0: k => v1\n- t1:  LoadingCache#refresh(k)\n- t2: k => null (expired, evicted, invalidated)\n- t3: k => ?, refresh completes\nIn Guava, Charles had this insert the new value. But we don't know why it changed underneath us - it could be benign like expiration, or we could be loading old data if explicitly invalidated. If we reject conflicts by default then the refresh drops the new value.\nI suspect Charles would argue that the consistency around caches is murky and users should source from the system of record if they need strict accuracy. That then leads to clobbering being a natural, because inconsistency may be unavoidable. If so, then exposing some conflict handling to the user isn't too important. And if exposed but generally unavoidable, then clobbering as the default highlights that.\nWhat would you expect / want?\n. Do you prefer clobbering with documentation (e.g. Guava), or clobbering by default with documentation?\nMy concern with blindly clobbering is that it hides races that could cause inconsistency. Yet you're right that its usually benign and not too worrisome. Is there tangible value to let the user opt-in to deciding the conflict resolution or do you view that as adding unnecessary conceptual weight?\n. > Doesn't a similar problem exist when an explicit write occurs while a computation is pending? What do you do in that case?\nDo you mean a put(k, v) when a LoadingCache#get(k) is in progress? Just like in Guava it blocks the write until complete. An AsyncLoadingCache won't block, but will attack a completion handler to forward to the removal listener. Unlike Guava, an invalidate(key) will block until the computation materializes.\n\nI think a reasonable argument could be made for discarding the refreshed result if that value at the time the refresh began had changed.\n\nA change like a put(k, v) is easy to identify, but what about an invalidate? The preferred idiom in Guava is to not insert directly, but invalidate and wait for the next load. How would a refresh know if when it is safe to insert when oldValue != currentValue?\n. I read through this again and now I see your point. In LocalCache you clobbered the computing entry and, when it completes, it notices this fact and publishes a notification that it was replaced. So every chance one can clobber in Guava it is done, making it very consistent.\nIn ConcurrentHashMap it locks the bin (ideally one per entry). Any subsequent write blocks on the bin waiting for the preceding one to complete. I think that's the only reasonable approach for the map because it doesn't have a removal listener, so clobbering could have surprising side-effects like resource leaks.\nSince we don't clobber on computations, it doesn't give us a enough of a pattern to emulate.\n. Maybe clobbering is the only consistent option? If we think of this in terms of the ABA problem there might not be any other correct way.\nI was assuming that if currentValue == oldValue there is no conflict. Otherwise we had to decide whether to store newValue or discard it. But the value is a user supplied object and the timeline could be,\n- t0. put (k1, v1)\n- t1. store in SoR (k1, v2) \n- t2. refresh(k1)\n- t3. restore in SoR (k1, v1) \n- t4. put(k1, v1)\n- t5. refresh returns v2; finds (v1 == v1); stores (k1, v2)\nThat is convoluted, but technically possible. The tag to compare would be the write time, which makes sense for refreshAfterWrite. But constructing LoadingCache doesn't require that be stored on the entry, and we wouldn't want to start just for an ABA safe refresh.\nSo either we block other writes (current), clobber, or have ABA unsafe conflict handling. We don't want the last one, so the whole method non-sense is out. We want to avoid the blocking writes, potential of wasting an async thread, and precedent is set in getAll.\nI guess I went the long way around to arrive back at what you did in Guava :-)\n. Writing tests shows that I'm bad at keeping all the details of Guava, Caffeine, and ConcurrentHashMap straight.\nGuava's LoadingValueReference is an intermediate value in the hash table entry. If the entry is modified (updated or removed), then the loading value is discarded upon completion. Otherwise the value is stored directly. This avoids the ABA problem, such as a refresh loading in an invalidated entry.\nThere's not much I can do here, unfortunately. The options appear to be,\n1. Leave as is with all the caveats\n2. Best effort detect changes (old==current and write time if available)\n3. Emulate Guava by storing a forwarding node that for unwraps itself, etc.\nI think (3) is possible and could be done without being excessively invasive. It requires care, but I think is doable. However I'd probably also define the behavior as (2) being acceptable. That allows integrating the current patch and, if (3) is done, not needing to do similar for the fully unbounded cache (a simple decorator for CHM). The latter would avoid adding a value wrapper for this case.\nThis seems reasonable. The worst cases of (2) failing are similar issues Guava has, e.g. that an invalidate(key) ignore loading values. That's exactly what would happen here for a prefetch, a SoR write, invalidate, and materialization of the stale value. The only addition is an ABA of exact instances for a LoadingCache#refresh(k) where we do not have the write time to check. For the more common refreshAfterWrite the time check is available, so not an issue. Anyone needing the precision of strict consistency would not obtain it from Guava today nor most infrastructure, and therefore have to add numerous countermeasures for protection.\n. Thanks @anuraaga. I think with the above mentioned comments it would guard against evicted entries being resurrected to some degree.\nThanks for the stack trace and sorry to hear there's an existing problem. It would be nice to figure out how to reproduce that in a unit test to validate that my patch won't suffer that effect.\nI hope to finish my patch tonight or tomorrow, so a release Friday or Saturday is likely.\n. > A part of me doesn't like how it means evicted entries would come back to life thanks to the refresh, but I guess some level of thrashing at the low end of the LRU scale is expected.\nGuava's performs a putIfAbsent if the old LoadingValueReference isn't present. This means than an explicit invalidate or an eviction will resurrect the entry from an in-flight refresh. My current patch does not do this, and therefore the new unit test passes for Caffeine and fails for Guava. I think that the reason for this appears to be the interpretation of the JavaDoc on refresh(key).\n\nLoads a new value for key {@code key}, possibly asynchronously. While the new value is loading\nthe previous value (if any) will continue to be returned by {@code get(key)} unless it is evicted. If the new value is loaded successfully it will replace the previous value in the cache...\n\nThis indicates the previous value is the instance. If updated Guava discards the refresh and explicitly calls out eviction. The statement of replace indicates to me that the putIfAbsent was a mistaken. However, if Guava had done this then it is not clear what the removal cause should have been, since the refresh action wouldn't know why the entry disappeared. In the patch I'm using explicit since the refresh is a user action and a conditional failure would be explicitly discarded rather than an implicit eviction. Yet that is misleading on refreshAfterWrite which is implicit and has the same scenario.\nWhat is the desired behavior here?\n. I think what kept causing me confusion is that if a user's mutation should cancel a refresh, then arguably an invalidate should too. Yet that is not true in Guava and could cause surprising behavior if assuming a happens-before of actions. I believe this would violate the linearizability property if guaranteed.\nMy mistake was to try to provide a predictable sequence that properly handled the various cases. In retrospect it unnecessary and following Guava's lead here is preferred. That is simple and consistent. There are other places where linearizability would be broken as well, in both Guava and Caffeine, and that's acceptable.\nIf a user wants much stricter behavior then asMap().compute(k, func) is available. And stale data is always a factor in concurrent / distributed systems, so refresh being best-effort is good enough when other necessary precautions are taken.\nDespite going around in circles for too long, I think we have a conclusion. At the very least I understand the rational and can have tests for them.\n. Released 2.3.0 which includes non-blocking refresh and fixes the deadlock that @anuraaga discovered.\n. This is surprising, because the maximum capacity of the work queue is 64M. FJP commonPool should create more threads to handle the load such that this wasn't expected. The logging was intended if someone supplied their own bounded executor to indicate a resource constraint.\nThis should be benign, except if you have a RemovalListener. Currently if that task is rejected it logs an error and drops the event. It would probably be better if instead it fell back to executing on the caller's thread so that there was a performance loss, but no correctness problem (e.g. resource leak).\n. Is there any chance you can provide a test program to reproduce this? I'm not sure how you were able to exhaust the work queue, and if there's anything much I can do about it. I'll fix the removal listener issue, though.\n. The executor is used by Caffeine for the for periodic maintenance (after a write or a bunch of reads), removal notifications, refresh / refreshAfterWrite, and AsyncLoadingCache futures. In your case of a manual cache that means only periodic maintenance or removal notifications should be enqueued by the cache.\nPeriodic maintenance is guarded by a tryLock and a simple state machine. That should result in only one maintenance task queue'd up in general, or a little less than one per write, so that there aren't too many filling the work queue. I did experiment with avoiding any duplicates but didn't see a problem in my write benchmarks that would cause excessive scheduling problems.\nThe actual maintenance work is pretty small, except that it calls into a CacheWriter#delete on eviction. If the delete is excessively slow and writes keep coming in, then I could see a potential problem. A slow writer should be replaced with one that is mostly queue'ing up the work for asynchronous processing. If that's the issue then we could update the documentation to reflect best usage patterns.\n. invalidateAll() should be safe. I think invalidateAll(keys) could use some tuning.\nAre these caches thread local? If so a bounded LinkedHashMap might be best regardless.\nI think if we can reproduce the issue then I should be able to make the cache more resilient.\n. I cleaned up the stress test to help investigate this. It prints out useful information like the ForkJoinPool pending queue size.\nFor write heavy workloads the state machine isn't perfect, so additional tasks do get scheduled. However that mostly doesn't occur if there is any intermediate work, like Thread.yield(), which minimizes the race and lets the maintenance work catch up. If we don't yield then the maintenance task is de-prioritized, as the other threads fight for all the CPU time, and the cache can't catch up with the work. This results in the known problem of exceeding its size, but only in that synthetic case, and shows that the executor's work queue is similarly lagging.\nThe perfect state machine doesn't have any race for the executor's work queue. Then even in the synthetic non-yielding case the queue stays empty, though as described before the cache can't evict fast enough to keep up with the writes. Again, that issue isn't a real concern because it requires the machine be dedicated to doing nothing but writing into the cache.\nSo the state machine change might help in your case and be a decent safety net. I'll play with it a little more and probably add it into the next release. It has a tiny performance penalty, but not enough to be noticeable outside of a micro-benchmark.\nHowever, I think your resolution is best served by asking why a concurrent cache is used as a thread-local cache? A cache usually isn't write heavy, nor cleared so frequently. It sounds like you need a bounded scratchpad area, so a LinkedHashMap in LRU mode is probably both faster and a better fit. Can you explain the use-case?\n. Integrated the improved scheduling, but feel free to continue the discussion.\n. Thanks for the update. I hope to finish #56 this weekend and cut a release. I'm been a little slow on that one because it has an API change, and I'm unsure if there is a better method name. If you can take a look and offer a suggestion I'd appreciate it.\n. Release 2.2.3 with this improvement. Thanks again for bug report.\n. In Guice wouldn't you instead inject a Caffeine builder? I'd expect that to have the same effect, since it is reusable.\n. Appreciate the thought and PR, though! :-)\n. hmm, actually I should clarify that the builder is reusable in regards to the build() method. Setting the fields like removalListener is not, because it can only be set once. If using setters never came up for Guava not sure if your approach or adding a toSpec to the builder would be helpful.\n. Oh yeah, that makes sense. Worst case you add binding annotations to qualify them.\n. In Guice, I usually injected the built cache instead of a builder. So the spec is just an intermediate if setting from an external file. At Google that would have been a custom Flag that parsed the spec string. I'd probably do something similar with other configuration formats, like Typesafe's.\nSo I'd wonder if the problem is that you probably don't want to hard code the values in the Java code anyways? Then you'd need a parser for whatever mechanism you externalize the settings with, which is beyond the scope of this project to provide directly. If that's the case, simply appending to a separated string and feeding into the spec is an easy way to build a custom configuration parser.\n. I think this is because the benchmark is performing only inserts with no delay between operations on all of the available cores. In that case the populating threads are given full cpu time quantums, do no additional work but increment a counter, and starve the asynchronous eviction so that it is unable to catch up. The stress test shows similar results, but demonstrates that adding a Thread.yield() resolves the issue.\nThe resolution will be when SingleConsumerQueue is replaced with a bounded array-based queue (preferably growing to a maximum to avoid wasting memory). The primary reason for the switch is because the hash table entry operation is the bottleneck and the queue doesn't show up in profiles. That leads to the conclusion that we probably want to prefer being more GC hygienic by not creating linked nodes rather than maximizing the queue's write rate. As a side effect the number of outstanding writes would be bounded, increasing the penalty in a synthetic test while avoiding the OOME.\nSince this is only observable in artificial tests its been a low priority.\n. All evidence points to this only occurring in synthetic stress tests due to intentionally starving the eviction process. This occurred in CLHM as well, which delegated the penalty to a calling thread, and we couldn't find any hiccups occurring in practice (e.g. Cassandra load tests). I bring this up every so often when write rates are discussed on issue trackers to see if users have evidence that my analysis is incorrect. The scenario is good as a stress test, but makes no practical sense and any cache would be a poor data structure choice for this workload.\nClosing as this has been on my roadmap for a while, and I prefer to track it there unless it becomes a user issue.\nThanks for pinging me about it.\n. Recursive computations are not supported. This is not supported by ConcurrentHashMap which livelocks in Java 8 and should throw an exception in most cases in Java 9.\nThe JavaDoc for the cache loader and get methods include warnings (see below). This is understandably surprising at first, but I think a reasonable limitation. I agree that hanging is a poor experience, which is why I worked with Doug to find ways to fail fast.\nthe computation... must not attempt to update any other mappings of this cache.\n. The only practical step I could take is embed a copy of Doug's fixed ConcurrentHashMap. I'd prefer not to do that. Its unfortunate that this has been fixed for over a year and yet never made it into a JDK8 release. Closing as I don't have any good resolution until JDK9.\n. Added a Testing wiki page. Hope that's helpful. Thanks!\n. True, but why are you reassigning the builder? It becomes a moot point if you don't, e.g.\njava\nCaffeine<Object, Object> builder = Caffeine.newBuilder().recordStats();\nif (maximum >= 0) {\n  builder.maximumWeight(maximum)\n         .weigher((String key, String val) -> key.length() + val.length());\n}\nCache<String, String> cache = builder.build();\n. Oh! Sorry, I didn't realize that was a documentary issue and not a question.\n. JDK9 is adding timeout support (orTimeout and completeOnTimeout to the future chain.\nIn the case you're showing the original request is times out and the cache is populated with the partial results. Other users may want the cache to eventually populate with the entire results, but have the user request have a timeout to obtain the completed subset.\nMy hope is that with the JDK9 helper methods then either use-case could be trivially handled in the client code. There are probably some external utilities providing those features in JDK8, since its annoying gap in CF. Then I'm not sure whether direct support in the API is a significant benefit, and exactly what we'd want that to look like.\n. Re-reading your code and my response it looks like I error'd.\nThe case of partial results might make sense as N calls to the cache (e.g. get(key)) and then letting the consumer take the results that materialized within the timeout. The other case is a single bulk call where the a timeout is again on the consumer side and retrieves only the entries that were in the cache already. Most async clients (like spymemcached) return a Future<Map>, not a Map<K, Future> so partial results can't be obtained.\nI think all of this could then be handled at the consumer side as follows\njava\nAsyncLoadingCache<Integer, Integer> cache = Caffeine.newBuilder().buildAsync(k -> k);\nList<Integer> keys = ImmutableList.of(1, 2, 3);\nreturn CompletableFuture.supplyAsync(() -> {\n  try {\n    return cache.getAll(keys).get(1, TimeUnit.SECONDS);\n  } catch (TimeoutException | InterruptedException | ExecutionException e) {\n    throw new RuntimeException(e);\n  }\n}).exceptionally(e ->  cache.synchronous().getAllPresent(keys));\nIn JDK9 that would be simplified as\njava\nAsyncLoadingCache<Integer, Integer> cache = Caffeine.newBuilder().buildAsync(k -> k);\nList<Integer> keys = ImmutableList.of(1, 2, 3);\nreturn cache.getAll(keys).orTimeout(1, TimeUnit.SECONDS)\n    .exceptionally(e -> cache.synchronous().getAllPresent(keys));\nDoes that work for you?\n. Let me know if you think an AsyncCache (non-loading) would be useful. This can be emulated using buildAsync(key -> null).\nI didn't see the use-case so I've been hesitant to add it prematurely and add more API to learn. Instead I added all the expected methods needed for a non-loading version (e.g. get(key, func) so that extracting the API later would be seamless.\n. My rule of thumb for an API is to find a balance so as to not throw in the kitchen sink and nudges towards the \"best practices\", while not restricting you from getting your task done. I'm sure Bloch has stated that much better, and his tips are pretty great. By being a library it should be easy to adapt for building more targeted solutions, including other caches with APIs for solving a specific use-case.\nThe blacklist is a kitchen sink feature, which you solve as well as I could. A whitelist could be implemented more cheaply as a filter (bloom or cuckoo). A \"bad key\" might also be viewed as \"negative caching\" (caching a miss), which can be implemented using a sentinel value (often an optional). The use of smaller weights on the negative could be used as either unbounded (0 weight is never size evicted) or at a ratio (1 for negative, 10 for positive).\nI wanted to keep the conceptual weight low, so I try to be compositional instead of bubbling everything up. Most usages of put are best replaced with loading through the cache, so valid but a red flag to justify each use. I thought of estimatedSize similarly, as usually you're only interested in them for statistics as the cache should manage the bounding. A nice benefit at Google was the ability to see lots of usages to understand how an API was consumed, and then adapt based on data instead of theory. So without seeing more code I'm still of the belief that most users shouldn't need it.\nThat might be a weak argument given invalidate is more likely to be needed, and its a hop away. Had there not be subtle differences then one would expect the api to be AsyncLoadingCache<K, V> extends LoadingCache<K, CompletableFuture<V>>. But since it doesn't restrict users while keeping the API small and easily understandable, it seemed like a good trade-off.\nIf I did introduce AsyncCache then I might bubble up some of those methods from the view for convenience. The argument would be symmetry, each interface remains small, and ease of use. Since AsyncLoadingCache is a bit of a one-off, adding a bunch more methods might make it feel too complex and wouldn't add to the power-to-weight ratio.\nSorry for the verbosity. I am very interested in discussions to improve the API while doing so carefully.\n. This is delegated to the executor to reduce response latencies by not penalizing the caller. As described further in Testing, you should use a same thread executor for unit tests (or simulations). It will then provide predictable synchronous behavior.\nNote that Caffeine lazily begins tracking accesses when the cache first reaches 50%+ full. This was based on observing that many usages set a bounding as a safety net, but otherwise never reach it due to expiration and the low logical count. This can skew small traces but has negligible impact for large ones, the latter being more like a long-running server.\nIn latest SNAPSHOT, but not released, has the initialCapacity setting pre-allocates the sketch. This is in order to avoid the lazy resizing for weighted caches (e.g. byte sized) where the entry count cannot be easily inferred. That has the side effect of removing this skew from small traces.\n. Hi @gruberjuliannama,\nI assume that you're replies were premature while you draft a comment. The simulator in this project includes support for both short and long traces, from a variety of academic sources.\n. Those are synthetic distributions. The trace files are available in the links on the before mentioned wiki page.\n. Okay, closing as this is going nowhere.\n. Please review my original reply. You want to specify a same thread executor. The rest was regarding the hit rate of small traces.\n. You'd want to use initialCapacity(maximumSize), as otherwise you are still forcing the sketch to resize to ensure its accuracy. When resized the sketch is empty (the frequency information is lost). As noted, this only applies to SNAPSHOT where the intent was for a different use-case that also serves to help in this scenario.\nSee CaffeinePolicy for a complete example.\n. Released 2.2.7 which includes the initialCapacity change, which will help your small trace simulations.\n. It would be misleading because ChronicleMap does not evict (unbounded) and has the additional overhead of persisting off-heap. The unbounded ConcurrentHashMap listings are to show a baseline for comparing the overhead and scaling effects relative to the optimal. It uses a Zipf distribution (per Pareto principle) and 100% hits to demonstrate contention. However real-world may be different, e.g. higher penalties in some policies (e.g. Clock) that use O(n) eviction. To keep the benchmark intellectually honest its important to limit the variables, test what can be done accurately and fairly, have an even playing field, and document my design trade-offs for transparency.\nA benchmark of serializing off-heap data structures would minimize the serialization penalty to show the overhead of the implementation. That would be a fair comparison, but obviously the serialization is the dominant cost and could make the differences negligible in practice. A benchmark would need to show both small and large serializations in order to determine the effects, e.g. locking and fragmentation. So I can't imagine doing this fairly if my focus is on-heap.\nI think serialization is a brute-force strategy and is not the right approach for off-heap. I'm a fan of instead using native bindings to the unmanaged data, which provides similar performance as on-heap data. For now that would require JNI overhead, but JEP 191 (Foreign Function Interface) would inline the calls to have minimal overhead. That compiled library vs serialization seems like a good trade-off given the few platforms and use-case of offheap. Apache Mnemonic is spearheading this strategy and Accumulo might want to collaborate with them to help mature it. I've talked to their lead about some improvement ideas, but haven't had the bandwidth to dig in myself.\n. It is a hop away through AsyncLoadingCache#synchronous() which provides a LoadingCache view. This allowed keeping the interface small and focused with minimal pain for the less frequently used calls, which usually means its easier to grok. So you can write,\njava\nCacheStats stats = cache.synchronous().stats();\nI think that is a good compromise for now. If there are requests for an AsyncCache (non-loading) then for symmetry the view's methods would be promoted as default methods doing the above. I made it easy to extract that interface, but didn't see a good need for it. Given that I might be wrong, its easy to add based on feedback.\nDo you have any recommendations for where to improve the documentation to make this more apparent?\n. Closing. If you have recommendations to improve the documentation and/or API, please let me know.\n. You're still trying to starve the out any other processes by inserting at a ridiculous rate. Its queuing theory and anyone doing this is using the wrong data structure, as it's not the purpose of a cache. You're not trying to get meaningful results or emulate anything real world. Adding a yield solves it.\n. Because the inserting thread is given full cpu quantums and run without any delays. The scheduler has no reason to deprioritze it. Eventually I'll add back pressure but it's been low priority given the artificial nature of the stress test. A yield to indicate real world work resolves the problem, indicating that it's not realistic.\n. Sorry. I felt you threw it in my face last time instead of trying to be fair, explain, and analyze trade offs. But that's not a good reason to get annoyed, so I apologize.\nI'll take a look. I'm fairly certain it's as explained above.\n. You might try to capture the eviction weighted size using a removal listener. Unfortunately the weight isn't communicated so it would have to be re-weighed, but should good be a good temporary measure.\nWhen I run through a stress test the GC stays fine and there isn't a stress on the total capacity. I do think backpressure here would be good, but it hasn't been necessary yet. A short-term solution is to have a rate limiter in front of writes to avoid a pile-up effect.\nYou might be promoting entries into old gen quicker than expected, since the cache does tend to hold long-term entries. You might also try using G1 (-XX:+UseG1GC) which does a much better job by collecting regions in parallel.\n. Can't reproduce with my stress test with the ascending list. Note that I'm on a 4-core (8HT) laptop, and I think you said before you test on a 2-core laptop.\nA bit of hacking during my commute using a chunked array queue keeps the bounding at a maximum of 1024 (16 initial) and processes 100M evictions / minute (1.67M/s). That's without any other system work except insertions with 16 threads. There's probably room to optimize that, such as a smarter back-off strategy, e.g. busy loop for half a context, try to schedule a drain, and yield.\nI probably won't get time to work on a production quality version over the weekend. So it might be a little longer before I have the bandwidth to have back pressure baked into a release.\n. Oops, that should be 150M/m (2.55M/s). I forgot that it was unnecessarily using the removal listener for counting evictions, rather than enabling stats. That consumes some of the ForkJoinPool and caused context switches. Still not amazing, but should be acceptable.\n. Can you run with the snapshot? I'm about to check-in a fix using a bounded queue. I still need to play with the backoff behavior. Its a little naive, but seems okay for an initial commit.\n. What OS are you running? The Windows scheduler is known to be very quirky due to legacy reasons, so it doesn't give a fair balance across threads.\n. The write buffer is a hand-off, meaning that there is a small amount of extra slack given and prioritized for immediate handling. A heavily overloaded system is already a problem in a clustered environment, as it won't be able to service requests with an acceptable latency. If the goal is keep latencies low and the variance narrow, then removing single points of contention is important. Hence most systems, e.g. async I/O, are used. That leads to queues, which are bounded to have back pressure if the system is falling over.\nFor my system the slack (write buffer's  maximum size) is 1,024. That showed a reasonable balance of handling a high write rates by applying a decent sized batch, without being excessive.\nThe alternatives are of course a coarse lock (wide variance on lock times) or handoff-with-notification (e.g. flat combining). Those synchronous modes trade off more unpredictable latencies to provide a more stricter constraint on a resource. That's not usually desirable in a server workload, hence the focus on latencies as a critical metric.\n. Interesting.\nYou're running with Parallel GC? That's the default and is a throughput collector, so it prefers to STW. That can have pretty long pauses, but for batch jobs its a net win. I usually run using -server -Xmx2G -XX:+UseG1GC -XX:-UseBiasedLocking to try and reduce pause times, without over tuning the settings.\n. Yep. I'm never claiming one design is better, as all are trade-offs. Finding a balance is of course hard, and resolving to better handle a variety of workloads takes time to mature. Other approaches may very well work wonderfully.\nMaximizing cpu is definitely the goal of batch processing, but I/O usually starves it. Most caches are read centric and writes are very expensive, so some form of queuing exists there too (e.g. write-ahead logs). But that could have been the rub that I didn't account for as well, since druid batch centric. The bounded queue should help alleviate that now, without diminishing the primary goal of handling user-facing latencies.\n. Closing as I think we have it fixed from both ends. I'll probably have a release by the end of the week, but want to let the changes bake a little.\n. Released 2.3.0.\n@drcrallen Please let me know if this release alleviates the problem that you experienced.\n. Thanks for the feedback. That makes a lot of sense and mimics my previous analysis and experience. G1GC tends to do a very good job at reducing latencies and collecting garbage promptly due to its multi-region layout. The change in 2.3 were positive, even if most likely synthetic, so there was still an overall win.\n. The problem appears to be due to JDK-8078490: Missed submissions in ForkJoinPool and unrelated to the queue implementation. For JDK releases 8u40-8u60 the executor may never execute a task. This results in the maintenance work being scheduled but never run, so either a memory leak (SCQ) or halting (WB) depending on the release.\nThe fix is to either use a different executor (e.g. Runnable::run) or upgrade to 8u65 or above.\n. Can you explain why this metric would be useful to you? Is it to gauge the average weighted size of an entry?\n. I think that's a very reasonable request.\nIt will look a little hacky interface-wise since by semver I can't change the API in a backwards incompatible manner. I'd like to wait until JDK9 var-handles before pushing the major version. So I'd have some hack on StatsCounter to include a no-op default method and add a new constructor to CacheStats. Then in v3 I'd make them required as an API break that would effect almost no one.\nIf you have other metrics you'd like to see added let me know. I think putCount was a common request.\nIf you're able to gather more insight into #67 as well, I'd appreciate it. I can't reproduce an issue in a realistic scenario, only when the system is dedicated towards inserts with an intent of destroying the cache. I'm not convinced its the queue, which has mostly been used as fodder, but its a known backlog item.\nI probably won't have the bandwidth to work on a new queue for a little while. I'd want it to be array-based with a small initial footprint (e.g. 16e) and a cap (e.g. 1024), and probably spin wait trying to reschedule. That would probably be done using some link chunking to let the consumer follow on a resize. I'm a little fearful of a spin wait by assuming that the consumer is prioritized quickly. In JDK9 the onSpinWait() hint to generate an x86 pause instruction would help alleviate some of my fears.\n. yep, monitoring is critical.\nevictionCount is defined as not including manual removal. A weighted version would naturally parallel that. Would you expect to see a removeCount and weighted version added?\njava\n/**\n * Returns the number of times an entry has been evicted. This count does not include manual\n * {@linkplain Cache#invalidate invalidations}.\n *\n * @return the number of times an entry has been evicted\n */\n. okay, great. I just wanted to make sure about manual removals in case that would cause a surprise. I can add or keep ignoring that, based on preferences.\nSince Long.MAX_VALUE / Integer.MAX_VALUE = 4,294,967,298 I suspect that it is not much of an issue.\n. While your asking, any other stats you think would be useful?\nI've seen requests on Guava for putCount. The intent is to nudge users to load through the cache, but arguably good monitoring trumps API hints.\nIf added, then I'd expect to probably add removeCount and removeWeight as the parallel for the automatic load / eviction stats.\n. Thank you for the details and sorry again that this is happening. I obviously missed a scenario and, by not forcing the callbacks to be executed asynchronously (e.g. whenCompleteAsync) that oversight caused the join to be unsafe. Unfortunately prior to #56 it seemed necessary to abide by the contract.\nHopefully we can put together a unit test to demonstrate this bug. Then regardless of the logic behind #56 we'll have confidence that this won't occur again. Awaitility makes this easy, so I'll try to reproduce the problem.\nMy goal was to have a release Friday or Saturday. With this out in the wild I'd very much like to know a fix is live by Sunday night. This weekend was going to be hectic, but hopefully I can find the time without rushing it.\n. @anuraaga do you think you could reproduce this in a unit test? I'm having a little trouble figuring out the sequence. When we reproduce this, then #56 should be ready to merge.\n. I wasn't able to decipher the exact steps into a unit test, but load tested both versions. I see master showing a deadlock and the non-blocking refresh passing. I also made all of the post processing use an the Async version.\nFor the stress tests I use a same-thread executor so that I can deterministically validate the internal state. Due to not being async, the heavy writes can cause the bounded buffer to get full. This appears to cause some havok that I haven't untangled yet, probably by calling back within the drain and deadlocking. If the buffer is excessively large or a threaded executor is used then there isn't a problem. For now I'm using the default executor and having the validator call awaitQuiescence which seems to work. I'd like to understand why the same-threaded version gets into a state where writes can't proceed, though.\n. Thanks @anuraaga! I'll have a release by Sunday evening for you. All that is left is investigating a weird edge case that only happens under load for a direct executor. That of course makes no sense for an async cache except in tests. Still, I'd like to understand it better before claiming it is benign or changing the queue bounding. I think 2.3 will be better than the latest, and if you discover more next week we can patch.\n. Got it!\nThe problem only occurred in weighted caches where 99% of it is has zero weight entries. These are entries not eligible for eviction, e.g. an incomplete future. For now they are retained on the LRU queues, but will later be moved to a dedicated queue to avoid skipping over them. The cache is partitioned into three parts (window, probation, protected) and uses TinyLFU for promotion from the window to probation. The load test used a random weigher so once the future completed it was likely to cause an eviction.\nIn this case the probation & protected spaces were filled with only zero-weight entries and out-of-order writes caused the cache to know it was exceeding its size. When searching it would exhaust the probation and scan the protected space. That would yield no fruit, but accidentally it would repeat the search. Instead it must search the window and, if empty, give up. Giving up is okay because we know there must be a pending write to be replayed, which then corrects the internal counts and makes an entry eligible for eviction.\nI'm not sure why using an unbounded write buffer doesn't fail, but perhaps the race is just harder to demonstrate or some completion action afterwards has an effect that avoided the problem. For the bounded writer buffer, when full it causes back pressure so that other writers don't complete and the infinite loop of course halts progress.\nThis wasn't an async bug, but exposed a problem of behavior it exploits. I think with this resolved we're good to go on a release.\n. Thanks! I hadn't meant to depend on the callback order.\nI added your test. It is now parameterized to run all of the different builder configurations (512) and the internal state is automatically validated.\n. Perhaps this is better handled in your code? There is a complexity budget and, when its only a little painful for a special case, I think the trade-off favors not adding.\nYou could avoid a write into the cache for an update by using a holder value. For example,\njava\nHolder<V> holder = cache.asMap().putIfAbsent(key, new Holder<>(newValue));\nif (holder != null) {\n  holder.setValue(newValue);\n}\nThis type of usage would make expireAfterWrite behave the way you want. There may be a few small quirks depending on the enabled features.\n. This feature is implemented, but additional tests are needed before release. You would be able to perform this expiration by using,\njava\nCaffeine.newBuilder().expiry(new Expiry<K, V> {\n  public long expireAfterCreate(K key, V value, long currentTime) {\n    // return expiration duration, e.g. 5 minutes\n  }\n  public long expireAfterUpdate(K key, V value, long currentTime, long currentDuration) {\n    return currentDuration\n  }\n  public long expireAfterRead(K key, V value, long currentTime, long currentDuration) {\n    return currentDuration;\n  }\n}).build(). Released 2.5.0. Thanks. I had some tests but didn't update everywhere, so perhaps the wiring is wrong. I was a bit swamped given Pesach started on Friday, so I might have rushed that to focus on the other issues. I'll take a look during my commute home and get a fix ready.\n. The evict_weighted test looks right and attaching a debugger I see its correct.\nIt looks like you have two cache instances, method and class scoped. I suspect that is the problem.\n. Looks like your tests pass now, glad I could help. :-)\n. Yes. The benchmark page links to the code so that everything is transparent.\nCaffeine performs a get then computeIfAbsent by optimistically assuming that the entry is present. Otherwise the performance will be limited by computeIfAbsent locking pessimistically (just like putIfAbsent). This is a simple trick, but people are often surprised its not built-in.\n. Thanks. Its nice to hear feedback :-)\nYep, definitely only a single load is performed or else you could have resource leaks. For a cache we have to validate that the entry has not expired or that its key/value were not GC'd, which adds some complexity during the computation. The older trick of using a Future and putIfAbsent to memoize tended to use a get prescreen too in practice, due to the locking. One would expect that computeIfAbsent would prescreen, but at 32+ cores the pessimistic approach is surprisingly better.\n. Nope, no difference. That's all that the implementation does. It stashes a predefined function from the cache loader and delegates.\n. I believe later versions of JDK8 started doing a lot better at caching lambdas. But I'm not sure if that is only for non-capturing lambdas, which are the rarer case. The LoadingCache is also a nudge that users really should delegate the computation to the cache instead of a racy get-compute-put which is way too common.\n. Cool, thanks for reaching out. Closing until next time :-)\n. This is way too messy to tackle and support long-term. At best it would reside as a fork that receives much less attention, so more of a one-off.\nI think we're all eagerly awaiting ValueTypes and wish they'd land sooner. Doing this now would require forking ConcurrentHashMap, updating the interfaces, reworking anywhere using null, and code generating the primitive types. To avoid boxing via OptionalX the interfaces would probably be more like Guava (exception on cache miss). I think the ripple effect is large.\nWhen value types arrive it will be interesting to see how the cache is effected. The changes to Map would be the model to emulate. If the changes could be shoe-horned in but APIs are much better with clean slate, I might do both (by transitioning to a new package namespace).\nI suspect many low-latency systems use thread-per-core as much as possible, where a non-concurrent cache is preferred. If someone wants to give it a try then I'd help, but otherwise waiting for Java 10 seems to be more practical.\n. I think a non-concurrent TinyLFU cache would be nice. Last I looked Agrona provided an n-way associative cache and an n^2 lru cache. A TinyLFU cache would be simple, very compact, and have much higher hit rates.\n. Yes, a fresh StatsCounter instance should be assigned to every cache. It holds the counters.\nTechnically someone could reuse the builder. If they did then the same StatsCounter would be used by multiple instances. The intent was to nudge anyone who did reuse it to realize that potential mistake. I don't think anyone really does reuse it, but also very few supply their own StatsCounter. The ability to configure your own was to make it a little friendlier for integrating with a metrics library (e.g. Dropwizard's).\n. Why not push the updates through the counter to Dropwizard? Something like,\n``` java\npublic final class MetricsStatsCounter implements StatsCounter {\n  private final Meter hitsMeter;\n  private final Meter missesMeter;\npublic MetricsStatsCounter(String metricsPrefix, MetricRegistry registry) {\n    hitsMeter = registry.meter(metricsPrefix + \".\" + \"hits\");\n    missesMeter = registry.meter(metricsPrefix + \".\" + \"misses\");\n  }\n@Override\n  public void recordHits(int count) {\n    hitsMeter.mark(count);\n  }\n@Override\n  public void recordMisses(int count) {\n    missesMeter.mark(count);\n  }\n// ...\n@Override\n  public CacheStats snapshot() {\n    return CacheStats.empty();\n  }\n}\n``\n. This turned out to be easily overlooked, so I added an [example](https://github.com/ben-manes/caffeine/tree/master/examples/stats-metrics). The visualizer, like Graphite, should be able to perform calculations like hit rates and averages, so I think that I used the correct primitives.\n. If (2) is acceptable, then you can use theCache.asMap()` view.\nThe cache uses only O(1) algorithms to provide consistent performance regardless of size and activity. As fixed expiration is often the most useful, it benefits by also being easily modeled by shuffling entries in a linked list. The proposal in (1) could suffer O(n) scans.\nA common approach to expiration (e.g. memcached) is to perform it lazily and rely on a capacity constraint. If the fetched entry is expired then emulate a miss (e.g. get/load, remove(k,v) if expired, get/load). This pollutes the cache with expired entries, but the size eviction ensures that they are removed eventually. If applicable you could implement that strategy by evaluating your custom criteria.\nOtherwise you're left with potentially O(lg n) and similar approaches. That means there isn't anything more intelligent that we can do, or that feels less hacky, then what you would have to do in your code. The API tries to be malleable enough to help you get your job done, but sometimes its better to use a different tool if its not the right fit.\nUsually domain knowledge in the eviction policy either has a huge gain or none whatsoever, with the latter being more likely. So if you go down that path you might want to measure to see if your optimization is helpful in practice.\n. If the window in (2) is not acceptable a very hacky but similar idea is to use a CacheWriter. That is called within a compute block so it cannot call back into the cache. However, it could populate a victim cache with the recoverable entries and schedule an async task that re-populates the cache. If the cache loader also checks the victim cache, then the window won't be visible except if you use getIfPresent and conditional (e.g. asMap.replace) lookups. I'm not claiming that's good and may be sketchy.\nBasically you want to disable expiration for certain entries. That's kind of like how an entry can have a weight of 0 indicating it won't be evicted by a size constraint. In that case the policy skips over those entries, but a pending task is to move them into a dedicated queue to avoid the scan. It hasn't been a problem since the feature is rarely used, except for AsyncLoadingCache for incomplete futures.\nThe request seems to be that a predicate evaluates an entry to determine if it is eligible for expiration and place it on the proper internal queue. That would be doable, but leads to a clumsy API if an ad hoc predicate. If like Weigher it calculates an expiration time (per-entry expiration) the API might be okay but it is no longer O(1). I've been resistant to providing that due to the time complexity and poor API evolution (e.g. http caching might want put(k, v, duration) not a callback evaluation).\nJCache requires per-entry expiration and, like the vendors, ours is currently performed lazily by relying on a maximum size constraint. I have been planning on playing with hashed timer wheel as an alternative, since JCache doesn't enforce that a size constraint must be applied. At best that might be promoted into the core library with a warning regarding the impact on the time complexity of maintenance operations.\n. Sorry if I'm overly detailed in my responses. Its a habit as I think through everything, perhaps talking to myself half the time :-)\nFrom an API standpoint the predicate to opt out of expiration is too ad hoc. Too many of those in a library (or language) results in something clumsy and hard to maintain. So short term its not the right solution due to Bloch's conceptual weight rule and your current workaround is an acceptable loss.\nBut iterating towards an efficient variable expiration scheme will probably be a good resolution. The cost then would be losing O(1) behavior for expirable entries, but timer wheels have been shown to scale well in real systems. But they can have poor cascading effects without small adaptions like adding random jitter. I need to follow up on the latest techniques, such as being tickless, so that we have a good implementation to support the resulting API.\n. You might be interested in JCTools #109. Since @mjpt777 subsequently tweeted a link I posted and that it is deprecated, there's a good chance there's doesn't include many additional optimizations.\nBut to be honest I haven't spent any time on evaluating this approach. I think it would work out well, but I'm hesitant to add it before doing a deep dive into the techniques to improve the worst-case scenarios. Maybe the holidays will be a good time to shift focus, as it would be a fun experiment.\n. I've started to think about this again. As discussed, the implementation would be based on a hierarchical timer wheel (see slide 10). The next challenge is to design an API that feels natural but also performs well. I'd appreciate help on this part as I am unhappy with the stub described below.\nExpiration response:\n - Never expires\n - Don't change current expiration\n - Expire in ... from now\n - Already expired\nConstraints:\n - Avoid allocations of response value\n - Callback API for loading caches\n```java\npublic interface Expirer {\n  /*\n   * @param now current time, in nanos\n   * @return\n   *   r < 0 : don't expire (eternal)\n   *   r == 0: no change\n   *   r > 0 : the amount of time before expiring in nanos\n   /\n  long create(K key, V value, long now);\n  long update(K key, V value, long now);\n  long read(K key, V value, long now);\ndefault long expiresIn(long expirationTime, @Nonnull TimeUnit unit, long now) {\n    return Math.max(1L, unit.toNanos(expirationTime) - now);\n  }\n  default long noChange() {\n    return 0L;\n  }\n  default long neverExpires() {\n    return -1L;\n  }\n}\n```\nI don't think already expired is a good response to support. To avoid allocation and the lack of value types, we either have to use a primitive or hope escape analysis would kick in. If we use a primitive then we need special codes, which is confusing. To try to simplify that, we could use helper methods that users could call to translate their response correctly. But if they don't realize it could be error prone if doing a simple math operation (e.g. value.creationTime() + 5m - now).\nI'm also unhappy with the interface name: Expirer, Expiry, etc are all a bit ugly.\nWould might also want to consider a Comparator or Collector style functional builders after the interface is settled upon.\n@blschatz @oakad @jgoldhammer @yrfselrahc @maaartinus @lowasser. You're right. I think I confused myself and wrote this up too quickly during my morning commute.\nAlready expired is a quirky concept. It would mean that a newly loaded value could have been expired, we should not return it to the user, and notify the removal listener. It might make sense if one we wanted to let the user hold the timestamp instead, but then we'd have to query it when shuffling the entry in the timer wheel or allocate a wrapper to publish into the read buffer. Any foreign interface can throw exceptions and we lose ability to dictate the memory visibility constraints.\nPerhaps to handle \"no change\", we pass in the current expiration time as well. If no change is desired then the user should return that duration. So we could reduce this down to a single answer of \"expire in ...\" which has to be positive.. java\npublic interface Expiry<K, V> {\n  /** Returns the duration in nanoseconds until the entry has expired. */\n  long expireAfterCreate(K key, V value, long currentTimeNanos) {\n    return TimeUnit.MINUTES.toNanos(5);\n  }\n  long expireAfterUpdate(K key, V value, long currentTimeNanos, long expirationTimeNanos) {\n    return expirationTimeNanos - currentTimeNanos;\n  }\n  long expireAfterRead(K key, V value, long currentTimeNanos, long expirationTimeNanos) {\n    return expirationTimeNanos - currentTimeNanos;\n  }\n}\nThe above example would satisfy the requests for expireAfterCreate. The stub interface handles never expiring (via large value), no changes (by calculating the current delta), and expiring at a future time. I'm still not comfortable supporting a already expired (zero / negative value), but if we did then loosening the constraints by not throwing an exception would be backwards compatible.\nI don't know of a use-case for update to be provided with the old and new values. It would also restrict when the evaluation is made.\nI still dislike all of the interface names that I can think of.\nThoughts?. Thanks Jens. That practical, well thought out, and pragmatic feedback that I appreciate. In regards to your points,\ninterface\nFor most usages, the expireAfterWrite or expireAfterAccess would still be preferred. That is both more efficient (linked lists) and simpler to configure.\nThe 3 method interface could be reduced via a functional builder, e.g. Comparator or Collector.  So the noise is easier to reduce since we can assume Java 8, whereas you remain Java 6 compatible.\nduration\nWhen I originally tried to model it as point in time it resulted in an ugly interface similar to others. The duration model appears to result in a cleaner API and adds only a two cycle penalty (one subtraction in user code, one in cache). I think we can iterate and, I think, the duration style is likely to come out on top.\nresolution\nNano matches Ticker, so it makes sense to be consistent. There are interesting quirks about the cost and accuracy of System.nanoTime vs System.currentTimeInMillis(). Many of them disappear on Linux, thankfully.\nrandomization\nYou're right, I would leave approximations to a decorator or custom variable policy. It is nice to provide utility methods or documentation to assist, though. The stampeding effect is something users have to consider for all resources, but this would allow for that compared to the existing API.\ndynamic expiry\nI'd probably lean towards asking users to store the timestamps in their value, as you suggest. I also think developers over estimate the value of domain-specific policies. Most often more general strategies are better and easier to reason about, e.g. using pub-sub instead of trying to dynamically tune the refresh time.\n_ efficiency_\nSee slide 10 for a very short overview of timer wheels. They can either rely on a ticking thread or be tickless, the latter being what Caffeine would use. There are many good papers, articles, and code examples that discuss in depth. While that is amortized O(1), for fixed expiration we use LRU/FIFO time bounded queues which are strict O(1).. @blschatz \nThis would not cover the scenario of resurrection. In your approach of an expired entry querying a predicate, this could have a storm of calls on near expiration. That's because reads would have to call before faking a miss. If the predicate was implemented as a synchronized or I/O operation (e.g. check database) then this would be harmful. I don't think that approach would be supported. Instead your 2nd approach would be doable with a CacheWriter without a gap in visibility if all queries are to a CacheLoader, allowing you to do intermediate shuffling by stashing it into a shared map.\nThe proposal is for expiration to be configured per-entry. Whenever a read or write operation is performed the user can determine when the entry will expire. They can use fields of the value to determine what makes sense, e.g. if the duration should be based on the entry's database timestamp. Then one entry might have a ten minute lifetime and another an hour lifetime, without requiring O(lg n) operations to sort them. This proposal would still require periodic operations on the cache entry to revisit its expiration time.\nSorry if that proposal doesn't fit your use-case (there were a few open issues, and yours was the most technical discussion). It would give you a little more flexibility in the re-evaluation during the writer/loader shuffling if you wanted to promote an entry to eternal or use a non-fixed expiration interval.. This feature is implemented, but additional tests are needed before release.. Released 2.5.0. A strong reference would be expected in the hash table, a queue (SLRU, expiration), or a buffer (read/write).\nFor the queues the culprit we'd expect is a dangling reference. I'm inclined to trust the tests which so far disprove that, due to the queues being operated on in single-threaded behavior. That lets the tests be exhaustive as we don't have to worry about races. If the cache is evicting from the SLRU queues then I'd rule that out.\nThe read buffer is bounded and lossy, so activity should cause it to flush eventually. Even if it holds a stale reference, there's only so many it can retain (16 * (4 * NCPUs)).\nThat leaves the write buffer, which until 2.3.0 was SingleConsumerQueue. That should be drained immediately and be very small, so 2.8M retained nodes indicates a problem. That could be either due to GC nepotism, a write rate that starves the async drain, or that the async task failed to run. In 2.2.6 a fix was made to avoid nepotism and in 2.3.0 a bounded (array-based) buffer applies back pressure to avoid starvation. After an upgrade we'd expect neither of those to be possible.\nIf this occurs on the latest release, that only  for the async drain to be stuck and not applied. That would halt any eviction from occurring since the maintenance task handles that. We'd expect that a race, unhandled failure, or executor malfunction to cause the scheduling state machine to be invalid. Since you still see evictions occurring that doesn't seem to be the cause either.\nI'm optimistic that 2.3.0 will fix this. It shouldn't suffer the nepotism or starvation that could explain the excessively large queue.\n. For sanity purposes I added Guava tests for the queues. All pass, with minor changes to support iterator removal for test compatibility.\n. For anyone else reading, please see #77 where we found it is JDK-8078490 which affects 8u40-8u60 (fixed in 8u65). Leaving open until @drcrallen has results to confirm our analysis.\n. oh you're right. Thanks for the correction.\n. oh no worries. I misread the bug which had an integration into 8u65. Good to know it was resolved sooner.\n. Okay, this is very frustrating then.\nYour usage is extremely write heavy (given 500k evictions per hour). The write rate exceeds the buffer's capacity and so it must wait for the maintenance task to catch up. That buffer is 128 * NCPUs, e.g. on an 8 core machine its 1024. The maintenance task should be scheduled after a write, so that the write buffer stays near empty. When full the writers spin by failing to append, trying to schedule, and eventually yielding before retrying. The yield is to avoid starvation, assuming that no CPU time is being given to the maintenance thread which performs the work. In 2.2.6 this write buffer was unbounded so the concern was starvation causing it to never catch up. The current behavior is similar to a file system's async io.\nWhat I'm confused by is whether this means there is a scheduling problem. In the case of 500k evictions per hour, that is 138 per second. That's a very high rate, but there's no reason why a thread shouldn't be able to dequeue that if running. On a stress test I see 2.55M/s sustained.\n. Okay, then this must be that scheduling is halted. That's the only explanation that would cause no progress to be made whatsoever. Otherwise we'd see a steady stream of evictions while the writers spin.\nCan you run a background thread that calls Cache#cleanUp periodically? That will force a drain and ignores the state machine. That must be left in a state claiming that scheduling is in progress but somehow isn't.\n. This might also be due to ForkJoinPool.\nWhen I configure Caffeine's executor to Executors.newWorkStealingPool(1) then scheduling halts. Yet if I use 2 or commonPool then nothing is wrong. It seems like a parallelism of 1 causes the pool to be in a bad state. Perhaps somehow that's happening?\n. I think FJP might be lossy under high contention. I switch from the state machine to a Semaphore(1) so that the lock is transferred to the maintenance thread. I then printed out whenever the permit is acquired and released. This results in,\n...\nScheduled (scheduleDrainBuffers)\nReleased (PerformCleanupTask)\nScheduled (scheduleDrainBuffers)\n-- halted\nThe task is scheduled into the executor but is never run. No exceptions are thrown (and logged). Since the state machine is disabled, the only explanation is that the executor discarded the task and we're left in a bad state.\nWe have to talk to Doug about this. You could switch to a ThreadPoolExecutor (e.g. Executors.newSingleThreadExecutor) which doesn't show this problem.\n. You can run this yourself if you'd like to see the weird behavior.\nfjp_stress branch\n./gradlew stress\nI can't reproduce yet on an isolated test, so I can't rule out Caffeine's code. Yet if I change executors no problem exists and nothing else makes sense so far.\n. @drcrallen What JDK version are you running?\nIf I run using oracle64-1.8.0.51 the failure above occurs. The latest is oracle64-1.8.0.92 which does not exhibit this problem.\n. Thanks. I emailed Doug to ask if he's aware of any fixes in FJP that might explain this. The Oracle release notes were not useful and I need to dig more into his CVS repository and JDK bug tracker in hopes to confirming this.\nCan you upgrade JDKs? The other option is to use a different executor, like Runnable::run to process it on a calling thread.\n. Likely candidate: Missed submissions in ForkJoinPool\n. Thanks for not getting too upset at me and reporting the issues :-)\nI'm in a quagmire myself of how to best handle this. I'd very much prefer not losing the commonPool() optimization, but this is also a very nasty failure.\n. Doug confirmed,\n```\nYes (sorry). Thanks for figuring this out without me having to\nrecheck with your tests!\n-Doug\n```\n. I meant that its effectively free due to, \n- delegate the all maintenance onto a background thread instead of the caller (Guava, CLHM)\n- does not require a dedicated thread\n- shared pool so the threads are free\n- FJP uses a ring buffer work queue (less object allocation)\nBut performance has to be good even if penalizing the caller (higher response times). CLHM did this very  well, but integration into Guava led to performance losses so it is less efficient.\nThere's nothing to be concerned about if you use a dedicated thread or direct executor. Its unfortunate that FJP had a bug that makes it error prone.\n. Great! Thanks for the load testing and sticking though it :-)\n. What about using stream::iterator? That's all I'd do if added as a default method to the interfaces.\n. I'm hesitant at providing an invalidateAll(predicate) because then might imply a more optimal approach than a linear scan. What about using removeIf instead? The Map view is a great escape hatch.\njava\ncache.asMap().keySet().removeIf(key -> key.namespace.equals(namespace));\n. Nope. Its backed by a ConcurrentHashMap.\n. Closing as I thinkremoveIf` was a good compromise. Thanks for the feedback!\n. One problem is that you are mutating the value outside of the cache. That's usually problematic in general because of the shared, mutable state. A put of the same value (x == x) wouldn't trigger the writer because the state didn't really change from the cache's perspective. The CacheWriter is a hook into the atomic operations that otherwise might be done silently (e.g. put or replace). For other operations like compute you have a direct hook into the action.\nTrying to optimize for your scenario would be a negative for others because it forms many opinions and exposes a lot of knobs (like flush). That would make sense if this was a framework which you had to integrate into for batching, rate limiting, retrials, etc. But this is a library and you should think of the cache as a data structure which means you're in control and its okay to go around it. If you want to flush the write back queue, well that's in your code so why not call that directly?\nYou might view that as a little ugly, but if you think of the cache as a fancy Map instead of a framework then its okay. The framework is the abstraction for you build on top of with your other data structures, which encapsulates everything. If CacheWriter doesn't help then ignore it. If its easier to use Map computations instead, do that. The writer is a synchronous hook into a lifecycle activity that you would otherwise not have access to, which means it could be useful for some cases (or not).\nI hope that helps and doesn't come out wrong. My sense is that you're trying too hard to work within the cache as the abstraction, rather than a data structure you mold however best suites your needs.\n. I'm glad you got it working.\nIt is a very fair request and a slipper slope. The cache provides the context of manual or eviction based removal, but why it was manually performed is part of your domain. Trying to convey that through the cache leads to a full fledged framework. That could mean a lot more complex, opinionated, and messier API. The worst case is not that something isn't built in, but that I block you from getting your job done because the API is too limiting by not conceiving a use-case. I'd rather limit the scope to let others build richer frameworks, e.g. imcache, than go overboard and make too many mistakes.\n. Can you upgrade to 2.3.0 and ensure that your JVM is 8u60 or above? There are a few issues that may be at play,\n- 2.2.6 fixed the scheduling of the fork join task which may not have been prompt enough after the changes in 2.2.4 which was a bad release. I was racing to resolve issues which you might be exposed to.\n- 2.3.0 adds back pressure to writes so that even in synthetic workloads writes cannot overwhelm the eviction.\n- JDK 8u60 fixed a bug in ForkJoinPool causes it to discard (not run) execution tasks. That can cause severe mayhem.\n. Thanks for the update and sorry for the inconvenience.\n. Good point. A null reference may be provided only for RemovalCause.COLLECTED (weak or soft collection). For any other cause a null should not be passed through.\nP.S. Long time, hope you're doing well.\n. The Cache interface is philosophically pure. A cache holds transient, recomputable state and an ideal usage wouldn't need to inspect it directly. Like a CPU cache, data would flow in and out to transparently improve performance. If the value was non-local (remote, persisted) then returning it may be costly, especially if seldomly used.\nMost real code quickly gets dirty, so only providing a pure interface would be frustrating by blocking you from getting your job done. Cache.asMap() is an escape hatch which should provide you with everything you need. This also keeps the conceptual weight low because it builds on known interfaces and the new ones you'd learn are all small.\nIts understandably subtle and if you have any recommendations for how to improve the documentation to make it more obvious I'd appreciate it.\n. The JCache 1.x API is a bit weird. Their original design extended Map, but that is confusing because it breaks the contract (get is not passive so equality issues). I described a few of these quirks in these slides for why we introduced the Cache interface in Guava.\nThe final JCache APIs avoid map and provide different methods that return void or V. They try to bypass the Map issues, except allow full traversal by being Iterable. That's weird given some caches (like memcached) don't allow this for obvious reasons. It also means a full Map interface can be emulated, but slower (e.g. there is no size so its O(n)). Its odd to both reject the Java Collection Framework while still providing all of the controversial methods so that it was reinvented.\nInstead we tried to design an API to feel like a natural extension to Java's collections. I think cache.asMap() is a nice compromise between the purist and pragmatist views. It promotes the preferred style for common (simple) cases, while solving @nikolavp issue by using cache.asMap().put(k, v) and cache.asMap().remove(k). It keeps the interfaces short and simple, but it is easy to miss that the power of Map is a hop away.\n. Closing because this capability is available and I'm not sure how to improve the documentation to make it more obvious.\n. I don't think it would carry its weight and be an inconsistent one-off. I agree ifs around builder methods are ugly, but you'll see them for size and expiration too. When externalized then you might take advantage of CaffeineSpec.\n. Its an interesting idea. I think nested builder syntax shows the limits of using a builder as a DSL, but its an okay compromise at times. JavaPoet being a good example where it feels like the right trade off.\nA generalization of your idea would be a utility to model an execution chain.\njava\nLoadingCache<K, V> cache = ExecutionChain\n    .start(Caffeine::newBuilder)\n    .when(hasStats, builder -> builder.recordStats())\n    .when(hasMaxSize, builder -> builder.maximumSize(maxSize))\n    .end(builder -> builder.build(cacheLoader));\nThen you could have a reusable abstraction with its own DSL that could work anywhere there is a lot of if-else verbosity.\n. Closing because I think there isn't a clean DSL for the builder to provide. The best options seem to be either ifs, CaffeineSpec, or some utility like the above. The utility is out of scope, but might be justifiable for a functional library (e.g. @danieldietrich's javaslang)\n. Thanks! I'm pleasantly surprised that you played with the simulator and its Bloomfilter. Since that was for policy experiments, I only eyeballed to see if the traces were similar when moving from Guava's. Its not a perfect filter because its hard coded at only 4 hashes to allow avoiding another computation for the CountMin sketch. I didn't see this because the adaptive policy's usage is a small BF that is rarely populated and cleared regularly.\nThankfully I haven't brought this into Caffeine's caches yet, where unit tests would be required and this is a really good test and fix. I wonder if there are test cases we can borrow from other bloom filter implementations as well.\nI was trying to use the offset to reuse the hash, where its also being modulated to find the array index and the shifted hash to find the bit index. I had thought that since the overlap wouldn't have a big effect because the array would be large, the bit index is a mod using 7-bits, and a shift would result in different results. It makes sense that the quick hash functions might result in more collisions in the middle bits, though I really hadn't expected it to be as dramatic.\nI'm really glad you went ahead, experimented, and fixed it. I think using the higher order bits was a good idea in general, too.\n. I ran your test as a comparison with stream-lib and Guava's. It shows a nice trend that after your fix the implementation is comparable. Guava's follows the theoretical without any optimizations, so it stays very close to 3% FPP by using 5 murmur hashes and uses the minimal space. Stream is a little more lax, using 4 murmur hashes with a slightly higher space. Caffeine's uses 4 balanced hashes with higher space due to being power-of-two sized.\nAcross multiple runs the numbers vary, but all stay close to the desired FPP rate. I couldn't find any exhaustive test suites using different distributions, e.g. to try to detect a weakness in the hash functions. After your fix I think this is fairly robust.\nA nice aspect of how Caffeine will use the bloom filter is to reduce space. The power-of-two sizing might make the bloom filter larger, but as a \"doorkeeper\" it will reduce the CountMinSketch by probably 50% (used to ignore one-hit wonders). This also means a higher FPP would be okay because we're merely trying to compactly query if one entry is more frequent than another. Most often that compares a hot vs. cold entry, which have a wide gap in their frequency counts.\n```\nNumber of Insertions    False positives(%)  True positives\n(s) 1024        22(2.148438%)       1024\n(c) 1024        21(2.050781%)       1024\n(g) 1024        26(2.539063%)       1024\n(s) 4096        102(2.490234%)      4096\n(c) 4096        95(2.319336%)       4096\n(g) 4096        122(2.978516%)      4096\n(s) 16384       403(2.459717%)      16384\n(c) 16384       397(2.423096%)      16384\n(g) 16384       489(2.984619%)      16384\n(s) 65536       1581(2.412415%)     65536\n(c) 65536       1612(2.459717%)     65536\n(g) 65536       2019(3.080750%)     65536\n(s) 262144      6212(2.369690%)     262144\n(c) 262144      6294(2.400970%)     262144\n(g) 262144      7818(2.982330%)     262144\n(s) 1048576     25130(2.396584%)    1048576\n(c) 1048576     25503(2.432156%)    1048576\n(g) 1048576     31529(3.006840%)    1048576\n```\n. I was using the same bits, but modulating based on a different bytes. You're right that using the highest byte is better by avoiding any overlap unless the table is unreasonably large.\n\na value of 1<<31 is a disaster, too\n\nOh that's a good point!\n\nthe method is public and can be called anytime by anyone\n\nNote that this is a utility for the simulator when experimenting with policies. When brought into Caffeine the class would be package private and resized when the cache is (rare event). As its usage in TinyLFU would clear the BF regularly and false negatives are less impactful (temporarily mispredicting \"hotness\") it should be okay.\n\nSpending a bit more memory and getting a bit better rate or the other way round is nearly always fine.\n\nYep, 100% agree.\n. I agree with both of you on disallowing a resize for a public API, and I'll make it private in the simulator too. Since I don't view this as a public API, I am a little more forgiving.\nSince the simulator is similar to test code I unfortunately do not always put in enough effort to verify the utilities. I would have if deciding to promote an idea into the cache and hopefully would have found this issue. I'm very happy about your input because it avoids mistakes in experiments, I might have let this oversight slip through, and anyone can take the code out of the project. I'm not sure why you decided to play with it, but I'm glad you did.\nWhen brought into Caffeine's cache the reason for a resize vs new instance is subtle. There are a lot of possible cache configurations, so the variations are code generated to minimize space. The expected insertions might change when either the cache is manually resized (roughly never) or when using a weighted maximum size. In that case the number of entries might vary widely, so ensuring the capacity is necessary. Performing it as a re-initialization vs. new instance is about code organization and ease of use. It definitely is not meant to imply that it implements a Scalable BloomFilter.\n. You're right. For the tests I was performing the maximum entries in the bloom filter was 256 which led to a very small table. This was to explore adapting the W-TinyLFU window size. I very clearly goofed in my train of thought for how to pick the bit index. \ud83d\udc4d\n\nYou can drop INDEX_MASK as Java guarantees the shift distance is taken mod 64.\n\nI'm not surprised that the JVM optimizes the modulus, but I do like the clarity by not making that assumption in bit manipulation code.\n\nWhat about calling the method ensureCapacityAndMaybeClear?\n\nI tweaked the JavaDoc and reduced the visibility, so hopefully that's good enough.\n\nI wonder if you could do better when using no intermediate int.\n\nIt would be interesting to hear more of your thoughts here.\n\nI integrated all of the suggestions and adapted @ashish0x90's method into a unit test with a pretty-printed table. Its now possible to plug into the simulator other membership filters, e.g. a cuckoo filter.\nDon't stop discussing due to the automatic closing to the ticket :-)\n. Trying to think through my original logic, what I should have done is shifted when calculating the table index. That is a little more natural if one thinks of it as modulating by the number of bits as a flat plane. When grouped into an array the higher bits represent the index and the lower in that location. The shifts and masks would have been optimized versions of that math. That would have had the same effect as the shift by 24, but been more obvious why the shift is taken.\n. Over optimized code for data structures is sometimes a good thing, so feel free to send PR or a gist of anything you've done.\nI don't think I grok your last bit of code (<< hash), but I agree with the statements. I think since the array is a power-of-two the tableMask could be inlined into the shift, which is probably what you're getting at.\n. That does sound neat. I checked in my minor optimizations and I'll try to digest your insights. :-)\n. I already found it and am playing now ;-)\nBefore I saw it, I tried switching to a LongStream and a sequential input. In that case it degraded at 2^29, while Guava did not. When I found and adapted your code, it degrades at 2^28 like you said. I think at 100s of millions of entries that's pretty reasonable given the potential usage for the cache.\nI haven't played with your variants, but looking forward to.\n. One implementation quirk to remember is that I want to reuse the hash computations. The reason is that I expect to have 3 sketches in the cache serving different purposes, all hashed on once per access. That's applied in batches asynchronously (draining buffers), but still it seems nice to reuse the hashes than have different ones per sketch.\nThe usages will be,\n- A tiny bloom filter for adapting the window size (small recency-skewed caches)\n- A medium-sized bloom filter for the doorkeeper (to filter out one-hit wonders)\n- A medium/large count-min sketch to retain the frequency history (TinyLFU)\nThis is why the four hashes were used (rather than finding the theoretical optimal). It looks like your improvements to in variant 3 don't degrade at 2^28, but might not be translatable into reusable hashes. Though I'm not sure whether to favor the reuse between the bloom filter and countmin or the improved quality of the filter.\n. Oh shoot. Another issue is that you're taking advantage of the simulator providing long keys. In the cache we'll only have access to an integer (Object#hashCode) which degrades the quality of the hashing that we can leverage.\n. So running all three variants (Caffeine, BloomFilter3, Guava) and its consistent at a 32-bit element (Long.hashCode(e)). All degrade at 2^28 with a similar false positive percent. Sorry that I didn't mention that constraint earlier, which is what you were trying to work around with improved hashing. It pretty much ties our hands in how far we can push this.\n. - Yep, moving to 64-bit hashes would be nice if Java made that more idiomatic. It would reduce the chance of HashDoS protection code from being triggered in the wild, etc. For an on-heap cache its probably not worth introducing a custom mechanism.\n- This is a cute idea. It would require jigging the frequency sketch to be similar, which would probably make it a Counting BloomFilter instead of a CountMin sketch.\n- This was shown in Less hashing, same performance: Building a better Bloom filter. I had thought this would create data dependencies and the independent hashes worked fine when testing the CMS. Your 2-bits per word is cheaper than the data dependency due to better memory access behavior, so a nice win.\n. The bloom filter as a way to reduce counters (\"doorkeeper\") would halve the number of counters. Since there would be less noise from one-hit wonders the use of 8-bit counters might be okay because the higher collision rate would be offset by the larger frequencies and fewer counters needed.\nI have a very simple JMH benchmark. At the time it appeared that multiplication and memory access were the biggest costs, which was a natural expectation. Since the operations are batched and we can probably assume a Zipf access distribution, the memory accesses might be cached effectively (assuming surrounding code didn't flush it).\n. That's a tough question. I have JMH benchmarks and CacheProfiler as a hook. Unfortunately this can be hard to profile and get meaningful results due to how fast the methods are. The best would be to use JMH with perfasm, but I don't have a Linux machine (cannot be used in a VM).\nDue to the eviction policy work being performed in batch asynchronously, it shouldn't penalize the response times. In that case adding an element to a sketch is the more common operation and likely the largest penalty. As with most caches evictions are the slowest operation due to hash table writes, etc.\nFrom a concurrent perspective the buffers are a synchronization point. The read buffer is a mpsc ring buffer and its hard to tell if the padding is beneficial. When full additions are dropped, so a slower drain might be incorrectly viewed as a gain. The write buffer is bounded, so a benchmark quickly exhausts it and forces writers to wait until free space is available. That means the drain throttles writes, but most likely only in a synthetic test would this occur.\nMy view is that once the single/concurrent throughput is high then the cache shouldn't be a hotspot. A further gain to that throughput is great, but likely has an insignificant impact. Then the goal is to reduce latencies, how widely it varies, and hiccups. Here the buffers serve to try to avoid that on user-facing threads and penalize a background thread (assuming the norm of spare cpu cycles). This is harder to measure and falls into the complexity that Gil Tene talks about. And of course higher hit rates are a huge latency win, so that's critical.\nFrom afar I would expect the biggest gains would come from improving the read buffer and the sketch write time. Sometimes I wonder if it would be worthwhile draining into a multiset so that the sketch is given an increment count (less hashing, better cpu efficiency). Probably a run length encoding scheme instead of a true multiset would work well.\n. I often use a scrampled Zipf because it best mimics real world (a few hot entries). That helps create contention and lets us leverages the cpu's cache, whereas a uniform distribution wouldn't. Using a micro-benchmark tool like JMH is really helpful, but overuse isn't if you measure the wrong things. I have a few that I've tried to be careful about which I think are the most realistic.\n\nhard to tell if the padding is beneficial (just switch it off and re-run)\n\nThe padding is tricky because it depends on the machine. A consumer laptop would have a single socket with a moderate cpu cache. A server might have multiple sockets with a large cache and we hit NUMA effects. The last time I tried turning it off there was a benefit for my laptop and a penalty for a 16 core cloud machine. I think I also tried the 32-core (2 socket) at one point. But my benchmarks could have been flawed or another mistake, so I don't know if that's true. I'm also unsure about if NUMA and false sharing interacts differently than on a single socket.\n\nHere, the doorkeeper obviously helps somehow...\n\nFrom my early analysis it looks like the doorkeeper might make sense to have only for moderate to large caches. For small ones it can have a small negative effect to the hit rate and the space savings is tiny (e.g. 128 longs). For a large cache it has no negatives and has a 25-50% savings. The number of counters needed doesn't grow linearly, but I'm not sure what the factor is. We'd probably need to plot of a couple traces to figure out a threshold and slope.\n\n(misc)\n\nAnother aspect I forgot to mention was GC friendliness. A read produces no garbage, but a write produces a little that should reside only in young gen. This would be the task (AddTask and RemovalTask), the hash table entry, and any lambdas. I didn't see a gain when I tried to remove the AddTask (inlined onto the Node) and removing the lambdas would require forking the hash table which I am not favorable towards. I think this is okay, but wanted to mention GC as an important factor to watch.\n\nCould you elaborate? (micro-batching)\n\nIn an LRU cache a read is performed as the steps of finding the entry in the hash table and then reordering it on the LRU list. The reordering is a mutation, so despite being fast a lock is required and that causes contention. Instead we store the event into a lossy buffer and when full the events are replayed on the LRU. This avoids lock contention as readers/writers don't have to wait, the policy stays non-concurrent, and we might get CPU efficiencies by batching the work. That made it easy to switch from LRU to TinyLFU by enhancing the policy.\nEach event is replayed in isolation: poll the event, add to sketch, reorder. If we assume a Zipf distribution then many events are for the same entry. For those hot entries the sketch's counters would quickly reach their maximum and the LRU reordering wouldn't really do anything. While the reordering is cheap, the sketch update is more costly due to hashing and scattered array reads. Its still fast, but might be in vain.\nThe ideal micro-batching would be to drain the buffers into a multiset (entry -> count) and then replay each entry once. This would improve the I$ by shortening the loop code. It would also let us hash and update the sketch once per entry, thereby avoiding wasteful work. But something like Guava's HashMultiset isn't a gain because we're hashing again each time.\nThe idea is a middle ground by using a run-length encoded batch (e.g. A3, B1, A5). When adding we check elements[i] == e and respond accordingly. I pushed a branch of some work in progress on this idea.\n. > Any idea why?\nI'm not sure. Its probably has something to do with entries arriving near the end of the sample period. Since the reset clears the filter those entries don't get a +1 boost as for as long as early arrivals. Since this was only for small caches it makes sense that the impact would be more pronounced. For a large cache where frequency dominates it wouldn't be an issue.\n\nI see, it's a huge monster. An advantage of forking would be the access to HashEntry.hash, which would save us hashing and spreading.\n\nThere are definitely efficiency gains at a high complexity cost. That's a lot to accept when starting from scratch and falls into premature optimization. We could justify late into development as an experimental spike, once all the other complexities were resolved. Even then its a big hurdle that could be difficult to see the benefits in practice.\nI think VarHandles and Value Types are blockers. They have a big impact on the hash table and it would be a lot of work to port over changes. I'm really keen on seeing how value types evolve, but JDK10 is a long ways off.\n\nIt's actually RLE, but on the consumer side, so no arrays are needed.\n\nOh that's a much better idea. We should probably integrate that into the CountMin4 so that reset handles it properly.\n\nHere you probably want >=.\n\nThanks! :-)\n\nI'm curious about the typical size of the ReadBuffer?\n\nCurrently it is sized at 16 elements per ring buffer. I use a variant of j.u.c.Striped64 to dynamically grow a table of them when contention is detected. For a cache with little concurrency there is low overhead and if there is a lot then it scales to meet the demand (up to 4x number of cpus). If we had a 16 core machine then it a maximum 1024 elements. Since each element is padded to take 64 bytes (1 cache line) then it would cost 64kb. Another way of thinking about it is 1kb per ring buffer, with up to 4x NCPUs based on contention.\n. Unfortunately @Contended in application code requires the JVM flag -XX:-RestrictContended or else it is ignored.\n. @Maaartinus I implemented your approach for RLE and ran a few cursory simulations to see the effect. The vast majority of run lengths is 1, then 2, and a few times 3-4. In the recency-skewed workloads where consecutive accesses would be most likely it dropped roughly 1 of every 30 increments. In a frequency workload it wouldn't even activate. The longest run I saw was 34 in a short LIRS recency trace.\nThe other negative I hadn't considered was that it reduces the sample period. If I add the run count (or maxed to 15) after an addition it may over increment the sample counter. That causes a reset (halving the frequencies) to occur more frequently. This would be noise in a large caches, but hurt small ones where resets happen too readily. If we determine the actual increment value then it adds more conditionals to get the minimum. All the extra logic probably negates the benefit of not doing a few extra hashes.\nSo I think my idea of run-length encoding was a flop. But it was a questionable idea to begin with.\nI think the next steps is to integrate the adaptive window technique into the cache. That had a nice boost for small caches with recency skewed workloads.\nThe other task is to analyze the doorkeeper. My cursory experiments indicated that it could hurt small caches, but could greatly shrink the CountMin4 with large caches without impacting the hit rate. As the size increases I think so does the factor the sketch can be reduced by. This hypothesis needs to be validated and the slopes calculated. If we could determine an equation for calculating the sketch's size that would be pretty nifty. However we're talking about an overhead of a few megabytes for millions of entries (e.g. database), so it may not be worth it.\n. Hey! I'm glad you're back (and on JDK8!). :)\n\nThe BloomFilter works great thanks to our fixes, but the usages aren't robust enough to promote it yet. I had hoped it would serve two purposes,\nDoorkeeper: This would reduce the FrequencySketch sketch size (per paper). But it increases the frequency-bias of the cache, causing worse performance for recency-skewed traces.\nFeedback: Evaluate the window entries rejected by TinyLFU and adjust the admission window size based in a sample period (increase on 2nd rejection, decrease otherwise).\n\nUnfortunately the feedback idea worked great for small traces, but not for a large trace (scarab). That meant I couldn't integrate the doorkeeper yet, but the sketch overhead is small so its not pressing. See (4) for a new direction you might be interested in helping on.\n\n\nConservative update is an option in the simulator, where I forked the FrequencySketch. It turns out not to have an observable change (diff is noise). This is probably because TinyLFU doesn't actually care about the counter value. Rather it cares about the difference between two frequencies in order reject cold entries in favor of hot ones (\"heavy hitters\"). If two are so similar that a smaller error rate changes the decision, then they probably have an equal likelihood of reuse. That would mean either is a good choice, so the extra cost of CU isn't worthwhile.\n\n\nincrement is called on a read or write, and frequency is called on an eviction (twice). So frequency is called much less often. Note that since this work is, by default, delegated to a background executor (commonPool) it's usually not an observable cost.\n\n\nFor adapting the admission window based on the workload, I recently had a new idea. At the end of the paper there is a chart of the hit rate vs. window size, which shows a very smooth curve. A hill climber algorithm could be a perfect fit for finding the optimal configuration dynamically, and adjust when the workload changes. I wrote a very naive version using simulated annealing which works pretty well on small and large traces.\n\n\nI think using a hill climber has a lot of potential. The current version still suffers from jitter due to sampling noise, which simulated annealing helps to alleviate somewhat. The only paper I found similar to this is from Intel, The Gradient-Based Cache Partitioning Algorithm, where they use Chernoff Bound rather than a fixed sample size. I don't have experience in this area, but I do think its one of the most promising topics to focus on.\n\nAnother area of interest is variable expiration. Typically this is done using a O(lg n) priority queue or lazily leaving dead entries and relying on maximum size. Both could be done easily in user code and are not very satisfying solutions, and current policies are O(1). A Hierarchical Timer Wheel could be the answer, but care is needed to ensure we don't suffer from its degradation scenarios often. This is an idea that I've read up on, but haven't prototyped.\n\nMost likely (4) and (5) are the most valuable areas to focus on and could be fun to explore.. All questions are wonderful. I'm going to edit your message to number the sub-items.. You may feel a little overwhelmed, but you're nailing every question. Since Caffeine's code is complicated, its very useful to disregard it by instead focusing on the tooling (e.g. simulator, sketches). That's been useful for me so that I can explore ideas separately from the cache itself.\n0. Background\n\nYes, with slight pragmatic differences (e.g. HashDoS)\nYes. I use the term \"admission window\" where new entries always enter for evaluation. TinyLFU is an admission policy and in our setup it guards the main cache (99%). This makes it generational, whereas the original conference paper had no windowing.\nYes.\nYes.\nYes. Segmented LRU is an eviction policy.\nYes.\nIt is discussed when describing SLRU, but briefly since there is a paper referenced to describe it. If using a simpler LRU it performed nearly the same and was my original version. But I observed a small improvement hit rates on the WikiBench with SLRU because a better victim was chosen. Since most workloads don't need a window, you can observe the effects with Lru_TinyLfu and the like.\nYes.\nYes.\nYes. Other policies use \"ghost entries\" to keep absent keys in the hash table.\nYes. Note that leads to an interesting HashDoS attack we mitigate in the library.\n\n1. BloomFilter\nThat was Roy and Gil's evaluation with their limited traces and I got involved later (from reading their conference paper). Their traces were very Zipf like, so frequency biased, so there was no negative effect. I hadn't evaluated the doorkeeper until we added the BloomFilter. When I did, I saw it did negatively impact recency-skewed traces (this was most likely on small traces). My rough guess is that a more frequent reset (clear) hurts recency by more often seeing them as \"one-hit wonders\" and diminishes their counts.\nIts interesting to hear byte[] is faster. I figured with hardware caching and cpu optimizations, the difference wouldn't be observable. But since reset is O(n) and we lack SIMD support, using a long[] would benefit its bit manipulations.\nAnother idea is to store all counts for a key in a single slot. See One Memory Access Bloom Filters and Their Generalization and A Comment on .... This could increase the error rate but improve memory access patterns.\n4. Hill Cimbers\nYes, headWindow was a rename of headEden in case Gil or one of Roy's students wanted to hack. Eden was my original term (as I was borrowing from GC generations) and when describing the idea to Doug Lea, he referred to it as windowing. That was a much more apt description, though I never cleaned up terminology in the code.\nI tried to factor the code so that we could easily try different HillClimber types, especially someone less familiar with the code. The simulator code is lower quality (as experiments), so could have bugs or unclear. Its also fine to copy/paste there to keep from coupling experiments, since its not production-oriented code.\nFYI, Roy and Gil would want to write a paper on this improvement if we had good results. Gil would like to help next month (code, math, or where ever) when his current project winds down.\n5. Variable expiration\n@ifesdjeen wrote a nice implementation worth reading and has helpful pointers to references. If we had one it would be tickless (\"deferrable\") and we might follow Linux's variant.\nHowever, its an interesting question of whether their worst-case is our common-case. The algorithm is optimized for timers not firing and instead being added/removed frequently. This would hold for time-to-idle expiration (any access) but probably wouldn't for time-to-live (on write). The latter is more useful by adding a \"freshness\" concept, e.g. we discard an entry if it is too stale and instead miss if read. An implementation would need to ensure it struck the right balance. Also note that there is a C project that claims O(1) in all cases, but I haven't investigated to understand its trade-offs to get there.. ### 1. BloomFilter\n\nThe BloomFilter allows for smaller FrequencySketch and this leads to more frequent reset, right? \n\nNope, the period is based on the cache size. I meant that a small trace is for a small cache, so it resets regularly. The doorkeeper reset is a clear of the bloom filter. That could mean often thinking arrivals are one-hit wonders which its trying to filter out.\nAlso the reduction equation doesn't seem to be linear. If we reduce the FrequencySketch too much it has a high error rate and impacts the hit rate. For large caches a 4x reduction was fine, but only a 2x for small caches. So we'd want to come up with an equation for that I suppose.\n\nA good part of the speedup comes from my sloppy hashing, which might be too sloppy. It all may be wrong, as I haven't tested it yet.\n\nNifty. That makes sense.\n\nDo we? Isn't ByteBuffer.asLongBuffer helpful? Or Unsafe?\n\nGood point. If we used those we'd have the best of both worlds.\n\nAnother posibility to reduce the overhead might be special treating the most frequent entries (in a sort of HashMap with no rehashing nor chaining).\n\nGil has a sketch called TinyTable, which is similar to a Cuckoo Filter. These have more expensive insertion costs but are very compact.\n4. Hill Cimbers\n\nThere's no maxProbation, but wouldn't changing this sum be possibly also useful?\n\nShould be and we should try once we have a good climber. That would only impact traces where LRU is optimal, but most would be somewhere in the middle. I think we should revisit this, but it won't be pressing until we have a solid climber.\n5. Variable expiration\n\nOne nice thing is that missing the timeout is no big deal\n\nAnother is the cost is usually not user-facing, since replaying is async. Most apps have spare cpu, so we can take a small hit without impacting latencies. But some use same-thread executors instead so its not a sure thing.. > Cool! Isn't using 4-bit frequency counters sort of equivalent to using 16-way-segmented LRU cache?\nIt is similar. Facebook's evaluation was that 4-levels is the best fit. See the S4LruPolicy where the number of levels is configurable.\n\nI can't see the table size being reduced when a Doorkeeper gets used.\n\nSee the reference.conf file, under count-min-4. There are multipliers to tune the sizes.\n\nVariable expiration...\n\nYour description sounds very much like Linux's improved variant.  They have a similar scheme where the resolution degrades the further out the schedule is.\nI'm also unsure why you think its not worth it, because your argument and further thoughts match the ideas well. An exact expiration would be modeled by a O(lg n) priority queue, whereas timer wheels use coarse buckets instead of exact time. Then there are different layouts to choose based on the desired optimizations. So they too are very pragmatic to be good enough and fast. I think you're in agreement with the various authors.. Sorry for another short response. It's late here so I'll be asleep soon, if the insomnia quits.\nRight now multiple files per trace format is supported, but not multiple formats. That could be done using a prefix, e.g. \"fmt:path\", without much effort.\nThe stats might also be made more extendable. Currently I've used debug statements to prototype with. But a nicer scheme shouldn't be hard to add.\nI think you're right about the timer wheels and pretty much everything else. :). Here's some answers to your questions that I missed last night.\n\nSo the increased round-off error due to the Doorkeeper is more relevant for small sized and bites us, right?\n\nThat's my guess, or it is something related to that.\n\nOr should the period be bigger when the Doorkeeper gets used?\n\nI'm hoping that adapting the window is still our best bet. The door keeper would cause the climber to adjust to a larger window. So we could rely on that to correct for us.\n\nHill Climbers: What traces are most interesting (really good or really bad) for them?\n\nFor recency,\n- Scarab traces: These are large caches (100k). A longer run is downloadable (see comment).\n- Cache2k traces: These are small caches. Sounds like a Hibernate session cache, so flushed manually or after a commit.\n- UMass Youtube, Lirs sprite, and ARC oltp are small\nFor frequency,\n- Lirs glimpse (gli.trace.gz) and loop. I think cs and ps, but would have to check. All small\n- ARC's DS1 and S3 are large\nI was changing the default window size, running a trace, and seeing it adapt in the desired direction. I also would take two traces, e.g. glimpse + sprite + glimpse, and seeing it adapted each time the workload changed.\n\nI guess, not, but I'm asking before diving into the code as there's so much stuff there...\n\nThere is a lot to digest. The simulator is simple and easy to extend. I'd ignore Caffeine's caching code as its complex due to concurrency, features, etc. So might be fun to read but not important until we have good schemes nailed down.\nYou're ramping up really quick with great questions and insights. :). Being busy is understandable. I seem to be always in a crunch these days, too. I'm glad you're still thinking over the problems, as that's valuable too.\nI agree noise and history are too big problems. I tried using a weighted average of the hit rates with poor results. Similarly a dynamic pivot size didn't work well for me either. A large sample resulted in too little movement and inability to correct well. The small sample results in too much noice so that the configuration improves but isn't optimal.\nSimulated annealing isn't quite right, but did help reduce the impact of the noise. The poor version I wrote doesn't try to escape a local minima as you're right that would be unnecessary. But the idea of a temperature to cool off with did help reduce the impact of noise.\nOne idea is to adjust the sample size based on the temperature. When hot the sample is small to make rapid decisions. As it cools off, the sample increases to filter out the noise.\nIntel's paper has an elegant solution to side step the sample period problem and deal with noise. They use Chernoff Bound as a confidence function. I recommend reading it as it sounds promising. Its probably worth implementing next to evaluate their approach.\nI think the hill climber idea has promise, but finding a robust solution is going to be tricky. :). @Maaartinus Can you send me your email (private is okay) so that I can loop you into conversations with @gilga1983 and Roy?. You mean Maaartinus? Sorry, I don't know your full name :). That was my assumption, but I also didn't want to send an introduction email that bounced.\nLikewise, I'm hoping that we don't need to run in split mode to take advantage of their ideas. Instead I think one side is the previous hit rate and the other side is the new adjustment. If the confidence function is positive then we grow else we shrink, perhaps with a little jitter if it fails to adapt as the workload changes.\nBut I don't fully grok Chernoff Bound either. Since the paper states it is very simply to code and is low cost, it sounds intriguing.\nWhen I had initially proposed hill climbing, Gil recommended two ghost caches to determine which direction was favorable. That would have avoided the noise and he hoped it would prove if the idea worked. If so, then we'd have an optimal version to judge against as we tried to find more compact alternatives to the ghost caches. Its a good idea, but I didn't get around to trying it.. Thanks for jumping in, Tobin!\n\n\nThat's a good observation. Its not clear if by using two filters the memory overhead would be the same as a single one experimented with, or be double. There would also be a little more overhead to insert/query into both.\nI also don't have a good equation for the potential space savings by using the doorkeeper. It appeared that the larger the cache, the larger the savings factor could be made. In general it was a 2x reduction to the CMS, but a 4x was okay for large traces. Perhaps more for even bigger caches. Ideally we'd have an equation to adjust by and know how much memory was reduced.\n\n\nThat may work, so its worth exploring. The idea of TinyLFU is to estimate the frequency of all items in the data set with aging, and thereby compare to retain the top-k heavy hitters. I don't know if some of the alternative stream summary structures would have higher overhead, as that Space-Saving implementation doesn't appear very compact.\n\n\n\nSo far overhead of the CM Sketch hasn't been a complaint, so I'm weary of spending too much effort optimizing it. I'd be happy for contributions as its a worthy endeavor, but energy might be better spent elsewhere. That's why I think adding an adaptive window and timer wheels are more impactful for a contributor to spearhead.\nfyi, I put some of this discussion in a slide deck with other design overview details. Hopefully that compensates the article and design doc.. FYI,\nGil committed an adaptive sketch, AdaptiveResetCountMin4. I haven't experimented with it much, but Gil's early evaluation shows good results.. I prototyped a simple hierarchical timer wheel. This needs a lot of work regarding testing, optimizations, and integration into the cache. But I thought it was a good starting point for us to iterate on.\nThere is also a paper by the Technion on replacing the sketch with a table-based approach (e.g. cuckoo filter). That avoids the replaces the reset interval with a random replacement when the table exceeds a threshold. See TinyCacheSketch for the implementation they added and evaluated against (by setting sketch =tiny-table` in the configuration file). Since its not published yet I shouldn't link it, but send me an email if you would like to read their analysis.\nThey are also working on the adaptive sketch and hope to have a paper ready in May. I haven't spent much time on it, but they're getting promising results so far.. The cache provides ordered snapshots, but that is O(n) to construct. Its primarily for warm restarts (e.g. Cassandra). This means you could get an order determined by the eviction policy, but that's not what you really want.\nImplementing FIFO views of a ConcurrentHashMap wouldn't be too difficult if you synchronize all writes but leave reads lock-free. Then you could have a linked list cross cutting the map with volatile prev/next fields. A removal would unlink the entry but not update the removed entry's pointers so that a concurrent traversal could iterate through it.\n. I've seen StackOverflow discussions where answers used ConcurrentLinkedQueue using O(n) removals or ConcurrentSkipListMap with timestamped keys (seems error prone).\nThere are caches like Apache Ignite and Infinispan that interleaved a ConcurrentLinkedDeque by exposing its nodes so that they could LRU move the entry safely. It looks like Ignite wrote a standalone FIFO version as a fork for ConcurrentHashMap, which seems much more complicated then I'd have thought necessary.\nI always found simpler alternatives whenever the need came up, which is probably why there is nothing popular floating around Github. For example if iteration is rare you can lock on writes to insert into a LinkedHashMap, provide a synchronized forEach(BiConsumer<K, V>) or snapshot, and get lock-free random reads. Probably most use-cases don't need high iteration concurrency so quick workarounds are good enough.\n. I agree insertion order with multiple writers quickly becomes meaningless and depending on it is very error prone. The idea only makes sense to me with a single writer, but not wanting to penalize reads by keeping them lock-free. That could be a priority queue with O(1) lookups. All cases I can think of either have better solutions or simpler workarounds.\nShouldn't it be the stream's responsibility to ensure ordering? The terminal action should receive the entries in order if the source was ordered and the StreamOps were constructed to retain it. I'd expect all you should do is set the collector's characteristics to preserve the original order. It looks like your fix is to remove UNORDERED characteristic from the toImmutableMap collector.\n. Thanks. It looks like CacheManagerImpl#getOrCreateCache should turn it on as well. The change passes the TCK, so I'll port your test over tomorrow and merge the fix in.\n. Let me know when you want me to release a new version for you. In the other issue, It sounded like you were going to do a little more testing to try and uncover any other mistakes.\n. Released 2.3.1 with this fix.\n. You're right that we don't do anything intelligent with the URI and followed the pattern from the specification's reference implementation. If desired perhaps we could let you specify a different resource base name, but I'm not sure how useful that is.\nThe configuration library allows you to specify the configuration source as a system property to the file, classpath resource, or a URL. That has always worked very well for me in practice and with the trend away from monolithic applications I'd question the need for a multiple root configuration files.\n. I'm closing, but will reconsider if reopened.\nThe most likely case for this feature is if the configuration files were not mergable and there were conflicts. The Typesafe library has a merging model so this isn't a problem.\nAnother case might be due to templates (\"default\") configuration. For simple cases this removes redundancy, but in complex applications the default shouldn't include policy information (e.g. max-size). If a library specified the defaults in its reference.conf then it might become an issue across all cases. I think the solution to that reuse problem is to allow a definition to specify a template to prototype from. Then libraries could use their own templates and the default could be merely the configuration like JMX. If you'd like that enhancement then please create another issue for us to discuss on.\n. > I guess that most users trying to integrate hibernate and caffeine via jcache would be a bit surprised that they cannot configure the cache as described in the hibernate docs..\nI reviewed the specification and other implementations, and users should be surprised because the documentation is misleading. The specification only requires that the URI provides a unique identity within a classloader. In regards to configuration it merely states,\n\nFor example an implementation may permit a URI to be used as the location of a configuration file, say for pre-configured caches.\n\nWhile this is permitted it does not allow implementations to restrict only use it for that case and reject a URI if the resource is not found. In this way Ehcache 3 may not conform to the specification by adding an additional restriction.\nWhen reviewing other open-source implementations it seems that only Ehcache uses this configuration option. I believe they do this because only one ehcache.xml is supposed to exist on the classpath and is not merged, so this approach provides isolation for different modules to define their own caches. Since the Typesafe Config format is more flexible this problem doesn't arise in the same manner.\n@sebersole\nI think the Hibernate's documentation should be reworded to not imply that others will configure similarly.\n. Sorry, I didn't mean to sound critical of Hibernate. I had to investigate to confirm that I wasn't violating the specification and its an easy misunderstanding to make. I think it was fine for Ehcache to highlight that ability given that they contributed support, so the fact that others don't is a detail Hibernate shouldn't have been expected to realize.\nI don't have much of a complaint against the current documentation because it is correct. Those two properties are used for configuration (with the URI as an optional namespace rarely used). The example of an Ehcache configuration is again valid, as they use the URI to support multiple configuration files. The fact that it confused a user is an unfortunate consequence of the example.\nI didn't have much feedback on configurability because it is messy in JCache. The standard's configuration object is very limited and only supports unbounded caches. It was enough to show demos and imply vendor-free configuration, but can't be used that way in practice. I think a unified configuration is the wrong approach and would only add unnecessary complexity for Hibernate. I am quite happy with the current state of Hibernate's support, so my only feedback would have been negative towards the proposal.\n. Unfortunately a different map implementation would either be deadlock prone, use non-atomic computes, or a single exclusive lock. There would be the case of (A) => (B) and (B) => (A), so at best an ordered set of computations might complete. As noted in your link, I worked with Doug to detect and fail fast because its an easy mistake to make. The cache's JavaDoc do warn that recursive computations are prohibited.\nA workaround for ordered computations is to use AsyncLoadingCache and its synchronous() view. In that the map's computation is merely inserting a CompletableFuture, which computes asynchronously. A deadlock may still occur due to unordered computes, but you can recover by adding a timeout to fail the computation (built-in to JDK9 futures).\n. Yes, it should be deadlock free when ordered.\nIn the regular case, an unordered computation might deadlock due to the computation. The ordered case would also be prone due to the hash table resizing during the operations. The latter case is why HashMap was updated to restrict recursion as otherwise it corrupted itself.\nIn the asynchronous case the map operations are immediate with no dependencies. The futures may deadlock on each other when unordered. But if ordered then it is a linear dependency chain and shouldn't cause a problem. The synchronous view is merely a helper that blocks on the returned futures so that the calling code isn't forced into being written in an asynchronous fashion.\n. I agree it shouldn't result in an infinite loop, but it is not something I can fix locally without a penalty. It is a bug within ConcurrentHashMap that we are exposed to.\nI brought this issue up and had it fixed in the 2011 preview and, after discovering it regressed, worked with Doug to fix it in 2014. Unfortunately that was never brought into a Java 8 release, so we have to wait for 9. Until then I'd need to embed the corrected hash table, which I've been holding off on. (Note that I also fixed recursion in Guava's loading and reported it as regressed in their new compute() support).\nAll of the computing methods are documented, e.g. CacheLoader states,\nhtml\n<b>Warning:</b> loading <b>must not</b> attempt to update any mappings of this cache directly.\nYes, an unbounded pool is ideal for async recursion. It relies on decoupling the map operation from the computation so that the future is inserted and then executed, rather than executed during the insert. This means that a caller-runs policy cannot be used (e.g. same-thread executor), which is ensured by an unbounded pool.. Can you check which Java version you are running? You might have hit JDK-8078490.\nThe write buffer is bounded, but should only block when full. That is rare unless under a synthetic load. However a bug in the JDK causes ForkJoinPool to discard some tasks, which causes the buffer to be stuck. The write buffer is used to allow the cache to do work asynchronously, just as you were requesting.\n. Then that is very weird. It means that somehow the maintenance task is stuck and I don't know why. If we can reproduce that would be very helpful.\nA quick fix is probably to set a different executor and see if that helps.\n. A quick hack to the stressing tool so far doesn't show any problems. The expectation is that the write buffer would be full, the drain status would be processing, but no thread was actually doing the work. If we can get that state to occur then we can determine what is wrong. I'm testing on 8u92 with a 4-core (8 SMT) macbook pro.\n. I took another angle and began looking at refreshing instead of normal writes. I was then able to reproduce this being stuck in a bad state (Drain status = Processing -> Required) without a processing task running. I haven't been able to figure out yet why that happens.\nFor a synchronous cache using a same-thread executor (Runnable::run) resolves it. For an asynchronous explicitly calling cache.cleanup() periodically fixes it. Those are temporary workaround until we can figure out what is going wrong.\n. It also resolves itself if I use Executors.newCachedThreadPool() instead of ForkJoin. All of my debug logic shows clean entry and exist of the performCleanUp method. So this may be an issue where the executor is discarding our tasks, like the JDK one mentioned earlier.\nEdit:\nIt seems any executor setting, except ForkJoinPool.commonPool(), does not exhibit this problem. Even Executors.newWorkStealingPool() which is another FJP seems to be fine. So it looks like something odd about the commonPool, as when I take away all other differences its the only one that triggers this failure.\n. I tried switching away from the state machine to a Semaphore(1) to transfer the lock state. This fails in the same manner, indicating that the state machine is not at fault. The log statements show the scheduling of the task, but the executor discarding it instead. Even when I change PerformCleanupTask to a simple runnable (to avoid FJP optimizations) this still occurs. So my best guess is that a bug in FJP is the root cause of the problems.\n. Please see the issue 90 branch and run the Stresser. That can be done using your IDE (guide) or running the command ./gradlew stress. You should see output like,\nshell\nscheduled drainBuffersTask\nbegin: performCleanUp -> 2\nexit: performCleanUp -> 1\nrelease: performCleanUp\nacquire: scheduleDrainBuffers\nscheduled drainBuffersTask\nWhen using a FJP it will hang after scheduled drainBuffersTask. Any other executor type will continue and finish successfully.\nThe reason Executors.newWorkStealingPool() was successful is because I forgot to remove some debug code that switched to whenCompleteAsync which using commonPool. After reverting back it hangs too, as one would expect.\nThe reason the state machine was left in a bad state with FJP is because scheduling moves the state to Processing -> Idle to guard other scheduling attempts. A further write transitions it to Processing -> Required. Since the task wasn't run the state is stuck, causing the buffer to fill up and block.\nI do not yet have this isolated to a FJP only test.\n. Doug also started poking at it and, not surprisingly, this is probably my fault.\n```\n\nIt's weird to do with a Thread.yield anyway.\n\nI had probably incorrectly assumed it would rarely be triggered, as the write buffer is set to\nabout 128 * NCPUs, and be used only to avoid starving the maintenance task's thread. That\nwas the case I had observed in write stress test (cache.put) with an unbounded queue, but\nthat required the machine be dedicated towards that goal. I hadn't considered the possibility\nof starving the FJP through a completion callback, so changing to a more conventional scheme\nsounds like the right solution.\nOn Sun, Jun 12, 2016 at 3:37 PM, Doug Lea dl@cs.oswego.edu wrote:\nOn 06/12/2016 06:30 PM, Benjamin Manes wrote:\n\nSince there are not many examples of ManagedBlocker, I've never been comfortable\nusing it. Its a bit of black magic that I'd need to play with to fully grok.\n\nIt's weird to do with a Thread.yield anyway.\nThe best diagnose would be if you replace that yield with\na condition wait of some sort triggered by whatever that other thread\ndoes.\n\nBy setting WRITE_BUFFER_MAX to a small value (e.g. 8) and using\nnewWorkStealingPool(512), I see only 110 FJP worker threads in the stack dump\nwhen it halts. Would that be indicative of an issue beyond not using ManagedBlocker?\n\nConceivable.\nBut I haven't been able to see any effect that looks like a FJP issue.\n-Doug\nOn Sun, Jun 12, 2016 at 3:08 PM, Doug Lea dl@cs.oswego.edu\n<mailto:dl@cs.oswego.edu> wrote:\nDo you have enough parallelism in pool to run?\nI see all workers spinning on\n         at java.lang.Thread.yield(java.base@9-ea/Native Method)\n         at\ncom.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:910)\n         at\ncom.github.benmanes.caffeine.cache.BoundedLocalCache.replace(BoundedLocalCache.java:1907)\n\nIf you can't guarantee enough workers, you could use a ManagedBlocker here.\n\n-Doug\n\n```\n. > Thanks for taking a look and showing where I made a poor oversight.\n\nI think the right solution is to replace Thread.yield() with a call to performCleanUp(), or a variant. That would block the thread on the eviction lock and help out by performing the maintenance work. Since the thread would otherwise be waiting anyways, there it wouldn't be incurring a significantly higher penalty for this degredation mode. Switching to that call now passes the stress test and doesn't seem nearly as awful as the ad hoc yield.\n\nI need to think about whether to use performCleanUp(), what an optimal variant might be, and if there is a better alternative. But thanks to yours and Doug's generous assistance I think we have a potential fix.\n. I'll make a release tonight once TravisCI finishes, so you can upgrade first thing Monday morning. :-)\nSorry for my oversight on this and thanks for the patience when you uncover my mistakes!\n. Released 2.3.1\n. No, but there are workarounds. If improperly designed it could lead to an O(n) scan to evaluate all entries when attempting to evict. Likely a different configuration is more suitable to your problem, so can you explain what you are trying to achieve?\nexpireAfterAccess turns out to be rarely useful in practice. Rather than being about data freshness, it is a time-based LRU to give a maximum lifetime of an inactive entry. This could make sense if using a reference-based cache to more eagerly evict prior to GC, but a maximum size is the preferred strategy.\nexpireAfterWrite handles data freshness and can be combined with refreshAfterWrite to retain the active entries. This is commonly useful for local caches which are allowed to be eventually consistent across a cluster.\nmaximumSize takes into account frequency and recency. It will usually outperform a custom policy, e.g. prioritizing certain customer's data over others due to business concerns. In those cases it makes more sense to evaluate ways to reduce the miss penalty, e.g. a memcached tier, to achieve good performance for all users.\nmaximumWeight will exclude zero-weight entries from size-based eviction. This can be useful in cases where an entry should be pinned, e.g. an incomplete Future should not be evicted. This can be useful in  some cases, like Hydra's mediated eviction for transaction leases (uses a fork of our prior project).\nPer-entry expiration is a sometimes useful feature that is not provided. The most common approach is left to user code where expired entries are lazily size-based evicted and emulates a miss when accessed. The other strategy like O(lg n) reordering and a dedicated thread can become too expensive. There is a high possibility that this feature could be implemented efficiently using a hashed wheel timer, but I have not experimented with that idea yet.\nA hacky workaround to achieve what you want could be to have a CacheWriter evict to a temporary map and schedule an asynchronous task to repopulate the cache. To avoid races, a CacheLoader could first check this map prior to fetching the data from a remote source. This would recycle the entry.\n. Closing. If your case is to resurrect an expired entry by checking if it is still valid first, then I think the best solution is to use refreshAfterWrite and implement that intelligence in reload.\n. Thanks for the supportive suggestion!\nPlease see SOLR-8241 where I supplied a patch for evaluation. My understanding is that Solr uses LRU by default and has an LFU option. There are custom multiple cache implementations and Guava's is sparingly used. The search traces I have support @elyograg's observation that frequency has a significantly higher impact than recency, so there is an opportunity to for a significant improvement to the hit rates.\nIt is a bit more difficult to make migration proposals as a library author. While I can show advantages, the changes don't resolve a user's problem and rightfully developers are weary of breaking something for theoretical improvements. @elyograg has been supportive of the idea and is currently using Caffeine in a project for work, hoping to gain confidence & familiarity to move that ticket forward. I think if we had real-world data that might help answer the open questions (does it improve performance? does it improve the hit rate? is the cache a current bottleneck?). For now progress depends on how much time @elyograg can steal away, which can be really difficult.\n. Perhaps then you'd be interested in experimenting with the patch to quantify the impact? Its a few months old so some updates are needed and I wasn't familiar with running Solr. (Our search needs are modest and there is only 1 other engineer, so I started with Algolia). Alternatively, if we captured a cache access trace then we could run it through the simulator to compare the hit rates.\nI suspect improvements to the hit rate will have a greater impact than to concurrency. It might allow for reducing the heap size, lower miss penalties (e.g. disk access), and discard low value entries sooner.\nAt some point I'd like to show a proof-of-concept of Caffeine as an off-heap cache using Apache Mnemonic. I think that could be an attractive option for large heaps, especially since it avoids the serialization overhead.\n. I/O resources can definitely be tricky. It is very tempting to use a cache as an object pool, but the resource constraints are slightly different. You'd still have a race with Guava or a same thread executor, but I guess its less pronounced under load. In both libraries the removal listener can still be called by a different thread. Guava evicts to a removal notification queue which all threads process (so the remover might not do the work). I think whatever leak you ran into was already present and may still be.\nUsually one should prefer having the cached data be fully materialized and using the I/O resource during the load operation. Then the function fetches using the connection pool (db, http), does the work, releases it, and multiple clients can access the cached result concurrently.\nSo you might want to revisit the code and see if the assumptions are actually being held. It sounds like you might want to play with an object pool like Stormpot or Vibur.\n. Oh, the awful Session-per-request pattern Hibernate liked to promote? Shudder.\n. I've done my fair share of migrating legacy systems. I've always been surprised how hostile most developers are towards services, events, etc. Too many war stories of truly awful home grown frameworks...\nRight now I'm finishing a rewrite of Django/Celery to Java, as small enough to make the change before its too late. I've never had auto-clustered frameworks work out well in practice, so using a stack I've had success with in the past. That's jsonschema2pojo, JAX-RS services, jOOQ, retrofit service clients (returning CompletableFuture), Guice, and RabbitMQ (for events & work queues). I'm hoping to setup Kubernetes to use DNS for service discovery by end-of-year, but for now its calls to localhost given the priorities. Surprisingly fewer LOC than Python, while much faster and easier to develop on. I generally prefer using stacks that are easy to migrate away from, so http + dns + json is my preferred starting point. Then its easy to switch frameworks or, if scaled, revisit the protocol.\n. Closing since the request is submitted and we're waiting for @elyograg to tackle it.\n. This is expected because the eviction policy is probabilistic, not deterministic.\nThe exact cause is due to using a random seed in the sketch's hash function to help protect against HashDoS attacks. However that isn't strong enough when adversaries use keys with the same hash code, so there is a small amount of random admittance in the policy when a potential attack is detected. See the design document for the details on the HashDoS protection.\nIt is very likely that the policy jitter negates the need for the random seed in the hash function. I hadn't found a good reason to remove it, so I left it as a cheap security measure. I ran into the same spurious test failures though and hacked it to be a strict value in my data provider (see RandomSeedEnforcer.ensureRandomSeed(cache)). When that is applied your test passes.\nCaffeine's policy uses a CountMinSketch to estimate the frequency of entries. This is used as a filter to avoid polluting the cache with infrequently used entries (e.g. \"one-hit wonders\"). Due to the small sketch size and random seed, sometimes the collisions cause key1 to have a higher estimated frequency than key2, so key2 is evicted. In larger caches this would go through an \"admission window\" to allow key2 to build up its frequency to avoid consecutive misses. That's not possible due to the cache only being able to hold a single entry. The takeaway is that the eviction policy tries to exploit that a cache is inherently probabilistic, so it takes advantage of time & space efficient sketches to maximize the hit rate.\nOn a side note, a pending enhancement that improves the hit rate (for small recency-skewed caches) and could remove the need for the HashDoS schemes is an adaptive window. This idea is implemented in the simulator but I haven't had the time to integrate it in yet. When added we could safely remove any randomness from the cache, though it would still be probabilistic so collisions on the key hash codes could cause similar spurious test failures if using random input.\n. Let's close. I think once I get the time to wrap up the adaptive W-TinyLFU enhancement then we should revisit the randomness to help in testability.\nOn a side note I was meaning to ask if you'd be interested in capturing access traces. We could then run it through the simulator to see how the eviction policy behaves with Druid's workloads and compares against other policies (e.g. LRU, OPT). It may offer insight for how to tune the cache size, though given the data volume it might not. I suspect your workloads would be similar to the database trace.\n. A logger should have low overhead and you can use an async appender if concerned. The format can drop all the metadata and be just the key message (one per line). The key's hashCode() should be fine. Ideally we'd have a 64-bit hash, but I don't think its necessary. Then xz compressing the results to make it very compact. The simulator consumes long keys, has no notion of values, and handles maximum size (no weights) only. I can then give simulate ~8 sizes and chart the results so we can see the curves, and you can do the mental math to line it up with the weighted size you use.\n. good idea, thanks!\n. Good eye! \ud83d\ude00\n. You're right. It was meant to be in the other block, like get. That's probably a little wrong too because the value could be null but not an error, indicating that the result was not computable. So both places should be checking the exception, thanks!\n. Thanks! This looks very nice.\nI hope you don't mind that I amended your commit to make a few random clean-ups. I wanted your name displayed as the latest commit =)\nP.S. I think you need to fix your git config as github is displaying your name, but not linking to your profile.\n. Atomix looks interesting and perhaps could be retrofitted as a cache. They prefer consistency over availability, whereas the latter is more common in the various distributed caching projects.\n. Unfortunately I might not be able to help much since I haven't used Tomcat or Spring in a long time. \nI guess the first step is verifying that the class is in your WAR, due to the underlying ClassNotFoundException. Since you also received an AccessControlException: access denied (\"java.io.FilePermission\" when trying to load it, you might have to check your SecurityManager restrictions. From what I can gather Googling, you might need to add the jar to the deployment assembly.\n. Glad to hear its working. It might be a quirk due to Tomcat trying to isolate multiple webapps in their own classloaders and ForkJoinPool being shared across the JVM. Some weird interaction that I don't get since I typically embed jetty instead.\n. I know its a rare but valid request. Hydra forked CLHM to add that selectivity to manage transaction leases. Its definitely a complex scenario and I'm not sure if there are any satisfactory solutions.\nMy concern with that type of API is that it feels fragile. An O(1) eviction could degrade to O(n) and possibly not be able to evict. Or it could \"advise\" not to evict but fallback to do so anyways (e.g. Ehcache's EvictionAdvisor) which could give a false sense of correctness in unit tests.\nAs you said, using a weight of zero is a possible option. The weight is also calculated on an update, so a reference count could be managed by an asMap().computeXXX block with an intelligent weigher. A quirk here is that weighing occurs outside of the atomic operation to avoid unnecessary lock hold times (e.g. a sizeOf weigher). In that case you'd need to be careful to avoid a race condition of the reference count changing from concurrent calls, causing the entry to be incorrectly weighed.\nA perhaps cleaner option is to use a victim cache. If the entry was removed prematurely then it could be put into a secondary, unbounded map. The cache loader could query this map to resurrect the entry, and fallback to computing it otherwise. I suspect that would be the simpler and more obvious code than trying to manipulate the weight, and avoids the addition of a fragile API.\n. If the entries are reference counted then could you use weak references for the victim cache? Otherwise you'd have to determine a more explicit way to evict from it when the reference count becomes zero.\nA lot of questions are difficult to distill into the wiki sections. Perhaps a FAQ would work well and you're welcome to try and flush that out from what you thought was useful know-how.\n. Oh, and you could also reinsert asynchronously when improperly evicted. Then the victim is almost always empty and you retain a stricter bound. \n. The eviction policy is TinyLFU. We don't want to provide a custom size-based policy because that's error prone and very unlikely able to make a significantly wiser decision than these types of heuristics.\nIf the creation time is from the perspective of the cache, then you are describing a FIFO policy. Otherwise the cache would have to evaluate the entry to get a priority, resulting in an O(lg n) insertion time. Currently we use only O(1) algorithms.\nDoesexpireAfterWrite handle your use-case? This tells the cache to evict entries beyond a certain age, e.g. due to the data being stale.\nIf you need variable per-entry expiration then this is where things get trickier for inclusion. There are good common solutions for your code, but not for the library. I have an idea for a nice approach (hash wheel timers), but haven't experimented much yet.\n. FIFO is time of creation, LRU is time of last access. Those are of course logical clocks, not wall clock time. The expireAfterWrite and expireAfterAccess are the wall-clock versions.\nI'm still confused if your actually want your weights to be based on wall-clock time or if your weights are logical. The former would be expiration, where a per-entry might be used for HTTP GET caching. But that is about data freshness, which doesn't seem like what you are trying to describe.\nIn either case per-entry weights often leads to either O(lg n) priority queues or random sampling find a good enough candidate. Can you describe your use-case?\n. I see, so you want a size-based policy that uses the createdDate to prioritize. Here time is an implementation detail, and has nothing to do with expiration.\nDo you really think this policy would provide a higher hit rate than using frequency and recency? The current approach is near optimal across many workloads. I think you should validate that your design ideas, if you haven't already.\nThis would be O(lg n) on insert or require random sampling on eviction to find the victim. There are libraries that provide those, but neither work wonderfully. I don't think making the strategy is useful for most users, I'm doubtful its beneficial for you, and the current one does extremely well.\nIf you'd like to capture an access trace we can easily simulate your policy against others. Then we'll have data to backup our opinions.\n. You can read the HighScalability article which provides an overview. The latter half is similar to Guava, too.\nCaching is very much about non-deterministic behavior and using probabilities to predict what is most likely best to retain given the limited space. Your case might just be too far out of the norm for what most expect from a cache. The more deterministic behavior you require, the less your scenario fits a traditional cache.\nI don't think you'll find exactly what you want with an off the shelf library. You'd probably be best served with a custom implementation.\n. Closing, as I don't plan on making the policy pluggable.\n. Since most caches are used in 1-3 places within a component, which guards access and masks the implementation details, this seems pretty easy to do in the calling code. This isn't too commonly needed, won't be a great simplification to be performed implicitly within the cache, and is fairly obvious logic. I'm hesitant to add it explicitly to the Caffeine builder because the power-to-weight ratio doesn't seem to be in its favor.\nDo you think your code would be greatly simplified by having this implicitly done, vs at the call sites? Being explicit has the benefit of only paying the cost when you need it, e.g. internal cache calls may not care while handing out the cached value might have an explicit copy. I believe Ehcache 2.x copied by Java serialization, making it a pretty expensive.\nAn alternative is to use the JSR-107 adapter which includes a pluggable Copier. The specification has a store-by-value requirement, primarily for serialization for distributed caches. If you're using Spring Cache then its changing the configuration from its native Caffeine support to its JCache support.\n. Unless there is additional feedback, I am planning on closing this issue. I don't think this would introduced into the API. But happy to leave open if there is opposition favoring a reconsideration.. If you are using the JCache module then it should be on every read. I think the original ask was for a similar feature in Caffeine's core, which isn't planned.\nThe read value should be passed through the Copier before being returned. Can you set a breakpoint in CacheProxy at, \nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/jcache/src/main/java/com/github/benmanes/caffeine/jcache/CacheProxy.java#L179\nIf you provide a full test case then I can help debug.. That probably uses the invoke() api and perhaps there is a missing copy call in the caffeine jcache adapter.. I think it\u2019s a missing copy on my side. See invoke\u2019s return,\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/jcache/src/main/java/com/github/benmanes/caffeine/jcache/CacheProxy.java#L794. It looks like the reason is because the return of Cache#invoke is user-defined, rather than the entry's value. So it isn't clear if a copy should be expected, since it might be a user-defined return of some computation rather than the entry's current value. I think its probably okay to add the copy here, but also a bit confusing.\n/**\n  * Invokes an {@link EntryProcessor} against the {@link Entry} specified by\n  * the provided key. \n  *\n  * @param <T>            the type of the return value\n  * @param key            the key to the entry\n  * @param entryProcessor the {@link EntryProcessor} to invoke\n  * @param arguments      additional arguments to pass to the\n  *                       {@link EntryProcessor}\n  * @return the result of the processing, if any, defined by the\n  *         {@link EntryProcessor} implementation\n  */\nThe entry processor mutates the given entry directly, so it is an in/out parameter.\n```java\npublic interface EntryProcessor {\n/\n   * Process an entry.\n   \n   * @param entry     the entry\n   * @param arguments a number of arguments to the process.\n   * @return the user-defined result of the processing, if any.\n   * @throws EntryProcessorException if there is a failure in entry processing.\n   /\n  T process(MutableEntry entry, Object... arguments)\n      throws EntryProcessorException;\n}\n```\nWould you expect the the user-defined result of the processing, if any. to be copied? Or perhaps the copy needs to be the given to the EntryProcessor and that is the mistake. I'd have to dig in a little deeper into how Spring uses it.. Cache#asMap() provides access to perform compute, computeIfAbsent, computeIfPresent, and merge. That opts out of automatically calling an attached CacheLoader or CacheWriter, which can be called manually inside of your atomic block.\njava\ncache.asMap().computeIfPresent(12, (key, list) ->\n  ImmutableList.copyOf(Iterables.concat(list, ImmutableList.of(5)))\n);\nNote that we have to be careful of concurrent reads accessing the value. Your options are either to make a new value (like above) or synchronize on the value prior to reading (e.g. Collections#synchronizedList). An update doesn't preclude you from having to consider thread safety from the reader's perspective.\n. Closing as I think using asMap is a nice solution. Feel free to reopen if you find difficulties where a specialized method would be beneficial.\n. Yes, the JDK bug would be my first guess too. I would also suggest to be on my latest release which tries to be a little more resistant if it detects the problem. I'm sorry to hear you're running into this and hopefully an upgrade will resolve it.\n. I've been unable to reproduce a memory leak when experimenting with different configurations using Stresser, v2.3.1 and 1.8.0_92. Locally I'm also printing out the access/write order deque sizes (used for size/expiration ordering), in case there was a dangling reference there. Other than the deques and the hash table there shouldn't be any other reference holders. Since the metrics are correct and the GC stays stable under a write load, I'm hopeful that upgrading will resolve your problems.\n. That makes perfect sense. I was concerned that the entry was dead but not removed, and forgot about that classic user error. For eviction we know the entry via a linked list and then use the key to remove it from the table. Because the hash map isn't under an exclusive lock, if a remove(key) fails we assume we raced with an explicit user operation (e.g. cache.invalidate(key)). Some hash tables might be able to detect this type of corruption, but at the cost of performance. Since this is usually a novice mistake that the Java Collection Framework doesn't defend against, we don't either.\nI've been meaning to look and see if we should null out the value after marking the node as dead. It would have masked the memory leak for a lot longer, as your heavy-weight Xt3 value wouldn't have been retained but the nodes would keep accumulating. Do you think it is friendlier to fail faster due to the leak or slower such that it may not be easily detected?\nClosing as the issue is solved, but I am interested in ways to help minimize the pain.\n. The internal lists would no longer have a deleted node, only the hash table holds it. That means currently we could only detect it on a clear() or asMap() iteration, which users rarely call and we have no other need to. So short of a periodic consistency check, there is no easy hook. Unfortunately proper eager detection would require directly embedding into the hash table to assert that the entry pointers are null when we mark the node dead. That's a clean solution, but a large undertaking.\n. Caffeine's cache is effectively a ConcurrentMap with fancy features so, as long as you are mindful of the concurrency scenarios, it should be easy to build your own shutdown logic on top.\nThe removal listener will be executed asynchronously on any removal. I'd expect that on shutdown you reject subsequent additions, clear the cache, and wait for the listener to process all of the removals. I don't think either (1) or (2) are constraints. I'd expect your shutdown request to block (with a timeout) waiting for all of its internal services to have halted for a clean shutdown (e.g. see Guava's ServiceManager).\nIt would definitely be easiest if using the executor's shutdown and awaitTermination methods. That unfortunately is ignored by ForkJoinPool#commonPool() due to being shared JVM-wide, so in that strategy a custom executor would be required (daemon allowed). Alternatively you could coordinate it using other java.util.concurrent utilities, such as a Semaphore or Phaser. If you use a CacheWriter instead, then delete is synchronous to the calling thread, so that all entries removed by invalidateAll would have notified the writer before returning.\n. You wouldn't print completed shutdown until the executor has terminated. I won't get a chance to play with your code until after work, but will take a look this evening to provide an example.\n. See this small update to your test case which prints the following and exits.\nRemoved 9 and cleaned up.\nRemoved 1 and cleaned up.\nRemoved 0 and cleaned up.\nRemoved 3 and cleaned up.\nRemoved 2 and cleaned up.\nRemoved 6 and cleaned up.\nRemoved 7 and cleaned up.\nRemoved 5 and cleaned up.\nRemoved 4 and cleaned up.\nRemoved 8 and cleaned up.\nClean shutdown\n. Here's another slight variation that removes the need for a dedicated executor. It tracks the number of submitted and completed tasks, and allows for waiting until there are no more in-flight. The decorating ExecutorService could be flushed out more if needing to implement the full contract, but its purpose here is just to remove the need for extra threads.\n. I think you are taking advantage of Guava's implementation details rather than an intended API. Guava performs all work on calling threads, populating a notification queue that callers can simultaneously drain. Guava's behavior can be emulated with a same-thread executor, though we don't have a notification queue since the executor tends to take care of that. The design difference stems from commonPool now being available and a no-brainer to take advantage of it.\nThe executor is a convenient way to achieve your results, but not the only approach. You could use a CacheWriter to perform the close() synchronously with removal. Perhaps the simplest scheme is to iterate (via cache.asMap()) the entries to explicitly close() them when shutting down.\nThe cache doesn't have a lifecycle concept and I don't think introducing one is necessarily beneficial for you use-case. It is a data structure that is malleable by the application, rather than a framework to bridge life-cycles with.\n. Unfortunately that was removed as an unused feature. But you can easy trace using a standard logger, appending to dedicated file, and reducing the format (e.g. just the message). The simulator works using long keys and no values, so a log of key hashes should work. You generally only need to log around a get. We can then either reuse a trace reader or add a new one.\nThe only scenario (that I know of) where it might underperform is recency-skewed distributions with a small cache. In that case the cache is too tiny to leverage frequency, the policy's frequency bias works against it, and the \"admission window\" isn't large enough to correct for that. In my experiments an adaptive window resolve this, but without a user report I hadn't gotten around integrating the optimization.\n. Thanks! Did you already integrate this into the simulator or should I go ahead and do that?\n. Perfect! If we decide to check-in the trace then we should use xz which reduces the file from 7.1mb to 3.1mb.\n. The results are interesting.\n| Policy | Hit rate |\n| --- | --- |\n| ARC | 76.09 |\n| Lirs | 74.30 |\n| Lfu | 73.95 |\n| Lru | 75.91 |\n| Lru_TinyLfu | 67.02 |\n| Clairvoyant | 79.25 |\n| Unbounded | 79.25 |\n| WindowTinyLfu | 71.57 |\n| AdaptiveWindowTinyLfu | 72.08 |\nIt seems that frequency is a negative signal in this workload. That's surprising given how large this cache is, as usually recency has little effect past a short duration and then frequency plays a major role. The adaptive window has a maximum, so the gain is slight. The optimal window size is 100%, meaning LRU is the best we could achieve. None of the policies show an ability to improve upon LRU, which seems to be the optimum.\nIf we decrease the cache's size to 5k or 10k then frequency has a positive impact, increasing from 41.20 (LRU) to 46.54 (W-TinyLFU) to 47.52 (adaptive). But at a larger size its a disadvantage.\n. Not presently, as ideally users shouldn't need to worry about the implementation details. I'm hesitant to expose a setting.\nI think our goal is to have a policy that the best (or very close) in all workloads. So being 3-4% worse here, while 20-30% better in other traces, is an okay trade-off.\nI don't have any good ideas, yet, for how to optimize for this peculiar trace. It would be nice to shrink that gap.\n. Do you have a prior filtering scheme before entries go into the cache? e.g. ElasticSearch uses an LRU but only allows candidates that have passed a heuristic. The intent seems to be to only let frequently used entries arrive at the cache.\nAt that point, perhaps, frequency no longer provides a strong signal so only recency is beneficial. The hope of a smarter policy like TinyLFU is that those heuristics could be removed.\n. \nraw data\n. @gilga1983 The window is 1% and is not configurable. Hopefully we can find improvements and not need to expose the policy's implementation details to users. That gives us the most flexibility to make changes.\n. @phraktle Can you acquire a longer trace?\nAn interesting aspect is that your trace length is ~10x the cache size, which is also the sample period for TinyLFU before it resets. If perform 3 runs of the trace in sequence, then I've artificially created frequency and W-TinyLFU outperforms LRU. But reality is that new items would probably show up. A trace that is 100x or more than the cache size might be helpful.\nMy hope is that frequency does play a role long term and we can argue this is poor warm-up, but over a long period it evens out to be a gain.\n. To highlight why I'm asking, if I run the trace 6 times (7.5M requests) in sequence @ 100k,\n| Policy | Hit rate |\n| --- | --- |\n| ARC | 76.96 |\n| Lirs | 76.73 |\n| Lru | 77.23 |\n| Lru_TinyLfu | 75.59 |\n| Clairvoyant | 85.94 |\n| Caffeine | 82.48 |\n| WindowTinyLfu | 80.47 |\nSo in this artificial run, TinyLFU is able to extract frequency and the window handles recency. Caffeine differs from the theoretical algorithm, e.g. adds HashDoS protection, which in this case is visible (but usually differs as noise).\n. That's great, thanks! My hypothesis didn't hold but the difference are smaller.\n| recs | @25k | @50k | @75k | @100k |\n| --- | --- | --- | --- | --- |\n| Lru | 67.71 | 75.49 | 79.42 | 81.77 |\n| Caffeine | 69.03 | 74.63 | 77.51 | 79.38 |\n| Clairvoyant | 80.55 | 85.06 | 86.98 | 88.03 |\n| pods | @25k | @50k | @75k | @100k |\n| --- | --- | --- | --- | --- |\n| Lru | 70.54 | 78.85 | 83.25 | 86.12 |\n| Caffeine | 75.66 | 81.65 | 84.75 | 86.75 |\n| Clairvoyant | 83.13 | 88.44 | 91.00 | 92.52 |\n. This is all about the dynamic between recency and frequency. Some policies are recency skewed and try to compensate for frequency workloads (cache2k w/ clockpro). Other policies are frequency skewed and try to compensate for recency workloads (caffeine w/ tinylfu). \nFor small caches under a recency-skewed workload, a frequency-biased policy is hit the hardest. Fortunately increasing the cache size isn't too detrimental, so the cost is small in these worst cases. Unfortunately recency-biased policies under perform for large traces, which are more typically frequency-biased, where it may be less affordable to increase the cache size. Those cases include analytical, search, and database applications which are already memory intense. When analyzing those traces we see Caffeine significantly outperform Cache2k.\nThe goal then is for each algorithm to try and compensate for its weaknesses, and strike an overall balance that best optimizes system performance. For TinyLFU this weakness was discussed above and is described in the research paper. The best way to compensate, dynamically, is still open for debate.\n * Bloomfilter feedback policy adjusts the admission window based on mispredictions. This corrects for small traces, allowing it to slightly outperform cache2k in orm-busy.trace. But the feedback fades away as the cache size grows, by assuming large caches are frequency-skewed, which is not the case in scarab traces. \n * Hill climbing policies are still very much a prototype and not yet finished. The idea here is to detect the curve for different admission window sizes, climb to the optimal value, and restart when the workload changes. There are a few variants of this idea and it should work on any cache size, but has not yet been fully flushed out.\n * Adjusting the counter increments is another approach. For example using logarithmic counters with an admission boost, e.g. see Redis LFU. This would give new arrivals a higher chance of admission, so perhaps dynamically adjusting the boost size would compensate.\n * Researchers at the Technion are looking at an LRFU inspired admission filter. This would mean the filter itself takes into account recency and frequency when making the admission choice.\nSo just like with sorting algorithms, each cache policy is best in certain circumstances. The more advanced sorts are in fact hybrids by adapting to a scheme based on heuristics. They try to be the most well-rounded, though perhaps not optimal in every scenario. We're trying to do the same with caching, where more experimentation is needed to discover the best heuristics.. Simulated Annealing, a hill climbing algorithm, shows good results. In the recs trace it corrects for recency and adapts to match LRU's hit rate. The pods trace shows only slight differences, since it was already friendly to W-TinyLFU. Similar improvements are observed in other recency skewed traces, such as the orm-busy trace mentioned earlier.\nI think there is still some fine tuning left, which I hope @gilga1983 can help out on. Once we're satisfied then adding the improvements to Caffeine should be cheap and straightforward. Thanks for the patience @phraktle.\n| recs | @25k | @50k | @75k | @100k |\n| --- | --- | --- | --- | --- |\n| Lru | 67.71 | 75.49 | 79.42 | 81.77 |\n| WindowTinyLfu | 68.83 | 73.76 | 75.07 | 77.52\n| AnnealingWindowTinyLfu | 69.41 | 76.20 | 79.28 | 81.54\n| Clairvoyant | 80.55 | 85.06 | 86.98 | 88.03 |\n| pods | @25k | @50k | @75k | @100k |\n| --- | --- | --- | --- | --- |\n| Lru | 70.54 | 78.85 | 83.25 | 86.12 |\n| WindowTinyLfu | 75.70 | 81.46 | 83.71 | 86.04\n| AnnealingWindowTinyLfu | 74.49 | 81.69 | 84.78 | 87.19\n| Clairvoyant | 83.13 | 88.44 | 91.00 | 92.52 |\n. The OLTP and Financial traces are the transaction logs, which mean they are record/replay of commits which has a low impact on performance. It is not the critical database page cache, which may include scans, et al. The ORM traces behave similar to what one would expect from a Hibernate / JPA cache with frequent invalidation and flushes. That too is not similar to many webapp usages which try to avoid db calls entirely, though I agree its a more practical example. I'd have to dig in and compare again, but off-hand I know cache2k had less than 1% on the glimpse trace. All of this was discussed in private email so it is not new revelations. Therefore, please refrain from comments that appear to be evangelical, but otherwise I'm glad to have your input.\nGil plans on pushing some new work of his soon which looks promising. Instead of adapting the window, he adjusts the reset interval. This ages faster (recency) or slower (frequency) and resides within the TinyLFU filter. We'll have to run more extensive tests to evaluate its robustness. It may be the ticket or we might merge other ideas previously discussed.. Thanks. @erikvanoosten. I haven't simulated LHD yet and their published code is not runnable. My experience is that sampling policies underperform due to the inability to capture history and cannot correct if they make a mistake. At best they tend to match LRU or LFU, but never exceed it. Policies like ARC, LIRS, and TinyLFU can greatly out perform classic policies.\n@ohadeytan and team have been working on adaptive schemes. The work so far has been quite impressive and seems to fix the problem, cheaply. That work is under review and he can speak about it, if interested. After its public I'd like to work with him on integrating the improvements into Caffeine.. FYI, the research paper with our strategy to fix this automatically is now freely available if you follow the README's link (see Adaptive Software Cache Management). This uses ACM's Authorizer service to let you bypass the paywall.\nI would like to explore adaptive moment estimation (adam) as an enhancement to the naive hill climber shown in the paper. The paper demonstrates that the techniques work, but leaves deeper optimizations to the community. In the case of hill climbing (aka gradient descent), the ML community has very good work on improvements. I am hoping to find some time over the holidays to implement their ideas.\nThe time/space overhead to the cache was shown to be minimal and the code complexity is low. After I have satisfied myself on the exact algorithm to adopt, I hope to find the time to implement it. This should then correct for this problem without requiring a user-visible configuration.. After experimenting with gradient descent (Adam), I've come to the conclusion that it isn't a good fit because I do not know how to provide an instructive gradient equation. The equation is to minimize the \u0394(miss rate), which can lead to converging at a low plateau. If the initial configuration is a very poor fit and the first step moves in the wrong direction, the hit rate might only change by 0.02%. Then it converges immediately, which is not what we want. The usual solution to this is random restarts at other configurations, which adds overhead to shift dramatically, and doesn't seem worth it. Alternately giving it a few fake gradients to start with works, but that feels error prone and might break down in practice.\nIn comparison, hill climbing doesn't get trapped and decaying to convergence seems more robust. It lacks some of the advanced features (momentum), but doesn't seem to need them yet. If we later obtain traces where hill climbing fails then we can add various improvements.\nSee this sheet for examples.\nNow that I have conclusions on the different approaches, I'm ready to start doing the development in the library.. There are papers that try to do this by running multiple policies at once, but are wasteful. A straight Lru or Lfu are not peak performers as they don\u2019t retain history outside of the working set, which has proven to be very valuable at improving hit rates.\nA better case is MiniSim, which samples requests to model a cache at different sizes. However sampling requests loses recency/frequency information to indicate policy choices, so its not appropriate when adapting towards that goal.\nIn a policy that is split for recency & frequency, sampling to see which portion recieved more hits is unbalanced. When one portion is larger than the other then it skews the probability, even if divided by the region's size.\nInstead we sample to compare if a configuration change had a positive impact. That\u2019s very straightforward and climbing works well. I am trying to steal bits of time to implement it for a release soon, I hope.. @phraktle I have an early branch with the adaptivity working - very simple to implement. I need to amortize the adaption, write tests, etc but it works. Sorry I haven't had any time, trying to steal some where I can but hard.. The results from a mixed workload is pretty neat. At 512 maximum, Caffeine starts with the 1% window running Corda's small trace, then runs Lirs' loop trace, and then back to Corda's. That case might occur due nightly batch jobs run, shifting the request characteristics away from normal users. We see the adaption kick in and reconfigure itself to the new environment.\n| Policy | Hit rate |\n| --- | --- |\n| Optimal | 44.53 % |\n| Arc | 11.64 % |\n| Lirs | 32.49 % |\n| Lru | 11.64 % |\n| Caffeine 2.6 | 22.81 % |\n| Caffeine 2.7 | 41.31 % |\n| Cache2k | 0.38 % |\n(Included cache2k as maybe our ideas can be applied to improve its use of ClockPro)\n\n. and we've merged! \ud83d\ude3a . Released 2.7. Thanks!\n. Thanks!\n. Thanks! I hadn't planned to start tackling JDK9 until its released, so I put this off. But its a good change.\n. Not yet. I planned on switching to VarHandles when that was released. The AtomicFieldUpdater classes had performance problems, which I think were fixed more recently.\n. Sorry I'm a little slow. Tuesdays are a bit hectic for me, so harder to steal a moment.\nDisabling JavaDoc was a good idea, since few would use it directly. The few dependency-based users that I know of would prefer digging into the code anyway.\n. I installed build 130 and checked out your branch. We should also delete SynchronizedBenchmark which plays with the removed UNSAFE.monitorEnter. Fails for a gradle build but JMH isn't compiled as part of an assemble.\n. Seems that Jacoco fails for tests, as reported in the Gradle forum. Upgrading to 0.7.8-SNAPSHOT results in,\n```\njava.util.ServiceConfigurationError: sun.util.locale.provider.LocaleDataMetaInfo: \nProvider sun.util.resources.provider.NonBaseLocaleDataMetaInfo could not be instantiated\nCaused by:\njava.lang.NoClassDefFoundError: Could not initialize class \nsun.util.resources.provider.NonBaseLocaleDataMetaInfo\n```\nSo we'd probably need to disable code coverage too.\n. Oh, when I ran with --no-daemon it passed and had a test failure\ncom.github.benmanes.caffeine.OSGiTest > sanity FAILED\n    java.lang.IllegalStateException: Stream handler unavailable due to: null\nReported as FELIX-5322.\njava.lang.IllegalStateException: Stream handler unavailable due to: null\n    at org.apache.felix.framework.URLHandlersStreamHandlerProxy.openConnection(URLHandlersStreamHandlerProxy.java:311)\n    at java.net.URL.openConnection(java.base@9-ea/URL.java:1049)\n    at org.apache.felix.framework.cache.JarRevision.initialize(JarRevision.java:150)\n    at org.apache.felix.framework.cache.JarRevision.<init>(JarRevision.java:77)\n    at org.apache.felix.framework.cache.BundleArchive.createRevisionFromLocation(BundleArchive.java:878)\n    at org.apache.felix.framework.cache.BundleArchive.reviseInternal(BundleArchive.java:550)\n    at org.apache.felix.framework.cache.BundleArchive.<init>(BundleArchive.java:153)\n    at org.apache.felix.framework.cache.BundleCache.create(BundleCache.java:277)\n    at org.apache.felix.framework.Felix.installBundle(Felix.java:2971)\n    at org.apache.felix.framework.BundleContextImpl.installBundle(BundleContextImpl.java:167)\n    at org.apache.felix.framework.BundleContextImpl.installBundle(BundleContextImpl.java:140)\n    at org.ops4j.pax.exam.nat.internal.NativeTestContainer.installAndStartBundles(NativeTestContainer.java:340)\n    at org.ops4j.pax.exam.nat.internal.NativeTestContainer.start(NativeTestContainer.java:209)\n    at org.ops4j.pax.exam.spi.reactors.AllConfinedStagedReactor.invoke(AllConfinedStagedReactor.java:79)\n    at org.ops4j.pax.exam.junit.impl.ProbeRunner$2.evaluate(ProbeRunner.java:267)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n    at org.ops4j.pax.exam.junit.impl.ProbeRunner.run(ProbeRunner.java:98)\n    at org.ops4j.pax.exam.junit.PaxExam.run(PaxExam.java:93)\n. and then we skip that to have mockito bomb\n. Seems the mockito issue was a fluke caused by Gradle retaining stale artifacts in its directories. The JDK9 build passed for me with the local changes and running,\n$ gradle test -x :caffeine:osgiTests -x :guava:test -x testCompatibilityKit\nThe first two were due to OSGi, which is isolated in :caffeine to avoid test framework clashes. The latter is because the TCK uses JmxMBeanServer which is no longer available.\n. MBeanServerBuilder resolves the need for JmxMBeanServer. That leaves only the OSGi tests.\n. I have the primary test suite passing now.\nThe slow tests (slowGuavaTest and slowCaffeineTest) fails due to soft-references. It seems that JDK9 does not honor -XX:SoftRefLRUPolicyMSPerMB=0 which is supposed to make soft references behave as weak references (predictably GC'd). This causes the ReferenceTest to fail.\n. Seems the guava tests get hung on Travis but not locally. No idea why.\n. Merged! There is some remaining issues we need to investigate. It would also be nice to have both JDK8 and JDK9 running.\n. The intent is of course to reduce the in-memory overhead due to the many configuration options at the cost of larger disk footprint (unloaded classes).\nI think the large enum used by NodeFactory could be removed. The simplest would be to getFactory compute a numeric code, and return a new factory instance whose methods switch on that code. This would remove the bloat for a tiny runtime cost of selection.\nLocalCacheFactory has a string switch statement. Using a numeric code would be slightly smaller by reducing the interned strings.\nI tried to use inheritance to reduce duplicate code. I think Node was optimal and LocalCache was close, but not perfect. Perhaps there are some more tricks to optimize reuse, e.g. interfaces with default methods. We could also remove the modifiers, as perhaps the JVM is creating unnecessary bridge methods.\nThe biggest win would be to review LocalCacheFactory features to determine if they carry their weight. Perhaps that factory should be removed and all the fields present, but null'd if unused. The top-level cost is smaller than per-node, which was my original intent for using codegen.\n. For variable expiration, I decided to avoid the extra codegen and encoded it onto the existing timestamp fields. Since it cannot be used with fixed expiration (expireAfterAccess or expireAfterWrite), but can be used with refresh (refreshAfterWrite), this was a bit of a dance. But the minimum number of fields and no new classes was the result, with just a few new methods.\nThe enum in NodeFactory appears to account for 100kb compressed. That's determined by generating with just a single enum and comparing the jar sizes. That enum could be replaced by a factory (strong keys and subclassed for weak). If we use MethodHandles, we should avoid most of the cost of reflection and near the raw byte code.\nThe unit tests run against every combination allowed by the @CacheSpec annotation. That helps to ensure that a change doesn't break a forgotten configuration by brute force. Any codegen size change can have some assurances that works if the CI passes.. As advertised, the overhead of method handles is near noise compared to a direct call. So I think we can switch to that for the enum and any other reflective calls. The difference is that a direct call has a much more stable throughput, whereas the handle varies more per iteration making the JIT sensitive. Since the difference is 1-2ns and only on an insertion, its a nice win.\nBenchmark                                   Mode  Cnt          Score          Error  Units\nFactoryBenchmark.direct                    thrpt   10   94865111.969 \u00b1  5923818.011  ops/s\nFactoryBenchmark.methodHandle_invoke       thrpt   10  102297433.106 \u00b1 29351617.853  ops/s\nFactoryBenchmark.methodHandle_invokeExact  thrpt   10  102624426.326 \u00b1 28142556.289  ops/s\nFactoryBenchmark.reflectionFactory         thrpt   10   41791855.873 \u00b1 25826017.256  ops/s\nThe subset the code of interest is,\n```java\nstatic final class MethodHandleFactory {\n   private static final MethodHandles.Lookup LOOKUP = MethodHandles.lookup();\n   private static final MethodType METHOD_TYPE = MethodType.methodType(void.class, int.class);\nprivate final MethodHandle methodHandle;\nMethodHandleFactory() {\n     try {\n       methodHandle = LOOKUP.findConstructor(Alpha.class, METHOD_TYPE);\n     } catch (NoSuchMethodException | IllegalAccessException e) {\n       throw new RuntimeException(e);\n     }\n   }\nAlpha invoke(int x) {\n     try {\n       return (Alpha) methodHandle.invoke(x);\n     } catch (Throwable e) {\n       throw new RuntimeException(e);\n     }\n   }\nAlpha invokeExact(int x) {\n     try {\n       return (Alpha) methodHandle.invokeExact(x);\n     } catch (Throwable e) {\n       throw new RuntimeException(e);\n     }\n   }\n }\n```. Okay, let me take a look to understand what you did. :)\nI was playing around with removing the enum in NodeFactory with method handles. I had a partial prototype and am fine dropping it. Please also take a look to see if any of that is useful with your technique.\nhttps://github.com/ben-manes/caffeine/tree/nodefactory. I think we can close this thanks to @jvassev shaving 300kb off the jar. Do you agree @jvassev or are there more ideas to play with?. Thanks @jvassev! We're at 648K now.\nIs everyone satisfied or do we want to leave this open for future iterations?. Yeah, runtime codegen might be a bit too much magic. Its nice to know a lower bound, so thanks for that insight.\nThe only idea I'd have is to review the LocalCache subtypes for which pull their weight. For example, is special casing removalListener a good tradeoff? It doubles the number of classes for a single field. I don't have a strong opinion as the decisions are a bit arbitrary.. Also, a note to myself, We should add a little JavaDoc to each generated class with the decoded configuration. Then debugging with the source jar (e.g. IDE) will be easier.. Added the JavaDoc so that anyone reading the types don't need to decode the names, but it states the generated and inherited features. Should make it a tiny bit easier for a user to debug.\nClosing, thanks all the help.. Yes, the cache might have been overkill. I wrote the generator and iterated as features were implemented. There are a lot of fields only used for particular configurations, which could be null. We could drop that codegen at the small cost of many unused fields, which is probably okay.. I think its worth giving it a try, if you are up for it. It would be a worthwhile experiment worst case. In this case I think we'd end up removing all of the LocalCacheFactory code generation, right? So we'd see how many unused fields are introduced as the trade-off, and discuss which is the better tradeoff. I don't have a strong opinion yet.. Released. Thanks @jvassev!. I'm sorry to hear this. I ran your test and see the same surprising behavior. Yet when I debug ReferenceTest#put(), I do see some collection occurring. I'm not sure off-hand what recent change would cause a regression for your test case.\nI'll keep exploring and if we find the cause, try to have a bug fix released by Sunday evening.\n. Sorry, I accidentally debugged the counts with the Guava provider which of course worked. The Caffeine provider passed because the assertion's count isn't restrictive enough, despite the cache not having evicted any entries. As you said, if I manually cleanUp then it discards.\n. I found the problem in BoundedLocalCache#afterWrite().\nThis was incorrectly modified to not call scheduleAfterWrite() if buffersWrites() is false. The surrounding logic was changed to use a bounded write-buffer when investigating what turned out to be the JVM bug Missed submissions in ForkJoinPool. The afterWrite() method had to then handle back-pressure due to a full queue, which typically only happens in synthetic tests. Accidentally the scheduleAfterWrite() was moved into the if-block, whereas in 2.2.7 it is called regardless.\nThe maintenance work should still occur after a number of reads exceeds a threshold. I'd like to shore up the ReferenceTest assertions prior to releasing this fix.\n. Yes, it looks like I introduced it in 2.3.1 only. I'm working on improvements to the unit tests and hope to release this evening, but it is already getting late. I might do so tomorrow.\n. Released. Sorry about any trouble this caused and thanks for understanding.\n. Thanks!\n. You're right and I'll remove that line. I was trying to fix OSGiTest and experimenting with different variations. It fails with the exception,\ncom.github.benmanes.caffeine.jcache.OSGiTest > sanity FAILED\n    java.lang.ClassNotFoundException: com.github.benmanes.caffeine.jcache.spi.CaffeineCachingProvider not found by PAXEXAM-PROBE-9081d622-0b8c-4ce3-b9da-2e04a1f4949b [20]\n        at org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1574)\n        at org.apache.felix.framework.BundleWiringImpl.access$400(BundleWiringImpl.java:79)\n        at org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:2018)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at javax.cache.Caching$CachingProviderRegistry.loadCachingProvider(Caching.java:485)\n        at javax.cache.Caching$CachingProviderRegistry.getCachingProvider(Caching.java:526)\n        at javax.cache.Caching.getCachingProvider(Caching.java:238)\n        at com.github.benmanes.caffeine.jcache.OSGiTest.sanity(OSGiTest.java:56)\nThe problem was due to JCache not supporting OSGi (issues #326). This jar doesn't require or provide service loader functionality, so those manifest entries are unnecessary.\n. I'm not experienced with OSGi, so a PR would be greatly appreciated =)\nMy understanding is that the underlying problem is that JCache doesn't properly export the required packages. The detailed failure from the test is,\norg.osgi.framework.BundleException: Unable to resolve com.github.ben-manes.caffeine.jcache \n[17](R 17.0): missing requirement [com.github.ben-manes.caffeine.jcache [17](R 17.0)] \nosgi.wiring.package; (osgi.wiring.package=javax.cache.configuration) Unresolved requirements: \n[[com.github.ben-manes.caffeine.jcache [17](R 17.0)] osgi.wiring.package; (osgi.wiring.package=javax.cache.configuration)]\nWhere the JCache provider needs javax.cache.configuration which isn't available. I don't know enough OSGi to correct for that and thought it had to be done at the application-level.\n. JCache 1.1 is in preview and should contain this fix. They said end of the preview is the 18th so hopefully the artifact will be out shortly after. Caffeine 2.6 passes the maintenance update's TCK except for backwards incompatible tests, where I'll update to 1.1's behavior when its official.. JCache 1.1 was released and it has a proper OSGi manifest. The unit test passes, too!\nHowever, I have to use test-cache-2 which does not use a CacheLoader. The error reported was that TestCacheLoader cannot be casted to CacheLoader, which could indicate a bundling problem. Since JCache's ClassFactory uses the thread's classloader to resolve, perhaps the dynamic lookup is the issue. Unfortunately my limited knowledge of OSGi prevented me from finding a fix, e.g. by using DynamicImport-Package: *\nThe build should be in the snapshot repository shortly, if you would like to verify.. Hi Jiming,\nYou can modify the expiration times at runtime using Cache.policy().\nThe reason why per-entry expiration times isn't provided is that it is difficult to implement efficiently. The fixed expiration times is performed as O(1) operations by moving nodes in a linked list to maintain order. This works because we know the accessed entry is the least likely to expire, so its pushed to the end of the list. We can then peek at the front knowing that if it hasn't expired, no entries behind it have too.\nWe can't use that trick if an entry's expiration time varies. This leads to a few common approaches which are each problematic.\n- Use a sorted priority queue, O(lg n). This adds a high penalty on every operation to reorder.\n- Use a periodic O(n) sweep. This is expensive and undesirable.\n- Use a timestamp and rely on size eviction. This is O(1) and used by memcached. However, it is easily doable in user code and I'd prefer a solution that is flexible to more configurations. \n- Use a hash timer wheel. This is amortized O(1) and used by OS kernels and network stacks for scheduling timers. However it can degrade to O(n), so it requires care to minimize the penalties.\nOf the choices, I'm prone to using a timer wheel. But that requires experimentation and understanding how implementors have resolved performance issues. I've never seen it used for caches and its not a widely known technique, so contributions exploring this would be helpful.\nFor the time being, I'd recommend using your own timestamp and relying on size eviction. That should be simple to code.\n. That idea is called negative caching, where you use a placeholder to indicate its absence to avoid query storms. If you use your own timestamp, then you can check if the entry is expired and, if so, use cache.remove(key, absent) followed by another cache.get(key) to force a reload. Alternatively if absent is okay, you can use refresh(key) to do it asynchronously.\nI started working on variable expiration over the weekend. That would support your use-case naturally and operates in amortized O(1) time. The timer wheel needs more tests and bug fixes, and it needs to be integrated into the cache. Please take a look at the interface and let me know your thoughts.. 2.5.0 is released and provides variable caching using the timer wheel, as described above. You can enable it using the new Expiry interface, e.g.\njava\nCaffeine.newBuilder()\n    .expireAfter(new Expiry<Key, Graph>() {\n      public long expireAfterCreate(Key key, Graph graph, long currentTime) {\n        return (graph instanceof NullGraph)\n            ? TimeUnit.MINUTES.toNanos(1)\n            : TimeUnit.MINUTES.toNanos(10);\n      }\n      public long expireAfterUpdate(Key key, Graph graph, \n          long currentTime, long currentDuration) {\n        return currentDuration;\n      }\n      public long expireAfterRead(Key key, Graph graph,\n          long currentTime, long currentDuration) {\n        return currentDuration;\n      }\n    })\n    .build(key -> createExpensiveGraph(key));\nNote that the other methods return currentDuration to indicate no change. If you want to disable expiration it would return a large duration, e.g. Long.MAX_VALUE is over 200 years. Some runtime inspection is provided by cache.policy().expireVariably().. There shouldn't be. It is amortized O(1), currently optimized for durations up to 6.5 days (not restricted by that, just not as optimal). If feedback that durations longer than a week is common then we can tune that, but it seemed like a safe bet for what I've seen.\nThe amortized penalty (cascading in a hierarchical timer wheel) is deferred to the executor, which by default is ForkJoinPool.commonPool(). That means even though it is cheap, it is hidden from user-facing requests by borrowing spare cpu cycles in a non-blocking fashion.\nJMH benchmarks showed that the operations were very fast, e.g. at about a dozen nanoseconds to reschedule. Of course the Expiry is user code, but I'd expect that to usually be a cheap operation too.\nThat said, please do let me know if you observe any negative behaviors.. Thanks. I'll take a look this evening during my commute (~2 hours away). Since the annotations are provided by the RI, from your description it sounds like a bug there.\nAs your code shows, @CacheResult is surprising by not being atomic so you have to use a CacheLoader and a stub method. To me that defeats the purpose as the method should be the computation (ideally atomic by wrapping in an EntryProcessor). That was rejected by the spec lead, so your code is how he intended the API. I don't think the RI implementors considered that scenario, so my initial guess is we'd have to file an issue there.\n. Unfortunately this is a limitation of the JSR's annotations RI. Because a @CacheResult method might have multiple key parameters, a composite key needed. However, DefaultGeneratedCacheKey does not expose the parameters to allow anyone to unwrap it as it is simply a surrogate for its own logic. This makes it incompatible with loading caches.\nThe annotations do not have a concept of bulk lookup and always delegates to Cache#get(key). This means your collection is wrapped as the key, which is why only the load(key) is ever called.\nFor atomic cache loading to work, it seems that you have to specify your own cacheKeyGenerator class on the annotation and unwrap it in the CacheLoader. The following passes your itLoadsPersonById test case.\nUnfortunately I think the JSR's annotations result in worse code that using their API directly. If you don't need the cache for integration purposes (e.g. Hibernate or multiple providers) then I think working directly with Caffeine's apis is a more pleasurable experience.\n``` java\nstatic class PersonFinder {\n  @CacheResult(cacheName = \"persons\", cacheKeyGenerator = PersonKeyGenerator.class)\n  public Person find(Integer id) {\n    return null;\n  }\n}\npublic static class CacheLoaderImpl implements CacheLoader {\n  private final Map persons =\n      Arrays.asList(new Person(1, \"Tom\"), new Person(2, \"Dick\"), new Person(3, \"Harry\"),\n          new Person(4, \"Sally\")).stream().collect(toMap(Person::getId, Function.identity()));\n@Override\n  public Person load(PersonKey key) throws CacheLoaderException {\n    return persons.get(key.id);\n  }\n@Override\n  public Map loadAll(Iterable<? extends PersonKey> keys)\n      throws CacheLoaderException {\n    Map result = new HashMap<>();\n    for (PersonKey key : keys) {\n      Person p = persons.get(key.id);\n      if (p != null) {\n        result.put(key, p);\n      }\n    }\n    return result;\n  }\n}\n@SuppressWarnings(\"serial\")\nstatic final class PersonKey implements GeneratedCacheKey {\n  final Integer id;\npublic PersonKey(Integer id) {\n    this.id = id;\n  }\n  public Integer id() {\n    return id;\n  }\n  @Override public boolean equals(Object o) {\n    return (o instanceof PersonKey) && ((PersonKey) o).id == id;\n  }\n  @Override public int hashCode() {\n    return id;\n  }\n}\nstatic final class PersonKeyGenerator implements CacheKeyGenerator {\n  @Override public GeneratedCacheKey generateCacheKey(\n      CacheKeyInvocationContext<? extends Annotation> cacheKeyInvocationContext) {\n    CacheInvocationParameter[] keyParameters = cacheKeyInvocationContext.getKeyParameters();\n    Object id = keyParameters[0].getValue();\n    checkArgument(id instanceof Integer);\n    return new PersonKey((Integer) id);\n  }\n}\n```\n. Hi Jiming,\nYou might consider this in terms of refreshAfterWrite. That is triggered when an entry is accessed and not yet expired, but stale enough that it should be asynchronously reloaded. The CacheLoader#reload accepts the old value and returns the new value. You could put your quick check and return the old value if unmodified, which would reset the write time to give it another full duration.\nThe benefit of that approach is it ensures that you don't resurrect inactive entries. That is of course error prone if no maximum size is set by leading to memory leaks. The size policy is pretty intelligent so it should correctly identify that the inactive entries are preferable to evict, so perhaps it doesn't matter too much.\nI think needing the cache in the listener isn't the common case and is an API breaking change. The workaround of setting the cache instance on the listener after construction is simple, so I don't think an API change is necessary.\nAlso note that the removal listener is called after the entry was removed. This means there is a slight race condition of removal -> get -> re-put. That's probably okay, though there are workarounds using CacheWriter.\nI think using refreshAfterWrite is probably the cleanest approach. What do you think?\nCheers,\nBen\n. Hi Jiming,\nThe refresh time should be shorter than the expiration time. Knowing a good number for either is guess work, unfortunately, because it is very domain specific. In both cases you have to judge when an entry is too stale and the system-of-record should be queried. If you have an expiration time set, then you have some hypothesis that could be extended to a refresh duration.\nI appreciate your API suggestion, but it feels very special case. I think there are adequate ways to achieve your desired effect without making library changes. Your original idea could be done by providing the cache instance to the listener after construction. Or you could use a second-level victim cache set with a longer expiration time. Regardless, whether you observe performance gains is questionable and I suspect that if you measured its not problematic to expire and let the next request load it.\nYour test passes for me, but by sheer luck. The problem is a race conditions by using the system clock and that refresh is performed asynchronously. You can control time using a custom ticker, e.g. Guava's FakeTicker, and disable asynchronous execution using a same-thread executor.\nBelow is your test using that style. Note the minor tweak of an off-by-one (30 -> 31) is necessary due to the comparison logic.\n``` java\n@Test\npublic void test_reload_about_to_expired_entry() throws InterruptedException {\n  FakeTicker ticker = new FakeTicker();\n  AtomicInteger counter = new AtomicInteger(0);\n  AtomicInteger reloadCounter = new AtomicInteger(0);\n  LoadingCache graphs = Caffeine.newBuilder()\n      .maximumSize(1000)\n      .ticker(ticker::read)\n      .executor(Runnable::run)\n      .expireAfterWrite(50, TimeUnit.MILLISECONDS)\n      .refreshAfterWrite(30, TimeUnit.MILLISECONDS)\n      .recordStats().build(new CacheLoader() {\n        @Override\n        public String load(@Nonnull String key) throws Exception {\n          return key + \":\" + counter.incrementAndGet();\n        }\n    @Override\n    public String reload(@Nonnull String key, @Nonnull String oldValue) throws Exception {\n      reloadCounter.incrementAndGet();\n      // System.out.println(\"old value = \" + oldValue);\n      return oldValue;\n    }\n  });\n\nAssert.assertEquals(\"jiming:1\", graphs.get(\"jiming\"));\n  ticker.advance(31, TimeUnit.MILLISECONDS);\n  System.out.println(reloadCounter.get());\n  Assert.assertEquals(\"jiming:1\", graphs.get(\"jiming\"));\n  Assert.assertEquals(1, reloadCounter.get());\nticker.advance(20, TimeUnit.MILLISECONDS);\n  Assert.assertEquals(\"jiming:1\", graphs.get(\"jiming\"));\n  Assert.assertEquals(1, reloadCounter.get());\nSystem.out.println(\"1.\" + graphs.get(\"jiming\"));\n  ticker.advance(20, TimeUnit.MILLISECONDS);\n  System.out.println(\"2.\" + graphs.get(\"jiming\"));\n  ticker.advance(20, TimeUnit.MILLISECONDS);\n  System.out.println(\"3.\" + graphs.get(\"jiming\"));\n  ticker.advance(20, TimeUnit.MILLISECONDS);\n  System.out.println(\"4.\" + graphs.get(\"jiming\"));\n  ticker.advance(20, TimeUnit.MILLISECONDS);\n  System.out.println(\"5.\" + graphs.get(\"jiming\"));\n  ticker.advance(1000, TimeUnit.MILLISECONDS);\n}\n```\n. Hi Jiming,\nI agree that in isolation your idea is very reasonable. But I think there are a few details you haven't considered yet.\nConcurrency\nA read like getIfPresent and iterators should be non-blocking. If they observe an expired entry then it is treated as not present, e.g. returning null or skipping over it. If a expired entry can be resurrected then these calls have side-effects of needing to call the expiration hook. To avoid stampedes this would require double-checked locking of the entry. As the expiry check is a foreign function it could be slow, so it might be surprising behavior that a simple read results in an expensive operation.\nThere would be a similarly large ripple effect across all methods, e.g. remove and replace, as all would have to take into account the entry may be recovered. Even though you want to remove a key, to give the correct removal cause we'd have to perform the check. That might be viewed as unnecessarily wasteful in some scenarios.\nNetwork\nYour proposal optimizes for a single server, but in a large network you'll have many with possibly different entries in their cache. That means if the call to query the data is prohibitively expensive, it will be encountered fairly frequently in either case. Instead you would want to optimize at the system level, e.g. using a secondary cache like memcached/redis, a materialized view, etc. If that was in place to reduce the worst-case latencies, then a few extra calls of an expired cache entry would have a marginal effect.\nThat means that most cases for a local cache is to avoid \"death by a thousand cuts\" due to the high volume of cheap queries. Examples might be DNS resolution or loading a schema to validate a json request.\n\nThe simpler approach of expiring and reloading the data seems like it should be adequate. It is easy to understand, predictable, and focuses the problem on optimizing the whole system rather than a single machine.\nI also believe you get most of the benefit if you use refreshAfterWrite to reload active entries that will expire soon. Your approach defers until after it has expired, which primarily helps when there has been no system activity. For example if your product is only used within a single region, so at night there is no usage and all entries expire. That leads more to a case of potentially wanting to periodically warm the cache for morning traffic. But once you have activity around the clock, which often happens at scale, then your idea doesn't seem overly beneficial.\nThat's why I feel this is a special case which optimizes for handling low volume scenarios and has high implementation complexity. At which point there seem to be very easy workarounds to get the desired effect in user code rather than needing to change the library.\n. expireAfterWrite means that the entry should be removed after a duration past its last modification. That means for a setting of 10 minutes and the next cache access is an hour later, the expired entry will be present, hidden from the user, and automatically removed. There isn't a timer thread that performs an immediate removal, but rather the work is triggered by application usage. Therefore I think the hook would need to be called to determine whether the entry is visible to a query.\nSorry, but splitting the expired removal cause would be an API break. An access policy is usually for cases like authentication tokens and similar. That's not as common, so most users probably want a write policy. Off hand I can't think of a use-case for enabling both and needing to distinguish in the listener.\nI agree your hook, if properly implemented, is optimal for your scenario. It is cleaner client code than the optimal workarounds, e.g. a victim cache to loop back through. But shaving off a few requests won't have an observable performance improvement for application. When I construct a case that would, the application would benefit from a secondary cache regardless making it moot. If you use Caffeine+Redis and refreshAfterWrite then you'll have good performance, the system behavior you want, and a lot of flexibility. If you remove Redis from that equation, you'd still want to optimize the database query to minimize worse-case latencies of the entry not being cached.\nI'm sorry, but I see a lot of complexity and struggle to see the upside. I really appreciate your thoughts, and please keep offering them, but I am doubtful that we're going to agree on this one.\n. I do think a DI factory would be nice. I'm also not sure the best way to specify the injector instance either. I dislike static injection due to tests, but the JSR doesn't provide many good options for us to consider. The alternative that the spec authors might have intended is a implementation-specific setting on the CacheManager. The reason being their odd method unwrap(class) to downcast with warnings suppressed. Or maybe we could hack Properties since for legacy reasons it allows Object values, though that seems error prone.\nI think we wouldn't want to have any Guice specifics in the library. Maybe instead allow users to plug in their own FactoryBuilder for us to call to get a factory for a class. Then you could provide the GuiceFactory and by default it would delegate to the spec's convenience class. This way others could write bridges to their DI framework. I'd be curious to know if Spring's JCache and CDI's implementations are able to inject into the factory beans.\nI'm also curious why you've decided to adopt JCache?\n. Its unfortunate that I made TypesafeConfigurator use static methods instead of being an instance. It would have been nice to pass the FactoryBuilder through so that you could define it on the CacheManager and avoid statics. But I supposed you'd have to try to isolate tests by using a generated uri in CachingProvider#getCacheManager. Though that's not trustable because some providers use the URI as a path to the config file and all can return the same instance regardless. I guess the specification is not friendly for running parallel tests within a JVM.\nI don't think that I need to add a FactoryBuilder to the CaffeineConfiguration, since it takes in Factory<X> directly on the setters. So it seems to be a responsibility of a \"Configurator\", in this case the TypesafeConfigurator. So I'd read the setting there, defaulted to a wrapper of FactoryBuilder, and off you go. Let me know if you were thinking something different.\nIf I was in your shoes, I'd probably write my own annotations or even redefine the CacheResult interceptor. Then you could perform the load atomically using an EntryProcessor wrapping the method invocation, optionally supporting bulk in a custom annotation. There wouldn't be a need for a CacheLoader and the method instance has access to the injected fields. So much simpler, but that doesn't solve the cases of wanting to inject into a CacheWriter, CacheEntryListener, or Copier. For that you'd have to supply the CaffeineConfiguration directly, e.g. by forking TypesafeConfigurator, so the above fix is still useful.\n. A slight variant of defining the FactoryBuilder in application.conf is to have a static method in TypesafeConfigurator where you could set instance. That would still leave a static, but not require static injection as you can use requestInjection to get the Injector during binding, or set it later after the injector is created (e.g. if you manage the startup's lifecycle using Guava's ServiceManager). That would result in cleaner code, imho, and avoid needing to perform a Class.newInstance(...) of the class path in the configuration file. Thoughts?\n. Yeah, I was thinking only one builder would be necessary. I stubbed the code as,\n``` java\n@FunctionalInterface\npublic interface FactoryCreator {\n   Factory factoryOf(String className);\n}\npublic final class TypesafeConfigurator {\n  static FactoryCreator factoryCreator = FactoryBuilder::factoryOf;\npublic static void setFactoryBuilder(FactoryCreator factoryCreator) {\n    factoryCreator = requireNonNull(factoryCreator);\n  }\n...\n  String writerClass = config.getString(\"write-through.writer\");\n  configuration.setCacheWriterFactory(factoryCreator.factoryOf(writerClass));\n...\n}\n```\nYou should also be able to set the static field in CaffeineJCacheModule. That could be done in the binding stage using requestInjection of a GuiceFactoryCreator instance, as an eager singleton to a provider that does the initialization, or registering a startup stage if you have a start/stop lifecycle. Or if we include javax.inject as a dependency, we could annotate the setter with @Inject so that you could use requestStaticInjection(TypesafeConfigurator.class) if you have a bind(FactoryCreator.class).to(GuiceFactoryCreator.class). The latter seems nicest, actually.\nI think we both feel the static setter is ugly, but the lesser evil of the options. Since JCache itself uses statics for the global registry of providers (and therefore managers, etc are globally scoped), putting it as an instance level doesn't buy us anything. So its weird but I don't have any bright ideas to improve it.\n. Yep. If I have time I'll try tonight, but otherwise get a release out over the weekend. Either way you'll have it come Monday morning.\n. One concern with javax.inject.Inject is that I don't know how Spring's classpath scanning interacts with it. I haven't used it since 2.0 days, but I think that one usually restricted it to your company's package to reduce startup time. I wouldn't want the annotation to cause a breakage due to the standard not supporting optional / providedBy.\nSince you've used Spring much more recently than I have, do you know if that's going to cause mayhem?\n. See JCacheGuiceTest for a full example of what we've been discussing. Its not too different from your code, except that you can encapsulate all of the details within your module. If you have a GuiceFactoryCreator that is @Inject with the Injector, your module should add the bindings\njava\nrequestStaticInjection(TypesafeConfigurator.class);\nbind(FactoryCreator.class).to(GuiceFactoryCreator.class);\nI'll release shortly. Just waiting to see the CI finish.\n. dang, I accidentally called that method setFactoryBuilder instead of setFactoryCreator. Oh well, too late now.\n. The cache does not allow storing null values. If you perform a computation and return null then this indicates the data doesn't exist and the caller receives null. So for example cache.get(key, function) or CacheLoader#load behave similar to ConcurrentHashMap#computeIfAbsent. An explicit put(key, null) is not allowed and will throw an exception.\nDoes that resolve your scenario?\n. Optional is a good technique for negative caching.\nCan you clarify if you have an open question? I'm unsure how to proceed.\n. ahh, sorry about that. I relied on the TCK with additional tests based on code coverage and discovered bugs. Unfortunately the TCK turns out to be pretty poor and admittedly I put most of my effort towards Caffeine's core test suite. The adapter is a lot larger than I'd like due to significant differences in designs.\nSince there might be other mistakes, can you use a SNAPSHOT? I figure you'll be banging on it a bit over the next few days and we can avoid version noise. If that's okay, then I'd bump the release when you gave the thumbs up.\n. Thanks! Just let me know when you're moving off either due to lack of time or no more issues found. I think we can delay because JCache has had low adoption, isn't the primary API, and the module has a tiny download rate (253 in July according to Central).\n. It seems that loadAll does not define what happens on a null return value. Neither the JavaDoc nor the specification mention it, and none of the other implementations have special handling so they too would NPE. So I'm going to be null averse in my reading and assume it is not allowed.\n. Snapshot should be good now that the CI behaves correctly again. \n. Thanks! Let me sort out #127 and then I'll get you a proper version.\n. I'd like to wait until JCTools is released with the fix, in case additional tests are going to be added. But if not then done soon then I'll go ahead regardless and apply the changes, which are non-invasive. Its critical enough that I won't delay it beyond this week and will have a release out over the weekend regardless of JCTools' status.\n. Released 2.3.4 with this fix. Thanks for being patient.\n. Sorry to hear about this. If you can help write a test case that demonstrates the problem, I'd appreciate it. It would be helpful to know if you're cache is synchronous or asynchronous. The asynchronous variant adds complexity to avoid additional threads, premature refresh while the original future is loading, etc.\nI'll try to take advantage of the long weekend to see if I can reproduce and debug the problem.\n. I have a theory that I'll need to explore further.\nrefreshIfNeeded ends up in a compute method and assumes that it will update the write timestamp when complete. However an afterWrite() is costly, so it has an optimization to use afterRead() if possible. The final check only takes into account expiresAfterWrite() and not refreshAfterWrite(). When using an AsyncLoadingCache the timestamp is bumped to infinity due to the async client computation, so without an external write to the entry it would be stuck. An immediate solution, if I'm right, is to enable expiresAfterWrite with an excessively high value.\nI'll have to write a test to fail, understand why my coverage and validation are inadequate, and review all other writes for a similar oversight. So assuming that's the culprit it will still take me the weekend to get a proper fix in place.\n. In the described case, the writeTime is properly set during the computation. That is perhaps redundant in the common case, but okay. So nothing stands out when reading through the code and debugging a test case.\nThe next possibility is a visibility problem where the relaxed write is lost so the infinite timestamp remains. That's going to require a threaded test to see if we can observe it. In general a relaxed write is fine as best effort is good enough, but this timestamp is thorny. Of all the features, for me refresh is the most confusing due to all of the interactions at play and less familiarity by not writing Guava's version. I don't recall why I felt the infinite was needed and only for an async cache, meaning either it was unnecessary or I'm forgetting the scenario being guarded against.\n. My best guess is that you're using an async cache and all entries somehow are stuck as if the refresh is in flight so no others for that entry should be triggered. If that's not the case then I'm stumped unless the executor is hostile by ignoring our submissions.\n. Please provide more details so that we can determine if it's a bug in the cache or usage. So far I haven't been able to reproduce the issue, so a failing test case would be very useful.\n. Thanks for the update. I'm not attending JavaOne, but will be watching the videos. Have a great time!\n. Cache#asMap() has full support for atomic computations. Your usage looks similar to merge which would let you reuse the lambda (currently a new one is required to capture the criteria).\nThe size policy is based on likelihood of reuse rather than age, so that might not be what you want. You might also want to consider whether a Weigher would be helpful, which lets you state how much room in the cache the entry consumes. If a criteria can become very large (e.g. a collection of dates), staying within the entry count might not be good enough\n. Please re-open if there is more to discuss or that I can assist with.\n. The build passes on JDK9 without any special flags. Its merely a warning, not a hard failure. I don't run it on Travis because there seemed to be weirdness in the required configuration for experimentals, causing the JDK (8 or 9) to hang on an interrupt. But it runs fine locally or on Travis if I use a more reliable config that only works with JDK9.\nWhen JDK9 is released then I'll migrate to VarHandles and be released as v3.0. The alternative is to use the multi-jar feature and stay on 2.x. Either way my preference is to wait until JDK9 is released before migrating to VarHandles. I think its only grunt work and not complex. Only Striped64 would require a little work to see if we can retain Doug's trick of using Thread's random seed field.\n. Since we're compatible I'm going to close this for now. But I'll try to have a purified release shortly after 9 ships.\n. Runtime should work as well. My understanding is that they announced that internal APIs would be removed once the use-case was covered by a JEP and had been released for at least one major version. There was discussion of requiring a runtime flag, but that was rejected. There's probably a JavaOne session about the latest status on all of this.\nIf the project with the dependency is unmaintained then I think there is cause for concern. But otherwise I think its okay to ignore and trust the authors will update once JDK9 is released. Since that was delayed again, it may be premature to worry about it.\n. I'd guess that JDK10 probably won't be delivered until 2019 or 2020. That leaves enough times for a stack refresh, so you'll probably have good alternatives by the time its a blocking issue.\n. Take a look at the Policy api. It provides snapshot iteration order and the ability to query the internal metadata.\n. If you don't care about order then, as you said, using asMap and ageOf would be best.\n. If you run into any snags be sure to let me know.\n. Thanks for the report. A few differences to consider.\nGuava uses the LRU eviction policy, meaning that it discards the oldest entry in the iteration. Caffeine uses W-TinyLFU, which tries to discard newly admitted entries that have a low probability of reuse. This \"frequency filter\" helps avoid the cache from being polluted, which causes LRU to under perform. Please see the efficiency wiki page for a short overview for why this policy was chosen.\nSecondly a test should use Caffeine.executor(Runnable::run) to avoid any asynchronous behavior. When that is used the test passes as the entry is in the window. If its not present then when the eviction catches up it seems to cause a few null observations. I agree its not intuitive and I'm a little surprised as well, so I'll dig in a little to see why the concurrency changed the results.\nThird is that Guava disallows null values from the cache loader and would throw an exception. Caffeine allows them indicating not present and does not retain null as a value. That fits with Map#computeIfAbsent, whereas Guava disallowed via a checked exception per Josh Bloch's recommendation when reviewing the API.\nIn the migration page I did try to document these differences.\nThe admission window is 1% so I agree its seems odd, as the most recent should be retained even if some asynchronous behavior is there. So not sure immediately why that makes a difference.\n. The race condition appears to be caused by the write buffer being resized. The write buffer is initialized to 4 and grows to a maximum prior to throttling writes. When I increase the initial buffer size, e.g. to 10k, then your test reports no errors.\nI guess resizing results in the order being LIFO instead of FIFO. Technically the buffer has no order guarantee, but I had thought it was FIFO given how it links array chunks.\nFor your use-case you should consider some scratchpad separate from the cache. The cache holds transient data so you can't rely on data being present. Instead a secondary map could be used to hold in-progress entries so they are not discarded until demoted to normal cache content.\n. I agree its an easy confusion, but allowed given the cache has a probabilistic contract. I switched the write buffer from my own implementation to one of JCTool's excellent queues, so I'd have to dig in further to grok the resize. Perhaps @nitsanw can explain why we're observing LIFO, instead of FIFO, consumption of a MpscChunkedArrayQueue after a resize.\n. I played around and realized, not surprisingly, its a benign oversight in my usage of the write buffer.  While the buffer is FIFO, it may fill up in a write-heavy workload. In that case, back-pressure is needed and the writer may block and help out to make forward progress. Its task is run prior to draining the pending ones from the buffer, causing it to be \"more recent\" and evicted prematurely.\nThe solution is to reorder the writer's task after the buffer drain. Then your test passes. I probably thought it didn't matter in real-world so didn't try to be strict. But since its observable here I'll make the fix.\n. Released 2.3.4 with this fix. Thanks!\n. Since a cache is transient, recomputable data its usually okay to update the system-of-record and invalidate. Then your database is updating all of the effected rows in a transaction so that it is performed atomically. Then removing all of the effected records from the cache causes subsequent calls to observe the correct state. So if possible, that's a simple and common solution that usually performs well and keeps consistency.\nI agree recursive, hierarchical concurrent operations can be really messy and best to be avoided. If you can defer that to a transaction it would be much simpler.\n. You have a persistNode method to store the results to some kind of database. That means you plan on eventually loading the nodes from the database, e.g. because the cache previously evicted them due to the size constraint.\nMy thinking was that you perform all this work inside of a database transaction. This makes sense because it has stronger locking support, e.g. if two machines try to create overlapping paths it won't result in potential corruption. A transaction handles row-level locking, with deadlock detection, so you have much less to worry about regarding concurrency control. Instead you start a transaction, perform all of the tree manipulation logic, persist, and invalidate all nodes in the tree that were mutated. The cache is remains a simple way to query nodes quickly, but the database still does the heavy lifting.\nJoe Celko's book is an excellent descriptions of how trees can be modeled in SQL. If you're using a NoSQL database instead then it might be harder to model.\n. In that case I'd still start from the data model and work up towards the cache, rather than the other way around. I think its probably simpler that way, but I could be wrong. It might be a good exercise regardless.\nIf its key-value then you might consider making the tree structure one value, with the leaves as separate key/value lookups. Then walk downwards instead of upwards.\nIf your on a column-family store, like Cassandra, you might be able to use composite columns or similar. You'd want to talk with developers experienced in that domain.\nBut if you plan on doing this yourself you might need to use node-level locks and manage deadlocks. You might move locking outside of the Map and either on the Node or use lock striping. Guava's CycleDetectingLockFactory could prove to be useful. Something along these lines would be similar to what a database does internally. However, they get to assume ownership of the data access whereas you may have multiple machines mutating in parallel, which could cause more problems. In that case you might have periodic process that scans, detects corruption, and correct it if possible.\nAlternatively, if you use a distributed commit log (e.g. Kafka) you might be able to simplify this. Store the changes and make them viewable immediately. Then replay on a single writer into NoSQL database. This removes the writer needing to worry about concurrency and you can try to give an immediate view of the changes in the cache, if needed.\nI guess my argument is to focus on the database side first, then propagate to the cache. You're taking the other route, which usually is more problematic but you might get to work nicely here. I think all I can recommend is to look for edge cases and explore what both approaches offer. But if it works nicely for you now, then great job.\n. Closing, but re-open if there is more to discuss.\n. I was thinking we might be able to merge this all into master if disabled by default. Since the jar is linked but class not loaded, it should compile and not break on JDK8. But to run with collision would require JDK9 and we can add precondition to the CollisionPolicy checking the system property.\nDid you run DS1 with a maximum size of 5,000,000? The setting is in the config file, defaulting to a mere 512.\nGlimpse is a LIRS trace named gli.trace.gz. The multiX traces come from there too and is described in the paper. I think the numbering is how many programs are being run simultaneously, or something like that.\nI do think the log counters are pretty cool. At some point we should experiment with them adapted onto TinyLfu and put that into the simulator. TinyLfu is frequency biased and it would be interesting to see if log counters help balance it with recency workloads. @gilga1983 is the expert in all things TinyLFU, so he might have some good insights.\nTravis has a lot more contexts switches due to other builds on the machine, virtualizing to 2 cores, more frequent GCs (less memory), etc. So it typically exposes subtle races more so than a nice laptop does, surprisingly enough. Its really frustrating to debug, though.\n. I collapsed our commits with some minor clean-up, keeping you as the author, and merged.\n. Thanks for the detailed report. I'm sorry for the inconvenience this caused.\nThe previous challenges with the write buffer were around scheduling the task when it is being added. Some of that was due to a FJP bug, and then later mistakes when I tried to make the logic more robust. Despite a lot of effort put into testing, its hard to predict some of these concurrency bugs.\nThis case is different because only 1 thread (for a given cache) can poll the write buffer. This drains and replays the changes onto the eviction policy. That polls up to a maximum, so the spin loop causing the failure is inside the WriteBuffer. That's a stripped down version of JCTools' MpscChunkedArrayQueue.\nI think the first step is try to reproduce the live lock, then re-sync with the latest JCTools version, and if broken then work with @nitsanw on a fix.\n. I probably won't have time until this weekend to dig in too much. I think that I can weaken JCTool's  spin loop safely, because the element will be consumed on the next maintenance cycle. That is scheduled after the write buffer is appended to. So assuming the buffer isn't losing the element, it should be visible the next time around.\nIf you're able to reproduce it in a test case that would be very helpful. I'll try to adapt my Stresser to see if I can give @nitsanw a useful failure report.\n. The polling occurs in drainWriteBuffer() which is guarded by evictionLock. In addition to my own tests, static analysis performed by ErrorProne should be asserting this behavior. So only a single thread should be able to consume the write buffer.\n. I created a SNAPSHOT version that adds jctools as a dependency and uses MpscChunkedArrayQueue directly. That should avoid any surprises if I incorrectly embedded the code. See the jctools branch for this commit.\nWhile that is running on travis, I've manually pushed a new SNAPSHOT. The travis build will take about an hour to verify against the tests before releasing a snapshot.\n. @normanuber Since you are using a high number of caches (1,000) it might be good to quickly discuss the use-case. For example if these are per-request, quickly populated and discarded, it could lead to a pile-up of cache instances and async processing tasks scheduled. The concurrency and hit-rate focus of this cache leads to a design towards long lasting instances with high utilization. It may be that this is mitigated using Caffeine.executor(Runnable::run) to penalize the caller or, if low concurrency, a LinkedHashMap in LRU mode is a better match.\nIt may be that we need to better handle high instance churn, which is observable as a filling up FJP with stale write tasks and we'd mistakenly focus trying to break the queue.\n. So far I have been unable to construct a test case that breaks WriteBuffer. It could be a OS/JVM/hardware match-up causing a visibility problem.\nMy best alternative guess is that you're workload is causing the cache to flood FJP. That's used to avoid penalizing the caller for policy maintenance, e.g. writes triggering eviction, and removal notifications. If you configure with Caffeine.executor(Runnable::run) then it will behave similarly to Guava by amortizing the cost on the callers. The only difference is that Guava uses a notification queue to process outside of any lock, whereas with an executor we assume it can be delegated to inside the policy's lock. So unless your removal listener is very expensive this won't make a big difference.\n\nMaven Central does not allow snapshots and has a dedicated repository for that workflow instead. You need to add that repository to your pom as,\nxml\n<repositories>\n  <repository>\n    <id>snapshots</id>\n    <url>http://oss.sonatype.org/content/repositories/snapshots</url>\n    <releases>\n      <enabled>false</enabled>\n    </releases>\n    <snapshots>\n      <enabled>true</enabled>\n    </snapshots>\n  </repository>\n</repositories>\n. Thanks @normanuber. From https://github.com/JCTools/JCTools/issues/135 we might want to try using the latest snapshot of JCTools' to see if that change fixes the problem.\n. Can you test if I update to JCTools' snapshot?\n. I uploaded a new snapshot. However since Maven only checks 24-hours you may need to force it to update (mvn clean install -U). If that's too much trouble, you can specify the snapshot timestamp explicitly as 2.3.4-20161023.211902-21.\nJCTools does not deploy snapshots and leverages jitpack.io instead. You'll need to add that repository as well.\nxml\n<repository>\n  <id>jitpack.io</id>\n  <url>https://jitpack.io</url>\n  <releases>\n    <enabled>true</enabled>\n  </releases>\n  <snapshots>\n    <enabled>true</enabled>\n  </snapshots>\n</repository>\n. Thanks @normanuber and @nitsanw!\n@nitsanw do you think its possible to write a unit test that exposes this bug and its fix?\nSince the next version of JCTools is imminent, I'd like to port over tied to a release. But if that gets delayed I'll try to have one out soon regardless. I'll revisit whether to shadow or add a dependency later.\nWhen this fix is released I'll close this issue. Thanks again for the help and patience.\n. It also doesn't support changing the class visibility, so it becomes part of the public API. If a direct dependency, then JCTools would be expected to have a stronger (semver) compatiblity story. E.g. the constructor for the queue was removed in 1.3 in favor of a new subclass. For now I prefer manually shading but jigsaw might remove my concerns, so it's good to keep reconsidering the current approach.\n. I know, but it's really annoying as IDE automatic imports often bring in the wrong one. So developers get frustrated by the conflicts of internal versions of common classes. I'm hoping jigsaw alleviates that mess.\n. Thanks @nitsanw, I really appreciate all of the hard work you put into jctools.\nI'll have a release out in the day or two, Sunday at the latest.\n. @nitsanw Can you please publish the test artifacts in future releases? Since I'm manually shading, I'd like to copy and run MpscGrowableQueueSanityTest against it. Rather than copy QueueSanityTest and its imports, simply having the test dependency would be ideal. Not sure how to query jitpack for this type of artifact.\n. Travis will take 1.5hrs to complete. Once the build passes I'll cut a release.\n. Released 2.3.4. I'm grateful for everyone's help, speedy responses, and patience.\n. For clarity that is an abbreviation for,\n- S(trong) keys\n- S(trong) values\n- S(stats)\n- M(aximum) W(eight)\nWhen I set OSGiTest with that configuration it passes. I'm not very familiar with OSGi, so I'd guess its a container bug or an issue with your build. \nAre you performing any type of minification, e.g. Proguard, or constructing a fatjar? The latter because they can use special classloaders, have class conflicts, etc. that cause quirks.\n. Oops, I decoded the abbreviation wrong (SSSMSW)\n- S(trong) keys\n- S(trong) values\n- S(stats)\n- M(aximum) S(ize)\n- Expire after W(rite)\nBut no difference on that test case passing.\nThe only case I can think of is if sun.misc.* cannot be imported (for Unsafe). Due to quirks in OSGi, when adding I remember reading that I had to set it to resolution:=optional. Perhaps there's a better configuration that makes it required but doesn't cause container issues.\n. @nedtwigg any advise on if I'm misusing OSGi? From https://github.com/diffplug/osgiX it sounds like I should use an extension bundle, though I'd prefer not adding a dependency if possible.\n. Since there are unit tests that should verify the stats, probably an incorrect comment when annotating all of the flags / consistent naming. In some places recordMiss is the parameter name, so I might have intended to rename recordStats due to being unclear. Unfortunately the code gets overwhelming due to all the different features, races, etc. so double checking which name is appropriate is a good idea.\n. There may be a bug, but my gut feeling is poor naming when refactoring. I put a lot of effort into the unit tests, so hopefully this would fall into one of those easy cases that are covered. For the limited set of Guava tests available (e.g. LocalLoadingCacheTest), Caffeine passes that port so that adds a bit of confidence.\nRegardless, the name recordMiss is meant as \"record a miss in the stats\". Caffeine does not perform negative caching (\"record a miss in the cache\") and leaves that to the user. A common pattern is to use an Optional<V> so that an entry does exist from the cache's perspective. I think deferring negative caching to client code is less confusing for everyone involved.\n. The Optional<V> is the sentinel I was thinking of. Then you can do cache.get(key).orElse(null) which isn't too bad.\nNegative caching adds a lot of subtle complexity. Do stats distinguish a miss from a negative hit? How does Map behave (size, iteration)? Is it okay to retain the user's key? What \"weight\" does a negative entry have? How does computeIfAbsent behave? Its just messy, the library tackled a lot of complexity, and it could be more of a burden than a gain. Hence my feeling of deferring to your code.\n. Both BoundedLocalCache and UnboundedLocalCache honor a recordStats behavior. The only direct call is LocalAsyncLoadingCache#get(key, executor, mappingFunc, recordStats). It appears that the comment & param name in LocalCache is wrong, but has no negative effect on internal expectations. These are the types of bugs I like :)\nI definitely see where you're coming from. I think that I've nearly exhausted the complexity budget from the implementation perspective, further leading to my hesitation. If the pattern was repeatably needed then writing a decorator that adds that capability is my current preference. As a library it should be easy to extend to provide richer capabilities and be encapsulated in other caching frameworks (e.g. Infinispan, imcache). And I might be proven wrong, so I'll keep revisiting the idea.\nBtw, you might have a middle ground for stats by using a custom StatsCounter so that you don't feel the need to give up on all of them. See the stats-metrics example for Dropwizard Metrics. Then you can have more control on publishing the default types (and names), in addition to your custom ones, in the metrics reporter. Might make it easier to wrap this all up into a reusable package to add negative caching.\n. Thanks for the investigative work! You're absolutely correct that I forgot about this scenario. I must have only considered reentrancy if the executor threw a RejectedExecutionException which FJP does not (BoundedLocalCacheTest#evict_rejected). The fix seems straightforward so I'll have a release over the weekend.\nIn the mean time, as I am sure you know, you can use a non-FJP executor or even ForkJoinPool.commonPool()::execute to force a reentrant lock.\n. I plan on releasing this tomorrow and will update you when its published. Thanks again for reporting and debugging the issue.\n. Released 2.3.5\n. This seems like the same problem in #98, but you went through the steps discussed already.\nI haven't used Tomcat or Spring in a long time, so I can Google to help but don't have the relevant experience.\nI would check if ClassNotFoundException is direct or masking a static initialization failure. If for example you are using OSGi and need to allow sun.misc.Unsafe (see #128) that might cause this problem. Maybe check to see if RemovalCause can be class loaded independently from the other cache classes. If so, then Unsafe might be the cause.\n. Great. So we definitely know that it is on the classpath.\nCan you try using UnsafeAccess.UNSAFE next? If that fails to load then its causing the restriction.\n. logger.debug(\"Loading the caffeine cache classes \" + UnsafeAccess.UNSAFE);\nShould be fine. We want to see if the static field will initialize.\n. Can you try using Caffeine.newBuilder().buildAsync(key -> key) next? If that works then we know its not your direct application.\nTomcat can host multiple applications on the same JVM. Since ForkJoinPool.commonPool() is shared JVM-wide, perhaps a thread is created under the context of a different application and the class loader doesn't have Caffeine? I always use an embedded container, so I'm not sure how to deal with colocated apps.\n. Great. So its due to ForkJoinPool.commonPool() being shared across applications in Tomcat so threads use different class loaders. There's probably a bunch of stuff on Google and StackOverflow on how to deal with that.\n. Can you try adding the caffeine jar to $CATALINA_HOME/lib? Might be the location to be shared across applications.\n. I'm sure someone else in your team is a Tomcat expert. I think you need to put the jar in some tomcat shared directory, but I don't know which. Hopefully someone who uses Tomcat regularly will immediately know the answer, now that we've narrowed it down.\n. Well ForkJoinPool.commonPool() is a static singleton. It creates threads as needed, so its a shared executor to perform background work in.\nTomcat uses classloader isolation to load and unload webapps. My guess is that a thread is created on a classloader that doesn't have access to the caffeine jar. Somehow you need to bypass this isolation, e.g. using a shared libs directory.\n. Might be this FAQ entry.\n. I hope you received assistance from a teammate. Sorry I don't have recent experience with Tomcat to help further.. Sure, we can do that. But my concern is that it might appear as an exaggeration. My understanding is that the library is used in non-critical scenarios, so the benefits might be negligible. The highly impactful use-case in neo4j is the page cache, which is a well-designed custom implementation. We wouldn't know if the techniques used by Caffeine would be useful without access traces to estimate the hit rate differences.\nI don't want to detract from @chrisvest and the rest of the team's work by overstating the usage, so let me know if it would be appropriate.. You're right, the query planner should be quite important. I don't want to de-emphasize your hard work either and I'm quite happy to list Neo4j at your recommendation. Thanks for the suggestion.. Short answer:\nCaffeine does not provide a pluggable eviction policy. This won't change, so your desired behavior will not be supportable.\nLong answer:\nInstead Caffeine implements a policy that is near optimal across a wide range of workloads. It takes into account recency and frequency, including the historic frequencies of entries no longer retained in the cache. This is performed in O(1) time, so the cache's size doesn't significantly impact the policy's performance. The goal is to maximize hit rates without users having to provide extra hints or bypass the cache to avoid thrashing. Most likely the current policy will out perform your custom one for the same workload.. Not by itself. Caffeine is only a local cache, so it could be a building block.\nFor example, Infinispan 9.0 swapped out its internal cache for Caffeine. That's a good indicator that others could do the same, build new ones using this library, or adapt Caffeine's ideas. There are a lot of options out there and I don't have any recommendations.\nHope that helped.. Unfortunately the burden is too high. This would require rewriting ConcurrentHashMap to use long keys and, I presume, templatize the entire project for code reuse. Project Valhalla is a long ways away, but it does most of the heavy lifting. From there it would require some hopefully minor rework, e.g. a per-entry state machine that uses sentinel keys.\nThe key escapes the cache due to the underlying computeIfAbsent which loading relies on. So the copying would require forking ConcurrentHashMap and might introduce subtle races with the hash lookup.\nI don't think there's a good option until JDK10.. Rather than copying keys, you could discard the MutableLong once the instance is assigned to an entry. At that point it should be effectively immutable to avoid map corruption and race conditions. Assuming you are using Apache Commons' MutableLong you'd write code as,\n```java\nprivate static final ThreadLocal mutable =\n    ThreadLocal.withInitial(MutableLong::new);\nprivate final LoadingCache cache = \n    Caffeine.newBuilder().build(key -> {\n      mutable.remove();\n      return new Object();\n    });\npublic Object get(long key) {\n  MutableLong mutableKey = mutable.get();\n  mutableKey.setValue(key);\n  return cache.get(mutableKey);\n}\n```\nAny put, load, or compute must discard the key once its been shared and may never modify it afterwards. But a get that doesn't invoke the loader, a lookup key, can be reused if isolated to the thread.. Here are some of the reasons we didn't go down that route.\n\nIncompatibility with reference caching: If stored with the wrapper, then weak and soft values won't behave as desired.\nIncreased GC pressure: If allocated on a hit this creates a lot of garbage, which isn't often escape analyzed away. For sensitive code that does matter. If stored directly to avoid this despite (1), then it adds memory overhead which is undesirable for large caches.\nNegative caching: Sometimes users want to retain a sentinel value to cache a miss. Often an Optional is the most convenient version of this. Unfortunately Java doesn't provide a flatten method for double optionals.\nHappy paths: In many cases users know a value will be returned, e.g. directly computable. The check for presence doesn't provide value, which leads to more usage of Optional.get. We tend to recommend users compute through the cache rather than check for presence, where a null indicates non-computable (e.g. record not found in the database).\nNon-idiomatic: Both Guava and the Java language team argue for judicious usage, primarily in application than library code, and neither are as full fledged as in other languages. Caffeine (and Guava's cache) were designed to feel like natural extensions to the Collections Framework. In Guava, Josh Bloch argued for disallowing null via checked exceptions so that null hostile API was chosen. But Map.computeIfAbsent didn't follow that idiom, lambdas are not exception friendly, and his advice was more applicable for the Java 4 style code of the day.\nLanguage adapters: Caffeine tries to feel intuitive for a Java developer, but Java styles feel out of place in other JVM languages. A thin adapter, e.g. Scaffeine for Scala, helps to bridge that gap.\nEncapsulated usage: Most usages of a cache are internal to a class and not exposed to consumers (an implementation detail). Just like it is common to see immutability to clients with internal mutability for performance, a similar convention makes sense for a cache. As you said, null can still be used internally, Optional only needs to be exposed to users.\n\nI think Optional in Java doesn't fit as cleanly as any of us would like, and many developers would feel it forced upon them undesirably. If you prefer an Optional-based API then I think writing an adapter API is a better route for the time being.. yikes, nice find! What traces did this impact significantly?. In the prior line, sizeMain--, is that still accurate? Since we're replacing I'd think the size should be the same, but you're more familiar with it at the moment.. okay, nevermind, I guess that's fine.. You probably want to take a heap dump to see what is taking up the most memory.\nNote that your cache value is an unbounded map. That means the cache might be correctly limiting it to the number of entries, but some of the maps grew at an uncontrolled rate. In that case you might prefer to use maximumWeight and a weigher based on the size of the map.. yes, lets see if that helps.. oh, and you should probably enable recordStats and monitor them. That way you can track the health of the cache. In addition if you want to track the size, too, then you can look at cache.estimatedSize() for the entry count, and cache.policy().eviction().weightedSize() for the overall weight. Should be pretty easy to wire it up to a reporting system, e.g. see Dropwizard Metrics in the example folder.. @timlincool is your problem resolved?. Closing, as no follow-up sounds like its fixed.. Thanks!. Yes, see the Cache.policy(). The Policy.Eviction#hottest(n) is what you're looking for.. Yes. If you know that a policy must be set, e.g. eviction, you can store the unwrapped optional to a field for cleaner code.. I'd prefer to leave this to a pending feature, variable expiration. That is more complicated to do correctly, but carries a much higher power-to-weight ratio. Otherwise we'd be stuck trying to provide one-off solutions for the easy problems, which adds usage and implementation complexity, with little overall gain.\nI have a general idea for how to implement variable expiration in O(1) time by using a timer wheel variant. That's nicer than the common solutions of requiring either a O(lg n) priority queue or relying on a maximum size to discard dead entries. But I haven't prototyped it, so unfortunately its on the backlog at the moment.. In memcached it might be set(key, value, duration, TimeUnit), where key1 and key2 can have expire at different intervals. That makes sense for a shared cluster, e.g. a session token has a 1 hour expiration that's being refreshed constantly, whereas a password reset token might be 48 hours.\nMemcached does the above by setting an expiration timestamp. When an entry expires it fakes a miss and the entry eventually gets removed due to a LRU eviction. The alternative is an O(lg n) priority queue, but that's expensive to maintain. The timer wheel approach is an alternative used by kernels that I'd like to explore.. See these slides for a talk I'll be giving soon. There is a section showing a hierarchical timer wheel.. This feature is implemented, but additional tests are needed before release. You would be able to perform this expiration by using,\njava\nCaffeine.newBuilder().expiry(new Expiry<K, V> {\n  public long expireAfterCreate(K key, V value, long currentTime) {\n    // return expiration duration, e.g. 5 minutes\n  }\n  public long expireAfterUpdate(K key, V value, long currentTime, long currentDuration) {\n    return currentDuration\n  }\n  public long expireAfterRead(K key, V value, long currentTime, long currentDuration) {\n    return currentDuration;\n  }\n}).build(). Released 2.5.0. Unfortunately this is a limitation of JCache and the RI's annotation providers. CacheManager does not provide a getOrCreate cache method. \nThe createCache method is documented as,\njava\n* @throws CacheException if there was an error configuring the {@link Cache},\n* which includes trying to create a cache that already exists.\nThe DefaultCacheResolverFactory uses a racy getCache then if null then createCache. If multiple threads call into it they will result in an exception thrown. The proper solution would be for the factory to synchronize on its own lock in a double-checked locking fashion.\nI had noticed this myself, but a provider is not supposed to implement the annotations. It is a bug in the RI that affects Guice and CDI (as Spring has their own) for framework integration. Unfortunately the annotations are also racy by not supporting a blocking memoization, so multiple threads could compute for the same key.\n@jerrinot has been the biggest advocate in pushing for fixes to these issues. I think @vbekiaris is now trying to work through some of them. Perhaps one of them can help resolve this.. The RI cache is not production quality, but the annotation RI is meant to be used. This has been stated by Greg in multiple contexts. The bug is not Guice's, but the annotation processors.. Can you provide a little more context on how you would use this?\nUnfortunately due to semver, this API change would require a major release. I plan on 3.0 being these types of tweaks and targeting jdk9 (no unsafe) whenever it ships.. To clarify, is this needed for testing or the implementation? It sounds like testing only.\nThe API change would mean any other implanting class would not compile. I'd also like to ensure changes carry their weight, as an API is long lived.. I think you could use Awaitility so that your tests tolerate this race.\nFrom your description, it also sounds like alternatives to refreshing may work. If you use ConcurrentMap<K, ImmutableSet<V>> instead, then the cache might be unnecessary. Or perhaps  some of the asMap view write methods useful if you insert into the cache after updating the Multimap.\nRight now I'm neutral on this change, but it would have to wait until 3.0 so hopefully we can find a workaround for now.. It never came up for Guava's Cache, perhaps because chained futures was a less commonly used idiom. Because of that, it didn't occur to me that users would take advantage of it. Most often refreshAfterWrite was relied on, instead.\nI added this to the roadmap. Due to the API change it will require the 3.0 release, which will JDK9 based (to leverage VarHandles). If I can toggle between Unsafe and VarHandles, I might make it JDK8 compatible.. To avoid race conditions of an load/invalidation storm, you should use cache.asMap().remove(cacheKey, cachedValue). \nI think variable expiration will solve your problem better, so hopefully I'll get it stabilized in a week or two.. 2.5.0 added support for variable expiration. This would resolve @costimuraru's use-case.\nI'm leaving this issue open for @boschb's scenario, which is to return the internal refersh future to the client so that they can add their own chain. That requires changing the API, so we have to wait for v3.0. The hold off on that is to make it a JDK9 release, though with the Jigsaw debacle I don't know what the timeline is.. Java 9 was released today, so my goal will be to release v3 by the end of the year. Hopefully sooner.\nThere are a bunch of tasks in the roadmap. Most are simple, except for batch refresh and JCache expiration overhaul. Most likely I'll keep punting on the refresh task, since that can be added scaffolded onto the interfaces using default methods.. Hey @johnou, you might be interested in this slide deck. I didn't know your email so couldn't ping you about it, but seemed to be in your area of interest. It was for a private talk at a startup nearby a week ago.. I fixed a similar bug in when clearing the cache usinginvalidateAll() or asMap().clear(). I added a simpler version of @yonik's test that fails prior to the changes. I've also added unit tests that rely on explicit lock ordering of the implementation to deterministically validate the eviction and clear bugs.\nI've run all the tests locally and will release after the CI confirms.. Released 2.4.0. When I brought it up he said it was on his backlog too, but I haven't talked with him about it since. I think jdk9 has a dedicated scheduler thread for the common fjp. Perhaps if its overloaded he'll add a ticking one.\nI would like to add that feature by a jdk9 release, since I'll bump up the version and fix API quirks. I don't think it's hard, just requires a little motivation.. If you are using the default executor, ForkJoinPool.commonPool(), it does not respect shutdown. So that could cause some confusion.\nAn alternative to RemovalListener is CacheWriter#delete, which is called synchronously with the operation. This imposes the operation's latency on a calling thread, which could be a negative if user-facing. That would simplify your case, though, so worse case it might be a good stop gap to iterate on while we discuss.\nIn #104, they wanted to close connections gracefully. When you say \"dump the cache when it is doing an orderly shutdown\" it sounds like you might be trying to allow for a warm restart. In that case you may prefer to use Policy, e.g. obtain the top-k entries in the cache. That's a bit of a reach, but thought I'd mention it if perhaps appropriate.\nOtherwise maybe you can put together a unit test for us to debug against?. Please also ensure that you are using the current release (2.4). It fixed two races causing stale notification values under a high write load. Doubtfully your issue, so hopefully you're able to find the time to dig into this.. @allen-servedio Do you think that you will be able to look into this soon?. Just to verify, you are specifying an executor in your production code? And that executor honors awaitTermination (unlike the common FJP)? And that executor indicates it has no pending work? If using a non-standard executor, can you please decorate it to log all incoming tasks in case its dropping them? (e.g. FJP had a bug in early versions of JDK8)\nDid using a CacheWriter show different (correct) behavior in your production scenario?\nCan you try calling these methods concurrently? You can use ConcurrentTestHarness. It could be a race condition, which your test isn't verifying.\nOn that note, you may also need to be careful to ensure calls coming in after shutdown are handled at the call site. That could be done by a volatile shutdown flag, for example.\nI'm fine closing or leaving open. My main concern is resolving your issue. If you won't be able to work on this for a while then we could close. Hopefully that's not necessary.. The idea behind using CacheWriter is that the executor is no longer involved. If the cache is empty then the delete method should have been called inline for the previously removed entries. That way there is no asynchronous surprises due to a race in my or your code. If we can verify that works, then we might have an easier time narrowing down the problem.. Oh, good find. I'm glad you were able to fix it.. The reason is because the array is populated inside of the lambda and evaluated outside. Due to the implicit final restriction, a variable would not be allowed by the compiler. Therefore we need a final variable that we can mutate to work around this language limitation.\nMost often you'll see people use an AtomicReference in that case, then though it doesn't cross a thread boundary. That's more expensive as it adds memory barriers and object overhead that may be harder for the JIT to eliminate. By using an array we can hope its more likely to be stack allocated by escape analysis.\nTechnically these arrays could be combined with careful indexing. I'm not sure how beneficial that would be if (a) the arrays are escaped to the stack, (b) computations are rare due to the optimistic lookup. So for simpler code in a complex method, I've kept them individual for now.. If you use a CacheWriter then I think you can achieve your goals. Lets assume you're using the API directly, as I am less familiar with Spring.\nCache 1\n - Configured with expireAfterWrite of 1 minute\n - CacheWriter#delete pushes the entry into cache 2 if RemovalCause is expired.\n - CacheLoader#load fetches from the db. If an error occurs, it fetches from cache 2 and verifies that the timestamp is in an acceptable range. If not, returns null or throws exception.\nCache 2\n - Configured with expireAfterWrite of 3 hours\nThe only flaw is the extra expiration check in the loader. This is because expiration a fixed time period and not handled by a dedicated thread, so it may be delayed until the next access or maintenance cycle. The algorithmic design for variable expiration is flushed out (slide 10) but I haven't gotten around to implementing it yet. (Alternatively, you might prefer to use an external fallback cache like Redis so that responses across servers is cached)\nDo you think this solution would work for you?. Closing as this is a month old and I hope you've found a nice solution. Please re-open if there were issues or you have suggestions. Thanks!. It is blocking only to perform the hash table operation. The value isn't returned, so it won't block on the future result. I don't think that deserves an asynchronous wrapping, do you?. Yes, but an async cache performs a load that returns a future which is very cheap. Your test is for a sync cache where the load can be slow due to performing the user operation.. Yep, it's removed and the removal notification is chained on the future. The asMap().remove(k) does block on the future since it returns the result, so invalidating demonstrates an optimization by the more opinionated API.. Cool, thanks!\nYou might want to use getIfPresent in your code examples, as get() was confusing if familiar with the interfaces.\nThere is a Guava bug on the invalidation problem. Since the entry isn't loaded Guava let's removal skip, which simplifies the locking. Caffeine has the same problem on a clear since ConcurrentHashMap suppresses the keys being loaded when we iterate.. In your first example get doesn't have a loader to demonstrate the race. But some libraries, like JSR107, only have get and behave differently if a loader is attached. I find that more confusing so the explicit getIfPresent and get (load) are more obvious to me, at least.. It would require either a wrapper or forking ConcurrentHashMap to support custom equivalences. I'm not keen on forking, as that leads to a desire to do further optimizations. Then it will be harder to support value types and track other fixes.\nI think the best option is a wrapper in user code. When value types land that might make it very low overhead option.. I think the cache should feel like a natural extension to the Java Collections Framework and be in line with Effective Java. That of course means to prefer collections over arrays, etc. It does put the onus on the developer to follow best practices, such as using static analyzers to catch these types of mistakes.\nDue to erasures we can't detect the type, so failing isn't an option. If the builder required a type literal then we'd know at the cost of a new class per usage. But I don't think that it carries its weight.\nThe byte[] key/value starts getting into the territory of offheap caches, e.g. ohc, and Apache Mnemonic.\nThe Compact String JEP might help your particular case.\nI understand the performance penalty, but its a platform problem that will hopefully be solved in jdk10. I know that isn't a satisfying answer. Its just there is no good generic solution until then.. One could argue weak keys would make sense for that case, since they use identity. Retaining a array key that has no other references makes little sense, as it would require walking the keySet to rediscover. But, similarly, some want weak keys with normal equivalence to create an interner.\nPerhaps lobbying Doug Lea (via concurrency-interest) would be your best bet. The Collections Framework probably disfavored the complexity for non-concurrent cases, as custom maps are abundant. For the concurrent case, especially for ConcurrentHashMap, a custom or fork are discouraged. While I don't think he'd go for it, I do think that's where the feature should reside and Caffeine would expose it.. Nice! \ud83d\udc4d \nSorry, I completely forgot that I promised to take a stab at this too.. Do you want to add this to the Simulator as well? That shows the hit rate efficiencies.. It looks like Caching.shutdown() isn't needed anymore, because you no longer schedule the crawler if the TTL is not set. In the benchmarks they call cleanUp during the setup, so it would have been turned off. But since its not loaded, I don't see any of the Rapidoid logging spamming the output. So I think I can remove that line?. I amended the simulator addition to your commit and merged. Thanks!. The Guice RI should work. Scala requires annotations to fully resolve, whereas Java ignores them unless reflective resolved, so there may be an ri compile time dependency you have to promote to runtime. I'd check the ri pom.\nThis Guice test might be helpful.\nJCache uses a ServiceLoader to discover the implementation. In your test, it's resolving to Caffeine again.\nI'd be weary of jcache outside of framework integrations. It has a brittle, limited api with lots of quirks. I don't think it was a positive step forward.. It looks like javax.enterprise.util.Nonbinding is a CDI annotation and I don't see it used in the JCache RI repository. It does use other javax.enterprise imports and has a dependency. So this shouldn't be needed for Guice.. Nice find. In their POM it states,\nxml\n<!--This is only needed if you are using a CDI based implementation of the annotations support.\n    In CDI 1.1. we should be able to remove this dependency completely. -->\n<dependency>\n  <groupId>javax.enterprise</groupId>\n  <artifactId>cdi-api</artifactId>\n  <version>1.0-SP4</version>\n  <scope>provided</scope>\n  <optional>true</optional>\n  ...\n</dependency>\nYou could request that they remove the dependency usage since CDI 1.1 was released 4 years ago.\n\nI think this is where I get the configureCachingProvider code from... However, what I need is to plug in a dummy implementation. I guess I can write one myself...\n\nThe configureCachingProvider was only needed because my suite has other caching libraries which also implement JCache. If you have only one, then you can probably don't need it except to override to a mock in testing code.\n\nYou mean it is not recommended to use JCache annotations outside of frameworks? I thought using such annotations is an easy way to enable caching instead of doing all the explicit gets and puts in the code... If not then what is a good way to enable caching without polluting my code with such cross-cutting concern?\n\nIt is my opinion that JCache does more harm than good, but you are welcome to disagree. The only value I see is to ease framework integrations, but it is highly flawed for application developers.\nThe annotations are quirky and interact poorly with the rest of JCache. For example,\n - CacheResult is a racy get-compute-put operation. Multiple concurrent calls for the same key will be executed, the value stampeded over each other, and finally quiesce. This is known as a cache stampede and a proper implementation would perform it atomically.\n - CacheResult does not support bulk loads, e.g. cache.getAll, which can be a useful optimization.\n - The annotations are mostly incompatible with CacheLoader and CacheWriter due to using a private key wrapper.\nYou'll discover more of these quirks as you become familiar with JCache and your use cases become modestly complex. You might then prefer to use Caffeine's apis directly which are fairly succinct and, hopefully, provide the right escape hatches to not get in your way. But you can definitely start with JCache, evaluate, and migrate if needed.. Thanks! I had fixed this in my expiry branch, but I'm still working on unit tests so hadn't merged it in yet.. In a prior thread, I replied with the main reasons, which you might find interesting. A few main points here...\nA concern with Optional is that it requires an allocation in the happy path of the value being present. This might go away in JDK10 with value types, but is an expensive choice until then as it puts a lot of pressure on the garbage collector. If one was to store the wrapped optional instead, this adds memory overhead and is incompatible with weak/soft values.\nThe preference is to compute atomically by using a mapping function or a CacheLoader. Most often you want to load the value into the cache, the api gives an alternative to a racy get-compute-put approach. In general a get(key) computes and most often the value is returned. If the value isn't computable some might return null for the caller to handle, throw an exception (e.g. JAX-RS BadRequestException), or negatively cache by storing a dummy value (perhaps an Optional).\nAn Optional makes the most sense when your orElse won't feed back into the cache. The difference is then pretty small compared to a null check and becomes stylistic. E.g. you could use Optional.ofNullable(cache.get(key)) or, if lookup only, cache.asMap().get(key, defaultValue). \nYou could also write a custom Cache interface with your preferred API that delegates to Caffeine, as Scaffeine did. In this way Caffeine should feel like a natural extension to the Collections Framework and those that love/hate Optional shouldn't feel too unhappy.\n. > What drives you to make the this conclusion? Personally, I don't see any pressure on performance/memory by my test/data/experiences.\nAn Optional requires a heap allocation and could be collected in the young generation. In a JMH benchmark that occurs in the eden space making it very efficient, but on an active server it would likely reach the survivor space before being collected. In the most minimal benchmark,\njava\n@State(Scope.Benchmark)\npublic class OptionalBenchmark {\n  static final Integer value = 1;\n  @Benchmark public void integer(Blackhole blackhole) {\n    blackhole.consume(value);\n  }\n  @Benchmark public void optional(Blackhole blackhole) {\n    blackhole.consume(Optional.of(value));\n  }\n}\nBenchmark                    Mode  Cnt          Score          Error  Units\nOptionalBenchmark.integer   thrpt   10  339097139.113 \u00b1 11957961.677  ops/s\nOptionalBenchmark.optional  thrpt   10  241541639.809 \u00b1  7378157.795  ops/s\nThis indicates a 29% penalty on the G1 collector. This doesn't mean you should avoid allocations, but that very hot paths might see a performance boost by being more watchful of GC usage.\n\nSecondly, Using the mapping function or CacheLoader to generate the return value when the key is not found in cache, it could be a good idea. but most likely not, at least to me. \n\nUnfortunately we'll have to disagree here because my experience, and the vast amount of code that I've seen internally or on Github, is the opposite. Most local caches have a narrow scope and are encapsulated, so there is only one or two answers given. That is often to memoize the value on a miss (to avoid dog piling) or throw an exception if not computable (e.g. 404 for a missing resource). A remote cache, like redis, is more likely to encounter the many different usages and strategies due being a shared collection of many types.\n\nit's get(k, mappingFunction) and getIfPresent(k), the Java 8 style....\n\nWe defined an earlier version of this API in Java 5 for Guava, and reviewed by Josh Bloch. An issue that Charles had was the passive nature of Map.get(), which didn't convey side effects or that the data structure might change underneath them. Therefore the Cache interface is more active where a get corresponds to JDK8's computeIfAbsent and the passive calls appear more special case. When you see cache.getIfPresent or cache.estimatedSize it indicates a warning that the user is inspecting the cache to do work, rather than delegating to it.\nJDK8 does not strongly advocate Optional, there was no attempt to retrofit it everywhere, its not highly used like in Scala. Like Guava's, it is useful when conveying the absence in an opaque API call, e.g. querying a time range class where an end date isn't required. Java collections very much rely on nulls as indicators and don't appear destined for a redesign, so I think conforming to their conventions is a positive.\nA Cache.get(k) could return a value, null (miss), or throw an exception (not computable). There is no Result type in Java, so Optional<V> or an exception might be more awkward. If only used on getIfPresent, then it means a mixture of null and Optional which is inconsistent.\nBut if you find the API is not an ideal fit for your code, then I don't see a drawback for writing a custom facade.\n\nIncompatibility with reference caching...(I don't understand it)\n\nWeak references can be a convenient strategy to clean-up resources when there are no more strong reference usages. This often when a size or time based policy could result in incorrect behavior, but a usage based policy is correct. There are many scenarios, but a simple generic one would be a dynamic stripe of locks,\njava\nCache<Key, Lock> stripe = Caffeine.weakValues().build(key -> new ReentrantLock());\npublic void process(Key key) {\n  Lock lock = stripe.get(key);\n  lock.lock();\n  try {\n    // Perform some work scoped to the key that cannot be done concurrently\n    // This is similar to synchronized (key), except interned to not be instance-based.\n  } finally {\n    lock.unlock();\n  }\n}\nIn this case the value is a weak reference and collected when no one holds the lock. A weak reference of Optional<Lock> wouldn't work, so storing the Optional in the cache could break this use-case.. I forgot to show a run of OptionalBenchmark with concurrency, to show a busy system causing GC delays. In that case I added @Thread(8) to the methods.\nBenchmark                    Mode  Cnt           Score          Error  Units\nOptionalBenchmark.integer   thrpt   10  1659249867.451 \u00b1 23404098.681  ops/s\nOptionalBenchmark.optional  thrpt   10   742319002.910 \u00b1 12702515.401  ops/s\nIn this case there is a 2.2x gain of avoiding allocations. This is slightly unfair because most systems are not cpu-bound and the GC able to mask the penalty by stealing free cycles concurrently. So we know that there is a 1.4-2.2x speedup, though that may not matter too much for most usages.. Your benchmark has, as expected, equal runtimes at 430k/s. In that case the computation is limiting the thread and the GC can steal spare cycles, as it is designed to do.\nMy benchmark wasn't to demonstrate real work, but merely that garbage is created that adds to the system. One hopes that escape analysis could remove it, but that is a finicky optimization. It also wasn't meant to argue against creating garbage, but that some applications struggle with GC churn and good data structures try to minimize their contributions. Similar to why Java's HashMap uses power-of-two sizes for shifts and AND masks rather than multiplication, division, and modulus. Its not an optimization that is critical or should bleed into the API, but an implementation consideration.\nAs I said originally, its a short-term concern until value types. That's not meant as a strong argument, e.g. as Bloch advises...\n\nConsider the performance consequences of API design decisions, but don\u2019t warp an API to achieve performance gains. Luckily, good APIs typically lend themselves to fast implementations.\n\nThe primary arguments are where we disagree. In those aspects, I think you would have to provide more examples to make a stronger case for the proposed API change.. Yes. I don't think performance should be \"the\" reason to reject an API design, but I do think it should be \"a\" reason to better scrutinize the options. In this case there are much stronger arguments that, when evaluating, led me against it in that API. It is used in Policy (cache.policy()) where I thought it was the better design choice.. Java's Optional is similar to Guava's, so I've used both quite a bit since being introduced. Both authors try to clarify it should be used judiciously.\nV get(key) is provided on a LoadingCache. I don't see Optional<V> get(key) allowing for removal of any of those methods. The get(key, mappingFunction) is equal to Map.computeIfAbsent, so I don't see why it would go away.\nJava's Optional is not as rich as in other languages like Scala and Haskell. In those languages it would be the sure idiom but Java it does not fit as neatly into the language's design or the conventions adopted by the community. The Cache APIs try to feel like an extension to Java Collections, which return null values to indicate not present. Returning an Optional is only in Java 8's Collections Framework for a few methods on Stream (e.g. findAny). Those methods are probably infrequently used compared to others.  This may change over time, but if so then there is a slow adoption by the language authors and community. So we risk violating one of Bloch's rules,\n\nAPIs must coexist peacefully with the platform, so do what is customary. It is almost always wrong to \u201ctransliterate\u201d an API from one platform to another.\n\nBecause of the language's constraints, there is a penalty for using Optional. To its favor it is explicitly designed to follow Bloch's rule,\n\nAvoid return values that demand exceptional processing.\nClients will forget to write the special-case code, leading to bugs. For example, return zero-length arrays or collections rather than nulls.\n\nHowever, Java is biased towards a structured programming style rather than functional. This means that it has typically been more readable to use if and for loops than nested function chains. Java 8 removes that for many of the simple cases, e.g. filtering by an anonymous class predicate was less readable than a for-if loop. But there are still holes such as a lack of Optional flattening, pattern matching, and for comprehension. This means developers are coerced into calling Optional.get() to drop back into the richer structured programming style.\nThat leads to another of Bloch's rules,\n\nYou can\u2019t please everyone so aim to displease everyone equally. Most APIs are overconstrained.\n\nIn this case, I think the majority of users would be displeased by using Optional here. Their usages of the Cache are encapsulated and they may use Optional at their API boundaries, but want succinct idiomatic implementation logic. Those like you who want Optional are perhaps slightly annoyed at writing a simple adapter to their preferred API. They might also be frustrated enough by Java's limitations to choose a language with stronger support, so a language adapter would be desirable regardless. To be pragmatic we have to gauge both the current users frustrations and the trend, which I think favors displeasing those in your camp.\nUnfortunately Guava's Optional has been around since 2011 and neither it nor Java's seem to have caused a big shift in programming styles or usage in their camps. Newer languages like Kotlin, Ceylon, and Dotty (Scala 3), and C# are favoring union types to replace Optional with stronger type information (e.g. nullable types). That might be a better fit for Java.\nOverall, I don't think there is a strong argument for or against Optional.. Have you seen Optional pipeline terminators used very often? (min, max, findAny, findFirst, and reduce) I haven't, and these methods was the reason for Java adding the new type. It seems that Optional is finding its way into application code mostly because developers are exploring it. How API designs evolve will be interesting to watch, because it could be that the other JDK8 features are adopted heavily, but Optional is used sparingly.\nScala's Optional is very nice to work with, but a lot of that is missing here. Theirs is Iterable to be a zero or one sized collection, flattens, matches, etc. The code comes out very neat with fewer ifs and loops. Java's hasn't been as pleasant for me.. Unless there is additional feedback, I am planning on closing this issue. I don't think this would introduced into the API. But happy to leave open if there is opposition favoring a reconsideration.. Oh, I think you're looking at the wrong configuration file. The reference.conf provides more detail. It is still sparse compared to the wiki, but mostly because the settings are either JCache or correspond directly to a Caffeine feature.\nI am working on unit tests for variable expiration, which I hope to get baked soon. Then I'd integrate that with JCache to make lazy-expiration eager (deprecate that block with a more appropriate name). This is based on a TimerWheel to be amortized O(1). That might work nicely if caching remote HTTP resources based on their expires header. Unfortunately JCache's expiry doesn't accept the key/value, but perhaps we can support either theirs or ours (if there is interest).\nJCache is incompatible with weak/soft references, so all entries are strong. There isn't any attempt to clear the cache by monitoring free memory, as that is error prone. So a worst case setting is either a memory leak or lack of data consistency by not refreshing.. What do you mean by \"hard to manage the overall size of the cache without some extra call back logic\"?\nJCache has a weird variable expiration that lets entries differ, but doesn't allow for inspecting them to decide on without threadlocal hacks. The spec authors rely on a size constraint to remove dead entries, but don't offer size as a JCache feature. There are a lot of odd decisions in JCache, but if your usage is simple then you can be vendor neutral.. Oh, then yes.. Can I close? Or do you have suggestions for where you think documentation is lacking, etc. that might be helpful?. Eager per entry expiration is now supported as of 2.5.0.\nJCache's per-entry remains lazy. It was too complicated to be backwards compatible, map to eager, and work around API differences causing the TCK to fail. The cost is that lazy acts the same as the spec author's implementations, which means it was intentionally a dangerous method when used without a size constraint. In v3.0, the \"eager\" and \"lazy\" expirations will be replaced with a single type that map's to JCache's. \nEager expiration can be used by setting eager-expiration.variable to an Expiry class. Alternatively the native API can be configured using Caffeine.expireAfter(Expiry).\nThe implementation is optimized for durations up to 7 days and degrades away from O(1) if that overflows.\nNote too that this provider supports DI, e.g. see JCacheGuiceTest.. Can you explain what you mean?. There are persistent caches like DiskLruCache, MapDB, RocksDB. You could use Caffeine as an in-memory tier with disk as a second level. Alternatively, you could use Caffeine as the eviction policy with persistent keys and values. The loader and writer make it very flexible.\nOther than fast warmup on a restart, disk caches have a history of being slow. Usually a remote cache like Redis provides a more efficient scheme. It's not a goal of this project and often isn't the best choice anyway.. I have not used RocksDB, but it sounds to be highly respected. Their homepage highlights an article on using it for application data caching, but the policy work may be left to users who treat it as a key-value store. \nThere were a few dedicated caching projects (Flashcache, EnhanceIO) but they seem to have been abandoned. Ideally there would be a project dedicated to that use-case, as often jack of all trade projects do them all poorly.\nIf you need something simple to deploy and embed, MapDB has a good reputation. I'm sure a little Googling would uncover a few other possible candidates.\nI haven't used a persistent cache for a long time since usually other approaches are better for the type of applications Java lends itself to.. The likely thought process was that if value in a remove(k, v) hasn't materialized yet then its not actually present. This would have been a valid optimization prior to the compute methods, but you're right it is now surprising. I should probably change all of those to be pessimistic by blocking.. > This is surprising to me because semantic-wise, I should be able to implement remove in terms of computeIfPresent (Neither of them should block)\nOh, I misread this. A compute should block because it is memoizing a value so that concurrent calls don't do the same work in parallel. If a race caused it to compute but lose then the value has to be thrown away. That could be surprising, e.g. if the value is a resource that must be closed. The fact that remove(k, v) does not block on an in-flight compute(k, f) contradicts the behavior of ConcurrentHashMap. While valid by the interface, a user relying on that linearizable logic would equally surprised. So I think the correct fix is to make it block.\nPotentially offering an Map<K, CompletableFuture<V>> might be handy if you wanted to have more non-blocking control. But I haven't seen a use-case yet, so its not offered.. It sounds like I need to review the remove and replace methods for linearizability, as I might have a similar prescreen in the synchronous case. The compute methods may prescreen for presence to lock on a load, but avoid that on a read. JDK9 will include prescreening for those methods, so the interleaving should follow expectations. I'll try to take a look at this over the weekend.\nUnfortunately exposing a Map<K, CF<V>> is a little more work. A decorator must intercept all writes to add callbacks when the future completes. If it is successful the callback calculates the weight and expiration time of the entry. If not then it automatically removes it. We then need to add a test suite for proper coverage. Ideally we could refactor AsMapTest to somehow work with sync/async values or worst case fork it.. I strengthened the methods to block rather than optimistically skip. I'll try to look into the async asMap view soon.. I've been working on this lately, sorry its taken so long. This will introduce an asynchronous map view on the newly introduced AsyncCache interface, which AsyncLoadingCache extends. For backwards compatibility, it will have to be a default method throwing an unsupported exception. But it will be implemented by Caffeine, so that shouldn't effect anyone.\nYou'll now have ConcurrentMap<K, CompletableFuture<V>> asMap(), which is the type of the underlying map. The only caveat is that CompletableFuture does not implement an equals and hashCode. This means that any methods that compare by value will be identity based, e.g. containsValue(value) and replace(key, oldValue). I think that's fair, and you'll still have the synchronous map view for convenience.\nSo far it passes Guava's TestLib, which has robust collections testing. I am still porting a copy of AsMapTest, which is quite large. Once that's functional, I'll have to add a few more test cases for the exceptional cases where the newly added future is automatically removed. Mostly just grunt work at this point to ensure proper test coverage.. Released 2.7. Sorry, I haven't updated the benchmarks page since version 2.0. It takes time to setup in order to avoid other processes from stealing cpu time and skewing results. There hasn't been any major changes in Caffeine to warrant a new evaluation and, since it is a manual process that I pay for, it hasn't been a priority to rerun. Usually running on a local machine is good enough for spitball testing to find bottlenecks.\nThe main advantage is getting your cache into a set of JMH benchmarks which could aid you for in tuning. From there you can fork it off into your own set of benchmarks (e.g. as done by cache2k, collisions, kudu) so as not to be tied to my project. It is more of a starting point than an annual shoot-out like the TechEmpower Benchmarks. If annual, you'd also want someone unbiased with authors providing recommendations, but otherwise unassociated.. Thanks for investigating, identifying the bug fix, and the workaround. Sorry if this caused any problems. I'll add a test case and have patch release out over the weekend.. Released 2.5.1. expireAfterCreate should return a duration and not a point in time. In your code change it to return myCacheableString.getTtl(). Also note that for async caches, the timestamps are relative to when the future completed rather than when inserted.\nIt could be simpler to use a FakeTicker and Awaitility to model the state changes in the test and avoid any races.\nAsync is confusing so there may be a bug causing it to call notify on update instead of create. I have to debug further to understand the flow. This is a new feature so there may be bugs that my tests didn't catch.. Can you try with a synchronous cache? I probably won't have time to look into the async problem for a few days, but will try to have a fix out by next Monday.. In the async case your expiry is wrapped by AsyncExpiry to translate the call. When the future is in-flight the duration is infinity (Long.MAX_VALUE). When it completes an callback updates it, which should calls expireAfterUpdate. The wrapper should notice that the creation was never called due to being close to infinity (larger than MAX_VALUE / 2) and delegate to expireAfterCreate instead. If a subsequent update occurs, e.g. cache.put, then it should delegate to an update call. Clearly that is not working as intended.. If I trace with a debugger it works. So perhaps there is a race in reading a field value.. I think that I've found the bug, which is due to an incorrect infinite time set on addition. The logic here is so that an in-flight future does not expire prematurely, though that block may no longer be necessary. It should have the infinite timestamp set already, but in prior releases was a little more racy (expiration was set outside of an entry's synchronized block).\nUnfortunately that was accidentally set to the duration MAXIMUM_EXPIRY (150 years), which is the highest valid duration that is honored (and half of Long.MAX_VALUE). Anything higher is used to signal that the future was in-flight and to adapt the calls correctly (e.g. update -> create). Setting it to Long.MAX_VALUE passes your test, as does removing the block.\nI reused the expireAfterWrite tests which sets a new duration on create or update. If I had tested on create only then I think this would fail without any threading needed. I'll try to get coverage for that configuration.\nI'll have to think through if the problematic block is even necessary. I'll have a release for you by the weekend, if that's okay?. Fixed. I'll release tonight.\nThe patch passes your test and I added expire-after-create to the suite.. Released 2.5.2. Yes, recursive computations are not supported. In the JavaDoc it states,\n\nWarning: as with {@link CacheLoader#load}, {@code mappingFunction} must not attempt to update any other mappings of this cache.\n...\n@throws IllegalStateException if the computation detectably attempts a recursive update to this cache that would otherwise never complete\n\nInternally, ConcurrentHashMap is stuck in a live-lock by spinning forever. In 2011 I advocated for this to fail fast and, while the fix was lost, the JavaDoc was changed to include a throws message. In 2014, I helped to more reliably detect it but that was not backported from Java 9 to an 8 release. See this StackOverflow answer for a few links.\nInterestingly, HashMap was deemed acceptable to perform recursive computations due to a lack of thread safety. However that fails due to internal resizing of the map caused corruption. I'm not sure if their fix was to restrict that or make the computeIfAbsent call handle it.. ahh, the HashMap corruption issue was only fixed in JDK9 too.\nFor Caffeine, you can resolve this by using an async cache, the synchronous view, and the default (unbounded) executor. That breaks the recursion by inserting a future, executing it on another thread, and blocking.. A striped data structure means that it is replicated N times with a strategy at the top level to select the instance (usually hashing). For example Java 5's ConcurrentHashMap was striped into multiple hash maps (called segments) that each had a lock for writes. This allowed for concurrent writes if two threads were mutating different segments.\nSimilarly a striped lock (e.g. Guava's Striped) would be used to simulate a lock per key by hashing into an array of locks. There could be some loss due to hash collisions, but rarely and avoids other complexities.\nMemcached can be thought of as a stripe of cache nodes using hashing to determine which to query from. When adding or removing nodes, a consistent hash can be used to find the right node regardless.\nA striped ring buffer is merely an array of ring buffers with a hash to select which to insert into. Just like the above cases the write is an expensive operation if contended on so the stripe reduces this penalty. Unlike the above cases, we don't need to deterministically find the value again and can resize at will. This is done based on contention by using the strategy in Java's LongAdder, which extends Striped64 - a stripe of counters. See StripedBuffer and BoundedBuffer for implementation details.. Thank you, but I think this actually introduces boxing and the benchmark was written specifically to avoid that overhead.\nThe key space is a precomputed array of Integer[] objects, with the selected object passed through the function to the cache's method. The caches are all object based, so a primitive would have to be boxed. The entry is always present and the value returned all the way back to the harness so that the JIT doesn't dead code eliminate the calls.\nYour change introduces unboxing the Integer[], which would be trivial to fix. Unfortunately since the caches all require object keys there is a necessary boxing within the function. The only way to avoid this is to use a boxed type throughout.. I plan on bundling this with #156, which asks for an async asMap() view. I may take the plunge and make an AsyncCache (non-loading)`, though still not sure that would be a good idea.\nIn yours, I need to remember to handle async caches since the puts go directly into the cache (rather than adapted through LocalAsyncLoadingCache). Just a note to myself as if I forget it would put V into the map instead of CompletableFuture<V>.. That is possible using an Expiry, but only by evaluating the entry as a callback. This is similar to Weigher where the weight is determined separate from the caller's put or load.\nThis request is to provide methods so that the caller can pass the duration explicitly. That would override the provided Expiry which would still be required for compatibility. This is possible and just requires me to find the time.\nAt the technical level this is done using a timer wheel so that it is O(1). That was the hard blocker that kept me from doing it in Guava as doing this right was not trivial. This further improvement is low cost.. I have this prototyped locally. I only need guidance on the API methods. I think the request is,\njava\nV put(K key, V value, long duration, TimeUnit unit);\nV putIfAbsent(K key, V value, long duration, TimeUnit unit);\nBehind the scenes, the put adapter method will wrap the calls in custom Expiry and delegate it to the cache's internal put(key, value, expiry, onlyIfAbsent). The other public put methods all delegate to that internal one, but resolve to the configured expiry. I presume that you want the value returned per Map?. Also, I presume that the duration is only set if the write was successful? As in, putIfAbsent will not modify the time if failing to a read.. and in that case, would a read delegate to the configured expiry or have no effect?. and if the cache is asynchronous, I presume the internal future is joined on to make the call appear synchronous? That would be required if you expect to have a return value. That leads me to prefer not returning the value unless there is a strong argument for it (as one generally does not manage the state of the cache explicitly).. That leaves me a bit stuck. If the return value, then the call is synchronous which is undesirable. If no return value then the previous state is unknown, especially on a putIfAbsent which is conditional. Does put return nothing and putIfAbsent return a boolean?\nThe code and tests are mostly done, but I don't know what the proper API is yet. Please advise!. ```java\n/\n * Associates the {@code value} with the {@code key} in this cache if the specified key is not\n * already associated with a value and returns null, else returns the current value. This method\n * differs from {@link Map#putIfAbsent} by substituting the configured {@link Expiry} with the\n * specified duration and by not returning the existing value.\n \n * @param key the key with which the specified value is to be associated\n * @param value value to be associated with the specified key\n * @param duration the length of time from now when the entry should be automatically removed\n * @param unit the unit that {@code duration} is expressed in\n * @return true if this cache did not already contain the specified entry\n * @throws IllegalArgumentException if {@code duration} is negative\n * @throws NullPointerException if the specified key or value is null\n /\ndefault boolean putIfAbsent(@Nonnull K key, @Nonnull V value,\n    @Nonnegative long duration, @Nonnull TimeUnit unit) {\n  // This method will be abstract in version 3.0.0; added & implemented as of version 2.6.0\n  throw new UnsupportedOperationException();\n}\n/\n * Associates the {@code value} with the {@code key} in this cache. If the cache previously\n * contained a value associated with the {@code key}, the old value is replaced by the new\n * {@code value}. This method differs from {@link Cache#put} by substituting the\n * configured {@link Expiry} with the specified duration.\n \n * @param key the key with which the specified value is to be associated\n * @param value value to be associated with the specified key\n * @param duration the length of time from now when the entry should be automatically removed\n * @param unit the unit that {@code duration} is expressed in\n * @throws IllegalArgumentException if {@code duration} is negative\n * @throws NullPointerException if the specified key or value is null\n /\ndefault void put(@Nonnull K key, @Nonnull V value,\n    @Nonnegative long duration, @Nonnull TimeUnit unit) {\n  // This method will be abstract in version 3.0.0; added & implemented as of version 2.6.0\n  throw new UnsupportedOperationException();\n}\n```. Merged into master what I think is the appropriate fix, but please reply tomorrow before I release it.\nAlso please let me know if any other methods are missing. I see Play's API has a getOrElseUpdate(k, func, seconds). However it is backed by APIs that are not atomic, e.g. Ehcache 2, so operations are racy get-compute-put. This makes sense since many users would swap the cache out for memcached or redis, which cannot be computed through. So I don't think having special cases for all the various methods makes sense, and the requested two cover the common usages.. > Are you referring to a race between a read and putIfAbsent that both try to (only) update the expiration? Or to a race where the read is trigerring value loading? or something else?\nI meant that the expiration can be set on a read (time to idle) or a write (time to live). If the putIfAbsent fails then it usually returns the present value, which means it becomes a read. The duration could be set regardless or only if on a successful write. I did the latter, leaving the entry unchanged, as I think that is the desired intent.\n\nNote that the javadoc for putIfAbsent return value is inconsistent with what you ended up implementing.\n\nThanks, I\u2019ll give it another pass. I think I see what you mean.\n\nDo you think it's worth the extra API surface area?\n\nI think it\u2019s not worth the surface area. Note that the policy is obtained through the sync view, so it should be consistent with that API. At first I imagined that a lower level API would expect more Map-like put methods, and we do offer asMap. So the return value confusion was trying to be consistent with Map, but the void method is consistent with Cache. For your case of memcached, it\u2019s set does not return the value so I guess the extra information isn\u2019t necessary for most usages.. I've looked over a few different libraries to see usage patterns.\n - Play's cache: void return type for set(k, v, seconds)\n - dropwizard-caching-bundle: void return type on put(k, response)\n - Netflix's EVCache: a memcached client, so boolean return type\n - ExpiringMap: the old value for put(k, v, dur, unit) and no time setting on putIfAbsent. I couldn't find usage in Github inspecting the return value.\nI couldn't find other well known frameworks that use a cache adapter api with an expiration option. Might be due to lack of imagination of what to look for.\nI think the APIs with the revised JavaDoc are okay. I'll release this evening if there's no other feedback. Thanks!. Released. Thanks for being patient!. I'd prefer not exposing low-level details like that as it confuses the API. I think there are workarounds, but you may feel they are hacky.\nThe asMap() view does not record stats for non-computing methods (@yrfselrahc might remember the reasons why / rules for Guava). Caffeine does record them for computing methods to capture loads, and whether that was the correct choice depends on @yrfselrahc's view. Guava's recent addition of these methods doesn't provide insight since they sporadically record stats, so it wasn't a consideration when implemented.\nThat means you should be able to use cache.synchronous().asMap().get(url) without stats recorded, If you supply the StatsCounter and retain a reference, you could update it on the else condition. That way (1) does not record and (3) always does,\nAnother solution might be to use a custom StatsCounter with a thread local flag to disable it temporarily. That's a more awkward solution, imho.. I think that approach has a poor power-to-weight ratio. A simpler solution is to supply a custom ConcurrentStatsCounter and decrement the miss counter in step three. The counters are merely LongAdder instances, so fairly simple. If you are proxying the stats (e.g. to yammer metrics) you might just have a compensating counter that is decremented and report the sum.. Do you mean find values based on a wildcard key? You could use the asMap() view to filter the values, e.g. using a stream.\nThere isn't any wildcard support. That is left to higher abstractions that can use this library as a building block. We focus on the raw data structures instead.. yeah, the intermediate inferences would still be needed to make its way through the chain or users would always have to explicitly set the types. This is @kevinb9n's trick when he designed Guava's builder.. Thanks for the report and sorry for the inconvenience.\nAs you noted, NodeFactory.WW holds only weak references:\njava\nstatic class WW<K, V> implements Node<K, V> {\n  volatile WeakKeyReference<K> key;\n  volatile WeakValueReference<V> value;\n  ...\nThe map should be referenced using node.getKeyReference() to store based on the weak reference and not the strong reference. The lookups should use Reference.LookupKeyReference to match.\nReferenceTest should be asserting this behavior. It seems to mostly have value-based tests and not enough weak key tests, unfortunately. When I quickly hack one of the tests to be based on keys it passes,\n```java\n@Test(dataProvider = \"caches\")\n@CacheSpec(keys = ReferenceType.WEAK, values = {ReferenceType.STRONG, ReferenceType.WEAK},\n    expireAfterAccess = Expire.DISABLED, expireAfterWrite = Expire.DISABLED,\n    maximumSize = Maximum.DISABLED, weigher = CacheWeigher.DEFAULT,\n    population = Population.FULL, stats = Stats.ENABLED, removalListener = Listener.CONSUMING)\npublic void put_weak(Cache cache, CacheContext context) {\n  cache.put(10000000, context.absentValue());\n  context.clear();\n  GcFinalization.awaitFullGc();\nlong count = context.initialSize() - cache.estimatedSize() + 1;\n  if (context.population() != Population.SINGLETON) {\n    assertThat(count, is(0L));\n  }\n  assertThat(cache, hasRemovalNotifications(context, count, RemovalCause.COLLECTED));\n  verifyWriter(context, (verifier, writer) -> verifier.deletions(count, RemovalCause.COLLECTED));\n}\n```\nCan you provide a failing test or direct me to where the strong reference is being held? I agree that at your write rate an explicit cleanUp shouldn't be necessary. Every write should trigger a cleanUp and empty WeakReference objects should be cheap even if held a little too long.. Oh, thanks!\nI think this was a mistake in the generated code. The keyReference is an object so that it could be a WeakKeyReference. The generated code is mistakenly,\njava\npublic final void setValue(V value, ReferenceQueue<V> referenceQueue) {\n  ((Reference<V>) getValueReference()).clear();\n  UnsafeAccess.UNSAFE.putObject(this, VALUE_OFFSET, new WeakValueReference<V>(key, value, referenceQueue));\n}\nI think if we changed this to call getKeyReference() instead of key then it would work.. I should clarify that keyReference is either key if a strongly held or (should be) WeakKeyReference if weakly held. I think changing the code generator to account for this would resolve the problem. I'll also add unit tests to better cover this case.\nIts morning in PST and I won't be able to get to this until the evening. I'll try to get it fixed and released tonight, if that's okay.. Thanks. I haven't had a chance to work on this tonight. I'll have it out by Monday if this week remains hectic, but I'll try to get it resolved sooner. I think most of the work here is to the missing write test cases to assert the failure and the fix.. Reminder to self - This is also a bug for weakKeys+softValues, unsurprisingly since the generated code is parameterized.. I have an improved unit test that fails with master and passes with the fix (branch). I need to review all the test cases in ReferenceTest to ensure they are generalized to capture this scenario.\nThe constructors are now generated as,\n```java\nWW(K key, ReferenceQueue keyReferenceQueue, V value, ReferenceQueue valueReferenceQueue,\n    int weight, long now) {\n  this(new WeakKeyReference(key, keyReferenceQueue), value, valueReferenceQueue, weight, now);\n}\nWW(Object keyReference, V value, ReferenceQueue valueReferenceQueue, int weight, long now) {\n  UnsafeAccess.UNSAFE.putObject(this, KEY_OFFSET, keyReference);\n  UnsafeAccess.UNSAFE.putObject(this, VALUE_OFFSET, new WeakValueReference(keyReference, value, valueReferenceQueue));\n}\n``. The code generator change also fixedWSo` to match your suggestion. The new unit tests should cover that case due to being parameterized to run against all configurations that match the specification. Of course that only helps when the tests are written or not flawed, which caused this case to go unnoticed. (It also results in millions of test executions, which now exceeds the maximum build time in Travis)\nSorry for the delay - its been an exhausting week. I'll have this released by Sunday night (PST) so that it is available Monday. Thanks for being patient.. I finished the tests (see branch) but there are two tooling issues that I need to address. Sorry that this will take another few days to work out. Specifically I need to split the Travis build to pass the time limit and the latest ErrorProne causes a compilation to fail.\n```\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/MultiThreadedTest.java:100: error: An unhandled exception was thrown by the Error Prone static analysis plugin.\n      (cache, key) -> cache.get(key),\n                               ^\n     Please report this at https://github.com/google/error-prone/issues/new and include the following:\n error-prone version: 2.0.21\n Stack Trace:\n java.lang.NullPointerException\n    at com.google.errorprone.matchers.IsLastStatementInBlock.matches(IsLastStatementInBlock.java:31)\n\n```. Seems errorprone was fixed in https://github.com/google/error-prone/commit/b9b492f554e1e12da2d3777a8e4e76095fda63bf. Sorry, I spent the holiday fighting with TravisCI and haven't gotten it to be stable yet. Despite splitting the build into multiple jobs, it still exceeds the timeout. That's probably because they are run on the same virtual machine, so the parallelism doesn't get any additional resources. I might need to switch to another CI that offers a longer build time.. I think I was able to get Travis under control by reducing the number of concurrent jobs to 1. Then the build is split into 3 jobs, each of which can run for up to 50 minutes. The only flaw is that code coverage can only be obtained for one of the jobs and the largest is near the limit.\nIf I am able to get a pass on Travis then I'll release sometime tomorrow.. TravisCI support agreed to increase the timeout if their idea didn't work, which sadly it didn't. So I'm waiting to hear back when the change is applied, can fully validate the build, and release. \nSorry for the long delay and noisy responses. Just want to keep you informed that I am actively trying to get this released and not ignoring the problem.. @davsclaus - LruCache\n@benson-git - ChannelPool\n@DavyLandman - RascalExecutionContext\nAccording to a github search, your projects may be affected by this bug. I have not evaluated when it was introduced, so it may have always gone unnoticed due to flawed tests. This bug causes caches with weakKeys + weak/softValues to not be collected, due to a strong reference and failed lookup. As you have not reported this issue hopefully it is a minor problem in your use cases, but please do upgrade promptly. Sorry for the inconvenience.\nA release is pending TravisCI support to enable additional compute time so that I can fully validate the build (suite passes locally). I will update this bug when the release is live.. Thanks, but my view is the opposite. It's an attempt to show respect and gratitude for the patience / understanding for any frustrations that my mistakes induced. I can't earn that generosity, but I can try to show that it is not taken for granted.. Released 2.5.3. Please remember to upgrade.\nThanks for the kind words.. Yes and no.\nThis library is strictly an in-memory cache. It can be integrated with another tier easily, but does not try to solve those. If you have a disk cache (e.g. DiskLruCache) then you can use a CacheWriter to evict into that cache, a CacheLoader to fetch through it, and a Weigher to provide an entry size.\nIf you want an all-in-one solution then the convenience will require using libraries that offer much less performance (but that might be acceptable). MapDB is a disk cache with on-heap cache. Ehcache supports multiple tiers, though v2 still sounds more robust than v3. I think some of the distributed caches (infinispan, ignite, etc) offer a disk tier, but that might be too invasive for your needs.\nDisk caches used to have a negative performance impact (spinning disks) and everyone switched to network attached caches (memcached). Flash has reduced that penalty, but mostly to reduce costs of memcached instead of reintroducing local disk caches. So that capability is not in much demand these days, which is why you won't find to many active projects with support. I'd probably try MapDB first, later pairing it with Caffeine if its on-heap performance wasn't good enough.. The first error is due to errorprone, where the plugin uses a dynamic version to always get the latest. They fixed this and haven't released, but did describe a workaround. I have that implemented in another branch. I could merge that into master as it seems ready to go from testing locally, you could build that branch, force the error prone version (e.g. 19), or cherry-pick the fixed code\nThe error prone plugin uses a JDK9-based compiler, so there are fixes to type inferences that are not in JDK8 yet. That is why this shows, even though the byte code is valid Java 8.\nThe release is blocked until I have Travis resolved, which I'm not sure how to fix at the moment.. The travis issue is that there are build timeouts (exceeds 50 minute maximum) due to the large number of tests. It looks like the branch is also failing remotely, but passes locally, which is another reason why I don't want to do a blind release. I'll go cherry-pick the code change so that master is building, at least.. Thanks! It looks like a mistake when I made changes to resolve ErrorProne warnings. For some reason they disliked default and wanted the switch to break out. I must have not noticed that had break instead of return like other cases. Does ErrorProne generate warnings for that still?. I think the error was due to UnnecessaryDefaultInEnumSwitch.. I think you guys are right. Please switch to return instead of break and drop the default, which is how I should to have fixed this originally. If you'd like to change to IllegalArgumentException then we should do a quick scan to make usages consistent.. Sorry that you are running into these issues. I probably relied too much on the TCK and use the native API in my projects.\nIt looks like the builder should be configured by the configureMaximumSize method. We should write a unit test.. I see evictions occurring in the following test. Note that the processing is asynchronous, so we could add a configuration setting of the Executor (uses FJP.commonPool), which makes testing harder.\n```java\n@Test(singleThreaded = true)\npublic final class JCacheMaximumSizeTest extends AbstractJCacheTest {\n  private static final int MAXIMUM = 10;\nprivate final RemovalListener listener = new RemovalListener();\n@Override\n  protected CaffeineConfiguration getConfiguration() {\n    CaffeineConfiguration configuration = new CaffeineConfiguration<>();\n    configuration.setMaximumSize(OptionalLong.of(MAXIMUM));\n    CacheEntryListenerConfiguration listenerConfiguration =\n        new MutableCacheEntryListenerConfiguration(() -> listener,\n            / filterFactory / null, / isOldValueRequired / false, / isSynchronous / true);\n    configuration.addCacheEntryListenerConfiguration(listenerConfiguration);\n    return configuration;\n  }\n@Test\n  public void evict() {\n    for (int i = 0; i < 2 * MAXIMUM; i++) {\n      jcache.put(i, i);\n    }\n    Awaitility.await().until(() -> {\n      for (Cache.Entry entry : jcache) {\n        jcache.get(entry.getKey()); // simulate usage to trigger pending evictions\n      }\n      return (listener.count.get() == MAXIMUM);\n    });\n  }\nprivate static final class RemovalListener\n      implements CacheEntryRemovedListener {\n    final AtomicInteger count = new AtomicInteger();\n@Override\npublic void onRemoved(Iterable<CacheEntryEvent<? extends Integer, ? extends Integer>> events) {\n  count.incrementAndGet();\n}\n\n}\n}\n``. I updated the test. I think the confusion is because of the asynchronous call to the eviction listener. Instead of blocking on a global lock on every write, atryLock` is used instead with a state machine to coerce a drain if replaying a write is still needed. The operations are buffered in a write-ahead log approach, which is defers and batches the penalties which is how concurrency is achieved.\nWhen you quickly write a bunch of values it triggers an eviction cycle and further writes move the state to REQUIRED. But that new write races with the async thread completing, which leaves it in the required state. When the next operation occurs, e.g. a read, then a new async cycle is scheduled. This means you can insert 20 items, but only 7 are evicted with the cache over capacity until the next read will evict the remaining 3. If the test doesn't do that it fails, which is why I simulate usage.\nThis high water mark generally works well in practice and the native APIs make it easier to test with, but the JCache API is more restrictive. If a same-thread executor was configurable then testing would be simpler, but without that option you have to do a little more work to coerce the state,. The only issue I can find is an unexpected merge behavior of on top of the default. If the default has the value set (e.g. maximum.size = 500) and the cache disables that (e.g. maximum.size = null), then on merge the result is 500. This fallback behavior bleeds through the underlying value when unset. To resolve this the query has to be more complex to identify that the configuration was disabled.\nInitial prototyping shows this method seems to work,\njava\nprivate boolean isSet(String path) {\n  return merged.hasPath(path) && customized.hasPathOrNull(path) && !customized.getIsNull(path);\n}\nwhere customized is the cache and merged has the fallback to default.\nBy chance is that related to your issue?. Can you provide your unit test? Or if you think the changes are good enough for a PR then bundle the test with that. I'm too detached from the problem to fully grok the flow without a debugger.\nThe CaffeineConfiguration constructor does a lot of copying, including from MutableConfiguration, so my guess was that it was assumed to have done that work. But I don't see how this loses the maximum size yet, so failing code to dig into would be appreciated.. Oh, I think I see what your code change does. I guess before it wasn't merging the defaults at all properly, and since the TCK fails there must be a gap. We have to merge the fields from Configuration or CompleteConfiguration interfaces into the defaults, most likely.\nI've been trying to fix travis and doing a bunch of forced pushes as I hack on the settings. If you can put a test failure together then I can help fix that between builds.. I think the assumption might have been that if you are not passing in a Configuration explicitly, then you are providing the full settings. But if you are resolving them from an external resource configuration, then any of those should be templated by the resource file's default type.\nWhat it sounds like you want is to provide the standard JCache type, but also get any defaults from the configuration file applied? If so, then we'd want to merge them in fully in the resolve (and actually follow what the JavaDoc states). If we do that then the TCK is probably failing because we have settings like maximum size interfering with its tests.\nOr if you think the two should continue to be independent, then defaults was a bad term and should have a pointer to a template. Then you could have multiple templates to customize on, rather than a global default.\nI guess it comes down to best honors the principle of least surprise. I'm also willing to redesign the configuration file for v3 for clarity.. This change passes the TCK and I think does what you wanted (and what I probably originally intended).\n```java\nprivate  CaffeineConfiguration resolveConfigurationFor(\n    Configuration configuration) {\n  if (configuration instanceof CaffeineConfiguration<?, ?>) {\n    return new CaffeineConfiguration<>((CaffeineConfiguration) configuration);\n  }\nCaffeineConfiguration template = TypesafeConfigurator.defaults(rootConfig);\n  if (configuration instanceof CompleteConfiguration<?, ?>) {\n    CompleteConfiguration complete = (CompleteConfiguration) configuration;\n    template.setReadThrough(complete.isReadThrough());\n    template.setWriteThrough(complete.isWriteThrough());\n    template.setManagementEnabled(complete.isManagementEnabled());\n    template.setStatisticsEnabled(complete.isStatisticsEnabled());\n    template.getCacheEntryListenerConfigurations()\n        .forEach(template::removeCacheEntryListenerConfiguration);\n    complete.getCacheEntryListenerConfigurations()\n        .forEach(template::addCacheEntryListenerConfiguration);\n    template.setCacheLoaderFactory(complete.getCacheLoaderFactory());\n    template.setCacheWriterFactory(complete.getCacheWriterFactory());\n    template.setExpiryPolicyFactory(complete.getExpiryPolicyFactory());\n  }\ntemplate.setTypes(configuration.getKeyType(), configuration.getValueType());\n  template.setStoreByValue(configuration.isStoreByValue());\n  return template;\n}\n```. I was planning on spending the 4th catching up on this project. Thanks for being patient and understanding regarding these issues. The JCache doesn't get as much attention as it is a quirky JSR, not very popular, and I don't use it myself.\nThe JSR really shouldn't have provided its own configuration classes, because all implementors have to extend them anyways. Since there is no bounds in the JSR's, its kind of useless. So my assumption was that almost everything would be defined in the configuration file since that is what the spec authors expected in their implementations, too. \nLet me dig into your test and see how things match up.. I think this case is wrong,\njava\ncheckCaffeineConfiguration(() -> cacheManager.createCache(\"test-cache-2\", cacheConfig), 1000L);\nFrom the JavaDoc it seems like either this should be honored as is and the declarative value ignored, or the declarative caches should be created immediately and an exception thrown. I'd have to dig into the other implementations to see if any eagerly resolve their caches rather than lazily as done now.\nFor the cache-not-in-config-file the changes pass your test.. Its not quite clear what the JSR expects. For createCache it states,\njava\nIf a {@link Cache} with the specified name is unknown the {@link CacheManager}, \none is created according to the provided {@link Configuration} after which it becomes\nmanaged by the {@link CacheManager}.\n...\nThere's no requirement on the part of a developer to call this method for each \n{@link Cache} an application may use.  Implementations may support the use of \ndeclarative mechanisms to pre-configure {@link Cache}s, thus removing the requirement\nto configure them in an application.  In such circumstances a developer may simply call\neither the {@link #getCache(String)} or {@link #getCache(String, Class, Class)} methods\nto acquire a previously established or pre-configured {@link Cache}.\n...\n@throws CacheException if there was an error configuring the {@link Cache}, which\n    includes trying to create a cache that already exists.\nThe configuration file is resolved lazily, so getCacheNames() is initially empty. At that point there is no cache with that name, so it would be created by the provided configuration. \nAlternatively we could eagerly initialize the caches, which would cause the method to throw a CacheException. Since you can destroy a cache, then its not clear if a named one would be reloaded, getCache would be expected to load it lazily, or if the configuration shouldn't be queried after initialization.. It looks like Ehcache loads all of the caches at initialization and is thereafter is ignorant of their configuration file. So in your case that should throw a CacheException and you would use have to getCache. If you think its cleaner to follow that approach, then I could switch it over.. And it looks like Hazelcast does it lazily.. If we merge the above change, then yes. This might help with the cache annotations which are very error prone because by default they are anonymously named, and the spec's configuration is unbounded. So a merge with caffeine.jcache.default could be helpful there.\nIf we make the config file resolved eagerly, then yes you would be forced to call getCache instead. But if you call destroyCache(name) then getCache(name) would return null and you would have to call createCache yourself.\nSince the spec lead worked on both Ehcache's and Hazelcast's providers, its confusing that they differ here.. I think the simplest refactoring is to disallow createCache when the name is defined externally. I could then refactor the CacheManager to eagerly resolve later. Then with the changes for defaulting, the exception should make the behavior clearer and you wouldn't have found it so surprising.\nI'm planning on doing an overhaul of JCache when I can bump the major version. That's blocked on Java 9 since I'd like to replace the core library's usages of Unsafe with VarHandle.\nOne flaw is that the configuration doesn't have a key-type and value-type, so you have to use getCache(name) and not getCache(name, String.class, String.class). Is that a configuration option that you'd like me to add while I'm in here?. Released 2.5.3. This is probably because processing is asynchronous by default. That leads to a race in a test that does not wait for them to complete. You can provide a custom executor to control this behavior. Use a same thread executor to disable it entirely, or a threaded that you shutdown and await on. It should work appropriately then.. I confirmed this was the cause. Here's an example using a threaded pool. You can use a direct executor (Runnable::run) to behave like Guava.\n```java\nExecutorService es = Executors.newCachedThreadPool();\nCache cache = . Caffeine.newBuilder().executor(es)\n...\ncache.cleanUp();\nes.shutdown();\nes.awaitTermination(10, TimeUnit.MINUTES);\nSystem.out.println(\"current time is \" + startTime + \" \" + System.currentTimeMillis());\nSystem.out.println(\"current cache size is \" + cache.estimatedSize());\nSystem.out.println(\"Total add \" + addCount.get() + \" and remove \" + removeCount.get());\n```\ncurrent time is 1499705555343 1499706155455\ncurrent cache size is 500000\nTotal add 37478050 and remove 36978050\nWhere 37478050 - 36978050 - 500000 = 0.\n. Yes, this is available in the asMap() view, which is an escape hatch to do more low level operations that wouldn't typically be done by an opinionated interface.. Yes, with the cause explicit. The writer won't be called, since you already have an explicit atomic block so it would be redundant / confusing.. Yes, but this would not be advisable. It would be equivalent to using ConcurrentHashMap with computeIfAbsent and always returning null.\nThe problem is that the hash table locks the bin, which can be coarser than a single entry though it is often one-to-one. The table won't increase the number of bins until it is populated, so the amount of concurrency is limited by the initial count. This will also have more overhead than a simpler scheme.\nWhat you want is called lock striping and can be done using a simple array of locks:\njava\nLock[] locks = new Lock[1024];\nArrays.setAll(lock, i -> new ReentrantLock());\nLock lock = locks[key.hashCode() % 1024]; // or & (locks.length - 1) if power-of-two sized\nlock.lock(); ... lock.unlock();\nOne can use a dynamic stripe if using a weakValued Cache<Key, Lock> with no other constraints. In that case the entry won't be eligible for collection until the lock is no longer used. This avoids unnecessary blocking due to hash collisions, but I've never seen it worthwhile in practice.\nOf course these patterns are available in Guava as Striped. So you could use that instead of rolling your own.. > I'd add that with striping, you may get a deadlock easily, ever for completely unrelated objects as they may get hashed onto conflicting locks. Good luck debugging. ;)\nIf the locks are reentrant then usually its not a big deal. But then anytime you use multiple locks without an ordering guarantee its deadlock prone. But I think that goes to the point that lock striping is good to know but should very rarely be used. There is usually a more elegant design that performs better and only makes sense in low-level code.\n\nI wonder when the cost matters? The code is even simpler than yours, there's one cache lookup and a Lock waiting for the GC...\n\nYeah, the cost probably doesn't matter. I think having a tested abstraction or implement it thoughtfully is more important than performance. I've seen too many incorrect versions on StackOverflow that it worries me :). Striped.lazyWeakLock does the caching trick, although its confusing by having a sizing parameter that may use a custom hash table. I think he over did it and made it not predictable enough, so I'd have done the same as you.\nSometimes striping is used for work distribution, e.g. see this StackOverflow thread. I feel like many people use it as a hammer that's often good enough, but there is a better abstraction out there.\nBut some, like you, do it right :)\nI'm starting soon on a row-level locking scheme that will have the deadlocking problem. Unfortunately I don't always know the rows in advance so I have to detect deadlocks, handle retries, and rollback. I've effectively modeled the application as a glorified document database, where usually each row is independent and goes through some complex processing. But sometimes there are side effects of creating or mutating other rows by a trigger, which requires them to be scheduled for processing. If two rows have side effects of needing to update each other synchronously, then its a deadlock. I can't see how to get around that with Sagas, so either one of them is rolledback completely or an asynchronous trigger must be used. I think that is good enough, but feels dirty by not solving it entirely. Its something I need to start implementing this week and have been spinning on hoping to see a better way.. > Are you going to do proper lock detection? Maybe you could just rollback and retry when it looks like a deadlock? Maybe you could use the information what rows were needed and lock them upfront?\nI'm still brainstorming at the moment. I'm not really settled with the design as it feels like error prone to quietly give up on a deadlock. That would be fine, but I don't think the programming models as imagined communicates that to a developer (often being.. me). See this brief proposal, though note that there is a lot of assumed context.\nI assumed that I would need to maintain a lease table (lock tracking) to retry with, e.g. when the task restarts due to a server shutdown. That would tell me what I already owned, hint at what I wanted, and who else was active in the system. The problem isn't in the technical details of deadlock detection, but the resulting programming model.\nIn the model an entity is a bag of data + metadata with a common CRUD api. For saving it does some light work on the caller's request, marks it as locked, and schedules async for heavy work which finally unlocks it. The metadata is used to dynamically construct an execution pipeline for what logic to run against the entity. This works great when entities are independent, but complex workflows require an entity to update other entities. Those updated entities would then be processed, where their business logic does some intelligent thing based on that state change. Then you abstract that up a level with UI tools based on your metadata and rules to easily support custom entities and workflows.\nI'm digressing from the original question, so ping me over email if you're interested.. Can you also try with an AsyncLoadingCache? For some reason I have the code,\njava\nlong refreshWriteTime = isAsync ? (now + Async.MAXIMUM_EXPIRY) : now;\nin refreshIfNeeded and I can't recall why they differ. It may be that both should simply be pushed forward to not be constrained by the refresh vs loading time.\nYou can use the synchronous() view to minimize the code change.. Okay. So I think what is happening is that when a refresh is needed, it CASes to the new write time to avoid a stampede of multiple. Then when the refresh completes it sets to the new value if the write timestamp matches. That way if you refresh but then insert, it won't stampede your insert and drop the refresh instead as likely a stale db read. Here it gets stuck because each new refresh being triggered causes the timestamp to increase, so the previous are dropped and the oldest value is kept until the last refresh completes. The async version is so far into the future that it won't hit that problem.\nAll of this is because we don't have a place to stash the Future. In Guava the hash table entry is subclassed with a special version for loading. That way the extra field(s) are only on the computing entry rather than on every single one. Caffeine doesn't fork the hash table so we can't do that, meaning I hacked the writeTime to get a similar effect.\nIt sounds like the resolution is to make it work like Async, add a unit test, and get a release out. I'll try to do that tonight if possible.. Yeah, just stealing time on the train and work trying to wrap it up. I'll release today when I get a chance.. Released. It may take up to 2 hours for Maven Central to sync with staging.\nThanks for the bug report and sorry for any frustrations this caused.. ouch, if that's true doesn't HashMap break the Map contract? I'll try to play with your snippet and think about this when I have some time, but at first glance its an annoying break in the collections.. java\n/**\n * Returns a {@link Set} view of the mappings contained in this map.\n * The set is backed by the map, so changes to the map are\n * reflected in the set, and vice-versa.  If the map is modified\n * while an iteration over the set is in progress (except through\n * the iterator's own <tt>remove</tt> operation, or through the\n * <tt>setValue</tt> operation on a map entry returned by the\n * iterator) the results of the iteration are undefined.  The set\n * supports element removal, which removes the corresponding\n * mapping from the map, via the <tt>Iterator.remove</tt>,\n * <tt>Set.remove</tt>, <tt>removeAll</tt>, <tt>retainAll</tt> and\n * <tt>clear</tt> operations.  It does not support the\n * <tt>add</tt> or <tt>addAll</tt> operations.\n *\n * @return a set view of the mappings contained in this map\n */\npublic Set<Map.Entry<K,V>> entrySet() {...}\nSince that's going through the APIs, I think my usage is correct and you might file a JDK bug? The iterator is supposed to be consistent.. I verified that your program passes in JDK7. Have you opened up a JDK bug or shall I?. Interesting note is that ConcurrentHashMap behaves correctly because its setValue also does a map.put(key, value). I think this is what the fix to HashMap should be. What do you think?\njava\n/**\n * Sets our entry's value and writes through to the map. The\n * value to return is somewhat arbitrary here. Since we do not\n * necessarily track asynchronous changes, the most recent\n * \"previous\" value could be different from what we return (or\n * could even have been removed, in which case the put will\n * re-establish). We do not and cannot guarantee more.\n */\npublic V setValue(V value) {\n  if (value == null) throw new NullPointerException();\n  V v = val;\n  val = value;\n  map.put(key, value);\n  return v;\n}. It's unfortunate that they untreeify. They don't shrink the capacity on removal by assuming the high watermark will occur again or the map will be short lived. It seems that their optimization makes the opposite assumption and breaks backwards compatibility to do so.. Any update?. @Maaartinus I think your example might be flawed because it assumes that writes to the map are visible to the entry. An entry's state should be a stable snapshot, outside of any direct mutations, and is not expected to change underneath the consumer. The collection classes tend to hand back a write-through entry rather than their internal one for this purpose.\n\nIterators, Spliterators and Enumerations return elements reflecting the state of the hash table at some point at or since the creation of the iterator/enumeration.\n\nSo I think the current behavior is correct, but perhaps subtle, detail of the Collections Framework.. In the case of HashMap that's true, but not for ConcurrentHashMap. That class still abides by the contracts and doesn't speak to breaking them, unlike say IdentityHashMap which states that it is not a general-purpose implementation. To me that means that an Entry may be live or a write-through copy, where the constraint is that it is stable for the given thread.. Thanks! I'll try to find the time to get this fixed and released. Great catch, btw.. Looks like the changes passed CI and its ready for release. I can do that now if this is impacting you in any way. If not, then I'll wait a little for #177 to see if there is another issue to fix. Since version numbers are free I am fine getting this to you, so please let me know.. Released!. @DougLea fixed the jdk bug, so I\u2019ll revisit this optimization in the future.. Sorry, I don't understand the problem. Can you provide a test case?. You may be observing that the removal listener is asynchronous from the operation. It will not be called atomically within the map operation that deleted that mapping and should happen sometime afterwards.\njava\n* Notifies the listener that a removal occurred at some point in the past.\n* <p>\n* This does not always signify that the key is now absent from the cache, as it may have already\n* been re-added.\nThat's nicer in the common case where you don't want to delay other writes or the calling thread, but only when the work is fairly decoupled. If you need to run code synchronously with the removal then you can try CacheWriter instead. The writer will be invoked within the atomic computation, so a failure bubbles up to the caller.\nI'm not sure if the rest is similar or not. Since the operations delegate to ConcurrentHashMap they should be atomic.. hmm, I am not sure then. The internal node might be recycled for expiration or weak/soft values, but that isn't the case here. I agree it shouldn't be able to give you a stale read after the mapping is gone, though a reader could obtain the value just prior to that.. I just checked all calls to BoundedLocalCache#notifyRemoval and it is invoked after the computation completes. If @jandam is using a version prior to 2.4.0, there was a stale notification bug causing the listener to observe the wrong value. A similar problem could exist, so I'm not discounting having bugs and flawed tests. If he's able to reproduce it in a unit test then hopefully I can work it out from there and resolve it.. In your test, I think there is an inherent race that can't be solved at the cache. The live value can be read from the cache by T1, the entry evicted and released by T2, and still in use by T1. A context switch makes that entirely possible. Imagine if you reduced the cache size to zero, you would expect that to happen too. Changing your code to that and it fails without threads. \nThe cache size and eviction policy make all the difference in your test, since entries are loaded and not reused. If the cache size is large enough then the race won't occur because eviction is delayed just long enough to not crash. Switching to Guava works at your cache size but fails at smaller. That's due to using LRU so the accessed entry is delayed by O(n) operations. Caffeine may evict much sooner to avoid cache pollution, which increases the hit rate by favoring frequency over recency.\nIn your case you need to provide additional synchronization around the value to handle races on mutable state. A simple approach is to synchronize on the page during usage, validating it before use under the lock. If that is too slow, you could use reference counting where the page has the count of in-cache + readers, the removal listener decrements, and the last reader performs the final clean-up when reaching zero. You might consider phantom references for this to let the GC do that implicitly so that releasing is delayed until the value is unreachable, e.g. a proxy. Then use the referent to lookup the page and release it. There are other techniques like optimistic reads with validation afterwards (e.g. StampledLock). Regardless, you'll need to manage these types of races.. No problem, happy to be of help.. @jandam I began a FAQ, so please let me know as you encounter gotchas we should add.. This seems to be the same problem that #177 ran into. An LRU would mask the race but not solve it, unfortunately. You probably will need to manage that concurrency yourself, though it often isn't too hard.\nThe only lock/unlock methods are asMap().compute. That would allow a single reader/writer, causing eviction to be blocked if the victim entry was in use.\nYou can disable size-based eviction using a weight of zero. I think that would get messy though, since you still have to track usage.\nYou might prefer using reference counting to allow evictions, but only delete when the count reaches zero. You can do that manually (where the cache increments on load and decrements on removal) and have the final reader performs the delete. You could delegate the counting logic to the GC by using phantom references instead.. I think that goes against our goals, where we want the eviction policy to not be depended on for business logic to work correctly. It tries to make the best prediction to retain the entries most likely to be reused. The behavior similar to an object pool influencing eviction, or making the policy configurable, begins to break down the idea of a cache holding recomputable, transient data.\nYou could use Guava and likely be okay, if that's the case. A cache doesn't seem like the correct abstraction, perhaps a work queue?. My concern with that is then the eviction could degrade to O(n), with the result of not being able to evict at all. Then when it can evict that notification likely won't be sent to the cache, so it will be over capacity until the next maintenance cycle (enough reads or a write). The penalty of the scan is okay in small caches, but not in large ones. So this works for small caches or when the advisor is rarely rejecting the candidate, but that means it is also likely to be abused by mistake.\nThe weight=0 has the advantage of no longer taking capacity in the cache, which you might consider good or bad. Currently it does require skipping over the entries, but technically it could be moved into its own queue. That's because the weight it communicated to the cache as part of a write operation so it can shuffle the entry.\nTo use capacity, a simple option is to have another map for active entries. You could have the cache pull from it during the load and the event remove it from the map. That wouldn't retain full capacity, unless you had a RemovalListener reinsert it into the cache. At that point you are forcing the entries to be pinned and masking eviction by this other map.. Note that weights are calculated for an entry when it is created or updated. So you could have a weigher that evaluated the entry, saw it was flagged as pinned, and gave it the weight of zero. In effect use a compute to pin and unpin.\nDefinitely things could go wrong with the reinsert hacks (assuming a re-insert is needed for a capacity constraint). Mostly that its clearly a hack of the known domain. That is a much more obvious smell then a feature that degrades poorly, where the code looks right due to the API but the subtle JavaDoc warns of the abuse.\nI think pinForMillis would get messy as more ways to shoot yourself and weird interactions between policies.\nMy main concern is over capacity and preventEvictionIf((k, v) -> true), which could be from a bug in the user's code. I know it is useful and those who want it wouldn't abuse it, but API design is about the edge cases. For example does it also stop expiration or clear? An explicit API is better in that respect by letting us know when to unfreeze it. There are just a lot of quirks of mixing in data that can't be recomputed and I don't know of another cache that has offered it successfully.. There's not a lot of usages with this, so do let me know if something doesn't work right. It looks like for put/replace, the weighing is outside of an atomic block by assuming the entry isn't changing. For computations / loads, it is in the block since the function has to be run atomically. We could ensure all weighing calls are within a entry lock, though it may not matter for you. (Its outside for explicit puts with memory estimating, e.g. see Cassandra).\n@wkuranowski would the weigher work for you?. Great! If any issues arise let me know and we'll sort them out.. I added a Faq for these advanced questions that don't fit into the main guide. Let me know if there are any additions.. Thanks for digging into the code! It is very complex and tests often catch my mistakes when making changes. Its just a lot that has to be packed into a few interfaces.\nIn this case code generation is used to minimize the number of fields on the Node implementations to just those used by the cache configuration. The generated class NodeFactory will override this value. Likely the code isn't called or, if it is, only when expireAfterWrite is solely enabled since it reuses that queue. If you are interested, the code generator is NodeFactoryGenerator.\nAs an aside, in this case I'd really like to encode those flags somewhere to avoid the extra field. ConcurrentHashMap takes a few bits from the hashCode, but that would require forking it to add ours into that.. A quick reference check and it looks like the calls are things like,\njava\nif (node.inEden() && (evicts() || expiresAfterAccess())) {\n   ...\n}\nSo we could switch the order and have getQueueType() throw an exception to be more explicit. I don't have a strong preference, do you?. The cache doesn't have manage any threads, but takes advantage of the JVM-wide ForkJoinPool.commonPool(). That can be set to a specific executor, perhaps a direct executor is preferred here, as the pool is primarily to mask small latencies from cache maintenance and other work.\nThere is memory overhead for an instance since it is designed for high concurrency, but I do try to be polite about it. Internally it uses ring buffers for concurrency, with the number growing as contention is detected up to a maximum. Thus if there is low demand there is a smaller footprint and memory is traded for additional speed as needed. The fixed overheads are reduced by code generating specialized classes to minimize the number of fields. That increases the jar size, but since few classes are loaded its just waste disk rather than memory.\nSo it should work without too much trouble. That said, if you have thousands then it may not be necessary. Thousands implies there is likely low concurrency, so a thread local or synchronized LinkedHashMap in LRU mode could be good enough. Caffeine might be good to start with, but its okay by me to choose a less fancy alternative when appropriate.. Can you provide more details? A reproducible test case would be perfect.. I should note that maintenance is asynchronous by default to minimize user-facing latencies. That means if you have a high write rate it will temporarily exceed the threshold, but should provide back pressure if excessive. For testing I usually use executor(Runnable::run) to force it to be performed on a calling thread.\nStresser is an example test that tries to overwhelm the cache.. Can you check the JDK version used in production? Earlier versions had a bug in ForkJoinPool causing it to lose tasks, including the async maintenance. You can use Runnable::run instead of cleanUp without much overhead most of the time.. hmm, okay. That issue was fixed in 60, I believe.. When I run Stresser using 16 writers, the eviction is rapid enough. It will exceed the threshold by a small amount, since a hard threshold would block all writes and rarely be useful.\n---------- 00:00:30 ----------\nPending reads: 0; writes: 577\nDrain status = Processing -> Required (3)\nEvictions = 11,612,513\nSize = 5,074 (max: 4,096)\nLock = [Locked by thread ForkJoinPool.commonPool-worker-6]\nPending tasks = 9,219,804\nMax Memory = 3,817,865,216 bytes\nFree Memory = 1,831,625,152 bytes\nAllocated Memory = 3,778,543,616 bytes\n$ java -version\njava version \"1.8.0_144\"\nJava(TM) SE Runtime Environment (build 1.8.0_144-b01)\nJava HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)\nCan you simplify your code to a failing test?. Any luck narrowing this down?. Well that's frustrating... The cache uses Unsafe as a replacement to wrapped volatile fields, e.g. AtomicLong, to avoid the overhead by directly operating on the heap fields. Of course if your entries have offheap semantics that must be freed, a writer or removal listener can be used. \nSorry that I can't be of much help and please re-open if you find anything I can assist in.. That seems reasonable. Thanks for adding the test case, too.\nLet me know if you want me to kick off a release.. right. without your change you couldn't get the additional statistics. Since versions are free, I'm fine releasing this for you if helpful.\nWhat are the internal Caffeine metrics that would be useful and not provided by CacheStatisticsMXBean? If its just registration, then perhaps JCache support should be proposed to the Prometheus project (or Micrometer which the Java client author's work on). I think Dropwizard added support by proxying the JMX values. (@checketts @jkschneider). I think also we'd need to have a flag to turn on Caffeine.recordStats() if you were to rely on unwrapping. Perhaps the internal stats could be proxied, but I think there were some minor distinctions due to expiration. The adapter was written prior to having native variable expiration, which added a lot of bridging complexity (I disliked the lazy or O(lg n) approaches; harder work but got O(1) via timer wheel in native). I plan on revamping the adapters in v3 (targeting jdk9) to hopefully simplify it, but that requires changing the configuration and therefore delayed to a major version bump.. I think you're right that a cache wouldn't make for a good queue. CLHM for instance is LRU only, since while FIFO is much easier it has a worse hit rate.\nI think you would need to clarify your exact needs. For example if the queue can evict if over capacity, then that presumes that it is non-blocking and always accepts writes. Does expiration need to be handled promptly or can it be delayed until polled by the consumer?\nIf we take the easiest cases then this could be implemented as a ring buffer. When the buffer it full it wraps around, swaps the existing value, and sends it to a listener. When polling, the call can check if the element has expired and, if so, retry. The logic for a concurrent non-blocking ring buffer is fairly simple, so I imagine adding these enhancements wouldn't be that difficult.\nOf course that's the best case. I do think you'll want to spin your own and I'd be happy to advise if you get stuck.. great! Note that my implementation is more low-level (Unsafe, false sharing) than you probably want. You'll probably want to use atomics, keep it simpler, and not worry about those extreme cases.\nClosing, but we can keep discussing if helpful.. That handles eviction order, but what about the consumer(s) polling? Wouldn't that degrade into a full scan on every offer/poll?\nI think we're going to struggle helping Bobby without more insight into his problem, acceptable trade offs, etc. I read the question as a Queue like class, but you might be right if it's more of a cache. My answer doesn't dig into the single or multiple producer / consumer contract. That can have a big impact on a concurrent algorithm (hint see JCTools). We might both be wrong and a much better solution exists because we jumped from the posed answer and didn't ask about the reason it's needed.\nHopefully he's already on the right track and we aren't making it worse with partial answers.. Quick fyi. If the problem is MPMC then the PTLQueue style algorithm might be the easiest to adapt. Then use a Turn object for an empty slot and catch up (evict) when a producer fails to acquire its slot. This way the writeCount position is an increment and there is less weird races on consumer as you can CAS its counter for acquisition and have an slot instance to manage. \nObviously things get confusing quickly which is why I'm hesitant going to deep into any suggestion.. @boschb Your evicting queue looks pretty good. Its definitely best to start at the simplest, iterate, and profile. Sometimes I get too deep into the woods and jump ahead to complicated solutions, e.g. the more custom circular queue. It sounds like any minor races a consumer has with the an evicting producer (causing unnecessary eviction) isn't a real problem. Since you'll have a bunch of them that problem and contention are unlikely to occur very often due to the nice balancing you described. You could probably switch to ArrayBlockingQueue if the linked node churn adds GC pressure.\n@christian-esken I think you might find my hierarchical timer wheel interesting, which is kind of like an incremental bucket sort. This is how Caffeine supports custom expiration policies, e.g. FIFO or by an external timestamp, in amortized O(1) time. That way it doesn't suffer from a O(lg n) incremental sort, periodic O(n) expiration scan, or leave pollution by relying on a size eviction setting. I don't think it could adapt to your caching approach, but you might find the technique interesting regardless.. The super types should let this call be more lenient. Can you provide an example causing problems?. This is the magic of the Gradle wrapper, you don't need to figure it out yourself. As the JavaDoc for the GetPutBenchmark states,\n./gradlew jmh -PincludePattern=GetPutBenchmark\nIn jmh.gradle it has the plugin settings. It states that you can use benchmarkParameters as...\nBenchmark parameters: Seperated by '&' for parameter types, and ',' for multiple values\nI'm usually lazy and edit the Java file locally when exploring, though.\nSome of the benchmarks are for checking for any performance issues or experimentation, rather than outside use. Since benchmarks are often wrong, misunderstood, manipulated, etc. I prefer minimalism to not over state (or imply), misreport by over complicating, etc in those that I describe. That's why you'll see just the two as I can explain them fairly, even though there are many other details that might be of interest. Those can be more difficult to explain without fear of misunderstandings, so best left to development and not to confuse users. For users showing if it is fast enough to meet their performance budgets is the primary goal.. I don't think it needs to be mentioned if you use the wrapper. That installs the version for the project defined in gradle/wrapper/gradle-wrapper.properties. Since that is the idiomatic way to run Gradle, I think it's better since upgrades are fairly regular. Of course sometimes that breaks plugins and I don't always check jmh when updating, so if I broke that let me know and I'll dig into a fix.. Okay, I use Eclipse but try to keep things working for IntelliJ folks. I run tasks at the command line. Since I don't check jmh or IntelliJ regularly, if it's busted then I can look into a fix. I guess we can close, but not a big deal either way.\nAlso feel free to fork the benchmarks to bootstrap your own suite, e.g. as cache2k and collisions did.. I did a quick test on the train and saw that I broke the benchmarkParams at some point. That should be fixed now, sorry. I gave it a non-scientific run so the numbers are rough as I am on a laptop, using battery, with other stuff running.\n./gradlew jmh -PincludePattern=GetPutBenchmark \\\n    -PbenchmarkParameters=cacheType=TCache_Lru,TCache_Lfu\nBenchmark                                (cacheType)   Mode  Cnt         Score         Error  Units\nGetPutBenchmark.read_only                 TCache_Lru  thrpt   10  30780838.627 \u00b1 4638449.427  ops/s\nGetPutBenchmark.read_only                 TCache_Lfu  thrpt   10  27041235.665 \u00b1  853354.312  ops/s\nGetPutBenchmark.readwrite                 TCache_Lru  thrpt   10  23615279.925 \u00b1 3828354.093  ops/s\nGetPutBenchmark.readwrite:readwrite_get   TCache_Lru  thrpt   10  19492469.512 \u00b1 3034092.917  ops/s\nGetPutBenchmark.readwrite:readwrite_put   TCache_Lru  thrpt   10   4122810.413 \u00b1  877353.105  ops/s\nGetPutBenchmark.readwrite                 TCache_Lfu  thrpt   10  28617429.557 \u00b1 2738865.133  ops/s\nGetPutBenchmark.readwrite:readwrite_get   TCache_Lfu  thrpt   10  24185446.587 \u00b1 2056324.626  ops/s\nGetPutBenchmark.readwrite:readwrite_put   TCache_Lfu  thrpt   10   4431982.970 \u00b1  727174.167  ops/s\nGetPutBenchmark.write_only                TCache_Lru  thrpt   10  17466518.407 \u00b1 5662496.764  ops/s\nGetPutBenchmark.write_only                TCache_Lfu  thrpt   10  19071051.318 \u00b1 2420626.652  ops/s\nGiven that your approach is a high watermark with async evaluation, in the past I remember thinking the numbers looked like TCache might be thrashing on a CAS field somewhere. The score looked like that type of contention (lock contention is similar but different numbers). I never dug in to find where, but using a profiler should find it readily. JMH's stacktrace might work, or I have CacheProfiler as a little testing tool for attaching YourKit to. I'm sure if you fix that it will perform closer to a raw ConcurrentHashMap as expected.. v1.0.4, so maybe jmh profilers will help. My ad hoc profiler is just for yourkit, but jmh's are more low level.\nCLHM decorates CHM so it might not be too insightful, unfortunately.. Nice! I didn't mean intend to enable statistics and it wasn't obvious when I wrote the code that they were on by default. We should probably disable it in these benchmarks to be fair/comparable to others. Glad the default behavior is better now!. It is atomic to other writes, as the removal operation is performed with a compute method. However, a read may be interwoven between writes, e.g. during the write or traverse a stale link chain (see ConcurrentHashMap). It sounds like you are assuming exclusive access for a read/write, which would incur a high penalty. The CacheWriter is an interceptor for ordering of writes to a particular key, but it doesn't block reads. You could perform all reads as a compute that returns the same value, but that degrades to the throughput of a write which is unlikely to be acceptable.\nMost likely you need to perform some type of liveliness validation, e.g. in your gist retry if the read is determined to be stale. You might prefer using phantom references for resource cleanup instead to ensure no direct usage. Its hard to advise without more details, as unfortunately these are the types of races you have to design for.. Actually in your case of lock striping the entry, there may not be a large penalty of using a cache.asMap().compute(key, (k, v) -> nv). That would stripe on the map's entries, which is documented as 1 / (8 * #elements). So delegating to the cache for locking might actually be simpler and offer similar performance.. When the Writer.delete call completes, the computeIfPresent hasn't yet returned. That means there is a gap when delete unlocks and ConcurrentHashMap fully removes the entry. There is also the possibility for a reader to walk a stale bucket chain, as in Java 5's it was immutably rebuilt on every write and may have similar properties in the Java 8 redesign. This is a small gap, but enlarged due to threads context switching out at the precisely wrong times.\nIf all operations were under a compute, then they are performed within a synchronized (entry) within the hash table. Then you have a guarantee that no read or write has a stale read because it must operate with exclusive access.\nI would probably have the read/write operations validate under lock and retry if they have a dead entry. That requires using a loop around the get call to re-read if necessary. This way you don't incur much of a penalty for the rare case that it has to retry, as most often the entry is still valid.. Oh, right. The compute method assumed that you didn't need an external lock anymore. But it sounded like you still wanted that, so it wouldn't be help.\nThe validate or retry seems to work in your test code. Good luck!. Nice job. You can also use cache.asMap.containsKey(key).. Sorry that I won't get a chance to dig into this immediately. Thanks a lot for the test case and I will try to prioritize it. I am trying to take a look tonight, but I can see how this week is going.. I think the reason is because you only redefined lazy-expiration.creation. The reference.conf (perhaps incorrectly) defaults creation, update, and access to eternal. That means the next read or update marks the entry as not expirable. I think this should have been defaulted to null to indicate no change.\nThe test was failing at the 3rd invoke and now fails on the 4th. Need to debug that next.. The second issue seems to be due to EntryProcessorEntry#setValue. The entry action is LOADED, but the if condition incorrectly resets it to UPDATED. If I update to check both CREATE and LOADED actions, it then passes your test case. It also continues to pass the TCK, so that change seems good.. Please try master (use snapshot repository). I ported your unit test and it seems to pass.\nI don't think that I can change the reference.conf defaults in a patch release, but maybe in a minor? So I'll hold off on that fix until the next bump.. Let me know how it goes. I can give you a release when I get the thumbs up.. Released. Why? It is immutable so a copy is probably unnecessary. You could use cacheStats.plus(CacheStats.empty()).. Thanks. Glad to see I\u2019ve mostly abided by semver, with exception of classes documented as exempt as beta. Perhaps a few lesser oversights early on, though.\nAny recommendations for where you would want this reported? I don\u2019t see an obvious spot in the wiki and it\u2019s not an issue to resolve. It\u2019s more a development tool like static analysis, which I assert in CI but don\u2019t publish.. Great. Then I'll close this and refer to it if anyone asks for a diff.. Note that you could use compute to handle both the absent and present case as a single atomic operation. If you want to avoid locking on an update, you could have computeIfAbsent return a counter so that updates are cache reads instead. Whether either of those helps depends on the workload.\nThe expireAfterWrite setting will update the expiration time for every mutation for a given key - meaning an insert or update. You can customize this policy by instead using Expiry if you want a more variable time, e.g. num * 30 seconds, and set different durations for read/create/update.\nFor rate limiting there are dedicated libraries, like resilience4j, that may help. They often implement a token bucket algorithm for the decaying behavior. For http/rpc calls, it is also popular to use a side car (like istio & envoy) to avoid special casing each call in an application.\nDoes any of that help?. Yes. Note that the policy, using your above configuration, will expire the entry after 1 minute from the time it was created.. My initial guess is that it is due to the compute time exceeding the expiration time. Currently the clock is read once to minimize expensive calls to System.nanoTime(). This is used to determine if the cached value has expired and is the writeTime set when computed. Thus, your threads might be having to recompute immediately, which causes the next one in line to recompute as well. That chain of C->B->A would lead to ~9 seconds and printouts that vary by ~3s depending on how long the chain was.\nWe could perform a re-read of the current time on the write to avoid adding the penalty of wait times and computation. For loads that are relatively quick compared to the expiration time it doesn't matter. For those like yours which are very tight it does. But I am also unsure if that's used and, if so, not already problematic? It would be nice to understand the use-case better.. I think that is fair. I don't know if I will get a chance to walk through the code during the week, but will try to get a release for you over the weekend.\nIf you use an AsyncLoadingCache then the behavior will be as you expect. Is that an okay short-term fix? You merely change build(loadingFunc) to buildAsync(loadingFunc).synchronous().\nThe async cache stores a future that subsequent callers block on and when it completes the write time is set, so that it doesn't expire while in-flight like you are seeing now. If we set it on the synchronous case to, then a waiter or subsequent read will get the full duration as expected. It only impacts narrow cases like yours, but the cost is tiny so I just need to work out the tests to assert the change.. Sorry that I didn't get a chance to work on this over the weekend. Hopefully I'll be able to catch up during my commute this week. Thanks for being patient.. Really sorry that I'm not on top of this. Usually try to have a week turn around on bugs. Just been hectic lately and hard to decompress enough to switch contexts over.. I found a little time Sunday night and I have the fix passing your test. I'll try to work on the unit tests on the train tomorrow. Again, apologies for not being able to work on this more promptly.. I stole some time at work to wrap this up. I'll release this evening if the CI passes.. Released. Thank you for being patient and apologies for being slow to resolve this.. This sounds kind of like my multi-way object pool prototype. I think the last commit broke it, but the previous one should work. It could be ported to Caffeine, since Guava was the bottleneck and the last commit was unfinished (and before Caffeine). It was just exploratory so unsupported and forgotten.\nYou might prefer to use maximumWeight to disallow unbounded collections exploding the cache size. But, it will evict the entire value which is undesirable.\nOtherwise, I am unsure what the question is.. Perhaps you want the asMap view and a computation? Then you could do compute, computeIfAbsent, computeIfPresent, or merge as atomic operations.. Makes sense. Be aware that compute is a locking operation, so it won't have a high throughput. In the benchmarks that's ~50M ops/s for a put regardless of core count. That's likely more than adequate, though, given the bottlenecks are likely elsewhere.\nThe MpmcAtomicArrayQueue might be unnecessary and an ArrayDeque used instead. You could create a local CompiledXQuery[1] that is populated within the computation. Then the calls outside of the computation check whether this array is filled or not. Since its on the same thread there are no races and no additional costs since you already use a capturing lambda. This assumes you only operate on the queue when inside a computation, since the hash table lock ensures exclusive access.. > I assumed that compute was lock per-key? Is it rather a single lock for the entire Cache?\nYes, you're right that it is roughly per-key. ConcurrentHashMap locks on the hash bin as a compromise and, assuming uniformly random writes, states that...\nLock contention probability for two threads accessing distinct elements is roughly \n1 / (8 * #elements) under random hashes.\nThis is a correct statement for the best they can predict for a general hash table, but in practice many workloads are not uniformly distributed. When that is used in a benchmark it is often idealistic (minimal contention) or to invalidate a competitor's optimizations (e.g. better caching). A good bet is to assume the Pareto principle, and even more so for caches which academics have shown follows the Zipf pattern. \nSo while there will be random placement in the hash table, your workload will likely favor certain keys. In a dedicated microbenchmark one might see a performance penalty due to contention. In practice, given the short hold time and real application work, it likely won't matter.\n\nI couldn't see how to operate on it in a thread-safe manner in borrowCompiledQuery\n\nYour current code is fine, so please don't take my suggestion as anything against JCTools! Merely a trick that I use in the cache to handle this same problem with lambdas, effectively final variables, and concurrent state. In these cases you are using volatile state in order to pass across the lambda boundary for data that is scoped to the same thread. You are using that queue to handle the race since another could steal your item.\njava\nCompiledXQuery[] query = new CompiledXQuery[1];\ncache.asMap().computeIfPresent(source, (key, queue) -> {\n  // do computation work....\n  query[0] = queue.poll();\n  return queue;\n});\nreturn query[0];\nHere the queue is never accessed outside of the key's lock, so it is not being operated on concurrently. An ArrayDeque or other non-threadsafe structure would be adequate.\nThe tradeoffs is that this trick isn't beautiful code, adds an allocation, and could add confusing logic. That allocation may be escape analyzed away and a mpmc queue read/write could be more expensive and adds a dependency.. I do recall refreshing being mind bending, so off-hand I can't speak to it without more thought. I'd first look into the prior issues (where some discussion took place) and Guava's behavior.\nWhen the refresh completes, the current code is...\n```java\ncompute(key, (k, currentValue) -> {\n  if (currentValue == null) {\n    return value;\n  } else if ((currentValue == oldValue) && (node.getWriteTime() == refreshWriteTime)) {\n    return value;\n  }\n  discard[0] = true;\n  return currentValue;\n}, / recordMiss / false, / recordLoad / false);\nif (discard[0] && hasRemovalListener()) {\n  notifyRemoval(key, value, RemovalCause.REPLACED);\n}\n```\nLike you said, when currentValue == null it is inserted. When discarded, currently it is assumed to be due to a replacement. I presume you would want that the value to be discarded and notified as a  EXPLICIT?\n\nMy initial guess would be due to how Caffeine#refreshAfterWrite is documented, specifially\n\nThe semantics of refreshes are specified in {@link LoadingCache#refresh}, and are performed by calling {@link CacheLoader#reload}.\n\nIn LoadingCache we state,\n\nLoads a new value for the {@code key}, asynchronously. While the new value is loading the\nprevious value (if any) will continue to be returned by {@code get(key)} unless it is evicted.\nIf the new value is loaded successfully it will replace the previous value in the cache; if an\nexception is thrown while refreshing the previous value will remain, and the exception will\nbe logged (using {@link java.util.logging.Logger}) and swallowed.\nCaches loaded by a {@link CacheLoader} will call {@link CacheLoader#reload} if the cache\ncurrently contains a value for the {@code key}, and {@link CacheLoader#load} otherwise. Loading is asynchronous by delegating to the default executor.\n\nThis is word-for-word (or nearly) the same as in Guava. Since a refresh of an absent entry will load the value and refreshAfterWrite is stated as having the same semantics, I believe the contract requires the current behavior.\n@yrfselrahc is the expert in this regard, since he wrote Guava's. I think the arguments would be,\n - Lower precedence to ensure the refreshed value is not stale, assuming the invalidate was after a system-of-record write.\n - Higher precedence to not waste work, since the cost was paid and may have been due to a eviction.\nIn retrospect I would lean towards stronger consistency guarantees.. It would be helpful if someone from the Guava team chipped in. (@kevinb9n @lowasser @yrfselrahc) since it would change drop-in compatibility if the specification was updated.. Internally there is a writeTime, as shown in the code above. That resolves the ABA problem for refreshAfterWrite, as you mentioned.\nUnfortunately, that is not the case for LoadingCache#refresh(key) as the timestamp may not be on the entry. It is only there if added by expireAfterWrite or refreshAfterWrite. Guava has a slightly easier time on this front due to forking the hash table, where the entry is special case replaced with a loading variant, so no timestamps are necessary. That means they could avoid an ABA concern. I think it could be okay if LoadingCache#refresh is allowed to have weaker semantics here.\nI don't have a strong opinion of whether this is a major or minor version change, according to semver. If we discard with the EXPLICIT removal cause then it seems fairly minor, since it clarifies the JavaDoc specification. But I am hoping to work on 3.0 as a Java 9 release, so it could be punted to that if there is opposition.. Wow, I feel pretty foolish.\nI think your test looks good. I don\u2019t recall why the configuration is cloned, but it might be for consistency of a different instance, e.g. if reusing the configuration to create multiple caches.\nI\u2019ve been swamped and trying to find the time to catch up. I have another fix promised but not sure if I can get to it soon. Is there a timeframe you need this released by?. Great. I'll try to catch up over the weekend, so I'll leave this open to prod me.\nIf you have any concerns or improvements on the JCache implementation, please let me know. Admittedly it doesn't get as much attention. There are upcoming changes in JCache 1.1 and I have the revised TCK mostly passing locally. I also plan on an overhaul for 3.0, when I get the time to start on it, so we could make braking changes if you have suggestions.. Released. Thanks again for PR!. Yes, I think it is a disconnect of expectations. I am open to changing this in Java 9, if we can work out the details.\nPassive expiration\nCurrently expiration is not viewed as a scheduling service, where an event would be fired when the timer elapses. This would require a dedicated thread to actively perform the task, but the cache itself does not create any threads. Your use-case is reasonable, but falls into the gray area of what should be expected from expiration.\nAs of now, expiration is performed passively. This is because the space was already taken and a time constraint is about freshness (not capacity). When an entry expires a miss is emulated if present, and it is discarded when the amortized maintenance cycle is triggered. Note that removing expired entries is O(1) for a fixed policy (LRU queue) and variable policy (timer wheel). The concept of a cache is to provide fast access to data within some policy constraints, so the passive behavior fits this purpose.\nThe maintenance work can be triggered explicitly using Cache.cleanUp(). That could be called by a ScheduledExecutorService on a fixed schedule, thereby causing your timeouts to be handled.\nInterceptors\nThe CacheWriter and RemovalListener are interceptors for when the hash table entry is manipulated. The writer is called within a computation method (see ConcurrentHashMap), thereby blocking other writes to that entry. As you observed, that hash table operation will not occur immediately upon expiration but be delayed until deemed necessary. The RemovalListener differs by being called outside of a computation after the write, so as to not block writes or cancel the write if an exception is thrown.\nActive expiration?\nJava 9 introduces a shared scheduler via CompletableFuture.delayedExecutor. This could be used to trigger a maintenance cycle when the oldest entry is set to expire. That scheduling would be O(lg n) due to the heap, the per-entry operations would remain O(1). Assuming the shared scheduler isn't being abused by others, its heap would remain small despite very large caches.\nThe problem is active expiration assumes a strongly guarantee of event order. If the timer is not fired because order was imperfect then this would be viewed as a bug. That can occur for read-based expiration policies (expireAfterAccess, Expiry's expireAfterRead) because the cache favors dropping reorder events to rather than induce latency when there is high throughput demands. In those cases the timestamps are still updated, but a weaker ordering is assumed acceptable since the external behavior is correct.\nIf active expiration was enabled, we would have to provide only best-effort for read-based but can guarantee it strongly for writes. That fits your expectations, but does add conceptual weight for anyone interested in this feature.\nAlternatives\nActive expiration might be outside of the scope of a cache. Its a fair debate. There are multiple ways you could perform the timing logic yourself.\nThe simplest is if you are on Java 9 and can use CompletableFuture.orTimeout. Assuming the future is the request, if timed out it could discard the cache's entry. If the request queue is combined with the request cache by using AsyncLoadingCache, then the failed future would be automatically removed.\nAnother possibility would be to schedule each entry in a ScheduledExecutorService. That is inefficient for large caches (e.g. millions), but reasonable for in-flight requests. That's less so if using a timer wheel variant, such as this non-hierarchical scheduler.\nOr, as mentioned above, if scheduled passively then you could have a periodic task that checks if there are any expired entries. Then you could still use the cache by calling cleanUp.. > I think especially that last sentence made me pretty sure that both writes and deletes would be synchronous. How else would a distributed cache know for example about the explicit eviction from another (cache) node? Maybe I'm not seeing this through though.\nIn passive expiration, there is no write until the cache notices the entry expired and cleans up. The actual mutation on the hash table is communicated to the CacheWriter. This will be delayed some time after the timestamp. Other eviction types (size, explicit) are active because there is an event, such as an insert, that can immediately trigger the work. For expiration, a scheduler thread is needed to trigger an event at some point in the future. The difference here is our expectations of how promptly the cache should detect that an entry has expired.\n\nI didn't really like the scheduled Cache.cleanUp(), that seems a bit too hack-ish to me. Also I would know what it would impose on the cache if I would schedule this every 100ms or so?\n\nI agree, 100ms is quite frequent and may seem wasteful when no work is needed. The overhead should be small due to using O(1) algorithms. This solution isn't great, but at least provides some support.\n\nIn the end I now schedule the task in a ScheduledExecutorService after adding it to the cache to check explicitly if the CompletableFuture was already handled or not.\n\nYou could have the request future cancel the scheduler's using a whenComplete. That would avoid running it for requests that have been successful.. Maybe we should have Camel in both sections? One is for projects using it internally, the other is for integrations for consumers to take advantage of. Camel is in both. Or perhaps we should include Camel only in the latter since that's more meaningful to users? If you render the README, you'll see its kind of a mess with this change due to how verbose the combined phrasing is.\nI guess I'd favor either leaving it as is, or moving Camel into the other section and linking to the documentation of camel-caffeine rather than the project. Thoughts?. How about we move it to the other section and change the link to,\nhttps://github.com/apache/camel/blob/master/components/camel-caffeine/src/main/docs/caffeine-cache-component.adoc\nPresumably that's useful for users and we should be more interesting in advertising that extension than an implementation detail.. I made a tad simpler change, if that's okay.. Oh, that is an clever hack! Thank you!. Cool, do you have a sense of the savings? and can you cherry-pick since I had rebased your earlier commit (so different hash)?. Oh, I see it in the commit message. even better. Yep, good call. Thanks again. :). I'm having an issue building with this change now. It looks like the imports are not being resolved, even though it should be in the same package.\n```\nben: caffeine $ gradle clean build\n\nTask :caffeine:javadoc\njavadoc: warning - Error fetching URL: http://typesafehub.github.com/config/latest/api/\n1 warning\nTask :caffeine:compileTestJava\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/BoundedLocalCacheTest.java:18: warning: auxiliary class BLCHeader in BoundedLocalCache.java should not be accessed from outside its own source file\nimport static com.github.benmanes.caffeine.cache.BLCHeader.DrainStatusRef.IDLE;\n                                                ^\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/BoundedLocalCacheTest.java:19: warning: auxiliary class BLCHeader in BoundedLocalCache.java should not be accessed from outside its own source file\nimport static com.github.benmanes.caffeine.cache.BLCHeader.DrainStatusRef.PROCESSING_TO_IDLE;\n                                                ^\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/BoundedLocalCacheTest.java:20: warning: auxiliary class BLCHeader in BoundedLocalCache.java should not be accessed from outside its own source file\nimport static com.github.benmanes.caffeine.cache.BLCHeader.DrainStatusRef.PROCESSING_TO_REQUIRED;\n                                                ^\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/BoundedLocalCacheTest.java:21: warning: auxiliary class BLCHeader in BoundedLocalCache.java should not be accessed from outside its own source file\nimport static com.github.benmanes.caffeine.cache.BLCHeader.DrainStatusRef.REQUIRED;\n                                                ^\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/IsCacheReserializable.java:261: warning: auxiliary class BoundedWeigher in Weigher.java should not be accessed from outside its own source file\n      if (weigher instanceof BoundedWeigher<?, ?>) {\n                             ^\n/Users/ben/projects/caffeine/caffeine/src/test/java/com/github/benmanes/caffeine/cache/IsCacheReserializable.java:262: warning: auxiliary class BoundedWeigher in Weigher.java should not be accessed from outside its own source file\n        weigher = (Weigher) ((BoundedWeigher<?, ?>) weigher).delegate;\n                                    ^\n6 warnings\nTask :caffeine:compileGenJava FAILED\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:16: error: package com.github.benmanes.caffeine.base does not exist\nimport com.github.benmanes.caffeine.base.UnsafeAccess;\n                                        ^\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:17: error: package com.github.benmanes.caffeine.cache.stats does not exist\nimport com.github.benmanes.caffeine.cache.stats.StatsCounter;\n                                               ^\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:36: error: cannot find symbol\n  static  BoundedLocalCache newBoundedLocalCache(Caffeine builder,\n                                                             ^\n  symbol:   class Caffeine\n  location: class LocalCacheFactory\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:37: error: cannot find symbol\n      CacheLoader<? super K, V> cacheLoader, boolean async) {\n      ^\n  symbol:   class CacheLoader\n  location: class LocalCacheFactory\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:36: error: cannot find symbol\n  static  BoundedLocalCache newBoundedLocalCache(Caffeine builder,\n                ^\n  symbol:   class BoundedLocalCache\n  location: class LocalCacheFactory\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:83: error: cannot find symbol\n  static class SI extends BoundedLocalCache {\n                                ^\n  symbol:   class BoundedLocalCache\n  location: class LocalCacheFactory\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:86: error: cannot find symbol\n    SI(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n       ^\n  symbol:   class Caffeine\n  location: class SI\n  where K,V are type-variables:\n    K extends Object declared in class SI\n    V extends Object declared in class SI\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:86: error: cannot find symbol\n    SI(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                               ^\n  symbol:   class CacheLoader\n  location: class SI\n  where K,V are type-variables:\n    K extends Object declared in class SI\n    V extends Object declared in class SI\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:100: error: cannot find symbol\n    private final Ticker ticker;\n                  ^\n  symbol:   class Ticker\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:102: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderEdenDeque;\n                  ^\n  symbol:   class AccessOrderDeque\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:102: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderEdenDeque;\n                                   ^\n  symbol:   class Node\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:104: error: cannot find symbol\n    private final Expiry expiry;\n                  ^\n  symbol:   class Expiry\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:106: error: cannot find symbol\n    private final TimerWheel timerWheel;\n                  ^\n  symbol:   class TimerWheel\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:110: error: cannot find symbol\n    private final MpscGrowableArrayQueue writeBuffer;\n                  ^\n  symbol:   class MpscGrowableArrayQueue\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:112: error: cannot find symbol\n    SIA(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n        ^\n  symbol:   class Caffeine\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:112: error: cannot find symbol\n    SIA(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                ^\n  symbol:   class CacheLoader\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:124: error: cannot find symbol\n    public final Ticker expirationTicker() {\n                 ^\n  symbol:   class Ticker\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:128: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderEdenDeque() {\n                    ^\n  symbol:   class AccessOrderDeque\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:128: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderEdenDeque() {\n                                     ^\n  symbol:   class Node\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:136: error: cannot find symbol\n    protected final Expiry expiry() {\n                    ^\n  symbol:   class Expiry\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:140: error: cannot find symbol\n    protected final TimerWheel timerWheel() {\n                    ^\n  symbol:   class TimerWheel\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:156: error: cannot find symbol\n    protected final MpscGrowableArrayQueue writeBuffer() {\n                    ^\n  symbol:   class MpscGrowableArrayQueue\n  location: class SIA\n  where K,V are type-variables:\n    K extends Object declared in class SIA\n    V extends Object declared in class SIA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:168: error: cannot find symbol\n    SIAR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n         ^\n  symbol:   class Caffeine\n  location: class SIAR\n  where K,V are type-variables:\n    K extends Object declared in class SIAR\n    V extends Object declared in class SIAR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:168: error: cannot find symbol\n    SIAR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                 ^\n  symbol:   class CacheLoader\n  location: class SIAR\n  where K,V are type-variables:\n    K extends Object declared in class SIAR\n    V extends Object declared in class SIAR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:187: error: cannot find symbol\n    private final WriteOrderDeque> writeOrderDeque;\n                  ^\n  symbol:   class WriteOrderDeque\n  location: class SIAW\n  where K,V are type-variables:\n    K extends Object declared in class SIAW\n    V extends Object declared in class SIAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:187: error: cannot find symbol\n    private final WriteOrderDeque> writeOrderDeque;\n                                  ^\n  symbol:   class Node\n  location: class SIAW\n  where K,V are type-variables:\n    K extends Object declared in class SIAW\n    V extends Object declared in class SIAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:191: error: cannot find symbol\n    SIAW(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n         ^\n  symbol:   class Caffeine\n  location: class SIAW\n  where K,V are type-variables:\n    K extends Object declared in class SIAW\n    V extends Object declared in class SIAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:191: error: cannot find symbol\n    SIAW(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                 ^\n  symbol:   class CacheLoader\n  location: class SIAW\n  where K,V are type-variables:\n    K extends Object declared in class SIAW\n    V extends Object declared in class SIAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:197: error: cannot find symbol\n    protected final WriteOrderDeque> writeOrderDeque() {\n                    ^\n  symbol:   class WriteOrderDeque\n  location: class SIAW\n  where K,V are type-variables:\n    K extends Object declared in class SIAW\n    V extends Object declared in class SIAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:197: error: cannot find symbol\n    protected final WriteOrderDeque> writeOrderDeque() {\n                                    ^\n  symbol:   class Node\n  location: class SIAW\n  where K,V are type-variables:\n    K extends Object declared in class SIAW\n    V extends Object declared in class SIAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:217: error: cannot find symbol\n    SIAWR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n          ^\n  symbol:   class Caffeine\n  location: class SIAWR\n  where K,V are type-variables:\n    K extends Object declared in class SIAWR\n    V extends Object declared in class SIAWR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:217: error: cannot find symbol\n    SIAWR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                  ^\n  symbol:   class CacheLoader\n  location: class SIAWR\n  where K,V are type-variables:\n    K extends Object declared in class SIAWR\n    V extends Object declared in class SIAWR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:236: error: cannot find symbol\n    private final RemovalListener removalListener;\n                  ^\n  symbol:   class RemovalListener\n  location: class SILi\n  where K,V are type-variables:\n    K extends Object declared in class SILi\n    V extends Object declared in class SILi\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:238: error: cannot find symbol\n    SILi(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n         ^\n  symbol:   class Caffeine\n  location: class SILi\n  where K,V are type-variables:\n    K extends Object declared in class SILi\n    V extends Object declared in class SILi\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:238: error: cannot find symbol\n    SILi(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                 ^\n  symbol:   class CacheLoader\n  location: class SILi\n  where K,V are type-variables:\n    K extends Object declared in class SILi\n    V extends Object declared in class SILi\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:243: error: cannot find symbol\n    public final RemovalListener removalListener() {\n                 ^\n  symbol:   class RemovalListener\n  location: class SILi\n  where K,V are type-variables:\n    K extends Object declared in class SILi\n    V extends Object declared in class SILi\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:253: error: cannot find symbol\n    private final Ticker ticker;\n                  ^\n  symbol:   class Ticker\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:255: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderEdenDeque;\n                  ^\n  symbol:   class AccessOrderDeque\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:255: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderEdenDeque;\n                                   ^\n  symbol:   class Node\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:257: error: cannot find symbol\n    private final Expiry expiry;\n                  ^\n  symbol:   class Expiry\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:259: error: cannot find symbol\n    private final TimerWheel timerWheel;\n                  ^\n  symbol:   class TimerWheel\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:263: error: cannot find symbol\n    private final MpscGrowableArrayQueue writeBuffer;\n                  ^\n  symbol:   class MpscGrowableArrayQueue\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:265: error: cannot find symbol\n    SILiA(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n          ^\n  symbol:   class Caffeine\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:265: error: cannot find symbol\n    SILiA(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                  ^\n  symbol:   class CacheLoader\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:277: error: cannot find symbol\n    public final Ticker expirationTicker() {\n                 ^\n  symbol:   class Ticker\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:281: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderEdenDeque() {\n                    ^\n  symbol:   class AccessOrderDeque\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:281: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderEdenDeque() {\n                                     ^\n  symbol:   class Node\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:289: error: cannot find symbol\n    protected final Expiry expiry() {\n                    ^\n  symbol:   class Expiry\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:293: error: cannot find symbol\n    protected final TimerWheel timerWheel() {\n                    ^\n  symbol:   class TimerWheel\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:309: error: cannot find symbol\n    protected final MpscGrowableArrayQueue writeBuffer() {\n                    ^\n  symbol:   class MpscGrowableArrayQueue\n  location: class SILiA\n  where K,V are type-variables:\n    K extends Object declared in class SILiA\n    V extends Object declared in class SILiA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:321: error: cannot find symbol\n    SILiAR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n           ^\n  symbol:   class Caffeine\n  location: class SILiAR\n  where K,V are type-variables:\n    K extends Object declared in class SILiAR\n    V extends Object declared in class SILiAR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:321: error: cannot find symbol\n    SILiAR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                   ^\n  symbol:   class CacheLoader\n  location: class SILiAR\n  where K,V are type-variables:\n    K extends Object declared in class SILiAR\n    V extends Object declared in class SILiAR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:340: error: cannot find symbol\n    private final WriteOrderDeque> writeOrderDeque;\n                  ^\n  symbol:   class WriteOrderDeque\n  location: class SILiAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiAW\n    V extends Object declared in class SILiAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:340: error: cannot find symbol\n    private final WriteOrderDeque> writeOrderDeque;\n                                  ^\n  symbol:   class Node\n  location: class SILiAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiAW\n    V extends Object declared in class SILiAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:344: error: cannot find symbol\n    SILiAW(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n           ^\n  symbol:   class Caffeine\n  location: class SILiAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiAW\n    V extends Object declared in class SILiAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:344: error: cannot find symbol\n    SILiAW(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                   ^\n  symbol:   class CacheLoader\n  location: class SILiAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiAW\n    V extends Object declared in class SILiAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:350: error: cannot find symbol\n    protected final WriteOrderDeque> writeOrderDeque() {\n                    ^\n  symbol:   class WriteOrderDeque\n  location: class SILiAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiAW\n    V extends Object declared in class SILiAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:350: error: cannot find symbol\n    protected final WriteOrderDeque> writeOrderDeque() {\n                                    ^\n  symbol:   class Node\n  location: class SILiAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiAW\n    V extends Object declared in class SILiAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:370: error: cannot find symbol\n    SILiAWR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n            ^\n  symbol:   class Caffeine\n  location: class SILiAWR\n  where K,V are type-variables:\n    K extends Object declared in class SILiAWR\n    V extends Object declared in class SILiAWR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:370: error: cannot find symbol\n    SILiAWR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                    ^\n  symbol:   class CacheLoader\n  location: class SILiAWR\n  where K,V are type-variables:\n    K extends Object declared in class SILiAWR\n    V extends Object declared in class SILiAWR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:413: error: cannot find symbol\n    private final FrequencySketch sketch;\n                  ^\n  symbol:   class FrequencySketch\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:415: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderEdenDeque;\n                  ^\n  symbol:   class AccessOrderDeque\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:415: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderEdenDeque;\n                                   ^\n  symbol:   class Node\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:417: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderProbationDeque;\n                  ^\n  symbol:   class AccessOrderDeque\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:417: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderProbationDeque;\n                                   ^\n  symbol:   class Node\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:419: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderProtectedDeque;\n                  ^\n  symbol:   class AccessOrderDeque\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:419: error: cannot find symbol\n    private final AccessOrderDeque> accessOrderProtectedDeque;\n                                   ^\n  symbol:   class Node\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:421: error: cannot find symbol\n    private final MpscGrowableArrayQueue writeBuffer;\n                  ^\n  symbol:   class MpscGrowableArrayQueue\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:423: error: cannot find symbol\n    SILiMS(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n           ^\n  symbol:   class Caffeine\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:423: error: cannot find symbol\n    SILiMS(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                   ^\n  symbol:   class CacheLoader\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:490: error: cannot find symbol\n    protected final FrequencySketch frequencySketch() {\n                    ^\n  symbol:   class FrequencySketch\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:494: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderEdenDeque() {\n                    ^\n  symbol:   class AccessOrderDeque\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:494: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderEdenDeque() {\n                                     ^\n  symbol:   class Node\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:498: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderProbationDeque() {\n                    ^\n  symbol:   class AccessOrderDeque\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:498: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderProbationDeque() {\n                                     ^\n  symbol:   class Node\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:502: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderProtectedDeque() {\n                    ^\n  symbol:   class AccessOrderDeque\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:502: error: cannot find symbol\n    protected final AccessOrderDeque> accessOrderProtectedDeque() {\n                                     ^\n  symbol:   class Node\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:506: error: cannot find symbol\n    protected final MpscGrowableArrayQueue writeBuffer() {\n                    ^\n  symbol:   class MpscGrowableArrayQueue\n  location: class SILiMS\n  where K,V are type-variables:\n    K extends Object declared in class SILiMS\n    V extends Object declared in class SILiMS\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:516: error: cannot find symbol\n    private final Ticker ticker;\n                  ^\n  symbol:   class Ticker\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:518: error: cannot find symbol\n    private final Expiry expiry;\n                  ^\n  symbol:   class Expiry\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:520: error: cannot find symbol\n    private final TimerWheel timerWheel;\n                  ^\n  symbol:   class TimerWheel\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:524: error: cannot find symbol\n    SILiMSA(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n            ^\n  symbol:   class Caffeine\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:524: error: cannot find symbol\n    SILiMSA(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                    ^\n  symbol:   class CacheLoader\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:532: error: cannot find symbol\n    public final Ticker expirationTicker() {\n                 ^\n  symbol:   class Ticker\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:540: error: cannot find symbol\n    protected final Expiry expiry() {\n                    ^\n  symbol:   class Expiry\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:544: error: cannot find symbol\n    protected final TimerWheel timerWheel() {\n                    ^\n  symbol:   class TimerWheel\n  location: class SILiMSA\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSA\n    V extends Object declared in class SILiMSA\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:564: error: cannot find symbol\n    SILiMSAR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n             ^\n  symbol:   class Caffeine\n  location: class SILiMSAR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAR\n    V extends Object declared in class SILiMSAR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:564: error: cannot find symbol\n    SILiMSAR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                     ^\n  symbol:   class CacheLoader\n  location: class SILiMSAR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAR\n    V extends Object declared in class SILiMSAR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:583: error: cannot find symbol\n    private final WriteOrderDeque> writeOrderDeque;\n                  ^\n  symbol:   class WriteOrderDeque\n  location: class SILiMSAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAW\n    V extends Object declared in class SILiMSAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:583: error: cannot find symbol\n    private final WriteOrderDeque> writeOrderDeque;\n                                  ^\n  symbol:   class Node\n  location: class SILiMSAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAW\n    V extends Object declared in class SILiMSAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:587: error: cannot find symbol\n    SILiMSAW(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n             ^\n  symbol:   class Caffeine\n  location: class SILiMSAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAW\n    V extends Object declared in class SILiMSAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:587: error: cannot find symbol\n    SILiMSAW(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                     ^\n  symbol:   class CacheLoader\n  location: class SILiMSAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAW\n    V extends Object declared in class SILiMSAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:593: error: cannot find symbol\n    protected final WriteOrderDeque> writeOrderDeque() {\n                    ^\n  symbol:   class WriteOrderDeque\n  location: class SILiMSAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAW\n    V extends Object declared in class SILiMSAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:593: error: cannot find symbol\n    protected final WriteOrderDeque> writeOrderDeque() {\n                                    ^\n  symbol:   class Node\n  location: class SILiMSAW\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAW\n    V extends Object declared in class SILiMSAW\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:613: error: cannot find symbol\n    SILiMSAWR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n              ^\n  symbol:   class Caffeine\n  location: class SILiMSAWR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAWR\n    V extends Object declared in class SILiMSAWR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:613: error: cannot find symbol\n    SILiMSAWR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                      ^\n  symbol:   class CacheLoader\n  location: class SILiMSAWR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSAWR\n    V extends Object declared in class SILiMSAWR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:632: error: cannot find symbol\n    private final Ticker ticker;\n                  ^\n  symbol:   class Ticker\n  location: class SILiMSR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSR\n    V extends Object declared in class SILiMSR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:636: error: cannot find symbol\n    SILiMSR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n            ^\n  symbol:   class Caffeine\n  location: class SILiMSR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSR\n    V extends Object declared in class SILiMSR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:636: error: cannot find symbol\n    SILiMSR(Caffeine builder, CacheLoader<? super K, V> cacheLoader, boolean async) {\n                                    ^\n  symbol:   class CacheLoader\n  location: class SILiMSR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSR\n    V extends Object declared in class SILiMSR\n/Users/ben/projects/caffeine/caffeine/build/generated-sources/com/github/benmanes/caffeine/cache/LocalCacheFactory.java:642: error: cannot find symbol\n    public final Ticker expirationTicker() {\n                 ^\n  symbol:   class Ticker\n  location: class SILiMSR\n  where K,V are type-variables:\n    K extends Object declared in class SILiMSR\n    V extends Object declared in class SILiMSR\n100 errors\n\nFAILURE: Build failed with an exception.\n\n\nWhat went wrong:\nExecution failed for task ':caffeine:compileGenJava'.\n\nCompilation failed with exit code 1; see the compiler error output for details.\n\n\n\nTry:\nRun with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.\n\n\nGet more help at https://help.gradle.org\n\n\nBUILD FAILED in 40s\n18 actionable tasks: 18 executed\n```. I think this works:\ngradle\ntask compileNoDebug(type: JavaCompile) {\n  mustRunAfter tasks.compileJava\n  source = \"${buildDir}/generated-sources/\"\n  destinationDir = compileJava.destinationDir\n  options.debug = false\n  options.incremental = false\n  classpath = sourceSets.main.runtimeClasspath + sourceSets.main.output\n}\nOr we can disable compileGenJava. Something like that.. Okay, I have it all working nicely. I'll check in a fix with some cleanup when I get a chance.. @jvassev It looks like the NodeFactory enum consumes ~100kb compressed. I don't think using reflection is appropriate in that case. There are a few ways to tackle it, though you might have a more clever idea than I can dream up :). I guess the only flaw is that this adds extra instance fields due to the factory node's unused ones? The alternatives would all have to add some, too, of course.. > There are no instance fields in the factory as it is now an interface.\nSure there is. WSo has a key and value field.. Yep, that's all I meant. It wasn't meant as more than an observation. That would be okay or you could make NodeFactory typed. Then no shadowing would occur.\nI'm not excited at the extra top-level fields, but I am also trying to see if there is any justification for that feeling. Its probably the right tradeoff.. When looking at the method handle approach, I guess it actually is worse because each MethodHandle holds a bunch of metadata fields. That prototype is at 680kb, with more work needed, but in comparison I think yours is nicer.. Oh I am definitely for removing the enum. It was just easy to write when trying to get the codegen working. There was a lot of upfront development costs to get this project going.\nI can merge this unless you are still iterating on any of it?. I typed the NodeFactory and its now at 700kb. Thanks! :). A few minor concerns.\n - All the classes are public, so we should make it generate package scoped.\n - The suppress warnings no longer applies. Would adding the annotation everywhere add back the bloat?. I changed the visibility and added the SuppressWarnings annotations. Since  the retention policy is SOURCE it doesn't end up in the byte code.\nThanks again :). This class is code generated by NodeFactoryGenerator.. I think any final methods that use the field should be inlined directly.\nI'm surprised expirationTicker is on the list, since the other expiration types have their own fields. I guess that's only due to making Expiry a field? I'd wonder if there was a better candidate since Expiry is very unlikely to be used, as after-write and after-read are based on a LinkedDeque. But then timerWheel is still generated, I presume, so it seems there's something here to review.\nI'm kind of swamped and burned out right now, so I might not get to this immediately. Thanks for this, though! :). I\u2019m sorry to hear this.\nI didn\u2019t think this could happen either, at least with the ForkJoinPool. If\na custom pool (even just commonPool()::execute) it uses a normal\nReentrantLock. I think you could switch to that until we have a fix ready.\nSee Caffeine.executor\nThe difference is negligible so we could switch over, as I was perhaps\noverly aggressive when experimenting. I would like to figure out a unit\ntest to assert the deadlock first, though.\nOn Tue, Nov 14, 2017 at 1:52 AM Agoston Horvath notifications@github.com\nwrote:\n\nUnder heavy load of JVM, we are experiencing deadlocks in caffeine version\n2.5.6.\nHere is the threaddump of the two threads in deadlock:\n\"ajp-0.0.0.0-8109-59 (P6V7aMaZbbXkfaYzYzXxID0tvHylFQJumanAAABmA)\":\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0x000000032a600bb8> (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n        at com.github.benmanes.caffeine.cache.NonReentrantLock$Sync.lock(NonReentrantLock.java:315)\n        at com.github.benmanes.caffeine.cache.NonReentrantLock.lock(NonReentrantLock.java:78)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1096)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:1017)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1655)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1602)\n        at com.github.benmanes.caffeine.cache.LocalManualCache.put(LocalManualCache.java:64)\nand\n\"ajp-0.0.0.0-8109-78 (-jIkyghhBePEsLll9i5dnGr65Dx8PfahGe2gAABxE)\":\n        at sun.misc.Unsafe.park(Native Method)\n        - parking to wait for  <0x000000032a600bb8> (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n        at com.github.benmanes.caffeine.cache.NonReentrantLock$Sync.lock(NonReentrantLock.java:315)\n        at com.github.benmanes.caffeine.cache.NonReentrantLock.lock(NonReentrantLock.java:78)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1096)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:1017)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1655)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1602)\n        at com.github.benmanes.caffeine.cache.LocalManualCache.put(LocalManualCache.java:64)\nLooking at the code, I can't imagine a scenario this could occur, but it\ndoes occur under heavy load of our JVMs.\nAny ideas what we could do? For now, as a workaround, I've added a\nsynchronized() block around the cache.put() methods to make sure only 1\nthread is adding new values to the cache at a time.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/203, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAXG9tZtHiwrHd_nAkTkRnGwUXyTupGtks5s2WLngaJpZM4QdEQj\n.\n. Looking at the stack, if these are the only threads blocked then it looks like a lost unlock signal. But of course I can't reproduce this using my Stresser utility. If this works fine with ReentrantLock, at least we'll know its a bug somewhere around this logic.\n\nThis code path should only occur when the write rate exceeds the drain rate, or that the drain task was lost such as due to the ForkJoinPool bug on older JDKs. The write buffer typically allows writes to make progress by batching the work onto the penalized thread, but if filled it will block on the lock. That improves performance by descheduling the threads, increasing overall throughput by reducing context switches.\nA few questions to help investigate:\n - Does using Caffeine.executor(ForkJoinPool.commonPool()::execute) solve the problem (trick it to use a ReentrantLock by not trusting the FJP).\n - Are there any other stacktraces using the cache not shown?\n - What is the cache configuration? (e.g. Is a CacheWriter doing heavy work?)\n - What version of the JDK are you running?\n - Any other suggestions for how to reproduce?. > I've checked ReentrantLock vs. NonreentrantLock, but in the end, they both end up in AbstractQueuedSynchronizer.acquire(), which is where the problem occurs.\nYes, but perhaps my NonreentrantLock has a bug that is hard to trigger. It is a slimmed down ReentrantLock since I don't expect recursive locking, and tried to build out everything upfront  and before I got demotivated by the weight of the project's complexity. While it is faster in a dedicated microbenchmark, it isn't a hot path and probably should be removed as unnecessary.\n\nI was thinking of replacing evictionLock construct by a simple synchronized(this). I'm not sure what the upside of using reentrantlock is, but is sure more error-prone.\n\nUnfortunately synchronized does not support try-lock semantics. When scheduling is performed (scheduleDrainBuffers), a tryLock is used to skip past it if another thread is taking care of it. That avoids unnecessary blocking and is simpler than trying to emulate it using a synchronized and CAS'd boolean, which might work but feel a little error prone.\n\nAll the stacktraces are in this code path, there is no difference.\n\nDo your logs include any warning like Exception thrown when submitting maintenance task? This would be if the executor threw an exception unexpectedly, such as a RejectedExecutionException. That shouldn't occur on a FJP unless the system is in a bad state, as it allows for up to 64M tasks. A problem like OutOfMemoryError might occur though, but the tests should cover that case. But I don't know when the lock could be left in a blocked state, resulting in a pile-up, if assuming it is a bug within BoundedLocalCache.\nSo I'd still recommend trying the trick to force ReentrantLock and seeing if this occurs. But other than that guess, there isn't enough information for me to debug with.. Nope, it would be an anonymous executor and use a ReentrantLock.\njava\n$ javarepl\nWelcome to JavaREPL version 425 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144)\nType expression to evaluate, :help for more options or press tab to auto-complete.\nConnected to local instance at http://localhost:53166\njava> import java.util.concurrent.*\nImported java.util.concurrent.*\njava> Executor e = ForkJoinPool.commonPool()::execute;\njava.util.concurrent.Executor e = Evaluation$7adjq3cgmtr50ps9z6b4$$Lambda$211/634150796@6a757ee\njava> e instanceof ForkJoinPool\njava.lang.Boolean res1 = false. Sounds good. I'll switch over regardless and close this when released, assuming this doesn't occur again. If it does occur afterwards, we should revisit with the new stacktrace. I won't get a chance to look at this until after the holidays, so that should give us some time to see if it reproduces.. Can you also try changing to use a ReentrantLock so we can see if it reoccurs?\njava\nCaffeine.newBuilder().executor(ForkJoinPool.commonPool()::executor)...\nThe inability to reproduce and eye balling the code, I haven't stumbled upon the likely cause. So I don't have much advice yet except bypass the most likely culprit.\nDo you see IllegalMonitorStateException in your logs? Also, can you list which methods you use on the cache? Perhaps the exclusiveOwnerThread is not properly set/unset causing it to block. There are explicit lock() calls in a few non critical paths, so perhaps its a race between lock/tryLock.. Yep, due to the holidays I don't plan on fixing it this week either =)\nThanks. invalidateAll() does use lock() since clearing is going to be an expensive operation and the alternative path would be slower. So next step would be to see if Stresser test could reproduce it with a mix of calls. However, this is done by MultiThreadedTest so I'd have expected it to fail during a CI run in the past.. Right, I'm trying to think of lock interactions that could be stress tested in an attempt to reproduce the problem. Maybe I should write a dedicated stress test on the lock class itself to see if I can break it, so we are sure about where the bug lies.. I'm sorry for the inconvenience this is causing. I'm surprised its happened all of a sudden, since that code wasn't changed recently and its had heavy usages and stress tests. I wonder if its a JDK change around memory fences, perhaps a newer optimization triggering this bug.. When I run the Stresser in write mode on 8_144, I see the lock holder change over time.\n```\n---------- 00:01:34 ----------\nPending reads: 0; writes: 1,024\nDrain status = Processing -> Required (3)\nEvictions = 325,998,568\nSize = 5,136 (max: 4,096)\nLock = [Locked by thread pool-2-thread-1]\nPending tasks = 2\nMax Memory = 3,817,865,216 bytes\nFree Memory = 158,083,696 bytes\nAllocated Memory = 481,820,672 bytes\n---------- 00:01:39 ----------\nPending reads: 16; writes: 153\nDrain status = Processing -> Required (3)\nEvictions = 343,609,572\nSize = 5,494 (max: 4,096)\nLock = [Locked by thread pool-2-thread-13]\nPending tasks = 0\nMax Memory = 3,817,865,216 bytes\nFree Memory = 155,875,704 bytes\nAllocated Memory = 318,767,104 bytes\n```\nThere are stacks of blocked threads, but that is because the write buffer was filled. Rather than being unbounded, it adds back pressure to block threads by helping to perform a full cleanup. In the common case writes are processed asynchronously to minimize user-facing latencies, and in the write storm it throttles when overwhelmed. The throttling improves throughput by reducing context switching.\n\"pool-2-thread-7\" #24 daemon prio=1 os_prio=31 tid=0x00007fcc0584a000 nid=0x7003 waiting on condition [0x0000700011e01000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000006c0ca3650> (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n    at com.github.benmanes.caffeine.cache.NonReentrantLock$Sync.lock(NonReentrantLock.java:315)\n    at com.github.benmanes.caffeine.cache.NonReentrantLock.lock(NonReentrantLock.java:78)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1100)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:1021)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1658)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1606)\n    at com.github.benmanes.caffeine.cache.LocalManualCache.put(LocalManualCache.java:64)\n    at com.github.benmanes.caffeine.cache.Stresser.lambda$run$2(Stresser.java:103)\n    at com.github.benmanes.caffeine.cache.Stresser$$Lambda$8/517380410.call(Unknown Source)\n    at com.github.benmanes.caffeine.testing.ConcurrentTestHarness.lambda$timeTasks$0(ConcurrentTestHarness.java:83)\n    at com.github.benmanes.caffeine.testing.ConcurrentTestHarness$$Lambda$9/1681433494.run(Unknown Source)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n    at java.lang.Thread.run(Thread.java:748)\nI would have expected the lock holder to be stuck and, potentially, the throughput to drop as if all threads get blocked. So this is frustrating since I cannot reproduce.... @agoston By any chance are you running this on Tomcat? In a private stacktrace from @levischuckeats, he mentioned the threads are from Apache Catalina (Tomcat's servlet pool). I know the Tomcat class loader can cause problems, especially when shared classes like FJP are in the mix. I wonder if it is an interaction because of the container... . Yep, if the system isn't generating writes then we'd all expect the cache to catch up and quickly be idle.\nA write cannot be dropped, since those are add/update/remove operations already performed on the hash table. Think of it as a database write-ahead log. The writes are applied in-memory, appended to a log, and replayed on the persisted form. In this way the expensive operation (random fsync) are batched and asynchronous, helping to scale throughput. The cache is similar, where the locking around the eviction policy is expensive because every read/write is a policy mutation. We can and do drop reads, which is the common case and non-critical state. But writes may effect things like require an eviction.. Any chance you have clearReferencesStopThreads enabled? That uses Thread.stop() and I\u2019m wondering if a redeployment causes the lock to be left in a bad state due to halted threads.\nAlso what version of Tomcat and did you recently upgrade?. I'm stuck then, because I can't work out the failure conditions.\n - All usages of the lock use the correct lock then try / finally unlock idiom\n - No calls have recursive acquisitions excluding foreign code\n - The shown configurations do not have foreign code that recurses under the lock\n - Tests against the cache (Stresser, MultiThreadedTest) don't show problems\n - An isolated stress test against the lock shows progress\n - No weird behavior like Thread.stop() to explain it away\n - Independent failures so not a user problem (e.g. infinite loop)\nThe only similarity so far is Tomcat which can be invasive, but not so much in your usages. I haven't used Tomcat in many years (preferring embedded Jetty) so I'm not good at debugging it anymore. But it also feels like a copout to presume its a weird interaction bug.\nI'm back to having to wait until sometime after the holidays when you both can test using ReentrantLock to see if the problem disappears or a similar stack emerges. If you have any better suggestions please let me know.. It would also be helpful if you track the cache stats. Then we could know for certain that eviction halted. Otherwise my testing only shows threads backed up, but progress continues. If I add a shutdown hook, then they all quickly complete normally. I cannot reproduce any hanging.. I spent a good amount of time on this today, since I had the day off. Unfortunately I am no closer but did discover a bug in the AsyncLoadingCache#synchronous()#asMap() view when it emulates a synchronous computation. That's not shown in your configurations, but thought I'd double check since it did allow for a deadlock scenario (e.g. future writing back into the cache for that key).. The only scenario that I can think, but cannot reproduce, is if FJP ever degrades to a caller-runs policy. If the pool detects that the caller is a ForkJoinWorkerThread and it cannot push to local or external queues, then perhaps the worker tries to run the given task directly. This would cause unexpected recursion when the drain task is scheduled under the lock, directly executed, and thus a deadlock. But nothing in FJP indicates it tries to help out this way, nor can I trigger any deadlock scenarios. This would be resolved using a reentrant lock, which is why we degrade to it for a non FJP, yet I cannot reproduce or eyeball code that might be the symptom.\n@agoston suggested JDK-8156765, but there isn't enough detail to debug with.. The asMap in a synchronous cache seems fine, which is what you've shown before. The asMap for an asynchronous cache (one that stores futures) had a potential deadlock bug. It has to emulate a synchronous cache for the view, since the entry is present but the future is in-flight. It was incorrectly waiting on the future within the map computation, and the stack traces were much more obvious. You would create such a cache using Caffeine.buildAsync.\nThanks for the additional information, though.. Thanks! I am particularly interested in two outcomes,\n a) w/ NonReentrantLock: Do evictions still occur or is that halted (evictionCount)\n b) w/ ReentrantLock: Does the error appear to be resolved?\nIf (a) then this would indicate it isn't a problem with the cache and a write storm is present, e.g. an infinite loop, causing it to be backed up. The write buffer grows to a decent size so that shouldn't happen very often, e.g. except when warming on startup.\nIf (b) then we can release with the switch. I'd really like to understand the problem, with a test to assert its fixed, rather than blindly change code. But I don't think its a bad change to do regardless.. To record statistics, please enable Caffeine.recordStats() and export it using Cache.stats(). The CacheStats includes useful information like evictionCount.\nThe default for Caffeine.executor is ForkJoinPool.commonPool(). If you put in your own non-FJP executor, we use a ReentrantLock since the interface allows it use caller-runs or throws an exception on rejection, both of which should not occur on a FJP. By using the method handle, it gets an anonymous Executor wrapper but still does the work in the pool, meaning just an extra method dispatch. The performance difference between the lock types went away when optimizing the internal state machine, so there isn't a justification (if there was one to begin with). So, the impact should be non-existent.. I have tried that mixed workload in the Stresser tool. However, I have only been testing on my macbook (4c/8ht). Perhaps this is only reproducible on the OS or CPU-family?. The write rate is quite high (17k/s) and the write buffer has a maximum size of\n128 * ceilingPowerOfTwo(NCPU), which I actually think might be too high. Assuming you have 32-cores, then this would absorb 4k concurrent writes before back pressure is applied.\nHowever, this should not deadlock and your stack traces should show a thread in the maintenance method doing the work.. I guess it isn't a dedicated machine, since that is a 12-core / 24-threads processor. Some cloud providers disable hyper-threading, so its either 8 or 16 \"processors\" according to Runtime.availableProcessors(). I guess we can estimate either a 1k or 2k write buffer size.\nGiven the write-rate is high enough to exhaust the write-buffer, you should see threads blocking on the lock. But you should also see a high throughput as the maintenance() work processes the buffer (up to the size limit for amortization). Thus a lot of writers should refill the buffer and progress, with the blocked writers context switched out to help improve overall throughput. Since the work under the lock is inexpensive and batched, there should be decent performance even under this high load.\nYou could play with the Stresser utility to see how it behaves and does a good job at progressing.\nAre you sure this is deadlocked and borked, or merely working through the backup? From the stacktraces you sent earlier it does seem deadlocked, but again I can't reproduce which is frustrating.\nOh, what OS are you running? We should try to spin up a similar instance and run the Stresser on it to see if its reproducable there. Then might be a kernel/JVM bug of a lost signal, which would explain a lot.. Thanks for your patience and helping to work through this. I hope its not causing too much disruption.\nThe clean-up thread is the preferred approach because it hides any of the maintenance work from user-facing threads. Whenever a write or enough reads occur, it is scheduled to be run. The writers then degrade to doing the work themselves if the buffer is full. Usually that's because the application is already using all of the threads in the pool for longer running tasks (e.g. rpc futures). The intent is that by default we hide cheap maintenance latency, but degrade to make progress otherwise.\nI don't know why all the threads are blocked trying to acquire, since the usage idiom of lock/unlock and AbstractQueuedSynchronizer appear correct. A lost unlock() signal that is OS/CPU specific is the only explanation so far. But if ReentrantLock solves it, then that's better to be my bug then us digging into kernel tickets. . I wonder if you are hitting this Linux lost wake-up bug? In this thread by Gil Tene, it seems to be occurring on Ubuntu 15.04:\n\nI'm seeing a similar problem on kernel 3.19.0-16-generic after having upgraded Ubuntu to 15.04 (Vivid Vervet). However, checking the github link above indicates that the fix should be in 3.19, so I'm not sure if this is a regression or something else.\nI've documented what I'm seeing here: http://ubuntuforums.org/showthread.php?t=2278238&p=13285416#post13285416 (I'm not really familiar with Linux debugging, though, so I'm not sure if I'm reporting the right information.)\nBasically, certain Java applications are regularly hanging, and when I run strace I see them in FUTEX_WAIT. So far it's affecting only Java, but this includes previous versions of Java that have been working for a long time.\nPaul Blair\n\nAnother user reported that the patch is in your version, but he still sees the problem in JDK8:\n\nI'm seeing the same thing on the same version of Ubuntu with a Haswell when running Jetbrains CLion (which runs in Java).  It hangs on a futex and I have to kill the program to unlock it.  Binding it to a single core seems to avoid the hang but kills performance.  I checked Ubuntu's version of the 3.19 kernel source and the patch is in there.\nIt does seem to be somehow related to Java 8 vs Java 7.  When I installed Oracle JDK 7 and forced CLion to run it, I no longer see the hang.\nJack Bradach\n\nAnd finally it was stated that Ubuntu 16.04 is fine,\n\nI never found the exact fix but I do know that 4.4.0-65 (Ubuntu 16.04 LTS)\nis also fixed so somewhere between that and at least 4.2.0-18 lies a problem\nDaniel Worthington-Bodart. @agoston Can you provide the cpu and operating system for your servers?\n\nSo far, the kernel bug is the most likely culprit since I cannot reproduce on OS X. I haven't yet tried debugging on a Haswell / 15.04 box, which I think would be a good next step. If the Stresser fails there and then passes on 16.04, that should confirm it. I won't get a chance until maybe this weekend, unless anyone else wants to give it a go.\n@levischuckeats Do you think your fleet could be upgraded to 16.04 LTS or newer? (Note that it is YY.MM, so you are running a 2.5yr old release). Thanks @agoston. In that thread,\n\nThanks Gil,\nI can confirm kernel-3.10.0-229.4.2.el7 shipped with Centos7.1 is impacted.\nIt has 3.14 b0c29f79ecea0b6fbcefc999e70f2843ae8306db backported but not the 3.18 fix: 76835b0ebf8a7fe85beb03c75121419a7dec52f0\nI filed a bug with Centos:\nhttp://bugs.centos.org/view.php?id=8803 \n-adrian\n\nSo I think the kernel bug is a good candidate. It would explain your deadlock in Ehcache, too, if their infinite loops appeared correct. Do you think that you could upgrade your fleet's OS?. I presume this has been working fine? If so, I'll make the switch over and release. I'll probably close this sometimes in January if it doesn't reappear.. Is this with the executor tweak or without?. Released. I haven't had a chance to reproduce on a VM, but this removes the need for the executor trick to coerce the lock type.. I plan on closing this at the end of next week, unless there is new information to point to a cause other than the Linux kernel bug.. Thanks!\nThe Linux commit acknowledges that it had deadlocks. So if possible please do upgrade.\n\nThe consequence is a race with a thread waiting on a futex on another CPU, allowing the waker thread to read \"waiters == 0\" while the waiter thread to have read \"futex_val == locked\" (in kernel).\nWithout this fix, the problem (user space deadlocks) can be seen with Android bionic's mutex implementation.... Thanks guys!. Its worth noting that people who have observed this deadlock, have also seen it on other caching libraries. That hints towards it being the kernel bug since we cannot reproduce it elsewhere.\n\n@agoston had mentioned switching to Caffeine from Ehcache since he had observed a deadlock there.\nSome Gerrit users have switched from Guava due to observing an unexplainable deadlock there. They also saw it occur in Caffeine at a lower rate. I had mentioned the kernel bug and, while Redhat had claimed the patch was in place, they were planning on upgrading regardless.\nI am suspicious mostly because while I may have made mistakes, it is odd that other mature libraries have deadlocks for those users too. Those being recent too, despite the development of those libraries being stagnant. I was not able to reproduce it locally (mac) and didn't have the time to spin up a Linux box to test with. That would be our next step if you're believe a kernel update won't fix it.. It could help us narrow this down if you upgrade the dependency. The NonReentrantLock was removed as a defunct optimization, given other prior work, so a normal ReentrantLock is used. That at least reduces the surface area of my code that could be wrong. I have not been notified with a deadlock using that lock, so maybe that helps resolve this case?. Sorry to hear that. A failing test case would be wonderful if you can produce one. When I review the lock usage it explicitly follows the idiom lock, try.., finally.. unlock. We may need a JDK engineer if we have a failing test, since I can't eyeball the problem (but more eyes the better, so take a look).. @ractive any luck reproducing this? Sorry that I cannot be of much help.. I\u2019m not sure how to help, unfortunately. What kernel and jdk?. In that type of setup, you would use Redis as a look-aside cache with a local cache. You would use a CacheLoader to fetch from the remote tier and then fallback to the database query. When an update is performed, typically in those setups one writes to the remote tier and then the local cache as two distinct operations. However that leaves a race, where another remote update happens and the local cache has a stale entry due to the newer value being overwritten. So it is a better practice to always invalidate or refresh the local cache and rely on loading instead.\nThat race could be avoided by making the local cache responsible for writing to the remote tier, too. This is effectively changing from Map.put to Map.compute so that other writes to that entry block while the remote call is in-flight. The CacheWriter is merely a domain oriented way of changing the internal map call, rather than doing it yourself using Cache.asMap.\nSimilarly CacheWriter.delete changes from Map.remove to Map.computeIfPresent. Unfortunately that mapping is wrong, because the local cache will be much smaller than the remote one. Thus, an explicit removal might be a no-op on the local cache but should still propagate externally. Instead the local cache should use Map.compute and pass a null value to CacheWriter.delete. That could be argued as an API change as it is not JavaDoc'd that way, so I've backlogged it to v3. Thankfully this probably hasn't been an issue because most often deletes aren't propagated to the cache as one instead allows the dead entry to be floating garbage that will be evicted due to inactivity.\nIn the case of Redis, you will probably want to use its pub-sub capabilities for cache synchronization. That way all your local caches could be invalidated or refreshed promptly due to the notification.\nI hope that clarifies things. I probably won't have the time to wrap it up and put it into the examples folder, but would happily accept a PR if you want to give it a shot.. That's true if you have a fixed TTL in the local cache (expireAfterWrite). However if you use variable expiration (expireAfter(expiry)) then the TTL can be synchronized assuming no clock skew. The cost is still O(1) due to a clever data structure called a TimerWheel.\nAlternatively, you might listen for expiration events from Redis and invalidate the local cache. However that won't be instantaneous due to their use of random sampling.. One recommendation that I forgot, which I plan to do at work someday, is to pipe the WAL into the caching tier. There are good examples of doing that with Kafka, and I hope to setup AWS Kinesis to do that from Postgres for ElasticSearch. A consumer could ingest into Redis, which ensures that the data is fed in after the transaction completes.\nThe benefit of using the WAL is that there are no race conditions on the server-side. If the server is shut down after the transaction completes but before the redis update, then stale data is left in the cache. Alternatively you would need a 2-phase transaction, which is undesirable. If you use the pub-sub capabilities to invalidate the local caches, then the span of holding stale data should be very short assuming the consumer is working.\nShowing all of that in an example is more work, especially since its been in my backlog at work so I haven't gone through the process yet. I haven't had many problems with the more ad hoc update after the transaction, but the WAL is nicer if you have additional storage tiers to sync with.. You could invoke it directly within the loader, e.g. delegate to a common method.\nThe problem is that a writer is usually meant to update some external resource. If say the db record was updated through the cache using a writer and cache.put, then the cache mutation would only occur if the transaction was successful. If it failed, e.g. not null constraint, then the cache write would be lost. If the CacheLoader loaded the entry and those writes were communicated, then for every load it would trigger a db update.\nYou're case is similar, the load might be present or absent in the remote cache. You only want the CacheWriter to be notified if the remote cache missed and the loader then queried the db. There isn't a way to communicate that through the loader. But if an explicit write like cache.put` occurred, you probably would want that pushed to the remote cache too.. Nope, the loader will insert it into the cache. It would be something like,\njava\nLoadingCache<Key, Graph> cache = Caffeine.newBuilder()\n  .maximumSize(10_000)\n  .writer(new CacheWriter<Key, Graph>() {\n    public void write(Key key, Graph graph) {\n      redis.put(key, graph); // called on explicit put() or replace()\n    }\n    public void delete(Key key, Graph graph, RemovalCause cause) {\n      if (cause == RemovalCause.EXPLICIT) {\n        redis.delete(key); // or don't bother and let it be LRU'd out\n      }\n    }\n  }).build(key -> {\n    Graph graph = redis.get(key, Graph.class);\n    if (graph == null) {\n      graph = db.selectFrom(GRAPH).where(GRAPH.KEY.eq(key)).fetchOne();\n      redis.put(key, graph);\n    }\n    return graph;\n  });. @stevelaw any feedback on your efforts so far? And if so, on any additions we should add to the examples directory?. Closing, but I would gladly accept contributions to the examples section. Expanding to include tutorials on popular frameworks, scenarios, etc seems like a nice idea.. I don't quite grok the expiration logic. Why not let memcached expire the entry itself, rather than fetch an expired entry client-side and reload? I'm not sure if it helps, but there is CacheLoader#reload method you can override.\nThe high-level step for the loader looks correct:\n1. Fetch from memcached\n2. If not present, load from source, push into memcached\n3. Return loaded value. Is it possible to reproduce in an isolated unit test that could be shared?\nThe first obvious check would be to inspect legacyChannelIdsCacheSize in case its near zero. You could print out the maximum using cache.policy().eviction().get().getMaximum() to see it from the cache's perspective.. 435982 - 435482 = 500. Another possibility is that you are always passing in UUID.randomUUID() rather than an existing key.. Thanks for debugging this. The fix seems pretty simple.\nMost likely we should also fix the test weighers, etc. in CacheSpec to assert non-null key/value when appropriate. I think this is why the null tests pass since they assert the entry being discarded, but the configured classes don't NPE due to not inspecting the value.. I'll try to have a fix out shortly after the holidays.. Released. Sorry this took a while to get out as it was held up while resolving other bugs.. Please take a look at Policy.isRecordingStats(). :)\nI try to not pollute the core interfaces to minimize the conceptual weight. Policy and asMap are nice escape hatches for lower-level behavior, with Policy being useful for all the ad hoc sort of things.. AbstractLoadingCache doesn't carry its weight and I think including it in Guava was a mistake. The code is trivial, it isn't used by the implementation, and doesn't assist much in a new one. LocalLoadingCache does a bit more, but was intended to be internal so I am hesitant to open it up. A full fledged cache would be fairly complex, so likely the reuse is very minimal.\nI think you'd be better served implementing the methods directly. Since AbstractLoadingCache doesn't do anything intelligent, you should be able to write similarly simple code.. Great. Also note that ConcurrentStatsCounter is our equivalent to SimpleStatsCounter, as that's perhaps the only class that is useful.. A recursive computation is not supported in Java\u2019s maps. ConcurrentHashMap live locks and HashMap corrupts itself. In Java 9 both throw exceptions in those cases and the JavaDoc was updated.\nGuava will throw an exception if the same key is recursed to. It\u2019s not an intended feature to allow it otherwise, but hard to detect.\nA solution is to use the synchronous view of an AsyncLoadingCache. The mappings will be established immediately with a future value and the computation scheduled. Then it will only deadlock if there is a circular dependency in the future chain.. Note that in your example, you can use AsyncLoadingCache#synchronous() to get a LoadingCache view. So hopefully the impact on your code is pretty minimal if you decide to use that trick.. hmm.. yes. I guess you would have to use Executors.newCachedThreadPool() instead to safely generate new threads. The deeper the recursion goes, the worse it gets as each step requires a new thread. Unfortunately there isn't much we can do here to help the situation.. Thanks for the test, again! :)\nI'm swamped today, but I'll try to look at this on my way home (PST). I owe people a new release so I should be able to get this out soon. Not sure about the fix yet until we both dig in a bit.. hmm.. you are right that this is confusing. I originally followed the reference implementation, but the JSR authors do not consider it good quality. The specification is primarily the interface JavaDoc pasted into sections rather than additional depth.\nThe right thing is to probably sketch out the scenarios from scratch, using the examples in EntryProcessor's JavaDoc as guidance. I think part of the confusion is that it states Only the net effect of multiple operations has visibility outside of the Entry Processor, but it also states weird things like The first get and the second put only are visible to the ExpiryPolicy which means it isn't entirely flattened out. So I don't know if we can rely on a single state machine, should use a mixture of flags, or a queue of events.. The fix passes your test. The lifecycle is still very confusing, so please let me know if you discover other bugs. Adding the simple check of whether the entry was originally present in the cache seems to pass all the tests.. Released. This has been in my backlog to see how other projects handle it (notably Guava). In particular due to the JSR having poor history of recommendations on module names and we'd likely want to follow Stephen Colebourne's advice instead.\nOne issue that keeps coming up is the jsr305 annotations and the lack of good alternatives. You might run into issues there due to package conflicts, even though I think the JSR spec was modified to allow it. If we switch, we might need to make the annotations a required dependency to be Scala friendly, since only jsr305 is well known / tolerated and the partial alternatives aren't. Of course then some Java users are frustrated due to their builds failing due to unused transitive dependencies, so it becomes a bit murky.. Added the manifest entries. You may still need to deal with javax.annotation as concerns have been expressed in other projects. This will be in the snapshot repository if you'd like to verify before the next release.. Released. The timetamps should be updated using whenComplete, e.g. LocalAsyncLoadingCache#get. Can you list the methods you use on the cache so that we can verify each has this completion logic? Also what is your executor set to?\nInternally when a future is in-flight it does get the duration of Long.MAX_VALUE to disable expiration. Any updates to that timestamp are done under the entry's lock, so it should never be stampeded on. When the future succeeds it should then callback into the cache using replace(key, future, future) to trigger the weight & expiration time calculations.. Do you think that you could write a failing test case? I don\u2019t see the cause yet when trying to reproduce it.. Yes, it can except for the synchronized (node) section, where a node is the entry metadata that wraps a value. The first snippet will ensure the writeTime is at the maximum when in-flight, while the second makes a callback using cache.replace(...) which goes here. That too has a synchronized (node) when updating the write time, so we should expect exclusivity.. I wonder if this is due to refreshAfterWrite, which is pretty complicated. That seems the most likely culprit from walking through the code, but I don\u2019t have a scenario yet.. That\u2019s too bad. I had a theory with refresh, but now I\u2019m unsure. What JVM and OS are you running? I have had issues reported that we found were due to bugs in the platform itself.. I believe #203 is caused by a kernel bug. This causes lost wake ups, in that case a deadlock. Perhaps here it\u2019s resulting in the future not to run its completion handlers. It\u2019s a wild guess but your version is old enough. . My belief is that in that case, the worse that could happen is the entry is set with too old of a write time, causing it to expire immediately. Since we use the delta and the timestamp is read on the same thread, there shouldn't be any races or word tearing. A slow computation could result in the premature eviction, but not cause what you are seeing (a wraparound would take 288 years). We should probably move the read, but I don't think its causing this issue.. It will run concurrently, except for the portions under the synchronized (node) blocks.\nWouldn't hasExpired be very large (since we think the entry is in-flight) until the replace finishes? Then it will set the timestamp to a real value, e.g. 0 in your example? Shouldn't the lock ensure we don't stampede the writeTime due to a context switch?. oh! I see what you mean. Yes, that's a real possibility. Lets definitely fix that.. I think that I made the proper changes here. The latest build should be in the snapshot repository shortly. Can you use that and see if we fixed the problem?. Released. I think the preferred means to export statistics is using a metrics reporter. Spring uses Micrometer which has native support for Caffeine, so it should be easy to use. Then the metrics library handles reporting to different endpoints, be it json / jmx / graphite / etc.\nSince the metric libraries, such as Dropwizard's, don't use bean-bindings this typically hasn't been an issue. So I'd prefer keeping the Guava-style API as is and I think you'll get much more by leveraging a metrics system. Thoughts?. I'm going to close for now. I think its too high impact to change and remove the old names and mostly cosmetic. It may not always be perfect, but such is API design. Thanks!. By default the policy work is performed asynchronously. This is to avoid locking which would blocking the caller, so instead policy work is batched when possible. After a read or write, an operation is appended to a bounded queue and draining this queue is scheduled on an executor. For reads this occurs after a threshold while writes are immediate. This causes the maximum to be exceeded temporarily, but to not expose a penalty as user-facing latency. The queues are bounded so this can never run away under load.\nFor unit tests, you can either use a same-thread executor (Caffeine.executor(Runnable::run)) or call cleanUp(). Both force the calling thread to perform the work. Since the policy is O(1) the work is tiny, as its really the locking that gets expensive under load that we're trying to avoid.. The statistics are documented as \"monotonically increasing over the lifetime of the cache\" so this is not directly supported. You can emulate it using the CacheStats.minus method, e.g.\n```java\nvolatile CacheStats snapshot = CacheStats.empty();\nvoid clear() {\n  cache.invalidateAll();\n  snapshot = cache.stats();\n}\nCacheStats stats() {\n  return cache.stats().minus(snapshot);\n}\n```\nAlternatively, you can provide your own StatsCounter and reset the metrics.\nGenerally this isn't a problem if you export your statistics into a time series database, e.g. using Dropwizard Metrics. Then one can run computations, like a time window, over the data set. These systems tend to expect increasing values rather than each actor performing its own computations, like histograms, by delegating that work to the metrics application.. The rational when designing this for Guava (Kevin's comment):\n\nThe stats that tend to matter most are things like \"stats over the last 2 minutes\" and \"stats over the last 15 minutes\" etc. If you reset, you'll be resetting constantly! And you will have to pick one increment or the other. If you reset every minute, then to get the stats for the last hour you have 60 things to sum up.\nIn contrast, just saving absolute snapshots and diffing any two endpoints you want to is a lot simpler and more flexible.\nGoogle's monitoring tools are designed so that components who want to report stats only have to worry about reporting monotonically increasing counts, and they handle the rest, which I think is a good approach that keeps the burden of things like \"resetting\" from being shoved onto every individual component that wants to report stats.\n``. Yep, please see my [StackOverflow answer](https://stackoverflow.com/questions/28840047/recursive-concurrenthashmap-computeifabsent-call-never-terminates-bug-or-fea/28845674#28845674). I raised the issue during the unveiling of the rewrite, and helped resolve it for JDK9. I had hoped they would backport the fix since it was early after JDK8's release. It turns out thatHashMap` corrupts itself in JDK8 in this scenario and they now throw an exception in JDK9 as well.\n\nIn the FAQ there is a workaround discussed using AsyncLoadingCache.. Hope that helped and thanks for checking in about it.. Sorry about that. We should add some unit tests against this then.\nThe time calculation should be tolerant to overflows by following the advice in System.nanoTime. It should be using t1 - t0 < 0 instead of t1 < t0 to avoid sign assumptions. Its easy mistake to do, so if I did that again then we should fix it. Otherwise, an overflow should be okay.\nIf I recall correctly, the Async situation is slightly different. It is used as a signal that the future is in-flight and should not be evaluated for expiration. Since the future can complete anytime, it is helpful to have that flag separated for querying the future's state. By capping the expiry that a user can set, it lets us differentiate between the states. But I don't recall enough if that safe-guard was necessary, as there is a lot of mental overhead to keep track of these details.\nCan you write a unit test and we can fix your issue?. Hi @facboy,\nI did a quick sanity check and was unable to reproduce this issue. We'll keep digging and make sure there is test coverage. Did you observe this bug or was it a mistake when reading the JavaDoc?. In case this helps, here is my passing test. Hopefully you can tweak it to fail and we can understand the problem.\n```java\n@Test\npublic void issue217() {\n  FakeTicker ticker = new FakeTicker();\n  ticker.advance(ThreadLocalRandom.current().nextLong());\n  Cache cache = Caffeine.newBuilder()\n      .ticker(ticker::read)\n      .executor(Runnable::run)\n      .expireAfter(new Expiry() {\n        @Override public long expireAfterCreate(Integer key, Integer value, long currentTime) {\n          return Long.MAX_VALUE;\n        }\n        @Override public long expireAfterUpdate(Integer key, Integer value, long currentTime,\n            long currentDuration) {\n          return Long.MAX_VALUE;\n        }\n        @Override public long expireAfterRead(Integer key, Integer value, long currentTime,\n            long currentDuration) {\n          return Long.MAX_VALUE;\n        }})\n      .removalListener((k, v, c) -> {\n        Assert.fail(k + \" \" + v + \" \" + c);\n      })\n      .build();\n  cache.put(1, 2);\n  assertThat(cache.getIfPresent(1), is(2));\n  assertThat(cache.estimatedSize(), is(1L));\nticker.advance(1, TimeUnit.MINUTES);\n  assertThat(cache.getIfPresent(1), is(2));\n  assertThat(cache.estimatedSize(), is(1L));\n}\n```. Thanks! That is very odd, but something we can debug with. Interestingly it takes 1.5M operations to fail, though this is using real time instead of a fake source so that may be understandable. Since my test suite primarily uses TestNG, here is a slight adaption to run there.\n@rash67 wants to get his hands dirty, so hopefully he can take a look. If not, I'll try to resolve it over the weekend.\n```java\n@Test\n@SuppressWarnings({\"unchecked\", \"RedundantStringConstructorCall\", \"InfiniteLoopStatement\"})\nvoid maxExpiryShouldNotExpireImmediately() {\n  LoadingCache cache = Caffeine.newBuilder()\n      .expireAfter(new Expiry() {\n        @Override\n        public long expireAfterCreate(Integer key, String value, long currentTime) {\n          return Long.MAX_VALUE;\n        }\n        @Override\n        public long expireAfterUpdate(Integer key, String value, long currentTime,\n            long currentDuration) {\n          return Long.MAX_VALUE;\n        }\n        @Override\n        public long expireAfterRead(Integer key, String value, long currentTime,\n            long currentDuration) {\n          return Long.MAX_VALUE;\n        }\n      })\n      .removalListener((k, v, c) -> {\n        //Assert.fail(k + \" \" + v + \" \" + c);\n      }).build(key -> new String(\"10\"));\nint counter = 0;\n  String expected = cache.get(10);\n  Stopwatch stopwatch = Stopwatch.createStarted();\n  while (stopwatch.elapsed(TimeUnit.MINUTES) < 1) {\n    String val = cache.get(10);\n    assertThat(\"Failed after \" + counter + \" iterations\", val, sameInstance(expected));\n    counter++;\n  }\n}\n```. To expire without any interaction with the cache would require a thread to schedule on. In Guava's Cache timeframe, there were some environments like JavaEE and AppEngine that disallowed creating threads. It is a fairly high cost when typically the promptness is not necessary.\nJava 9 added a JVM-wide scheduler (via CompletableFuture), which we might leverage as an option. I am way behind on a JDK9 version, unfortunately.. Yes, the timerwheel is a passive data structure. There is a nice article regarding Kafka's implementation that schedules on a thread. They combine the timerwheel with a ScheduledExecutorService (heap-based) and schedule the bucket instead of the entry. This way the heap is bounded to O(lg # of wheels) instead of O(lg # of timers), which in our case would be a maximum of O(lg 165) ~= 7.. #195 is basically asking for this. It has definitely come up many times for Guava, etc. The cost of maintaining dedicated threads is high for a library, and while we can use FJP in JDK8 it doesn't support any scheduling semantics.\nThe hope is that we can use optionally enable using CompletableFuture.html#delayedExecutor to trigger a cleanUp() on the next expiration event. This uses a global JDK-wide scheduling thread, so we do not impose our own. We might want to offer a simple Scheduler abstraction for unit tests, etc. (like we did with Ticker for time). I don't know what we'd call the enablement of this feature, as it would be off by default.\nThat's the idea of how to do it properly, at least. I am pretty behind (on this bug fix and v3.0 release in general).. @facboy Can you try testing with .executor(Runnable::run) added to the builder? It doesn't fail for me then.\nI think the issue might be that threads read different timestamps when calling System.nanoTime(). When enough reads occur, they are replayed on the policy asynchronously. Then it somehow overflows and goes negative. I haven't debugged more than that, but it seems like a clock synchronization issue if my theory holds.. It\u2019s likely that the async thread has an older time stamp and the overflow occurs since it\u2019s a tight boundary. I just need to better accustom myself to this. I think that the solution you gave will be the right fix, though.. I generalized the test to a pass/fail scenario, showing that the concurrency due to threading was the cause. When the read buffer is full (~16 reads per stripe), the async maintenance is triggered. The entry continues to be read, bumping its expiration time beyond the overflow point. This means that the maintenance's currentTime read is less than the entry's currentTime read, so t1 - (t0 + duration) will overflow with Long.MAX_VALUE. If all operations are using a single thread, this cannot occur due to order of operations causing linear timestamps. Since the unit tests default to same-thread cleanup for simplicity, it was an oversight on my part.\nI updated the internals to always enforce a Long.MAX_VALUE / 2 as you suggested to provide enough leeway for the overflow to not occur in practical code.\nI'll make a release tomorrow and will let the CI do its thing tonight. Thanks for reporting the issue.. Released. Thanks!. Thanks for the PR!\nI think using 16 makes the most sense. I must have misread the default constructor of ConcurrentHashMap and thought it would default to zero when unspecified and lazily update. My intent was to have the same defaults as the underlying map, not to reduce it. Can you confirm that 16 resolves your scenario? Is this something we can add a unit test for?. I have heard of usages creating thousands of caches (per-request), e.g. Gatling. That's not the expected use-case, but I do try to be friendly to whatever users throw at it. So for example the internal structures are lazily allocated when possible.\nMy expectation would be that the cache would resize as it fills up, thereby increasing to an acceptable concurrency.  Most caches will be reasonably sized, so it should increase upwards. This would then follow into the behavior as documented of,\n\nLock contention probability for two threads accessing distinct elements is roughly 1 / (8 * #elements) under random hashes.\n\nIs my understanding that the concurrency increases as the capacity increases incorrect? So this would only impact the warm-up, assuming most operations are reads?. That's true. But as you said, any setting is arbitrary. Do you think we should add something to the JavaDoc (perhaps on Caffeine.initialCapacity) or the FAQ? If the FAQ, can you provide suggested wording and I'll update it?\nIt is a flaky test. The coverage might fluctuate due to multi-threaded tests.. Thanks a lot! I really appreciate that you debugged this and provided a patch, rather than updating the settings on your side and moving on. I updated the FAQ with your wording.. Released. Thanks!. Please take a look at cache.policy(), which allows inspection and operations specific to a configuration. For example Policy.Eviction#getMaximum() is your use case.. Good point. I didn't see the benefit of providing stable iteration order based on the input. It would be interesting to know if specifying this would be a nice-to-have feature or unimportant. I don't see the harm, but also don't see the benefits.\nThe main reason why I decided not to replicate this was another possible bug. Guava and Caffeine do not consider weak keys in their result mapping. The weak key configuration uses reference equality and identity hash code for lookup. If k1 and k2 are logically equal but different instances, then there may be two entries in the cache. When the result of getAll is computed, the keys are deduplicated and only one of them is returned. This is undocumented behavior in Guava, though noted in Caffeine. If anyone raises an issue about it then I'd want the flexibility to switch to IdentityHashMap, which is unordered.. Released. Thanks! (now has Guava ordering, but similarly not doc'd). I guess we should write some tests for these and try to resolve them. From my reading of Guava's code, this was the behavior and so I followed it (even though I thought it was odd as not fully ordered). In Guava's LocalCache#getAll(keys), a simplified version is below. You'll see a peek into the cache (Map.get(key)), a bulk load, and appending to the earlier results. I don't see a point where the results are reordered by the input keys order.\n```java\nImmutableMap getAll(Iterable<? extends K> keys) {\n  Map result = Maps.newLinkedHashMap();\n  Set keysToLoad = Sets.newLinkedHashSet();\n  for (K key : keys) {\n    V value = get(key);\n    if (!result.containsKey(key)) {\n      result.put(key, value);\n      if (value == null) {\n        keysToLoad.add(key);\n      }\n    }\n  }\nMap newEntries = loadAll(keysToLoad, defaultLoader);\n  for (K key : keysToLoad) {\n    V value = newEntries.get(key);\n    result.put(key, value);\n  }\n  return ImmutableMap.copyOf(result);\n}\n```. Oh! It clicked what Guava was doing and I wrote a quick test to verify. Sorry that I misunderstood and implemented it wrong.\nThe first result.put(key, value) will insert a null value when the entry was absent. It will then perform the loadAll and finish populating the map, causing these entries to be fully present. If a key was not found by loadAll then an exception is thrown. Since its a LinkedHashMap the order is preserved.\nSince I didn't notice the use of null values, I naively assumed it out of order. Sorry that I didn't write tests to verify that.. Don't you think the builder readability is worse, e.g. Duration.ofHours(5) vs 5, TimeUnit.HOURS? As you had mentioned, you can pass around a duration and use duration.getSeconds() to convert, so there isn't much of a penalty. I would also use Duration in my application code for normal usages.\nI don't see the drawback of the pair. What am I missing?. Sorry, maybe I'm being obtuse. Wouldn't the user pass around java.time.Duration and then deconstruct to the pair only when building the cache? The raw long would be my penalty for internal code, but not exposed to the user.. Yep, so it adds a little convenience. Do you think that carries its weight?\nIn most of your usages, wouldn't you often externalize using the spec parser? Then you'd write expireAfterWrite = 1h in the configuration file. I thought their was a convenience parser for CacheBuilderSpec as a single variable for Google's binding (I've done similar using Typesafe Config for CaffeineSpec).. Okay. I'm actually surprised because my recollection was the Guava team tended to dislike small overloads like this as not adding a significant benefit. Obviously if possible there were many refactorings to update usages to a newer convention, rather than allow rot. But I always got the sense that this was one of the API design differences from how Apache Commons approached things. When I reviewed against Bloch's rules, my thinking was to prefer being consistent with j.u.c, Guava, and that I hadn't observed a problem.\nDid dropping sub-second precision have a negative effect. I know CacheBuilderSpec only allows down to seconds and expiring early shouldn't typically have much of an impact.. I am certainly curious of what other opinions are.\nI respect yours enough that I am inclined to add it.. Thank you both. I am surely not going against two opinions that I look up to.. With regards to low-level inspections in cache.policy(), should methods use java.time instead? This would mean the current versions would be removed in v3.0.\njava\ninterface Expiration<K, V> { // To be renamed FixedExpiration in version 3.0.0\n  OptionalLong ageOf(K key, TimeUnit unit);\n  long getExpiresAfter(TimeUnit unit);\n  void setExpiresAfter(long duration, TimeUnit unit);\n  Map<K, V> oldest(int limit);\n  Map<K, V> youngest(int limit);\n}\njava\ninterface VarExpiration<K, V> {\n  OptionalLong getExpiresAfter(K key, TimeUnit unit);\n  void setExpiresAfter(@Nonnull K key, long duration, TimeUnit unit);\n  boolean putIfAbsent(@Nonnull K key, V value, long duration, TimeUnit unit);\n  void put(K key, V value, long duration, TimeUnit unit);\n  Map<K, V> oldest(int limit);\n  Map<K, V> youngest(int limit);\n}. The interfaces are public (I shortened them for readability here). This hides all the nasty details from the Guava-style core interfaces, while also giving the functionality needed for some use-cases.\njava\n// Modify the cache's expiration time\ncache.policy().expireAfterWrite().ifPresent(policy -> {\n  policy.setExpiresAfter(1, TimeUnit.MINUTE);\n});. Yes, defaults would be used here. For now they would call the existing methods to adapt from.\nLong term I'd prefer having only one style, since it adds a lot of noise, so I'd remove some in v3. I think you'd prefer to have the java.time style promoted, so I would use that if possible. Just wanted to confirm and I'll add them.. Released. Thanks!. Thank you. Do you have any recommendations for how we should approach this? We could either wait until JDK10 (March) or rely on reflection instead. . In our case of instantiating the cache, the benefits are probably moot. For individual entries we create them directly. Since method handles were advocated by the JDK team it seemed equally valid for us to use, and below you'll see them being ~3x faster. I'm okay reverting back since the usage is pretty minor.\nBenchmark                                   Mode  Cnt         Score         Error  Units\nFactoryBenchmark.direct                    thrpt   10  80030436.680 \u00b1 1273539.573  ops/s\nFactoryBenchmark.methodHandle_invoke       thrpt   10  62577082.484 \u00b1 2720153.408  ops/s\nFactoryBenchmark.methodHandle_invokeExact  thrpt   10  62964398.800 \u00b1 2959115.745  ops/s\nFactoryBenchmark.reflectionFactory         thrpt   10  24098580.087 \u00b1  428434.865  ops/s. I suspect only P1 bugs are put into LTS releases. These bugs in the Collections Framework seemed serious enough to backport and isolated code to be done without much effort, yet they decided not to.\nJDK-8071667: HashMap corrupts itself with recursive computations\nJDK-8142175: ConcurrentHashMap live-locks with recursive computations\nJDK-8186171: HashMap's breaks the Iterator contract for Entry.setValue. @johnou I think that I have the next release ready, so let me know if you have remaining concerns. If so, any suggestions on what you'd like to see done? The only option I can think of is to inspect the Java version.. Released. Thanks!. Yes, I would like to reduce the visibility of implementation details and only expose the API. This way I can modify algorithms and optimize it for my needs, rather than provide a generalized interface. You are welcome to copy it into your own project.\nWhen I started this project, I thought it might expand beyond caching so I made more things public (e.g. UnsafeAccess, SingleConsumerQueue, and a few now hidden). That was a mistake as its complex as is and the singular focus is perhaps better, so I'd like to hide those internal leaks in the next major version.\nOne justification is that we've been experimenting with adaptive algorithms (I can send a conference paper privately if interested). So that might be built into the sketch to automatically tune the window/main sizes based on an indicator. The ability to rework internals without forking or worrying about breaking someone else is nice.. Is this with a current version? I think we fixed it in v2.3.0 but it has been a while (see #56).. Oh, this is as expected, isn't it?\nThis is a computeIfAbsent behind the scenes to memoize the value when loading. In Guava a put will stampede the value and the computation will be dropped (notified as replaced). This would be similar to https://github.com/google/guava/issues/1881 which caused some people problems. Since we're building off of ConcurrentHashMap, we get its behavior which is stricter than Guava's.\nThe async version is much simpler because the mapping is established immediately and the value computed asynchronously. In that case you can replace an in-flight future. If you instead use the synchronous() view, we should be blocking for the current value to be complete before establishing the mapping.. We can definitely update the JavaDoc if suggestions (or PR). I do think it makes sense for AsyncLoadingCache#put to be non-blocking as the existing future may not be completed yet. There were very enough slight differences that I didn't feel comfortable making AsyncLoadingCache implement LoadingCache<K, ComputableFuture<V>>, though I would have liked to.\nIn the async case, you are establishing a mapping immediately and the future populates asynchronously. It can fail, or be null, and any consumers will receive that. You are explicitly asking for it to work outside of a computeIfAbsent block in ConcurrentHashMap, other than for the basics of setting the future.\nOne could argue that its okay to do similar for a synchronous case, like Guava did, but you'll run into linearization complaints like the issue above. The choice is gray for a cache, but wasn't for a regular map which is why ConcurrentHashMap is more pessimistic. Since I'd prefer not forking or writing my own hash table, we respect its behavior. In most cases I think its less surprising and like you've shown, the async is an escape hatch that is probably acceptable when a performance issue.. Yes. But also consider the return value of asMap().put when a load is in progress. Should it block, replace, and return the loaded value? Or should it insert and the load be canceled as a replacement? Both are valid implementation decisions, I think.. (a) This would be great if you have a PR or suggestions for the FAQ page.\n(b) That would make a lot of sense if we had a custom hash table. I might be a tad hesitant as low level tweak, but it would be a reasonable ask. Since we didn't fork CHM (like Guava did), unfortunately I'd have to say no as modifying its logic is required. Guava was stuck on JDK5's CHM (rewritten in JDK8) and I am sure it will be heavily modified in ~JDK11 when value types are implemented. I think its just too much complexity to absorb, since we'd want to do other tricks for memory efficiency, unfortunately.\nP.S. Closing per \"Anyways, closing since the issue...\". A refresh(key) does not populate the cache until the asynchronous action completes and only if the entry of the cache was unchanged. Guava has a nice optimization here to ignore redundant refreshes that we haven't yet implemented, as explicit calls are rare so I haven't gotten to it yet.\nHave you considered using an AsyncLoadingCache to compute asynchronously? You can then call get(key) and it will not block other than initializing the map with the future.. Sorry, I missed your last paragraph. You can use the synchronous view to minimize code impact by interacting like a LoadingCache. Your main thread doesn\u2019t need to block, as the future will execute by default on a ForkJoinPool. You can query its state or chain handlers, so that should allow your caller to skip if still computing.. I don't recall changing anything in the eviction logic and the EvictionTest#evict passes. Can we get a test case showing it failing? Note that by default eviction is asynchronous so a test either needs to call cleanUp(), use executor(Runnable::run), or wait (e.g. using Awaitility).. yeah, even if ForkJoinPool is too busy to do our work, the write buffer should force a clean-up due to back pressure. The maintenance work replays the writes onto the policy and evicts, but the write buffer has a maximum size. When full, it will block on the eviction lock as a fallback to a thrashing or otherwise bad system state. Since there were many writes and your threads are not all blocked, its not obvious.\nCan you get a thread dump just to check?. I do know many people were burned by the Linux futex bug, which broke locking everywhere. They of course decided not to inform anyone, it went out into many distributions, and has been a quiet nightmare. If you are on an older kernel then you could be effected.. - Linux lost wake-up bug\n- Gil Tene's warning\nUbuntu 16.04 should be fine, but earlier were broken. What does the AWS version map to?\n. According to Gil, the issue started being observed on Haswell-based processors. So if you upgraded to that class, you should also get onto on a newish kernel. I am only making wild guesses based on the limited information we have so far, though.. Oh, wonderful!\nYes, unfortunately there is no way for us to detect that without forking the underlying hash table to add a check. The eviction's removal can be unsuccessful due to a concurrent cache.invalidate(key), so we cannot tell if it is due to a user bug or concurrent operations.. By default the cache defers eviction to ForkJoinPool, which appears to be taking longer than expected to evacuate the entries. Alternatively you can use executor(Runnable::run) to amortize the work on the calling threads, like Guava does. Then your test passes. It is odd though. When I switch to Integer keys the test passes without a hitch. So not sure why your key class is more problematic,. Ah, I see. The reason is because maintenance work occurs due to a cache access. When the work is deferred to the FJP, the task can run while new work is being added. This leaves it in a dirty state, but after it has processed its work, so the next access will trigger a new task to be scheduled and the remaining items evicted. When running your test with a removal listener, we see it quickly evict and then do nothing while your test sleeps. If we add a simple read then it will evict the remaining items.\nPotentially the maintenance task could reschedule itself on the executor immediately. That would probably mask this issue.. For testing, it is easier to not deal with asynchronous logic and use a same-thread executor. If you use dependency injection, it is simple to setup.\nFor production, the cost isn't that bad. But if you use an expensive removal listener, the caller is penalized. To provide a small and consistent response time it can be beneficial to defer the work to an executor. Since its available, we take advantage of it.\nI think it wouldn't hurt to more aggressively retrigger the maintenance work if it detects a new run is needed after it completes, rather than defer to the next cache read or write. That won't change much on production, but avoid confusion cases like you saw.. For clarity, changing your test to the following passes due to the maintenance work being re-triggered as not all items were evicted on its previous run.\njava\n// TimeUnit.SECONDS.sleep(5);\nTimeUnit.SECONDS.sleep(1);\ncache.getIfPresent(cache.asMap().keySet().iterator().next());       \nTimeUnit.SECONDS.sleep(1);. Yes, something like that. There is a state machine, set to REQUIRED when idle and another clean-up is necessary. The task could check and reschedule itself, assuming it is not a custom executor. If a custom executor is used, then I'd prefer not doing this because it might over-penalize a caller, e.g. write storm causing a single user-facing thread to do all of the work. Something like this shouldn't be hard.. You can call cache.cleanUp() if you need to manually retrigger it. Or in your example the same-thread executor shouldn't be a performance problem and you can just use that.. Added the prompt rescheduling. Still working on 2.7, but haven't had the time to wrap up those tests and changes.. Released 2.7. That is a very unusual scenario. Yes, you should be able to use expireAfter(Expiry) to model this. Note that due to races, etc. there is always the possibility of multiple readers to an entry, as it is not exclusively locked. You may find that a different data structure, even just a synchronized LinkedHashMap, is a better fit. Hard to say.. You might also consider the asMap() view, e.g. remove(key) returns the value or computeIfPresent() can return null for removal.. It is threadsafe. It really sounds like a regular ConcurrentHashMap may be enough for you. You might want to review its documentation on the removal and compute methods.. Closing, but feel free to reopen or discuss further.. Yes, you can suppress it in your logger configuration. The exceptions from async logging as warnings in case you don't have any handling yourself, as it is a common oversight when using futures. You could raise the log level, e.g. to ERROR in logback.groovy\ngroovy\nlogger('com.github.benmanes.caffeine.cache', ERROR). hmm, that's surprising. This has not changed much for quite a while, so if there was a race then I'd have hoped the CI would have hit it in the thread thrashing tests.\nA quick glance and nothing looks wrong to me, but another set of eyes would help. I am too familiar with the code so I miss issues, unfortunately.\nIn this case the entry was present so it tries to update (rather than insert, in the earlier block). The entry's lock is acquired for exclusive access, for double checking its state and modifying it accordingly.  If a race occurs like the entry's deletion, it will release the lock and retry. So there is an infinite loop, but it usually terminates quickly since it is unusual of many threads to be modifying an single entry at once.\nThe entry's lock is held during a computation, e.g. Map#computeIfPresent or removal / eviction. If you are using a CacheWriter with an expensive delete(k, v, cause) implementation, that will execute under the entry's lock. That can block eviction since it is forced wait until the writer completes so that the entry is removed atomically. If you do implement it or other computations, then can you check if this is prone to timeouts or similar failures? If so, do you handle timeouts, etc. correctly?. For immediate term, can you use a get(key, func) or a CacheLoader? That is the preferred usage since it avoids races causing redundant computations (two threads getIfPresent, compute, and put). That might fix it for the short term, and then maybe you can help me figure out what the problem with put is.. Are you certain that the threads never return, or do you think you have some DoS attack (usually a misfit internal service) causing a write storm?. When the entry is no longer \"alive\", it is either \"retired\" or \"dead\". When retired, it should no longer be in the hash table. In all usages that I see, it either retires after a Map.remove or as the final statement to a Map.compute and returning null for removal. The loop should then be that if the cache read the entry, but had a stale read causing it to see a now retired / dead entry, it will retry. In that case, the next read should cause it to see the latest mapping which is either absent or a new insertion. There shouldn't be a case where a retired or dead entry is present in the cache. So the continue should be very rare, which makes this weird. I could imagine cases like an improper expiration time causing constant eviction, but nothing jumps out as causing it to spin forever.. @panghy have you had any luck in your investigation?. If it helps, you could use a patched version of the cache that logs the key info (hash, identity hash, etc) if the loop retries > N times. Should be a very simple hack and easy to add. If you don't have an internal repository then I can push a change to the snapshot. Let me know if there's any way that I can help.. Perhaps you could try using cache.asMap().compute(key, (k, v) -> value) as an experiment? This method does not have an infinite loop in Caffeine's code, but does in the underlying ConcurrentHashMap. It of course cannot be as optimized. Either it works or fails in a different way, which may tell us something (though I'm not sure what).. Any updates?. It does become a rats nest in all the variations one might want of \"quiet\" methods. Does that mean recording with stats, eviction, expiration policies? Does that notify the writer / removalListener / etc? How many variations are supported (get, put, compute)? Does one one-offs, flags, or a flag builder?\nI have tended to try to hide valuable but non-core methods in Cache.policy() and to a lesser extent Cache.asMap(). So if we could find a way of fitting it into the Policy then I'd probably be more open, since the ad hoc messiness is hidden from most users. There is a lot to be said of keeping the conceptual weight of an API small.\nI am also unsure exactly what you need. Is it just a getIfPresent(key) that does not record a cache hit? If so, then the penalty is maybe polluting the cache, but we don't know the real impact to the hit rate without testing. The policy is fairly resistant to malicious requests, so if its just that then I wouldn't be too concerned about impacting the hit rate.. I can send you the paper if you email me, since I probably shouldn't attach it. They ran malicious workloads that tried to pollute a cache and saw how well the policies coped. You could view non-user operations as similar noise, especially when full scans. As you said, these can have a greater impact on recency-biased policies than frequency ones. W-TinyLFU did quite well, so I would suspect that the impact wouldn't be too bad if you continue to perform the operations naively.\nIf helpful, also remember that asMap() has computation methods, so you can do a read and conditional remove as an atomic operation.\nIt might also be worth mentioning Caffeine's variable expiration. I'm not sure how much that helps beyond not having to scan the cache, since you still have to do the distributed validation. But it does provide a hook for more efficient processing.\nI would ask that as a first step you might capture a problematic trace so that we can evaluate the impact. It just seems worth making sure we spend time on the right things rather than rely on our gut feelings, since both of our arguments seem reasonable at the moment.\nI also think a fuzzy, best-effort approach is probably okay. Eviction and Expiration are fairly non-deterministic policies that clients shouldn't make hard assumptions about. That gives us a lot of leeway that usually masks many of these types of problems, which is why I am not overly concerned.. @wburns any thoughts on the above? I'm fine either exploring this with you or closing until later. If there isn't negative experiences and it is just a concern, then I'd prefer closing until we can confirm a problem. I am fine adding something to Policy someday if we conclude that is the right solution.. Oh, I remember that issue! Its unfortunate that he didn't respond to requests for a reproducible test for us to investigate with.\nTechnically we would want a trace from Infinispan's (user facing) and Caffeine's (internal) perspective. Then when we replay and the hit rates differ significantly, we know that the extra accesses are confusing the policy. Then it is a lot easier to decide on a fix. If, on the other hand, the two hit rates are similar then we know there isn't a problem.\nIn Caffeine we have 3 regions: admission window, probation, and protected. The admission window is LRU so that the entry's recency has to age. If the inflated hits cause it to pass TinyLFU's filter, it is in probation. If it isn't used again, it will be aged out. The worst case is it is inflated after the TinyLFU filtering, which will cause it to be in the protected region and take the longest to be removed. But that means it was pretty useful in the past, so the harm shouldn't be too high.\nThe problems we have seen are with recency-biased traces where frequency is a negative signal. We have experimental ideas for an adaptive policy that resizes the region sizes. I can send you a pre-published paper on those ideas so far. I'd love some help here if you get bored \ud83d\ude09, since its still an open problem.. Yep. The admission window has a minimum size of 1 to avoid the surprise of being immediately evicted. If those are consecutive accesses to the same entry, then it will have to be present.\nThe author was trying to cache images, saying they are ~100kb each. I'm not sure what the unit is for <binary eviction=\"MEMORY\" size=\"1000000\" />, but that would be only 10 entries if in bytes (1 MB). This would be 10x smaller than the other configuration, <binary eviction=\"COUNT\" size=\"100\" />. That would explain the different hit rates if a misconfiguration.\nAnother possibility is that some images are much larger than 100kb. If the cache's capacity is smaller than the item's, then it will evict that item immediately. There is no point flushing the entire cache in that case. So he might have a few large image and that impacts the hit rate, but doesn't demolish it.\nUnfortunately without his test case we don't know.. Thanks for the test case!\nThere is a small mistake in your test that is causing the problem. The addition is performed non-atomically as,\njava\nAtomicInteger currentValue = new AtomicInteger(generateRandomInt() % 10);\nString key = generateRandomStringInAZ();\nAtomicLong v = cache.get(key, k -> new AtomicLong(0));\nif (v != null) {\n  v.addAndGet(currentValue.get());\n}\naddCount.addAndGet(currentValue.get());\nThis can cause the removal to occur prior to the addition, resulting in a loss of precision. Consider the scenario of,\n1. Adder obtains the value\n2. Adder is context switched out\n3. The entry is evicted (due to 1s TTL)\n4. The removal listener updates the removal count\n5. Adder updates the value and add count\nSince (4) occurs before (5), the addition is lost due to the entry no longer existing and it was already processed by the listener. Instead, you should make the addition atomic to avoid this race.\njava\nint currentValue = generateRandomInt() % 10;\nString key = generateRandomStringInAZ();\ncache.asMap().compute(key, (k, v) -> {\n  if (v == null) {\n    v = new AtomicLong();\n  }\n  v.addAndGet(currentValue);\n  addCount.addAndGet(currentValue);\n  return v;\n});\nThen the test passes! \ud83d\ude05 . I don't recall much about my multiway-pool, except that I broke it in the last commit. You could resurrect it if you revert the last commit, as it had been working up to that point. It was pre-Java 8 and I am sure it could be improved upon.\nYour use-case is too far outside of the norm for the cache to offer directly. The solution might be more complicated than you're willing to accept, but I think it could be done.\nFirst you would need to create intermediate mappings. I think the cache would be normalized to a key/value pair, e.g. Cache<VersionedObject, String> where the value is a lookup to the Set. Your set is then a materialized view that is kept in sync with the cache. This could be done atomically using a CacheLoader and CacheWriter, or else could be done in a more racy/best-effort fashion.\nTo have the size policy ignore entries, you can use a Weigher and set the weight to zero when in use. This has to be computed up-front, so whenever acquiring you will need to do the increment/decrement within a cache computation. The size policy will then not count the zero weight entries in its capacity constraint and will ignore them on eviction.\nSo to do this with Caffeine, you'll probably want to use:\n - CacheLoader to fetch from storage and add to the Set\n - CacheWriter to remove from the set on eviction\n - Weigher to skip over referenced entries\n - asMap() to perform ad hoc computations. Happy to keep chatting, but closing otherwise.. Thanks, I read it a few weeks ago. Unfortunately the C code is not released in a runnable manner, making it more painful to implement for comparison. I am skeptical of their decision of comparing only with policies that lack historical insight, since theirs does too, which likely means it underperforms. The workloads chosen are not a representative set. Researchers tend to discourage implementations so unless the results are repeatable it\u2019s worth being a cautious believer of their assertions.. I would like to see this implemented in the simulator, as there are probably new insights to learn from. I don't know when I'll have the time. Since they didn't open-source their simulator, it will be a bit more work to verify a correct version.\nhttp://www.cs.cmu.edu/~beckmann/publications/code/2018.nsdi.lhd.tgz. Sorry if I came off negative. I do like their ideas and think it could work well, but needs a more robust evaluation. Most papers I've read make bold claims and, when I test them, there are significant flaws the authors failed to mention. Sometimes this is new information, but it can feel intentional when the paper has an arrogant tone.\nI would like to see this algorithm in our simulator. I think it would offer new insights on adaptivity and, like you said, could let us use weight as an eviction choice. Since they don't retain history and their published data, I think it can only match a classic policy. TinyLFU's idea of using a sketch and aging it could be adapted as storage (vs filter), instead of retaining the per-entry counters. That might work, but we'd need to experiment across lots of workloads to know.\nThe papers on policies that take a cost function (weight, load time) only compare against similar. Its not clear though if a policy with a higher byte-hit-rate is better than a higher hit-rate. I'd guess that some users would prefer favoring large and others small weights, and others just normalize to equally sized chunks instead (e.g. video). I've also found that user supplied cost functions are usually pretty bad (e.g. favor paying customers).. Sorry, I'd have to review knapsack again on v/w and how it relates to LHD. So I'm only lightly following that at the moment and you might need to dumb down what you're thinking a bit for me. :)\nLHD does not retain history after items are evicted, so it only knows what happened to the working set while the item was present. For most policies that hurts how well they perform because once an entry is evicted, any insights about it are lost. Typically that insight is frequency, so it takes longer to recover if a poor victim was chosen. Since the ideal policy should adapt to mimic the optimal (MRU, LFU, etc) it will make mistakes and need to correct from that. My guess is that without maintaining any history outside of the working set (as ghosts or counters) it cannot recover as well. Sampling has the unfortunate chance of choosing between valuable items to pick from. In a loopy trace, sampled MRU has half the hit rate of linked MRU probably due to poor distribution choices. My thinking is if LHD was augmented, it might do even better. But first step is someone needs to implement it :)\nRegarding weights...\n- A paper that appeared on ACM recently, but isn't out yet, and might be interesting to you:\nBetter Caching in Search Advertising Systems with Rapid Refresh Predictions\n- Their prior paper is an analysis of the problem, whereas I believe their new paper is an algorithm to solve it. Here the cost function is revenue potential.\nWorkload analysis and caching strategies for search advertising systems\n. Thanks for the refresher! That all sounds familiar now. \ud83d\ude04 It hadn't clicked that v_i was from the cost function, but that's a great observation.\nWhere it differs is that recency and frequency are additional variables, and their impact vary across workloads. Would you consider that to be part of an equation to compute v_i? Regardless, v_i would require some form of aging to smooth out the function (e.g. if load time was slow due to a temporary network timeout).\nI do think that I need to review optimization algorithms again, especially more advanced ones. At work we're starting to run into the nurses scheduling problem for dispatching and routing, so I should probably get a few good books to prepare for that eventuality.. hmm.. that's going to be hard to debug then!\nIs there anything you can share to help us narrow it into a unit test? I would need a failing test to debug with. Any logs or info of how you discovered this?\nI could imagine a race where the now is less than writeTime due to racy reads of System.nanoTime(). If that was true, it would be a concurrent reader with a write and a redundant refresh. Is that by chance match what you are seeing? . hmm, then I don't have any wild ideas to start with. How did you stumble upon this?\nI think that I remember seeing in the past that ForkJoinPool may retry (assist) tasks if the worker executing it is blocked. I added guards to AsyncLoadingCache to no-op on those calls. I could imagine a similar scenario happening here. I think this gets into the complexities of ManagedBlocker and FJP's tricks. If this was the case, you'd be seeing multiple refreshes for the same entry. That doesn't match your premature observation, though.\nI think some background would help.. Oh, I make those types of mistakes in my code too often, too. Just try to avoid people figuring that out publicly. \ud83d\ude04. Removals are special since they might be done automatically, e.g. size eviction. In those cases your code isn't in control, so you need a callback mechanism to handle any clean up work.\nFor inserts and updates, your code is in control so a listener isn't required. It might be a nice-to-have, but there is nothing stopping you from adding the call in the loader. That may even be preferable as more obvious code than thinking through how different abstractions interact (a common frustration with obtuse frameworks).\nThere is also the question of whether a listener should be called asynchronously (ordered or unordered) or synchronously. We have RemovalListener for asynchronous unordered, and CacheWriter for synchronous and therefore ordered. CacheWriter, though, doesn't get notified on a load/computation because that could cause surprising double work (e.g. loader: compute and insert a row into the db; writer: notified and updates the row with the same value).\nSince we're then running into a lot of complexity of how things interact, with the need for a lot of variations, it begins to become messy. When messy, that increases the conceptual weight for users to learn the APIs and have a higher chance of making mistakes. So it is an abstraction that isn't necessary, doesn't provide a lot of simplicity for users, and does not promoting a best practice. Then it seems better for us not to provide it and recommend users do it themselves.\nAt least that's been the conclusion so far. What do you think?. I actually wrote the key mapper when I was first starting on caching, when doing a bunch of performance work. That was bundled into an internal caching framework, but is what got me hooked. My case was like yours, where the keys were unique components of the value (e.g. User=>uuid, email). I viewed it as a primary key and secondary indexes that lookup a unique table row.\nAt the time I started exploring how to fix concurrency problems with the synchronized LRU cache (ehcache), which got all these things started. I put some of the internal caching utilities on a wiki page, and thankfully saved those when Google Code shut down. The indexable data structure has aged poorly, given its 10 years old, but might offer some assistance in rolling your own.\nIt is a little noisy since I had to first introduce the concept of lock striping and an implementation, which was the basis for Guava's Striped (as was the Bloomfilter wiki page, the cache was integrated, etc). In the framework I used annotations to extract the keys auto-magically and then lookup by the POJO. I don't recall how I had it wired up for a load() function, which was done by the SelfPopulatingMap decorator. The secondary keys were kept outside of the cache to avoid confusing its maximum size constraint, and removed on the listener callback.\nMight help, or not. Most of the code could disappear with better libraries today. It worked fairly well, but I haven't needed anything like it again.. @osklyar just asked this question on StackOverflow.\nI realized you could avoid the secondary mapping due if the primary/secondary aspect of the keys can be determined by the Weigher and Expiry. In those cases, you can disable the policy (zero weight, infinite duration). However, you cannot perform a cache update within a computation/load as that is prohibited by the underlying ConcurrentHashMap. This would require asynchronously (CompletableFuture) populating the cache with the additional mappings.. Is that homegrown? How does it compare to our AsyncLoadingCache?. @osklyar For us, we store the CompletableFuture directly and remove it if it fails or computes to null. The quirk is then making the weigher and expiry async aware to disable them while in-flight, and then callback when complete for the evaluation. Since the future is handed out directly, if completed methods like thenRun will execute on the calling thread unless thenRunAsync is used instead. There's not too much logic on just once loading, merely a guard to ignore ForkJoinPool's attempts to rerun the completion handler if delayed.. @apragya I don't remember, except it was loading. Presumably there were two options,\n1. Allow the loads to be non atomic if computing by secondary key, under the assumption that a cache was best effort.\n2. Load the primary key based on the secondary key, thereby simplifying to a single loader function.\nThat caching framework had our local caches backed by memcached with a JMS topic for invalidation, so typically the cost of a local cache miss was quite low. It was also built on a model-driven architecture, so all the storage and other details were code generated from descriptor files (which had the annotation metadata). So all of the queries, table information, etc was known at compile time and the cache had figure things out from the type's details. It was a hairball framework that I had to work within, and probably led me to be strongly in favor of libraries over framework designs.. Thanks @osklyar! If you can, also consider giving Caffeine's version a shot.. Thanks! I'm glad it's helpful.\nIt would be interesting to see a more real-world usage of keyMapper in your tests. If the simple case of (id, email, phone)=>User, then would the keyMapper be a SQL lookup to the ID? So on a full miss, you would expect two database calls (key=>PK, PK=>Value)? However, it looks like the value loader is by secondary key, e.g. email, so should keyMapper instead take the value to derive from instead of the secondary key? I'm curious if we can shrink it to one db lookup on a full miss, though I'm sure the cost is negligible. I'm also unsure if these API details are not be a big deal or might be confusing in your realworld code.. That does seem quite nice and usable. A few notes that may help.\n\nFor your tests, you might want to use Guava's FakeTicker to control time. Then you won't need the Thread.sleep to cause a delay or wait for expiration. If you inject the same ticker everywhere, including the cache builder, then you can control time in your tests.\nCaffeinatedMultikeyCache#onRemoval will remove the various mappings. Since the listener is called asynchronously, you might have a load for the same key replacing it. For example due to a refresh (via refreshAfterWrite) to reload when stale but before its expired (to hide the latency). The safe way is to use conditional removal by an instance check, if possible (asMap().remove(key, oldValue). Since it is a cache this won't be a big deal, but its a race to be aware of.\nA common API wisdom is to avoid multiple arguments of the same type if used for different purposes (e.g. setVisible(true, false) is unclear). Your two lambda expressions are like that, though much more obvious. A builder might not be much better, so mostly just FYI of the potential quirk.\n\nOn our side of not having an AsyncCache and having to fake it by throwing an exception in the loader - what are your thoughts? I didn't want to introduce the abstraction unnecessarily and did want to promote loading through the cache. Every once in a while a case like yours comes up, but its not seemed ugly enough to introduce the API. I did make sure enough was defined to do so, if users pushed for it, so I am glad it didn't block you.. > In fact, a more irritating API decision for me was the impossibility to construct a loading cache without a loader function in the builder.\nThanks. I have seen multiple integrations have to do this workaround with a dummy loader. It seems to occur when building ones own cache abstraction where a single loader doesn't fit their scenario (e.g. Akka's http cache). I'm do think users reach too quickly for manual get/put calls, but I'm open to conceding on this and providing AsyncCache.\n\nI probably would prefer the means to construct the cache without the loader and get an exception if no loader is provided to the get.\n\nOh, I wouldn't go that far. Perhaps sometimes a rarely used unimplemented method is okay, like Iterator's remove, where there are a variety of possible implementations. This is too central and since provided during construction, the consuming code might not know when its safe (e.g. JCache configures externally so runtime code must make assumptions). I do prefer Bloch's idea of the loading interface extending the manual API to make it very explicit.\n\nwould prefer to deal with nulls in the callbacks rather than on the future interface directly in cases like getIfPresent().thenApply\n\nThis is a quirky one. The preferred get won't return null because it will always try to compute. For most users, using getIfPresent outside of a test is a code smell. The null future means the mapping is absent in the cache, while a future holding null or an exception is a failed computation (which will be automatically removed). If we returned CompletableFuture<V>(null), you would not be able to discern between a missing value in the data store or simply not in the cache. That may be okay most of the time, but it conflates that only on that method and might frustrate someone who needs to know. . @apragya would the solution that @osklyar kindly put together help for your use-case?. I agree that deriving the full set of keys from the value, but loading it from any secondary key, is ideal.\n@osklyar would it make more sense to have an api like the following?\njava\npublic CompletableFuture<User> getByUsername(String username) {\n  return sessionStore\n    .get(username, $ -> {\n      return userDao.getByUsername(username);\n    }, user -> {\n      return ImmutableSet.of(\n          user.getUsername(),\n          user.getEmail(),\n          user.getId());\n    });\n}\nPerhaps your session example is odd because you won't store the password, so you can't derive the key from the value. I think @apragya would need a more general case like the above. Do you think your library could handle his scenario too?. Oh, I'm glad you implemented something.\nIf the two caches are not circular then a deadlock cannot occur as there is no shared, global state. However, ConcurrentHashMap does not support computations that write into itself. As in map.compute(key, (k, v) -> map.put(1, 1)) will fail, but map.compute(key, (k, v) -> other.put(1, 1)) is okay.. Yes, that is what I meant by circular. You have to have a strict ordering of locks.\nOnce the value is in the cache, you can of course put it into the other map outside of the computation. However you would then run into a race of compute=>evict=>addIndex. That can be solved, so you're right that things things require a lot of care.. Yes, I think we're in agreement.\nPlease do note that Caffeine is not prescriptive. It is a fancy ConcurrentHashMap and you are very welcome to use the asMap view or do other things yourself. You don't have to use the loading apis if they are not helpful. In that way our rule is to make it easy to do the right things, but not block you from doing the hard ones.\nTechnically what you need is a lifecycle, as per-entry locks are not compossible. It has to be asynchronous to the computation, whether that all is done by one thread or delegated. If you think of it as a transaction, they recover by either restarting or aborting the work. You could have a PK=>tryLock and abort if another thread owns it. In that scenario, you can still return the loaded value to the caller but not bother to set any of the mappings if you lost the race.\nAnother thing to remember is as a cache, its okay to embrace the fuzzy nature of things as long as you abide by the contract. A wasteful index update is probably very cheap. An extra load on a rare race is okay, since you're absorbing most of them. If you abide by the contract and have correctness, then its okay to cheat somewhere else to make things fit together.. Oh, thank you too. This is the enjoyable part of the day when I get to to talk to smart people and learn from them. :-). To clarify my understanding of your dilemma:\nThis is a race due to the staleness of the cache, correct? For a duration the cache can return back the wrong value due to an old mapping. When the cache is informed (via expiration, update notification, etc) then the mappings are synchronized. There isn't a long-term data leak where the mapping is stuck in a bad state. Since PK is immutable over the lifetime of the value, it can be used to coordinate the internal consensus as the mappings are adjusted.\nIf so, then usually this falls into the case of being a benign race. There was a clear ownership of the secondary keys to the primary according to the value. When that ownership changes there would be some propagation delays, but small enough to not impact business logic. If strong consistency is required, then one interacts with the system-of-record directly. Then one runs into the different transaction isolation levels, which often run afoul to strictness expectations.\nI probably misunderstood, so can you explain the danger of your scenario?. I think we might be coming to it with different expectations. I'm viewing it as a useful performance optimization for classic business logic, where the associations don't change very often (such as user lookups). In those cases the implementor opts in because they know the trade-offs and its a benign race. But perhaps you are looking at it from the author of more generic infrastructure, like a database, where users won't consider the tradeoff and need stronger guarantees.\nThe easy solution to this is to validate on the lookup and retry. The index and value can be tagged with a version number and compared. If a write causes the reader to see the newer version, then a retry loop should resolve it. Alternatively you could validate by recomputing the index from the retrieved value, though the memory savings probably wouldn't justify the penalty.. You're right. That's probably simpler without much a drawback.. @osklyar Introducing AsyncCache in #246.\nClosing as I think there's nothing left for me to do here.. Sure, if you'd like to send a PR to improve the example that would be great.\nIt used to be a best practice to record only incrementing values within the application, and let the aggregator perform any rollups. This is a lot simpler for all parties, and much more flexible. Is that no longer the case?. Oh, and of course you can use the polling approach if desired.. Oh yeah, Counter would have been more appropriate.... Yes, it's a flaw that I've meant to fix.\nGuava has an advantage by forking the hash table and swaps out the underlying entry during the computation. This allows it to serve the current value during the refresh, and the callback restores it to a lighter-weight key/value entry. It is a cute trick that we cannot emulate easily.\nThe proper solution may be to have a lazily initialized Map<K, CompletableFuture<V>> in the LoadingCache of the in-flight refreshes. This way if one is on-going, it can no-op. However, it is not clear what linearizability expectations users might have with refresh, since a put will stampede it. That could mean that refresh => put => refresh should cancel the first one, update, and finally contain the last refresh's result. We'd need to emulate that in our map by capturing the current value to decide if a new async operation should be triggered or not.\nWhich all means it got complicated enough that I didn't get to it. Since it is a rarely used feature (vs. refreshAfterWrite) no one seemed to notice until now. \ud83d\ude00 . yep, absence and present values are treated similarly by their respective implementations. . For now please use get in your executor. If you are returning futures anyway, you might want to switch to AsyncLoadingCache and simplify the logic to its get(key) call. If I'm lucky then I may get to this over the weekend, but may not.. Thanks! I think this comes from me not properly updating Guava's JavaDoc. Likely I should have fixed up the naming more, too.\n\nFirst, it should be clearer if \"loading\" comes only to the loading procedures provided during the cache building, or also when Cache.get(k, mappingFunction) is called.\n\nYes, recordLoadSuccess or recordLoadFailure are called for a load or computation.\n\nSecondly, the specification of StatsCounter.recordLoadFailure() doesn't mention that when Cache.get() is called and the mapping function returns null, it is also considered a failure, it only mentions exception throwing.\n\nYes, it is considered a failure when either the value is null or an exception is thrown. This is why I renamed it from Guava's recordLoadException.\nPR or JavaDoc suggestion would be appreciated. \ud83d\ude00 . merged!. Thanks! I tweaked the wording to be a little more broad on StatsCounter and decided to not include it on the Cache interfaces. My concern is that a user might have to think through a lot of cases, with stats being the least important, and it may be a bit overwhelming. . The elegant solution to this, which I think you'll quite enjoy, is to use Awaitility. That provides async safe assertions using Hamcrest matchers. I updated your test to pass using\n```java\nimport static org.awaitility.Awaitility.await;\nawait().until(() -> cache.synchronous().asMap().size(), is(3));\n```\nThe gory details is that we use a whenComplete handler to remove the entry if the future fails. This creates a race where your thread may be notified and evaluate the size prior to our handler being run.. Perhaps because the future fails immediately, so it does not wait 5 seconds to possibly timeout. If you add a breakpoint on the whenComplete handler linked above, you should see the removal occurring and observe it in the cache in the test thread until discarded.. Thanks! I see that the cause is due refreshIfNeeded using the expirationTicker() for the initial timestamp, and the statsTicker() when to calculate the load time. For all other calls, the timestamps are independent and, usually, the reuse is not harmful since they are both System.nanoTime(). There is therefore two options for us to consider,\n\nMake an additional call to statsTicker().read() prior to refreshing the value. This would make the timestamps independent as expected elsewhere.\nMake the statsTicker configurable by using the supplied ticker. These would stay separate internally so that turning on expiration doesn't penalize with statistics timestamp reads, or vice versa. However, manipulating time in a test to go backwards would result in this same exception.\n\nGuava does not use the configured ticker in its stats recording and I don't see a good reason to for users to configure it. So my preference would be (1). Any objections?\nReviewing the usages, all others are correctly using the statsTicker for the start time. One bug does appear to be in AsyncBulkCompleter which accidentally uses recordLoadSuccess(result.size()) instead of recordLoadSuccess(loadTime). So that should be fixed as well.. There's a few items on the backlog, but I'll try to wrap them up and release this weekend.. Released 2.7. The notification is delegated to an executor, by default the shared ForkJoin common pool. For testing an easy fix is to use a same-thread executor by setting Caffeine.executor(Runnable::run). Alternatively you might use Awaitility to test asynchronous behavior.. Oh, and our expiration is triggered by activity on the cache to cause periodic maintenance cycles. Therefore nothing will happen due to your thread sleeping. This is to avoid creating our own threads, which is typically problematic for libraries. In the future we will offer more eager expiration using the new shared scheduled executor, but for now we restrict ourselves to Java 8 apis.\nYou can instead use a FakeTicker to manipulate time and call cleanUp afterwards to see the expiration occur.. Thanks. I hadn't noticed that the missing annotations were added to ErrorProne, so I don't think there are any blockers. In semver, would you consider this a patch or minor revision?. Thanks. That's what I was thinking too and a second opinion is helpful.. I'm not entirely sure where all of the new mappings are, any suggestions?\nJSR 305 | New |\n---|---|\njavax.annotation.CheckForNull | Perhaps Nullable?\njavax.annotation.Nonnegative | ???\njavax.annotation.Nonnull | org.checkerframework.checker.nullness.qual.NonNull\njavax.annotation.Nullable | org.checkerframework.checker.nullness.qual.Nullable\njavax.annotation.concurrent.GuardedBy | com.google.errorprone.annotations.GuardedBy\njavax.annotation.concurrent.Immutable | com.google.errorprone.annotations.Immutable\njavax.annotation.concurrent.NotThreadSafe | ???\njavax.annotation.concurrent.ThreadSafe | ???\n. The 350kb checker jar is also a bit obnoxiously bloated. I don\u2019t see a good alternative, but it feels excessive for just nullability annotations.. I double checked and I must have downloaded the wrong jar. The checker-qual jar is 200kb, so a bit better. The checker-compat-qual is 6kb and doesn't contain type annotations so we'd use NullableDecl and NonNullDecl if we went that direction. I'm not too picky either way, I guess.\nI have a branch, jsr305, with the migrations so far. Do you have any more recommendations for it?. Also, I should note that I didn't use the type annotations' ability to be put within the generic argument, e.g. @NonNull Map<@NonNull K, @NonNull V>, because I didn't think that would be helpful. But I'm willing to be convinced otherwise.. Oh! I forgot to use your non negative mapping. I\u2019ll go back and add them in tomorrow.. Oh, I guess because I\u2019ve used the annotations as documentary rather than assumed a static analyzer would be run by users. As an easy way to glance without reading the JavaDoc, it answers basic questions. Since developers are accustomed to assuming collections don\u2019t have null elements, that is noise and not useful.\nPerhaps now that view is outdated so I\u2019d be fine adding if you think it\u2019s worthwhile.. My recollection is that Checker assumes null by default, which makes using it painful. I tried setting it up once and felt it was just too invasive, among other issues. I think Kotlin switched from null by default to non-null due to that, which masks noise but makes the type system silent about potential bugs. I do think that's a fair choice though and libraries should annotate their public interfaces for documentation and tooling.\nIn my next round of changes, I'll add the annotations to the type arguments and restore the non-negative. Thanks!. Thanks, that's a good point! I had forgotten about the JCIP originals.\nDo you think adding them back in would be helpful? I did like it as another hint of expected behavior, but I'd also guess that most users assumed the behavior already when chosing this library.. I'll reopen so that I don't forget :). I took a quick look and I think they can be ignored. All the public apis are thread-safe so it is only internal documentation for non-threadsafe data structures. That's nice for anyone brave enough to dig into the code, but otherwise not critical. I think users can assume all public apis are threadsafe and act accordingly.\nThe only semi-public api is in the simulator, which is used for researchy purposes. Its Policy api had the annotation to communicate that assumption. The simulator uses Akka and proxies to each policy with a dedicated actor, which allows for parallel execution with a back-pressure by bounded mailboxes. I think anyone adding a new policy would skim the code for examples, so this wouldn't be too confusing.. yep, though for now that's used via enabling simulator flags (vs by a policy directly).\nSomeday I'd like to bring that into the library for the \"doorkeeper\" optimization. However, it biases the policy more towards frequency so it hurts LRU-friendly workloads. We have a pre-published paper on an adaptive scheme to correct for this (automatically tuning based on sampling the workload's characteristics). When that's passed peer review I'll work with @ohadeytan and @gilga1983 on implementing it.. Released 2.7. In this case you would use a \"capturing lambda expression\". That would be like using Guava's where the anonymous class captures the surrounding environment. So you could do,\njava\ncache.get(userId, key -> {\n  return userService.get(userId, requestContext);\n});\nThe benefit of non-capturing lamdas is that an instance can be cached by the JVM to avoid additional allocations. That limits its usefulness, so a capturing one is valid and the Function is merely to help give you the choice. The only annoyance is checked exceptions, but utilities like jOOL's Unchecked.function cover that pretty well.. You might be interested in\nhttps://www.infoq.com/articles/Java-8-Lambdas-A-Peek-Under-the-Hood. Yeah, you can use a dummy loader and avoid using get(key). The get(key, func) should do what you want.\nI didn't provide an AsyncCache initially so that I could understand why it might be helpful. The most common reason users use Cache instead of LoadingCache is poor - they do a racy getIfAbsent=>compute=>put instead of an atomic get that computes for them. For async code this is perhaps more valuable, since the code is more explicitly concurrent than the synchronous case.\nI did layout the interface to make it easy to add AsyncCache later. I've seen a few integrations (e.g. Akka's) that would have wanted that, using a dummy loader as required, to bind to their caching APIs. There's been enough valid use-cases that I think it makes sense to add the interface now.. I think what you want is,\n```java\nfinal AsyncLoadingCache cache = Caffeine.newBuilder().buildAsync(key -> null);\npublic CompletableFuture getUser(UUID userId, RequestContext requestContext) {\n  return cache.get(userId, key -> userService.get(userId, requestContext));\n}\n```\nThe dummy loader would be for the builder and you'd avoid calling cache.get(key) which would invoke it.. get(key) just calls get(key, func) with the attached loader, so it is mostly sugar. You only miss out on features like refresh and bulk loading, as would be expected.\nI'll try to get around to AsyncCache to avoid this hack. Just never have the time to get through my backlog.. Yes, and it has a higher than classic LFU hit rate.. https://github.com/ben-manes/caffeine/wiki/Efficiency. Thanks!. Yes, it is a bit surprising at first. (Sorry for being long, but trying to explain my thought process)\nThe write method could be used to persist the changes to an external system (db, remote cache, etc). It wouldn't be desirable to invoke write as a side effect of your code loading from that resource, only to push it back in. Then you do both an external read and write for every cache miss, which is confusingly wrong.\nThe intent and desired property is for a user to have the ability to synchronously intercept changes that they otherwise can't. This would be a put, replace, remove, or any form of eviction. In the case of a computation, the user already has a synchronous block controlling the evaluation. If the user knows that the change should be propagated to the external resource (update, delete), then it seems less confusing to have their function do that work. In that case they might extract the shared code (e.g. deleteFile(entry)) and call it both from the compute method and CacheWriter.\nThe RemovalListener is a slightly different concept. In that case you want to be notified on a removal to do some work. If that work is expensive and does not need to block the map operation, the listener is appropriate because it is asynchronous. However that could lead to conflicts due to the race, e.g. it seems that guava-cache-overflow-extension was abandoned due to not being a good fit with an asynchronous listener.\nSo use the RemovalListener when you want to perform some action after an entry was removed, and don't need to slow down the map operation. Switch to CacheWriter when you need to expand an atomic operation that you don't control to perform some work. And when using a compute method, call the writer logic manually if needed. I think that leads to the least surprising code, even if a subtle characteristic of how things can interact.\nSince the interface names are all different (function, listener, writer) it seems okay to have different semantics even when aspects might seem to overlap.. > Shouldn't the user be using a CacheLoader to do this?\nLoadingCache.get(key) delegates to Cache.get(key, func) with the CacheLoader, which in turn delegates to Cache.asMap().computeIfAbsent(key, func). I think most users would expect that get behaves the same as computeIfAbsent, and that compute and computeIfPresent behave similar to computeIfAbsent.\nTake the simple case of a local cache backed by memcached, e.g.\njava\nCaffeine.newBuilder() // ... plus eviction config\n    .writer(new CacheWriter<K, V>() {\n        public void write(K key, V value) {\n          memcached.put(key, value);\n        }\n        public void delete(K key, V value, RemovalCause cause) {\n          if (!cause.wasEvicted()) {\n            memcached.delete(key);\n          }\n       }\n    }).build(key -> {\n      V value = memcached.get(key);\n      if (value == null) {\n        value = repository.get(key);\n        memcached.put(key, value);\n      }\n      return value;\n    });\nIf the writer was invoked after a load/compute, then we could avoid the loader's memcached.put. Unfortunately, we also wouldn't know if the load was satisfied by finding the value in memcached or the database, and wouldn't have a way to communicate that. In that case we'd perform a memcached.put unnecessarily on a remote cache hit.\nFor caching libraries with a CacheLoader and CacheWriter, they don't typically have ad hoc compute methods. JCache kind of does, but its APIs are also wildly inconsistent and often incompatible with itself. Instead most caches restrict flexibility to provide more features (like multiple caching tiers). Rather than build lots of variations of similar features, by being in-memory we can try to keep it simpler and more adaptable. Then our library feels more like a fancy ConcurrentMap and treated as another encapsulated data structure, rather than a full blown storage service like memcached or Postgres.\n\nThe core issue is that the creation of a Cache may be completely disconnected from the invocations of said Cache. The only way to keep it all at creation time is to build the ConcurrentMap and then wrap that using a forwarding map wrapping any user Function in another one, causing additional allocations.\n\nYes, that's unfortunate and understandable for Infinispan. I think most users can get away with encapsulating the cache within domain abstraction, e.g. UserService, and use dependency injection to wire it up. Then their code is making on a few explicit calls to the cache, can wrap tweak as needed, and hide that from consumers of their interface. The cost for them is really low.\n\nLuckily the code I wanted to use this for is only a single level of indirection away so I can I merge the function together with the logic I am invoking in the CacheWriter. It just seems like this shouldn't be required and a CacheWriter should handle all writes into the map from the user.\n\nI think that would have to be another variation, as that's understandable but the current behavior is also required for other users. However, it might not carry its weight since the current versions let you do something you can't do otherwise. This new variation would only assist in a less common scenario, which also adds confusion of which type to use. So far my feeling is that it does not have a good power-to-weight ratio, but I am open to being convinced otherwise.. That's an interesting take, thanks!\nI would have found the behavior surprising to have computeIfAbsent attempting to use the CacheLoader (if set) before falling back to the supplied function. The user-visible behavior (return value) would change depending on how the cache was constructed. That's an aspect of JCache's get(key) that I don't like, because the caller cannot differentiate between a cache miss and the resource not existing. Since the JCache configuration is typically externalized, the calling code is obscured from this and it breaks the principle of least astonishment.\nThe way that I approach it is that Cache is an opinionated interface to ConcurrentMap. LoadingCache simply adds additional convenience by attaching the mappingFunction, which allows for a few more features (like getAll and refreshAfterWrite). But in the end it is just a fancy Map that may automatically remove entries. This mindset makes sense given our lineage of Guava's Cache, which tried to give a less error prone interface to MapMaker. Guava's addition of compute methods behave like ours do, despite coming later and written independently.\nOh, maybe that explains it! Our Cache.get is not Map.get. In the JavaDoc you linked to it says the method roughly implements the sample code,\njava\nif (map.get(key) == null) {\n V newValue = mappingFunction.apply(key);\n if (newValue != null)\n   map.put(key, newValue);\n}\nMap.get does not invoke the loader and only peeks into the cache, as Cache.getIfPresent. A self-populating map, a la MapMaker, broke the get(key) contract. It wasn't completely surprising to users at the time, but would be so now with the compute methods. I documented our reasons for disfavoring Map as the primary interface here, and instead proposed asMap() as an escape hatch for when the Cache interface was too limiting.\nBased on that JavaDoc of Map.computeIfAbsent, shouldn't the expected behavior be what is implemented today? And if so, then its back to CacheWriter not fitting cleanly when bridging between caching and collection mindsets?\n\nNot that it matters, but Infinispan supports this.\n\nWell of course it does! Infinispan is quite rich and well thought out \ud83d\ude04 . Not yet. I do plan on adding that in my next sprint of energy.\nFor now you can use a dummy loader in the form of,\njava\nCaffeine.newBuilder().buildAsync(key -> null);\nand avoid calling get(key) which would invoke the loader.\nI wasn't sure if users would have good reasons to not use a cache loader, so I delayed adding a manual AsyncCache until later. However I did add all the necessary methods to the AsyncLoadingCache to make it easy to extract later and not block users. There have been enough users with good rationals that I think it makes sense to add now.\nMy initial concern is that most users of Cache use the racy getIfPresent, compute, put idiom. Ideally they would use a loading get, but many don't consider concurrency of the cache stampede. An async cache is more explicitly concurrent, so I more strongly promoted delegating the loading to the cache. Then my intent was to introduce the API once I better understood why users might want it.. I'll leave it open and close when I add this feature.\nAlso if you can explain your scenario, it would be nice to catalog as a reference.. Sorry, I'm pretty behind at the moment. Work and a family medical emergency has caused a lot of chaos over the last few months.\nI have work on adding a Map<K, CompletableFuture<V>> view to the async caches, which is 80-90% done where the remainder is to finish the test cases. It's a little quirky on statistics, e.g. computeIfAbent(key, k -> future) should records the loadTime as the future's most likely. So I think that I need to iterate on the stats, or release it with stats being good enough with later fixes since its not rigidly spec'd. If I can get some time to wrap that up, then I should be able to catch up on the other backlog tasks quickly and cut a minor release.. Released 2.7. Yes, please upgrade.\nThe issue is that the executor can have all its threads busy and the maintenance task is scheduled on its work queue. This means that a back off won\u2019t help, since there are no free threads to run it. By instead taking the lock and doing the work as a last resort, it fixes the problem and flushes the write buffer. It also increases write throughput by reducing context switches, more aggressively flushes the write buffer, and does not busy wait.. This was fixed in 2.3.1 (#90) so you can do a patch update if necessary. Since I do try to strictly abide by semver, a full upgrade would be ideal for any other bug fixes.. Sorry for any trouble caused, but thanks for checking in with us about the issue.. Thanks for the suggestion.\nThis isn\u2019t general since some configurations do not use maximumSize, e.g. maximumWeight. So pushing additional metrics like this might make sense for your reporting (e.g. Dropwizard) rather than ours.\nSince most caches fill to capacity (100%), I\u2019m not sure how useful it is. What you really want is to know the miss rate curve at different maximum sizes. Can the cache be reduced while maintaining the effective hit rate? Will an increase result in a large gain? This is likely out of scope for us, though papers like MiniSim provide mechanisms. It requires sampling with ghost caches, which could be helpful as profiling but not generally enabled. I don\u2019t think we should venture here as it\u2019s fairly murky. . That's a good point, sometimes a cache is merely a limit against unexpected growth.\nThere are two reasons I'm hesitant. First is still the generality of the metric. For example weak or soft reference-based caches, while less ideal, do not have an occupancy rate due to being bounded by the JVM. The existing metrics are a baseline that are not coupled to a particular setting.\nThe other reason is philosophical of trying to provide capabilities that users can't easily implement themselves, but also avoid excess \"kitchen sink\" bloat. This is pretty minor request, but pragmatically I'd imagine wouldn't gain much versus doing it yourself. Typically metrics are reported using Dropwizard, Prometheus, Micrometer, etc. As a library (vs framework) it doesn't prefer a transport (e.g. JMX) and instead you can wire it up. For example to do this using Dropwizard Metrics you might write,\njava\nMetricRegistry metrics; // likely injected\nGauge<Double> occupancyRate = () -> \n    cache.estimatedSize() / (double) cache.policy().eviction().get().getMaximum();\nmetrics.register(name(\"usersCache\", \"occupancyRate\"), occupancyRate);\nWhen you wrap that into an instrumentation helper class, for any cache constructed, you'd have all the metrics exposed using shared code. Then the benefit of having them in CacheStats is a very minor convenience factor, saving maybe a line of code. When using a library like Micrometer, you can report the stats anywhere (console, jmx, grafana, etc) to easily create alerts and dashboards.\nFor now I do think you write the above calculation isn't such a bad thing. What do you think?. And thank you for the suggestions! \ud83d\ude04 . Thanks Ohad! Rapidoid switched to using Caffeine; sorry for the incomplete removal.. Unfortunately this is a limitation imposed by the underlying ConcurrentHashMap which does not expose a key iterator that includes in-flight operations. For more advanced configurations an invalidateAll requires iterating over the cache, removing an item by key, and sending the value to a listener (RemovalListener and CacheWriter).\nIf no bounds are configured, then UnboundedLocalCache is used as a light-weight adapter to ConcurrentHashMap. It will use Map#clear() when no listeners are involved. That method knows of these in-flight loads, so it blocks as desired.\nWhen a bound is configured, then BoundedLocalCache is a much more complex decorator. It uses an iterator, in this case the values() iterator, for removal. All of the map iterators hide the pending load. We cannot use clear() since it would not let us do the necessary processing work.\nThe only solution to this would be if we forked ConcurrentHashMap to peek at its internals. There are justifiable reasons, such as better memory packing, but it also incurs a high maintenance cost. For now I've preferred not doing that, especially since Value Types could dramatically change the internals. Unfortunately without an obvious mechanism I don't think we can fix this.\nI haven't looked at the internal iterators recently, so I don't recall if they block or not (removeIf). Most likely they don't by using the iterator, rather than the blind sweep of the underlying table[]. Sorry for the bad news. The only other option that I've thought of is to maintain an in-flight key set, e.g. ConcurrentMap.newKeySet(). If we inserted / removed during the computation, then it would allow us inspect that keyset during the invalidateAll() iteration. Obviously that still leaves races, so it is not perfect.. I found the thread where I discussed this with Doug Lea.\n\n\nOn 12/20/2015 02:09 AM, Benjamin Manes wrote:\nOh, one issue that I don't have any good suggestions for resolving is invalidation of suppressed keys due to computations https://github.com/ben-manes/caffeine/issues/13. The request is that a clear operations (aka invalidateAll) should block until all in-flight loads entries are complete and subsequently removed. While this is done by CHM#clear(), it cannot be done by a decorator which needs to process the entries (listeners, policy, etc). All iterations of the map suppress entries until they have fully materialized.\n\nThe desire to do this is racy, even using clear, which non-atomically traverses. So there's only weak consistency wrt arriving/ongoing/completed computeIfAbsent's etc. In principle, CHM could offer a method with similar properties, but it still wouldn't provide a simple-to-state guarantee.\nThe only solution I know is for users themselves to arrange quiescent points if they need a strict demarcation. (And I wonder if this desire about invalidateAll obscures assumptions they might have about consistency.)\n-Doug. The key is your call. It must be unique enough to load with, and have good equals/ hashCode methods. Using an id is pretty common.. I hope you solved your problem. Sorry it wasn't clear how to answer your question and hope I helped. Happy to continue discussing if there's more clarity.. Oh, I see now. Sorry that I misunderstood and thanks for the explanation.\n\nThe cache does not create its own dedicated threads to schedule and promptly expire entries. Some caches do this, either as a framework singleton or per cache instance. That can be problematic or create a lot of overhead. But we might offer something similar in a JDK9+ release by taking advantage of the new CompletableFuture.delayedExecutor API, which provides a JVM-wide scheduler. This would allow best-effort prompt expiration if desired.\nInstead the cache hides expired entries when queried for and evicts them during a maintenance cycle. The maintenance cycle is triggered on a write, accumulation of reads, or Cache.cleanUp(). The expiration order is kept in amortized O(1) data structures, so the work is pretty fast. But since is concurrent and we don't want to stop all reads/writes with a blocking lock, there will always be a possible race of reading a to-be-expired entry before the eviction occurs.\nIf your cache has low activity and you need a little better promptness, you can schedule a call to cleanUp() yourself on a ScheduledExecutorService.\nDoes that help?. I guess so.\nAn implementation detail is that getIfPresent (and thus asMap().get) does trigger a maintenance cycle if the entry is expired, but currently the contains don't. That wouldn't fix your race, just be more prompt. I don't think there is any workaround to some racyness except if all accesses were synchronized, which of course you wouldn't want either.. If you send a PR, then we can let TravisCI run the build.\nI have noticed Gradle gets confused if you run with a JDK mismatch (output Java8 but run with JDK10). I'd recommend using jenv to set the shell to 1.8, stopping the daemons or running with --no-daemon, and running a clean build. If it hangs, you can use ps aux | grep java and jstack <pid> to find likely culprits. From there we can try to debug the cause.\nI'm not sure of a good fix, so I'd be interested in seeing your proposal.. Sounds like you worked things out. I don't think there is a good fix on my end, as there are only two bad options.\n\n\nReturn a different CompletableFuture that what the user supplied, to wrap their computation with synchronous logic that removes from the cache before the future completes. This would invalidate any other place logic that might depend on that future instance, e.g. some work queue, where identity comparison was used. Not wrong, just a surprise. However, users can do this trick themselves.\n\n\nKeep the current behavior, which bubbles up the AsyncCacheLoader future. This leaks the race condition to the caller, but such is how asynchronous code is in the real world. They'll have to consider \nthe appropriate mitigation strategy.\n\n\nClosing, but feel free to reopen if you want to discuss. Thanks!. As a local cache, distributed synchronization is outside of its scope. The ad hoc way is expiration and refreshing.\nYou could use the pub/sub feature of Redis to synchronize with. Then cache updates notifies other machines to invalidate the key (perhaps reloading it).\nThere are also distributed caches, though I don\u2019t have experience using any. Infinispan uses caffeine underneath.. Best of luck!. Hi @azagniotov,\nThere are a few potential issues. I think it would be helpful if you described the problem you are trying to solve, which led this your idea.\nFirstly, I think your code will fail if you try running it. CacheWriter methods are invoked within a ConcurrentHashMap#compute block to allow the work to be performed atomically. A nested write to the map within this block is not allowed and will either livelock (JDK8) or throw an exception (JDK9). This might be solved using a RemovalListener which is invoked outside of the computation.\nAnother issue is that expiration is performed lazily, as in when the maintenance cycle runs or the entry is overwritten. This means that there will be a delay as the cache does not create dedicated threads to event on, but rather is triggered by usage activity. JDK9 introduced a dedicated JVM-wide scheduling thread via ComputableFuture#delayedExecutor, so a future version might leverage this for prompt expiration as a configuration option. This might mean your maintenance trigger is less useful if you are expecting immediate expiration events.\nCache#cleanUp is fairly fast, assuming the eviction compute call is not slow (e.g. an CacheWriter#delete). All of the algorithms that Caffeine uses are strict or amortized O(1), and that work is inexpensive. If you are concerned about the usage of ForkJoinPool#commonPool() by default, you can use Caffeine#executor(Runnable::run) instead to avoid the background thread.\nAnyway, I hope that helps. If you can provide more background then maybe we can find a better solution.. The background worker is really just there to avoid variability in your response times. It definitely can be run on the callers without much of a penalty. So I am glad that's not an issue.\nWe trigger maintenance pretty regularly when there is activity. Effectively we have an access and write queues, which when populated the maintenance is scheduled. We replay the operations to keep the policies up to date, rather than have the callers bang on our locks. So other than adding improvements for prompt expiration, its usually quite seamless and performant.\nHappy to answer more questions; Have a great weekend!. Your observation makes sense when there is also a maximum size, as it would leave pollution but not require extra work. That improves concurrency and is simpler, but wastes space. Ehcache, memcached, and redis all use that solution. However, the pollution adds significant waste which is why both memcached and redis have dedicated threads to scan the cache and remove expired entries.\nIn a size unbounded cache, then of course waiting until the entry is queried would be a potential memory leak. I've seen some do this and may use the background thread to assist, but then they usually use a full scan to discover the expired entries.\nFor us the maintenance task is used for a few cases. In particular for size-bounding, where every cache read is actually a write to the eviction policy (e.g. LRU). That mutation requires locking on shared state, which is a bottleneck. We could use a concurrent policy like Clock (FIFO-based) or random sampling, but that can negatively impact the hit rate or runtime performance. Instead we record the work into ring buffers and replay it in batch under a tryLock, so that calling threads don't block. We replace lock contention with the cost of appending to a ring buffer (a single CAS). The actual policy work is usually just pointer manipulation, so its very tiny per element.\nWith these ring buffers in place, we get to use the maintenance task to easily handle complex algorithms that are not concurrency friendly:\n - size eviction: We maximize the hit-rate using an advanced algorithm called W-TinyLFU\n - fixed expiration: We use time-ordered LRU lists for expireAfterWrite and expireAfterRead\n - variable expiration: We use a Hierarchical TimerWheel as an O(1) priority queue\n - references: We can poll a ReferenceQueue without synchronization overhead\nSo instead of the maintenance task being something to avoid, its actually a great approach that allows us to offer a lot of performance and capabilities.. I missed your question on the queue sizes. They are based on the number of CPUs and grow based on contention / usage. (On phone so not checking code for sizes)\nThe read buffer is dynamically striped, with each ring buffer holding 16 elements if I recall correctly. I think it\u2019s 4xcpu max stripes. When full it drops additions and triggers the maintenance task. The striping is based on LongAdder.\nThe write buffer is a jctools growable ring buffer. It starts small and expands as needed. If full there is back pressure by forcing the caller to block on the policy lock and perform the maintenance. This improves throughput by descheduling threads. The maximum is likely overly generous but hasn\u2019t been an issue so far. It was set by monitoring the throughput in a synthetic write heavy stress test.. Note that we also use that executor for user-facing async tasks (removalListener, refreshAfterWrite, AsyncLoadingCache). So depending on your configuration and needs, you might want to ensure there are enough threads available in the pool. If you aren't using any, then a single thread is all that the maintenance task can every consume.\nInternally scheduling the maintenance task is guarded by a state-machine and a tryLock, so it can only schedule one maintenance task when one is not already running.\nLet me know your findings :). The task will still be scheduled automatically when enough reads or a write occurs. I imagine your schedule would kick in to more proactively notify listeners when an entry expires. Or do you delay the tasks submitted into your executor?. great, looks good!. The dedicated thread for maintenance does seem to have better heap behavior, if we go by percent used. A likely cause could be that ForkJoinPool is share JVM-wide so its threads are being used for handling http requests, etc. That would delay the maintenance task from running, so more writes accumulate. The dedicated thread wouldn't have this problem, but is of course less friendly in the general case to have a thread-per-cache (but okay for dedicated systems). You could verify that by using your threadpool w/o the periodic task.\nIt would also be interesting if you used Runnable::run, e.g. same-thread executor, which would defer to calling threads. Since you are not doing heavy work during eviction, the penalty imposed on the caller's latency is probably insignificant. That would avoid the dedicated thread and any task scheduling issues.\nWhat are your thoughts when reading the graphs?. You shouldn't need to ignore the submitted task. The same effect will be more safely done by using:\njava\ndefault Cache<String, T> build(final long expireAfterWriteSeconds, final long maxHeapEntries) {\n    return Caffeine.newBuilder()\n                .expireAfterWrite(expireAfterWriteSeconds, TimeUnit.SECONDS)\n                .maximumSize(maxHeapEntries)\n                 .executor(Runnable::run)\n                .softValues()\n                .build();     \n}. Thanks for the feedback! It\u2019s definitely interesting to see. Are there any takeaways we should highlight from your investigation?. Slightly unrelated, why do you enable soft references? They are known to cause GC churn due to heap pollution. It makes sense if a fallback in case of an unexpected memory spike and using maxSize otherwise. Just curious if that\u2019s your reasoning.. That sounds reasonable. If you have a few big caches in critical sections, e.g. Druid's or Cassandra's row caches, then the performance gain is worth the dedicated thread. If a typical web application with lots of ad hoc caches, then it won't be worth it.\nI typically avoid soft references and view it as a last resort. It can cause hard to diagnose performance problems if unbounded, due to causing a GC death spiral of full collections. It sounds good in theory, but typically isn't. However, some managed environments (like AppEngine) were so memory constrained that it was impossible to size and forced poor GC behavior regardless, so it can be a necessary evil. For the most part, avoiding the GC from doing extra work is a performance gain.\nI do try to be respectful to the GC regarding references, so hopefully the setting doesn't change your results. When an entry is evicted / removed, Reference#clear() is invoked which hopefully helps the GC know not to do the weak/soft reference work. If you have the free resources, it would be interesting to know if softValues() causes heap pollution regardless or if your heap utilization is unaffected.. It does, but seems like a hard optimization problem. A challenge with soft references is that the GC doesn't know which entries are more valuable, so cheap items could cause the removal of valuable ones. It would seem then that to do this properly, you might want to take into account the hit rate, load penalty, popularity, etc. of the caches. Then optimizer would need to monitor and retune when the workload changes. I think this could all be done above the caching library itself. If someone wanted to explore that, I would certainly be interested in their findings but its not an area I'd be good at or have the time to explore. You're welcome to try =). > but somehow, I'm always short of time....\nhaha, yes. I think we're in the same boat. I do enjoy these conversations, though.. cool, thanks for the info! Its good to see the JVM takes the opportunity to scavenge proactively when it makes sense.\nG1GC (now default) treats soft references different than previous collectors and is more aggressive. Older collectors, like CMS, are generational and require waiting for the full GC. G1 is region-based so it does not wait until a stop-the-world event and collects within that region. This can mean it will discard soft references too quickly, e.g. my tests run with parallel GC for predictable outcomes. By chance did you capture the hit rates between runs? Just curious if as a side effect the hit rate dropped significantly, or if comparable and it was purely a gain. . The cache does not maintain insertion or sorted order. The asMap() order is from ConcurrentHashMap, which is hashed mappings. The cache otherwise can provide order from the policy() for the hottest/coldest or youngest/oldest as determined by its eviction policy.\nYou could maintain a separate map that is kept in sync with the cache by using a CacheWriter and CacheLoader.. A cache is a ConcurrentHashMap with automatic eviction based on size, time, or references. If you do not need an eviction policy, then you should prefer to use a Map instead. If you need a map that removes entries due to a boundary condition, then you should use a Cache.\nCache is a fancy Map, not better or worse. Just a different data structure when appropriate.. Yes, you should be able to. expireAfterAccess is a fixed policy that resets the timestamp on any read or write to the entry. The corresponding Expiry would look like:\njava\nvar duration = Duration.ofMinutes(???);\nnew Expiry<K, V> {\n  public long expireAfterCreate(K key, V value, long currentTime) {\n    return duration.toNanos();\n  }\n  public long expireAfterUpdate(K key, V value, long currentTime, long currentDuration) {\n    return duration.toNanos();\n  }\n  public long expireAfterRead(K key, V value, long currentTime, long currentDuration) {\n    return duration.toNanos();\n  }\n}\nExpiry is called on the creation, update, or read of an entry. If you want to ignore the change, such as expire after a fixed time from creation, you can return currentDuration.\nYou can't combine expireAfter(Expiry) with expireAfterAccess or expireAfterWrite, so you'll be fine there. Expiry provides a variable, custom policy whereas the others set fixed lifetimes.\nThe implementations differ because Expiry was added later. The fixed policies were simple to implement using time-bounded LRU list, e.g. it merely reorders the entry to the tail of a doubly-linked list. The cache uses only O(1) algorithms, but variable expiration traditionally requires either O(lg n) priority queue (like a heap) or polluting the cache with dead entries and evicting on a size constraint (e.g. memcached). Instead, we use a Hierarchical TimerWheel which does this using a nifty hashing trick. Since the fixed policies were implemented, it didn't seem worth the effort / bugs to replace their logic with an Expiry.. I have wondered if some utility like your Expiries would be helpful, but the need didn't come up. The main quirk with Expiry is that it uses long instead of Duration for time. That's to avoid allocations on hot paths, as the cost is non-negligible until value types land. Since it is a special case feature, it felt okay to put that burden on those few users. Since Expiry doesn't get used very often, I haven't been sure what utility methods might make it more attractive / usable.\nWe did add expireAfterAccess(Duration) and expireAfterWrite(Duration) convenience methods. What is the benefit of your Supplier parameter? I'd have thought it would be BiFunction<K1, V1, Duration> if you wanted the time to vary per entry.\nIf you're just wanting to change the fixed expiration setting at runtime you can use cache.policy().expireAfterAccess().get().setExpiresAfter(Duration).. Yeah, policy() turned out to be a handy place to stash all the crazy asks that pop up without corrupting the core interfaces.\n\nIs there any performance gain you think to using the original expireAfterAccess version instead of Expiry?\n\nIn practice, nope. The fixed expiration is strict O(1) because it is just LRU reordering, so insanely cheap. The variable expiration is amortized O(1) because it hashes to a bucket (linked list of items in the same time range) for add/delete, but cascades to lower buckets as time progresses. The Kafka folks wrote a nice article on the algorithm, and its used by OSes for interrupt scheduling. I fine tuned my implementation with bitwise tricks, so it's very speedy. You might enjoy taking a look. :)\n\n@ConfigScoped\n\nIs that a custom scope? I grok what you're doing, but haven't seen related code. I've tended to use static flags and bounce the fleet to avoid oddities that occur when runtime changes are only partially picked up. But I can appreciate both perspectives.. This is probably because there is a race condition between the error and the callback removing the entry from the cache. This was also in #254, but I don't see any good solution to that.\nSince you are dealing with asynchronous code, you probably want to use Awaitility and await().until(cache::synchronous, is(anEmptyMap())).. pardon? There is a race condition that the cache cannot control\nt0. Future is added to cache\nt1. Future starts computing\nt2. Future sets state to error\nt3. Future notifies callback\nt4. Callback removes from cache\nThere isn't an opportunity to remove from the cache between t1 and t2 because we do not control the computation logic. We have to wait until t3 to be notified, but that is after the future's state was set. Therefore, there is a small duration where the cache will return the failed future.\nThe only way to inject ourself into the computation is to wrap the user's. However, if the user provides a future then they might expect the same instance be returned by the cache. There could be adjacently managed logic depending on that, as it seems reasonable to not expect an intermediary. If we broke that assumption would could hide the race, but valid arguments can be made in either decision. Instead, you could eagerly invalidate the entry as part of the computation by providing your own callback and caching the chained instance. For example,\njava\nvar future = ...\ncache.get(key, k -> {\n  return future.whenComplete((r, e) -> {\n    if (e != null) {\n      cache.invalidate(key);\n    }\n  });\n});. I think that's the trouble with any asynchronous / concurrent code, because we typically think synchronously. It's the same behavior if you wrote the code manually using a Map<K, Future> and using callbacks, which would be idiomatic. I usually find it best to have that type of code at the edges of an application, so that most code can be in their own sequential universe.\n\nShouldn't invalidate be part of the AsyncLoadingCache interface?\n\nCurrently you have to drill through the synchronous view. It could be added as a default method for convenience, but that's not much of a difference.. I guess the future's computation was invoked immediately, e.g. if the Mono was already done. Sorry, forgot about that case.\nThe reason is that ConcurrentHashMap does not support writes within a computation, so it livelocks (Java 8) or throws an exception (Java 9) when that occurs.. I think you deleted the comment. But you're right, lookupAndWrite2 is safe and lookupAndWrite3 is unsafe. Because the mono was completed, the whenComplete executed on the same thread and wrote back into the cache. Thus, it was in a computeIfAbsent calling remove which is a recursive write. For a bit of history, you can follow the links in this SO answer.. I think we explored this fully, so I'll close. But please reopen if we forgot something.. Cache#get is the same as ConcurrentHashMap#computeIfAbsent, and we use the hash table behind the scenes. It will take a (roughly) per-key lock so the first thread creates the entry, while the second blocks until complete and returns the entry's value. The cache will return the same entry, assuming it wasn't discarded due to an error / null value / eviction.\nIf the Mono fails, then I think in lookupAndWrite it will propagate that error until the cache is notified by its whenComplete on the future to remove the entry. Thus the entry will be completed and failed for a short duration within the cache. In lookupAndWrite2 the failed entry will be removed from the cache prior to being completed, as it removes itself from the cache as part of its finalization logic. The cache will only hold successful or in-flight entries. All previous lookups or lookups during the whenCompleteAsync execution will see obtain a Mono that will fail.\nThe only difference is whether you see the cache holding success, error, in-flight or success andin-flight`. However, reads are lock-free so they may walk an immutable chain created by a writer. So there is always a chance (measured in nanoseconds) that all three states might be in the cache, from a consumers perspective. I think your code won't be different between the 2 or 3 state conditions, so I think 3 is okay as you have to handle failures regardless.. Oh, thank you. These types of things are always fun to discuss (less so to debug!).\nCaffeine will execute the map operations on the caller and then defer any future it creates to the configured executor. If the loading function is synchronous, then it will be wrapped as asynchronous using the executor (see CacheLoader#asyncLoad).\nI would think if users don't have a preference, then they'd expect to have their work executed on Caffeine's executor. Sometimes when integrating with an async library, e.g. an http call, it is already being done on the http client's pool and wrapping that is undesirable. One reason why I think returning the given future instead of chaining it (per original discussion) is then cancelling / obtrudung / etc might be hard to propagate correctly (user cancels A' but original A still running).\nI agree you don't want to block on a future / promise / mono unless absolutely required. I'm not super familiar with Reactor / RxJava, so the best idioms there is your expertise. \ud83d\ude04 . MonoToCompletableFuture appears to be invoked by onNext and calls complete(t). So I think it shouldn't be blocking. . right, that's how I found MonoToCompletableFuture earlier. But shouldn't that only be called when the Mono completes? If you use Awaitility, you can test concurrent lifecycle behavior using AtomicBoolean and awaits(). Then you'll know for sure.\nA quick usage example,\n```java\nvar started = new AtomicBoolean();\nvar done = new AtomicBoolean();\nvar future = cache.get(context.absentKey(), () -> {\n  started.set(true);\n  await().untilTrue(done);\n});\nawait().untilTrue(started); // wait until future is running\ndone.set(true); // allow future to complete\nawait().until(future::isDone, is(true)); // wait until completed\n```. That's an interesting use-case, thanks for the suggestion.\nI think the only quirk to figure out will be how to evolve the code-generated entry types. We try to minimize the number of fields to the configuration's minimum to reduce memory overhead. Basically the issues are:\n1. For refreshAfterWrite, we reuse the expireAfterWrite timestamp if both are enabled. This won't always be the case anymore, so we'll have to generate a slight variation.\n2. A minor issue is that expireAfterWrite and expireAfterAccess can both be enabled, though that's likely a mistake from Guava that we carried over. Instead of generating a new field, I think we can restrict it here since I don't know of a use-case. (This is odd because Expiry was introduced later, as variable expiration is hard to do efficiently, so the fixed policies were not implemented on top of it)\n3. Then it is a little game to make sure we minimize the jar size by generating the minimal number of classes by intelligently mapping the methods to various timestamp fields.\nIf that all works out, we can brainstorm the Refreshy name and add the necessary test coverage.. Yes, that might work. If I understand correctly, it uses a java.util.Timer to schedule a task per entry, based on its expiration time, and the thread then reloads it asynchronously. This would let you have per-entry policies at the cost of dedicated thread(s), O(lg n) writes, and entries would not disappear if unused. In Caffeine the refresh is triggered if the entry is actively used, does not conflict with expiration (as can be used in conjunction), does not use dedicated threads, and is O(1). So tradeoffs, with both implementations having their benefits and limitations.\nI think we can add this feature fairly easily, but I don't know when I'll have the time to work on it. So I think in the short-term you should choose whatever is best for you and not wait on me.. Thanks Jens, it\u2019s interesting to hear how you approached things and where my understanding of how cache2k\u2019s mechanisms work is incorrect.. The current refresh capability is that when an entry is accessed and it's age exceeds the refresh time, then it will be asynchronously reloaded. If the entry is not accessed and exceeds the expiration interval, then it will be evicted. However the refresh interval is global, and the ask would be to let it be customized per-entry.\nIf you are describing proactively refreshing, e.g. maintaining a known hot set by reloading based on an interval, then I'm not certain that proactive refresh would be a desirable feature for the cache itself. The existing scheme has the benefit that stale entries are refreshed without incurring user-facing latencies, while allowing inactive ones to be removed.\nIf some entries could be proactively refreshed but others not, that's a bit of a complicated scheme to grasp. It could be done by the cache using some decider interface and a thread, but I'm not certain why the cache would be better than custom logic.\nSo I'm not entirely sure what you'd change. But if you want to explore, the code is complex and located in BoundedLocalCache. To avoid our own scheduling thread, we'd want to make use of JDK9's shared thread by way of CompletableFuture.delayedExecutor.. I guess you figured it out. I usually set the executor to run on the caller thread in tests to disable any asynchronous behavior. Otherwise I use Awaitility to assert with to more robustly handle sleeping as the timing is non deterministic.. Thanks @blsemo! I'll migrate TestEdenResize to using the parameterized infrastructure.. I apologize that I didn't have the time for this project earlier.\nIn the adaptive branch, it dynamically reconfigures the window size. This means that it starts at a poor hit rate but becomes the ideal hit rate over time, so a long running application should see a high hit rate. In that way it's like the JIT's warmup. Because of this, I don't think that there needs to be a user-visible knob. What do you think?\ntrace_vaultservice.xz @ 512\n| Policy | Hit Rate\n|---|---|\nLru | 33.33 %\nCaffeine 2.6 | 0.89 %\nCaffeine 2.7 | 29.10 %\ntrace_vaultservice_large.xz @ 512\n| Policy | Hit Rate\n|---|---|\nLru | 33.33 %\nCaffeine 2.6 | 0.58 %\nCaffeine 2.7 | 32.52 %\nHere is a visualization of this behavior at the adaption points in the large trace.\n\nSimilarly adaption and good hit rates are observable with a 99% window and running the loop trace, which favors frequency. It also does not degrade when starting at a good configuration, so it appears robust in my testing.\nDo you think this change is valuable for users or is it okay if we continue to hide this detail?. Thank you.\nSorry for the confusion, the last question was meant to be a repeat of the former.\nI need to write unit tests still. I hope to make progress towards having a release soonish.. This work is now in master! \ud83d\ude3a . Here is a mixed trace of Corda large, 5 loops, and Corda large. Caffeine achieves a hit rate of 39.5% where the optimal is 40.3%, and all other policies are at 20% or lower. I am really happy with these results, but would love your confirmation when you have the time \ud83d\ude04 \n\n. If no objects are to be evicted, maybe you can use an unbounded cache? That is optimized for as a lightweight wrapper on top of ConcurrentHashMap. Then if your code never explicitly removes, the cache won't either. This would merely be using a setting without any bounding types.\nAlternatively, if you are fetching it up front you might prefer to use Guava's Suppliers.memoize(...) and an ImmutableMap or equivalent. You won't get built-in stats, but it might be more natural if the data isn't really distinct key/value pairs but a glob.. An unbounded cache is merely Caffeine.newBuilder().build(). You can turn on various features, like stats or refreshAfterWrite, without an explicit bound. This way if the interfaces are more natural or you want to quickly adjust the configuration, its pretty flexible to your needs. It should only fail when incompatible features (like writer + async) are requested, but otherwise give you what you ask for in an optimized variant.. yeah, just don't use a custom weigher as no pinning is needed. Pining is if you use eviction but some items have to be excluded temporarily because there is a client-side lock. Kind of an edge case for some infrastructure projects (like databases) and natively supported by coincidence, so explained in the FAQ. For yours, just don't enable the features and you should be set.. hmm... yeah, I should rewrite that. Thanks for the note. I think it was before I had variable support, initially implementing the adapter, and was listening to one of the spec members talk\u2019s. But it\u2019s not quite right as you point out and just got pulled along with other edits, rather than revisited.. @Maaartinus fyi since you enjoy these types of topics \ud83d\ude04 . Thanks! I would say some constants that are magic numbers in the code should be in the config file. They aren't obvious and probably won't be modified, but it does remind us that its a variable we might tune.\nFor abstractions, I'm okay either way. Main point would be to avoid too much reuse that it becomes harmful (e.g. the Node classes) but policy reuse isn't a huge deal.. @ohadeytan I cleaned things up a bit. I don't know enough about are the constants except that they look a bit scary.\nFor MiniSimClimber, should that be a modulus? Or is it a warm-up period and you expect it to adapt on every operation? It's not obvious from the code.\nFor 'Indicator', what would a good name be for the stream constant?. Thanks! \ud83d\ude04 . Yes, this is the same as #220 where I misread the Guava behavior. It wasn't a guarantee but we do try to be consistent with Guava, and I misread to emulate what I thought was a bug (but created one here instead!).. It will be fixed, but hasn't been yet. I've been swamped lately and I'm not sure when I'll get through the backlog of tasks here. Sorry for that.\nCan you sort at the getAll call-site for now?. That looks correct to me, too. Can you build a minimal project that I can help you debug with?. One idea might be that the periods in com.mycomp.Book are confusing it. That's a json path expression, so it expands to com { mycomp { Book { ... } } }. This might be causing the current parsing code to be looking under the com namespace incorrectly. That might be resolved short term by quoting and require a fix on my side to more intelligently traverse nested namespaces. . From the code, I think the dot-notation might work but I'd need to run through some tests. If you get a sample together, I'll try to debug it this evening on my commute home (7pm PST). Thank you! I'll try to debug through it tonight.\nMy only guess is that I think that the caches are be loaded when first fetched from the manager. That would mean that getCacheNames() would be empty until some code fetches it, which might occur in your application during startup. I think if we queried for your cache name it might load up, but I'm not sure.. When I add the query,\njava\nCache<Object, Object> cache = cacheManager.getCache(\"com.example.model.Country\");\nI do get an instance:\ncom.github.benmanes.caffeine.jcache.CacheProxy@3a5a4d7e\ninspecting I see that the configuration has maximumSize = 50 and isStatisticsEnabled = true, so that part does work. It is loaded on-demand for the first getCache(...), so we need to figure out how to have Spring associate to it.. I also get, as probably expected:\nhtml\n<html><body><h1>Whitelabel Error Page</h1><p>This application has no explicit mapping for /error, so you are seeing this as a fallback.</p><div id='created'>Tue Sep 18 18:41:04 PDT 2018</div><div>There was an unexpected error (type=Internal Server Error, status=500).</div><div>Type definition error: [simple type, class com.github.benmanes.caffeine.jcache.configuration.CaffeineConfiguration$$Lambda$578/885535138]; nested exception is com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class com.github.benmanes.caffeine.jcache.configuration.CaffeineConfiguration$$Lambda$578/885535138 and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) (through reference chain: java.util.ArrayList[0]-&gt;com.github.benmanes.caffeine.jcache.CacheProxy[&quot;configuration&quot;]-&gt;com.github.benmanes.caffeine.jcache.configuration.CaffeineConfiguration[&quot;executorFactory&quot;])</div></body></html>. Stepping though Hibernate's call into my CacheManagerImpl...\nThe Country cache is configured and it loads correctly.\nThe Continent cache is not configured, so Hibernate makes by default an unbounded cache with monitoring disabled. I filed an issue with them about changing this, so I think latest versions are supposed to print a warning. (See HHH-11953)\nIt looks as if the Caching resolves to a different instance of the CachingProvider, which is my CaffeineCachingProvider. In the one spun up by Hibernate, I see a WeakHashMap with instance id 113 that creates one mapping. In the one from your code, it resolves to instance id 580 is empty and creates a new mapping.\nThis appears to be due to it using Caching.getCachingProvider(String providerClassName) which is the non-default instance. If that property is unset, it will use the default on the classpath. Your example uses the default, so those are two separate mappings in the JSR lib.\nAfter removing javax.cache.provider from your application.yml, the next issue seems to be classloader matching. JCache uses ClassLoaders to query with, so there are two. Hibernate resolves to RestartClassLoader while your code resolves to TomcatEmbeddedWebappClassLoader (which has  RestartClassLoader as its parent). Because it's not the same one, you get two provider mappings.\nYou can confirm by changing the lookup to use the parent classloader to see Hibernate's caches:\njava\nCaching.getCachingProvider(Thread.currentThread().getContextClassLoader().getParent()). > Is there a way to define default cache settings in application.conf?\nUnfortuntely not quite, due to the specification defining the concrete default configuration via MutableConfiguration. Since this is created by Hibernate (or the JSR's annotation proxies), this is cannot be overridden to by the provider for saner behavior. At most we can decide the defaults for non-standard options, but that becomes more confusing of half-in / half-out as non-obvious. The defaults only relate to named configurations, which means it perhaps should have been better referred to as a templating mechanism.\n\nIs it possible to define a custom name (and possible location) for the application.conf\n\nYes, see Config library for details; e.g. -Dconfig.file=path/to/config-file. There isn't currently any option for changing from ConfigFactory.load() (default behavior) to one of its custom strategies, assuming the system property approach is usually fine.\n\nAnother thing I'm unable to unwrap the jcache to the Caffeine Cache \n\nYou should be able to call jcache.unwrap(Cache.class). See CacheProxyTest.\n\nI think that would be the only way to get the actual statistics?\n\nUnfortunately not. Caffeine's statistics are enabled only on construction, whereas JCache's are dynamically set. This means we'd always have to enable them or an additional flag. The stats are different, originally I think due to JCache TCK bugs. By using JCache you are more confined to its work view, as its a framework-oriented approach towards caching. They took a more rigid approach than I'd prefer, so I would only use it where absolutely necessary (like Hibernate). We can try to find a compromise, like setting a StatsCounter, but it will always be working against the JSR's intents.\n\nThx again for your help!\n\nOf course =). > The JCache Cache interface has nothing to retrieve cache statistics as far as I can seen.\nJCache defines statistics itself and exposes them on JMX. They provide CacheStatisticsMXBean for the statistics and CacheMXBean for configuration. Caffeine implements these in JCacheStatisticsMXBean and JCacheMXBean respectively. However we don't add extra details to these mbeans currently, though perhaps could. JCache expects you to either use a JMX console or adapt the JMX registry to your metrics library (e.g. see Dropwizard's module).. Yes, Caffeine\u2019s built-in stats are disabled as unused, so all zeros. We\u2019d have to add a new option to enable them, which seems questionable given the JSR\u2019s.\nFrom the JSR\u2019s perspective, your application code should have minimal vendor-specific logic, so no dowcasting. So while ugly to go through jmx, it\u2019s proper given the standard. I wouldn\u2019t use jcache unless absolutely necessary, like your case, because I don\u2019t consider it developer friendly in many respects. But that\u2019s what we got, so it seems best to respect its idioms for now.. It sounds like we figured this out, so I'll close \ud83d\ude04 . The unfortunate problem with this is it may result in a full scan where nothing can be removed. Then the contract is broken and it is confusing. An alternative is to \u201cpin\u201d the entry by excluding it from the size / expiration policies (see FAQ for discussion). Would that fulfill your needs?. I think @Maaartinus is correct. You should be able to achieve your needs, as far as we can tell, using expireAfterAccess or a custom expireAfter(Expiry) policy.\n\nThe contract could be refined as the overhead being O(1 + numberOfManuallyRefusedEvictions) and this value could be reported in the stats.\n\nThat's true. It would cause the cache to be aggressively re-evaluating each maintenance cycle but hopefully the custom check would be cheap. By using weight / expiration, there is the potential for the item(s) to be ordered such that they are not evaluated.\nMy biggest concern, though, is how easily error-prone this feels. A simple coding mistake could cause no eviction and, if not monitoring stats, a silent memory leak. So I'm hesitant until there is a really strong argument, because I'm afraid it might cause more harm than good.. > Can you please show me how can I achieve this according to the example? I do not catch how can I re-weight entry dynamically.. is compute method invoked everey time the entry is accessed?\nYou can't re-weigh outside of a write, so a compute on read would be necessary. That makes sense if you need to capture a resource that is pinned, e.g. a database lock. Your solution is what AddOn did with our older library, with a fork and interface called EvictionMediator. They used to have a blog post about why they made these changes, which I think was for a client-side lock in Hydra. \nIf you need long-running hold of a resource then the compute to acquire & release are cheap in comparison. In that case you are basically doing reference counting, so it would become:\n```java\nclass Reference {\n  int count;\n  V value;\n}\nCache> cache = Caffeine.newBuilder()\n    .weigher((key, ref) -> (ref.count > 0) ? 0 : 1)\n    .maximumWeight(10_000)\n    .build();\nV acquireShared(K key) {\n  var reference = cache.asMap().compute(key, ref -> {\n    if (ref == null) {\n      return new Reference(load(key), 1);\n    }\n    ref.count++;\n    return ref;\n  });\n  return reference.value;\n}\nvoid releaseShared(K key) {\n  cache.asMap().computeIfPresent(key, ref -> {\n    ref.count--;\n    return ref;\n  });\n}\n```\nObviously this is uglier code, no doubt about it. However the life-cycle is very explicit and your usage code would have a clear lock-like idiom such as acquireShared(key); try { ... } finally { releaseShared(key); }. Internally the cache can then put all zero weight entries in a dedicated area where they are not evaluated for eviction.\nI should also note that one reason I'm harsh on this is merely to follow the skeptical model set by the Bumper-Sticker API Design rules. It would be unfortunate to introduce an API too casually which is misused dangerously.. I agree 128ms is very high, as it usually executes in terms of nanoseconds. Sometimes a sampling profiler will penalize fast but frequently called methods incorrectly, so it may not be the problem.\nThe design reason is because we want to maintain eviction policies that are cheap but not concurrent. For example LRU is merely moving the current node and appending it to the end of a doubly-linked list. That's a cheap O(1), except it cannot be done efficiently lock-free or with naive locking. Doing so creates a lot of contention, which is more expensive than the operation itself. Instead we record into a buffer and replay the operations when its full. That way we can batch the work under the lock and acquire it using a tryLock since the buffer holds their work if unsuccessful. This approach (similar to a database's write-ahead log) allows for implementing an LRU-type cache with a small overhead.\nI'd probably need more information to help you debug this performance issue. I would recommend switching to a computeIfAbsent style call instead of a get/put to avoid the thundering herds problem. If you can reproduce it in a smaller benchmark that might help, as my own stress tests never showed this. If you look through the code, you should see it shouldn't be doing much expensive work in that configuration.. I would also recommend testing with the current version, 2.6.2. It appears they are use 2.3.3. It should be api compatible.. You might be affected by https://github.com/JCTools/JCTools/issues/135, which was fixed in 2.3.4. That race condition caused the consuming thread to be stuck in the write queue and may explain why you're seeing the drain not function properly.. hmm, that seems really weird. \nI know early versions of JDK8 had a broken ForkJoinPool, which could cause trouble (but you're using the same thread). That and the JCTools bug caused the write queue to not be drained, or be a lot more expensive with some back-pressure fixes on our end. Otherwise at your configuration and release, it should be very cheap. If you look at both methods, they aren't doing much. Do you see everything return to a steady state or is it stuck, e.g. in an infinite loop or blocked on a lock? It's weird because its more like the maintenance isn't being run (e.g. due to the earlier bugs) and things are stalling. That would make sense if the JCTools bug was still present, e.g. if you can verify it's really hitting 2.6.2.\nTime-based operations don't use a scheduling thread and expired items are evicted during the maintenance process. But I don't know if that could help here.. Do you think it would be possible to isolate this to a stress test that I can run? I\u2019m not sure how to help investigate.\nUsually the same thread executor is fine. Mostly the executor is useful for callbacks like the removal listener, where the same thread could increase tail latencies.\nIs your workload write heavy? The only time the drain should be doing a lot of work in your case is if it can\u2019t catch up with the writers.. Thanks a lot for your persistence on this issue. If it's helpful, I have a simple Stresser that I use to detect deadlocks and such things.\nThe main difference should be delays due to context switching. For same-thread it can execute the task directly when scheduling the work. For the thread pool, a thread must become available and pick up the work. Since there is an additional delay, such as if you use FJP for application tasks, this would give a larger opportunity for writers to append to the buffer and the FJP thread to perform a larger batch.\nAssuming there is no data structure bug, I can think of two ideas we might explore:\n1. There might be a high level of contention on the hashmap causing evictions to be slow. ConcurrentHashMap uses bin locks, which are coarser than per-entry. This penalty would be on your calling thread, though ideally should be fast if no expensive computation. You might try increasing the initialCapacity, as the larger the map the more locks there are.\n2. If the write-rate exceeds the drain rate, then the runner is penalized by draining all up to WRITE_BUFFER_MAX (~ 128 x npus). For the same-thread, you might argue it should be better amortized across callers. However, at such a thrashing afterWrite should degrade to blocking on the lock and thus not contending with the draining thread. This did seem to increase throughput and provide a good failsafe backoff. But we could try decreasing this parameter to see how it effects things.\nIf you have any questions on the design or the code, please let me know. I wrote an overview since I know its a lot of code to wade through.. Can you enable stats and check the hit rate?. That's a good point, @Siva-R. The execute is only called within a ReentrantLock, so all other threads would skip over the call since the work is being handled. This would only make sense if there are multiple cache instances.\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L184\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1065-L1088\nThe only other place where execute(Runnable) is called is by a removal listener, which is often outside of the lock, but supposedly that's not being used.\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L281-L296. 1. Yes, it would be interesting to see if sizing it upfront changes things. ConcurrentHashMap should grow the table as the cache is populated, but will start out small with few locks. It's purely speculation because your profiler runs should have highlighted that as the cause.\n2. There are two buffers: read and write. The read buffer is lossy as the information is just a hint to the policy of what to keep. The write buffer isn't, e.g. adds trigger evictions, so this grows to capacity and then blocks. The drain of the write buffer is capped by an internal constant WRITE_BUFFER_MAX, so that if writes keep coming in the caller isn't stuck forever (e.g. if same-thread and delaying user-facing responses). So a random guess is if its too lenient and a smaller cap would better amortize this across callers. The same amount of work would be performed, just spread out more granularly. (This would require a custom build to experiment with)\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L145-L146\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1251-L1264. yeah, I really don't recall the reasons. I suspect it's because a lot of various map methods might be considered for recording a hit or miss, but its not clear if desirable. For example put or replace, as well as the iterators. I wouldn't feel comfortable changing this much without @yrfselrahc and/or the Guava team's opinion to do so, too.\nYou could use Guava's ForwardingMap as a quick workaround to delegate to getIfPresent or record stats manually.\nMy thinking in this regard perhaps your workload underperforms with TinyLFU, e.g. by being heavily recency skewed. If the cache is always missing then writes are far more common, which might be the real problem here. If so, then we can discuss the recent research to fix that (#266) and the short-term workaround (#263). . It may be a memory limit. Java is difficult to size for, since the byte size is JVM and platform specific. To some extent you can make guesses using the Weigher to assist, e.g. JAMM for a rough (but inaccurate) guess. JavaObjectLayout is the closest match, but isn't really meant for anything beyond experiments. Caffeine has its own metadata so that also adds some overhead per entry, too. When it gets too large then it might be worth exploring multiple layers, etc. instead.\nThe backing hash table is ConcurrentHashMap which is designed for a large number of entries. However that's more about how it degrades big-O than byte sizes. An off-heap or alternative might become necessary, which gets out of scope for this project.. Thanks! In that case then I'm not sure. Can you open up the afterRead block to see the breakdown?\nThe concurrent calls should be okay, because they accumulate into a striped ring buffer. That should be the limiting factor where contention can occur, as its shared state that we CAS on. The striping should help alleviate that, so its usually not visible in a profile given application overheads. Then scheduleDrainBuffers should be cheap because its a tryLock that is allowed to fail and skipped over. It sounded like the maintenance was a bottleneck, but if you read the code it should be simple code under the lock that reads from the buffer and does a few linked list operations. So all of these should be very cheap, at least compared to the alternative of reads going through an exclusive lock.. expirationTicker is a no-op, so the second trace looks like its incorrectly assigning the blame. In your case of no expiration it returns DisabledTicker which is always 0L, otherwise its System.nanoTime(). To be fair that can be a bottleneck on non-Linux machines, but again application code usually dominates.\nOtherwise I really don't know. The code for afterRead is not too expensive. The readBuffer will have contention that is optimized for, and scheduleDrainBuffers uses a tryLock so penalizes only 1 thread.\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L831-L848\n. It\u2019s skipped if (a) a drain is taking place, (b) a drain is not required (write buffer is empty) and the read buffer isn\u2019t full. Due to a state machine this is a simple switch on an enum rather than poking at things, so cheap to figure out. . That's odd. You could replace the anonymous class with a nested one, just to give it a name. At least then its clear which lambda its referring to. Obviously with 280+ its too much to do everywhere, but you could do spot checks on the ones you are most concerned about. There might be tools in jitwatch or similar to indicate which lambdas refer to which code block, but I can't think of anything offhand.. I see putFast but that method doesn't exist anymore. There was a fast-path and slow path, which was merged into one. So you definitely are not running 2.6.2, just in case you thought so.. The only reason scheduleAfterWrite could be slow, if directly the contributor, is if the PROCESSING_TO_IDLE => PROCESSING_TO_REQUIRED failed and had to be retried. That's a CAS, so pretty much either one or two iterations. It shouldn't be a hotspot, if that's the claim of the profiler.\nhttps://github.com/ben-manes/caffeine/blob/fe68b00ac52b303dd20231123d61e7c3e03782ac/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L927-L954. One experiment you might do is run using Guava, since the APIs are identical in your case. Caffeine should be faster, but if not then that confirms your results. Might help figure this out?. > can you please help me to interpret these results?\nSure. This shows some of the internal states to see if there is a runaway conditions. So it helps answer a few questions like,\n - Is the cache able to abide by the high watermark (exceed it by a small amount, temporarily) or does it have a runaway leak?\n- Is the lock held by a single thread, e.g. is it stuck?\n- Are the read or write buffers full and never seeming to be drained?\n- Is the state machine (drain status) transitioning or get stuck?\n- Are there any crashes due to internal corruption when stressed?\nSome questions are answerable in unit tests, such as the small stress via the MultiThreadedTest, but a longer case can uncover more issues. It's also scaffolding to quickly hack and explore with based on a bug report, e.g. when I didn't think of a test scenario and it broke. It doesn't really show throughput numbers, though you could tweak it to retain a count of operations performed. All it can really do is indicate if things seem to be running smoothly without hiccups, which appears to be the case.\nPending reads / writes - ring buffer sizes\nDrain status - state machine to optimize the buffer draining\nEvictions - number of items evicted\nSize - current & max size (high watermark)\nCache Size - your addition? the configuration I guess\nLock - which thread holds the evictionLock if draining\nPending tasks - write operations to replay on the policy; an update w/o weight change can be mimicked as a read, which is faster.\nMemory - JVM memory use (if stable or runaway)\nFor determining the average latency (throughput) you can see the JMH benchmarks. That shows the upper bar, which we want better than the application can induce. However, beyond that we know from  Amdahl's law that further speedups won't be helpful. In Caffeine's case, we trade-off some surplus read throughput for trying to reduce the maximum latency (especially on writes), e.g. the long tail response times, to provide more predictable performance. The design decisions also have some benefits for how we implemented certain features, which was an added bonus.. Hi @Siva-R \ud83d\ude03 \n\nIs it not that when ring buffer is full, scheduleDrainBuffers() will be called?\n\nThere are two types of ring buffers.\n - The read buffer captures cache hits and consists of many small ring buffers (e.g. 1 per core) to reduce contention. When full, new events are dropped (lossy) because the data doesn't have a negative impact of the eviction policy (hot items are more frequent, so hits accumulate). This triggers scheduleDrainBuffers() when one of the ring buffers is filled.\n- The write buffer is a ring buffer used to avoid blocking all threads on the global policy lock. This is not lossy and blocks writers if full. When populated a drain is applied immediately, since this might require an eviction.\n\nIf yes, is there a possibility that because of small ring buffer size, scheduleDrainBuffers() is called frequently?\n\nYes, the the method will be called frequently but only executed by a single thread at any given time. The work involved should be very cheap and O(1), e.g. an LRU is merely pointer manipulation (access: remove node, move to tail; evict: remove head). The observation is that the work involved is cheap, but blocking on a lock is not (e.g. if you benchmark 1 vs 10 threads access a synchronized LinkedHashMap in LRU mode). We use intelligent scheduling to avoid the lock penalty.\n\nWill there be an improvement if we increase the ring buffer size? Can you suggest an ideal size depending on the load?\n\nA smaller read buffer actually has better performance. This is because adding to the ring buffer is a CAS to acquire the array slot. Since the additions are lossy, when full no work is performed so its free. That's under the belief that scheduleDrainBuffers() is inexpensive, but if not then you're right that increasing the buffer size would reduce the frequency of it being executed.\n\nIs the ring buffer size dependent on the type of thread pool executor?\n\nNope, it is dependent on the number of cores available to the JVM and amount of contention observed.\n - The read buffer uses 16 element ring buffers and adds additional ring buffers when CASes fail due to contention. This grows up to 4 * ceilingNextPowerOfTwo(Runtime.getRuntime().availableProcessors()). If you are on a high cpu machine, e.g. 32 cores, then it starts at 0 if unused to avoid wasting memory but grows up to 128 buffers if all are accessing it concurrently.\n- The write buffer is a single ring buffer that grows in length, starting at 4 and ending at 128 * ceilingPowerOfTwo(Runtime.getRuntime().availableProcessors()). In the 32 core case, that means 4096 write operations might be enqueued before blocking. This maximum would only occur on a bulk insert (e.g. cache warming) or many writers outpacing the immediate drain. In the worst case all these writes might be adding new items, so we exceed the maximum size by 4k temporarily. But that should be okay since a 32-core machine would not be memory constrained to that degree, which lets us support a high write throughput by the buffer batching the work under the lock.\nI hope that helps. Happy to answer more questions! \ud83d\ude04 . I would recommend putting your own timings to verify the accounting. If you are not using expiration, then the expiration ticker returns a singleton (enum) instance via Ticker.disabledTicker(). That has a single method that returns a constant. Ideally if inlining and dead code elimination occurs this call is removed entirely. In the case of expiration being enabled, then the ticker field should be inlined for the method, and may inline the implementation as well. If the implementation is slow (as System.nanoTime() can be), you can override it with a custom version that is cheaper but less accurate.\nIt may be that the instrumentation disallows optimizations (e.g. C1 instead of C2) or is assigning blame incorrectly. For example, this may be a GC safepoint. It doesn't make sense from a code perspective and seems to be a runtime aspect.. I think your instrumentation agent is the root cause, since class names like xyz.dt_0 are not mine. Does it only instrument request threads (and not ForkJoinPool)? Have you tried using an exclusion rule to not instrument the cache package, since the instrumentation overhead is likely the culprit due to access frequency? Hopefully excluding the cache avoids it incurring an instrumentation penalty, while also showing any hotspots in the calling code to verify if there is a performance issue.. @alejo-17 any updates here?. Are you sure? I ran your test and it seems to work:\njava\n@Test\npublic void test() {\n  LoadingCache<String, Long> cache = Caffeine.newBuilder().expireAfter(new Expiry<String, Long>() {\n    @Override public long expireAfterCreate(String key, Long value, long now) {\n      return TimeUnit.SECONDS.toNanos(3);\n    }\n    @Override public long expireAfterUpdate(String key, Long value, long now, long current) {\n      return current;\n    }\n    @Override public long expireAfterRead(String key, Long value, long now, long current) {\n      return current;\n    }\n  })\n  .removalListener((string, timestamp, removalCause) -> {\n      System.out.println(Duration.ofMillis(System.currentTimeMillis() - timestamp));\n  })\n  .executor(Runnable::run)\n  .build(key -> {\n        long time = System.currentTimeMillis();\n        System.out.println(key + \" | \" + new Date(time));\n        return time;\n  });\n  for (int i = 0; i < 5; i++) {\n    cache.get(\"1\");\n    System.out.println(\"Iteration #\" + i);\n    Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n  }\n}\njava\n1 | Sun Sep 23 09:31:31 PDT 2018\nIteration #0\nIteration #1\nIteration #2\n1 | Sun Sep 23 09:31:34 PDT 2018\nPT3.032S\nIteration #3\nIteration #4. Yes, the cache will only evict items when there is activity on it and doesn't use a scheduling thread. If you access the entry, access a few other entries, or write an entry then this \"maintenance\" work will be triggered. This is good enough for expiration of removing items, but does not give you a guarantee of when the listener will be invoked. That's usually okay or sometimes the workaround of calling cleanUp in your own scheduled thread is needed.\nLonger term, we might offer an opt-in for a scheduling thread for more active expiration. Some users want that for business logic that depends on the event being promptly fired (#195). The implementation approach we're thinking of would be to use JDK9's global scheduler thread (via CompletableFuture#delayedExecutor) rather than create our own threads.. It will be triggered if activity on the cache occurs, as in you are fetching or writing to it. If your cache idle and unused, then you won't get any removal event. So most often this isn't an issue when your application has constant usage, as the behavior is good enough.\nIf your business logic requires the event firing to be more strict, such that the item is evicted at approximately the right time even if the cache is idle, then this is not yet supported. The workaround is to have your own ScheduledExecutorService that invokes cache.cleanUp() periodically, e.g. every N minutes. That will do any of the pending maintenance work, such as expiration, and let your business logic run promptly. Then you would have a maximum delay based on the chosen period.\nIt is usually rare that you would need prompt firing and that the lazier approach isn't acceptable. Is it a business logic requirement that your listener is called immediately after expiration, or only a bit confusing that it is notified only when the cache is accessed (e.g. lookup of other items)?. hah, no worries. Mine is pretty awful and it's my only language \ud83d\ude04 \nHopefully my time will free up over the next few weeks as I have a lot of improvements to make. Just hard to find the bandwidth lately.... This was recently requested in #261, if I understand the requests correctly. Currently there isn't built-in support and I agree it's a pretty good idea. The implementation is probably not too difficult either, but I haven't had bandwidth to work on this library lately.. Sorry I've been swamped and falling really far behind. I usually work on this project during my weekends, but I haven't had any time or been too exhausted. So lots of things need to be taken care of.\nIt's odd that they require that now, given that Unsafe still works fine. Maybe it to prepare one LTS before the 2021 LTS that will likely remove Unsafe for good?\nFor v3, I would like to rewrite the JCache implementation to use the native per-entry expiration support, since that wasn't implemented when the adapter was written. Instead it has to use an uglier approach, but also used by the spec leads, of relying on maximum size with dead entries. Since this would change the configuration format, I want to make all of the breaking changes together. It might be better to update the config format and punt on the rewrite work, though, given my struggles to keep up with things.\nFor VarHandles, my only concern is how to port StripedBuffer, which uses the thread field (as forked from Java's Striped64). I think the VarHandle should peek into it, but not positive. So likely it should be updated with a fallback chain (e.g. to threadlocal field) just in case.. You might use a CacheWriter for that. It probably depends what semantics you want for flush. If you do so on size eviction, CacheWriter to persist and CacheLoader to fetch would be a good fit. If you are updating through the cache, you can of course use asMap().computeXXX which bypasses the loader and writer. Most likely the only thing you want to avoid is using RemovalListener since it is asynchronous to the cache operation, so you would have possible data races.. In that case you might just use the asMap() view and iterate over the contents.\nThe CacheWriter is mostly helpful when you need a synchronous hook for removal, e.g. when done automatically by eviction. But it's also a little brittle by not being able to intercept all calls, e.g. loads, since the various concepts (load, map methods) don't always mix together well.. Its true that it is not a strict LFU, as we do not retain frequency order. The latest work to appear at the Middleware Conf in December (can send privately) further evolves the policy to adapt towards recency or frequency depending the the workload. The goal is to maximize hit rates rather than be deterministic.\nThe implementation is explicitly non-deterministic, which can cause test failures. This is to protect against HashDoS attacks (explanation). This small amount of jitter protects against an attack vector without degrading the hit rate, but also adds enough noise to make tests flaky.. As a shortcut, you might be able,\n1. Get the cache, implicitly creating it, and fetch its configuration\n2. Destroy the cache\n3. Update the configuration\n4. Create the cache\nMaybe drop some of that if you use TypesafeConfigurator directly.\nI agree this should probably be configurable. Can you explain your use-case, since it probably was done for testing and I hadn't considered it.. Have you considered using Spring's native Caffeine support? That would let you configure the cache as bean with the executor. Our use of FJP is lightweight, so usually it doesn't compete much and setting to a same-thread executor is okay. We mostly use it to avoid impacting calling threads if user-facing, e.g. any latencies incurred by a configured removal listener. Otherwise the cache's maintenance work is cheap O(1) operations so fast and cheap.\nSorry for that bit of pain due to the missing configuration. I'm on a bit of a hiatus and I know a lot of work is piling up. I do hope to get back to this project, perhaps as things slow down for the holidays, but otherwise I can only promise to provide prompt responses and fix critical issues. So I hope you can live with this pain for a little bit longer, but thanks for pointing it out and we should definitely fix it.. Released 2.7. Thanks! Sorry it took so long to get to. This was a big release with some cool additions.. Try setting Caffeine.executor(Runnable::run) in the builder. This would then match Guava's default behavior. Guava runs the refresh operation on the caller, as in synchronously, due to the default behavior in CacheLoader#reload. To make it asynchronous you have to supply the future and executor. Caffeine takes advantage of ForkJoinPool#commonPool to run operations asynchronously by default, so you are observing the race condition this imposes.. Oh, it\u2019s because you have one final refresh at the end after the sleep and before the assertion.. Thanks @jbduncan. Yes, a writer is called within the underlying CHM compute method for put, replace, remove, and evictions. However it is not called on a user-defined compute method (load), since you already have an atomic scope that you control. So if you are loading the value, you\u2019ll need to generate the indexes there too.. That computeIfPresent will break due to recursive computations (index -> cache -> index) which CHM does not support.. It might be. The map may livelock, detect and throw an exception, or work - it\u2019s a bit unpredictable.. There\u2019s some discussions in https://github.com/ben-manes/caffeine/issues/234 that might be helpful. I used two maps like you are ages ago, with lock striping due to a lack of compute methods.. That looks reasonable, except that you should use ConcurrentHashMap for indexes as it is not thread safe otherwise.. Since mutations are rare (compared to reads) you might get away with a synchronized data structure. If you can simplify to a primitive key, then FastUtil is my current favorite for alternatives to look at.. Thanks! How did you find all of those?. Caffeine does have many generated classes, but they should be fine as only the configurations used will get class loaded. Does your system use class scanning that includes this library to force loading of all classes? If so, can you limit it to your packages?\nYou may want to increase the FD limit as it\u2019s low by default on Linux, if I recall correctly.. Can you provide a small sample application with this failure? I think somewhere (maybe a dependency) it is scanning too broadly.. Please reopen if you can provide more information or sample to debug from.. Unfortunately I don't know if there is a good solution here.\nGuava forks the hash table, so it can use a custom Map.Entry that holds a future value when recomputing. This lets the current value and future value be encoded on the entry (but optimized out when unnecessary). This lets to detect if a refresh is in progress to avoid redundant calls.\nBecause we don't customize the hash map, we maintain encode to a value wrapper Node. Currently we use a CAS to change the expiration time for refresh to avoid redundant calls and update accordingly when the refresh completes (See BoundedLocalCache#refreshIfNeeded). I think we'd have to retain an extra field on the Node if refresh was supported, which adds to its memory overhead. Given the recent requests for variable refresh (#272, #261) we might have to do that. But it does feel wasteful and we'd want to find ways to minimize per-entry overhead where possible.\nCurrently its a trade-off that most load actions should be shorter than the expiration time, so that the optimization generally works. It's not perfect and requires finding a good balance between needs.. Yes, I'll update the JavaDoc in refreshAfterWrite for the time being. The isComputingAsync is for AsyncLoadingCache where the value is a future. There is hasExpired for validating with, in case the expired entries haven't been purged yet.\nI think it's worth brainstorming possible fixes. I see a few options,\n\nUse a dedicated bit flag for whether a refresh is in progress, rather than extending the expiration time to disable refresh attempts. Ideally we would encode this to avoid extra state, such as by using the sign bit of the timestamp. Unfortunately nanoTime() allows negative times and overflow, so we'd need a new state field per entry. Due to alignment it might be free, but otherwise could add 32-bits per entry.\nAdd a future reference per entry. That adds 32-64 bit ref to each and is usually null.\nReplace the Node with a decorated version holding the delegate and future. Then an instanceOf check is a fast flag. However, this requires updating the map, a blocking call for a read when it may be in an expensive compute. That breaks the assumption that reads can overlap with writes.\nExternalize the state, e.g. in to a Map<Node, CompletableFuture>. Then when past the refresh time, each get would have to do a membership check. The write into this state would be cheap because there wouldn't be any long running computations. The additional lookup per read isn't ideal, but probably a good tradeoff. We have be extra careful about reads racing with removal, that could result in a dangling entry being left in this map.\n\nGuava's fork of the hash table lets it use an optimal mixture of 3 & 4, since the computations are futures within the Entry and don't block hash table operations. This also lets it detect redundant refresh calls, e.g. LoadingCache#refresh will no-op. We don't have a good mechanism for that currently, and forking was otherwise very problematic for Guava. Retaining the future has a slight benefit for supporting #143. So the plan will probably be to implement (4) to solve this flaw.. The Weigher interface does not extend Serializable (I think this was a best practices in Effective Java). However you can define it as such in your custom implementation so that the cache is serializable. The internal ones are enums for that property, e.g. see Weigher#SingletonWeigher. However custom ones have to define it themselves, and there are unit tests to verify this.\nDo note that the cache contents are not serialized, only the configuration, so you will get a new cache instance when deserialized. This matches Guava, which did it for FlumeJava (a Spark-like precursor).. Wonderful! \ud83d\ude04 . Yes.\nCaffeine is a fancy ConcurrentHashMap that includes eviction policies and other aspects towards caching. It avoids doing nasty things like creating its own threads, accessing files, registering into JMX that caching frameworks tend to do. The right way to think and use this library is that its a Map, so its familiar and just another utility like Guava. It doesn't have any additional lifecycle - it's just a ConcurrentMap.\nThere will be memory overhead because its ConcurrentHashMap plus additions, but we try to be kind by code generating to optimal classes for the given configuration (avoid excess per-entry fields) and lazy initializing. The space overheads, such as the LRU lists, is what you'd expect from a slightly beefier Map. The wiki page, memory overhead, provides a very rough estimate as JVM details make it inaccurate, but good enough for back-of-the-envelope discussions.\nAll operations are amortized O(1) time complexity, both user-facing and internal. So it will not degrade by having thousands of caches or one massive cache. Since read concurrency exceeds user needs, we steal a little overhead from it in order to utilize eviction & expiration policies that O(1) time.\nI have seen successful usages with one large cache and others with thousands (e.g. per http request). A few issues here and there that we resolved, but nothing major for quite a while now.. You can override CacheLoader.reload(K, V) to return the current value on an exception. Does that help?. If the reload method doesn\u2019t fail then the timer is reset. If you return the old value instead of propagating the exception then the cache will think it was successfully.. Guava forked Java 5's ConcurrentHashMap to add loading (basic computations). This was built as a future within the entry so that multiple threads could wait on it and the exception propagates. Guava switches the hash table entry (k/v, k/future. k/v/future) based on the context in order to minimize memory waste.\nCaffeine is built on top of Java 8's ConcurrentHashMap which can load through computeIfAbsent. Its built by using synchronized on the hash bin (~ per entry). This is why the exception cannot be shared as it is an intrinsic lock. The hash table entry is not swapped based on the load state, though different versions are used for a list vs tree bin for HashDoS protection.\nBecause of the different algorithms, we can't emulate Guava's approach without using a custom hash table. The Java 8 table is far more advanced, Guava's emulation of the compute/merge methods is poor, and they don't interact with loads correctly. I had brought this up to Doug Lea when reviewing his first draft of the Java 8 version, and he was against handling exceptions the way Guava did.\nLonger term I think users will be more familiar with ConcurrentHashMap computeIfAbsent than with Guava's. So from a behavior perspective could be argued Guava's will be surprising in a few years. I agree it has nice characteristics in rate limiting from a caching context. From a collection context, one could argue that's incorrect with capturing lambda using different parameters, so the failure with a different parameter set would be surprising. That may be quirky and surprising, but from a JDK perspective they should ensure correctness in puzzlers like that.\nI don't think we could do anything smarter than adding a note to the Guava wiki page. I probably couldn't do more than emulate your hacky solution in the Guava adapters, either. Unfortunately I don't have any good ideas for you.. I think one would be hard pressed to say ConcurrentHashMap isn't a low-level performance library then too. You could bring this up on concurrency-interest as it's really Doug's call on the proper behavior.\nThe hacky solution isn't very general by materializing the entry and requires extra storage of the exception, whereas we try hard to minimize per-entry waste. It would be an invasive change. It's becoming common place to rate limit / throttle on a generically on a sidecar instead of at every point within an application. You might consider using a rate limiter as a better solution to this problem.\nThe Expiry idea should work (duration <= 0). That's probably one of the cleanest ways.. Actually, you can have exactly the same behavior if you use AsyncLoadingCache. This stores a future that is removed if the result is null or a failure. That should behave identical to Guava.\nNote that the non-loading version hasn't been released yet, as I haven't been able to work on this project due to other commitments. You can emulate it in your test case as,\njava\ncom.github.benmanes.caffeine.cache.Cache<String, String> caffeineCache =\n    Caffeine.newBuilder().buildAsync((String key) -> (String) null).synchronous();\ntestConcurrentException(() -> caffeineCache.get(\"key\", this::doWork));\nThe output is,\n[pool-1-thread-2] 663140678692693: Thread started\n[pool-1-thread-1] 663140678638680: Thread started\n[pool-1-thread-3] 663140679292165: Thread started\n[pool-1-thread-5] 663140680252611: Thread started\n[pool-1-thread-4] 663140680059127: Thread started\n[pool-1-thread-6] 663140683278439: Thread started\n[pool-1-thread-7] 663140685093971: Thread started\n[pool-1-thread-8] 663140686922632: Thread started\n[ForkJoinPool.commonPool-worker-1] 663140688284229: Doing work\n[pool-1-thread-1] 663141693963491: Caught exception\n[pool-1-thread-2] 663141693963480: Caught exception\n[pool-1-thread-5] 663141693963951: Caught exception\n[pool-1-thread-7] 663141693979484: Caught exception\n[pool-1-thread-3] 663141693963541: Caught exception\n[pool-1-thread-4] 663141694121909: Caught exception\n[pool-1-thread-8] 663141694051358: Caught exception\n[pool-1-thread-6] 663141693984187: Caught exception\nNov 30, 2018 11:42:39 AM com.github.benmanes.caffeine.cache.LocalAsyncCache lambda$3\nWARNING: Exception thrown during asynchronous load\njava.util.concurrent.CompletionException: java.lang.RuntimeException\n    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)\n    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)\n    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1592)\n    at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582)\n    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\nCaused by: java.lang.RuntimeException\n    at ConcurrentCacheExceptionTest.doWork(ConcurrentCacheExceptionTest.java:63)\n    at com.github.benmanes.caffeine.cache.LocalAsyncCache$AbstractCacheView.lambda$1(LocalAsyncCache.java:206)\n    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1590)\n    ... 5 more\nThe exception is logged as a common mistake is to not handle failures on futures, so errors can be swallowed and lost. You can disable logging if desired through your logging framework (e.g. slf4j's jul bridge).. You can set Caffeine.executor(Runnable::run) to perform the work on the calling thread.\nRate limiting was if you were concerned about a query storm due to errors, as it would help still have a problem at the client-side, without failing over the shared resource due to a self-imposed DoS attack.. This looks right. It will extend the duration by X or 5X depending on the value.\nI got stuck using the poor long types due to performance, as allocating Duration objects could cause a lot of GC churn until value types. So I know that makes it a bit unfriendly.. PR https://github.com/ben-manes/caffeine/pull/263 adds the setting option. Unfortunately I haven\u2019t been able to work on this project but hopefully can catch up over the holidays.\nThere is a new research paper that corrects this problem by automatically tuning the window size based on the workload. I can email it privately if interested. I want to explore some further ideas along those lines.. FYI, the research paper with our strategy to fix this automatically is now freely available if you follow the README's link (see Adaptive Software Cache Management). This uses ACM's Authorizer service to let you bypass the paywall.\nI would like to explore adaptive moment estimation (adam) as an enhancement to the naive hill climber shown in the paper. The paper demonstrates that the techniques work, but leaves deeper optimizations to the community. In the case of hill climbing (aka gradient descent), the ML community has very good work on improvements. I am hoping to find some time over the holidays to implement their ideas.\nThe time/space overhead to the cache was shown to be minimal and the code complexity is low. After I have satisfied myself on the exact algorithm to adopt, I hope to find the time to implement it. This should then correct for this problem without requiring a user-visible configuration.. @jandam can you provide an access trace to verify my fix with?\nBelow shows the results for an extremely recency-biased trace. The eden queue (admission window) is increased from 1% to 80%, the maximum allowed in this prototype. That corrects the hit rate from 0.6% to the optimal 33.33%.\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2564\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Policy                                                                  \u2502 Hit rate \u2502 Hits    \u2502 Misses    \u2502 Requests  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 linked.Lru                                                              \u2502 33.33 %  \u2502 624,106 \u2502 1,248,216 \u2502 1,872,322 \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 opt.Clairvoyant                                                         \u2502 33.33 %  \u2502 624,108 \u2502 1,248,214 \u2502 1,872,322 \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sketch.HillClimberWindowTinyLfu (adam 1% -> 80%)                        \u2502 33.33 %  \u2502 624,070 \u2502 1,248,252 \u2502 1,872,322 \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sketch.HillClimberWindowTinyLfu (amsgrad 1% -> 80%)                     \u2502 33.33 %  \u2502 624,071 \u2502 1,248,251 \u2502 1,872,322 \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sketch.HillClimberWindowTinyLfu (stochastic_gradient_descent 1% -> 80%) \u2502 33.16 %  \u2502 620,863 \u2502 1,251,459 \u2502 1,872,322 \u2551\n\u255f\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2562\n\u2551 sketch.WindowTinyLfu (1%)                                               \u2502 0.60 %   \u2502 11,169  \u2502 1,861,153 \u2502 1,872,322 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2567\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d. The adaptive improvements are now merged. I think this should solve your problem, but I don't have a weight-based recency trace to verify with. There could still be some fine tuning around weight eviction from the admission window. I'm closing due to lack of response, but you are welcome to reopen anytime.. The concern is that the returned future is not the same instance as the supplied one (from mappingFunction). That could be surprising if a user depends on interacting with the underlying future.\nFor example, what if the future is supplied by an an http client and cached directly. If the client's future supports the cancel() method, then this would not be communicated. Similarly since futures don't have equality, one might have mapping somewhere else with lookups via reference equality. If the cache returns a different instance than the mappingFunction, it could break a reasonable expectation.\nSince a future highlights that there is asynchronous and concurrent behavior, it seems less surprising that a caller might race to obtain a failed future before its been removed. I agree your ask is the common case desire, but I think the principle of least astonishment promotes the existing logic. Your logic also leads to the cache returning both futures (one on load, one on cache hit), though that might be easy to resolve.\nA pending enhancement (90% done) is to add an async map view (Map<K, CompletableFuture<V>>) which would let you perform a conditional remove (remove(key, firstFuture)). Then your retry would be guaranteed to be a new load. That work is done except resolving statistic calculations.\nYou might also consider using Failsafe with a delay time for your retry logic. That's not a great fix, but you probably want a delay / backoff anyway rather than an immediate retry.. In your unit tests, you can set the executor to same thread to disable asynchronous execution (Caffeine.executor(Runnable::run).. Nope. The cache is a fancy ConcurrentHashMap and that sounds more frameworky.\nI use Guava's ServiceManager for startup/shutdown logic, e.g. start and wait until healthy before accepting http requests. Then let Guice aggregate all of the binded Service instances to register it, where each might do various work (such as initialize a spatial timezone index). Something like that might be more appropriate?. Spring Cache is meant to be a basic abstraction for the common, simplest cases. It doesn't support modestly complex scenarios like this, and the authors suggest using the underlying implementation directly. As in, they don't want to add more advanced features and don't think forcing all cases through their abstraction is healthy. In this case, I'd recommend using Caffeine directly.\nSpring doesn't provide the CacheLoader, as you figured out. Instead it provides a function to the cache.get(key, func) method. So we don't have a reliably stable reload method to invoke, which is why a CacheLoader is required for refreshAfterWrite.\nFor Spring questions, I think you're best bet is StackOverflow since I don't use it.. True, it is lightly touched on in the Guava compatibility, although the reason has changed now. It is not a goal of this project to run on Android.\nHowever, I think we could provide graceful fallback such as to ThreadLocal for broader compatibility. It won't be a goal of this project to target & optimize for Android, though.. Closing because neither of us originally considered the DEX issue.\nSince we codegen to reduce memory footprint, that's good for servers and more troublesome for Android. Users would have to use Proguard and know which classes to keep, which is flaky and annoying. I think Android requires so many different design considerations that Caffeine will never be a good fit. Another library or your custom implementation seems to make a lot more sense.. Your expireAfterCreate looks correct. The others should return cuurentDuration, as they are currently saying to never expire the entry if read or updated.\nThe currentTime is an unfortunate quirk due to the cache ticker using System.nanoTime. That isn\u2019t good for most user calculations since it isn\u2019t wall-clock time. So using your own clock is right, as you probably can\u2019t use the timestamp from the ticker.. Please reopen, etc if you run into other difficulties.. Are you running with JDK9+? I had early builds against that JDK when new and some things like the ErrorProne gradle plugin hadn't worked correctly then. It looks like I cleaned this up, but haven't thought much about JDK9+ since those early attempts. This project is still Java 8 and I haven't had the time to start on a JDK11 (v3.0) overhaul.\nOh, this is also probably due to this configuration. I don't recall why, but might be to avoid it in testing code where the checks are not necessary. It was probably back when you were helping me resolve false positives and I wanted to reduce the burden. We can try removing that conditional. The JavaPoet is codegen task that produces classes for inclusion into the compileJava task.\nhttps://github.com/ben-manes/caffeine/blob/55f5078d485bc14c14736a4a8d6a5e244e5b1467/gradle/codeQuality.gradle#L97-L101. If I add that line to BoundedLocalCache.java it gives the warning. Though I would have thought it should be an error, given the above. Anyway, it's working in some places and I agree it needs to be revisited.\n```gradle\n$ gradle build -x test\n\nTask :caffeine:compileJava\n/Users/ben/projects/caffeine/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java:229: warning: [NullAway] dereferenced expression x is @Nullable\n    Object x = null; x.toString();\n                      ^\n    (see http://t.uber.com/nullaway )\n1 warning\n. It seems to be due to using the [older approach](https://github.com/tbroyer/gradle-errorprone-plugin#custom-error-prone-checks) of,gradle\ndependencies {\n  annotationProcessor \n}\ninstead of the newergradle\ndependencies {\n  errorprone \n}\n```\nto register the custom checks.\n\nannotationProcessor -> compileJava\ntestAnnotationProcessor -> compileTestJava\netc\nThat caused the checks to not be run. I was previously using the apt plugin which was deprecated for Gradle's built-in annotationProcessor and didn't realize that they behaved differently in this regard.\nOf course now I get a bunch of warnings for code that I don't particularly care about. \ud83d\ude03 \nbash\n/Users/ben/projects/caffeine/caffeine/src/javaPoet/java/com/github/benmanes/caffeine/cache/NodeFactoryGenerator.java:110: warning: [NullAway] initializer method does not guarantee @NonNull field nodeFactory is initialized along all control-flow paths (remember to check for exceptions or early returns).\n  public NodeFactoryGenerator(Path directory) {\n         ^\n    (see http://t.uber.com/nullaway ). Fixed in the adaptive where I'm trying to wrap up the eviction policy improvements. They are ready to merge in except for a lack of unit tests.. Yes, that is what the loading (or Map.compute) methods do. They will ensure that the loader is executed atomically and that other callers wait instead of racing to load as well. In your case you want to use an AsyncLoadingCache. This will store a CompletableFuture into the cache and execute it asynchronously, so that other callers can obtain a future immediately while the load is in-flight.\nYou can enable stats, execute a test run, and see the output. You should see the minimal number of loads. \ud83d\ude03 . Thanks. I've been meaning to switch reads to use a compare-and-swap to update the timestamp, but thought it was a benign race so it was in the backlog. I hadn't thought of this case, obviously.\nDo you think you can write a test case to demonstrate the race? I'm hoping to make a release this weekend when I merge the async branch (fix test cases). I'll try to fix this too for that release.. Oh, take a look at Awaitility which is really handy for these cases. For example,\nhttps://github.com/ben-manes/caffeine/blob/1c71c67dfafef0bcf95872e2df590116fc8669ed/caffeine/src/test/java/com/github/benmanes/caffeine/cache/AsyncCacheTest.java#L202-L218\nbut I can also give it a shot when I get free over the weekend. Its a 3 day'r in the U.S. so I should be able to find the time.. okay, thanks. I think this is an easy fix if I understand it correctly, as I've been meaning to get around to making the update on read stricter. Since you found its not easily testable, we can try testing using a thrashing approach, like the MultiThreadedTest does.. I have a possible fix, but I need to manually verify like you did to see it fail and (maybe) succeed. Then devise if I can get a test case by thrashing it with threads until it breaks. Hopefully have this fixed and released over the weekend.\nhttps://github.com/ben-manes/caffeine/tree/expiry. I wrote a test, got it to fail, and verified that my change passes it. This was using Awaitility to setup the at Expiry calls.\nTechnically I shouldn't call expireAfterRead if the currentDuration is greater than the user settable maximum (220yrs in-flight, 150yrs cap for user setting). That is both more correct if there is logic in the expiry (perhaps even expensive) and avoids this problem. It also breaks my test as I can't wait.\nI'll take another crack at the test, but otherwise I'm close enough to promise a release over the weekend.. Released 2.7. Great! Thank you for investigating and reporting the bug.. If you use the computing get apis, then only one thread will do the work per key and pass extra information through to the lambda. Then you don't need all of the information on the key while still loading. If that doesn't work, you might consider some form of lock striping to avoid the stampede.\nUsually refresh is the mechanism to do this, but since your keys lack enough details there isn't a means through the cache itself. You might need to build some queuing system around the cache to schedule a prefetch. It's not very obvious since your keys lack enough details to load, so I don't know how you can associate when expiring regardless.. Oh, at then you are restricted to their abstraction. I haven't used Spring since 1.x - 2.0, so my knowledge is very out of date. You might ask on StackOverflow?\nI do know that the Spring team views it as the common case abstraction, and not intended to be feature rich. For more advanced cases they recommend using the implementation directly. I don't think you have the desired feature directly in their abstraction, which means you may need to discard it.. You can use the cache.policy() api to inspect lower-level details, like expiration times.\nI don't know what you mean by immolation. You can use the get/put or Map apis to store items.. We do support that, but in terms of Caffeine#refreshAfterWrite, LoadingCache#refresh, and Map.compute methods. If refreshAfterWrite is used, then the item will be reloaded (by key and old value) before expiration if accessed after the refresh time period. This is typically the solution to your problem.. Yes. We don't have a refresh(key, function), which I suppose would do this. Otherwise you'd need hacks like a request-scoped object (e.g. ThreadLocal) to pull from.. You might also consider wrapping the value with metadata, to determine if it should be reloaded. Then use an atomic boolean to CAS so that only one thread does the work. So then (1) fetch, (2) check, (3) CAS, (4) reload. That would be outside of the cache but would give you the desired properties without much pain.. Thinking through this, I think that using custom metadata is probably the best to reload with. When it fails your check, you would use an asMap().compute method. I don't think there is an addition to the API or general logic that I could add to do this any better, and most likely would be worse than a custom solution. Since the custom logic should be very straightforward (at least outside of Spring), I think it okay to close this issue. Please reopen if you disagree / have other thoughts.. The side effect should only be as benign runtime dependencies, unless you are seeing more issues? The intent was to follow Guava\u2019s example which has\nhttps://github.com/google/guava/blob/2763649a49907a645b97afd860ca74257368ab45/guava/pom.xml#L33-L40\nSome JVM languages can\u2019t handle missing annotations so excluding causes surprising errors. The old jsr305 annotations are not module friendly so moving off was requested. Excluding should be safe and okay. Or did I screw something else up?. I\u2019d prefer to have it as an optional dependency, but the thread on Guava shows it\u2019s annoying for people either way.... Oh thanks. Note that this is only for the project's own build (plugins like JMH) and wouldn't impact user's, but definitely worth fixing.. There are already checksum files generated on the repositories that the dependency manager should verify with, so I don't think that plugin would help?\nFor example,\nhttps://repo1.maven.org/maven2/com/github/ben-manes/caffeine/caffeine/2.7.0/. The build is reproducible and everything appears to match. . Unfortunately distributionSha256Sum isn't generated with gradlew wrapper.\nMaven's workaround for dependencies is the enforcer and pgpsigner plugins. Gradle had the witness plugin, but it's been abandoned since 2014 and appears to no longer work. Neither seem to have put much effort here.\nCentral, Spring, and jitpack repositories have good enforcement policies that make it easy to track the history of dependencies. The most unsafe is bintray since they allow stealing a coordinate and by their very lax rules. Sadly none seem to instruct the http clients to redirect to https.\nSo the ecosystem is in a bit of a bad state I guess.. Thanks!. I don't know enough about the mechanics to make a fair judgement. If you mean to use different caching policies, then I think using the multi-level cache is best. Since Caffeine will adjust to the best configuration, you shouldn't have to tune this level.\nI do know that LRU tends to perform poorly in db / analytics / search workloads, because those queries scan lots of rows which flushes LRU. Postgres and ElasticSearch use local LRU buffer caches to protect their main LRU cache from this pollution, thereby holding more frequent items. If you were to have use only LRU on historical nodes then you may see very low hit rates. I guess what you're thinking is then to have a filter to determine when to promote to the remote cache, thereby reducing pollution / serialization cost / wasted bandwidth, which makes sense to me. The first step would be to quantify the cost and benefit from an X% improvement, to determine if it is worth investing in.\nI don't feel comfortable making a recommendation, but would be happy to help brainstorm or analyze data.. fyi, @drcrallen had a nice chart of the multi-level cache performance in Druid, which may help?\nhttps://github.com/apache/incubator-druid/pull/3028#issue-71618510. I'll close this in favor of https://github.com/apache/incubator-druid/issues/7186 since this isn't directly related to Caffeine. Happy to discuss further wherever though.. We probably won\u2019t add more than the basic stats, but it\u2019s a fair point. Usually the eviction cause metrics won\u2019t be useful, e.g. all expire eventually if unused. The eviction metrics we currently offer are mostly for sizing which is manually determined, whereas expiration is typically about data freshness.\nIn your case, would capturing custom metrics in a RemovalListener suffice?. Sorry for the misunderstanding. Yes, it\u2019s of all eviction types (but not manual removal). The cause is not taken into account.. For the future, I think we can add recordEviction(int weight, RemovalCause cause) and deprecate the prior one, like we did when adding weight. The cause won't be utilized in the standard stats, but you can implement your own StatsCounter to push (instead of polling from Cache.stats()). That would allow optionally capturing this metric. Potentially we could do that for other cases (manual removal, puts, etc) as requested, where the default behavior ignores those operations but custom ones can instrument as desired.\nhttps://github.com/ben-manes/caffeine/blob/4a8c9690e56e24f24c194ffe9bafa60171994146/caffeine/src/main/java/com/github/benmanes/caffeine/cache/stats/StatsCounter.java#L73-L94. Yep! I use it on Java 11 at work without a problem.. I had started to think through this in #7, but never finished. For now you are right we only support individual keys. If we added support for a reloadAll in CacheLoader then you could delegate to the cache a bulk operation. At the moment you could instead write a bulk load method and use putAll to insert them into the cache.. Are you sure? This asserts that there is no value par (e.g. recordStats=true). The condition that value == null is asserted. See CaffeineSpecTest for the unit test.. Right, the stats (as originally defined by Guava) would be\nspring.cache.caffeine.spec=recordStats,maximumSize=5000,expireAfterAccess=600s. No problem, it is a little confusing. Maybe we could make it optionally handle true/false in the future.. By default we defer all work to ForkJoinPool.commonPool, whereas Guava does it on the calling thread. If you set executor(Runnable::run) then you\u2019ll see the same behavior as Guava.. Many of the behavior differences are described in the migration guide\nhttps://github.com/ben-manes/caffeine/wiki/Guava. Thanks, that makes sense. I would accept it in the examples directory and then we might consider if it should be promoted as a supported feature. That way others can benefit from your work without library design (e.g. scope) being a blocking consideration.\nI do wonder if rxjava / reactor would make this easy to implement, like it did in write-behind-rxjava for collecting writes to batch update? If so then it might not be much code and simpler as an example integration between libraries.. That makes sense and I agree those are the characteristics you'd expect. I haven't gotten used to the reactive-x libraries yet due to poor fits in my day-to-day work. Do you want to submit your implementation as an example?. The code for AsyncLoadingCache#getAll could easily be generalized to call a function instead of the supplied loader. So I think this is a pretty minor change, and copying the unit tests to over cover this variation. We should probably do the same for LoadingCache?\nFor the method signature, it would be that and a variant where the function also accepts an executor and returns the future. That way if your bulk call is asynchronous you can pass back the client's future.\nI use Guice and most often my context would be request-scoped, e.g. the calling user's details. I would then be able to inject in a Provider<UserContext> and resolve to the scoped instance during the call. When setting up other threads, I establish the scope for that execution (e.g. via ServletScopes.scopeRequest). It is likely a thread-local thing, but using DI to propagate things instead of passing them through manually. For Java that's quite natural, but may feel awkward in Scala's idioms.\nhmm, not sure about the CLA - will look. I'm lenient about it since it's not really required, but seemed like an easy thing to do to make companies feel more comfortable adopting or contributing.. This doesn't appear to need to be in their library's package. Instead put it in our admission package as package private.\n. Oh, I see. You do need to do this to access their package private fields.\n. LFU --> Lfu to conform to Java conventions for abbreviations in camel-case names.\n. Please delete the commented out code following (// new ...)\n. Typesafe Config recommends snake case so this should be sample-size\n. Can we use JavaVersion and keep errorprone if on JDK8?\n. I think you meant this to remove the not condition.\n. or maybe you meant to write it as a 1-liner.\ngradle\nenabled = !JavaVersion.current().isJava9Compatible()\n. Ehcache3 is painfully slow for large traces due to O(n) eviction (it naively scans the cache to construct a random sample). You'll want to turn it off most of the time. (It was even worse until 3.1.2 when it was MRU causing a high miss rate).\n. Lets move both of these into dependencies.gradle under benchmarkLibraries. I think this can be removed in favor of Guava's hasher (Hashing.murmur3_128()...asLong()). java\npublic void setStep(int step) {\n  this.step = Math.max(1, Math.min(15, step));\n}. fyi, Java will call this automatically so not needed.. Please add Apache license header (copyright to your name) and the a little class JavaDoc. . Update year and switch to your name?. Why the odd formatting? We use 100c lines so maybe an auto-formatter quirk?. I think we should generally prefer having each policy independently namespaced to avoid confusion. Do you think that makes sense to do here or be more surprising?. Do you think that it would make sense to have a single FRD climber class and abstract the algorithm, like we tried to do with the TinyLfu version? Or do you think that is more confusing to debug through?. indent. Can pull make this constant a configuration setting?. This seems odd and maybe we should refactor the CountMin4 constructor and have an indicator block in the configuration file with this constant. . Should this be configurable?. Should this constant be configurable? Should it change based on the cache size?. JavaDoc. return Adaptation.HOLD for the constant (I know this api sucks). Guava case described above. This means it will try to adapt every access after 1000000? Should it be a modulus? Should it be a configuration constant?. Can we pull it into the config file?. ",
    "gitblit": "Clearly you have a better handle on caching than I do.\nMy understanding of JCache is that it is an abstraction layer like SLF4J.  The distributed caching providers (Ehcache, Infinispan, Hazelcast, etc) are all JCache providers.  The frameworks that have spent effort to define and abstract their own cache interface (Spring, Play, etc) may not care about JCache.  The Spring abstraction you cite supports the JCache annotations, but that is not to say it is a JCache provider.\nThere are Guice extensions provided by the reference implementation.  That's how I am using it.  It's mucho bueno.\nLike any abstraction, it's lowest common denominator functionality.  Interacting with the provider in this way allows the provider to be runtime-pluggable.  My app may not need the distributed power of Hazelcast so a lighter-weight caching solution would be more attractive but as my needs change I can swap it out.  Or possibly I want to swap providers based on production vs. development - similar to how many shops use H2 or SQLite.\nIf I do want vendor-specific power then I can choose to configure my provider through a vendor-specific config file (ehcache.xml, infinispan.xml, etc) that can be specified with the JCache abstraction layer.  I'm not suggesting that this part of the spec be supported.\nHowever, being able to define Caffeine caches with JCache's builder and reference caches using Guice+JCache annotations would be huge. To be transparent, my end goal would really be to see the official Guava Cache become a JCache provider since I regularly use Guava as a standard library in my projects.\nIt's not clear to me what your long-term plans are for Caffeine.  I am assuming that your design will be used to inform a Guava Cache rebirth.  Either way, I think that any new Java cache implementation should not ignore the JCache spec.\n. Fair enough.\u200b\nI'll keep an eye on your project & thanks for the consideration.\n. That's great news!  Your choice of TypeSafe Config works for me - I use it\ntoo.\n. Like I wrote before you clearly are more expert at caching than I.  I have simple needs and I'm not doing anything elaborate which is why the JCache spec attracted me.  Your review of the JCache spec is not very flattering, but I'm pretty sure it's scheduled to be part of JEE8. :|\nEither 2.2 or 1+2.4 is acceptable.  This is your baby and I'm ok with either solution.\nThank you so much for really digging into this!\n. Awesome!\n. ",
    "abatkin": "To clarify: Would the solution reloadAll(Map) imply that refreshAll(keys) (where some are present and some are not) would end up with calls to both reloadAll(Map) and loadAll(keys) except where both are using the default implementation?\nOr are you saying that the default implementation (in the interface) of reloadAll() will actually call loadAll() (even though the user may have provided their own implementation of loadAll()) and in that case you optimize to call loadAll() once? This sounds pretty slick and I'm imagining that it would be better than the extra Iterable - not only is it cleaner, but most people would probably end up pulling out all of the keys from the Map in reloadAll(Map,Iterable) and then adding all of the remainingKeys anyway, which is a ton of extra/unnecessary work.\n. The table looks entirely reasonable.\nThe only thing of note is that existing code that has implemented loadAll() could suddenly experience a change in behavior where loadAll() is now called to reload items (instead of n calls to reload(). I'm guessing that in most cases this is fine (and may even be a pleasant surprise, since they wouldn't have implemented loadAll() if they didn't care).\n. Very interesting. I agree that refreshAfterWrite does 90% of what I want, with slightly different semantics (and given the design constraints for Caffeine, they are entirely reasonable). I'm not sure why I didn't fully grasp that when reading the documentation, but after reading through this conversation and re-reading the docs, it is really quite clear that's exactly what I want.\nThe reload() method doesn't make it easy to batch up refreshes for later, but that's covered in #7. At least from my perspective, the easiest way to implement batch-reloading would be to somehow use the refreshAfterWrite logic but turn that into more of a notification, at which point I'm free to do whatever I want to batch up reload requests and manually putAll() them back. Of course that opens its own world of complexity too, but at least that's on the client side (like what to do if an item is actually evicted while my reloader is still processing, since if I'm not careful there could then be two in-flight requests for the same key).\n. Thanks again for your help - unless you see this is a useful placeholder for anything, I think it's safe to close this issue.\n. If I had to vote, I'd say that you can intuitively understand what \"retain\" and \"preserve\" will do without having to think, whereas with \"resolve\" it is not clear exactly what that means.\n. I see what you are saying. And names are important. I have no preference between \"retain\" and \"preserve\", but vote against \"resolve\" (even if it might drive more folks to read the documentation).\n. ",
    "Dirk-c-Walter": "I have run into this issue too. Since the overhead for loading a single item is huge I just use an Async Loader and  queue the reload requests until I have enough to run a batch. Not the cleanest solution but a workable one.\n. Yes, and your code snipets give me some ideas for improvements, thanks.\nAdmitedly I am mostly working with an odd duck data source, the overhead for a single item is huge, but it's constant as batch sizes go up, and it returns an async stream of results. So the effect is much like a list futures, one where I don;t know the key until I get the actual item. I think instead of the loader I will just have to populate the cache myself to get it working well with that.\nthere are plans to move to more async friendly systems at some point in the future, but that is likely after Java9.\n. That would be usefull for a few scenarios. I'm currently emulating such a thing by avoiding calls to get and just using getIfPresent, since I do want the loader for reloads. My usecase is  probably a pretty rare one though.\nWhile we are wishlisting anyway; One other option that would be usefull is an integrated blacklist for known bad keys, currently that is handled by a second cache and just testing keys against that but something more integrated might be usefull.\nthe final thing on my wishlist would be for the AsyncLoadingChace to be able to return the estimated size directly instead of having to hop over the synchronous view.\n. I hope this can be of some minor help. :)\n. ",
    "dalegaspi": "@ben-manes i'm not sure if i'm understanding this feature or if there's a real issue.  I'm using the loadAll() feature, and i'm using the scaffeine wrapper (which i'm sure you're familar with) which doesn't muck around with the loadAll feature (as shown here)\nhere's my simple code to demostrate this:\n```scala\nval cache = Scaffeine()\n     .refreshAfterWrite(100 millis)\n     .buildString, String\n       key\n     }, allLoader = Some((keys: Iterable[String]) => {\n       println(s\"keys: ${keys.mkString(\",\")}\")\n       keys.map(k => k -> k).toMap\n     }))\nval values = cache.getAll(Seq(\"1\", \"2\", \"3\")).toList\nThread.sleep(200)\ncache.getAll(Seq(\"1\", \"2\", \"3\")).toList  \n```\nnormally since i have expireAfterWrite() and not explicitly calling refresh, i would expect the loadAll() to be called twice, but that's not the case...instead, the load() 3 times resulting in:\nkeys: 1,2,3\nkey: 1\nkey: 2\nkey: 3\nreplacing it with expireAfterWrite() behaves as expected, though...but with the obvious penalty of blocking.\nthoughts?. Ah...thanks for the quick answer @ben-manes, i shouldn't have assumed that this is implemented since the ticket is still open \ud83d\ude0a.  I guess for now i'm gonna see what i can do with what we have.  @Dirk-c-Walter's solution is interesting but it wouldn't work for me.. unfortunately, i don't have a unit test available that easily reproduce the problem.  not sure if this is relevant or not...but I'm using the (default) ForkJoinPool for the asynchronous loading but the read happens in another thread (which uses a different thread pool which is a ThreadPoolExecutor).  i don't write to the cache explicitly (just relying on the automatic async loading provided by the LoadingCache). ok this is embarrassing...i thought it's doing a premature refresh, turns out that the refresh is happening on a different cache...therefore it's working as expected.  apologies for raising a false alarm.  but once again i appreciate the swift replies @ben-manes.  I'm closing this ticket :). ",
    "nitsanw": "Q: \"I'd be interested in hearing your feedback\"\nA: \"I took the lack of documentation and minimal test suite as indicating it is a library for experimenting and demonstrating ideas.\"\nThanks!\nThe test suite is indeed minimal, but covers most of the relevant API and I'm happy to say I've had no complaints on the released version (the state of the master branch is at times experimental).\nThe documentation is lacking in places, but the many of the classes have a good javadoc I think. In any case, I'm always happy to fix shortcomings as they are pointed out to me, so please file issues where you see fit. The main data structure JCTools makes available at the moment is Queue and that is a well understood API.\nI'm aware to date of a few big projects using JCTools(or parts thereof) in production. So... it's not a demo project, and it has some happy users. But it is young and far from perfect.\nI realize your main focus is on caching, which is why I think we can assist each other on the concurrent data structures part. I would be interested to hear what it would take for you to consider using JCTools where it meets your needs and maybe contributing remaining parts for the benefit of all.\n. happy to take this chat off-github, Skype: nitsanw.work\n. The problem is:\n(head.getNextRelaxed() == null) doesn't mean the queue is empty.\nThe real indicator for empty queue is (head == tail)\nThis sucks, but there's really no way around it AFAIK.\nThe solution I arrived at in JCTools MpscLinkedQueue :\n```\npublic final E poll() {\n    LinkedQueueNode currConsumerNode = lpConsumerNode(); // don't load twice, it's alright\n    LinkedQueueNode nextNode = currConsumerNode.lvNext();\n    if (nextNode != null) {\n        // we have to null out the value because we are going to hang on to the node\n        final E nextValue = nextNode.getAndNullValue();\n        spConsumerNode(nextNode);\n        return nextValue;\n    }\n    else if (currConsumerNode != lvProducerNode()) {\n        // spin, we are no longer wait free\n        while((nextNode = currConsumerNode.lvNext()) == null);\n        // got the next node...\n    // we have to null out the value because we are going to hang on to the node\n    final E nextValue = nextNode.getAndNullValue();\n    consumerNode = nextNode;\n    return nextValue;\n}\nreturn null;\n\n}\n```\nThe conventions are slightly different (sp = store plain, lp = load plain, lv = load volatile), but I think the intentions are clear.\n. Imagine 2 threads.\nT1 is a producer and has been interrupted at the line pointed in first comment.\nT2 is a producer and the single consumer:\nT2 offers -> no problem, wins the CAS and sets the next pointer on the tail that T1 set up\nT2 polls -> queue is empty\n. I'll have a proper look at some point. This is a recurring bug, happened to me too.\nThe thing is that Mr. Vyukov is not bound by the Java Queue contract and we are, so need extra care when assuming offer == offer and poll == poll.\n. My bad, coffee underflow error.\n. Yuck :P never liked that one...\nI'm traditional, I'll put the kettle on.\n. JCTools queues benchmarks are (I think) good. The 'state' of your benchmark can have significant impact on the results (i.e. is the benchmark measuring behaviour at full/empty/other). There's some interesting issues around concurrent code warmup/profile. It's hard to get right and I'm not 100% sure I did...\n. JCTools now has a growable MPSC (in experimental, but will move to core shortly), was not familiar with the paper you point to, will have a look.\n. @ben-manes \"I think that I can weaken JCTool's spin loop safely, because the element will be consumed on the next maintenance cycle. That is scheduled after the write buffer is appended to. So assuming the buffer isn't losing the element, it should be visible the next time around.\" - this approach will not work. If producerIndex is not consumerIndex, and the consumer is not making progress then they cannot be equal on the next cycle, which brings you back to looking for the next element in the array, which is not there because if it was you'd not be stuck in that loop.\n@normanuber Can you please quote the JVM version (java -version output) and OS? it's a long shot, but certain JVM builds have concurrency bugs and it's a variable that is easy to eliminate.\nAt a glance the code seems very similar to the original, I would have to diff line by line to verify. Is there absolutely no way that 2 different threads are acting as consumers?\n. @normanuber would you be able to run a hacked caffeine build to help narrow down the source of this issue?\n. @ben-manes Thanks!\n. @normanuber please report back if the issue is still present with vanilla JCTools 1.2.1\nI'm holding off a release on the chance that this is an issue I need to resolve.\nThanks!\n. Thanks @normanuber for your efforts in testing and verifying :-)\nThanks @ben-manes, I'll be in touch but expect a release this week. I would love for you to shadow JCTools, but I understand it can't be your top priority :-)\n. @ben-manes \"JCTools would be expected to have a stronger (semver) compatiblity story. E.g. the constructor for the queue was removed in 1.3 in favor of a new subclass.\"\n1.3 is not out, and given the above should perhaps be called 2.0. Thanks for the feedback.\n. @ben-manes note that v2.0 is out with fix. I can do a 1.2.2 bugfix release if it helps your cause.\n. ",
    "txshtkckr": "Obviously it only matters when there is a listener, but we will always have one for our use case because we are mapping events back to another API that allows dynamic registration of its listeners.  Some options that occur to me:\n1. Use a read/write lock.  This is what I'm using to work around the issue in my spike, by acquiring the read lock inside of the cache loader and dropping it in a finally block around the get if it is held by the current thread.  The wrapper for invalidateAll acquires the write lock.  I can do this without changing the underlying implementation, so that's what I'm going with for my spike.  The main drawback is that it's pretty sloppy to have the cache loader acquire a read lock that code outside the get has to release iff this thread holds the lock.  This works, but it is gross, and I don't know how well it will handle high lazy-load concurrency.\n2. Decorate map items with a version number and bump that version number in removeAll.  The decorated loader would read the version number before calling its delegate and the cache would return values that include the version so that get(K) can verify it.  The drawback here is that the underlying datatype being stored has to change to something that includes the version, otherwise only the thread performing the load() call will know what version applies to the loaded value; other threads that merge in to the computation may see the newer version number but still get the stale value.\n3. Hotswap that cache's underlying storage by making data an AtomicReference.  The implementation of removeAll would create a new ConcurrentHashMap and use getAndSet to replace the current value of data before starting the cleanup process.  The code that does the loading would have to check that the data reference hasn't changed before returning, because if it has then it will need to send a removal notification for the value that it just loaded.  Seems hard to avoid races, here, but as both removeAll and the failed-to-put-back cases should be rare, using heavier synchronization around just those paths could be doable.\nI haven't looked at the async option, yet.  If other threads can wait on that CompletableFuture then that sounds promising.\n. Ahhh, I didn't think about whether or not invalidate  would block on the currently loading keys, but I suppose an explicit removal on a key works regardless.  That raises a few more questions, though:\n1. How does the explicit secondary ConcurrentHashMap for tracking the loading keys would compare to the read/write lock in overhead terms?  My understanding is that ReentrantReadWriteLock uses ThreadLocal storage to keep track of read lock ownership, and having additional entries in the ThreadLocal map for every one of these might be unacceptable.\n2. Is it strictly true that the loader is never entered from more than one thread for a given key, or can interactions between load and remove lead to multiple threads loading the same key concurrently, in which case the loadingKeys set wouldn't be good enough.\n3. Most importantly, what you've described still has a small race between loadingKeys.remove(key) and the reservation node getting updated.  If a removeAll looks during that window, then it will not see the loading key entry even though the value it is loading is stale and will survive.  It wouldn't be safe to call loadingKeys.remove(key) until after computeIfAbsent has returned.\nIt does occur to me now that I think about it that you are already doing what you need to in order to know whether or not to call remove on the loading keys, however -- to track the statistics you are already passing in a boolean[1] called missed so it can record whether or not the loader got used, and that's exactly what I need to know in order to decide whether or not to unlock, remove from loadingKeys or whatever...\n. In my spike (and in my workaround for Guava as well) I went the fair R/W lock route.  While this does mean that new loads have to wait, this should be offset by the rarity of doing a removeAll in the first place -- after all, if removeAll is something you're doing frequently, then why bother with the cache?\nBut I agree that the ConcurrentMap approach with loadingKeys.remove moved outside is worth a look, though I think that instead of using it like a set, I think it would have to be a ConcurrentMap<K,Long> and remove from it using loadingKeys.remove(key, Thread.currentThread().getId()).  I don't love the idea of throwing an extra native call in there, but R/W lock would do one as well, and the point is that you need to know whether or not this thread is one that started the request.  The main case I worry about is this rather unfortunate scheduling coincidence:\n| Thread 1 | Thread 2 |\n| --- | --- |\n| computeIfAbsent |  |\n| loadingKeys.add |  |\n|  | computeIfAbsent |\n| computeIfAbsent returns |  |\n| loadingKeys.remove |  |\n| invalidate |  |\n| computeIfAbsent |  |\n| loadingKeys.put |  |\n|  | computeIfAbsent returns |\n|  | loadingKeys.remove |\n|  | invalidateAll |\nThread 1's concurrent load doesn't get invalidated because Thread 2 removed a loadingKeys entry that didn't belong to it.  sigh\n. Another detail....\ninvalidateAll() {\n  delegate.invalidateAll();\n  for (Iterator<K> it = loadingKeys.iterator(); it.hasNext()) {\n    delegate.invalidate(it.next()); // blocks\n    it.remove();\n  }\n}\nFor correctness, I think you have to do this in the other order -- that is, you should call delegate.invalidateAll() after iterating, not before it.\n. No worries.  It's admittedly a very hard problem, and the fact that all of the technologies we have already been using (Guava, EhCache, and Hazelcast) also fail the concurrency problem in one way or another should be some indication of just how hard it is to get these things right.\n. ",
    "dpratt": "This is very much on my radar - I don't want to delay you, but would a discussion early next week derail you?\n. ",
    "cruftex": "Hey, just got a ping by bens' post. I missed that here are some decent thoughts on async cache APIs....\nHere is what I currently have in mind:\nFrom a \"light, simple and high performance\" perspective I consider the CompletableFuture/CompletionStage as an impedance mismatch. The CompletionStage interface is intended for chaining tasks that are time consuming. OTOH it is the nature of the cache, that the task should not be time consuming (remark: some caches are always time consuming, too....). \nIn the hope that the cache is doing its job, is it possible to implement the critical hit path efficiently? Idea: In the hit case return a CompletionStage that executes non-async methods within the calling thread. Only if the async methods are used the heavy CompletableFuture \"engine\" is needed. If this is feasible (by defined semantics), then the CompletionStage with a custom implementation for the cache use case would be preferable.\nIf the cache guarantees that there is always a Future, then declaring only CompletionStage would be not perfect in terms of API design. What actually is missing in the JDK (any discussion pointers here?) is an interface that implements both, the CompletionStage and the Future. But no one stops a cache API to do this. As a gut feeling I would stay away from returning CompletableFuture in a cache API, since it is an implementation, not an interface.\nMore progressive thoughts:\nAll interfaces I have seen yet, always have one operation and one result or one result interface. What about:\n``` java\ninterface CacheOp {\nCacheOp get(K key);\n  CacheOp getIfPresent(K key);\n  CacheOp put(K key, V value);\n  ...\n  CacheOp atomic(boolean f);\nCompletionStage> executeAsync();\n  Map execute();\n}\n```\nUsage:\njava\n    Cache.operation()\n      .get(47)\n      .get(11)\n      .put(12, 34)\n      .atomic(true)\n      .executeAsync()\n      .thenApply(....);\n. Good job, many thanks for this!\nComments:\npublic V get(K key) {\n  cache.get(...\nI think it should be cache.peek() which is the cache2k semantic equivalent to cache.getIfPresent() in Guava. cache.get() means the cache is the authoritative source in a read-through configuration.\nCache2k_Lru and use of CacheBuilder.implementation()\nIn version 0.21 LRU is still the default. This will change in the next versions. CacheBuilder.implementation() is an API hazard. It will go deprecated and disappear in the near future. One of the aim in cache2k is to provide a universally good cache replacement algorithm (which is not LRU), so, consequently there should be no option to change the cache implementation on the API level. I don't want the normal user to bother with this.\nIf you do not use CacheBuilder.implementation() there is no need to depend on the cache2k-core module. Just the cache2k-api is enough. Everything in API module gets a proper and careful evolution.\nSo the question is:\nOption 1: Just add a \"Cache2k\" to the benchmark. This will come with a replacement algorithm better than LRU in the future.\nOption 2: Expose the different implementations and run benchmarks on it, like:  Cache2k_LRU, Cache2k_ClockPro or even Cache2k_Random?\nI would say 1 is better. It keeps your benchmark thin and OTOH I don't want Cache2k to be connected with good old LRU that much any more :)\n. BTW: I'd like to put you together a pull request. I just try to get the source into Intellij Idea 14.1, however, the needed dependencies for the JMH does not get added the the IDEA project. Probably jmh.gradle needs a tweak, but I have no idea where to start here.\n. clahub says: \"We're sorry, but something went wrong.\"\nProbably: https://github.com/clahub/clahub/issues/109\n. Is there anything I can do? Should I write an email stating the consent with the CLA?\n. > There should be additional benchmarks to provide very different information. It would have to be constructed carefully to determine exactly what it is proving.\nThat is correct. And there should always be a benchmark with high or 100% hitrates. The cache hit ultimately needs to be fast. It is good to have an isolated benchmark for this. However, this is one aspect of many.\n\nFor microbenchmarks the current scheme is reasonable, given that all use LRU. When a different policy is in the mix then the results are skewed in that policy's favor. A long running application may have many hiccups due to the cost of eviction, as mentioned above, whereas LRU is O(1) in all scenarios.\n\nGotcha!\nThat is an assumption, and falls into the \"measure, don't guess\" category. You don't know for sure, until you benchmark it.\nIn most real world workloads modern eviction strategies outperform LRU in terms of achieved hitrate. So additional costs for eviction, might get amortized. As soon as there is some real cost for generating the cache value (CPU usage or latency) it will get amortized.\n. I use the traces in the cache2k benchmark, which are quite small, to validate and compare the eviction algorithm implementation against the original papers. By using the same traces from the old papers I can compare the diagrams.\nFor evaluating certain worst case situations and larger data sets, I use artificially generated traces.\nA potential source for larger traces: The ARC Authors used larger traces. You can find them on the homepage from Dharmendra S. Modha\nBTW: In the separate cache2k benchmarks GitHub project I put together some a library to work with cache traces, e.g. generate traces, calculate OPT and RAND hitrates, etc. The general library is in a separate module: org.cache2k.benchmark.util\n. > I added ClockPro for completeness and a little more familiarity of IRR. A quick comparison to cache2k's  non-adaptive version shows roughly 5-8% improvement, hopefully due to being adaptive rather than other implementation differences.\nHmm, can you give me a hint how I can reproduce this statement?\nMy experiments with the adaption has shown, that actually using no adaption yields better results overall. \nI assume that the implemented adaption, tends towards maxColdSize=1. For low hitrates test page removals will always be more than hits. All adaptions implemented in this simple approach, will ultimately tend towards the upper or lower bound, depending on the trace.\n. > I ported the PyClockPro and did a rough check against the paper. So what causes the differences was a guess.\nThe Clock-Pro implementation in cache2k went through some iterations to make it more production ready.\nOne of the main differences is that it uses three different lists for the clock, instead of one clock and a marker. This is useful, since it reduces the number of entries to inspect upon a scan. OTOH the order gets shuffled when the entries become promoted to hot and back. \n. > You're right, CLOCK requires some mitigation like a maximum walking distance before a marked candidate is selected. While a FIFO should retain most of the gains, another idea might be to scan a small distance, select the victim that has the lowest sketch frequency, and then use TinyLFU's filtering.\nThis is what I am doing to get the Clock-Pro algorithm to production quality in cache2k.\nHere is the link to the code: https://github.com/headissue/cache2k/blob/master/core/src/main/java/org/cache2k/impl/ClockProPlusCache.java#L222\n. Ben, albeit the interference, your message came through.\n\nIn latest SNAPSHOT, but not released, has the initialCapacity setting pre-allocates the sketch. This is in order to avoid the lazy resizing for weighted caches (e.g. byte sized) where the entry count cannot be easily inferred. That has the side effect of removing this skew from small traces.\n\nSo you mean that initialCapacity(0) will make the eviction algorithm to record & analyze the accesses from the beginning of usage?\n. Yes, I got that. Sorry my question was confusing. Corrected it:\nSo you mean that initialCapacity(0) will make the eviction algorithm to record & analyze the accesses from the beginning of usage?\n. Perfect timing. Thanks!\n. > I would have expected that the lagging eviction thread might be the cause, however, a benchmark run with 1,2 or 4 threads produces similar results.\n. Hey, I am spending my time here, to help and give a useful bug report.\nI added now for tear down:\nint cnt = 10;\n    while (cnt-- >= 0) {\n      Thread.sleep(3);\n      Runtime.getRuntime().gc();\n    }\nGC says:\n[Full GC (System.gc())  1620085K->1614786K(1864192K), 5.1165447 secs]\n[Full GC (System.gc())  1616307K->1613851K(1864192K), 5.2649184 secs]\n[Full GC (System.gc())  1615393K->1612797K(1864192K), 5.2099570 secs]\n[Full GC (System.gc())  1614387K->1611609K(1864192K), 4.9768815 secs]\n[Full GC (System.gc())  1612992K->1610564K(1864192K), 5.1135074 secs]\n[Full GC (System.gc())  1611937K->1609454K(1864192K), 4.7591053 secs]\n[Full GC (System.gc())  1610773K->1608395K(1864192K), 5.4222420 secs]\n[Full GC (System.gc())  1609831K->1607246K(1864192K), 5.0065981 secs]\n[Full GC (System.gc())  1608608K->1606134K(1864192K), 5.2420726 secs]\n[Full GC (System.gc())  1607564K->1604988K(1864192K), 4.7984943 secs]\n[Full GC (System.gc())  1606421K->1603833K(1864192K), 4.9631911 secs]\n[Full GC (System.gc())  1605444K->1603829K(1864192K), 5.5147763 secs]\n[Full GC (System.gc())  1604948K->1603769K(1864192K), 5.5836861 secs]\n[Full GC (System.gc())  1604604K->1603659K(1864192K), 5.7016917 secs]\n[Full GC (System.gc())  1604219K->1603563K(1864192K), 5.4341942 secs]\n[Full GC (System.gc())  1603885K->1603536K(1864192K), 5.3091407 secs]\nBesides, there is no change when I add Thread.yield() after every cache operation.\n. The above mentioned stress test limits the key space to 1 << 20 and uses a pre-initialized array with integer instances. This limits already the amount of memory in the test.\nI did the same in the above benchmark, but I had undesired effects that need evaluation and discussion another time. So I changed to a continuous ascending sequence to make sure that never a hit can happen. Now, there is a new key and value object created for every access, which changes things dramatically.\n. > ... I think you said before you test on a 2-core laptop.\nRecently I switched to a dedicated benchmark system, which has 4 cores. The problem shows up also with 1 thread and with yields, so there should be enough CPU power available. Furthermore, if it is just a queuing problem, the memory should free up after the load ceases, which is not the case.\nBesides the queuing rate problem, I still have the feeling that something else is odd here. Maybe it is not Caffeine at all, but a problem in the JDK, JMH or the OS scheduler?! One possibility is also that there is another cache implementation, bundled in the JMH jar, that screws things that Caffeine needs.\nMaybe you can just take two minutes, pick my JMH benchmark and run it isolated within your environment?\n. > What OS are you running?\nOops, that's an essential information missing from my initial report. Sorry!\nUbuntu 14.04\nKernel:\nLinux version 3.19.0-58-generic (buildd@lgw01-39) (gcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) ) #64~14.04.1-Ubuntu SMP Fri Mar 18 19:05:43 UTC 2016\n. > Can you run with the snapshot? I'm about to check-in a fix using a bounded queue. I still need to play with the backoff behavior. Its a little naive, but seems okay for an initial commit.\nI can, and I am sure it will influence what we experience, but still I doubt that is the cause for the memory leak. \nOf course, limiting the queue is good, too. What would be the guarantee of the cache? Let's say if I configure the maximu size to 1000 entries, how many entries would the cache hold, regardless of the call rate?\n. Ha, I found the problem in my above testing scheme:\nint cnt = 10;\n    while (cnt-- >= 0) {\n      Thread.sleep(3);\n      Runtime.getRuntime().gc();\n    }\nFor this heap size the GC took about 5s (see above), so actually the sleep is not giving Caffeine CPU time. When there are just sleeps the memory goes away again.\n. > .... That leads to queues, which are bounded to have back pressure if the system is falling over.\nFor interactive applications minimizing the latency is, of course, a valid design choice. For batch processing (e.g. analytical workloads) you want throughput and trade in the low latency.\n. > ... the system is falling over\nFor batch processing 100% CPU usage is the goal. This doesn't mean the system is \"falling over\" :)\n. > You're running with Parallel GC?\nAt the moment I just use -Xmx2G and stay away from other tunings. For presenting benchmark results picking up \"tuning parameters\" is a quite sensible topic.\n\nI usually run using -server -Xmx2G -XX:+UseG1GC -XX:-UseBiasedLocking to try and reduce pause times, without over tuning the settings.\n\nThanks, will give it a try!\n. I did think about that quite a lot. Actually, the current interfaces in cache2k for expiry went through several redesigns in the last years after we found some caveats in production. The interface is called ExpiryPolicy there are five return values with a special meaning as described in ExpiryTimeValues. Besides that, there is also an interface that can be implemented by the value class that provides the expiry time, ValueWithExpiryTime. It also needs to be noted, that the ExpiryPolicy is used for the refresh ahead timing as well and there is a special policy that controls expiry times for caching or suppression of exceptions, called  ResiliencePolicy. Seriously speaking, the whole thing is quite over engineered, but it proofs very useful. (Sorry, I am aware this has an evangelical touch. However, we already have some practical experience on exactly that topic.)\nSome comments on the actual proposal:\nExpiry interface with three methods create(), update() and read()\nLooks similar to JCache. So far, I never had a use case for a different expiry on create(). Maybe it exists, but it is not the majority. IMHO an interface with a single method for the common use case is better to avoid code bloat. Any more sophisticated use cases could be covered by another interface AdvandedExpiry. I would also recommend to separate the read() use case. The cache \"should know\" whether there is a special handling for read or not, to do useful optimizations.\nduration\nWith a time reference in the parameter returning a duration or point in time can do the same things. I decided for point in time for two reasons:\n\nIf an entry needs to expire at a point in time, the value just passes through, since we need the point in time for the internal timer. Returning point in time saves redundant calculations.\npoint in time seems more future proof for further enhancements. For example, there can be proper support for platforms that might have wall clock changes, such as mobile phones.\n\nresolution\nA millisecond resolution seems to be enough and more practical. OTOH the nano resolution better fits the design philosophy of Caffeine. See below.\nrandomization, sharp or lagging expiry\ncache2k doesn't do expiry time randomization currently, however we experience some spikes in production and see a need for it. The interface already has a distinction between expiry at an exact point in time (called sharp expiry) and expiry that should happen approximately at a time, so the cache can optimize and expire, which is in reality mostly a refresh, when there are sufficient resources.\nTalking about randomization as example, cache2k would typically have a configuration switch, while Caffeine, with the library philosophy, would probably go with a randomization adapter on the user expiry policy. In that way using nanosecond resolution makes sense.\nHowever, with the current proposal the cache implementation needs to decide for one semantic:\n\nexpire exactly when the duration has passed\nexpire approximately when the duration has passed and optimize on resource usage\n\nTypically, it would be the first one. But, OTOH, if exireAfterWrite() is specified, an exact expiry isn't needed and the cache could do optimizations and randomization.\n\nI don't know of a use-case for update to be provided with the old and new values. It would also restrict when the evaluation is made.\n\nWe call the use case \"dynamic expiry\". The duration can be adjusted, depending on how aggressively the data is changing. For example if the available seats in a flight change from 10 to 3 you might do a shorter duration for refresh, then when they change from 180 to 179. You need also the load time of the previous entry, that's why we put in a complete CacheEntry as parameter which has getLastModification(). Storing the additional value, is producing some additional memory overhead, however, knowing the last modification or load time, is useful at other places, too. I am feeling quite ambivalent at that point. In Caffeine, if a user likes to do it, it can store the time in the value object.\nSide note: Providing the previous value is useful for the loader, too. See the comments in AdvancedCacheLoader\nJust shoot, if there are any questions.\nFor the next release of cache2k I am planing a reimplementation on the actual expiry process, to improve resource usage, so I am looking forward to see some more thoughts on how to implement it efficiently.. > Most often more general strategies are better and easier to reason about, e.g. using pub-sub instead of trying to dynamically tune the refresh time.\nCannot agree more, but so far no system we had to integrate provided pub-sub :(. My benchmark runs of different eviction algorithms also showed a performance lower then LRU on a database access trace (see: \"OrmAccessBusytime Trace\"). The graphs show that Caffeine actually performs worse then random eviction for small cache sizes. Since there are more traces like that, maybe its worth digging into.. Did run the recs trace against cache2k and others today. Algorithms prefixed with \"S/\" use the respective Caffeine simulator. The implementations of Clock, ARC, CAR and RAND use the code from the cache2k-benchmarks:\n\nInterestingly the trace seams in favor of cache2k.\n. > [ ... ] Those cases include analytical, search, and database applications which are already memory intense.  When analyzing those traces we see Caffeine significantly outperform Cache2k.\nCan you please share more detail about these observations, especially name and source of the traces, so this can be reproduced?\nI am curious because for all database traces I did analyze so far (Oltp, OrmAccessBusytime/Night, UmassFinancial1, UmassFinancial2) cache2k performs better then Caffeine. The results are published in the blog article Java Caching Benchmarks 2016 Part 2. > Therefore, please refrain from comments that appear to be evangelical, but otherwise I'm glad to have your input.\nCommenting on a statement like \"Caffeine significantly outperforms Cache2k\" for many different application traces, which cannot be reproduced without further information, will probably always \"appear evangelical\".\n\nI'd have to dig in and compare again, but off-hand I know cache2k had less than 1% on the glimpse trace.\n\nYes. I also published that along with the blog article at Java Caching Benchmarks 2016 Part 2 graphs. We can see that there are traces and cache sizes where Caffeine performs better and also others where cache2k performs better. I cannot see that one cache is significantly outperforming the other in \"analytical, search, and database applications\". Maybe there is a slight tendency that Caffeine is doing better with the (legacy) short traces and small cache sizes, while cache2k is doing better with the longer and more recent traces.\nThe Clock-Pro algorithm plus my augmentation to it, has a lot of tuning parameters. For example, a cache consists of two data sets \"cold\" and \"hot\" data, with \"hot\" data being the entries that are seen more often. When an entry is considered as hot, when it is removed from hot and how big the hot portion is sized, is one of the many tunable elements. That means that Clock-Pro is adoptable to different workloads and is not weak on recency or frequency biased traces per se. During my experiments I found tuning parameters that made the eviction superior to any other, but only for one specific scenario. The reason cache2k is underperforming in Glimpse is, that I specifically chose not to tune against it in favor of performing better for more realistic workloads. What is realistic and important is of course very debatable, so I admit that I specifically tuned against the traces and workloads we see in our internal production environments.\nOver all, there is still room for optimization. Looking at the benchmarks both, Caffeine and cache2k, cannot claim to be better then LRU in every scenario.. > Included cache2k as maybe our ideas can be applied to improve its use of ClockPro\nThanks Ben! The results look indeed very promising. Opened an issue in cache2k so I have it on my radar.. > I had noticed this myself, but a provider is not supposed to implement the annotations. It is a bug in the RI that affects Guice and CDI (as Spring has their own) for framework integration.\nThe RI, as it is designed by the JCache spec leads, isn't meant to be production ready or thread safe.\n\nCacheManager does not provide a getOrCreate cache method.\n\nThe JCache standard is vague about a cache's life cycle. In case of distributed or persistent caches, caches would be created only once, probably by the administrator or some initialization routine. The createCache method is meaningless in this context. I suspect that the actual routine in the RI is just for the matter that things work OOTB for the TCK test cases, which need to run without prior configuration, but actually shouldn't be a real implementation reference per se.\nA getOrCreate method, IMHO, isn't the solution, because it gets even more unclear who actually is controlling the cache life cycle. Would a getOrCreate method need to check that the requested configuration fits?\n\nThe proper solution would be for the factory to synchronize on its own lock in a double-checked locking fashion.\n\nExactly. Alternatively all caches could be created first, instead of being initialized lazily.\nThis bug affects every JCache implementation and should be reported against guice.. @denghongcai just saw the discussion here and the reference to cache2k. Some remarks:\nI see expiry and refresh as two sides of the same coin. A value is considered fresh (enough) for some period of time, after that it expires or gets refreshed. Historically, in cache2k, the expiry policy was named refresh policy, because its very common that we use the automatic refresh in our applications. Besides, expiry and eviction are quite negative words. Refresh is more positive.\nA typical way to deal with the cache invalidation problem is to set a low expiryAfterWrite duration. But this introduces additional latency for loading the expired value quite frequently. The refresh ahead feature in cache2k is designed to make this go away with a minimum of additional configuration. Simply enable it with builder.refreshAhead(true). I see \"Values need to be updated after 5 minutes\" as a business level decision, while \"Enable refresh ahead\" is an operational optimization. Regardless whether refresh ahead is enabled or not, the business decision about the maximum duration a value is considered fresh, is identical. This is why I'd decided to use the timing values from expireAfterWrite and ExpiryPolicy as well for refresh ahead.\nTo understand my point better, maybe I need to note that I think of the term \"expiry\" differently than it may be used in Guava, Caffeine or EHCache. In these caches, historically, expiry means \"eviction based on time\". In my usage expiry means just the value is not valid any more, whether or if at all the entry gets evicted is a separate thing. In cache2k an entry may expire but still be hold by the cache, for example to reuse the data for a conditional reload like \"if-modified-since\". IMHO, with the additional features in caches its more crucial to keep the naming and semantics separate. That's my take on it.\n@denghongcai wrote:\n\nbut this will only refresh once if no query after refresh period, that means it's only make expire time \n twice....if expireAfter and refershAfter both exist, we can achieve our goals better.\n\nRight now the refresh ahead feature in cache2k only refreshes automatically once. If no access happens after the second refresh, the value expires after the expiry duration. That's a heuristic and turns out to work well for most use cases but not for all. I am thinking quite a long time to improve that, see some first discussion here: https://github.com/cache2k/cache2k/issues/34\nPlease give some more hints about your goals. How often or how long should a value be refreshed without being accessed?\n@ben-manes wrote:\n\n... and entries would not disappear if unused.\n\nNo, entries disappear if unused. See above.\n@ben-manes wrote:\n\nIn Caffeine the refresh is triggered if the entry is actively used, does not conflict with expiration\n\nHere is a mayor difference. In cache2k the refresh is triggered always at the expiry time, to have a fresh value when next used. There is a only a small time gap, which is the load time, when stale data might be served. In Caffeine, if you'd set the refresh time to CDN max age and a higher expire time, the cache may return stale data that is much older. To avoid this, you would set the refresh interval to a smaller value then max age and the expiry to max age.\nCaffeine does not refresh entries which have no activity, to avoid unnecessary system loads. That's a fair reason. I decided not to do that with the big picture in mind. The basic concept of a cache is that it contains entries that are used often. If the system load because of refreshing becomes a problem, the cache size could be reduced. If a lot of entries should be cached, but only a select few of hot entries should be refreshed, a smaller cache with refreshing enabled could be stacked on a huge cache.\nThe real problem I see is predictable behavior. I don't think that users really do an in depth study of the semantics. What is probably happening is that the refresh time in Caffeine is configured as the time span the value is considered fresh enough. In the CDN case this means it is set identical to max-age. But the refresh time parameter has no guarantee at all. The age of the returned data can be rather arbitrary and depends on activity.\nOn the other hand, triggering the refresh upon the activity has the advantage to add more randomness. With things always happening on constant intervals its more likely that resource spikes occur.\n. @denghongcai, @ben-manes thanks for the interesting discussion.\n@denghongcai \nI'll try to repeat in my own words to check whether I got your point correctly:\n\n\nYou would refresh at shorter intervals than maxage, but only if there is activity on that entry.\n\n\nThe problem with cache2k's approach is, that it refreshes always even if there was no activity.\n\n\nIs that correct?\nThat's interesting, since my initial idea of the shortcoming of cache2k's approach was exactly the opposite: It might be needed to refresh more than once even if there is no activity.\nAbout your CDN use case: There is a lot content, that uses versioned URLs and high maxage values. This content actually never needs a refresh at all. Refreshing at shorter intervals then maxage would put unnecessary load on the source. So I think the refreshing should occur closely before maxage is reached.\nSolutions I can think of, a little bit more concrete:\n\n\nRefreshPolicy: Allows to trigger a async refresh, do nothing, drop the entry, or put it in a probation mode (that is cache2k's internal mechanism to trigger additional functions when an entry is accessed), depending on various properties (e.g. item was accessed, was refreshed already X times, etc.). cache2k has a ResiliencePolicy for suppressing exceptions, which allows a wide variety of behavior see ResiliencePolicy. Probably it goes in the same direction in terms of complexity. It needs a lot of tests and is used not very often.\n\n\nUse a combination of caches. Put a smaller cache on top of a bigger cache, by using the cache loader. The small cache has the entries with activity, the big cache uses all the available memory to cache as much as possible. That's a cache hierarchy, so let's call the small one L1 (level 1) and the big one L2 (level 2). That probably sounds familiar. On a cache miss on L1 the entry gets populated from L2 via the cache loader. When this happens the loader (and the expiry policy) whether the entry is fresh enough and how long it can be served via the L1 cache, plus it can trigger an async reload, if that is needed via L2 (in cache2k that could be done via Cache.reloadAll). That means the logic of the RefreshPolicy would be in the ExpiryPolicy and CacheLoader of the L1 cache. The expiry time on L1 would be the refresh interval, while the expiry time on L2 would be the terminal expiry. With this construction you can actually mimic the behavior of Caffeine with cache2k. On the other hand, you can do the same construction with Caffeine to gain more flexibility.\n\n\nVariant 2 gives a lot of flexibility to adapt to specific needs.\nAfter some more thought I think its reasonable to make the L1 cache not much smaller or even the same size of L2. Since both caches store references on the same data, the only overhead is the internal data structures of the cache. So the decision on the L1 cache sizing is the trade off between the additional overhead and the reload from L2 in case the entry gets evicted from L1.\nBTW: We use cache2k a lot for proxies, that's why there is the AdvancedCacheLoader, which provides the old value for if-modified-since queries.\nBTW 2: The variant 2 makes also sense because it might be useful to implement L2 via a distributed cache in your CDN use case.\nLooking at the standard refresh ahead behavior of Caffeine and cache2k again, I'll take away that Caffeine's behavior could lead to unwanted stale data reads, while cache2k's behavior leads to unnecessary resource usages, which could be quite massive, if not limited via the thread pool. Let's see, maybe I come up with some better general semantics here.. @denghongcai Sounds good!\n\nAdvancedCacheLoader can solve my problem if combine with Cache2kBuilder.keepDataAfterExpired(boolean), but It's synchronized\n\nYou could start an async load job that is simply a cache.put when finished and just return the existing cache value. However, this is not really very elegant and things get quite confusing.\nI realize that an async version of the AdvancedCacheLoader is a something I should implement with high priority. Opened an issue here: https://github.com/cache2k/cache2k/issues/93. ",
    "michaelhixson": "Ah...  If I'm reading you right, it sounds like the com.github.benmanes.caffeine.cache.tracing package was not meant to be included in caffeine.  Could it be a glitch in the deployment then?  If you download the two jars from Maven and look at the class files inside, they do both have that set of classes.\nHere are the jars I just verified with:\ncaffeine - http://search.maven.org/remotecontent?filepath=com/github/ben-manes/caffeine/caffeine/1.2.0/caffeine-1.2.0.jar\ntracing-api - http://search.maven.org/remotecontent?filepath=com/github/ben-manes/caffeine/tracing-api/1.2.0/tracing-api-1.2.0.jar\n. Thanks!  I imagine this was the least interesting change in the release. :)\n. I get the same thing.  It's a runtime error; the project builds fine.  Was tracing-api always <scope>compile</scope>?\nEdit:  Here's a pom.xml and Java source file that reproduces the error for me.  I can't figure out why this happens... the caffeine poms look fine.\nAt /pom.xml:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n4.0.0\ncom.example\ncaffeine-tracing-test\n1.0-SNAPSHOT\n\n\ncom.github.ben-manes.caffeine\ncaffeine\n1.3.0\n\n\n\n\n\norg.apache.maven.plugins\nmaven-compiler-plugin\n3.2\n\n1.8\n        1.8\nUTF-8\n\n\n\n\n\n```\nAt /src/main/java/Main.java:\n```\nimport com.github.benmanes.caffeine.cache.Caffeine;\nimport com.github.benmanes.caffeine.cache.LoadingCache;\npublic class Main {\n  public static void main(String[] args) {\n    LoadingCache cache =\n        Caffeine.newBuilder().build(String::length);\n    System.out.println(cache.get(\"hey\"));\n  }\n}\n```\nOutput:\nException in thread \"main\" java.lang.NoClassDefFoundError: com/github/benmanes/caffeine/cache/tracing/Tracer\n    at com.github.benmanes.caffeine.cache.LocalCache.tracer(LocalCache.java:64)\n    at com.github.benmanes.caffeine.cache.UnboundedLocalCache.<init>(UnboundedLocalCache.java:71)\n    at com.github.benmanes.caffeine.cache.UnboundedLocalCache$UnboundedLocalManualCache.<init>(UnboundedLocalCache.java:769)\n    at com.github.benmanes.caffeine.cache.UnboundedLocalCache$UnboundedLocalLoadingCache.<init>(UnboundedLocalCache.java:822)\n    at com.github.benmanes.caffeine.cache.Caffeine.build(Caffeine.java:841)\n    at Main.main(Main.java:6) <5 internal calls>\nCaused by: java.lang.ClassNotFoundException: com.github.benmanes.caffeine.cache.tracing.Tracer\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 11 more\nIntelliJ seems convinced tracing-api:1.3.0 does not exist.  Running it via mvn exec:java -Dexec.mainClass=\"Main\" gives me a similar error.\n. Yeah that's it.  1.2.0 had <scope>test</code> before <scope>compile</code>.  1.3.0 has them in reverse order.  ~~~Apparently Maven uses the last defined scope in that situation.~~~ or maybe not.  Not sure what Maven is doing there.\n. ",
    "wburns": "Yeah actually we added a completely new LIRS implementation a few months ago which doesn't utilize a segment lock, which you can find here.\nAlso we don't support weights with LIRS, only LRU.  I personally felt that it isn't very feasible to support weights with LIRS due to the stack and queue usage and weights changing as you mentioned.\nAlso I am curious as to the memory leaks you mentioned.  Is this because the implementation actually supported Non resident HIR entries?  The prototype we utilized before never supported them.  The new version we have does support them, and can \"leak\" memory with a lot of cache misses in a row (granted it cleans itself up automatically with some hot hits).  It would be interesting to see on a real system if we had something like your trace/simulator.  Unfortunately I can't provide any trace data in this regard.\nAnd I apologize I am not as familiar with caffeine as I could be, but what did you mean by the following?\n\nSo now I'm just starting to on a simulator with traces and policies.\n\nThis is https://github.com/ben-manes/caffeine/wiki/Simulator?  Unfortunately it doesn't have a lot of details explaining it.  Are saying to replay back a set of cache accesses/modifications to see how it behaves?\n. My guess is the difference in hit ratio would be related to using a segment based implementation.  The more segments you have the greater the chance of something getting evicted earlier than if you had a an eviction taking into account all entries.\n. Ah, yes with 1 segment I would expect it to be the same.  In that case I agree the only thing I can think of is the non resident queue size being bounded or not.  Btw thanks for the update :+1: \n. What we have internally is basically just a getIfPresent(key) that doesn't record a cache hit, as you mentioned.\nThis comes in a couple flavors, but we have some unrelated to user interaction and others that may require additional accesses per operations.\nAn example of unrelated to user interaction is we have an expiration reaper that runs every so often to remove expired entries. Due to nature of clusters, max idle is a very precarious setting. We have to query on all nodes to make absolutely sure that the entry is expired, before removing. In this case we just want to get at the entry and don't want to update the access policies as this isn't a real user interaction. In many cases this entry may be removed soon after, which can mess with recency based eviction algorithms, having an access and a removal so close to each other.\nAlso all of our write operations first perform a get on the underlying container, as we have various interceptors that may change the behavior of such operations. In this case we try to limit it so that write operations don't cause multiple access occurrences in the policy. In some case scenarios we may have up to 3 cache hits (2 reads and 1 write). I don't know if 3 hits like this right away might be promoting entries preemptively.\nUnfortunately I can't read the document you linked to, but as I can see it seems the eviction algorithm should do well against malicious requests. We don't really have that, but just repeated accesses against the same key. I am just a bit worried about promoting keys more often then they should.\nBut I defer to you if you think these extra accesses shouldn't cause too much trouble.\n. I wrote up something, but then abandoned it and got distracted :)\nMy only issue with a trace is that from the perspective of Caffeine vs Infinispan the hit % will be quite different. Since we will be polluting the Caffeine side with extra gets, it may get an inflated hit %. I could generate something from the perspective of Infinispan in that case though. And tbh with you one of the problems is I am a bit disconnected from users workload to generate a trace at this point. Although I wonder if the person from https://issues.jboss.org/browse/ISPN-9023 could eventually share theirs :) I am still not sure why memory based (ie. weighted) eviction is giving him troubles.\nAt this point my worry is more about correctness and as you said is more of a gut feeling. As such you can probably just close this.\nThe one benefit as I have said to others and you even mentioned is that eviction is not deterministic in regards to what entries are kept and we just need to verify we have the correct number in the cache.. Let me explain my use case. I am using this for is to store entries into a bounded Caffeine map, but I want to index these entries in an external resource, so some operations don't have to read all contents of the map. In this case I never populate the Caffeine map with values from the index (always the inverse).\n\nIt wouldn't be desirable to invoke write as a side effect of your code loading from that resource, only to push it back in.\n\nShouldn't the user be using a CacheLoader to do this?\nThe core issue is that the creation of a Cache may be completely disconnected from the invocations of said Cache. The only way to keep it all at creation time is to build the ConcurrentMap and then wrap that using a forwarding map wrapping any user Function in another one, causing additional allocations.\nLuckily the code I wanted to use this for is only a single level of indirection away so I can I merge the function together with the logic I am invoking in the CacheWriter. It just seems like this shouldn't be required and a CacheWriter should handle all writes into the map from the user.. > I think most users would expect that get behaves the same as computeIfAbsent, and that compute and computeIfPresent behave similar to computeIfAbsent\nI am not sure what you mean by computeIfAbsent working like get. Unless you mean if the value is present, then I would think it work like get. But I would think computeIfAbsent would work like the Javadoc states https://docs.oracle.com/javase/8/docs/api/java/util/Map.html#computeIfAbsent-K-java.util.function.Function-. That is that it inserts the value if there was no value before and the function returned a non null value. I would expect to get a write notification in this case.\n\nIf the writer was invoked after a load/compute, then we could avoid the loader's memcached.put. Unfortunately, we also wouldn't know if the load was satisfied by finding the value in memcached or the database, and wouldn't have a way to communicate that. In that case we'd perform a memcached.put unnecessarily on a remote cache hit.\n\nIt sounds like you are lumping CacheLoader.load invocation in with compute methods. These are totally different things. They might be the same in the current implementation, but that is just implementation detail. I personally believe that the CacheLoader.load should not cause notifications for write operations, otherwise you would get the double remote invocations as you stated. But there is no reason a compute method should cause double in this case.\nThe code that you showed above wouldn't need to change if only compute and friends were changed to raise write notifications. It sounds like just the underlying caffeine impl of CacheLoader.load would have to change to no longer call into compute allowing notifications (put already has an argument like that).\nIn fact the code above seems it could be a bit cleaner even. This uses two external sources (memcached and some resource to create data). In my opinion memcached should only be in the CacheWriter and CacheLoader and the user would use compute  or other method themselves to call into the resource. If compute notifications are generated then the following could be done.\njava\nCaffeine<K, V> caffeine = Caffeine.newBuilder() // ... plus eviction config\n    .writer(new CacheWriter<K, V>() {\n        public void write(K key, V value) {\n          memcached.put(key, value);\n        }\n        public void delete(K key, V value, RemovalCause cause) {\n          if (!cause.wasEvicted()) {\n            memcached.delete(key);\n          }\n       }\n    }).build(memcached::get);\n   ConcurrentMap<K, V> map = caffeine.build().asMap();\n   V value = map.computeIfAbsent(repository::get);\nThis way storage isn't tied to how objects are created (meaning you could have different services per key even). This seems much cleaner to me. Lets say for example if you want to only do a get and ignore calculating the value if it can't read from memcached, there is no way with the impl you posted. And I assume that getIfPresent doesn't query the CacheLoader, allowing the user to possibly hit both cache levels (caffeine, memcached) and the actual service, as needed.\nAnd as I mentioned above, your code would still work and the user can just call get if they wanted instead of computeIfAbsent.\n\nFor caching libraries with a CacheLoader and CacheWriter, they don't typically have ad hoc compute methods.\n\nNot that it matters, but Infinispan supports this.. > Map.get does not invoke the loader and only peeks into the cache, as Cache.getIfPresent\nI didn't realize this. I guess I thought since LoadingCache.get was the same name they did the same thing. In that case I agree that the behavior should be the same between the value returned from Map.get and the value provided to Map.computeIfAbsent etc. Although I think this was a bit off course from the original issue.\nAlso, this means that the writer beneath can't hold the various Map contracts like put since it won't be able to return the previous value if an entry was evicted. So Writer|Loader and asMap should only be used in conjunction for secondary caching, and not storage with eviction (Cache/LoadingCache hold their contracts). Just putting this here for posterity :)\nI am still not fully convinced that Map.compute shouldn't raise a notification, but I understand the predicament you have now :( So I can go ahead and close this out. Luckily I have an easy enough workaround for my use case anyways.\n\nWell of course it does! Infinispan is quite rich and well thought out \ud83d\ude04\n\nIt took a few versions and me wondering why we weren't consistent with the API, to get there, haha.\n. ",
    "thomasmueller": "FYI I have ported my H2 LIRS implementation to Apache Jackrabbit Oak. This is much closer to the Guava cache, including loader, and (new) callback on eviction. And it is Apache Licensed, and well tested (with unit tests and in production). \nBy the way, my implementations does not always re-order entries near the top of the stack, which improves read performance quite a bit. This idea might be interesting for infinispan to further improve performance. \nIn my implementation, changing the weight is currently not supported, but I don't think it would be hard to implement. I don't see how queue and stack usage could be a problem. It's basically the same as changing weights in an LRU cache: you just leave the entry where it is. Sure, you might need to evict old entries if the total weight is too high, but that's no different in the LRU case.\nAs for memory leaks: I think a cache with a possible memory leak is not really a general-purpose cache. The access pattern needed for the memory leak is really basic: a scan though many entries. And it is quite simple to avoid the leak; my implementation uses a separate queue for non-resident entries. Possibly the size limit on that queue should be bigger or configurable.\n. Hm, the reported number of requests is also different...\n. I see. I'm not quite sure right now why the implementations for get(K, Callable) and get(K) (with a CacheLoader) are so different. I think in Jackrabbit, we currently don't use the CacheLoader and didn't optimize for it. It looks like it's unnecessarily synchronizing on a segment in this case... I will have a look if this can be fixed easily.\n. > I suspect its due to the size of the non-resident queue.\nI suspect that too. I guess the non-resident queue size of the Jackrabbit implementation is too short for many use cases. It probably makes sense to make it at least 3 times bigger. It would be good to have real-world traces to find out what works best. Plus, make the queue size configurable. Sometimes, the keys use very little memory compared to the values, but sometimes that's not the case. I wonder if the memory usage of keys should be taken into account.\n. > capture trace data\nI'm still working on that. I'm adding way to capture traces for Apache Jackrabbit Oak (using log level \"trace\"), so that getting real-world data should get easier.\nI had a look at the traces that cache2k uses, but (for my taste) the traces have relatively little data (cache sizes are small, and the number of captured traces is relatively small).\n. EdenQueue is very interesting, specially if it's simpler than LIRS. I would probably ask the authors of TinyLFU if they know of any patents in that area. Roy Friedman does seem to have quite many patents. A small disadvantage might be that sometimes, O(n) operations are needed (to divide the counters by two). But a regular hash map has the same issue (resize), so I guess it's not a big problem.\nAs for my LIRS implementation, I recently updated the \"long key\" variant to use a larger, and configurable non-resident queue size. The Oak version is not yet updated, I hope I can do that soon. In my implementation, the stack does not always contain all non-resident entries by the way.\n. About the 4-bit CountMinSketch: you seem to be only using 2 bits of the hash code, this is problematic if the hash function has a low diffusion. For example, Double keys 0..524288 all have start = 0.\nint item = e.hashCode();\nint start = (1 << (item & 3)) - 1;\nYou should use a \"supplemental hash function\", similar to java.util.HashMap. Basically the hashcode of the hashcode. I suggest to use:\nstatic int supplementalHash(int x) {\n    x = ((x >> 16) ^ x) * 0x45d9f3b;\n    return (x >> 16) ^ x;\n}\nI understand you do use a (simple) supplemental hash in indexOf, but I think it can be improved. Also, it might be enough to use one supplemental hash (one multiplication) per key, instead of 4, and rotate the result, to pick the index in the table.\n. Sure, here a first version of the code. It needs 5 multiplication operations per call, I guess it could be reduced to just 1, if \"indexOf\" uses a faster operation.\n```\npublic final class FrequencySketch2 implements Frequency {\n    private static final long MASK_A = 0xF0F0F0F0F0F0F0F0L;\n    private static final long MASK_B = 0x0F0F0F0F0F0F0F0FL;\n    private static final int SAMPLE_SIZE = 5_000;\n    private static final int LENGTH = 512; // must be power-of-two\n    private static final int LENGTH_MASK = LENGTH - 1;\n    private final long[] table;\n    private int size;\npublic FrequencySketch2() {\n  table = new long[LENGTH];\n}\n\n@Override\npublic int frequency(E e) {\n  int count = Integer.MAX_VALUE;\n  int item = supplementalHash(e.hashCode());\n  int start = (1 << (item & 3)) - 1;\n  for (int i = start; i < (start + 4); i++) {\n    int index = indexOf(item, i);\n    long x = table[index];\n    int c = (int) ((x >>> (i << 2)) & 0xF);\n    count = Math.min(count, c);\n  }\n  return count;\n}\n\n@Override\npublic void increment(E e) {\n  int item = supplementalHash(e.hashCode());\n  int start = (1 << (item & 3)) - 1;\n  for (int i = start; i < (start + 4); i++) {\n    int index = indexOf(item, i);\n    increment(index, i);\n  }\n\n  if (++size == SAMPLE_SIZE) {\n    reset();\n  }\n}\n\nprivate void increment(int i, int j) {\n  long x = table[i];\n  int offset = j << 2;\n  long mask = (0xFL << offset);\n  if ((x & mask) != mask) {\n    // increment, clear, set the bits, and store\n    table[i] = (x + (1L << offset) & mask) | (x & ~mask);\n  }\n}\n\nprivate void reset() {\n  size = (SAMPLE_SIZE >>> 1);\n  for (int i = 0; i < table.length; i++) {\n    long x = table[i];\n    long a = ((x & MASK_A) >>> 1) & MASK_A;\n    long b = ((x & MASK_B) >>> 1) & MASK_B;\n    table[i] = a | b;\n  }\n}\n\nprivate int indexOf(int item, int i) {\n    int hash = supplementalHash(Integer.rotateLeft(item, i));\n    return hash & LENGTH_MASK;\n}\n\nstatic int supplementalHash(int x) {\n    x = ((x >>> 16) ^ x) * 0x45d9f3b;\n    return (x >>> 16) ^ x;\n}\n\n}\n```\n. Here a test case, with \"Double\" keys:\npublic static void main(String... args) {\n    // FrequencySketch<Double> s = new FrequencySketch<Double>();\n    FrequencySketch2<Double> s = new FrequencySketch2<Double>();\n    for (int i = 100; i < 100000; i++) {\n        s.increment((double) i);\n    }\n    for (int i = 0; i < 10; i += 2) {\n        for (int j = 0; j < i; j++) {\n            s.increment((double) i);\n        }\n    }\n    for (int i = 0; i < 10; i++) {\n        int x = s.frequency((double) i);\n        System.out.println(i + \" freq \" + x);\n    }\n}\nThe output of your current code is:\n0 freq 8\n    1 freq 8\n    2 freq 10\n    3 freq 9\n    4 freq 12\n    5 freq 7\n    6 freq 15\n    7 freq 8\n    8 freq 15\n    9 freq 8\nThe output of my version is:\n0 freq 2\n    1 freq 1\n    2 freq 2\n    3 freq 0\n    4 freq 6\n    5 freq 0\n    6 freq 9\n    7 freq 1\n    8 freq 10\n    9 freq 3\nThe main difference is that in your version, \"start\" is always 0. I think my version better matches the most optimal output, which should be between 0, 0, 2, 0, 4, 0, 6, 0, 8, 0 and 1, 1, 3, 1, 5, 1, 7, 1, 9, 1 AFAIK.\n. OK, all those results are fine, except for the original code (the one that gave 8, 8, 10, 9,...). The grouping of 2s is not a problem (it's to be expected with some probability). What the test does and what to expect: it increments the frequency of keys 100..100000 by one, and then the frequency of each even number x between 0 and 10 by x, so that frequency of 0 is 0, f(2) = 2, f(4) = 4, f(6) = 6, f(8) = 8. It then prints the estimated frequency of 0..10. In the ideal case, the result should be 0, 0, 2, 0, 4, 0, 6, 0, 8, 0. However, because there are false positives, the estimated frequency of odd numbers is incremented as well sometimes (and scaled back in \"reset\"). That's why odd keys also have a result of 1, 2, and sometimes 3. That's fine. However, in the original code, the frequency of odd keys was very high (8), which is not good. The reason was that \"start\" was always 0, with keys of type java.lang.Double. Double is kind of the wost-case possible key for a hash table, because its hashCode method has very low diffusion. See also JDK-4669519 : Hashmap.get() in JDK 1.4 performs very poorly for some hashcodes..\nWhich supplemental hash and algorithm to use for \"start\" and \"indexOf\" does not matter all that much, as long as the statistical distribution is good. But there are two things to consider: performance, and security. \nPerformance: you may want to avoid using multiplication too often. In your case, you multiply 4 times as far as I see. This is a bit slower than one multiplication and 4 rotate operations (depending on the CPU). \nSecurity: if your tool is used for general purpose caching, then you might want to consider \"hash flooding\". A regular LRU cache is not affected by this, but the hashtable implementation is affected (including java.util.HashMap), and FrequencySketch. What you could do is use a random offset per cache, and xor each key.hashCode with it, before you process further. That will prevent most attacks.\n. Performance: instead of multiplication, a combination of faster operations can be used, but to get the same diffusion, you usually need more operations. So it might not be worth it, depending on what you need. Optimizing other things might be much more important (memory access patterns, usage of background threads, concurrency).\nSecurity: if it degrades to LRU, then there is no problem I guess. Maybe just document that case, for paranoid people like me.\n. ",
    "Stephan202": "@ben-manes, doesn't the Arxiv PDF have your name misspelled in the header?\n. Very nice discussion here. FWIW, in our company I also strongly advocate sticking to a small set of well-established java.time patterns:\n\nAlways use Duration to denote logical durations (as discussed here).\nAlways derive the current time from a provided (generally: dependency-injected) time source (generally a java.time.Clock) instead of using the various static java.time.SomeClass#now methods.\nUse Instant instead of (Zoned)DateTime when reasonably possible. (Push local or timezone-based computations to the \"edges\" (e.g. REST interfaces) of the application.)\nDon't pass a Clock to methods that are only concerned with the current time; just tell them the current time.\n... etc.\n\n@kevinb9n I hope the Google team open-sources the Error Prone/Refaster checks you're alluding to. :D\nBack on topic: in our code base we also have a bunch of places where we decompose a Duration (either a private static final constant or some Spring property) as a (long, TimeUnit) pair. And indeed, some devs went for millisecond precision and others went for seconds. So +1 on this feature request.. ",
    "vlsi": "@ben-manes , can you please clarify if W-TinyLfu can be used in PostgreSQL cache eviction? PostgreSQL's license is like BSD.\nIt looks like \"patent\" in used the paper is only to reference ARC, but it would be nice to know what are the limitations on use/implementation of W-TinyLfu.\n. @ben-manes , thank you. Am I right TinyLFU requires a global lock around doorkeeper and a lock for counter updates?\nIt seems that \"concurrency\" aspects are not discussed in the paper.\nIt is not yet clear how caffeine resolves that issue (accessing bloom filters and updating them at the same time).\n. @ben-manes , thank you for the pointers. That makes sense. I did expect not such an elaborate answer. I'm not ready to implement that right away, however it definitely looks to be simple enough modulo \"absence of j.u.c in C\".\n\nPostgres might prefer to retain CLOCK and use a concurrent counter.\n\nThat's one of the major problems there: sometimes PG has to sweep the clock several times in order to find a \"good\" victim. That results in lots of cache misses (especially, for big cache sizes), thus very high latency of \"finding a victim\", thus very bad scalability/throughput.\nIt might be an interesting experiment to drop counters from CLOCK and just put TinyLFU admission.\n. > Thanks! I hadn't planned to start tackling JDK9 until its released, so I put this off. But its a good change.\nDoes caffeine work without Unsafe?\nI think it makes sense to start testing JDK9 to be prepared and report bugs to JDK9 team.\n. That makes sense. Afaik JDK9 beta does have VarHandles integrated.\n. Obviously akka has some problems with JDK9.\nakka_2.10:2.3.15 fails with\nCaused by: com.sun.tools.javac.code.ClassFinder$BadClassFile: bad class file: /home/travis/.gradle/caches/modules-2/files-2.1/com.typesafe.akka/akka-actor_2.10/2.3.15/d4cdc7f139d21806a3e265c2aab51440698467dd/akka-actor_2.10-2.3.15.jar(/akka/actor/FSM$$anonfun$1.class)\n      undeclared type variable: D\n      Please remove or make sure it appears in the correct subdirectory of the classpath.\n        at com.sun.tools.javac.jvm.ClassReader.badClassFile(jdk.compiler@9-ea/ClassReader.java:273)\n        at com.sun.tools.javac.jvm.ClassReader.findTypeVar(jdk.compiler@9-ea/ClassReader.java:916)\n        at com.sun.tools.javac.jvm.ClassReader.sigToType(jdk.compiler@9-ea/ClassReader.java:623)\n        at com.sun.tools.javac.jvm.ClassReader.sigToTypes(jdk.compiler@9-ea/ClassReader.java:830)\n        at com.sun.tools.javac.jvm.ClassReader.classSigToType(jdk.compiler@9-ea/ClassReader.java:746)\n        at com.sun.tools.javac.jvm.ClassReader.sigToType(jdk.compiler@9-ea/ClassReader.java:659)\n        at com.sun.tools.javac.jvm.ClassReader.sigToTypeParam(jdk.compiler@9-ea/ClassReader.java:\nand akka_2.11:2.4.9-RC2 fails with\naused by: com.sun.tools.javac.code.ClassFinder$BadClassFile:\nbad class file: /home/travis/.gradle/caches/modules-2/files-2.1/\ncom.typesafe.akka/akka-actor_2.11/2.4.9-RC2/2983907774623d6e811d37f3e0465a78ebd6060a/akka-actor_2.11-2.4.9-RC2.jar\n(/akka/actor/ReflectiveDynamicAccess$$anonfun$getObjectFor$1$$anonfun$apply$1.class)\n  undeclared type variable: T\nShould I temporary disable javadoc for JDK9 somehow?\n. @ben-manes , do you think it is worth applying something like https://github.com/signalapp/gradle-witness to validate the checksums of the downloaded artifacts?. Eh, I should probably test it, but it was not clear if Gradle verifies them: https://discuss.gradle.org/t/is-there-an-option-to-validate-downloaded-artifacts-via-the-sha1-checksum/6054/3. >There are already checksum files generated on the repositories that the dependency manager should verify with, so I don't think that plugin would help\nJust in case: the checksums you list are located on the remote site, and they can be compromised.\nSo the proper way to approach it is to declare expected checksums and/or expected GPG keys at use side (e.g. how MacPorts , NixOS, Brew, and so on package managers declare expected sums).\nIn other words, Maven should enable users to write like:\n<dependency>\n   <artifactId>caffeine</artifactId>\n   <version>2.7.0</version>\n   <sha1>c3af06be4a7d4e769fce2cef5e77d3becad9818a</sha1>\n</dependency>\nThen, the downloaded file should be compared with user-provided checksum.\nUnfortunately, Maven is not there yet.\nGradle does not have a notion to declare \"expected checksums\" either :(\nI see Caffeine does not have much of the (runtime) dependencies, however every dependency matters.\nFor instance, a compromised Gradle Wrapper might do bad things, so you might want to add distributionSha256Sum=... to gradle-wrapper.properties just in case (e.g.  https://github.com/vlsi/jmeter/blob/gradle/gradle/wrapper/gradle-wrapper.properties#L3 )\n. Of course I meant gradle-witness, thanks for pointing that out.. Thanks for the link, will try.\n. ",
    "gilga1983": "I actually experimented with a variant of Clock and TinyLFU in the past, the evaluation was very limited as it was part of a distributed Kademlia routing/caching technique named Shades. \nhttp://link.springer.com/chapter/10.1007%2F978-3-319-09873-9_33\nBasically the technique is called LazyLFU and it is very similar to clock. In a gist, we try to implement LFU but are searching for the LFU item in a lazy manner. At any given time the previously discovered LFU item is maintained as the cache victim and search steps flash out new items that might actually be less frequent than that item. (In that case we update the LFU item). \nThe clock hand does a small number (a single in my old implementation) of search steps and until there is an admission the least frequently itme is the cache victim. (Search steps may not change the least frequently item). \nThere some slides about it: \nhttp://www.cs.technion.ac.il/~gilga/EuroPar_SlidesV2.pptx\n(slides ~19)  \nSince TinyLFU seldom admits an item it is a nice O(1) approximation of LFU policy. For example consider the case of 10 search steps and 1/10 admission probability. In this case by the time there is an admission, your clock hand have already scanned 100 items (10 at a time) and the victim that is replaced 'deserves' it. \nLazyLFU is slightly better then just have a clock with 10 steps that compare against TinyLFU and the minimal one is the victim because you remember the previously discovered victim so the old searches are not lost. (Until there is an admission).  So LazyLFU gives the benefit of larger seek distances in an efficient manner.  I think that a smart implementation can have additional benefits.  (Concurrency - search for victims in one thread and use the already discovered cache victim when you need a victim? The sky are the limit really.) \n. Sure, as long as you are willing to review what I write I'll be very\nexcited.\nI did not have many people to review my code over the years.\nOn Sun, Aug 16, 2015 at 11:09 PM, Ben Manes notifications@github.com\nwrote:\n\nlgtm after you make the minor formatting changes.\nThe common approach is to either make changes as new commits and then\nsquash them before merging, or amend the existing commit and force push.\nMost people end up doing the latter because its easier, but a little less\nsafe if you want to revisit your old history.\nSo you would make the changes, git add ., git commit --amend, git push -f.\nThen write a comment to ping me that its ready to review. Or if I lgtm it\nyou could merge into master directly, e.g. git checkout master git merge\ngilga, git push.\nOnce you have the hang of git, the project's conventions, etc. you don't\nneed to send me pull requests unless you want feedback or want to propose a\nchange outside of the simulator module.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/pull/24#issuecomment-131623002.\n\n\nThanks\nGil Einziger (aka Gilga)\n. Well it is not ready yet. I was working on it before and then lost momentum\nI hope to get it ready soon enough.\nOn Tue, Aug 18, 2015 at 10:01 PM, Ben Manes notifications@github.com\nwrote:\n\nI cleaned up your branch. Can you please take a look at let me know if the\nchanges are okay? If so then I can merge it in.\nNote that you didn't wire the new TinyCacheAdmission class for the\nsimulator. I can do this if you tell me what the nrSets and itemsPerSet\nconstructor values should be set to (e.g. config file defaults). Would you\nwant your version and the CountMinSketch to both be evaluated with, or\nshould the count-min version be removed?\nfyi I deleted the simulator/TinyCache/* files (e.g. NewTinyCache since\nthat looked like a temporary directory used for porting. It wasn't part of\nthe project so neither the build or Eclipse saw it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/pull/25#issuecomment-132333453.\n\n\nThanks\nGil Einziger (aka Gilga)\n. Not right now, when I am ready I'll work on merging it.\nOn Wed, Aug 19, 2015 at 8:29 AM, Ben Manes notifications@github.com wrote:\n\nOkay, do you want this merged in as is or do you plan on continuing to\nwork on this pull request?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/pull/25#issuecomment-132461672.\n\n\nThanks\nGil Einziger (aka Gilga)\n. Ben,  I still have some delicate bugs in there. I am continuing to debug in\nmy own project and I'll port it when I am done. Yes it is based on,\nTinyTable.\nOn Thu, Sep 3, 2015 at 12:10 PM, Ben Manes notifications@github.com wrote:\n\nI'm looking forward to playing with this. I think this is based on your\ntiny table, right? I'd like to see the performance gain of this scheme in\nyour tinylfu (memory & cpu costs). If I shouldn't have deleted the\nTinyCache let me know and I can revert them back into the branch.\nAlso, let me know if you have ideas for a better name for the EdenQueue.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/pull/25#issuecomment-137400275.\n\n\nThanks\nGil Einziger (aka Gilga)\n. oh... I did not realize it was still open. I saw that it did some bugs and\nmoved it back to testing. I do not have many hours to code nowadays but\nI'll get there.\nOn Mon, Sep 21, 2015 at 7:41 PM, Ben Manes notifications@github.com wrote:\n\nClosed #25 https://github.com/ben-manes/caffeine/pull/25.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/pull/25#event-415019373.\n\n\nThanks\nGil Einziger (aka Gilga)\n. This is very interesting, \nBen how large is the window of W-TinyLFU ?  I think that perhaps as a workaround it can be configured with a large window cache and small LFU cache? (are these configuration parameters exposed?)  Perhaps with 99% LRU and 1% LFU for example, it would work quite nicely and Viktor can use Caffeine with the improved hit rate. \nThanks for the trace Viktor, we'll defiantly try to come up with something. \n. We're working on something toward adopting the policy also to these kind of traces in the Technion, but it is research and even if successful will take a while. \n. ",
    "trauzti": "Thanks for this discussion, it got me on the track of understanding the S size limiting operation in LIRS that keeps a bound on the number of non-resident HIR entries. My understanding from the LIRS paper is that the oldest non-resident HIR block should be removed from S. Searching from the tail of S on every reference can become a major bottleneck so I discussed with Song Jiang an optimization that stores the pointer to the last non-resident HIR block and start searching from there on next operation and that improves performance significantly. \n@ben-manes after looking at your size limit operation in LirsPolicy.java it seems to me that you are removing the youngest non-resident HIR block which is not the method described in the paper.\n@thomasmueller good idea to use a separate queue for the non-resident HIR blocks.. I see, I got just confused with \"headNR\" and thought that was the top of the stack but seems like that is the bottom of the stack. To be precise, an empty node, one position after the bottom of the stack.. Ah okay, understand now.. I ran this on the LIRS traces and it had some effect on the hit ratio, not that much actually. I just wanted to get the exactly same hit ratio and thus I delved into this implementation to understand what was different. The logic looked the same on the surface so it was quite puzzling to see this discrepancy. To track this down I printed out the contents of A1in on every reference and compared the output to see where it started to be different and that let me to this find.\nI see that you follow the 2Q pseudocode strictly on the A1in hit. i.e. \"do nothing\" and I agree with following the pseudocode strictly. This seems like an odd thing based on the fact from the text in the paper says that A1 is for pages seen once and Am is for pages seen more than once. Originally my implementation moved the block to Am on an A1in hit but I have now changed it to \"do nothing\". . Yeah that's fine. The data.put(...) could also be removed from these three cases and just put at the end of the reclaimfor function.. ",
    "woidda": "Well, thanks for your quick reply. I see the point. I do not think that the stats are very useful for doing some serious monitoring / statistics. Dropwizard Metrics and other are indeed better suited for achieving that goal.\nOn the other hand, for me it seemed to be a little bit strange that after a cache invalidates its content, the hit count and other metrics remain unchanged. But I see the point.\n. I guess, that makes sense. In the JavaDoc for Cache.stats() it describes this behavior implicitly. \nThanks for your great work. \n. Excellent ;-)\n. ",
    "alexander-riss": "ah ok i did not realise that there are transitive dependencies - i'll give it a try\n. i added the tracing-api bundle but as it seems this does not help much, since the com.github.ben-manes.caffeine is not importing the tracing bundle.\nSo adding that import would probably fix the issue and document this runtime dependency all at once\n. ",
    "ybayk": "Yes, that's async cache. Not sure how it behaves on a blocking cache as we do not use it. \nAlso whether the future is immediate or real one does not make any difference as we have the same issue with true futures. \nSo as far as I understood you do not need ticker based test anymore?\n. Thank you for looking at it promptly. Yes, the issue seems to be common at least with our use case, where we deal with a few keys used side by side within a short period of time. We noticed it by observing our Cassandra queries being way too more frequent than expected, which immediately raised a question about cache layer not working properly. \n. all good. cache is now behaving ok with v1.3.3.\n. ",
    "PaulKlint": "Thank you for your extensive answer. As documented above I would say: yes that is enough. However, I have only read the information on the Caffeine Wiki pages and there I did not find any mention of null pointers. Perhaps you could add a warning there.\n. ",
    "ldaley": "Thanks for the info.\n. So just to confirm, is your starting recommendation for a bounded concurrent cache for pre Java 8 ConcurrentLinkedHashMap?\n. Thanks again.\n. Yeah thanks, I do intend to introduce it there. This \u201cthread\u201d was actually in the context of Gradle.\n. Will do. Thanks for the pointer.\n. ",
    "nedtwigg": "I'll take a look.  I can see the problem in your build.gradle now, but I don't know much about unit-testing OSGi (I do all unit testing as plain-jane Java, and OSGi problems get rattled out in final integration tests).  I'll check out your branch and take a look.\nGreat work, btw, big fan of the library :+1: \n. Good find, I was stuck!  I've got a workaround, so no rush on my account.\n. I can't find com.github.benmanes.caffeine.jcache.OSGiTest.  Maybe you need to push your latest to osgi?\n. Aha!  My bad.\n. Sorry, I've got no clue.\n. Verified good.\n. Thanks for putting so much work into the tests!  I use a lot of OSGi plugins, they all break the manifests all the time, you're the first person to actually add tests to fix it and keep it fixed.  :+1: \n. 1) what OSGi container are you using\n2) we need the full stacktrace\nIt might be that the exception looks like this:\n```\njava.lang.NoClassDefFoundError: Could not initialize class com.github.benmanes.caffeine.cache.LocalCacheFactory$SSSMSW\nbecause could not load sun.misc.Unsafe\n```\nIn which case the @wclaeys could fix this by adding an extension bundle from osgiX.  But it depends on the rest of the exception and the runtime, so @ben-manes there's no need for you to add this as a dependency for everybody.\n. ",
    "tellisnz": "Perfect, thanks Ben.\n. Looks good, thanks for the speedy response Ben!\n. ",
    "seidler2547": "I was about to finish changing our current caching system over to Caffeine as I stumbled upon this also. It is also harmful if \"cleanUp\" is called explicitely on a Cache since then this (unchecked and undeclared) Exception is thrown from the cleanUp method, which will lead to problems.\n. ",
    "panghy": "Cool, thanks for the help, I am trying to see what we are doing that could be triggering this. Simple tests pass for sure, so I am trying to make a self-contained reproduction.\n. Looks like we are invalidating another cache when loading an entry of another cache, would that be considered recursive in that case?\n. Oh, actually we found the issue. In some cases it actually invalidated another entry in the same cache which seems to cause the recursive issue. Thanks for the help.\n. Thanks for looking into this and the detailed analysis. If anything, we noticed that nothing was being refreshed. Since the cache had many entries and stack frames show that the cache was returning results, it's going to be a case of somehow the cache getting into a state where all refresh attempts are ignored. We are turning on record stats to get more insights into it at this time.\n. Not a problem. I am on vacation right now, when I get back, I'll try to see\nif I can create one. Perhaps an OOM during refresh?\nOn Sep 9, 2016 10:21 AM, \"Ben Manes\" notifications@github.com wrote:\n\nPlease provide more details so that we can determine if it's a bug in the\ncache or usage. So far I haven't been able to reproduce the issue, so a\nfailing test case would be very useful.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/120#issuecomment-245850415,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAgKM3Lf5_mwmeRgUubHeDSHGp3ytWz_ks5qoRb0gaJpZM4JzTbw\n.\n. I think at this point we can't really reproduce this so I think it's safe to say that perhaps it was an OOM (which usually causes havoc in many places anyways). We have since turned on exit on OOM just in case but I don't think it's something that's specifically wrong with Caffeine.\n\nThanks for looking into this. I am not sure if you will be attending JavaOne this weekend but I am going to be speaking at the keynote briefly with Georges Saab from Oracle. I'll make sure to give a shout out to Caffeine :)\n. Indeed that seems to work:\npublic static void main(String[] args) throws InterruptedException, ExecutionException {\n    AtomicInteger atomicInteger = new AtomicInteger(1);\n    AsyncLoadingCache<String, Integer> cache = Caffeine.newBuilder().\n        refreshAfterWrite(1, TimeUnit.SECONDS).buildAsync(new CacheLoader<String, Integer>() {\n      @CheckForNull\n      @Override\n      public Integer load(@Nonnull String s) throws Exception {\n        log.info(\"calling load\");\n        long sleepMs = (long) (Math.random() * 10000) + 5000;\n        log.info(\"sleep for: \" + sleepMs);\n        Thread.sleep(sleepMs);\n        log.info(\"slept for: \" + sleepMs);\n        int toReturn = atomicInteger.incrementAndGet();\n        log.info(\"returning: \" + toReturn);\n        return toReturn;\n      }\n    });\n    int matched = 0;\n    while (true) {\n      int result = cache.get(\"hello\").get();\n      if (atomicInteger.get() != result) {\n        log.info(\"got: \" + result + \" from cache but atomic integer is: \" + atomicInteger.get());\n      } else {\n        matched++;\n        if (matched % 10000000 == 0) log.info(\"matched: \" + matched +\n            \" current: \" + atomicInteger.get());\n      }\n    }\n  }\nUpdated to make sure that load is still being called (it is).. Makes sense, thanks so much!. Thanks @ben-manes, any chance we could get a release soon?. No, thank you for the quick turn around!. Yeah, mostly. We caught the stacktrace on 2.6.1 (essentially the put() call is blocked on CHM).. Actually, it wasn't a refresh that blocks it but a get():\nCountDownLatch latch = new CountDownLatch(1);\n    LoadingCache<String, String> cache = Caffeine.newBuilder().\n        build(s -> {\n          if (s.equals(\"foo\")) {\n            latch.countDown();\n            Thread.sleep(30000);\n            return \"bar\";\n          }\n          return \"world\";\n        });\n    // trigger to load which will block for 30s.\n    Thread t = new Thread(() -> cache.get(\"foo\"));\n    t.start();\n    latch.await();\n    // put a value, does not expect to block.\n    cache.put(\"foo\", \"world\");\n    // expect to see a value of \"world\"\n    assertEquals(\"world\", cache.get(\"foo\"));\ncache.put() is where it blocks, the assertEquals works though.. And the Async version doesn't block.\nCountDownLatch latch = new CountDownLatch(1);\n    AsyncLoadingCache<String, String> cache = Caffeine.newBuilder().\n        buildAsync(s -> {\n          if (s.equals(\"foo\")) {\n            latch.countDown();\n            Thread.sleep(30000);\n            return \"bar\";\n          }\n          return \"world\";\n        });\n    // trigger to load which will block for 30s.\n    Thread t = new Thread(() -> cache.get(\"foo\"));\n    t.start();\n    latch.await();\n    // put a value, does not expect to block.\n    cache.put(\"foo\", CompletableFuture.completedFuture(\"world\"));\n    // expect to see a value of \"world\"\n    assertEquals(\"world\", cache.get(\"foo\").get());. So the synchronous version actually has blocking puts against the same key then (compared to the async flavor). Always thought the asynchronous version exhibits the same behavior but enables asynchronous programming paradigms.\nWe did ultimately just switch to the async cache but perhaps this is just a documentation issue then (or maybe it was, I dunno).. Of interest, get() with a supplier has the same behavior for both synchronous and asynchronous versions (which I assume it would since in both cases the computation is in flight and so it ignores the supplier).\nCountDownLatch latch = new CountDownLatch(1);\n    AsyncLoadingCache<String, String> cache = Caffeine.newBuilder().\n        buildAsync(s -> {\n          if (s.equals(\"foo\")) {\n            latch.countDown();\n            Thread.sleep(30000);\n            return \"bar\";\n          }\n          return \"world\";\n        });\n    // trigger to load which will block for 30s.\n    Thread t = new Thread(() -> cache.get(\"foo\"));\n    t.start();\n    latch.await();\n    // put a value, does not expect to block.\n    cache.get(\"foo\", key -> \"world\");\n    // expect to see a value of \"bar\"\n    assertEquals(\"bar\", cache.get(\"foo\").get());. Alright, this makes sense. I guess this is mainly going to be a synchronous loading cache issue. Background refreshes are fine in that case too as far as we can tell (meaning if a background refresh is in flight, it does get preempted by the put). Perhaps it's just the load() part that can potentially block puts that needs to have somewhat of a gotcha.. Yeah, this led me to think that perhaps you want this behavior to be configurable (even perhaps to cancel the future and interrupt even). Anyways, closing since the issue should really either be a) a documentation request/PR or b) configurable behavior for synchronous loading cache when dealing with puts (although even for asynchronous case you could say that whichever return first, the refresh vs the put could determine the replacement policy).. Trying to inspect the commits since 2.6.0 to see what could have been the cause.. This is actually in production so it seems odd that it stopped evicting entries after days.. But anyhow, looking into a test case.. I can see commonPool workers awaiting work unfortunately.. We are on 4.4.0-1050-aws (ubuntu), not sure if we upgraded together with the 2.6.1 rollout (also java 1.8.0_161). We might have also switched instance types as part of the roll-out so there are some additional (m4 -> m5) which is a new AWS base image, trying to generate a small test case to just test on the base metal.. Just rolled a very simple test that triggers loading from a cache of max size 100k in a loop for a million times and printing out estimateSize(). The works both on my mac and on the production machine that's seeing the issue. :(. Just for some reference, these are the top jmap entries:\n1:      16979066      950827696  [The object that's cached, it's both present as a key and in values]\n   2:       7559480      665234240  java.util.concurrent.ConcurrentHashMap$TreeNode\n   3:       3824275      438191872  [Ljava.lang.Object;\n   4:       7770486      435147216  com.github.benmanes.caffeine.cache.PSMS. We found the issue, this happens if the key is not stable (it's hashCode changes and its equality). Sorry for the false alarm but it's interesting to note that I guess the behavior for a size limited cache would be as such if keys are changed underneath the hood.. Will take a look at the code to see how it could happen. We are not using a CacheWriter. We are really just doing getIfPresent and puts against the cache but since we have metrics around the code that actually does those calls, we are pretty certain that they are not exiting at all. I also don't see past culprits (from puts with in-flight refresh on a non-async cache or our last embarrassing issue of keys changing hashcode/equality after being in the cache for instance).. For our particular use case we can't actually use a loader but thanks for the suggestion. Will continue to look. The strange thing is that it happened to a cluster of machines which seems to suggest that there is a particular key or data shape that can trigger this (would almost be impossible for a race condition by itself to happen across a fleet of machines).. Yes, because we have counters around the entry points into the cache calls, we are pretty sure that they are in a loop. jstack also shows that they are basically all contending on the same monitor, blocking on each other, with only one single thread gaining access but apparently relinquishing the monitor after the synchronized block (I can only see that from the continue call) and then another thread acquires it, does the same.. Still trying, we just had another occurence of that and again since it hits multiple machines at the same time, I am still going by a data issue (hashCode inconsistencies?). Or there's a bug in CHM? :) dunno\nStill chasing.. We do, we are planning to have the JVMs listening for debugger attach, should be pretty easy to see what's going on once it happens.. Unfortunately I have been on a business trip and AFAIK this hasn't happened again, let me close this for now until we can get another repro. ",
    "gokhanoner": "Thanks for the info.\nTBH, I'm not a fan of JCache either. My current apps. uses spring cache annotations with Guava and I'm quite happy. And hearing that caffeine is nearly 10X faster than Guava, thats really nice to have.\nHearing future native support is awesome. I suggest I can implement one easily to try.\nPS: I'd be good to add this to spring-boot also. and some config. for specs directly from config file. like Guava impl.\nhttp://docs.spring.io/spring-boot/docs/1.3.0.M1/reference/htmlsingle/#boot-features-caching-provider-guava\n. Yep, I think I can use it, it'll be much more simpler :) I have a certain app. that I want to test it to see the performance gain..\nCacheBuilderSpec is like configuring your caches, eviction time, cache size etc, without any line of code :) While using spring-boot, these king of stuff is useful. If you need different specs for different caches, than you need to implement it anyway. Than one can use Java based builder.\nOf course, another option which I believe could be useful also, they can put a param. for config. file if jcache config file also applies to caffeine, than users can modify this file. And with this, each cache can be configured separately.\n. Thats perfect. Thanks for the update..\n. As the original poster for this request, i would like to add something as well.\nAfter reading response from spring team, i realized that they are right. Currently, spring-boot support multiple caching solutions. And to configure each of them, you need their own config file. And spring boot gives you a property that u can show your config file. If not found, it uses tje library default specs. For guava, there is no config file but a config string. If caffeine already has a config file, it could be used, there is no need to add yet another config. If not, instead of guava spec, something more powefull, a json based conf. File that can configure each cache seperately would be usefull. \n. Yep, and ive already give it a try ;)\nOn 6 Apr 2016 8:31 p.m., \"Ben Manes\" notifications@github.com wrote:\n\nOh, Spring Boot 1.4-M2 is already out. That's the version that added\nCaffeine support.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/37#issuecomment-206478744\n. \n",
    "koen-serry": "I was looking to migrate and didn't find the CacheBuilderSpec as in guava so I stumbled onto this ticket. \nBut reading all this it looks like the fix in Spring Boot doesn't seem like a proper equivalent to the Caffeine.newBuilder().from(\"maximumSize=100,expireAfterAccess=1h\") syntax that is easier to get from a configuration file of some sort. Furthermore the Spec class seems trival enough to add, unless I haven't had enough coffee yet ;)\n. Yes I'm using spring and I like my caching configuration outside my application. This way if I see that I'm running low on memory I can still squeeze out a bit if I'm really in trouble. Or just the opposite. While testing I can see how certain options work out for the better or for worse...\n. No, I was already considering adding it myself via a basic split & set approach. Nothing really fancy.\n. ",
    "bobbymicroby": "It will be interesting to include  Martin Thompson's non-concurrent, open addressing lru Int2ObjectCache  to the benchmarked implementations.\n. I think IntLruCache was written by Richard Warburton, and yes, your definition of the data structure is more complete. I must do some benchmarks definitely.  Me and my friends are trying to write a matching engine with good latency profile. We are pretty much trying to use single threaded code, cpu pinned. We are trying to avoid concurrent code at all (  accept for the message parsing between threads ). This is why I was wondering about your opinion. \n. Thanks for the answers and your time. Wish you nice weekend :)\n. Once again, thanks again for spending so much time and for the detailed summary. \n. This is fair as well. Still the pitfall exists, and whenever possible it should be avoided . By the way  I have tried to to do ./gradle jmh in the project dir, but there was an error about non existing 'includePattern' . I have removed the check from the build file in order  run the benchmarks, but if you point me how to run them as intended it will be very nice.\n. Thanks, I look around the benchmarks then . Bye bye.\n. ",
    "rogerkl": "Thanks @ben-manes, It's not critical to have a release now, as we're not using caffeine in production now, so I could do some more testing. Our jcache usage is not very performance critical for the moment, we're using @CacheResult annotation (with guice) to avoid some expensive lookups, but I'll be on the lookout for more bugs :-)\n. Sounds good @ben-manes, thanks. I will check it out. Keep up the good work.\n. ",
    "opendedup": "Thanks for the Reply Ben,\nI believe Guava locks inserts based on the RemoveListener because its what\nI use to throttle IO based on pending writes. The cache itself is small,\nsometimes only 100 entries, which may mean that Guava's lock segmentation\nstill blocks on most inserts.\nThanks for the consideration and this great project.\nSam\nOn Sat, Dec 26, 2015 at 3:53 PM, Ben Manes notifications@github.com wrote:\n\nIn Guava the processing is performed under the segment lock, thereby\nblocking future writes to that segment. The removal of segments means that\nthere is a global lock, which is operated on asynchronously to not become a\nbottleneck. I'd expect you'd see a similar problem in Guava because\nRemovalListener is asynchronously processed too. So I'm a little\nsurprised by this as it seems the problem could occur in either library.\nYou can specify the Executor which is where the maintenance work is\nperformed. Perhaps that can be leveraged to throttle writes? It might be\nenough to put the penalty back on the calling thread (Runnable::run) or\nuse a lock that blocks writers until complete. There could be a hack with\nCacheWriter.\nI'm not sure what makes sense to change at the library level, but I think\nwe could figure out a way to tune it at your usage.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/41#issuecomment-167371952.\n. \n",
    "JiahuiJiang": "@ben-manes Can we reopen this ticket?\nCurrently we are using Caffeine where it builds a working directory during population time, and in the removal listener we remove the directory. And for the same key, the path to the directory is always the same for debugging easiness.\nBut what happened to us is, the removal listener doesn't get called right away (or at least before next time someone accesses the key), so when a user access that key again, it doesn't recreate the directory because it's still there, then the working directory gets deleted.\nSo maybe we can make it optional to be \"guarantee to finish cleanup before the next access\"?\nThanks!. ohh! wasn't aware of the existent of CacheWriter. This looks exactly like what I need. Thanks! :). ",
    "vicApakau": "oops. found it on your link.\n. ",
    "danhyun": "It looks as though we can get access to ConcurrentHashMap methods via Cache#asMap. Is this the way to go?\n. No this is excellent, I knew I was missing something. Thanks again!\n. ",
    "valdo404": "It's true, but in my case the listener is never called. Even if expiration is not proactive, we expect it to happen. Which is not the case. \n. I'm sorry. I stripped the insertion from my code. I will add them tomorrow :/\n. ",
    "valery1707": "Only some people's questions may be a real bug that need to be added in issue tracker. Questions asked by email is not so bad, but it not public and other people with same question will be ask it again and again.\nLink to SO can show appropriate way to ask questions about project. If SO is really appropriate way :)\n. Thanks\n. ",
    "guidomedina": "Hi Ben, sorry for using this issue to ask a quick question:\nI have been using Guava and now for small caches I just switched to Caffeine as I don't need Guava features, there are some concurrent maps I'm using for very specific caches which are growing to a finite level not a big number, mostly load almost all at start up and then just read, am I better of with ConcurrentHashMapV8 or ConcurrentLinkedHashMap ?\nI mean, most of my caches are say 95%+ times read ops hence I'm using computeIfAbsent when writing and few times computeIfPresent for specialized writes.\n. Ah, I confused you there when I mentioned the CHMv8, what I meant was that I'm already using Java 8 and AFAIK its CHM is better than older versions (v8) correct?\nSo what I'm trying to figure out is Java 8 CHM performance vs CLHM and Caffeine counter parts (if any present which I didn't find), for specific expiring/loading caches I'm already using Caffeine cache which I migrated from Guava's cache.\n. oh sorry I didn't notice the benchmark link, thanks again for the nice work you have done with Caffeine.\nThe trick is simple indeed, but you have to make sure you call the loading function once, hence falling back to a pessimistic locking but for a cache that's perfectly OK.\nI have another huge project which I worked before that I recommended them to migrate from Guava to Caffeine.\n. Nothing to worry about then as the projects I work for they won't get hands on 32+ cores for at least 1 more year AFAIK, is there any performance difference between:\nCache<K, V>.get(key, mappingFunction) vs LoadingCache.get(key) with a predefined mapping function? i.e.: newBuilder().build(key -> mapping)\nAssume the same, neither expiration time nor size bounded.\n. Then it is more convenient to use as people will tend to go crazy with lambdas believing Java will just use the same function without realizing they are using a dozen of different functions even if syntactically and semantically they are the same function.\nI know I know, JIT might help, but who knows. \n. Yeah, the huge project I mentioned I introduced Guava long time ago and LoadingCache is heavily in use but they are still in Java 7, I have loading caches for things like Pattern to keep compiled patterns in-memory forever.\nI will start a new contract next month with them and my first task will be to migrate to Java 8 and while at it migrate to Caffeine.\nThanks a lot again for your help.\n. I also wonder why there is only a long version of NBHM, why not for int also.\n. I believe now that it is more of a specialized case for NBHM as there it can be just copied & pasted, for non-concurrent cases Agrona provides these int versions which I started using.\nIn the long term there will be some nightmares to tackle first when ValueTypes make entrance.\nClosing it, thanks for the ideas.\n. Nice !!!\nI can only see good things coming out of this, my client uses lots of caches extensively and Solr to do lots of map searching, etc, millions of results and have the philosophy of: Give 31g heap sizes to JVM as the services are memory hunger due to heavy use of caches, that to tackle a design problem where distribution of services was not possible -which I will gradually add with the aid of Akka or Vert.x- hence caches are essential.\nSolr have been of great aid so reduction of time and space is also of a great importance.\n. Patching it is possible, I only have to find the time and wait till my client moves to Solr 6.\nI just had to change the default executor to a SAME_THREAD_EXECUTOR approach to emulate Guava's behavior; some applications use ThreadLocal to store I/O resources like DB connections so the cache was leaking connections.\nI can expect something similar might happen with Solr in terms of I/O resources so that's something to be aware of, so you probably want to start in Solr with same thread approach, investigate further and then remove it if not needed.\nEdit: I meant it for CacheLoader or -unlikely but probable- RemovalListener\n. Legacy code, DB connections are cached at ThreadLocal and released at the end of the whole process so if you use a CacheLoader inside this application, you re-use the existing connection or get a new one, but you don't release it there hence cannot load on a different thread -unless you release there but then you would have to change so much legacy code-\nMy personal preference for JDBC pool is HikariCP\n. Yes, now you know my pain and why I cannot use CacheLoader on a different thread, but not all is bad, that's the legacy part including a custom DB object mapping.\nBut in the other hand it has been a successful business, good team, the code keeps growing and there are good patterns in it, but no time for re-write, business too big and too many things to take into account so it just keep moving, mixing the old with the new, etc.\nImagine PostgreSQL + Riak + Solr, my next step is to add micro-services and slowly move some services out of the main application.\n. Asynchronous design in general requires some mindset and most developers indeed are hostile to it, it has always taken me some time to convince people of queues and fire-and-forget approach, you might enjoy Vert.x, take a look at it, it is very simple, easy to work with and I think it scales very well.\n. Ah, never mind, I see this feature is already present:\nhttps://github.com/ben-manes/caffeine/wiki/Eviction#time-based. @ben-manes is this a proper implementation for the problem I described on this issue? or am I extending the cache duration with this implementation? what I'm trying to accomplish here is a counter part of expireAfterAccess(time, unit) but to expire cached null values in 1/5th of the time of real values:\n```\nprivate static final Object NULL_VALUE = new Object();\nstatic Expiry createVariableExpiry(long duration, TimeUnit unit)\n{\n  final long standardDuration = unit.convert(duration,NANOSECONDS);\n  final long nullDuration = standardDuration / 5;\nreturn new Expiry()\n  {\n    @Override\n    public long expireAfterCreate(String key, Object value, long currentTime)\n    {\n      return value != NULL_VALUE ? standardDuration : nullDuration;\n    }\n@Override\npublic long expireAfterUpdate(String key, Object value, long currentTime, long currentDuration)\n{\n  return value != NULL_VALUE ? standardDuration : nullDuration;\n}\n\n@Override\npublic long expireAfterRead(String key, Object value, long currentTime, long currentDuration)\n{\n  return value != NULL_VALUE ? standardDuration : nullDuration;\n}\n\n};\n}\n``. Oh, no worries aboutlong, I would had probably done the same thing, too much GC is the JVM's performance greatest enemy, in fact I would had hated returning anew Duration(...)` per call ;-). You didn't screw up anything ;-) I was just making you aware of these dependencies being dragged now, since this seems to be a norm then close this issue, I'm also doing the same for Guava (excluding these explicitly). ",
    "nheitz": "Argh, sorry for the misdirect....after further excavation, it seems like jacoco instrumentation was interfering with the caffeine execution.  Bumping to the latest jacoco (0.7.5.201505241946) seems to have eliminated the failure.\nI will close, and continue to lament the usage of jacoco.  :-)\n. Is Guava's implementation safe for recursive functions?\n. You are absolutely right in my case such protection would be unnecessary.  I did find this interesting solution to support recursion, using a Reentrant lock, towards the bottom:\nhttps://www.opencredo.com/2015/08/19/lambda-memoization-in-java-8/\n. That bug claims to have been resolved in build 59 and only affecting 8u31.  I will check it on 8u71 tomorrow, as this is where I am currently. \n. Thanks, Ben, for your insightful and complete answer.  If I am reading successfully, then, the onus for managing complete termination is effectively left external to the cache itself, specifically to the semantics of the executor service.  This is fine, and I understand this pathway well enough.\nI suppose it was unfortunate that Guava by comparison seems to offer a blocking \"shutdown\", though I imagine that is a performance negative in other scenarios.  Would you ever consider wrapping the executor shutdown and and await with timeout into the Caffeine API?\n. Ok.  :+1:  .  Perhaps I have been conflating the notion of \"cache\" with a \"service that provides data backed by a cache\". Thanks again. \n. ",
    "nosideeffects": "Nah, I was just wondering. Our use of your cache is very write heavy during high traffic, so I was just looking for the lowest overhead mechanism for the job. Thanks!\n. No observations with Caffeine as of yet. Still in the early testing phase to swap out Ehcache for Caffeine. I am only asking about it because there are bouts in our system where we can actually have more writes than reads and Ehcache's put was far slower than its get. Thanks for the quick responses by the way.\nOn a separate note, is there any current or potential support for an asynchronous weigher. As in, allow the entry into cache immediately, but weigh and evict if necessary on a separate thread?\n. How do I make use of the asynchronous weigher without a loading cache? My use case is as follows:\nprivate static final Cache<String, Object> cache = Caffeine.newBuilder()\n        .maximumWeight(MAX_WEIGHT)\n        .weigher(expensiveWeigher)\n        .build();\nThat is, I already have the values that I want to put into the cache, but I don't want to wait for for the weighing to complete before the put returns.\n. Thanks! I will give your initial suggestion a go first. I think it might be an acceptable race, since for our use case it is more important that larger weights (which take longer to compute) to be accurate than the smaller ones.\n. If I return a weight of 1 instead of 0 initially, the first suggestion may be good enough for now. Thanks for your help.\n. Thanks a bunch!\n. I will see if I can extract a reproduction. The code in question seems to put pretty infrequently (like once per web request), so this is extremely strange.\n. So we do have a long living cache for each web thread that we invalidateAll at the beginning of each request. Could this be causing the issue? Should we be building a new cache instead?\n. The caches are ThreadLocal. And they often only contain a few entries. I haven't reproduced locally yet.\n. Thanks for looking so much into this! I've taken your suggestion and used a LinkedHashMap in this instance, instead, as there was no need for a concurrent structure. No doubt your improvement to scheduling will help others, and prevent this issue from occurring in our caches that do need concurrent access.\n. ",
    "AlecZorab": "If you'll excuse the use of some genuinely appalling scala:\ndef getCache[T: ClassTag]: Cache[Handle, T] = {\n    var cache = cacheManager.getCache[Handle, T](cacheName[T], classOf[Handle], ct[T].runtimeClass.asInstanceOf[Class[T]])\n    if (cache == null) {\n      cache = cacheManager.createCache(cacheName[T], getConfig[T])\n    }\n    cache\n  }\nI would, in an ideal world, like to be able to use just the getCache call and have that come back with a cache configured with using the application.conf.\nShould I expect the following block to return true?\nval provider = Caching.getCachingProvider\n  val cacheManager = provider.getCacheManager\n  val sampleCache = cacheManager.getCache[Handle, String](\"aCache\")\n  sampleCache == null\n. Yeah, if I explicitly an empty object in the config it finds it fine. My problem is that I don't know ahead of time what caches I'll need.\nI had hoped I'd be able to just code the whole module against jsr107 without reference to a specific implementation, but I guess I'll have to drop that and just use the TypesafeConfigurator\n. Yeah, I think the most pragmatic thing to do is for me to just configure it directly as needed and just provide multiple implementations at my end to provide the relevant configs.\nMy reading of the spec around getCache was obviously a little bit overly optimistic. Apologies for the noise!\n. Yeah, I'm expecting to need to move to an as yet unknown distributed cache at some point in the next 12 months, otherwise I'd just be coding against a proper api.\n. ",
    "yrfselrahc": "This all sounds sensible to me, though I'd workshop the proposed default\nmethod name for simplicity.\nI agree with your hesitation to add another per-entry field for\nexpireAfterWrite.\nCharles\n. One idea would be resolveRefreshConflict, and then have it return V instead\nof boolean.\nCharles\nOn Mon, Mar 14, 2016 at 12:46 PM Ben Manes notifications@github.com wrote:\n\nI'm not sold on the method name either and am hoping to hear suggestions.\nI figured that I should try to get the rest of the code ready in the\nmeantime, though.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/56#issuecomment-196405812.\n. Alternatively the method could be called resolveRefreshConflict and it\ncould return an enum indicating how to resolve the conflict.\n\nCharles\nOn Tue, Mar 15, 2016 at 4:45 PM Etienne Houle notifications@github.com\nwrote:\n\nAlso, if the user decides to return a new object, it means he would\nprobably be blocking the thread because he might need to load that object\nfrom somewhere. So we possibly want to discourage that...\nAlternatively we could assert that V is either the current or new value.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/56#issuecomment-197012492\n. My preference is for retainOnConflict or resolveOnConflict, mainly since\nthey sound more like caching behaviors. I'd go with retainOnConflict for\nthe currently proposed boolean return value, or resolveOnConflict if it\nwere to return an enum.\n\nI continue to feel that returning an enum, while strictly unnecessary, goes\na long ways towards increasing the readability of the method and its use in\npractice.\nLooking back at this after previous discussions I had to reread your text\nto figure out if the retain/preserve was referring to the refreshed element\nor the modified element. The use of an enum would make that very explicit.\nCharles\nOn Tue, Apr 19, 2016 at 8:06 PM Ben Manes notifications@github.com wrote:\n\nI plan on cutting a minor release (2.3) soon and, given how close this is,\nit bugs me not to finish it off. I think my only mental blocker is the name.\n- retainOnConflict: a query, but it is not clear which value is\n  being retained\n- resolveOnConflict: a action, that sounds heavy, but is really just\n  a boolean response\n- preserveOnConflict: a query, indicates that the current value\n  would be preserved\nThere's probably a few other good options on a better thesaurus. Ideally\nthe method chosen sounds like a question and it is obvious what the\nreaction will be to the answer. I think \"preserve\" is the closest word I\ncan find to describe intent and minimize confusion.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/56#issuecomment-212179434\n. When you phrase it that way I actually prefer the clobbering.\n\nIt sounds like the refresh behavior simply needs to be documented,\nindicating that the routine refreshes could clobber other asynchronous\nchanges to the cache.\nCharles\nOn Wed, Apr 20, 2016 at 1:08 PM Ben Manes notifications@github.com wrote:\n\nI agree that \"retain\" and \"resolve\" feel more natural in the context of a\ncache. I think preserve is less ambiguous. I'm a little weary of an enum,\nbut could be convinced.\nI'm unsure if I have created a more confusing mess by deviating from\nGuava's clobbering behavior, or at least not making it default. Take for\nexample the timeline,\n- t0: k => v1\n- t1: LoadingCache#refresh(k)\n- t2: k => null (expired, evicted, invalidated)\n- t3: k => ?, refresh completes\nIn Guava, Charles had this insert the new value. But we don't know why it\nchanged underneath us - it could be benign like expiration, or we could be\nloading old data if explicitly invalidated. If we reject conflicts by\ndefault then the refresh drops the new value.\nI suspect Charles would argue that the consistency around caches is murky\nand users should source from the system of record if they need strict\naccuracy. That then leads to clobbering being a natural, because\ninconsistency may be unavoidable. If so, then exposing some conflict\nhandling to the user isn't too important. And if exposed but generally\nunavoidable, then clobbering as the default highlights that.\nWhat would you expect / want?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/56#issuecomment-212517588\n. Well, I favor a well-defined clobbering that does not expose a knob to\ncontrol it. I think a reasonable argument could be made for discarding the\nrefreshed result if that value at the time the refresh began had changed.\nThat would be semantically equivalent to cancelling a refresh when a write\noccurred.\n\nDoesn't a similar problem exist when an explicit write occurs while a\ncomputation is pending? What do you do in that case?\nCharles\nOn Wed, Apr 20, 2016 at 2:29 PM Ben Manes notifications@github.com wrote:\n\nDo you prefer clobbering with documentation (e.g. Guava), or clobbering by\ndefault with documentation?\nMy concern with blindly clobbering is that it hides races that could cause\ninconsistency. Yet you're right that its usually benign and not too\nworrisome. Is there tangible value to let the user opt-in to deciding the\nconflict resolution or do you view that as adding unnecessary conceptual\nweight?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/56#issuecomment-212546642\n. My personal preference with cache loading is to think of it in terms of\nlinearizability, which requires that it appear as if the get which\ntriggered the load appears to have taken place at some unique sequential\npoint between the start and end of that call. If the entry was absent when\nthe get began then this allows for two potential outcomes: either a newly\nloaded instance is returned, or an instance created concurrently with the\nget is returned. Either are acceptable. In terms of linearizability it\ndoesn't matter if a new entry is loaded and then discarded due to a\nconcurrent insert, as it will never have been visible within the cache.\n\nI've thought less about refresh. It seems fine to me if a refresh begins\nand then successfully stores its computed result even if the old value has\nsince been automatically evicted. This allows eviction to be used to\nprevent the return of stale data, and refresh to be used to ensure that\nfresh data is generally available. (Admittedly this argument applies more\nto time-based eviction than size-based eviction; I would be fine if the two\nwere treated differently, and if a size-based eviction essentially\ncancelled the refresh.) The more complicated scenario is when a user\nmanually inserts a new element into the cache. In this case I'd be inclined\nto follow the precedent of cache loading, and to discard the refreshed\nresult in this case.\nIn other words, I argue for all manual user additions to the cache (as well\nas size-based evictions) invoking a \"cancel\" on any pending loads or\nrefreshes, and for allowing refreshes to succeed in all other cases.\nCharles\nOn Fri, Apr 22, 2016 at 5:04 AM Ben Manes notifications@github.com wrote:\n\nA part of me doesn't like how it means evicted entries would come back to\nlife thanks to the refresh, but I guess some level of thrashing at the low\nend of the LRU scale is expected.\nGuava's performs a putIfAbsent if the old LoadingValueReference isn't\npresent. This means than an explicit invalidate or an eviction will\nresurrect the entry from an in-flight refresh. My current patch does not do\nthis, and therefore the new unit test passes for Caffeine and fails for\nGuava. I think that the reason for this appears to be the interpretation of\nthe JavaDoc on refresh(key).\nLoads a new value for key {@code https://github.com/code key}, possibly\nasynchronously. While the new value is loading\nthe previous value (if any) will continue to be returned by {@code\nhttps://github.com/code get(key)} unless it is evicted. If the new\nvalue is loaded successfully it will replace the previous value in the\ncache...\nThis indicates the previous value is the instance. If updated Guava\ndiscards the refresh and explicitly calls out eviction. The statement of\nreplace indicates to me that the putIfAbsent was a mistaken. However, if\nGuava had done this then it is not clear what the removal cause should have\nbeen, since the refresh action wouldn't know why the entry disappeared.\nWhat is the desired behavior here?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/56#issuecomment-213341054\n. At first glance I would agree that the more correct behavior would be for refresh operations to be cancelled by concurrent writes or removals. That seems to be the only way to maintain linearizability.. \n",
    "ehoule": "Also, if the user decides to return a new object, it means he would probably be blocking the thread because he might need to load that object from somewhere. So we possibly want to discourage that...\nAlternatively we could assert that V is either the current or new value.\n. I think with retainOnConflict it's not clear if it means retain the modified value or the refreshed one. I think 'preserve' is clearer that it should keep the one already there (the modified one). I also like the 'resolve' with an enum.\nAlternatively with could invert the query, eg. clobberOnConflit (or any synonymous of clobber).\n. ",
    "anuraaga": "Looking forward to this :)  (had to temporarily rollback using caffeine which was causing this deadlock in our asynchronous server, not sure why exactly it happened but pretty sure full asynchronity would cause it to go away - https://gist.github.com/anuraaga/49eb6d92a9b4c656126743314599a56c)\nAs a user of the cache the clobbering sounds fine - as you say, I wouldn't expect perfect consistency from a LoadingCache and generally would expect an extra refresh cycle to catch updates in cases like this one. A part of me doesn't like how it means evicted entries would come back to life thanks to the refresh, but I guess some level of thrashing at the low end of the LRU scale is expected.\nAlso FWIW, I guess in practice, most use-cases of a LoadingCache probably only ever call get(), and some may call refresh() (critical read). IMO, as long as puts aren't completely broken, it's probably not worth worrying about their consistency too much\n. Thanks - I've looked into our thread dump in more detail and put what details seemed relevant in #69, maybe it provides clues on a repro.\n. Sure I can try, but not sure I can get to it before Monday (when I definitely can). If I have a bit of focused time this weekend will try though.\nBy the way, one key point may be that my ComoletableFuture is a wrapped Guava ListenableFuture. Looking at the implementation of AbstractFuture addListener (on my phone so not so precise...), it appears to me that listeners are called in reverse order of addition (probably to avoid having to maintain a tail pointer). If this is different behavior than jdk CompletableFuture then it may not repro without Guava.\n. @ben-manes Thanks for fixing this issue :)\nI have written a simple unit test that passes with 2.3.0 and times out on 2.2.7 due to the deadlock. Would you be able to incorporate this into caffeine's tests (caffeine tests have a lot of abstraction so wasn't too sure how to best do it to send a PR myself)?\nhttps://gist.github.com/anuraaga/f56ae65a9fd3975afe99cc725108850a\n. Also FWIW, the issue reproed just fine with jdk8 CompletableFuture and not guava - I guess it has the same linkedlist optimization that causes callbacks to be run in reverse order.\n. Yeah we had noticed the issue here referencing the JDK bug and were excited to first try updating to 1.8u92 from an affected version of the JDK. But it didn't change what we're seeing unfortunately.\nAlso, just for an idea, 23 out of our 64 event loop threads are waiting at that same stack trace so it seems to be happening quite a lot. \n. Yeah unfortunately currently have no leads on an easy repro, but we will first try replacing the executor to see if it helps and will report back.\n. Thanks a lot for helping on this! Also glad to see that it looks like Stresser now has a case closer to our production usage (all get / refresh, no puts) to keep us safe going forward. :)\n. Oops, sorry for not checking there! Glad the method is there :). ",
    "mjburghoffer": "Ah yes, I suppose that does make more sense.  My bad.  Thanks for looking at the PR!\n. I think for Guice, I'd just make sure to not annotate the provider with @Singleton, and i'll get a fresh one every time.  For the simple case of just setting max size, etc, it should be fine ex:\n@Provides\n  fun provideCaffeine(): Caffeine<*, *> {\n    return Caffeine.newBuilder()\n        .maximumSize(2000)\n        .expireAfterAccess(60L, TimeUnit.MINUTES)\n  }\n. I guess the one issue I can see is Caffeine has generic type parameters, and that can sometimes be a pain with Guice.  It might be nice to have some way to inject all the settings that are independent of the types of the keys and values of the cache.\n. Realistically we'd have some sort of YAML document or app.properties for configuring the cache.  I think your first suggestion makes the most sense, just used @Named providers for Caffeine's - or providing the cache directly.  Thanks again for the help.\n. ",
    "drcrallen": "@ben-manes yes, you are a faster typer than I am. This issue is an attempt at documenting some of the (minor) issues I came across implementing a weigher. Hopefully someone searching for similar problems will find this thread later and use your suggestion in https://github.com/ben-manes/caffeine/issues/62#issuecomment-204101123 \n. I'm really curious what turns out to be \"stressed\" here.\nWe had an issue today where nodes with caffeine cache fell over on memory usage.\n(the %JVM heap is old-gen with CMS)\n\nThe # evictions per minute reported by Caffeine can be seen below:\n\nOne thing I don't have is weight (size in bytes in this case) eviction rather than straight up eviction count.\nI suspect in our case that the result was from not trying to modify GC tunings to accommodate an on-heap cache (even though it was small compared to total heap) rather than a Caffeine cache problem directly.\nOther nodes which see lower # of requests are not having these issues.\n. To make the prior statement a little clearer. The nodes having issues have about 4x the number of cores, and see about 7x the number of requests (not all requests hit cache). The eviction count on both nodes reported by Caffeine is roughly equivalent.\n. @ben-manes Just to follow up on this. I've had two tests run internally. One intentional and the other unintentional.\nOn the intentional side, changing to G1GC (with some basic tunings) kept the problematic nodes from having issues and tucked in the long-tail of response times (yay!)\nOn the unintentional side, some of our dogfood machines (internal use only) using caffeine had OOME. These machines had \"maximum weight\" tuned to estimate the on-heap size of the data. The actual heap-dump size retained by caffeine turned out to be 99.89% of the configured maximum weight. So I call that a major good sign.\nSo for my specific related issue, I think it was GC tunings.\n. Tuning purposes. My intended use case is to look at the eviction velocity by weight to help tune cache sizes.\nTypically a user sits down during a session and does some quantity of requests. The cache allows requests which they (or similar users) repeat avoid performing more expensive operations. (for my use case this actually functions in three tiers: a small local cache on heap, and a larger cluster-wide cache in memcached, and a \"no cache found, recalculate it\" layer). \nA key question is: \"How much cache do I churn through per hour during peak usage?\"\nIf I know how long is typical for a group of requests that I would hope would be cached (ex: one browsing session), then I can make sure the cache has the proper maximum weight to accommodate that quantity.\nSo lets say I know that normal eviction churn during peak is about 1GB / hr, and I know 90% of my users have a session length of 1 hr. To make sure my weight maximum can accommodate 90% of the browsing sessions (approximately), I'll need at least 1GB headroom in the cache. If I only give the cache 100MB, then I would expect it to only hold about 6 minutes of data, and therefore is probably way under-provisioned. If I give it 10GB of weight then I can expect to waste heap space keeping cache around that will probably never be used.\nMostly I'm interested in getting some sort of metric that can track \"Is my weight upper bound right-sized?\"\n. Sounds good, thanks.\nNote that my current best method for right-sizing the cache is to spin-up/reboot a node and monitor cache.policy().eviction().weightedSize() over time until it hits its limit. This does not give good live monitoring of cache, but it does give me some ballpark for expected cache performance.\n. java\n  /**\n   * Returns the sum of weights of evicted entries. This total does not include manual\n   * {@linkplain Cache#invalidate invalidations}.\n   *\n   * @return the sum of weights of evicted entities\n   */\n  @Nonnegative\n  public long evictionWeight() {\n    return evictionWeight;\n  }\nSomething similar to the above would fit the use case. Since weights can be up to Integer.MAX_VALUE, I'm not sure what the best way to account for long overflow in a long-running service would be.\n. @ben-manes validation halted due to https://github.com/ben-manes/caffeine/issues/77\n. This seems to be resolved through a combination of updating to 2.3.0 and using a single threaded executor for maintenance tasks.\n. There is a gap between the eviction bytes and when the problem appears.\nHere's the eviction throughput in bytes reported by caffeine (weight == bytes)\n\nHere's the sys time:\n\nHere's the number of requests per minute going into the cache:\n\nYou can see the eviction weight reporting stops well before the number of processed requests stops. And the processed requests stop at the same time the SYS cpu kicks into high gear.\n. During peak, cache hit rate is about 20%, so 80% of the queries end up writing new data into the cache that will probably just get evicted within an hour.\n. java version \"1.8.0_45\"\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\n. @ben-manes we're a little bit looser with JDK version enforcement than I'd like to admit, so while I'm happy to upgrade the JDK version, I think the solution for the druid-cache plugin for caffeine is going to be setting a single-threaded executor so that the behavior is more consistent among 1.8 releases. For now I've simply disabled local caching through caffeine (so nodes go directly to memcached instead). I'll get the fix in early next week and report on this thread and the other when I have results.\n. Thanks for looking into this!\n. @ben-manes what is the commonPool optimization you are talking about?\n. @ben-manes thanks for the explanation.\n. As a general update this is currently baking. Should have results sometime next week.\n. This seems to be working fine with a single threaded executor.\n. Minor feature request\n. Sounds reasonable to me.\n. As a concrete example, right now I have some code which is doing this:\ncache.asMap().keySet().stream().filter(key -> key.namespace.equals(namespace)).forEach(cache::invalidate);\nbecause cache.invalidateAll(cache.asMap().keySet().stream().filter(key -> key.namespace.equals(namespace))) doesn't work, and I can't pass in a predicate to invalidateAll either.\nNot sure the desired code workflow for something like that would be.\n. @ben-manes would that cause concurrent modification exceptions by chance?\n. @ben-manes I found this video from the JCache guys to be pretty good\n. @ben-manes thanks for the excellent explanation. I can confirm setting the seed does indeed allow the test to pass, and the reason for the randomness affecting the tests makes sense.\nThis PR can either be closed out, or I can modify it with the thing that makes it work for example purposes if desired.\n. @ben-manes I can likely do the instrumentation with the restriction that it doesn't screw around with the system more than, say, the YourKit agent, and doesn't leak confidential information that might be in the cache.\nWhat method are you thinking of regarding capturing traces?\n. For posterity, https://bugs.openjdk.java.net/browse/JDK-8078490 is the bug @ben-manes was referring to.. ",
    "dlmarion": "Fair enough, thanks for the quick response. I was thinking it might fit well into the 100% read use case. I have a use-case for a large static map that is loaded at application start up and read concurrently by many threads. Thanks also for the pointer to Apache Mnemonic, I had not heard of it.\n. ",
    "jgoldhammer": "Thanks, that is great.. ",
    "iroh": "That makes sense. I am not reusing the builder. I just wondered if the same cache instance requires a new StatsCounter instance periodically. But I guess it doesn't and the purpose of supplier was to protect against builder reuse.\nI am using my own StatsCounter supplier which returns the same instance every time its get() is called. I did this in order to have a handle on the statsCounter through which I could get a snapshot of stats and publish metrics using Dropwizard's metrics library (I didn't know back then that I could get stats snapshot even with an async cache by calling cache.synchronous.stats()). I have a scheduled poller which gets latest cache snapshot periodically from the statsCounter and stores it. My registered gauge metrics basically read off this stored snapshot instead of calling statsCounter.snapshot(). I did this because I have multiple metrics and I wanted to reuse the same snapshot for all the metrics instead of each metric triggering a snapshot request. Is this the right way to do it? Or should I not worry about storing the snapshot and reusing it across all my metrics?\n. That would surely be a better way to do it. \ud83d\udc4d \n. ",
    "blschatz": "Thanks for the detailed replies Ben. I ended up using your CacheWriter recommendation to implement what I need. \nI think you have captured what I am trying to achieve, with the clarification that the predicate would only be evaluated if the time based expiry evaluation indicated that the entry was expirable. My way of looking at it is as an override that might cause a \"reset\" on the expiry timer of a cached object when the timer expires and the predicate matches. \n. No need to apologise. I appreciate the thought put into the answers. \nI agree, the current workaround is ugly but works fine, and my proposal is rather specific. I look forward to a general approach evolving. Happy to keep the dialog moving forward as your design crystalises.\n. In case you weren't aware, Agrona has adopted Netty's hashed timer wheel. https://github.com/real-logic/Agrona\n. I'm not sure I follow. Is the proposal that at expiry time, the Expirer.update() method would be called, wherein the specific logic (where we want to keep the entry alive) could return a time in the future, thus keeping the entry alive?\n. ",
    "Maaartinus": "\n\nr < 0 : don't expire (eternal)\nr == 0: no change\nr > 0 : the amount of time before expiring in nanos\n\n\nTo me, this feels wrong. Eternal and in 2**63 nanoseconds (292 years) is about the same, so I'd go for r == Long.MAX_VALUE instead of eternal (no special casing needed, saturated math does the job). Any negative value should mean \"already expired\".\nNo idea how to handle \"Don't change current expiration\" properly.. I surely agree with you, but what do you think about this?\n.conditional(onceInABlueMoon())\n    .recordStats() // gets mostly ignored\n    .maxSize(1000) // also conditional\n.conditional(true) // revert to normal\nSadly, the IDE autoindent knows nothing about conditional and destroys the formatting.\n. > I had thought that since the overlap wouldn't have a big effect because the array would be large, the bit index is a mod using 7-bits, and a shift would result in different results.\nIIUIC you were using the same bits for selecting a word and for selecting the bit, i.e., using only one bit per word: https://github.com/ben-manes/caffeine/blob/master/simulator/src/main/java/com/github/benmanes/caffeine/cache/simulator/admission/bloom/BloomFilter.java#L97\nUsing the highest 8 bits works well, as long as table.length <= 1<<24. Bigger tables are no better as a table with  2**32 bits and should never be created. If you really needed them, then you can't use int item.\n\nIn constructor, you do checkArgument(randomSeed != 0), but a value of 1<<31 is a disaster, too. I'd either require or enforce randomSeed to be odd.\n\nIn ensureCapacity you replace the table in case it needs to grow. This leads to false negatives, which is pretty bad as the method is public and can be called anytime by anyone. I'd make it private as resizing Bloom Filters is no good idea (it can be done, but costs more than an initially properly sized filter).\n\n\nThe power-of-two sizing might make the bloom filter larger\n\nSure, larger, faster and also better.\nI think that the power-of-two sizing is nearly always a good idea, as all you lose is the ability to fine tune the FPP. And I can hardly imagine a case where the exact FPP matters. Spending a bit more memory and getting a bit better rate or the other way round is nearly always fine.\n. > I was using the same bits, but modulating based on a different bytes.\nFor With tableMask==2**16-1, I can't see how you could access e.g., bit 1 of table[0].  From index==0 follows that the two LSBs of hash are zero, therefore also hash >>> 8.\nAFAIK it could work well for table.length <= 256 only and started to lose with every additional bit up to table.length <= 256*64, so that there were only 256*64 bits in use for such table sizes.\n\nYou can drop INDEX_MASK as Java guarantees the shift distance is taken mod 64.\n\n\nNote that this is a utility for the simulator when experimenting with policies.\n\nWhat about calling the method ensureCapacityAndMaybeClear? It should do exactly what you want, but with such a name it's perfectly safe even when taken out of context by someone else 100 years later.\n\nPerforming it as a re-initialization vs. new instance is about code organization and ease of use.\n\nI guess you're clearing the filter from time to time anyway, so clearing it when growing does no harm  (especially as it never shrinks).\n\nI wonder if you could do better when using no intermediate int. My point is that collision with 2**32 bits start already at about 2**16 entries. Sure, you need many collision before you get problems.\n. > I'm not surprised that the JVM optimizes the modulus, but I do like the clarity by not making that assumption in bit manipulation code.\nIt's not an optimization. The behavior is defined in the JLS and luckily it's defined in exactly the way most CPUs (including i386) do it.\nSo I'd call the shorter version clearer (as any additional operation can make someone ask WTF?). Unfortunatelly, unlike Javadoc, the JLS is more than just one click away.\n\n\nIt would be interesting to hear more of your thoughts here.\n\nI've tried it and got no improvement (tried up to 2**24). The measurement may be influenced by the pure quality of Random, so I've tried SecureRandom, too.\nI'm still on Java 7, so I can't directly use your code (and what I did is not worth posting).\n\n\nTrying to think through my original logic, what I should have done is shifted when calculating the table index.\n\nSure, just as if we could index bits directly. In some over-optimized code I did\nreturn (array[hash >>> distance] << hash) < 0;\nwhich uses the 32-distance MS bits for indexing and the six LS bits for bit position (where bit zero is the sign bit), so that one operation is saved.\n. It looks like your table gets problems at 2**27 and 2**28 elements with 5% and 19% false positives. The cause seems to be the reduction to int causing collisions. At 2**26 elements there's no problem yet (3.7%).\n\nOver optimized code for data structures is sometimes a good thing\n\nWhen I get some good result, I will post it all. I'm just fooling around with setting two bits in each long, i.e., doing two steps in one for the price of violating the independent hashes rule. It's like forcing the second index to be equal to the first one in the original algorithm. It looks like this\n```\npublic void put(long e) {\n    e = spread(e);\n    setTwo(e);\n    e = respread(e);\n    setTwo(e);\n}\nprivate void setTwo(long e) {\n    table[index(e)] |= firstBitmask(e) | secondBitmask(e);\n}\nprivate int index(long e) {\n    return (int) (e >>> tableShift);\n}\nprivate long firstBitmask(long e) {\n    return 1L << e;\n}\nprivate long secondBitmask(long e) {\n    return 1L << (e >> BITS_PER_LONG_SHIFT);\n}\n```\n\n\nI don't think I grok your last bit of code\n\nIt's simple: Instead of masking the bit inplace (and constructing the mask using a shift) with \nx & (1L << hash)\nI shift it and use a fixed mask like\n(x >>> hash) & 1L\nThis returns a different value, but it's equivalent as all we care about is equality to zero. As the direction does not matter, I do\n(x << hash) & Integer.MIN_VALUE\ninstead and then I leave out the masking as the bit to be tested is the sign bit. You might want to look at some usage, it's two lines only.\n. Nice that you're interested. I've just added the reverse masking idea. I haven't profiled it, but I guess it's pretty fast due to reduced memory accesses. Instead of talking about it, I posted it. It's copy-pasted ugly temporary crap, but it illustrates the idea.\nIn BloomTest (my MembershipTest), I avoided storing the whole array as it made me swapping for the big capacities I tested. The other files aren't worth looking at.\n. > One implementation quirk to remember is that I want to reuse the hash computations.\nI avoided the 32 bit intermediate in order to avoid collisions. All other changes were mainly to show some alternatives.\nIf you can reuse the computed value, then you can pass the hash or some preprocessed value (like smear(e)) as the argument to put and mightContain.\nIf you can't reuse the computed value, then I'd probably go for different hashes, so that collisions in one place don't correlate with collisions elsewhere.\n\nIt looks like your improvements to in variant 3 don't degrade at 2^28, but might not be translatable into reusable hashes. \n\nThe degradation is caused by using an int. With 2**28 inputs, there are tons of them mapped onto the same 32 bit quantity. Whenever an absent value collides with a present one, you get a false positive. I'd bet, you can keep your hashes when you extend them to long.\nNote that Guava uses 128 bits of the input, I use 64 and you use just 32.\n\n\nOh shoot. Another issue is that you're taking advantage of the simulator providing long keys. In the cache we'll only have access to an integer (Object#hashCode) which degrades the quality of the hashing that we can leverage.\n\nThen there's nothing we can do as the collisions are already there. As a typical cache has not many millions entries, it's fine. If you wanted to support huge caches, you'd probably need some 64 bit hash (using Funnel or some user-supplied longHash). And this not only for the Bloom Filter but everywhere.\n\nSo running all three variants (Caffeine, BloomFilter3, Guava) and its consistent at a 32-bit element (Long.hashCode(e)). All degrade at 2^28 with a similar false positive percent.\n\nGIGO. No surprise here.\n\nTo summarize:\n- Avoiding 32 bit intermediates prevents collisions unless there are already there.\n- Using two bits per word makes it faster while partly giving up hash independence. The FPP does not seem to suffer.\n- Using one hash as the input for another one provides some more spreading, so you could make spread a bit simpler and faster (which I did not). You'd need to benchmark and to test with badly distributed values and with real data to find out more.\n. > It would require jigging the frequency sketch to be similar\nI don't think so. The implementation of the BloomFilter does not matter as long as it keeps the promised FPP. Concerning CountMin4, a similar optimization can be done in a straightforward way, but it may work less well as there are only 16 counters per word instead of 64 bits. So constraining two operations to a single word may be too much.\nI'm just thinking about using 8 bits counters. Using a byte[], they're surely faster and simpler, the problem is how to amortize the doubled memory cost (or how to use half the number of counters).\nCan you show me some stats concerning what parts are most time-consuming?\n. I'm going to switch to Java 8 soon, so I'll write more when it's done.\nI see that my question was unclear, so I retry: When using the cache with some real data, what percentage of the runtime do the main parts need? I mean, I could imagine making CountMin4 faster, but it doesn't matter when it's already way faster then the other parts.\n. Wow! That's damn complicated. Given that, I'd stay away from all tools until I'd find some tiny piece to be optimized. I'd write and measure some real-like code instead: multiple threads, each running some known access trace in a long loop, measure the work done in a few minutes. This has tons of problems, but so do profiling and microbenchmarking.\nIn order to find out how much some part takes, I often use a trick of doing the part twice. I guess this could work for the sketch write time (write to two sketches, read the first and blackhole the second; or read from one randomly chosen), but not for the read buffers. The results may or may not be realistic, e.g., because of a single buffer just fitting in the CPU cache (and two buffers causing many misses).\nOf the problems you mentioned, it could help with\n- hard to tell if the padding is beneficial (just switch it off and re-run)\n- a slower drain might be incorrectly viewed as a gain (not when it causes cache misses)\n- a benchmark quickly exhausts it (a real-like benchmark does not)\n- higher hit rates are a huge latency win, so that's critical (a real-like benchmark measures it right)\n\nthe sketch write time\n\nHere, the doorkeeper obviously helps somehow. I guess that conservativeIncrement could make it faster (not slower), when used with an optimizied version of incrementAt: When min<15, then a simple addition works (no overflow possible). When min==15 and all values were right-shifted, then the situation is the same. When min==15 and nothing was right-shifted, then just do nothing (already saturated). When only some values were right-shifted (IncrementalResetCountMin4), then you're out of luck.\nNow, some dirty tricks gaining speed for the cost of a reduced hash independence come in my mind.\n\nSometimes I wonder if it would be worthwhile draining into a multiset so that the sketch is given an increment count (less hashing, better cpu efficiency).\n\nCould you elaborate?\n. > like the doorkeeper might make sense to have only\nI meant w.r.t. the speed. It should be way faster than CountMin4 and save it some work.\n\nFor small ones it can have a small negative effect to the hit rate\n\nAny idea why?\n\nforking the hash table which I am not favorable\n\nI see, it's a huge monster. An advantage of forking would be the access to HashEntry.hash, which would save us hashing and spreading.\n\nIf we assume a Zipf distribution then many events are for the same entry.\n\nYes, but not necessarily following each other.\n\nI pushed a branch\n\nNow it's clear. I'm a bit sceptical about the efficiency of RLE as the entries may be interleaved. I was thinking about a middle ground between RLE and Multiset and pushed something. It has got far too complicated when compared to what I wanted.\nIn case interleaving is a problem, there's a simpler way: Use RLE which compares against a few most recent entries.\nAnyway, I'm afraid that managing any additional data structure eats most of the savings it provides. I'd integrate the micro-batching into CountMin4 itself. In case of the RLE it's pretty straightforward:\n Just add two fields: lastEntry and count and... you know.\nThis data structure mangling is ugly, but it saves a lot of overhead. A much nicer and maybe equally fast solution would be a BatchingFrequency implements Frequency responsible for the batching and forwarding to another Frequency. I was curious and wrote it and it's surprisingly trivial. It's actually RLE, but on the consumer side, so no arrays are needed.\nI'm curious about the typical size of the ReadBuffer?\n\nHere you probably want >=.\n. > The vast majority of run lengths is 1\nThat's too bad. An extension would be to remember the two most recent samples, but given the other problems it's probably not worth trying.\n\nthe benefit of not doing a few extra hashes.\n\nThen I'd suggest some rather low-level optimizations speeding up the process. I haven't time to do anything serious (and still haven't switched to Java 8), but I'm sure there's some gain possible:\n- single spreading common to BloomFilter and CountMin4\n- simpler operations in both for obtaining multiple hashes (using rehashing of a long)\n- branch-free incrementAt\n- optimized conservativeIncrement\nSee here for the ideas (nothing tested yet).\n\nHowever we're talking about an overhead of a few megabytes for millions of entries (e.g. database), so it may not be worth it.\n\nRight, but we may also gain some speed, if 1. the doorkeeper is way faster than the frequency and 2. it catches a good deal of accesses. But this is questionable, too.\n. Hi Ben! I'm writing here, as it's the place where we stopped half a year ago. Finally, I'm on Java 8 and can run your benchmarks and inject my ideas.\nIt looks like I can get some speed improvement for the FrequencySketch, it just isn't as good as I hoped (maybe 20% only). Anyway, I believe, that much more can be done.\nWould you kindly answer these questions, so I can get on the right track?\n\nThere's no BloomFlter there, except in the simulator, right? Why? Wasn't if useful enough, or did it cause problems, or were you busy elsewhere?\nThere's no conservative increment implementation yet, right? Why?\nWhat is the ratio of the number of calls to frequency vs.increment? Sure, it depends on the R/W ratio and the miss probability, but some estimate would help.\n. ### 0. Tons of questions\nI'm pretty lost, but it's getting slowly better. Am I right with the following:\nThe standard policy works like ...simulator.policy.sketch.WindowTinyLfuPolicy.\nDespite speaking about admission policy, on a cache miss, the key gets always inserted into data (i.e., into a ConcurrentHashMap` when not simulating).\nThe admission is about deciding if an EDEN or a PROBATION key should get evicted.\nThe EDEN is typically small. It's helpful when a key gets accessed in short succession.\n\"main uses Segmented LRU\" - do you mean \"main = probation + protected\"?\nThere are only the following transitions: new -> EDEN -> PROBATION <-> PROTECTED plus dropping from EDEN and PROBATION.\nThere's nothing like PROBATION and PROTECTED in the paper. What would happen if there was just a single combined queue?\nWhile all three parts use LRU, LFU comes in deciding about the promotion from EDEN to PROBATION.\nWhile using three queues for maintenance, there's jsut one ConcurrentHashMap used for the lookup.\nThe counters can't be stored in cache entries as they're needed for absent keys as well.\nIn order to keep the memory footprint small, they get stored based on the hash rather than the key.\n\n1. BloomFilter\n\"Doorkeeper ... increases the frequency-bias of the cache\" - but according to the paper, it only should save memory. It shouln't change the behavior, except due to the increased truncation error, should it?\n\"sketch overhead is small\" - OK, I'll give it just one more shot or two. I'm just trying a byte[] instead of the long[] table, taking the low and high nibbles on the even and add steps, respectively.\nThis is like fixing one bit when choosing the nibble, so it shouldn't be a problem. I see now, you're constraining the nibble choice even more (start has 2 bits, while 16 would be needed for choosing the nibbles from 4 longs randomly).\nUsing byte[] is a bit faster as it saves some shifting (reset is much more expensive, but it's  rare enough). It'd allow to use tables for the saturated increments (twice 256 bytes), but I'm rather sceptical about trading trivial computation for memory access (even when there should be hardly any L1 misses due to batching and tiny table size).\nI could imagine letting the FrequencySketch play a doorkeeper for itself: Depending on the state of one nibble, exit the increment loop early. For example, you could exit when its old value is even. This surely changes the way how frequency gets obtained and it may badly influence its quality. It may help the speed a lot, assuming that the vast majority of cases is not saturated.\n2. Conservative update\nI see, I missed the fork (i.e., the class I was playing with last year:D). the extra cost of CU isn't worthwhile - I had a benchmark, where CU was faster, but this somehow doesn't make sense.\n4. HillClimberWindowTinyLfuPolicy\nI guess, this is what I'll play with when I understand more of Caffeine.\nIs HillClimberWindowTinyLfuPolicy.headWindow the same thing as WindowTinyLfuPolicy.headEden?\n5. Variable expiration\nAlso very interesting. When the request came, I read everything I could find about Hierarchical Timer Wheel (which isn't much). I haven't looked on any implementation yet.\n. ### 1. BloomFilter\n\nMy rough guess is that a more frequent reset (clear) hurts recency by more often seeing them as \"one-hit wonders\" and diminishes their counts.\n\nThe BloomFilter allows for smaller FrequencySketch and this leads to more frequent reset, right? Maybe using longer counters could help? If you can make the FrequencySketch four time smaller and use six bits per entry, you still win a factor of 4/1.5 concerning the memory footprint and the reset frequency stays the same.\n\nIts interesting to hear byte[] is faster. I figured with hardware caching and cpu optimizations, the difference wouldn't be observable.\n\nI could avoid all shifting in increment, s. https://raw.githubusercontent.com/Maaartinus/caffeine/514e7a0876ab6a4345a0d78203711ed350229075/caffeine/src/main/java/com/github/benmanes/caffeine/cache/MgFrequencySketch1.java\nBenchmark                                    Mode  Cnt         Score         Error  Units\nFrequencySketchBenchmark.frequency          thrpt   30  40489828.815 \u00b1  406619.027  ops/s\nFrequencySketchBenchmark.frequency1Regular  thrpt   30  52234258.368 \u00b1 2345943.230  ops/s\nFrequencySketchBenchmark.increment          thrpt   30  34641127.583 \u00b1 1129743.478  ops/s\nFrequencySketchBenchmark.increment1Regular  thrpt   30  62642190.431 \u00b1 1928634.127  ops/s\nA good part of the speedup comes from my sloppy hashing, which might be too sloppy. It all may be wrong, as I haven't tested it yet.\n\nBut since reset is O(n) and we lack SIMD support,\n\nDo we? Isn't ByteBuffer.asLongBuffer helpful? Or Unsafe?\n\nusing a long[] would benefit its bit manipulations.\n\nActually, what I did is applicable to long[] as well. A single rotation per incrementAt could suffice, but unfortunately, we need both 0xfL << offset and 1L << offset.\n\nAnother idea is to store all counts for a key in a single slot.\n\nI'm going to read the papers. It might be similar to my two-bits-per-long BloomFilter idea.\nAnother posibility to reduce the overhead might be special treating the most frequent entries (in a sort of HashMap with no rehashing nor chaining).\n4. HillClimber\nYes, I thing writing an own HillClimber should be easy once I get an idea. I see, it only shuffles from maxWindow to maxProtected or back, so their sum is kept constant. There's no maxProbation, but wouldn't changing this sum be possibly also useful?\n5. Variable expiration\nOne nice thing is that missing the timeout is no big deal. I'll read the links now.\n. ### 1. BloomFilter\nhttp://www.cise.ufl.edu/~tali/1569342377.pdf\nThat's what I also did: multiple bits per word. The paper is more general and contains math. I'm afraid, it doesn't generalize to the counters well, as we have just 16 of them per word, but it could work.\nhttp://sci-hub.cc/10.1109/TPDS.2014.2378268\nFixed formula, I may care about when in more mathematical mood.\nhttp://sci-hub.cc/10.1109/2.268884\nCool! Isn't using 4-bit frequency counters sort of equivalent to using 16-way-segmented LRU cache? Every time an entry gets accessed, it gets promoted to a more protected area, from which the LRU entry is demoted. Sure, this does nothing for absent keys, but the analogy might lead somewhere.\n\nNope, the period is based on the cache size.\n\nDo you mean period in PeriodicResetCountMin4.ensureCapacity? It is, but so is the table size. And I can't see the table size being reduced when a Doorkeeper gets used. +++ I see, I missed the countersMultiplier\n\nThat could mean often thinking arrivals are one-hit wonders which its trying to filter out.\n\nSo the increased round-off error due to the Doorkeeper is more relevant for small sized and bites us, right?\nOr should the period be bigger when the Doorkeeper gets used?\n\nFor large caches a 4x reduction was fine, but only a 2x for small caches.\n\nA reduction factor of 1/0.75 for small caches should be easily doable (the reset seems to be cheap enough). Nothing like this can be done with the Doorkeeper, but we could try to reset it only every n-th period.\n5. Variable expiration\nI don't think that a Hashed Wheel should be used for expiration. For scheduling it's surely useful, but for expiration I don't think it's worth it. Imagine our time resolution is 1 ms and someone specifies an expiration exactly in 1 hour. Do we care about the exact time or is it good enough to remove the entry one minutes later? I guess, the second as the only advantage of doing it at the exact time is that a cache slot gets occupied 60 minutes instead of 61.\nIMHO using lower precision for far future is fully acceptable and reduces the overhead a lot. We need precise slots for the near future, but buckets like \"60 to 61 minutes\" or \"120 to 122 minutes\" are fine. After 60 minutes, we would re-label the buckets to \"60 to 61 seconds\" and \"61 to 62 minutes\", respectively (the ranges shrink to the right as slightly missing the timeout is what saves us from having to process entries repeatedly). We can still maintain exact expiration by checking the exact time on each read. The only cost of my proposed simplification is that the entry occupies the cache 2% of time longer than needed (assuming it doesn't get evicted earlier).\nIf I'm right, then we can use exponentially growing buckets and this way keep both their number and the overhead low. The above re-labeling is like moving a piece of a time wheel down in the hierarchy. The big difference is that we do want to miss the timeout by a few percent.\nA bucket can be implemented as a doubly linked list, so we need two pointers per cache entry (plus the expiration time as long or int). Additionally, one pointer and one sentinel per bucket; I guess, depending on the precision, hundreds or thousands of buckets should do.. ### 4. Hill Climbers\nWhat traces are most interesting (really good or really bad) for them?\nCan I run multiple traces and get separated outputs or some summary helpful for tuning the Hill Climber? I mean something like myHitRate / orgHitRate with best and worst and average values? I guess, not, but I'm asking before diving into the code as there's so much stuff there...\nI guess, running traces of different formats at the same time is not supported yet?\n5. Variable expiration\n\nbecause your argument and further thoughts match the ideas well.\n\nI guess, I re-re-invented the wheel. There's probably a big mismatch between what they wrote and what I understood (and could recall). I guess my ideas correspond with the Hierarchical Wheel in the improved variant (but not with the Hashed Wheel). Where \"improved = sloppy\" and my claim is that sloppiness is fully acceptable for cache expiration. So I don't think we can ever run into degradation scenarios (unless maybe with carefully prepared timeouts, where the programmer sabotages their own cache).. Hi Ben,\nunfortunately, I have no time for this at the moment. Probably in two weeks, it gets better.\nI understand enough of your hill climbing, so I could try my ideas. But, despite the problem being univariate and having a smooth curve, this optimization is difficult, because of two things:\n\na lot of noise\na history dependence\n\nThe noise can be countered by using a big sampling period and/or making small steps. An evaluation doing more than just comparing the two most recent results could help, too.\nThe history dependence is much worse as the current hit rate depends on changes done many steps ago. We know these changes, but we have no idea which of them were relevant for the current hit rate.\nI guess, these problems are not so bad in real usage as in simulation. In simulation, you want to try many different settings and algorithms and you need it to work fast. In reality, a server running for many days could profit even from very slow optimization progress. Obviously, reacting to changing input characteristics is a much harder problem.\n\nI tried a simple variable step optimization where the step size slightly increases on an improvement and not so slightly decreases otherwise. This failed badly because of the noise, which made both outcomes about equally frequent and so reduced my step size to a minimum.\nI fixed it, but got no really good results.\n\nI'm skeptical about simulated annealing and other non-deterministic optimizations, as there's already more than enough randomness there. Randomness gets used for escaping local minima, but that's not the problem here. Even if there were local minima, they'd be no problem because of the noise giving us a chance to jump out.\nNonetheless, there are traces for which SA works better than the other algorithm and than what I tried. My claim is that it's not because of its non-determinism, but rather because of other differences between the algorithms. Or worse: It's just that lacking a really good algorithm, some algorithm simply wins for a given input.\nWe'd need some way of evaluating the optimizer globally, i.e., over all samples we can grab and also over some generated samples. This is surely something I could do. We'll need to select a single scalar value telling us what algorithm is better. It'll probably be some weighted miss rate, where longer traces and worse results count more.\n\nCurrently, I can't see an algorithm working well given the history dependence. Giving the algorithm more input than just the target function could help, but I don't know how to use the already provided input and I don't know how to provide other input efficiently. The idea is like \"if we set the parameter higher, we wouldn't get this miss\", but tracking this is complicated and we'd probably get other misses, which are even harder to track.\nA funny idea could be to (optionally) collect traces in real usage and run an optimizer in a simulator using idle times of the application (or just in the background, assuming spare CPUs). This would work with a trivial optimizer like \"simulate with a bit higher and/or a bit lower parameter, choose the better, rinse, repeat\".. Sure: It's my name at gmail.com.. Sure. Sorry for the confusion. If you knew my real name, it'd be ambiguous (first, or last, or both, or both reversed, ...). I was assuming you don't know my full name and you'd assume I mean the only thing I know you know. ;)\n\nI guess, for implementing the idea from \"The Gradient-Based Cache Partitioning Algorithm\" we'd need to be able to run the cache in a split mode, so that we can evaluate two settings in parallel. This should help a lot with the history problem: The first half always uses a smaller parameter then the other. So when the first part performs better, then we know that \"smaller means better\". The only uncertainty remaining is \"smaller than what\" and is only relevant when crossing the optimum value.\nUsing the magnitude of change instead of its sign only is a pretty natural idea (which is sadly failed to try). I know that many algorithms (e.g., some neural network learning) ignore the magnitude and it helps them with badly conditioned problems - which is irrelevant for our univariate optimization. I guess, the magnitude helps a lot with the noise.\nThe Chernoff Bound is something I haven't understood yet, but it can't be too hard given their simple implementation. The idea of waiting for enough confidence is surely good.\nAll in all, I love the paper. I wonder if we really need the split, how complicated it is, and how running two smaller caches instead of a big one changes the hit rate.. Thank you for the email (and I surely agree that asking is better than bouncing).\n\nLikewise, I'm hoping that we don't need to run in split mode to take advantage of their ideas.\n\nWhat you did is like splitting in time rather than splitting in space. One disadvantage is that the number of samples in the past part is fixed. Another disadvantage is that we can never say that \"smaller is better\" as any gain may be due to a past change in the opposing direction. There are advantages, too, and I hope, the ideas from the paper may work, somehow.\n\nChernoff Bound\n\nThe whole implementation is in Fig. 2. I rewrote it for myself as\nscoreOfGreater += (isHit == isGreaterPart) ? +1 : -1;\n++total;\nif (scoreOfGreater * scoreOfGreater > someConstant * total) {\n  param += (scoreOfGreater > 0) ? +stepSize : -stepSize;\n  scoreOfGreater = total = 0;\n}\nIt's trivial and nicely eliminates the sample size threshold (at the expense of someConstant).\n\ntwo ghost caches\n\nThe split cache does about the same for free. It's not exactly the same and it may introduce some bias due to how it gets split. As you wrote in the email, there may be a problem when one partition is much bigger than the other. There may be an even worse problem, when one partition is much luckier than the other (a few frequently hit entries could possible cause this). The ghost caches don't suffer such problems.\nA funny idea: Inside of the real cache, we can approximately simulate a slightly smaller cache by simply marking entries which would get lost if the cache was smaller (I hope, we can). Imagine one cache having the same LRU size and a slightly smaller LFU size. Imagine a second cache which is smaller in the other way. We could simulate both caches and by comparing their performances determine which part should grow.. IIUIC the usefulness of the smarter algorithms grows with the dimensionality of the problem. I'm not sure how many dimensions your problem has, but it's surely nothing like neuronal networks, where the Ada* algorithms excel. I guess, the efficiency of the smarter algorithms gets partly lost due to noise and caching performance is IMHO pretty noisy.\nThere's also some \"inertia\" (unrelated to the momentum used by the optimizers) due to the cache content being dependent on the previous settings (IIRC this is what killed my ideas as I played with it last year).\n. > The biggest win would be to review LocalCacheFactory features to determine if they carry their weight. Perhaps that factory should be removed and all the fields present, but null'd if unused. The top-level cost is smaller than per-node, which was my original intent for using codegen.\nI could imagine, that keeping the small factories small makes sense, but for example, WISMW has tons of fields. Adding all subclass fields to it would only grow it by a few percent. So you could generate only WISMWAWR (with some runtime tests) and use it in place of WISMW  and all its subclasses. I might be using wrong names as without being really familiar with the code, they're hard to tell apart.\nBut such optimizations are ugly. For it, to be efficient, you may have to reorder the hierarchy so that the heaviest additions come first. Let's say, you deal with evicts first. Any evicting factory is already huge and you can add all fields and save half the hierarchy. Adapting the generator could be rather easy (nice code!), but changing the hierarchy could be confusing and the reward unsure.\nThe size optimizations for nodes are clearly much more important and in the long run, you'll probably need something smart as the code size doubles with every new feature. A funny optimization would be to declare field1,  field2, etc. (so that classes would differ only by their count), and use them via methods using offsets, but for this is even Unsafe too much high-level (and the fields are of different types).\n. You could allocate a single bean instead of the many arrays. Assuming, you declare a single class for all usage places, some fields would be left unused somewhere, but the total number of allocated bytes could still be lower than when using arrays. The code would be shorter, maybe even nicer, but that's a matter of taste.. @Valloric \n\nThis is of course because of a design mistake in Java where equals() on arrays is the same as ==\n\nI wouldn't call it a mistake. Arrays are mutable and for mutable objects, identity equals makes more sense. Though I agree that content-based equals would be handy quite often.\n\nyou might want to at least throw an exception from build() if the key is an array\n\nThere are legitimate uses for arrays compared by identity. I can't recall a real use case, nor can I construct a realistic example, but some heavily optimized code can need them.. > Perhaps a better solution would be to return from each case statement?\nAgreed. An enum-switch without a default clause is more future-proof as you'll get a warning when a new enum member gets created. That's actually the reason for UnnecessaryDefaultInEnumSwitch.\nShouldn't the IllegalStateException be an IllegalArgumentException (it does depend on the input)?\nIt gets thrown and immediately caught, that's a bit strange, but tuning something what can't really happen doesn't make much sense.. > This avoids unnecessary blocking due to hash collisions,\nI'd add that with striping, you may get a deadlock easily, ever for completely unrelated objects as they may get hashed onto conflicting locks. Good luck debugging. ;)\n\nbut I've never seen it worthwhile in practice.\n\nI wonder when the cost matters? The code is even simpler than yours, there's one cache lookup and a Lock waiting for the GC.... > If the locks are reentrant then usually its not a big deal.\nDeadlock is improbable, but easily possible. Let thread 1 lock A, thread 2 B, 1 C and 2 D, possibly with completely unrelated locks and let both A and D map to x while B and C map to y. The bad thing is that you have zero control. With 16 stripes, the probability is about 1/256.\nWith Guava Striped, you can't do anything about it as the mapping isn't  exposed. So even if you knew upfront what locks are needed, you can not order them properly.\nI needed locking for whole HTTP requests (where the cost really does not matter) and I went for LoadingCache with weak values (as you wrote above). There are multiple locks, I know in advance which ones are needed and sort them before acquiring. It's very simple and hopefully correct.. > Sometimes striping is used for work distribution, e.g. see this StackOverflow thread.\nI really like your answer. This separation of concern is perfectly obvious, once someone mentions it. And the Java 8 (nearly) one-liner shown me that there are still quite a few Java 8 I haven't learned yet.\n\nBut some, like you, do it right :)\n\n:) I was thinking about an optimization which I may need in a few years :D When A, B, C should be locked, I'd (really) lock A, try-lock B and on success try-lock C. On failure, I'd release all locks, so that other threads won't be blocked needlessly. Then I'd (really) lock the failed one and release it again, so that I know when to retry. Now, I'd retry the whole sequence again.\n\nI have to detect deadlocks\n\nOnce I wondered why Java does not provide it, but I haven't got a satisfactory answer. Actually, I think that the complexity is something like O(M) where M is the length of the actual cycle. My reason is that, at the time of a deadlock, the cycle has been already found and you only need to verify it. The real cost is the bookkeeping and using tryLock instead of lock (no idea by how much it's more costly).\nI guess, with using tryLock in places where the deadlock may occur, you can get away without much overhead. Only when the tryLock fails, you need to start worrying about what's going on. I can't see how to get the whole lock graph from ReentrantLock, but I guess, I'm missing something (and you could always add your own tracking).\nAre you going to do proper lock detection? Maybe you could just rollback and retry when it looks like a deadlock? Maybe you could use the information what rows were needed and lock them upfront?. There are some minor problems:\nif (value == null) throw new NullPointerException();\nObvious.\nmap.put(key, value);\nThis would make the next next() throw, but it can be fixed.\n\nWhat's bothers me is the \"unnecessary\" table access, but setKey is not that common.\nIn case of getValue, a similar situation occurs:\n{\n    final Map<Key, String> map = new HashMap<>();\n\n    for (int i = 0; i < 10; ++i) map.put(new Key(), \"A\");\n    final Key favoriteKey = new Key();\n    map.put(favoriteKey, \"A\");\n\n    Map.Entry<Key, String> favoriteEntry = null;\n\n    for (final Iterator<Map.Entry<Key, String>> iter = map.entrySet().iterator(); iter.hasNext();) {\n        final Map.Entry<Key, String> entry = iter.next();\n        if (entry.getKey().equals(favoriteKey)) {\n            favoriteEntry = entry;\n        } else {\n            iter.remove();\n        }\n    }\n\n    for (final Map.Entry<Key, String> e : map.entrySet()) e.setValue(\"B\");\n\n    // just to be sure\n    map.put(favoriteEntry.getKey(), \"C\");\n\n    System.out.println(favoriteEntry.getValue()); // still \"A\"\n}\n\nThe Javadoc for Map.Entry#getValue says \"If the mapping has been removed from the backing map (by the iterator's remove operation), the results of this call are undefined\". There are two iterators in my test, but neither has removed my favorite mapping.\nI guess, there's no solution. Calling map.get(entry.getValue()) would sacrifice too much efficiency for a fairly strange usage. Explaining this usage in the Javadoc would probably confuse everyone.\n\nI guess, insertions replacing regular nodes by tree nodes can break the entry the same way.\n. @ben-manes But an entry is neither a view into the map (as my example shows), nor a stable snapshot (just replace the 10 in my example by a smaller value). Speaking about  \"some point at or since the creation of the iterator\" allows both kinds of behavior and even its mix.\nIn the javadoc for Map.Entry#getValue, the first part of \"If the mapping has been removed from the backing map (by the iterator's remove operation), the results of this call are undefined\". could be removed. :D\nIt should be explicitly stated, that any change to the map since this entry was returned, makes the result of getValue undefined. It seems to be hardly more than a syntactic anti-sugar for the lack of for (K k, V v: map) {...}.. I guess, I understand the OP. They problem is not that the notification happens later, but rather that after the notification containing value1, the same value1 is still present. The test case in pseudocode could be like\ncache.put(key1, value1);\ncache.addListener(new Listener() {\n    onRemoval(K key1, V value1, Cause cause) {\n        assertTrue(value1 != cache.getIfPresent(key1));\n    }\n}\ncauseEviction();\nNo idea if this really happens, but I'd bet, I'm interpreting it right as he speaks about recycling resources.. Ben, I surely agree with you that eviction policy should not depend on the business logic. Moreover, the program correctness definitely should not depend on the cache policy or size or alike.\nHowever, I have a (possibly very) similar use case. Some entries must be guaranteed to be available until some event happens. I could store them somewhere in a small Map, but then I'd have to do two lookups, which doesn't sound right.\nSomething like\nCaffeine.newBuilder()\n    .maximumSize(...)\n    ...\n    .preventEvictionIf((k, v) -> isStillNeeded(k, v))\ncould be the perfect solution (probably also for the OP). Returning true would veto the eviction for now. It would bump the entry a bit higher (so that the situation won't repeat immediately), but I don't care about how exactly as this is needed for only a small fraction of entries. It's not about changing the policy, it's about (temporarily) exempting some entries from the policy.\n. - over capacity - This won't matter in my case because of having just a few pinned entries. Moreover, it'd save me the map, so it'd cost less memory in total. I'm not sure, if the pinned entries should contribute to the total weight or not.\n- penalty of the scan - I'd surely be rejecting the candidate only rarely, but I can imagine how it could go wrong. However, you'd run in a similar problem if the RemovalListener would re-insert the entries (or with another naive racy pinning attempt).\n- weight=0 - But the weight is \"effectively static during the lifetime of a cache entry\", which is a problem. Such an entry won't ever get removed, right? I'd have to remove it manually, which would probably be better than additionally accessing a map, but it's still rather ugly. Actually, I don't wan't to remove the entry, just unpin it, so it gets managed normally.\n- Loading from the map - I have to think about it.\nWould something like .pinForMillis((k, v) -> v.isSpecial() ? 1000 : 0) work better? It could use the time wheel logic and respect or ignore the weight of the pinned entry (whatever is easier to implement). It could go wrong with pinning many entries for a very short time interval, but then it's (more) obviously a user's fault.\nOr maybe being explicit with cache.pin(k) and cache.unpin(k)? Or putPinned(k, v) or alike.\n. > In effect use a compute to pin and unpin.\nThank you. I guess, this would work perfectly for me. I'm unsure, if also for the OP. If not, apologies for hijacking their issue.. I may be talking nonsense, but no risk no fun, so bare with me.\n\nIn some case scenarios we may have up to 3 cache hits (2 reads and 1 write). I don't know if 3 hits like this right away might be promoting entries preemptively.\n\nThe original bug report reports the pattern MISS-HIT-HIT-HIT when it works correctly. This mean that there are three hits to the wanted entry. When infinispan also accesses the entry three times, possibly much later, then it theoretically makes sense to keep the old entry instead of replacing it as it also leads to three hits.\nThis doesn't explain the difference between count and size based eviction. However, it might explain how the late access by infinispan can keep an entry needlessly. I guess that for this access, it doesn't matter if it hits or misses, so getIfPresentQuiet may solve the problem.\nOn the second though, it's probably a nonsense. The cache is big enough for 100 entries, so even when the region is just 1%, the most recently read entry must be kept for the next three reads.. I really can't rate their algorithm, but I find the idea of including the size valuable. When you just count the misses, then evicting bigger entries earlier definitely makes sense (unless the bigger entry is much more probable to cause a hit). With growing size ratio, this gets more and more important.\nHowever, just counting the number of misses needn't be enough.  It reminds me of the 0-1 knapsack problem as not only the size (weight) of an entry can vary, but also its value. Actually, all caches I know about, solve the problem assuming v_i=1 while maintaining sum w_i * x_i <= W, but without using the weights for anything else. Or maybe they optimize assuming v_i = w_i?\nIn some cases, the user could provide the value of an entry, e.g., as the time it took to generate it.\nAs a real example, I'm caching the JSON responses of my server. Their sizes vary a lot and therefore I choose to not cache responses above a threshold. The time their generation needs is strongly correlated with their size, but some queries are clearly over-proportionally expensive.\n. I surely agree that the cost function is non-trivial. The load-time can be measured exactly, but it depends on the current load and the cost may get overrated a lot. Such entries could be then kept in the cache far too long. No idea, if this really happen, but some sanity check can be devised (in my use case, maybe classifying the requests according to the pathInfo, computing linear regression per category and limiting the cost for outliers).\nI could also run experiments with different cost functions, but currently, the load is too low for this. Anyway, I guess, I can get a useful cost function at the end.\n\nSince they don't retain history and their published data, I think it can only match a classic policy.\n\nIIUIC for distributions, where it doesn't badly misbehave (like LRU failed by a sequential scan) and using v_i=1, it can easily win against any algorithm ignoring the weights when the weights vary a lot. The other thing is that the assumption v_1=1 is unrealistic.\nI guess, with v_i=w_i, ignoring the weights in the policy is just right.\n\nTinyLFU's idea of using a sketch and aging it could be adapted as storage (vs filter), instead of retaining the per-entry counters.\n\nSorry, I don't understand.\n. Your confusion is surely my fault. I'll try to imprecisely rephrase both problems so that my thinking gets clarified.\n\nThe 0-1 knapsack is about packing of items, where each item has a value v_i and weight w_i.\nThe total weight of included items is subject to an upper bound.\nThe total value of included items is to be maximized.\n\nIt's deterministic and the solution is just a bit vector.\n\n\nThe caching problem is about packing of items, where each item v_i has a value (\"cost of a miss\" or equivalently, \"saved cost on a hit\") and weight w_i.\n\nThe total weight of included items is subject to an upper bound.\nThe total value of hit items is to be maximized.\nIt's probabilistic and the solution is a strategy.\n\nSo I see the caching problem as a \"dynamic knapsack\" where items get inserted and excluded on every step. It's a far-fetched analogy, but the meaning of v_i and w_i matches well.\nCaffeine has w_i but no v_i as its input. For the caching problem as I described it, it works well when v_i = w_i.\nWhen weights and values and also v_i / w_i vary a lot, then an algorithm taking them into account has a unfair advantage.\nThank you for your answer and the links, I'll answer later (this will take quite some time).\n. > Where it differs is that recency and frequency are additional variables, and their impact vary across workloads. Would you consider that to be part of an equation to compute v_i?\nNo, for me v_i is an input to be set by the user. It's the degree of happiness :D the user says they gain every time, the item i is wanted and present. Recency and frequency are heuristics you use to maximize the gain and you may want to use an internal v_i' which takes them into account and is subject to aging and other adjustments. For the success evaluation, the user-supplied v_i should be used as is.\n\naging to smooth out the function (e.g. if load time was slow due to a temporary network timeout).\n\nIn theory, this should be the user's responsibility as it's the input, a parameter of the target function rather than a heuristic. OTOH preventing users from shooting themselves in the feet can be important and aging can be a simple solution. I guess, I'd use some faster aging initially and I'd like to switch to a slower aging once I can handle outliers well enough.\nActually, when the user wishes aging, then it's a different target function.\nCurrently, I don't have time to read the papers or to think seriously about it all. :cry: \n. >  If a typical web application with lots of ad hoc caches, then it won't be worth it.\n\nI typically avoid soft references and view it as a last resort. It can cause hard to diagnose performance problems if unbounded\n\n@ben-manes In case of many caches, it's unclear, how to optimally size them. Assuming all of them are about equally important and use somehow compatible settings, it'd be probably best to sort of join them in a single big cache. I wonder, if something like\nSharedWeightLimit limit = Caffeine.sharedWeightLimit(1000);\nCache<SomeKey, SomeValue> cache1 = Caffeine.weighter(...).maximumWeight(limit).build();\nLoadingCache<AnotherKey, AnotherValue> cache2 = Caffeine.weighter(...).maximumWeight(limit).build(key -> createValue(key));\nmakes sense to you?. > might want to take into account the hit rate, load penalty, popularity, etc. of the caches\nThis can get arbitrarily complicated and you may spend years tuning the settings... My idea was actually the opposite: Inside of a cache, there's a clear policy telling us which entry is more important. So, I'd apply the same idea across caches: The very same algorithm should be used for comparing entries from different caches.\nA very hacky implementation for two caches having the same settings would be just:\nCache<Object, Object> implCache = Caffeine.weighter(...).maximumWeight(limit).build();\nCache<String, Integer> cache1 = new Cache<>() {\n    public Integer get(String key) {\n        return (Integer) implCache.get(composeKey(key, 1));\n    }\n    ...\n};\nCache<Whatever, Whatever> cache2 = new Cache<>() {...};\nYes, I mean put them all in a single cache and let them all compete against each other. As long as their settings are the same (or somehow compatible), all what's needed are some facades. With different settings, some support inside of the cache would be needed (I can't see how much and if it could work well).\nThis is assuming that all caches are equally important. Otherwise, we could use something like my old knapsack analogy, giving not only weight but also value to entries.\n\nYou're welcome to try =)\n\nI'd gladly would (caching is one of the most interesting topics for me), but somehow, I'm always short of time.... I can recall, some long time ago I wanted to continue playing with the simulator.\n. > Once expireAfterWrite trigger is on - the session object is evicted, but the session itself (i.e. session last accessed time - creation time is lower than its timeout) is still alive. so that it has to stay in cache.\nShouldn't you use expireAfterRead instead?\n\nMay be it is meaningful to introduce such a functionality with exceptional state in the docs that its usage brakes contract and use it at own risk somehow\n\nThe contract could be refined as the overhead being O(1 + numberOfManuallyRefusedEvictions) and this value could be reported in the stats. This doesn't sound as good as O(1), but it's exactly the same for everyone not using the callback. Some time ago, I also though I'd need it, but in the end, I did something different.\n. nanoTime() has nanosecond resolution, but a much worse precision. And even if it had a precision of a nanosecond, you can guarantee nothing even remotely close to it for the expiration. So I guess, you can steal the least significant bit (or two). The computational overhead for the needed bit fiddling is IMHO much lower than any alternative. The implementation overhead and risks should be pretty small, too, as it concerns a single field only, which is used in pretty few places.\n. ",
    "johnou": "@ben-manes the fix isn't included in 8u60? http://www.oracle.com/technetwork/java/javase/2col/8u60-bugfixes-2620228.html\n. Wasn't trying to break balls, but had to confirm since one of our application relies heavily on FJP.\n. @ben-manes would you please share the reproducer? \n. \nseems that once the FJP gets into the bad state no further tasks are executed (not sure if it recovers yet, only just started poking about).\n``` java\n        System.out.println(\"scheduling drainBuffersTask\");\n    CompletableFuture<Boolean> future = new CompletableFuture<>();\n    executor().execute(() -> {\n      System.out.println(\"invoking performCleanUp\");\n      future.complete(true);\n      performCleanUp();\n    });\n    try {\n      future.get(3000, TimeUnit.MILLISECONDS);\n    } catch (TimeoutException e) {\n      System.out.println(\"lock detected\");\n      executor().execute(() -> {\n        System.out.println(\"never invoked\");\n      });\n    }\n\n```\n. @wimdeblauwe could you see this being used as the basis of a sync distributed cache? \n. @wimdeblauwe I am in search of an atomic / strongly consistent distributed or replicated (consume more memory less cost for lookup / increased redundancy?) cache for an actor system used as the activation directory.\n. I'm sure @ben-manes checked this too but I thought it would be worth adding a comment. I checked out master, built the jar locally and used javap -l to verify debug symbols are still present in the core classes.. @nitsanw unfortunately the gradle shadow library doesn't support minimizeJar https://github.com/johnrengelman/shadow/issues/115\n. @ben-manes you could also relocate JCTools to an \"internal\" package, nice warning for developers.\n. Hahaha several years ago when auto-completion was taking off I ran into some really exotic imports in code reviews :)\n. @normanuber awesome work!\n. http://stackoverflow.com/questions/34413/why-am-i-getting-a-noclassdeffounderror-in-java\njava.lang.NoClassDefFoundError This exception indicates that the JVM looked in its internal class definition data structure for the definition of a class and did not find it. This is different than saying that it could not be loaded from the classpath. Usually this indicates that we previously attempted to load a class from the classpath, but it failed for some reason - now we're trying to use the class again (and thus need to load it, since it failed last time), but we're not even going to try to load it, because we failed loading it earlier (and reasonably suspect that we would fail again). The earlier failure could be a ClassNotFoundException or an ExceptionInInitializerError (indicating a failure in the static initialization block) or any number of other problems. The point is, a NoClassDefFoundError is not necessarily a classpath problem.\n@wclaeys can you see any exceptions before the NoClassDefFoundError?\n. @ben-manes you bet I am, always welcome to reach out to me at johno.crawford@gmail.com. @ben-manes did you ever hear back from Doug about the tickless hierarchical timing wheel in Java?. @levischuckeats any OOMs or other exceptions logged? . @agoston what JVM options? Are you monitoring your heap? Could an OOM have occurred?. Can you dump the JVM options here so we can run the Stresser with them?. Discussion for Netty project can be found here https://github.com/netty/netty/pull/7245. @ben-manes couldn't we ask them to back-port the fix to JDK8 before it's EOL?. @ben-manes are you sure?\nhttps://bugs.java.com/view_bug.do?bug_id=8162795 states\nRedoing MemberNameTable changes as per 8152271.\nIntended for jdk9 and jdk8, while JDK-8174749 will introduce a better solution for jdk10.\nI would stick with method handles as reflection can produce scary warnings with jdk9... @ben-manes we can always go back to method handles if they backport the fix, don't let my concerns on the issue hold you from releasing :+1: . @ben-manes https://github.com/brettwooldridge/HikariCP/issues/835 and https://github.com/brettwooldridge/HikariCP/commit/f75b0896de6ca075e1b0d2f4b3c792cf39b77643#diff-0dac8ca2574085e5192a8188ca22d42e might be interesting.\n. looks like a non-issue in the latest Caffeine, I only see usages of yield in tests, @520CHINA please update, latest version is 2.6.2.. ",
    "oobles": "Thanks for the fast response! It took me a bit of time to digest what you were saying, but I've been able to do as you suggest.  I'd prefer if the Cache library provided a difference between invalidated (just remove from cache) versus removed/deleted (with intention to delete from underlying store) .  However, I understand why you wouldn't want to.\nI've been able to get the desired outcome by using the write back to store on expiration (so mutated state is captured).  Also wrote a custom flush command which removes required elements from the cache and writes mutated state back to underlying store.  In effect as you suggested going past the cache when required.  Other than expired items from cache (which are written to store) all other items explicitly removed from the cache are ignored.\n. Sorry..  stupid error in a test case I was looking at all day while trying to resolve race condition. :(\n. Thanks for the fast response.  I had just found the asMap() and was implementing and about to come back and close this. :)  If I return null from computeIfPresent and the value is discarded will the removal listener get called?. Thanks! Just what I was hoping.. ",
    "oakad": "Thank you for a swift reply.\nWe are running openjdk 1.8.0_71-b15 here, so we should be fine (?) on this account.\nI will get v 2.3.0 and try it out.\n. After doing a quick run I can confirm that with 2.3 a problem would not manifest itself, at least for a time being.\nThank you for your help.\n. Suddenly I realized than issue #70 talks about the same thing, so sorry for double posting.\nCould you kindly point me to the description of \"variable expiration\" feature (if it exists in some written form, that is)? I have looked around but could not quite establish what exactly you mean by that.. Thank you, most interesting.. This is most impressive (TimerWheel in particular).\nThank you for your effort.\n. ",
    "nikolavp": "I can confirm that the asMap did the trick for us. It is actually written in https://github.com/ben-manes/caffeine/wiki/Population#manual (the last sentence).\n. A secondary victim cache worked. What bothers me a little is that it is unbounded and the system can run out of memory. Thankfully the special case isn't hit that often so it should be OK.\nProbably this is for closing.\nNB: I learned a lot by just opening and looking into closed issues, this no how should probably be ported to the wiki some how ;)\n. ",
    "ashish0x90": "Hi Ben,\nThanks for the prompt reply full of interesting details :). Also thanks for running comparision tests with other alternatives and sharing the results. Glad to see the new numbers. Yup you're right, seems like shared hash indeed was the issue. \nI also agree with Maaartinus, IMHO better to keep BloomFilter capacity immutable. If we need a bigger bloomFilter later, can easily create a new bloomFilter with new capacity instead of resizing existing bloomFilter (we lose the existing entries anyway as part of resize process) unless there is a good performance reason I might be missing here? Anyhow, if this becomes package private than it doesn't really matter.\n. > The padding is tricky because it depends on the machine.\n@ben-manes Assuming that the primary purpose of padding fields here is to avoid false sharing, did you also try sun.misc.Contended? \n. ",
    "senderista": "Hi, I'm barely familiar with the Caffeine code, but I noticed a couple of things in this fascinating discussion:\n\n\nClearing the doorkeeper bloom filter all at once creates spurious \"one-hit wonders\", as you noticed. You could address this problem by making the aging process more gradual. There are a bunch of time-decaying bloom filter variants around, but perhaps the simplest aging bloom filter is this implementation: http://ieeexplore.ieee.org/abstract/document/5066970/ (I know, I know, I have a copy if you want it.) The algorithm in this paper is very simple:\nif x in cache1:\n  result = true\nelse:\n  if x in cache2:\n    result = true\n  else:\n    result = false\n  cache1.add(x)\n  if cache1.full():\n    cache2.flush()\n    swap(cache1, cache2)\n    cache1.add(x)\nreturn result\n\n\nSince you don't really care about exact frequency counts, I wonder if you need frequencies for all cache entries at all. What if you just maintained a top-k heavy hitter structure (Space-Saving is probably the simplest), and only made frequency-based eviction decisions based on membership in this structure? (You have to choose k of course, but that can be changed pretty easily on the fly with the Stream-Summary data structure, so you might be able to incorporate that parameter into an online optimization algorithm like a hill climber.) \n\ufffc\ufffc\ufffc\ufffc\ufffc\ufffc\nAnyway, as I said I haven't studied the Caffeine code in much detail, so I apologize if these suggestions don't make sense in this context.. The slides are a nice intro, thanks!\n\n\nSomething I forgot to mention is that you can incorporate exponential decay into the Space-Saving algorithm in a very natural way: https://pdfs.semanticscholar.org/8e44/278c1da454600e88be3065130fbac4360806.pdf\n\nOur algorithm, a modified version of the \u201cSpace-saving\u201d\nalgorithm, tracks a set of O(1) pairs of item names and\ncounters, with the counters initialized to zero. For each item x_i\nin the stream, we see if there is currently an (item, counter)\npair for that item. If so, we update the quantity of w_i exp(\u03bbt_i),\nand add this to the counter associated with x_i. Otherwise, we\nadd the same quantity, w_i exp(\u03bbt_i), to the smallest counter\n(breaking ties arbitrarily), and set the item associated with the\ncounter to x_i. Pseudo-code is in Figure 1. To find the heavy\nhitters, visit each item stored in the data structure, item[i],\nestimate its decayed weight at time t as exp(\u2212\u03bbt) count[i],\nand output item[i] if this is above \u03c6D.\nInput: item x_i, timestamp t_i, weight w_i, decay factor \u03bb\nOutput: Current estimate of item weight\nif \u2203j. item[j] = x_i;\n  then j \u2190 item^{\u22121}(x_i)\n  else j \u2190 arg min_k(count[k]);\nitem[j] \u2190 x_i;\ncount[j] \u2190 count[j] + w_i exp(\u03bbt_i)\nreturn (count[j] \u2217 exp(\u2212\u03bbt_i))\nThe decay constant \u03bb (half-life: ln(2)/\u03bb) is another parameter to guess, of course, but maybe it could be learned online as well.. \n",
    "cowwoc": "Are you aware of an existing implementation? I'm trying to fix a bug in my own library. I'm not really interested in implementing this myself (it's out of scope of what I'm trying to do).\n. @ben-manes My use-case is as follows: https://bitbucket.org/cowwoc/guava-jdk8/issues/1/toimmutablemap-does-not-keep-iteration\nA user wants me to convert Stream entries into an ImmutableMap where the iteration order of the resulting map is equal to the insertion order of the input elements. Somewhere down the line, I decided that I wanted this to be thread-safe but thinking about it now: does it really make sense to talk about iteration order in this context of multiple writers? I mean, how could someone guarantee a meaningful insertion order on the input-end without using locks themselves?\nSome people on Stackoverflow are even writing that the problem is technically unsolvable: that you must use locks in order to provide insertion iteration order.\nAnyway, I think I will end up doing the following:\n- If the Stream is sequential, retain insertion order without providing thread-safety.\n- If the Stream is parallel, provide thread-safety without guaranteeing the iteration order.\n. Fortunately, I just noticed that https://docs.oracle.com/javase/tutorial/collections/streams/parallelism.html states that:\n\nThe Java runtime performs a concurrent reduction if all of the the following are true for a particular pipeline that contains the collect operation:\n- The stream is parallel.\n- The parameter of the collect operation, the collector, has the characteristic Collector.Characteristics.CONCURRENT. To determine the characteristics of a collector, invoke the Collector.characteristics method.\n- Either the stream is unordered, or the collector has the characteristic Collector.Characteristics.UNORDERED. To ensure that the stream is unordered, invoke the BaseStream.unordered operation\n\nNotice: there is no such thing as concurrent ordered collectors. If the collector is CONCURRENT then it must also be UNORDERED. Yay for me :) I guess that solves that problem.\n. ",
    "eiden": "It's not super urgent for me to get a new release. I have created a workaround for now. Thanks for the help :)\n. Thanks for the system properties tip. I'm not so familiar with the typesafe config library. For now I have just added an application.conf to my resources directory.\nI agree that the need to have multiple root config files is probably small. I don't have the need for multiple config files myself. \nBut it would be convenient if I could use the hibernate config hibernate.javax.cache.uri to point to the config file. And that would make caffeine behave the same way as the ehcache-jcache implementation. \nI guess that most users trying to integrate hibernate and caffeine via jcache would be a bit surprised that they cannot configure the cache as described in the hibernate docs..\nThanks for the prompt feedback, looking forward to some more caffeine testing. Feel free to close this issue.\n. ",
    "sebersole": "Well to be fair, I have reached out to you a number of times for you input on the configurability and just never heard anything back ;)\nTBH such a JCache-specific  discussion is beyond my scope of knowledge wrt to JCache.  If there is some specific changes you'd like to see in the Hibernate documentation it would be better to propose some the specific changes you'd like to see.\n. ",
    "theangrydev": "As you pointed out, this kind of thing only really makes sense for ordered computations, such as a tree structure that shares subtrees.\nI didn't spot the JavaDoc, it could be worth adding your comments about ordered computations there to help point anyone else that stumbles across this kind of problem in the right direction.\nYour tip about using\u00a0AsyncLoadingCache and its synchronous() view looks promising, I'll see how that goes for my problem. If I understand you correctly, are you saying that solution is deadlock free if I can guarantee that there are no unordered computes?\n. Great, thanks for your comments, it's nice to see a library maintained by someone who clearly knows their domain very well!\n. ",
    "protogenes": "I just encountered this problem when looking for a replacement because of google/guava#1881\nI assume you need an unbounded executor for the asynchronous workaround to operate?\nPlease consider to document this behaviour more prominently.\nAn attempt to perform a recursive should not lead into an infinite loop within computeIfAbsent, but throw an exception if it is not supported.. ",
    "ghost": "Just realised I can do:\nasyncLoadingCache.synchronous().estimatedSize()\n. ",
    "wimdeblauwe": "@johnou Can you elaborate on what you mean exactly?\n. I like Hazelcast as a distributed cache. Very easy to setup, nice API. On the commercial side, I have used Oracle Coherence as well. Also good.\n. ",
    "kdempster": "Thanks for the quick reply.  I am not sure of what the problem was, but by forcing the classloader to load the RemoveCause enum in the class that i defined the cache seemed to do the trick.  I simply logged the name of the enum.\n. ",
    "hshankar": "So, my use case is to be able to set custom weights on each entry such that the entries with lower weights are removed first. LRU is a special case where the custom weight is the time of creation of the entry. \nThe problem with expireAfterWrite is that it assumes a time (= current time). I want to set a different time on which to base eviction.\n. I am not sure I understand your confusion. Here is a use case:\nThe records I store in the cache have  a \"createdDate\" of their own. This is not the creation date of the entry in the cache, but createdDate of the record itself. I want to evict the records with oldest createdDate instead of the oldest entry in the cache. The records can arrive out of order. So if we say record Cx has created time x, I have a use case where we get C10, C20, C30 and then suddenly get a flood of C11, C12 etc. In this case, I want C10, C11, C12 to be evicted before C30.\n. I need more deterministic behavior than regular caching. In my use case, I have enough RAM to not worry about evictions at all. As long as I can guarantee that all entries with creationDate > some time (say currentTime - T) are in the cache. So it is like FIFO but with custom time instead of insertion time to determine order. And the weight is static. I am actually not aware of caffeine or guava cache internals on how they manage evictions so I was not sure how easy/hard it would be to support it.\n. ",
    "GuillermoBlasco": "Since we are using the application cache just to cache read only entities the issue is just limited to caching collections of those entities that is the modifiable stuff. Our problem is solved, because it was limited to 10 lines more or less. Easy to solve. So it is not a need, just a \"I think it would be good\".\nBut, sincerely, I feel more confortable reading a copy of the value rather than getting the reference to the original object. And yes, deep copy by serialisation is costly, but it depends on the objects you know you are caching if they require deep copy or not (for example, not in my case).\nIf I found further problems I'll move to JCache backed by Caffeine, yes, that was the idea.\nThanks for the support Ben ;)\n. ",
    "jmiguelsamper": "Hello,\nI'm trying to achieve what the original reported described, particularly getting a new instance of the stored object on every retrieval. However, I'm getting always the same stored instance.\nI've added a debug message with a dummy Copier and it seems that the Copier object is not invoked on every read.\nThis is my test configuration:\n    CaffeineConfiguration<String, Object> config = new CaffeineConfiguration<>();\n    config.setStoreByValue(true);\n    Factory<Copier> copierFactory = (Factory) new FactoryBuilder.SingletonFactory<>(new Copier() {\n        @Override\n        public <T> T copy(T t, ClassLoader classLoader) {\n            LOG.debug(\"Copier invoked for {}\", t.getClass().getName());\n            return t;\n        }\n    });\n    config.setCopierFactory(copierFactory);\n\nOf course, this copier does not copy anything, its purpose is just debugging.\nCould you confirm if the Copier should be invoked on every read or if I understood that wrong?\nThanks in advance and thanks for this great project.\n. Hello Ben,\nThank you very much for your response, that has pointed me on the right direction.\nYou're completely correct, Caffeine is \"copying on read\" as expected. My problem seems to be related to the Spring configuration (Spring 5 + JCache) because the objects are not copied when Caffeine is used through Spring. But it works perfectly when Caffeine caches are invoked directly.\nI'll post the solution when/if I got it.\n. I don't know the underlying cause but it seems that Spring cache synchronization (https://docs.spring.io/spring/docs/current/spring-framework-reference/integration.html#cache-annotations-cacheable-synchronized) prevents cache elements to be copied on read.\nDisabling synchronization makes it work as expected.\nSo, when I remove the sync=true on\n@Cacheable(cacheNames=\"foos\", sync=true)\n\nvalues are copied on read.\n. I tried to follow the source code to provide a better explanation but I got lost (Spring is all about proxies and decorators), sorry about that and thank you very much again for your help.. Ok, thank you very much for the additional information.\nRemoving sync works for us but if that missing copy is added in future versions we will be able to enable it again.. ",
    "korri123": "Thanks, I hadn't thought of the concurrency implications.\n. ",
    "Spikhalskiy": "We already use last 2.3.1 now. Ok, will let you know if we have any updates.\n. Looks like reason of this issue is absolutely different. We modified key sometimes after inserting it into cache. Looks like after that caffeine could mark it as dead and eligible for cleanup but can't actually cleanup it. Maybe because it makes lookup in second case, can't find element and do nothing. Didn't take a look deeply, because main issue is clear.\n. @ben-manes I think it doesn't make a lot of difference how fast we will get an OOM :) It would be much better to fail or at least print loud logs in we can't remove dead entity from map. By current design looks like it could be hard. On the same time, I see an option to add additional flag like \"deleted\" to key and setup it even if we didn't find entity in the map (consider it already deleted for now) and make something if we find \"deleted\" entity on any list traversing after.\n. ",
    "phraktle": "Thanks for the fast response! It was easy enough to record a trace (just dumping hashes on gets in a file) and simulating it. I'm closing this ticket and will open another one with the trace to see if it can be optimized.\n. I will submit a pull request shortly.\n. Interesting indeed. The workload is product recommendation lookups for several hundred e-commerce sites of varying sizes worldwide. I don't currently have an intuition as to why frequency would be a negative signal here (I would have assumed popular products should cache well based on frequency).\nAny options to configure Caffeine to use an LRU policy?\n. There's no prior filtering or heuristic, it's just a straightforward LoadingCache.\n. Here's a longer trace for the same database (~1 hour period): http://download.scarabresearch.com/cache-traces/recs.trace.20160808T073231Z.xz\n(There's also a trace of accesses to a somewhat different, but related database for the same timeframe: http://download.scarabresearch.com/cache-traces/prods.trace.20160808T073231Z.xz)\n. Indeed, the differences are not dramatic \u2013 depending on the time period / exact workload Caffeine tends to stay within being less than 5% worse than an LRU strategy on this workload.\nI hope the trace gives you some ideas for potential improvements, or perhaps justifies adding an option to switch the strategy to plain LRU. But if not, I'm okay with closing this issue :)\n. A naive question: could some kind of simple sampling approach help pick the right caching policy? What I mean is having two pools with different strategies (eg LRU and LFU), split requests between them randomly and pick the one with better hit rate metrics. . Thanks @ben-manes, the MiniSim paper was interesting!. No rush on my account @ben-manes, at this point I just have an intellectual interest in this problem as I\u2019m no longer involved with the project where this came up :). Just an off-topic remark: wow @ben-manes :) Beyond building the best caching library, your level of attentiveness is exemplary. Thanks for your efforts!. ",
    "erikvanoosten": "This paper claims an algorithm that automatically adapts to the workload. In this trace it would fall back to behave similar to LRU.\nhttp://www.cs.cmu.edu/~beckmann/publications/papers/2018.nsdi.lhd.pdf. ",
    "geoand": "You're welcome!\n. ",
    "jvassev": "Have a look at https://github.com/ben-manes/caffeine/pull/199\nThe trick is to collapse multiple responsibilities (node+factory) into the same class to spare us the metadata bloat. Also, after the enum is gone there is not need to instantiate all possible factories in advance.. Last effort: https://github.com/ben-manes/caffeine/pull/200\nIf we use non-nested classes that's extra 49KB in savings (shorter FQN in the classfiles).. I think at this point only a minifier/obfuscator can help. The only low-hanging fruit left is to put the generated classes in a package with a shorter name but I wouldn't like that (and it may not play well with java modules).\nIf runtime code-gen is used this the total would be around ~400K (250core + 90asm + 50generator) but its a whole new territory.\nSo probably this can be closed.. Regarding your comment about removalListener why wouldn't just a few (even one) factory be enough? Is a factory on the hot path somehow? I got the idea that nodes need to be fast and with smal footprint?\nI see BoundedLocalCache defines almost all methods as throw new UnsupportedOperationException().\nSince the only code calling these methods is generated, it seems safe to back the methods by fields in the parent and don't generate classes for factories at all?. When I removed the nodeFactory classes (making a node a factory for self) there were 90K in savings.\nI think by removing the CacheFactory classes we can save about the same amount.\nWould you feel like merging it if I try it out or you have some concerns wrt performance?. Yes, I missed that. After the rebase there are only 11K saved. Still worth it in terms of the simpler code :). Not really, the factory is stateless. Take the generated WSo for example:\n```java\n    ...\n   // factory methods:\n    public  Node newNode(K2 key, ReferenceQueue keyReferenceQueue, V2 value,\n        ReferenceQueue valueReferenceQueue, int weight, long now) {\n      return new WSo<>(key, keyReferenceQueue, value, valueReferenceQueue, weight, now);\n    }\npublic <K2, V2> Node<K2, V2> newNode(Object keyReference, V2 value,\n    ReferenceQueue<V2> valueReferenceQueue, int weight, long now) {\n  return new WSo<>(keyReference, value, valueReferenceQueue, weight, now);\n}\n\npublic <K2> Object newLookupKey(K2 key) {\n  return new LookupKeyReference<K2>(key);\n}\n\npublic <K2> Object newReferenceKey(K2 key, ReferenceQueue<K2> referenceQueue) {\n  return new WeakKeyReference<K2>(key, referenceQueue);\n}\n\n...\n```\nThere are no instance fields in the factory as it is now an interface.. Yes, the Wso used as factory will have some extra fields, but the Wso instances used as Node will have 4 more methods - no extra fields.. Sorry, I thought you are concerned with bloating the Node footprint :). Btw the K2, V2 are there to fix a few warnings. If we can live with the shadowing warnings that's 2K more saved and the psychological barrier of 700KB is passed :). The enum constants would also create 144 field + 144 classes (in the metaspace) when probably 2-3 are really used. I would expect that memory usage is reduced a bit - assuming an app has a few caches now it would also need a create a few nodeFactories. The enum approach would always create 144 factories.. Hey, np. I was about to do the same :)\nFrom this point I think only using a minifier/obfuscator can help.. ",
    "tepitebson": "Hi. Good to see that the problem has been located, we will revert to 2.3.0 meanwhile since that version does not seem to be affected by this issue. Thank you for your swift response for the issue.\n. ",
    "lburgazzoli": "Woul it be instead possible to fix it and mark the  capabilities as optional ? so if you get an osgi aware api your bundle would work but capability won't be mandatory\n. I'll try to experiment a little bit then a PR if results are good \n. damn I forgot this issue, I may resume working on it in a couple of weeks.. ",
    "anhldbk": "@lburgazzoli Any progress?. @lburgazzoli Let's me share this burden with you then :D . @ben-manes I'm sorry to ask the wrong question. \n\nCaffeine provides an in-memory cache using a Google Guava inspired API.\n\nMy question is related to some kind of disk caches. Just wonder how to enhance caffeine with disk persistence.. @ben-manes Perfect! Gonna have a look at your suggestions. Thank you!. @ben-manes I'm sorry to bother you once again. But I'm reading about RocksDB's Block Cache. So RocksDB would be a good caching solution which supports in-memory & persistent caching, right?. @ben-manes Thank you.. ",
    "jiming": "Hi Ben,\nGot it, thanks a lot for the detailed answer!\nBest regards!\nJiming\n. Dear Ben,\nMy concern was there has time window between expire and refresh duration. I guess I should set refresh duration shorter than expire duration, but how many? What is the number? It is not easy.\nBTW, I wrote unit test to verify refreshAfterWrite, it doesn't work, and I found refreshAfterWrite even affect the expire duration of expireAfterWrite, no idea why:(\n```\n    @Test\n    public void test_reload_about_to_expired_entry() throws InterruptedException {\n        AtomicInteger counter = new AtomicInteger(0);\n        AtomicInteger reloadCounter = new AtomicInteger(0);\n        LoadingCache graphs = Caffeine.newBuilder()\n                .maximumSize(1000)\n                .expireAfterWrite(50, TimeUnit.MILLISECONDS)\n                .refreshAfterWrite(30, TimeUnit.MILLISECONDS)\n                .recordStats()\n                .build(new CacheLoader() {\n                    @Override\n                    public String load(@Nonnull String key) throws Exception {\n                        return key + \":\"  + counter.incrementAndGet();\n                    }\n                @Override\n                public String reload(@Nonnull String key, @Nonnull String oldValue) throws Exception {\n                    reloadCounter.incrementAndGet();\n\n//                        System.out.println(\"old value = \" + oldValue);\n                        return oldValue;\n                    }\n                });\n        Assert.assertEquals(\"jiming:1\", graphs.get(\"jiming\"));\n        Thread.sleep(30);\n        System.out.println(reloadCounter.get());\n        Assert.assertEquals(\"jiming:1\", graphs.get(\"jiming\"));\n        Assert.assertEquals(1, reloadCounter.get());\n    Thread.sleep(20);\n    Assert.assertEquals(\"jiming:1\", graphs.get(\"jiming\"));\n    Assert.assertEquals(1, reloadCounter.get());\n\n\n    System.out.println(\"1.\" + graphs.get(\"jiming\"));\n    Thread.sleep(20);\n    System.out.println(\"2.\" + graphs.get(\"jiming\"));\n    Thread.sleep(20);\n    System.out.println(\"3.\" + graphs.get(\"jiming\"));\n    Thread.sleep(20);\n    System.out.println(\"4.\"+ graphs.get(\"jiming\"));\n    Thread.sleep(20);\n    System.out.println(\"5.\" + graphs.get(\"jiming\"));\n    Thread.sleep(1000);\n}\n\n```\nAm I write the code not correct?\nPlease kindly help me check the code!\nThanks\nJiming\n. Dear Ben, \nI agree that, breaking API is not good idea. What about adding a new hook api, checkPreExpire(new Prediccate(key, oldValue, Node theExpiredNode){}), \nIf the Predicate return true, remove the entry. Otherwise, keep it and extend its life.\nBest regards\nJiming\n. Dear Ben,\nUsing refresh and expire together is easy to be confused, so I prefer to not use it.\nI think add the checkPreExpire hook is a clean method. You said it is a special case, and which I think it is a very useful feature.\nMost data put in cache are not modified often, that's why we can put it into cache. I believe most data  are not modified when they are evicted from cache. When this happened, we need re-query the WHOLE data from database and serialize it into object again. However, with the checkPreExpire hook, we can simply check id/updated_time, which is indexed, ten of hundreds times faster than reload the whole object. Which is very valuable to a heavy system. And which is why we use cache.\nWhat do you say?\nBest regards,\nJiming\n. Dear Ben,\nThis feature also can easy developers' pain on setting duration of expiration. \nFrom performance side, we want it the longer the better. Therefore, from functionality side, we want it shorter. With this feature's help, we can simply set the expired time shorter. \nThanks\nJiming\n. Dear Ben,\nI agree there have some other way to resolve this, including your advice. It's just it is not clean in my view:)\nIf the main difficult it about concurrency, I think I did not describe myself clear. Here you only need to consider RemovalClause.EXPIRED, the hook method is only called before expire(expireAfterWrite, NOT expireAfterAccess), and if the Hook method return true, you just never need to do the real job of remove the entry. You simply extend its life in cache is fine. Which I suppose will not cause concurrency problem.\nI already used redis, but even redis, I guess it is at least 1000 times slower than load from cache. I just wanted some extreme performance. In order to avoid creating a complex notify cache clean system, I set some cache entries' TTL only seconds, I really do not like the entry to be reloaded againg and again.\nBTW, could you please consider add a RemovalCause.EXPIRED_AFTER_WRIETE and RemovalCause.EXPIRED_AFTER_ACCESS.\nThank you for your time!\nJiming\n. Dear Ben,\nThanks for your kindness and  patiance! I will try some other hack.\nHave a nice day!\nJiming\n. Thanks!. Thanks Ben,\nI read document about Striped in guava, and found that your advice is very much valuable and helpful.\nHave a nice day!\nJiming. Thanks very much!. Great! Sorry, I didn't notice it is immutable.\nThanks a lot!. Dear Ben,\nI think the way I use cache might not totally same with you. Some cases, the cache fill to capacity(100%). However there have other case, I hope capacity should not over n%, otherwise it should be warned. \nFor example, some config information in database, I expect that all of them should be in cache and can be refreshed by expireAfterWrite. I do not know exact config item count. So I will set the maximum sizes to 100 if I estimate the count is less than 80. When OccupancyRate is over 80%, I should be warned and I will tuning the maximum sizes.\nI think maximumWeight is resolvable. If maximumWeight is set, use maximumWeight to calculate OccupancyRate. Otherwise use maximumSize.\nThanks!\nJiming\n. Thanks ben, I'll use your method:). ",
    "costimuraru": "@ben-manes, when you're saying I'd recommend using your own timestamp and relying on size eviction. do you think you could help with an example?\nI'm trying to achieve the exact same thing in my application: have an expire time of X seconds, but for empty values have an expire time of X/10 seconds. The use case is the following: I'm querying the data store in order to retrieve a value. If the value is present then ok, cache it. However, if no entry is found, then it's ok to cache it, but I want to retry it more often.. Thanks @ben-manes, you're the man!\nWill plug this in and see how it goes. Do you happen to know if there is a performance penalty with using the variable expire time?. \ud83d\udc4d\nI was a bit surprised to see that the refresh() method returns void and there is no way to be notified when the value is loaded. In my use case if I detect my value is stale, I want to refresh() and block until it's loaded.. I believe I got it to work using invalidate().\njava\nCacheValue cachedValue = lookupCache.get(cacheKey);\nif (cachedValue.isExpired()) {\n    // Refresh value.\n    lookupCache.invalidate(cacheKey);\n    cachedValue = lookupCache.get(cacheKey);\n}\nThe only thing that bugs me are the cache statistics. In the case when the value is expired (related to https://github.com/ben-manes/caffeine/issues/114), the above code will generate 2 cache requests (1 hit and 1 miss). Ideally, this should be 1 cache request with 1 miss.\nBy the way, great job with this library! This is gold.. Thanks, Ben! I've updated the code with your recommendation.\nThe variable expiration would be cool and the hash timer wheel sounds intriguing. . ",
    "chrisstockton": "Darn, that is unfortunate.  Thanks for investigating.\n. We haven't officially adopted JCache, but we do have several APIs that use Hazelcast and several that use Guava caches.  We're also currently looking at Apache Ignite.  So using the standard APIs seems like a good idea with Caffeine replacing any existing Guava caches.\nI originally was prototyping just using the cache annotations, but (like you know) they turned out too limited for what I wanted to do.\nInstead of the annotations, I was looking at just injecting javax.cache.Cache caches everywhere.  It'd be easy enough to just have a CaffeineCacheModule and an IgniteCacheModule, etc to make swapping the actual implementations in and out very easy with Guice.\nHazelcast and Ignite both use spring xml configuration so you can do DI pretty easily:  https://github.com/apache/ignite/blob/master/modules/core/src/main/java/org/apache/ignite/Ignition.java#L487, though when using Guice, you're binding objects created by Spring, which is weird.\nUsing the FactoryBuilder approach, could you just add a FactoryBuilder instance to  CaffeineConfiguration that could be configured via application.conf, defaulting to one that uses javax.cache.configuration.FactoryBuilder?  \nThat seems like it would keep Guice out of the library, although it wouldn't solve the static injection problem, but at least Caffeine's conscience would be clear :)\n. I think this is what you were getting at with the second approach:  https://github.com/ben-manes/caffeine/compare/master...chrisstockton:static-factories?expand=1\njavax.cache.configuration.FactoryBuilder is a final class and doesn't implement any interfaces, so I ended up adding some indirection in \n```\npublic final class TypesafeConfigurator {\n  private static CacheLoaderFactoryBuilder cacheLoaderFactoryBuilder = FactoryBuilder::factoryOf;\n  private static CacheWriterFactoryBuilder cacheWriterFactoryBuilder = FactoryBuilder::factoryOf;\npublic static interface CacheLoaderFactoryBuilder {\n    Factory<? extends CacheLoader> getFactory(String className);\n  }\n  ..\n  public static interface CacheWriterFactoryBuilder {\n    Factory<? extends CacheWriter<? super K, ? super V>> getFactory(String className);\n  }\n}\n```\nMaybe we don't need more than one builder, but I got a little tripped up with the generics on the static methods.\nLike you said, this got rid of any problems with static injection in client code and made Class.newInstance(...) unnecessary:\n```\n  @Test\n  public void testGuiceCacheLoaderFactoryBuilder() {\n    System.setProperty(\"config.resource\", \"/read-through-test.conf\");\n    Module module = Modules.override(new CacheAnnotationsModule()).with(new CaffeineJCacheModule());\n    Injector injector = Guice.createInjector(module, new AbstractModule() {\n      @Override\n      protected void configure() {\n        Map persons =\n            Arrays.asList(new Person(1, \"Dick\"), new Person(2, \"Jane\"), new Person(3, \"Spot\"))\n                .stream().collect(toMap(Person::getId, Function.identity()));\n        bind(PersonCacheLoader.class);\n        bind(PersonDao.class).toInstance(new PersonDao(persons));\n      }\n    });\nGuiceCacheLoaderFactoryBuilder builder = new GuiceCacheLoaderFactoryBuilder<>(injector);\nTypesafeConfigurator.setCacheLoaderFactoryBuilder(builder);\n\nCacheManager manager = injector.getInstance(CacheManager.class);\nCache<Integer, Person> cache = manager.getCache(\"read-through\");\n\nPerson p = cache.get(1);\nassertThat(p, is(notNullValue()));\nassertThat(p.getName(), is(\"Dick\"));\n\nMap<Integer, Person> persons = cache.getAll(Sets.newHashSet(2, 3));\nassertThat(persons.size(), is(2));\nassertThat(persons.get(2).getName(), is(\"Jane\"));\nassertThat(persons.get(3).getName(), is(\"Spot\"));\n\n}\n```\nIt does feel a little weird with static setters on TypesafeConfigurator, but it definitely works and keeps Caffeine agnostic.\nI guess it comes down to static setters vs Class.newInstance and static injection.  I think I agree with you that the static setters are the less messy way to do it.\n. Much better!  I think I would also prefer the @Inject annotation on the FactoryCreator, but either way works.\nAny chance you'd add this to the library?  It's definitely good enough for me to continue with what I wanted to do when I started.\n. Fantastic!  Thanks for all the help.\n. I don't have any experience mixing Spring annotations with the JSR-330 annotations.  \nBrowsing through the latest spring docs, it looks like you can specify multiple base-packages, including packages in 3rd-party jars.  But I don't really know.\n. This is great.  Nice little trick with using a provider method reference to implement the actual factory:\n@Override\n    @SuppressWarnings(\"unchecked\")\n    public <T> Factory<T> factoryOf(String className) {\n      try {\n        Class<T> clazz = (Class<T>) Class.forName(className);\n        return injector.getProvider(clazz)::get;\n      } catch (ClassNotFoundException e) {\n        throw new RuntimeException(e);\n      }\n    }\nThanks again.\n. Sure, sounds good.  It might take a little longer than a few days, \"real-work\" is getting in the way lately.  But I'll be playing with it when I have spare time.\n. I had to take a break for a while there, but I picked this back up a few weeks ago.  I've been using the cache loader with read-through from the snapshot in production for about a week now and haven't found any more issues.\n. Does it look like 2.3.4 will be out soon.  It's looking like #127 is fixed?\n. Okay, I'll add the configs in my application.conf and test out with the snapshot.  Thanks for the quick work!. Seems to be working perfectly.  Thank you!. ",
    "ankit8051": "Thanks, Ben,\nThanks for your quick reply.\nYou are right. Cache is not allowing the null values. \nActually, I was storing java.util.Optional so that's why I was not getting an exception.\n. My question is resolved now.\nClosing this question.\n. ",
    "trejkaz": "A few things I'm wondering now:\n- If compilation works, is it safe to assume that runtime will work? Because currently the checks jdeps is running are done against the contents of the jar files. I don't have a way to test this overall, because there are several dozen libraries in our way before we can even consider running a test. :/\n- If Unsafe is ... safe ..., is there a new version of jdeps which doesn't emit a warning for that class? Otherwise, how do I know which ones are legit warnings and which ones aren't? I will note that it isn't recommending use of VarHandles either - so it's at the very least missing the bit where it's supposed to give tips on how to fix the issue. Maybe a newer version has that too?\n. Yeah, my biggest worry with all of this is actually libraries like Documentum, where the authors clearly have no idea how to do Java in the first place, and probably can't be trusted to update in a reasonable time frame.\n. ",
    "kutchar": "I'm assuming you're referring to the following, right?\nPolicy.Expiration.oldest\nPolicy.Expiration.youngest\nPolicy.Expiration.getExpiresAfter\nSo considering the oldest and youngest are not constant time, wouldn't it be better to use the asMap since I just need a snapshot of the cache and not necessarily in any order?\nAlso, I'm assuming in either case I have to iterate through the iterator and call Expiration.getExpiresAfter on each item to get the expiration, correct?\n. Thanks :)\n. ",
    "krm1312": "Thank you for the explanation.  How we are using it is indeed not ideal, but, I imagine someone else might run into this so I thought I'd at least post it.\n. ",
    "adamretter": "I guess that I could use a Cache<Id, Cache<Id, Node>>, however my gut feeling is that this could be quite expensive?\n. @ben-manes I am not sure if I am quite following what you are saying.\nWell... I had an attempt today to try and create something, it's pretty wild but I would be interested to get your opinion: https://gist.github.com/adamretter/11ccca59cebd07b4612b2b1f4e7bfb4f\nThe function of interest which plays with Caffeine is getOrCreateNodeExplicit\nOne of the helpful properties I have is that an id for one of my nodes is really the path to the node, which allows me to make some simple calculations. I managed to encase most of the updates inside a compute block, I allow different threads to collaborate in linking a child node into the tree, basically each node has an AtomicBoolean which indicates whether the node is active (i.e. inserted into the tree yet).\nI am not sure what the concurrency is like, but I take a bottom-up approach to adding nodes, so hopefully I only call compute on the closest existing ancestor of the tree node.\nThere are also some basic tests: https://gist.github.com/adamretter/0b58dffa9ab7476cc96ed5b0d0f526b9\nWith some naive testing it seems to show that it works, but I might be missing something obvious that further testing would reveal. Also I have no real idea of the concurrency (or not).\n. @ben-manes Thanks for the explanation, that helps me to understand your previous comment. However in this instance I am using a NoSQL database and it does not provide Transactions so I need to manage my own concurrency here.\n. I guess the question is, if there is any support for adding/removing form the value Deque, or working with Collections as values in Caffeine. I have to head off now, but I will take a look at your suggestions tomorrow AM. Thanks Ben. @ben-manes Thanks for the suggestion. I did end up using asMap along with a concurrent ring-buffer implementation for the value stack. Not sure what you might make of this - https://github.com/adamretter/exist/blob/refactor/caching/src/org/exist/storage/XQueryPool.java#L130. @ben-manes I assumed that compute was lock per-key? Is it rather a single lock for the entire Cache?\nRegards your comment about ArrayDeque I couldn't see how to operate on it in a thread-safe manner in borrowCompiledQuery, which is why I opted for the MpmcAtomicArrayQueue. Unfortunately some of the operations that I need to perform in borrowCompiledQuery rely on consulting the database and may throw exceptions. That said... I do believe that you might see a better way than I can, so if you have any ideas there that I have misunderstood or not yet explored, please do let me know.\nThanks for all your help as always. Cheers Adam.\n. Thanks @ben-manes . @ben-manes Okay cool. Shame about the C code. Nice to hear your take on it :-). ",
    "normanuber": "Thanks for acknowledging the issue! I hope you can reproduce it. If you need any more info, I'll try my best to help.\n. @nitsanw \njava -version\nOpenJDK Runtime Environment (build 1.8.0_102-8u102-b14.1-1~bpo8+1-b14)\nOpenJDK 64-Bit Server VM (build 25.102-b14, mixed mode)\nlsb_release -a\nNo LSB modules are available.\nDistributor ID: Debian\nDescription:    Debian GNU/Linux 8.6 (jessie)\nRelease:    8.6\nCodename:   jessie\nTo answer your question, I suppose I can run a hacked build as long as it is through maven central (using SNAPSHOT versions is fine).\n. Looks like the email didn't make it here.\nI've already switched my production application back to Guava since I require removal notifications or else I would use a CHM.\nThere are different levels of the cache, but the most threads touching any of the caches is below 10. 4 dedicated writers for the highest level cache while less than 6 readers at any time reading from it. The highest level cache is a permanent data structure while the child data structures containing a cache last as long as the child element will last (these children can last for weeks). A few hundred of these datastructures are expired/created per day from what my stats say.\nI will setup a test environment around the application and attempt to include your library when github is back up.\n. Is the snapshot version in Maven Central? If not can you push one out there? or can you get me pom directive to add your dev version?\n. I'm am attempting to repro the issue on the test environment with 2.3.1 then 2.3.3 finally 2.3.4-SNAPSHOT and will get back to you.\n. Alrighty, so all of them got stuck (2.31 - 2.34-SNAPSHOT). I tested in production instead >_>, since the test environment is too small.\n2.31 was the fastest one to get stuck. Maybe 1-2 mins to get a stuck worker.\n\n2.33 at about 12 minutes in\n\n\n2.34-SNAPSHOT about 27- 30 mins\n\n\n2.34 FJP\n\n. Sure, tell me when you have updated.\n. 1 hour runtime so far and no problems. I'll keep it running for 4 more hours before rolling back to the stable version on my software.\n. Looks good after 5 hours of runtime.\n. (deleted)\nThis response is a duplicate above when github was down and i tried to send via email.\n. ",
    "wclaeys": "I'm using org.apache.felix.framework;version='[5.4.0,5.4.0]' together with Bndtools (3.3.0). \nFull stacktrace:\nCaused by: java.lang.ClassNotFoundException: sun.misc.Unsafe not found by com.github.ben-manes.caffeine [15]\nat org.apache.felix.framework.BundleWiringImpl.findClassOrResourceByDelegation(BundleWiringImpl.java:1574)\nat org.apache.felix.framework.BundleWiringImpl.access$400(BundleWiringImpl.java:79)\nat org.apache.felix.framework.BundleWiringImpl$BundleClassLoader.loadClass(BundleWiringImpl.java:2018)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\nI was able to solve this issue by adding the following in my bndrun file:\n-runsystempackages: sun.misc\nThx guys for the pointers to solve this issue!\n. ",
    "Valloric": "I was looking at this because I was trying to answer does Caffeine cache misses as well as hits for a LoadingCache. I'm trying to avoid 50 gets for key X that's not in the cache or in the backing store resulting in 50 gets to the backing store (which is on disk). If Caffeine cached misses (with the standard timeout it uses for records), I'm done. If it doesn't, I need to implement it on top of Caffeine (with another Caffeine cache for misses). \nWhen I saw the recordMiss part it made me think that yes, misses will be recorded but the implementation uses recordStats.\nSo I'm still not sure are you saying recordMiss means \"record a miss in the stats\" and the param names are not in sync (but the code is), OR recordMiss means \"record a miss in the cache\" and the code has a bug. :)\n. > I think deferring negative caching to client code is less confusing for everyone involved.\nPossibly, though I don't see how adding another cacheMisses config method to the Caffeine builder would be confusing. It would have great docs like any other method on the builder. :)\nBut like you said, it's easy to emulate. Instead of returning null for misses from my CacheLoader, I'm just going to return some sort of sentinel empty object. Caffeine will cache that just fine. My logic will need to be a bit funkier in that I'll have to recognize the sentinel object, but I can live with that.\nThanks for the quick responses! (Feel free to close this issue if needed.)\n. > The Optional is the sentinel I was thinking of. Then you can do cache.get(key).orElse(null) which isn't too bad.\nYeah but that's another pointer dereference in the happy path, no? Optional.get() isn't free and I'm writing perf-critical code. Storing a sentinel V means happy path just has the V right away.\n\nNegative caching adds a lot of subtle complexity. \n\nAgreed, which is kinda the reason why it might be better to have it in the library so that it only needs to be written and tested once instead of in all the clients that are doing this. For instance, I'll now have to stop using the nice stats provided by Caffeine (thanks for supporting stats BTW, I love it) so I can easily track miss rate. I'll need to write custom stats code in my CacheLoader so that I have the \"real\" miss rate.\nNot saying that any of this is obvious or that my suggestion is correct; this is very much in the trade-off space where both approaches strike me as reasonable. Punting on this to the caller is an understandable choice; complexity comes with non-trivial costs.\n. A wrapper around the raw array would indeed work, but it comes with big drawbacks: first, it's an extra layer of indirection. Instead of derefing a pointer, the CPU now needs to deref a pointer to the object and then deref the array pointer within it. This two-layer indirection screws up memory prefetching because while modern CPUs are good at prefetching data behind a one-layer indirection, two layers doesn't work nearly as well.\nAnd on top of this, there's the extra space overhead of another pointer. We're moving away from String keys to byte[] compressed keys precisely to reduce RAM usage, and the extra 8 bytes for the pointer are painful when the average compressed byte array is 9 bytes. \nWhether you special-case arrays to work or not is up to you of course, but even if you don't, you might want to at least throw an exception from build() if the key is an array to prevent people from blowing their foot off (we almost did).. We'll just go with the wrapper approach; hopefully it should be enough.\nThank you for your time and for considering this though! . ",
    "kmisaal": "As caffeine is coming in from external dependency we do not access the classes directly. However in my root configuration I tried loading the class by logging it as below.\nlogger.debug(\"Loading the caffeine cache classes \"+RemovalCause.class);\nThis logger statement does load the class.\n. Can you elaborate on how to use UnsafeAccess:UNSAFE ?\n. Looks like UnsafeAccess.UNSAFE is loading fine. See the below output of logging.\nLoading the caffeine cache classes class com.github.benmanes.caffeine.cache.RemovalCause\nLoading the caffeine cache classes sun.misc.Unsafe@1bef94d2\n. I tried this\n logger.info(\"Loading the caffeine cache classes \"+RemovalCause.class);\nlogger.info(\"Loading the caffeine cache classes \" + UnsafeAccess.UNSAFE);\nCaffeine.newBuilder().buildAsync(key -> key);\nlogger.info(\"Build async is successful\");\nIt worked no exception.\nLoading the caffeine cache classes class com.github.benmanes.caffeine.cache.RemovalCause\nLoading the caffeine cache classes sun.misc.Unsafe@74cdde64\nBuild async is successful\n. I tried copying the caffeine-2.3.3.jar into $CATALINA_HOME/lib however it did not work. The exception I get it \nWARNING: WebappClassLoader.findClassInternal(com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache$LoadingCacheView) security exception: access denied (\"java.io.FilePermission\" \"C:\\java\\tools\\apache-tomcat-7.0.42-pacman\\webapps\\cmt\\WEB-INF\\classes\\com\\github\\benmanes\\caffeine\\cache\\LocalAsyncLoadingCache$LoadingCacheView.class\" \"read\")\njava.security.AccessControlException: access denied (\"java.io.FilePermission\" \"C:\\java\\tools\\apache-tomcat-7.0.42-pacman\\webapps\\cmt\\WEB-INF\\classes\\com\\github\\benmanes\\caffeine\\cache\\LocalAsyncLoadingCache$LoadingCacheView.class\" \"read\")\n        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)\n        at java.security.AccessController.checkPermission(AccessController.java:884)\n        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)\n        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)\n        at java.io.File.exists(File.java:814)\n        at org.apache.naming.resources.FileDirContext.file(FileDirContext.java:768)\n        at org.apache.naming.resources.FileDirContext.doLookup(FileDirContext.java:198)\n        at org.apache.naming.resources.BaseDirContext.lookup(BaseDirContext.java:483)\n        at org.apache.naming.resources.ProxyDirContext.lookup(ProxyDirContext.java:308)\n        at org.apache.catalina.loader.WebappClassLoader.findResourceInternal(WebappClassLoader.java:2970)\n        at org.apache.catalina.loader.WebappClassLoader.findClassInternal(WebappClassLoader.java:2830)\n        at org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:1173)\n        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1681)\n        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)\n        at com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache.synchronous(LocalAsyncLoadingCache.java:258)\n        at com.sony.sce.np.services.domain.protobuffer.LookupDomainImpl.logStats(LookupDomainImpl.java:804)\n        at com.sony.sce.np.services.domain.protobuffer.LookupDomainImpl.lambda$init$0(LookupDomainImpl.java:175)\n        at com.sony.sce.np.services.domain.protobuffer.LookupDomainImpl$$Lambda$2/808263140.load(Unknown Source)\n        at com.github.benmanes.caffeine.cache.CacheLoader.lambda$asyncLoad$0(CacheLoader.java:105)\n        at com.github.benmanes.caffeine.cache.CacheLoader$$Lambda$69/1675304567.get(Unknown Source)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1582)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1574)\n        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1689)\n        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\nNov 14, 2016 5:06:25 PM com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache lambda$get$3\nWARNING: Exception thrown during asynchronous load\njava.util.concurrent.CompletionException: java.lang.IllegalAccessError: tried to access class com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache$LoadingCacheView from class com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache\n        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)\n        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1584)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1574)\n        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1689)\n        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\nCaused by: java.lang.IllegalAccessError: tried to access class com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache$LoadingCacheView from class com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache\n        at com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache.synchronous(LocalAsyncLoadingCache.java:258)\n        at com.sony.sce.np.services.domain.protobuffer.LookupDomainImpl.logStats(LookupDomainImpl.java:804)\n        at com.sony.sce.np.services.domain.protobuffer.LookupDomainImpl.lambda$init$0(LookupDomainImpl.java:175)\n        at com.sony.sce.np.services.domain.protobuffer.LookupDomainImpl$$Lambda$2/808263140.load(Unknown Source)\n        at com.github.benmanes.caffeine.cache.CacheLoader.lambda$asyncLoad$0(CacheLoader.java:105)\n        at com.github.benmanes.caffeine.cache.CacheLoader$$Lambda$69/1675304567.get(Unknown Source)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1582)\n        ... 5 more\nNov 14, 2016 5:06:25 PM org.apache.catalina.loader.WebappClassLoader findClass\nWARNING: WebappClassLoader.findClassInternal(com.github.benmanes.caffeine.cache.RemovalCause$1) security exception: access denied (\"java.io.FilePermission\" \"C:\\java\\tools\\apache-tomcat-7.0.42-pacman\\webapps\\cmt\\WEB-INF\\classes\\com\\github\\benmanes\\caffeine\\cache\\RemovalCause$1.class\" \"read\")\njava.security.AccessControlException: access denied (\"java.io.FilePermission\" \"C:\\java\\tools\\apache-tomcat-7.0.42-pacman\\webapps\\cmt\\WEB-INF\\classes\\com\\github\\benmanes\\caffeine\\cache\\RemovalCause$1.class\" \"read\")\n        at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)\n        at java.security.AccessController.checkPermission(AccessController.java:884)\n        at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)\n        at java.lang.SecurityManager.checkRead(SecurityManager.java:888)\n        at java.io.File.exists(File.java:814)\n        at org.apache.naming.resources.FileDirContext.file(FileDirContext.java:768)\n        at org.apache.naming.resources.FileDirContext.doLookup(FileDirContext.java:198)\n        at org.apache.naming.resources.BaseDirContext.lookup(BaseDirContext.java:483)\n        at org.apache.naming.resources.ProxyDirContext.lookup(ProxyDirContext.java:308)\n        at org.apache.catalina.loader.WebappClassLoader.findResourceInternal(WebappClassLoader.java:2970)\n        at org.apache.catalina.loader.WebappClassLoader.findClassInternal(WebappClassLoader.java:2830)\n        at org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:1173)\n        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1681)\n        at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1559)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$remove$12(BoundedLocalCache.java:1795)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache$$Lambda$72/1155635852.apply(Unknown Source)\n        at java.util.concurrent.ConcurrentHashMap.computeIfPresent(ConcurrentHashMap.java:1769)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.remove(BoundedLocalCache.java:1786)\n        at com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache.lambda$get$3(LocalAsyncLoadingCache.java:145)\n        at com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache$$Lambda$70/1811334594.accept(Unknown Source)\n        at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:760)\n        at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:736)\n        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1587)\n        at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1574)\n        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1689)\n        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\n. On local environment I have only one tomcat and only one application is deployed. Therefore I am trying to understand your reply \n\nSo its due to ForkJoinPool.commonPool() being shared across applications in Tomcat so threads use different class loaders.\n. I will try out your suggestions. I will also try to find out in house tomcat expert. \nI will let you know soon.\nThanks for your help so far.\n. \n",
    "Mats-SX": "I see what you mean. I'd say that one of the use cases, the Cypher planner cache, is critical, but it's typically a fairly small cache that gets accessed only once per request. For highly concurrent workloads, the cache's performance (especially if full) would probably matter, but I haven't seen any numbers on that.. ",
    "markelliot": "Cool, thanks, that should work here.. ",
    "timlincool": "In this case, my hashmap could have 1~300 element. \npublic Cache<String, HashMap<String, Double>> getItnameCpPro=Caffeine.newBuilder()\n                                                                .expireAfterWrite(25, TimeUnit.HOURS)\n                                .maximumWeight(1_000_000)\n                                .weigher((String string, HashMap hashMap) -> hashMap.size())\n                                .build();\n Is that correct? Thanks for the help.. ",
    "hennr": "You're welcome. ",
    "davidhoyt": "Thanks!. @ben-manes is it safe to always hold a reference to the returned Policy?. @ben-manes I didn't understand your response (and perhaps the request neither). I was going to submit a feature request myself and wasn't sure, based on your response, if my request was the same.\nWhat I was seeking to do was to specify an expiration per entry. So entry \"foo\" expires in 1 hour but entry \"bar\" expires in 2 hours. Is that already possible and does this issue capture that same request?. ",
    "enigma1510": "Meant to include... tested with 2.3.1 and 2.3.5.\nA few more tests runs, and sometimes it does pass... but most of the time if fails with the above.. After reviewing the annotation RI code, I realized this race condition only happens for caches unknown to the CacheManager (or unspecified in the config for Caffeine).  I had one such cache that was relying on the cache forever defaults.\nAs a workaround, I was able to declare \"caffeine.jcache.cache-forever: {}\" in the config and this race condition no longer happens.\nThanks for the quick response Ben.. ",
    "boschb": "Sure thing.\nMy scenario is I have a Multimap<Key,Value> that I use as truth.  I want to provide an immutable 'snapshot' of the map using a Cache<Key,Set<Value>> with a CacheLoader that simply synchronizes on the Multimap and builds an ImmutableSet from the current values (for a given key).  This might seem like overkill, but the idea is that I will always have a 'snapshot' set that I can iterate over (at high frequency) without blocking the underlying Cache or Multimap.  Adds and removes are less frequent and not under the same blocking requirements. Also my service allows for non-parity between the cache and map (for short a short period of time) as best effort, mostly accurate, snapshots provide all we need.\nBy utilizing the features of refresh() where a new value is loaded in the background while the old value continues to serve, this gives me exactly the behavior I need.\nThe only catch that I have is that I need to be able to gate (especially during tests) the point at which a new entry added to the Multimap actually is present in the underlying Cache.. Regarding versioning, wouldn't a @IgnoreReturnValue and ListenableFuture be safe?  method is already void.  I would even be ok with a completely new method.  It's also entirely possible I don't know what i'm talking about though in regards to semver as I'm not that familiar.  \nAlso FWIW, if this has to be a AsyncCache thing that's fine too (The line is a little grey regarding refresh()). Technically both.  Though the implementation in practice I think avoids this race probably every time because of network latency.  The timing hole still exists though (and tests prove it) so it does make me feel more correct to use it if I can.\nI was hoping that we could just bubble up the future from the CacheLoader easy peasy:\nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/LocalLoadingCache.java#L158\nI see your point about breaking other implementors though.  Api wise though I think it makes sense to bubble up this future from the loader, or maybe even move the insert logic to the loader itself by passing the Cache to the reload function... It's a tough call.  They are pretty interlinked, but this def is a Loading/Loader specific feature...  Your thoughts?. Awaitility wont work as this is happening in integration/loadtesting too... however yes I can probably fall back on a ConcurrentMap and just do the updates inline manually.  I may just do that actually.  I've gotten so used to just using a Cache instead of a Map because of the nice features.\nI would say I think there is still merit to bubbling up the Future (especially since it's there).  Can't hurt, and I would imagine any implementor has similar bits of code with the Loader that you do.   there's always immediateFuture too.  I'm not sold on putting Future stuff in the non-async version though, even though the documentation says it can happen asynchronously.\nAnyways thanks for the help and let me know if it makes it in 3.0.\nCheers,\nBobby. Expiration could be delayed (in my case) until an poll/write is made.\nThanks! i'll look into the concurrent ring buffer.  That sounds (almost) like what I have so far. Just need to add time based eviction.\nThanks again!. Hey guys,\nThanks for the great discussion.  I am trying to follow all the terminology\nbut i'm no expert in this field, you guys are pros!\nHere is what I ended up doing.  I will profile this eventually and maybe\nmake optimizations then but here is where it is now. It's necessary to know\nthe following:\n1.) I will have a lot of these ListeningEvictingQueues managed by a parent\nclass.\n2.) These queues are probably going to be of size 16-128.\n3.) I also have another single Cache that holds the super set of all\nentries.  This Cache has time (and weight) based eviction properties and a\nlistener that calls evict() when removed.  For completeness the Consumers\nalso removes from this master Cache.  This is going to be how I limit the\nimpact on memory of all elements traversing all queues.\nhttps://pastebin.com/BxNmyNRN\nWhat this is used for is what I call a SourceBalancer, where there are\n'sources' and 'targerts'.  Imagine many sources producing objects at\ndifferent rates and sizes.  Every target will have a Map of\nsource->ListeningEvictingQueue and will round robin pick from each.  This\nwill produce a 'fair' sampling of all sources objects to the target without\nworry of one source dominating the flow.\nI'm sure I can do better, but this was a small amount of code that I don't\nhave to worry too much about getting something wrong.  Let me know your\nthoughts though on this approach, I'd love to hear them.\nThanks again,\nBobby\nOn Fri, Sep 8, 2017 at 5:17 AM, Christian Esken notifications@github.com\nwrote:\n\nAbout the consumer(s) polling. The consumers - as I understand that word -\nare the eviction listeners. As a listener, they do not do a full scan. The\nfull scan exists, but this is done by the background eviction thread.\nNeither Threads writing to the Cache nor consumers take any penalty due to\nthe background work (not even amortized costs).\nIn general I understand that boschb is now looking into the concurrent\nring buffer you proposed. Without knowing the exact use case I think the\nring buffer is the most appropriate way.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/183#issuecomment-328087873,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADU1JbbHlAxu8B-UZO_gfxxJwmWtpdexks5sgTBigaJpZM4PKd8g\n.\n. Good point on the Array vs Linked.  I had originally had the Array version in the file but I think I moved it so that evict() would be more efficient.  I will revisit this once I have some numbers, thanks for the +1 though, makes me feel a little better about the whole thing :). Yah ideally for my case it would remain not loaded and invalidated.  Further I would love to see a removalListener get fired for this event too as otherwise there is no where (except a finalizer) to catch this value if it has a lifecycle that needs to be managed.\n\nEither way it would make sense to have a work around I think.  Something does feel like it's  missing here.. Yes agreed, cancel might seem most appropriate in this case, and would certainly work for my case.\nThe more I thought about it the more I realize there is a big unanswered question:\nGiven the flow:\n1.) Refresh Started\n2.) Invalidate\n3.) Refresh Completed\nIs the refreshed value newer or older than the invalidate call?  I don't think you can broadly say one or the other and that might be the crux of the confusion.  There probably exists cases where either makes complete sense.\nRegardless any action I can think of would be a major change to existing functionality as it is today so that is also an issue.\nI think I've been able to code around this issue by maintaining a separate Set of keys that should be filled and then in the async loader operation check against this Set once the value materializes and return null instead of the discovered value if the key no longer exists.  It's a bit hacky in my opinion but I think it's sound from a concurrent point of view.. If we can discard the loading/refreshed value with EXPLICIT that solves everything for me, but it is really your call as to whether this is acting in accordance with what the API should be.  You are the expert for this one and I can adjust as necessary based on the outcome so long as things are deterministic.. I should have wrote my tests first because they would have confirmed this behavior indeed works.  If you're interested here is what I was doing:\n```java\n/ Helpers for creating a custom {@link Expiry} using lambda {@link Supplier}s */\npublic final class Expiries {\n  /\n   * combines: {@link Expiry#expireAfterCreate(Object, Object, long)} and {@link\n   * Expiry#expireAfterUpdate(Object, Object, long, long)} to simulate the same behavior as {@link\n   * com.github.benmanes.caffeine.cache.Caffeine#expireAfterWrite(java.time.Duration)}\n   */\n  public static  Expiry expireAfterWrite(Supplier supplier) {\n    return new DefaultExpiry() {\n      @Override\n      public long expireAfterCreate(K1 key, V1 value, long currentTime) {\n        return Durations.toNanos(supplier.get());\n      }\n  @Override\n  public long expireAfterUpdate(K1 key, V1 value, long currentTime, long currentDuration) {\n    return Durations.toNanos(supplier.get());\n  }\n};\n\n}\n/*\n   * combines: {@link Expiry#expireAfterCreate(Object, Object, long)}, {@link\n   * Expiry#expireAfterUpdate(Object, Object, long, long)}, and {@link\n   * Expiry#expireAfterRead(Object, Object, long, long)} to simulate the same behavior as {@link\n   * com.github.benmanes.caffeine.cache.Caffeine#expireAfterAccess(java.time.Duration)}\n   /\n  public static  Expiry expireAfterAccess(Supplier supplier) {\n    return new DefaultExpiry() {\n      @Override\n      public long expireAfterCreate(K1 key, V1 value, long currentTime) {\n        return Durations.toNanos(supplier.get());\n      }\n  @Override\n  public long expireAfterUpdate(K1 key, V1 value, long currentTime, long currentDuration) {\n    return Durations.toNanos(supplier.get());\n  }\n\n  @Override\n  public long expireAfterRead(K1 key, V1 value, long currentTime, long currentDuration) {\n    return Durations.toNanos(supplier.get());\n  }\n};\n\n}\n/* see: {@link Expiry#expireAfterCreate(Object, Object, long)} /\n  public static  Expiry expireAfterCreate(Supplier supplier) {\n    return new DefaultExpiry() {\n      @Override\n      public long expireAfterCreate(K1 key, V1 value, long currentTime) {\n        return Durations.toNanos(supplier.get());\n      }\n    };\n  }\n/* see: {@link Expiry#expireAfterUpdate(Object, Object, long, long)} /\n  public static  Expiry expireAfterUpdate(Supplier supplier) {\n    return new DefaultExpiry() {\n      @Override\n      public long expireAfterUpdate(K1 key, V1 value, long currentTime, long currentDuration) {\n        return Durations.toNanos(supplier.get());\n      }\n    };\n  }\n/* see: {@link Expiry#expireAfterRead(Object, Object, long, long)} /\n  public static  Expiry expireAfterRead(Supplier supplier) {\n    return new DefaultExpiry() {\n      @Override\n      public long expireAfterRead(K1 key, V1 value, long currentTime, long currentDuration) {\n        return Durations.toNanos(supplier.get());\n      }\n    };\n  }\n/* Default implementation of {@link Expiry} which has no expiration /\n  private static class DefaultExpiry implements Expiry {\n    @Override\n    public long expireAfterCreate(K key, V value, long currentTime) {\n      return Long.MAX_VALUE;\n    }\n@Override\npublic long expireAfterUpdate(K key, V value, long currentTime, long currentDuration) {\n  return currentDuration;\n}\n\n@Override\npublic long expireAfterRead(K key, V value, long currentTime, long currentDuration) {\n  return currentDuration;\n}\n\n}\n}\n```\nAnd I use it like this:\n```java\nAtomicLong expireTime = new AtomicLong(1_000); // Just some mutable reference\nCaffeine.newBuilder()\n        .expireAfter(Expiries.expireAfterAccess(() -> Durations.fromNanos(expireTime.get())))\n        .build();\n```\nI have the test too if you want just let me know.. Ohh that is very interesting, I didn't know there was a way to change the policy at runtime.  I'll have to see if that works better for the real implementation which is pulling from a @ConfigScoped Guice binding that might change infrequently O(days) (this is CDD/ProtoConf if that rings a bell).  Is there any performance gain you think to using the original expireAfterAccess version instead of Expiry?\nThanks! I'm seriously starting to get how many fine details you've put into this project.. Thanks I will check that out.\nYah it is a custom scope for configuration that can get pushed out at runtime.  I'm experimenting with this for things that should be immune to oddities (like cache expiration time for instance).  Worst case should just be a cache miss (I hope).  There is pretty good validation protection around the values during pushes so I thought I'd give it a go for once and see what \"large fleet runtime configuration tuning\" feels like :). ",
    "allen-servedio": "Thank you for your quick reply!\nI will take a look at the CacheWriter#delete call and see if it is triggered on shutdown. I am passing in the result of calling java.util.concurrent.newWorkStealingPool(int parallelism) to the cache as its executor in case that matters.\nOur basic use case is that we are using the cache to hold counts of API calls in which those entries will expire after a minute (the cache key includes the minute that the call was made on and I set \"expireAfterWrite\" to about a minute - it does not have to be super precise and if entries come in after the minute they get added and still expire out). When the entry is removed from cache, we write it into our database.\nThis works great, right up until we shutdown the service. At that point, we are not flushing the current contents of the cache - they appear to be cleared without the removalListener being triggered. That is what I am going after now.\nI will try out what you listed here and then see if I can work up an isolated unit test that shows the behavior (and, if it doesn't, see if I can start nuking code out of the service until I identify what is interfering with this). Thanks again for your response (it will take me some time to reply as I won't get to this again until tonight at the earliest).. Sorry Ben. I should not have opened this without more time set aside to look into it. The current project I am on is taking more time than I anticipated. I will try to block some time this week to try out what you suggested and report back (if it worked) or provide a unit test (if it did not). Thank you for your patience.. Hi,\nOkay, so I am not yet able to reproduce this in a unit test (I pasted the code that I tried below). The code below is roughly what I am doing in my code when interacting with the cache. The difference between what I see and what this test shows is that when the shutdown hook code runs, the cache is 100% empty already AND the data that was in it was not processed by my removalListener code. \nI was able to determine this via the follow code (you will see it commented out in the larger script below.\nval cacheAsMap = cache.asMap()\n  println(s\"SHUTDOWN CACHE MAP: $cacheAsMap\")\n  cacheAsMap.asScala.foreach{\n    case (key, value) =>\n      println(s\"$key = $value\")\n      cacheAsMap.remove(key)\n  }\nThe above code prints an empty map in the first println in my code... In the script below, it prints out the cache as you would expect.\nHere is the result of running the script below:\n$ amm evict-on-shutdown-example.sc\nIncremented first\nIncremented second\nIncremented second\nIncremented third\nIncremented third\nIncremented third\nfirst = 1 because EXPLICIT\nthird = 3 because EXPLICIT\nsecond = 2 because EXPLICIT\nDid not have to force shutdown.\nSo, that works 100% as expected.\nI have a bad feeling that this started failing when I dragged in some dependency that is doing something clever... Do you want me to close this ticket until I have something more concrete? It could take me some time before I find something. I could reopen it or comment if I find something useful.\nHere is the script:\n```\n!/usr/bin/env amm\n// Uses https://github.com/lihaoyi/ammonite 0.8.2 for Scala 2.11 (see http://www.lihaoyi.com/Ammonite/#OlderScalaVersions) - so it has to be installed to run this.\n// After install, run with: amm evict-on-shutdown-example.sc\nimport $ivy.com.github.ben-manes.caffeine:caffeine:2.4.0, com.github.benmanes.caffeine.cache.\nimport java.util.concurrent.{Executors, TimeUnit}\nimport java.util.concurrent.atomic.LongAdder\nimport scala.util.{Failure, Success, Try}\nimport $ivy.org.scala-lang.modules::scala-java8-compat:0.8.0, scala.compat.java8.FunctionConverters.\nimport scala.collection.JavaConverters._\nval processMessageExecutorService =\n  Executors.newWorkStealingPool(Runtime.getRuntime.availableProcessors)\nval cache: Cache[String, LongAdder] = Caffeine.newBuilder()\n  .maximumSize(1000)\n  .expireAfterWrite(60, TimeUnit.SECONDS)\n  .removalListener(new RemovalListenerString, LongAdder {\n    override def onRemoval(key: String, value: LongAdder, cause: RemovalCause): Unit = {\n      println(s\"$key = $value because $cause\")\n    }\n  })\n  .executor(processMessageExecutorService)\n  .buildString, LongAdder\ndef increment(key: String) = {\n  val wasIncremented = Try(cache.get(key, ((: String) => new LongAdder()).asJava).increment())\n  wasIncremented match {\n    case Success() => println(s\"Incremented $key\")\n    case Failure(ex) => println(s\"Failed to increment $key: $ex\")\n  }\n}\nsys.addShutdownHook {\n  // val cacheAsMap = cache.asMap()\n  // println(s\"SHUTDOWN CACHE MAP: $cacheAsMap\")\n  // cacheAsMap.asScala.foreach{\n  //   case (key, value) =>\n  //     println(s\"$key = $value\")\n  //     cacheAsMap.remove(key)\n  // }\ncache.invalidateAll()\n  cache.cleanUp()\n  processMessageExecutorService.shutdown()\nTry(processMessageExecutorService.awaitTermination(60, TimeUnit.SECONDS)) match {\n    case Success(terminated) => if (!terminated) processMessageExecutorService.shutdownNow() else println(\"Did not have to force shutdown.\")\n    case Failure(ex) =>\n      println(s\"Failure during process message termination: $ex\")\n      processMessageExecutorService.shutdownNow()\n  }\n}\nincrement(\"first\")\nincrement(\"second\")\nincrement(\"second\")\nincrement(\"third\")\nincrement(\"third\")\nincrement(\"third\")\n```. Yes on the executor - the line of code you see is pretty close to what is in my code. It calls this method in java.util.concurrent.Executors:\npublic static ExecutorService newWorkStealingPool() {\n        return new ForkJoinPool\n            (Runtime.getRuntime().availableProcessors(),\n             ForkJoinPool.defaultForkJoinWorkerThreadFactory,\n             null, true);\n    }\nThis is the version of Java I am running with:\n$ java -version\njava version \"1.8.0_121\"\nJava(TM) SE Runtime Environment (build 1.8.0_121-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)\nBecause the map is empty by the time the shutdown hook runs, I don't have anything to send to the CacheWriter.delete method...\nFor shutdown processing of incoming requests, I wind up calling this after I have stopped all incoming requests and they have finished processing (I am able to determine that).\nOkay, in thinking about what you are saying around the executor, I take it that you suspect that the cache was actually flushed prior to it hitting the shutdown hook (because I check the contents of the cache right at the start of the shutdown hook code and it is empty already)? And that work is actually pending. But, because of the executor I am using, I don't wait and just shutdown?\nCould be... I did try adding a couple of Thread.sleep(5) type of statements in there as a course grained way to let stuff not in my thread to finish up - it did not help.\n. Okay I am closing this issue as user error. I managed to make TWO copies of the cache in two separate places of the app (there was JUST enough indirection that I did not realize the two were unconnected). My shutdown hook referenced the first one (which never got data) and the rest of the code referenced the second one. \nApologies for wasting your time - but sincerely appreciate your feedback.. If it is an consolation, the above Scala code works well for this use case - should anyone want to do something like that. Thanks again Ben.. ",
    "jandam": "Thank you for fast reply. I missed lambda expression.. Problem:\nLoadingCache.get returns value on which RemovalListener.onRemoval was already called.\nI want to use onRemoval for recycling resources. So I \"release\" the resource. But LoadingCache.get returns already \"released\" resource. I would expect to return new \"loaded\" instance.\nTomorrow I will try to send test case.. I suppose that I'm using latest version: 2.5.x. Have to check tomorrow. \nMartin\n\n\n\n2017 v 18:51, Ben Manes notifications@github.com:\n\n\n\n\nI just checked all calls to BoundedLocalCache#notifyRemoval and it is invoked after the computation completes. If @jandam is using a version prior to 2.4.0, there was a stale notification bug causing the listener to observe the wrong value. A similar problem could exist, so I'm not discounting having bugs and flawed tests. If he's able to reproduce it in a unit test then hopefully I can work it out from there and resolve it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Tested on 2.5.2 and 2.5.4. Attached test with CacheLoader. Same issue with cache.get(, )\ncaffeine-issue-177.zip\n. Thank you very much for detailed analysis. This is not Caffeine issue. This is design feature. Now I'm able to fix my code. Sorry that you wasted time on this. \n\nThank you very much.\n\n\n\n2017 v 18:21, Ben Manes notifications@github.com:\n\n\n\n\nIn your test, I think there is an inherent race that can't be solved at the cache. The live value can be read from the cache by T1, the entry evicted and released by T2, and still in use by T1. A context switch makes that entirely possible. Imagine if you reduced the cache size to zero, you would expect that to happen too. Changing your code to that and it fails without threads.\nThe cache size and eviction policy make all the difference in your test, since entries are loaded and not reused. If the cache size is large enough then the race won't occur because eviction is delayed just long enough to not crash. Switching to Guava works at your cache size but fails at smaller. That's due to using LRU so the accessed entry is delayed by O(n) operations. Caffeine may evict much sooner to avoid cache pollution, which increases the hit rate by favoring frequency over recency.\nIn your case you need to provide additional synchronization around the value to handle races on mutable state. A simple approach is to synchronize on the page during usage, validating it before use under the lock. If that is too slow, you could use reference counting where the page has the count of in-cache + readers, the removal listener decrements, and the last reader performs the final clean-up when reaching zero. You might consider phantom references for this to let the GC do that implicitly so that releasing is delayed until the value is unreachable, e.g. a proxy. Then use the referent to lookup the page and release it. There are other techniques like optimistic reads with validation afterwards (e.g. StampledLock). Regardless, you'll need to manage these types of races.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Sorry for late answer. \n\nI have nothing for FAQ. \nMaybe there should be several lines of code for write-through and write-back cache in Writer section. RxJava example is nice. \nhttps://github.com/ben-manes/caffeine/wiki/Writer \nThank you. \nMartin \nFrom: \"Ben Manes\" notifications@github.com \nTo: \"ben-manes/caffeine\" caffeine@noreply.github.com \nCc: \"jandam\" jandam@crcdata.cz, \"Mention\" mention@noreply.github.com \nSent: Friday, August 18, 2017 3:23:53 AM \nSubject: Re: [ben-manes/caffeine] LoadingCache::get returns value after invoking RemovalListener (#177) \n@jandam I began a FAQ , so please let me know as you encounter gotchas we should add. \n\u2014 \nYou are receiving this because you were mentioned. \nReply to this email directly, view it on GitHub , or mute the thread . \n. Thank you for fast reply. \nI will do more investigations on Monday. \nFrom: \"Ben Manes\" notifications@github.com \nTo: \"ben-manes/caffeine\" caffeine@noreply.github.com \nCc: \"jandam\" jandam@crcdata.cz, \"Author\" author@noreply.github.com \nSent: Friday, December 7, 2018 5:09:58 PM \nSubject: Re: [ben-manes/caffeine] Note: Eviction anomaly (#289) \nPR #263 adds the setting option. Unfortunately I haven\u2019t been able to work on this project but hopefully can catch up over the holidays. \nThere is a new research paper that corrects this problem by automatically tuning the window size based on the workload. I can email it privately if interested. I want to explore some further ideas along those lines. \n\u2014 \nYou are receiving this because you authored the thread. \nReply to this email directly, view it on GitHub , or mute the thread . \n. ",
    "rkluszczynski": "@ben-manes Hello, thanks a lot for your suggestion. I think I will try your way:) I'm also thinking about getting rid of spring abstraction. Today, I have managed to do some performance tests between version with only cache 1 and first attempt with second cache implementation (based on Spring's value loader). \nDefinitely have still something to work on, because request times have increased based on Apache Bench percentiles.\nI will let you know about results next week. Thanks a lot for your help!\nKind regards,\nRafal Kluszczynski. ",
    "adamw": "According to my experiments it blocks if a concurrent load is happening for a given key, see my test: https://github.com/softwaremill/cache-get-or-create/blob/master/src/main/java/com/softwaremill/TestCaffeine.java (the loader artificially sleeps for one second). Ah! True :) And even though invalidate returns immediately, the value is invalidated after the cache completes (test here: https://github.com/softwaremill/cache-get-or-create/blob/master/src/main/java/com/softwaremill/TestCaffeineAsync.java) - which was the original problem that lead me to Caffeine. \nI'll put a blog on that up tomorrow probably :) - thanks for the project. Managed to finish it today, here's the blog I mentioned: https://softwaremill.com/race-condition-cache-guava-caffeine/. Thanks for the pointer, adding a note to the blog.\nI think get is the correct one, as getIfPresent would suggest to skip the loading if the value isn't already in the cache?. Ah there, correct, thanks, I was thinking of the later examples :). ",
    "nmihajlovski": "No worries, I was happy to contribute a bit. :) I finally found the time to do it.. That would be nice, unless it's time-consuming, so I would consider it as optional.. That's correct - Caching.shutdown() isn't needed if TTL isn't set. Thanks for the help!. OK, thanks for the quick response and for the clarification.. ",
    "charlesxucheng": "Thanks for the fast response.\nI did a dependency check and found javax.cache.annotation.CacheResult annotation uses javax.enterprise.util.Nonbinding\nBut javax.enterprise.util.Nonbinding is not in the dependency chain.\n[info]   +-com.github.ben-manes.caffeine:caffeine:2.4.0\n[info]   +-com.github.ben-manes.caffeine:jcache:2.4.0\n[info]   | +-com.github.ben-manes.caffeine:caffeine:2.4.0\n[info]   | +-com.typesafe:config:1.3.1\n[info]   | +-javax.cache:cache-api:1.0.0\n[info]   | +-javax.inject:javax.inject:1\n[info]   +-org.jsr107.ri:cache-annotations-ri-guice:1.0.0\n[info]   | +-com.google.inject:guice:3.0 (evicted by: 4.1.0)\n[info]   | +-com.google.inject:guice:4.1.0\n[info]   | | +-aopalliance:aopalliance:1.0\n[info]   | | +-com.google.guava:guava:19.0 (evicted by: 20.0)\n[info]   | | +-com.google.guava:guava:20.0\n[info]   | | +-javax.inject:javax.inject:1\n[info]   | | \n[info]   | +-javax.cache:cache-api:1.0.0\n[info]   | +-org.jsr107.ri:cache-annotations-ri-common:1.0.0\n[info]   |   +-javax.cache:cache-api:1.0.0\n\nThis Guice test might be helpful.\n\nI think this is where I get the configureCachingProvider code from... However, what I need is to plug in a dummy implementation. I guess I can write one myself...\n\nI'd be weary of jcache outside of framework integrations. It has a brittle, limited api with lots of quirks. I don't think it was a positive step forward.\n\nYou mean it is not recommended to use JCache annotations outside of frameworks? I thought using such annotations is an easy way to enable caching instead of doing all the explicit gets and puts in the code... If not then what is a good way to enable caching without polluting my code with such cross-cutting concern? \nThanks!\n. Thanks Ben. I will explore what you have suggested.. ",
    "landawn": "Here are my opinions:\n\n\n\"puts a lot of pressure on the garbage collector_\"...\nWhat drives you to make the this conclusion? Personally, I don't see any pressure on performance/memory by my test/data/experiences. \nSecondly, Using the mapping function or CacheLoader to generate the return value when the key is not found in cache, it could be a good idea. but most likely not. Why? Usually it's the application responsibility to handle this scenario when the key is not cached. and there could be different strategies to handle null value even in the same system for the same cache, For example, some call may query the record  from db, some call may create a new object, some call may just return 'not found', in other words, the mapping function/CacheLoader won't help developers too much to handle non-cached key.\n\"...Collections Framework and those that love/hate Optional shouldn't feel too unhappy\u201c\nYou know, what make me having this question? it's get(k, mappingFunction) and getIfPresent(k), the Java 8 style. I won't ask this question if it's the traditional V get(key). Since Caffeine is targeting to Java 8 and above, Since Optional is widely used in JDK 8 or other libraries, avoiding it only makes the people who love Optional/Java 8 style unhappy. Meanwhile won't make the people who hate Optional happy because they have already been flooded by JDK 8. \n\n\n. I also looked into the main reasons.  Except the first one : Incompatibility with reference caching...(I don't understand it). My thoughts are pretty simple: Either V get(K key) or Optional<V> get(K key)is perfect to me. But the combination V get(K key, Function<? super K, ? extends V> mappingFunction) and V getIfPresent(K key)  doesn't give me the best taste.. Just talk about the benchmark, Could you run benchmark for below code?\npublic String get() {\n    return UUID.randomUUID().toString();\n}\n\npublic Optional<String> optionalGet() {\n    return Optional.of(UUID.randomUUID().toString());\n}\n\nIn most real use cases/scenarios where cache is used, if not all, the load will be much bigger than the code you ran for benchmark above. Probably there is only 0.0...1 speedup, if it's not zero. I think the benchmark you used to draw the conclusion is meaningless.\n. Thanks. You use HashMap as an example, but how about HashSet? back implemented by HashMap and more unnecessary objects (Map.Entry...) are created...\nMy point is that performance/GC should NOT be a reason of using Optional or not in this API design because the cost of creating/recycling Optional instance is so tiny and it can be totally ignored. \nDo you agree?. Good. \nThen let's talk about Optional<T>.  Personally, I disliked it too at the begin because I didn't feel it's as useful/good as what a few people described. After learned more, I found it's a good type to improve the design as long as it's used appropriately and start to use it in my own API design. I strongly agree with the quote: Java 8 Optional: How to Use it\nThe JSR-335 EG felt fairly strongly that Optional should not be on any more than needed to support the optional-return idiom only.\nAs you can see, Optional is adopted in the new introduced APIs in Java 8 (e.g. Stream API). And I think Map.get(K key) is a perfect example where Optional should be used. (But it can't be changed due to backward compatibility). We could continue to debate other items one by one... My point is actually very simple: Optional<V> get(K key) is not necessary to be better than V get(K key) in the design of Cache. But it's a way better than the combination of V get(K key, Function<? super K ,? extends V> mappingFunction) and V getIfPresent(K key).\n. Very Good!\nI think there are couple of reasons we don't see the big shift. 1) Keep backward compatibility, that means the existing JDK API can't be modified to adopt it and any libraries which need to support JDK 7 or older version can't use it either. 2) Without the lambdas support, There is no conviction for developer to adopt the functional interfaces/style, including Function/Predicate/Consumer and Optional. for example:\n// Traditional for loop \nList<Account> result = new ArrayList<>();\nfor (Account e : accounts) {\n    if (e.getFirstName().equals(\"jack\")) {\n        result.add(e);\n    }\n}\n\n// Stream API without Lambdas\nresult = accounts.stream().filter(new Predicate<Account>() {\n    @Override\n    public boolean test(Account account) {\n        return account.getFirstName().equals(\"jack\");\n    }            \n}).collect(Collectors.toList());\n\n// Stream API with Lambdas\nresult = accounts.stream().filter(e -> e.getFirstName().equals(\"jack\")).collect(Collectors.toList());\n\nI saw the some libraries which only support Java 8 and above start to use Optional and other Function APIs more and more. (e.g. Spring 5) and developers are writing more and more functional style code with  Java 8.. ",
    "wsargent": "Okay, so the advantage of the jcache layer here is that it's implementation independent and I can configure it from application.conf... the disadvantage is that it's hard to manage the overall size of the cache without some extra call back logic, and expiration can't be done per entry.  Does that sound about right?. > What do you mean by \"hard to manage the overall size of the cache without some extra call back logic\"?\nUsing a weigher for example:\nhttps://github.com/ben-manes/caffeine/blob/master/jcache/src/test/java/com/github/benmanes/caffeine/jcache/configuration/TestWeigher.java. If you keep it open, I'll have some suggestions on Monday.. Thanks.  I haven't forgotten about this -- there's been a number of fires to put out each day today and we're looking at RC1 this week.... ",
    "ahasani": "Cant help but to comment that at the current version of RocksDB java5.15.10 random lookup is still a lot slower than the venerable Caffeine of @ben-manes. RocksDB was at ~200,000 while Caffeine ~1,000,000 ops. I use RocksDB for persistence and sorted key iterations and range scan which is what its best at. Data was 1 million with 1kb value so this was in memory operation for RocksDB too. We are yet to see how the hash index would improve this on RocksDB's side. If you do random lookup use Caffeine cache mate.. Like other cache/map implementations using sun.misc.Unsafe we can use --add-exports java.base/sun.nio.ch=ALL-UNNAMED jvm option for now. ",
    "jatcwang": "I think the blocking semantics you mentioned here makes total sense given that's very common in the Java ecosystem. I think having the option to choose between a blocking interface or a non-blocking one will prove to be beneficial for everyone.\nIs this simply just exposing LocalAsyncLoadingCache's cache variable? If o I'm happy to contribute a patch. ",
    "laurentvaills": "I tried both the point in time and a duration before submitting the issue but none worked. \nIndeed, I also think that there may be a bug that calls notify on update instead of create.. I confirm it works with a synchronous cache. (And I see the trace for ExpireAfterCreate). \n. Thanks for the update and for your working chasing that bug. \nI can wait until next week for a new release or pull-request to try. . I confirm that is working for me. \nThanks for your reactivity.. ",
    "ahadadi": "Thanks for implementing that!\nI didn't get that bit:\n\"Also, I presume that the duration is only set if the write was successful? As in, putIfAbsent will not modify the time if failing to a read. And in that case, would a read delegate to the configured expiry or have no effect?\"\nAre you referring to a race between a read and putIfAbsent that both try to (only) update the expiration? Or to a race where the read is trigerring value loading? or something else?\nNote\u00a0that the javadoc for putIfAbsent return value is inconsistent with what you ended up implementing.\nI guesss that the root cause of your API doubts is that LoadingCache & AsyncLoadingCache have different semantics for put / putIfAbsent. Also AsyncLoadingCache has a void put method with CompletableFuture, while its sync counterpart does not.\nThe most consistent implementation is to split VarExpiration to a sync and async version, and keep the same semantics as in their non expiration setting counterparts.\nDo you think it's worth the extra API surface area?. ",
    "wkuranowski": "What do you think about adding similar method to getQuiet from Ehcache 2?\n. I think that LRU is good enough in my case. Recent files are very important. Old, inactive files were most likely processed by all of my services and could be removed. Even if that race will happen sporadically, it's not a problem, because operation will be repeated later. So, for me LRU is the best option.\nPlease consider adding this eviction policy.\n. I need a cache, because I don't know how many operations will be performed on a single file. They are triggered by users. Older files are less likely to be reused.\nGuava is missing an asynchronous interface, so Caffeine with LRU is the best combination :)\nI know that you want to provide single best policy for every use case. But sometimes it's not possible. In my case inactive entry that was frequently used in the past is the best candidate for eviction.\n. I think this would work for me too.\n. Thank you, it works! :)\n. ",
    "arnaudroger": "nevermind, realised it's not relevant to return type of build().. ",
    "lauredogit": "Hi Ben,\nThe strong reference is held by com.github.benmanes.caffeine.cache.References$WeakValueReference\n```java\n/*\n   * The value in a cache that holds values weakly. This class retains a reference to the key in\n   * the advent that the value is reclaimed so that the entry can be removed from the cache in\n   * constant time.\n   /\n  static final class WeakValueReference extends WeakReference\n      implements InternalReference {\n    private final Object keyReference;\npublic WeakValueReference(@Nonnull Object keyReference,\n    @Nullable V value, @Nullable ReferenceQueue<V> queue) {\n  super(value, queue);\n  this.keyReference = keyReference;\n}\n\n@Override\npublic Object getKeyReference() {\n  return keyReference;\n}\n\n@Override\npublic boolean equals(Object object) {\n  return referenceEquals(object);\n}\n\n@Override\n@SuppressWarnings(\"PMD.UselessOverridingMethod\")\npublic int hashCode() {\n  return super.hashCode();\n}\n\n}\n```\nThe keyReference field is a strong reference and prevents the GC from reclaiming the keys, which created our leak.\nIf I understood correctly, you need the strong reference for the com.github.benmanes.caffeine.cache.BoundedLocalCache#drainValueReferences method, unfortunately if the cache does not call this method sufficiently, the memory leak happens.. Well, it seems so.\nIn the com.github.benmanes.caffeine.cache.BoundedLocalCache#put(K, V, boolean, boolean),\nYou are creating new Nodes with\njava\n          node = nodeFactory.newNode(key, keyReferenceQueue(),\n              value, valueReferenceQueue(), newWeight, now);\nand with the WW NodeFactory it does:\n```java\n  WW {\n     Node newNode(K key, ReferenceQueue keyReferenceQueue, V value,\n        ReferenceQueue valueReferenceQueue, int weight, long now) {\n      return new WW<>(key, keyReferenceQueue, value, valueReferenceQueue, weight, now);\n    }\n<K, V> Node<K, V> newNode(Object keyReference, V value, ReferenceQueue<V> valueReferenceQueue,\n    int weight, long now) {\n  return new WW<>(keyReference, value, valueReferenceQueue, weight, now);\n}\n\n<K> Object newLookupKey(K key) {\n  return new LookupKeyReference<K>(key);\n}\n\n<K> Object newReferenceKey(K key, ReferenceQueue<K> referenceQueue) {\n  return new WeakKeyReference<K>(key, referenceQueue);\n}\n\n},\n```\nwhich creates a WW Node:\n```java\n  static class WW implements Node {\n    protected static final long KEY_OFFSET = UnsafeAccess.objectFieldOffset(WW.class, \"key\");\nprotected static final long VALUE_OFFSET = UnsafeAccess.objectFieldOffset(WW.class, \"value\");\n\nvolatile WeakKeyReference<K> key;\n\nvolatile WeakValueReference<V> value;\n\nWW(K key, ReferenceQueue<K> keyReferenceQueue, V value, ReferenceQueue<V> valueReferenceQueue,\n    int weight, long now) {\n  UnsafeAccess.UNSAFE.putObject(this, KEY_OFFSET, new WeakKeyReference<K>(key, keyReferenceQueue));\n  UnsafeAccess.UNSAFE.putObject(this, VALUE_OFFSET, new WeakValueReference<V>(key, value, valueReferenceQueue));\n}\n\n```\nAnd in the WW Node, the key is currently not being wrapped into a WeakReference in\nnew WeakValueReference(key, value, valueReferenceQueue)) .\nRegarding the issue, thank you very much for your prompt reaction.\nBut no hurry, you can take your time, we are currently applying a workaround in production.\nAnd congratulations for Caffeine, by the way, it's really nice!. Hi Ben,\nGreat news, this is indeed the needed fix for the WW Node! \nAs you said , you should also apply the same fix to the WSo Node using weak keys and soft values.\n```java\n  static class WSo implements Node {\n    protected static final long KEY_OFFSET = UnsafeAccess.objectFieldOffset(WSo.class, \"key\");\nprotected static final long VALUE_OFFSET = UnsafeAccess.objectFieldOffset(WSo.class, \"value\");\n\nvolatile WeakKeyReference<K> key;\n\nvolatile SoftValueReference<V> value;\n\nWSo(K key, ReferenceQueue<K> keyReferenceQueue, V value, ReferenceQueue<V> valueReferenceQueue,\n    int weight, long now) {\n  UnsafeAccess.UNSAFE.putObject(this, KEY_OFFSET, new WeakKeyReference<K>(key, keyReferenceQueue));\n  UnsafeAccess.UNSAFE.putObject(this, VALUE_OFFSET, new SoftValueReference<V>(key, value, valueReferenceQueue));\n}\n\nWSo(Object keyReference, V value, ReferenceQueue<V> valueReferenceQueue, int weight, long now) {\n  UnsafeAccess.UNSAFE.putObject(this, KEY_OFFSET, keyReference);\n  UnsafeAccess.UNSAFE.putObject(this, VALUE_OFFSET, new SoftValueReference<V>(keyReference, value, valueReferenceQueue));\n}\n\n```\n=>\njava\n    WSo(K key, ReferenceQueue<K> keyReferenceQueue, V value, ReferenceQueue<V> valueReferenceQueue,\n        int weight, long now) {\nthis(new WeakKeyReference<K>(key, keyReferenceQueue), value, valueReferenceQueue, weight, now);\n    }. ",
    "davsclaus": "Hi\nI can only second what Viktor says. Awesome help and support from Ben. We\nare happy to use Caffeine in Apache Camel in both the camel-core, and now\nin the upcoming camel-caffeine component as well:\nhttps://github.com/apache/camel/tree/master/components/camel-caffeine\nOn Thu, Jul 6, 2017 at 8:49 AM, Viktor Szathm\u00e1ry notifications@github.com\nwrote:\n\nJust an off-topic remark: wow @ben-manes https://github.com/ben-manes\n:) Beyond building the best caching library, your level of attentiveness is\nexemplary. Thanks for your efforts!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/167#issuecomment-313311221,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAdHrJ7Ua45QoEAfQMJ8i-U7lW7sp7jYks5sLINwgaJpZM4OFdj4\n.\n\n\n-- \nClaus Ibsen\n\nhttp://davsclaus.com @davsclaus\nCamel in Action 2: https://www.manning.com/ibsen2\n. ",
    "DavyLandman": "I have to say, if all library authors were as proactive as @ben-manes, we would be a lot less weary of new dependencies. So thanks for the report, updating today or tomorrow (after update release).. just an update, as @johnou marked, the bugfix was backported to jdk8-121 and jdk9-b152, so maybe you could reintroduce the method handles?\nhttps://bugs.java.com/bugdatabase/view_bug.do?bug_id=8162795\n\n. ",
    "piegamesde": "Thank you. With its overflow mechanics, it seems to match my needs. If you care about StackOverflow, you can get 25 easy credits by answering my question with a MWE (otherwise I'll do it once I know the library).. ",
    "nickrobison": "Awesome. Thanks for the quick response!. I think in general it still checks for that, though I'm not seeing any warnings during build.\nPerhaps a better solution would be to return from each case statement?. Regarding the 'no-evictions' issue, that's an error on my part. I didn't put together the fact that evictions were async, so my test code was flawed.\nRegarding the max size, I can see your new changes, but the problem seems to be that the newly merged defaults are not getting returned from  resolveConfigurationFor in CacheFactory. It looks like the configuration variable is of type MutableConfiguration which is a subclass of CompleteConfiguration` is thus stepping into the if block on line 95, which simply returns the configuration with only the copier factory added from the defaults. \nIf I modify the method as follows:\n```\nCaffeineConfiguration defaults = TypesafeConfigurator.defaults(rootConfig);\n    if (configuration instanceof MutableConfiguration<?, ?>) {\n      defaults.setTypes(configuration.getKeyType(), configuration.getValueType());\n      defaults.setStoreByValue(configuration.isStoreByValue());\n      return defaults;\n}\nCaffeineConfiguration<K, V> config = new CaffeineConfiguration<>(\n        (CompleteConfiguration<K, V>) configuration);\nconfig.setCopierFactory(defaults.getCopierFactory());\nreturn config;\n\n```\nThen I can see a new cache being created with the merged values from the config file, of course, that change breaks the TCK tests, as I'm not sure the difference between the two JCache config types.. The way I've been approaching my caching implementation, is to set as many properties as possible through the JCache API (into a MutableConfiguration) and then have caffeine specific implementation details be filled in via the reference.conf file. I'm also willing to modify my approach to what seems best to you. I'll admit to being a relatively new Java developer, with minimal production experience.\nHere's a quick Unit test I put together, which is how I've implemented my JCache configuration, as well as how I've been thinking about the problem.\n```\npublic class JCacheCaffeineConfigurationTest {\n    private static final String PROVIDER_NAME = CaffeineCachingProvider.class.getName();\n    private CacheManager cacheManager;\n    private MutableConfiguration cacheConfig;\n@BeforeClass\npublic void beforeClass() {\n    final CachingProvider provider = Caching.getCachingProvider(PROVIDER_NAME);\n    cacheManager = provider.getCacheManager();\n    cacheManager.destroyCache(\"cache-not-in-config-file\");\n    cacheConfig = new MutableConfiguration<>();\n    cacheConfig\n            .setTypes(String.class, String.class)\n            .setStatisticsEnabled(true);\n}\n\n@Test\npublic void validMaximumSize_createCache() {\n    checkCaffeineConfiguration(() -> cacheManager.createCache(\"cache-not-in-config-file\", cacheConfig), 500L);\n    checkCaffeineConfiguration(() -> cacheManager.createCache(\"test-cache-2\", cacheConfig), 1000L);\n}\n\n@Test\npublic void validMaximumSize_getCache() {\n    checkCaffeineConfiguration(() -> cacheManager.getCache(\"cache-not-in-config-file\", String.class, String.class), 500L);\n    checkCaffeineConfiguration(() -> cacheManager.getCache(\"test-cache-2\", String.class, String.class), 1000L);\n}\n\nprivate void checkCaffeineConfiguration(Supplier<Cache<?, ?>> cacheSupplier, Long expectedValue) {\n    final Cache<?, ?> cache = cacheSupplier.get();\n\n    @SuppressWarnings(\"unchecked\")\n    final CaffeineConfiguration configuration = cache.getConfiguration(CaffeineConfiguration.class);\n    assertThat(configuration.getMaximumSize(), is(expectedValue));\n}\n\n}\n``. Also, major thanks for being so responsive on this issue! Sorry if I've interrupted other development plans.. That makes sense. Well that's for being flexible with me. I think your changes look great. Ideally it would be nice to pull the named cache configs, if possible, and then fall back to the default. But that's something I can work on for a separate pull request.. Not sure I'm following. Are you saying it's wrong to expect the cache to have a maximum size of 1000, as described in the typesafe config file? . Ok, I think I'm tracking now. The EHCache approach seems to make the most sense. If we need to instantiate any new caches during the program execution (by callingcreateCache`), they'll use the provided caffeine.jcache.default.* values, correct?\nIf so, that seems like the appropriate way to go.\n. But regardless of when they instantiate the caches, if you have a named cache in the config file, you need to call getCache instead of createCache, correct?. Hah! And now you get to decide how your implementation will differ, fun for you!\nI actually don't have a preference as when the caches are resolved (either eagerly or lazily), but clarifying that a named cached (in the config file) will result in a CacheException if you call createCache on it would probably be good, and resolving from the default configuration for any other caches does seem like a good practice.\nSorry if that's not a very helpful response.. ",
    "ipnm": "I also think that JDK behaviour is at least strange. There is a comment for setValue that could justify such behaviour:\n\nV setValue(V value)\nReplaces the value corresponding to this entry with the specified value (optional operation). (Writes through to the map.) The behavior of this call is undefined if the mapping has already been removed from the map (by the iterator's remove operation).\n\nBut I presume it means the explicitly deleted entry cannot be edited. Will ask JDK guys.. I've submitted a bug report. Will give an update once have a feedback.. No updates so far. Ticket created:\nhttps://bugs.openjdk.java.net/browse/JDK-8186171. Thanks for a fix! It's not critical for me so feel free to release later.. ",
    "lifoveBible": "I'm just studying the code and learned a lot from your detailed explanation. Thanks a lot!. ",
    "itmachina": "I tried using jmap to check the amount in the cache\uff0cAt peak times\uff0cabout 410000  in the cache.\nI waited for an afternoon,but nothing has changed,there is no sign of declining memory usage.\nI'm trying to use the timed task to call cleanup.. java version \"1.8.0_77\". I will hava a try.\nThank you very much,. I did some testing and found no problem with eviction.\nI found that  off-heap memory taking up a lot of memory.\nThe pmap report shows that there are many memory sizes of 504k.\nI try to modify MALLOC_ARENA_MAX, but there is no effect.\nnative memory overflow?maybe...I'm not sure what caused it.\n:broken_heart: . ",
    "kdombeck": "Thanks for the quick turnaround. After looking into this further it looks like if we use Caffeine with JCache we can only get JCache JMX stats and not the native Caffeine stats, so I won't be able to register this with Prometheus.. It was the load times that would be really useful. Plus the times in JCache appear to be kinda useless from a Prometheus standpoint since it returns the average rather than the raw total time.\nI am going to work on a generic JCache module for Prometheus and create a PR if it looks good. \nThanks for your help.. Makes complete sense. Again thank you. No timeframe on my side. Thank you for being so responsive with issues. Truly you are the most responsive of any open source project I have ever worked on. \nThanks again.. The only thing I can think of is enabling the native Caffeine statistics. The stats in Caffeine are a lot better than the ones exposed in JCache.. Works perfectly!! Thank you. ",
    "christian-esken": "A Cache would work @boschb, but an eviction listener would normally not give you perfect FIFO ordering. You can get close though, by implementing your own eviction algorithm. Triava Cache allows custom eviction implementation, and you could use insertion time and tiebreaker to get very close to FIFO.\nYour implementation could look about this (tiebreaker is by default automatically applied):\npublic class FifoEvictor<K,V> extends FreezingEvictor<K,V> {\n  long getFreezeValue(Integer key, TCacheHolder<Integer> holder) {\n     return holder.getCreationTime();\n  }\n}\nSee https://github.com/trivago/triava/blob/master/src/examples/java/com/trivago/examples/CacheJSR107Example.java#L178 for a more complete example.\nI am using such a solution with about 1000 writes per second on https://www.trivago.com/ .\nPlease be aware that (as usual) eviction listeners can block adding entries to the queue. If you do not want writes to block, you could discard if it is acceptable in your scenario.. About the consumer(s) polling. The consumers - as I understand that word - are the eviction listeners. As a listener, they do not do a full scan. The full scan exists, but this is done by the background eviction thread. Neither Threads writing to the Cache nor consumers take any penalty due to the background work (no blocking, and not even amortized CPU costs).\nIn general I understand that boschb is now looking into the concurrent ring buffer you proposed. Without knowing the exact use case I think the ring buffer is the most appropriate way.. I am mainly interested in core benchmarks that can be run for most caches, like GetPutBenchmark. Once  knowing the name of the class (GetPutBenchmark) it gets easier. I am not yet familiar with the gradle jmh plugin to spot that benchmarkParameters is applied to \\@Param. Thanks for clarifying, this makes it very easy.\nWhat do you think of mentioning the gradle 4.0 requirement?. I tried to run it within IntelliJ IDEA via \"use default gradle wrapper\" and that failed. Only when pointing IntelliJ to a local gradle 4.0.2, I got it working. Its not really worth keeping this issue open for that, though I personally tend to run gradle on server machines directly instead of the wrapper. I will be offline for a couple of days, and if you like you can close this issue. I will try to reproduce the gradle trouble when I get back, and will come back to you if I can reproduce it reliably.. You ran it with triava v1.0.3, right? There has been 2 performance issues that I fixed in v1.0.4. Still there are issues in the GET that I cannot explain, even using YourKit. I will try to see what your CacheProfiler is about. Performance should be close to ConcurrentHashMap at least for GET's, and I will benchmark that. Also I want to plugin another backing Map, for some fun and insight I thought I might try ConcurrentLinkedHashMap.. FYI: I today tested plenty in YourKit with \"CPU, sampling\" and \"CPU, tracing\", but the results were misleading. Then I simply scanned code and you are right. I am thrashing on AtomicLong in the cache statistics. I moved to LongAdder, boosting the readonly benchmark from 30 Mio/s to 80 Mio/s. That is going in the right direction. :-). ",
    "ksperling": "Thanks for the lightning fast response.\nI guess the bit that puzzled me is why my separate lock doesn't protect me from this case. So what's happening is that the thread doing the eviction does a computeIfPresent, my Writer.delete method invalidates under my external lock (but has no way of holding that lock long enough until the entry is actually no longer visible in the cache). Then a reader comes along, takes the external lock and does a plan old get which can return the stale entry because it's not a compute* call?\nIn my real code the external lock is actually a ReadWriteUpdateLock, with most read and write operations just taking a shared lock during most of the disk IO, so I can't easily do away with this in favour of just letting the Cache do the locking. Just using a no-op compute instead of get may work though.\nThe other thought I had was to maybe use a RemovalListener instead of a CacheWriter, presumably this is called after an entry is guaranteed to no longer be visible from the cache.  The main issue there might be preventing the callback from clobbering a file that has been evicted but loaded again before the removal listener gets run.. Just tried using compute in the test code, and it ends up deadlocking due to the two threads both acquiring the external and entry locks, but in the opposite order. So it seems your suggestion of get and retry is the only feasible approach.\nThanks for your help!. Just thought I'd mention how I solved this in the end in case anyone runs across a similar scenario:\nInstead of a CacheWriter I'm using a RemovalListener. Within the listener I acquire my external lock as before, and then check if the key has been re-added using cache.getIfPresent(key) == null. In my loading function I also handle the case of \"finding\" that the value I'm about to load actually hasn't been evicted yet; the actual loading of the data can simply be skipped in that case.. ",
    "lvc": "Hi,\n\nAny recommendations for where you would want this reported?\n\nOne can just open the issue tracker, find this issue by the ABI keyword and go to the report by the link provided to see reports for the latest releases or beta versions.\n\nIt\u2019s more a development tool like static analysis, which I assert in CI but don\u2019t publish.\n\nYou can create the text ABI dump file for one of the past versions (say, 2.0.0) and save it in the source tree:\njapi-compliance-checker -l caffeine \\\n-dump caffeine-2.0.0.jar \\\n-dump-path caffeine-2.0.0-API.dump\n\nAnd then compare it with the latest API dump in the CI to generate report on compatibility (if you are expecting API changes):\njapi-compliance-checker -l caffeine \\\n-dump caffeine-latest.jar \\\n-dump-path caffeine-latest-API.dump\n\njapi-compliance-checker -l caffeine \\\n-old caffeine-2.0.0-API.dump \\\n-new caffeine-latest-API.dump \\\n-report-path ./compat_report_2.0.0_to_latest.html\n\nThank you.. ",
    "yanquanyu": "Yes, Thanks a lot !!\nCan I use cache like this?\nAtomicInteger atomicInteger = cache.get(key, s -> new AtomicInteger(0));\nint sum = atomicInteger.addAndGet(1);\nreturn sum <= 5;. Thank you very much!. ",
    "perlambaek": "So we kinda stumbled on this trying to debug an issue, where our caches seem to get stuck, unable to ever really return any value. We are not entirely sure this is the cause, but still felt this was odd, we will do more testing for this issue with the expireAfterWrite, and see if the issue persists.\nAs for the use case: in our case we have some work that is normally pretty fast (<1 second), but during certain peak times, our work may suddenly take up 90 seconds to complete instead of 1 second, due to unavailable database connections, I/O and so on. This leads us to exceed our expireAfterWrite (not by design, and not something we desire.), even though we in 99.999% of the time are well within it. I am then guessing we get a whole chain put together of threads wanting to recompute the value, putting our system to a grinding halt, as no cache value ever really seems to be produced in that case. (We are taking steps to reduce the 90 seconds, as this is also not exactly desired functionality, but the contract of expireAfterWrite still seems broken.)\nWe often support peak loads of >500 pageviews/sec so having 50 threads hitting the caches at 100ms interval is not at all unusual.\nI am not at all into the inner workings of this, but it seems to me that when 1 thread is done recomputing, all others waiting for the result should be given the result of that computation, no matter if the expiration has been reached or not, as this will surely be valid since it was just produced, and thus cannot have exceeded the expireAfterWrite unless it was effectively 0? Subsequent access would then be able to read a correct writeTime if it was set at the computation end?\nPlease let me know if you need more details or something is unclear.. Yea I think we can work around it, so no rush on our account. Thank you very much for the swift response, and a very great library!. ",
    "aukevanleeuwen": "Thanks for your elaborate response. I think I can follow most of it, and it makes sense that for this to be a general purpose (if you will) cache, there are bound to be some things falling outside the scope of a cache. I don't think I have a very strong idea on this definitely belonging in the scope of a cache considering I really don't oversee all of the drawbacks this might cause on the cache as a whole. \nHaving said that I do think the documentation in that sense is a bit misleading:\n\nSynchronous Listeners\nA CacheWriter may be used to publish to synchronous listeners.\nA synchronous listener receives event notifications in the order that the operations occur on the cache for a given key. The listener may either block the cache operation or queue the event to be performed asynchronously. This type of listener is most often used for replication or constructing a distributed cache.\n\nI think especially that last sentence made me pretty sure that both writes and deletes would be synchronous. How else would a distributed cache know for example about the explicit eviction from another (cache) node? Maybe I'm not seeing this through though. \nI didn't really like the scheduled Cache.cleanUp(), that seems a bit too hack-ish to me. Also I would know what it would impose on the cache if I would schedule this every 100ms or so?\nIn the end I now schedule the task in a ScheduledExecutorService after adding it to the cache to check explicitly if the CompletableFuture was already handled or not:\nthis.timeoutExecutorService.schedule(() -> {\n        if (!future.isDone()) {\n            log.warn(\"Didn't receive a response on the topic {} within {}ms for payment {}.\", topic, timeoutInMs, key);\n            value.completeExceptionally(new TimeoutException(\"Didn't get a response within \" + timeoutInMs + \"ms.\"));\n        }\n    }, timeoutInMs, TimeUnit.MILLISECONDS);\nI'm kind of disliking the fact that I'm running this task for every request (and not only the ones that are actually timing out), but the intent is pretty clear at least and I don't expect thousands of requests per second yet. Thanks for the timer wheel read, interesting and I keep that in mind if I need the performance later on.. ",
    "oscerd": "I guess the original position for Camel is good as is, maybe adding a note about camel-caffeine somewhere in the docs can be useful too.. Do you mean something like this?. Great thanks :-). ",
    "agoston": "I've checked ReentrantLock vs. NonreentrantLock, but in the end, they both end up in AbstractQueuedSynchronizer.acquire(), which is where the problem occurs.\nAll the stacktraces are in this code path, there is no difference. I do get a deadlock warning from JVM (this is the full output, the other 2 threads are not relevant here, they are waiting on locks held by the 2 threads deadlocked above):\nFound one Java-level deadlock:\n=============================\n\"ajp-0.0.0.0-8109-128 (-jIkyghhBePEsLll9ic8a19e79f354)\":\n  waiting to lock monitor 0x00007f73f881a308 (object 0x0000000508b1d908, a com.bol.order.CustomOrder),\n  which is held by \"ajp-0.0.0.0-8109-78 (-jIkyghhBePEsLll9i5dnGr65Dx8PfahGe2gAABxE)\"\n\"ajp-0.0.0.0-8109-78 (-jIkyghhBePEsLll9i5dnGr65Dx8PfahGe2gAABxE)\":\n  waiting for ownable synchronizer 0x000000032a600bb8, (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync),\n  which is held by \"ajp-0.0.0.0-8109-36 (P6V7aMaZbbXkfaYzYzkTBzHO7PeW5fQUKx8QAAAao)\"\n\"ajp-0.0.0.0-8109-36 (P6V7aMaZbbXkfaYzYzkTBzHO7PeW5fQUKx8QAAAao)\":\n  waiting to lock monitor 0x00007f73d40846a8 (object 0x000000048855c020, a com.bol.order.CustomOrder),\n  which is held by \"ajp-0.0.0.0-8109-59 (P6V7aMaZbbXkfaYzYzXxID0tvHylFQJumanAAABmA)\"\n\"ajp-0.0.0.0-8109-59 (P6V7aMaZbbXkfaYzYzXxID0tvHylFQJumanAAABmA)\":\n  waiting for ownable synchronizer 0x000000032a600bb8, (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync),\n  which is held by \"ajp-0.0.0.0-8109-36 (P6V7aMaZbbXkfaYzYzkTBzHO7PeW5fQUKx8QAAAao)\"\nJVM version is 1.8.0u131.\nCache configuration:\n@Bean\n    public Cache myFluffyCache() {\n        return Caffeine.newBuilder()\n                .maximumSize(10_000))\n                .expireAfterWrite(1, TimeUnit.DAYS)\n                .recordStats()\n                .build();\n    }\nWe can not reproduce this bug, it only happens on production, very sporadically (a few times a week, then nothing for months, then returns to few times a week).\nI was thinking of replacing evictionLock construct by a simple synchronized(this). I'm not sure what the upside of using reentrantlock is, but is sure more error-prone. Make sure you check out this document:\nhttps://bugs.openjdk.java.net/browse/JDK-8156765\nwhich, in turn, is caused by a serious design flaw in java.util.concurrent that is only fixed by an epic hack in java9:\nhttps://bugs.openjdk.java.net/browse/JDK-7011862. > Do your logs include any warning like Exception thrown when submitting maintenance task?\nNo, I see nothing of the sort.\nForcing the reentrantlock is not so simple, as the code in BoundedLocalCache goes like this:\njava\n    evictionLock = (builder.getExecutor() instanceof ForkJoinPool)\n        ? new NonReentrantLock()\n        : new ReentrantLock();\nSo the recommended FJP::commonPool() would still use a NonReentrantLock.. \ud83d\udc4f, neat :)\nI will push a test to production. Note that since this issue is not reproducible manually, I won't be able to say if it's fixed or not.. @ben-manes \ncorrect, we are running under tomcat (jboss to be even more precise). The thread names are telling it already (ajp-* are tomcat's AJP worker threads).. I wonder if we work at the same company @levischuckeats :) we do the same thing, we do a rolling restart only, never redeploy.. @ben-manes I'm right here, just can't add anything -- because of season peak load, we allow critical fixes on production only. The synchronized{} hack I've outlined in OP held up well (and it was a non-performance-critical path, too).\nWe've got about 50 caffeine Cache instances, and only this very specific one shows issues. I'm unsure why this happens, there seems to be some deeper issue. See, before caffeine, we used ehcache, which was replaced to get rid of a weird deadlock in there, at the very same code path. But looking at the code in ehcache (v2.10), it was totally explainable -- it had infinite while(true) {} in there at key locations.\nIn the new year, we're beginning to kill off this code path completely (not because of this -- we're migrating away from jboss and similar EE-style solutions), so I don't think I will be able to further reproduce this problem.\nAs for the OS, it's a CentOS7, on 32 core xeon's, some insane amount of ram, and an uber-tuned GC (java8 + CMS, tuned for best throughput and low GC overhead). There are multiple such JVMs running on one physical box.. I was always under the impression that it is a JVM/other bug, so I don't mind.\nThanks a lot for your investigation and support! And of course for this awesome library.. I am still under the impression that this is caused by the JVM bugs outlined in https://github.com/ben-manes/caffeine/issues/203#issuecomment-344542136\nThe fact that this happens sporadically under heavy contention makes it very hard to pinpoint.\nI can confirm that ehcache 2.10 had the exact same issue, that was the point to begin migration to caffeine.\nFor us, with the synchronized() hack at the contention points, it works OK. Note that we have not seen this issue at any other place besides that one despite caffeine being used in hundreds of other places and purposes.. Just happened again for us with caffeine 2.6.2.\nat sun.misc.Unsafe.park(Native Method) \nat java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) \nat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836) \nat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870) \nat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199) \nat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209) \nat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285) \nat com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1106) \nat com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:1027) \nat com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1664) \nat com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1612) \nat com.github.benmanes.caffeine.cache.LocalManualCache.put(LocalManualCache.java:64). Nah, I know there's not much we can do, this is more to keep track for other unfortunate souls.\nI still believe this is a JVM/kernel bug, not a caffeine bug.\nFor us, java version is 1.8.0_161 and the kernel version is 2.6.32-696.23.1.el6.x86_64.. JVM options are really super complex (a CMS GC tuned to the teeth), but I'm assured there was no OOM situation.. ",
    "levischuckeats": "Commenting here, as we are also affected. It seems to only happen on production and at odd hours. Today (the third event) happened on a server with little to no load at these hours.  \nThe stack trace is the same as far as I can tell above. This was with version 2.5.5.\n- parking to wait for  <0x00000002cdaff260> (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync)\n        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n        at com.github.benmanes.caffeine.cache.NonReentrantLock$Sync.lock(NonReentrantLock.java:315)\n        at com.github.benmanes.caffeine.cache.NonReentrantLock.lock(NonReentrantLock.java:78)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1096)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:1017)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1655)\n        at com.github.benmanes.caffeine.cache.BoundedLocalCache.put(BoundedLocalCache.java:1602)\n        at com.github.benmanes.caffeine.cache.LocalManualCache.put(LocalManualCache.java:64)\nWe use an async weigher which uses a pessimistic (largest object of that class so far) at first, then in  a thread pool it will re-insert the object with a thread local set, such that it is actually weighed within that thread pool. So most objects are inserted twice under the same key.\nAsyncWeigher<Object, Object> weigher = asyncReference.get();\n            cache = Caffeine.newBuilder()\n                    .maximumWeight(1024l * 1024l * 1024l * GIGS_OF_CACHE)\n                    .weigher(weigher)\n                    .build();\n            weigher.setCache(cache);\n. Hello @ben-manes \nDue to the holidays, we won't be deploying significant code changes this week, so I won't be able to try this out.\nAs for IllegalMonitorStateException, there are no error logs that include that in the stacks from any of our servers. \nWe currently use the following functions (duplicates for specific types that we use, if it makes a difference):\n invalidate(Long)\n invalidate(Object)\n put(String, Object)\n put(Object, Object)\n invalidateAll()\n getIfPresent(Long)\n getIfPresent(String)\n getIfPresent(Object)\n getAllPresent(Collection)\n .asMap().remove(Long)\n. invalidateAll in our code is rarely used and really only comes to play in a single threaded situation where the server just finished running migrations before becoming available to the outside world. \nSo I wouldn't focus on that.. While preparing a censored jstack to share (if you so wish to see, reach out to levi.schuck@eatstreet.com ), here are the counts I saw.\n\n378 threads were stuck on com.github.benmanes.caffeine.cache.LocalManualCache.put\n136 threads were stuck on com.github.benmanes.caffeine.cache.BoundedLocalCache.remove\n4 threads were stuck on com.github.benmanes.caffeine.cache.LocalManualCache.invalidate\n9 threads were stuck on com.github.benmanes.caffeine.cache.BoundedLocalCache.replace\n\nThere was one thread in a fork join pool that was made to do a clean up, but it didn't get anywhere\nat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n    at com.github.benmanes.caffeine.cache.NonReentrantLock$Sync.lock(NonReentrantLock.java:315)\n    at com.github.benmanes.caffeine.cache.NonReentrantLock.lock(NonReentrantLock.java:78)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1096)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask.run(BoundedLocalCache.java:2963)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache$PerformCleanupTask.exec(BoundedLocalCache.java:2952)\n. Interestingly.. as I was censoring the jstack and getting those numbers, the same server had a deadlock with caffeine again. . It's really odd, because we have not updated in a while but it is only happening this last week and now.\nThese servers are running with about 10GB of cache (GIGS_OF_CACHE being set to 10 in the builder above), and are running with java -version\njava version \"1.8.0_144\"\nJava(TM) SE Runtime Environment (build 1.8.0_144-b01)\nJava HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode). @jbduncan\nI don't think so, the folder our java bins are in is named java-8-oracle, the COPYRIGHT mentions oracle in name, there's a release file that says commercial under build type. The README just points to java.com\n. I understand the back pressure, though when the CPU for all cores is near 0, I don't think work is being done \ud83d\ude03 \nFor our use case (an intermediate cache for entities, which are backed by the database), we'd prefer things getting dropped, rather than being blocked.. On our end, we actually kill java when we redeploy, so every time Tomcat is running for the first time in the JVM.. I was planning to add metrics to our cache wrapper anyway \ud83d\ude03 \nAs for the asMap stuff, I don't think we do any call to synchronous. Our use case is return cache.asMap().remove(key) != null; for a delete call to our wrapper.\nI wasn't behind the original writing as to why asMap is used. . It looks like there's no invalidate / remove method exposed by Caffeine directly that returns whether or not the key was found and subsequently evicted. If that asMap functionality has a bug there, that may explain what we are seeing with one or two servers dying at the same time.\nWe have server-local caches that rely on Caffeine for entities. To prevent stale data from reaching the end user, we have a pub-sub bus that every server publishes to when it updates an entity, all other servers listen and drop the entity from the local cache so the next load goes to the database. It really isn't the most efficient or smart mechanism, but it works for the small cluster we have.\nThese crashes are happening often around 4 AM Central time, which is when we run a lot of jobs--so one server is dirtying a lot of entities at once, and other servers subsequently have caffeine deadlock as they attempt to remove all of those entities from local memory cache. \nAssuming the deadlock is possible from the asMap, which is only used by us in removing things from the cache, it would explain the timing and multiple server problem. \nThe reason why we need a boolean on whether or not an object was removed from the cache is because we also have some indexes on entities in memory to keep up to date.. I now have monitoring in place, initially load counts are around:\ngetIfPresent:  3,200,000 per minute\nput: 22,600 per minute\nasMap().remove: 14,200 per minute\nThe next time we get a crash, I can get some load counts to see if they are particularly different. \nLatency has an average of <1 ms (registers as 0 for currentTimeMillis - before) per operation.. I can add minute-granularity collection of more metrics, but I don't see things like eviction count exposed on the Cache interface.\nAgain for reference, this is how we build our cache.\ncache = Caffeine.newBuilder()\n                    .maximumWeight(1024l * 1024l * 1024l * GIGS_OF_CACHE)\n                    .weigher(weigher)\n                    .build();\n            weigher.setCache(cache);\nPer the stack traces in the jstack, it seems this uses NonReentrantLock.\nI see you mention Caffeine.executor(ForkJoinPool.commonPool()::execute)\nAre you suggesting something like:\ncache = Caffeine.newBuilder()\n                    .maximumWeight(1024l * 1024l * 1024l * GIGS_OF_CACHE)\n                    .weigher(weigher)\n                    .executor(ForkJoinPool.commonPool()::execute)\n                    .build();\n?\nHow much risk do you associate with this change? What performance impact might it have?\nOnce 18 or so hours pass, I can provide some more numbers for both natural load and nightly job loads. . Alright. I'll see what I can do to canary these changes the next day.. I don't think I'll be able to make these changes this week, but here's the max numbers within a server (not a sum of multiple servers) encountered during the times we saw crashes in the past. \nget: 55,916,000 / minute\nput: 697,300 / minute\nremove: 201,000 / minute. Just a simple 8 core server :)\nTo be specific the cores seem to be Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz\nAgain, those numbers are the maximum of any of the servers with minute granularity. The spikes I saw for this were when major nightly jobs were running--loading all sorts of relational stuff.\n. Ubuntu 15.04 (GNU/Linux 3.19.0-22-generic x86_64)\nAmazon m4.2xlarge apparently \nHtop shows 8, cat /proc/cpuinfo shows 8 entries too. \nI could see how a missing signal might derail all of this. But I am perplexed as to why the clean up thread is also blocked on the same lock. \nWhen this happens, there is no load by the time we find out, everything is either sleeping or parked on that lock. (anything that could touch the database goes through Caffeine first.) Htop showed practically idle on all cores at the time I took that jstack. \nI now have code for internal review in to monitor the internal caffeine stats (collecting internally every 10 seconds, logging totals every 60 seconds), as well as a way to canary the alternate executor on specific instances. But don't expect results until next week. . Ben,\nThank you for sharing your research into the kernel java futex problem. \nThis week I am on call, and I will be canarying the alternate executor for a day. If nothing bad happens on that server, I'll make half the servers (including the ones that crashed prior) run on the alternate executor. \nOn my end, we still don't know what made those few days rather sensitive to crashes, but it seems to have disappeared.\nThis may be a very slowly moving ticket as mentioned by agoston.\nPerhaps getting agoston's environment details may corroborate this.. Indeed upgrading our fleet is desirable. I don't think it will happen this month though. \nSo far the canary is behaving well, so I will put half on the alternate executor on the next deploy.\nI have more numbers now: at most only 36 evictions are happening per minute, the average is around 7 evictions per minute. . No events on my end for the normal executor, nor the alternate thread pool executor.. Had a crash this morning at around 2:06 AM central, though it was only identified at 3:48 AM\nBefore the event, the internal metrics that Caffeine gave back had about 2K/misses per minute, and about 1K/evictions per minute.\nImmediately at 2 AM, suddenly 53K/evictions per minute and about 51K/misses per minute.\nThe period this spike lasted was about 6 minutes and then the metrics are silent.\nUnfortunately it seems something broke in my counting of which external methods we called on Caffeine, so I can't say what put/remove/get kind of interaction there was. . This crash happened on a server without the executor tweak active.. @johnou No, no OOMs, we get scary alerts when those happen.\nUnfortunately our logging looks at configs which are backed by the cache, so when Caffeine dies, pretty much everything else we can review goes silent. . Alrighty, I'll update our deps and remove the canary code. May get deployed next week. \nEDIT: actually will be deployed today, oh my.. I'm fine with this being closed.\nWe dearly need to upgrade, there aren't even repos for the ubuntu version we are on anymore D:\nAnyway, thank you for your time and effort into this Ben, I really appreciate it.. Hello @ractive \nWe have since upgraded ubuntu and the kernel and have not encountered these problems since. . Your stack trace has the same key indicators that our crashes had.\nIt gets locked during cleanup\ncom.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:994). ",
    "jbduncan": "@levischuckeats Am I right to assume that this Java version you're running is some sort of build of OpenJDK? :). > Also, I should note that I didn't use the type annotations' ability to be put within the generic argument, e.g. @NonNull Map<@NonNull K, @NonNull V>, because I didn't think that would be helpful. But I'm willing to be convinced otherwise.\n@ben-manes, may I ask why you think it wouldn't be helpful to label the generic arguments with @NonNull as well? :). @ben-manes In my humble opinion, that is a perfectly good view on the use of nullness annotations but also a bit of an outdated one now.\nAFAICT, nullness-checking static analysis tools like Checker Framework and Uber's NullAway are becoming more and more popular, so having consistent nullness annotations would help there.\nAlso, AFAIK Kotlin takes nullness annotations into account when exercising the null-checking part of its type system.\nHaving said all this, though, I don't know if Kotlin assumes things are nullable or non-null by default, and I think Checker Framework assumes things are non-null by default, so I don't know if adding @NonNull to generic arguments will help Kotlin and Checker Framework users much.\nBut regardless, my gut reaction is it would be very sensible to be explicit and label both types and generic arguments as @NonNull where applicable. :). @zmssqqli I don't believe any of us can read Chinese, but I've just Google-Translated your messages so far, and I'm unsure what your problem is with regards to Caffeine.\nCan you clarify?. Oh no wait. AFAICT, this user has posted the same set of messages on at least one other repository, so I'm inclined to believe that this is spam.\nSo unless @zmssqqli can clarify their intentions, I'd suggest this issue be closed.. ",
    "ractive": "Hi guys\nUnfortunately we're facing the exact same issue running a webapp with Spring Boot using an embedded tomcat inside a kubernetes cluster. The kernel version of the coreos hosts is 4.14 - so pretty recent.\nFrom inside the docker container:\n```\nuname -a\nLinux ... 4.14.11-coreos #1 SMP Fri Jan 5 11:00:14 UTC 2018 x86_64 Linux\n```\nThe kernel fix you mentioned is therefore in place.\n@levischuckeats and @agoston: Could you alredy upgrade to a newer Linux version and did it solve your problem?\nDoing an strace on the java process shows things like:\n[pid    24] restart_syscall(<... resuming interrupted futex ...> <unfinished ...>\n[pid    24] <... restart_syscall resumed> ) = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999399} <unfinished ...>\n[pid    24] <... futex resumed> )       = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999473} <unfinished ...>\n[pid    24] <... futex resumed> )       = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999553} <unfinished ...>\n[pid    24] <... futex resumed> )       = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999402} <unfinished ...>\n[pid    24] <... futex resumed> )       = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999687}) = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999498}) = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999564}) = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n[pid    24] futex(0x7f92c42ee678, FUTEX_WAIT_PRIVATE, 0, {tv_sec=0, tv_nsec=49999496} <unfinished ...>\n[pid    24] <... futex resumed> )       = -1 ETIMEDOUT (Operation timed out)\n[pid    24] futex(0x7f92c42ee628, FUTEX_WAKE_PRIVATE, 1) = 0\n(The full strace output would be available if it'll help you.)\nNow the hard question: could this still be a bug in the Linux kernel? \ud83e\udd14 \nAny tips where this could be discussed as well?\n. Good to hear that your issue has been resolved by upgrading the kernel, @levischuckeats!\nThe deadlock occurs with the following scenario:\nWe use hystrix to do requests to another web service inside hystrix's threadpool like:\n```\npublic class ExampleClient {\n    private Cache cache;\n    private RestTemplate restTemplate; // spring's rest client\n@HystrixCommand(fallbackMethod=\"emptyReply\")\npublic Something doSomething(String someKey) {\n    ....\n    return cache.get(someKey, key -> restTemplate.getForObject(\"http://example.com.?id=\" + key, Something.class);\n}\n\npublic Something emptyReply() {\n    return new Something();\n}\n\n}\n```\nNow when the calls to the example.com rest service get slow or timeout, the number of used threads in the hystrix thread pool pretty quickly jumps to the max pool size and threads are never released anymore, all hanging with exactly the same stacktrace as initially described in this issue here.\nHere's the copmlete stacktrace we have from our threads:\n```\n\"hystrix-ExampleClient-29\" #101 daemon prio=5 os_prio=0 tid=0x00007fca0c607000 nid=0x71 waiting on condition [0x00007fc9a37f5000]\n   java.lang.Thread.State: WAITING (parking)\n    at sun.misc.Unsafe.park(Native Method)\n    - parking to wait for  <0x00000000c393ebf8> (a com.github.benmanes.caffeine.cache.NonReentrantLock$Sync)\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n    at com.github.benmanes.caffeine.cache.NonReentrantLock$Sync.lock(NonReentrantLock.java:315)\n    at com.github.benmanes.caffeine.cache.NonReentrantLock.lock(NonReentrantLock.java:78)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:994)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:914)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2027)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:1954)\n    at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:113)\n    at com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:54)\n    at com.example.ExampleClient.doSomething(ExampleClient.java:110)\n    at sun.reflect.GeneratedMethodAccessor59.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at com.netflix.hystrix.contrib.javanica.command.MethodExecutionAction.execute(MethodExecutionAction.java:116)\n    at com.netflix.hystrix.contrib.javanica.command.MethodExecutionAction.executeWithArgs(MethodExecutionAction.java:93)\n    at com.netflix.hystrix.contrib.javanica.command.MethodExecutionAction.execute(MethodExecutionAction.java:78)\n    at com.netflix.hystrix.contrib.javanica.command.GenericCommand$1.execute(GenericCommand.java:48)\n    at com.netflix.hystrix.contrib.javanica.command.AbstractHystrixCommand.process(AbstractHystrixCommand.java:145)\n    at com.netflix.hystrix.contrib.javanica.command.GenericCommand.run(GenericCommand.java:45)\n    at com.netflix.hystrix.HystrixCommand$2.call(HystrixCommand.java:302)\n    at com.netflix.hystrix.HystrixCommand$2.call(HystrixCommand.java:298)\n    at rx.internal.operators.OnSubscribeDefer.call(OnSubscribeDefer.java:46)\n    at rx.internal.operators.OnSubscribeDefer.call(OnSubscribeDefer.java:35)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30)\n    at rx.Observable.unsafeSubscribe(Observable.java:10151)\n    at rx.internal.operators.OnSubscribeDefer.call(OnSubscribeDefer.java:51)\n    at rx.internal.operators.OnSubscribeDefer.call(OnSubscribeDefer.java:35)\n    at rx.Observable.unsafeSubscribe(Observable.java:10151)\n    at rx.internal.operators.OnSubscribeDoOnEach.call(OnSubscribeDoOnEach.java:41)\n    at rx.internal.operators.OnSubscribeDoOnEach.call(OnSubscribeDoOnEach.java:30)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:48)\n    at rx.internal.operators.OnSubscribeLift.call(OnSubscribeLift.java:30)\n    at rx.Observable.unsafeSubscribe(Observable.java:10151)\n    at rx.internal.operators.OperatorSubscribeOn$1.call(OperatorSubscribeOn.java:94)\n    at com.netflix.hystrix.strategy.concurrency.HystrixContexSchedulerAction$1.call(HystrixContexSchedulerAction.java:56)\n    at com.netflix.hystrix.strategy.concurrency.HystrixContexSchedulerAction$1.call(HystrixContexSchedulerAction.java:47)\n    at com.netflix.hystrix.strategy.concurrency.HystrixContexSchedulerAction.call(HystrixContexSchedulerAction.java:69)\n    at rx.internal.schedulers.ScheduledAction.run(ScheduledAction.java:55)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:748)\n``\nNot sure how hystrix comes into play here as well as it defines its own timeouts for the calls to thedoSomething` method etc.\nMaybe this setup gives you other ideas why these deadlocks can occur... \nWe will try to reproduce this issue with a stripped down version of our app running on pure docker. I'll keep you posted.. Yes, exactly.. Ok. We'll update the caffeine version and see what happens.. It happened also with the latest caffeine version:\n\"hystrix-AutosuggestClient-29\" #131 daemon prio=5 os_prio=0 tid=0x00007f510c286800 nid=0x8f waiting on condition [0x00007f5090cca000]\n   java.lang.Thread.State: WAITING (parking)\n  at sun.misc.Unsafe.park(Native Method)\n  - parking to wait for  <0x00000000c3c0abd8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\n  at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)\n  at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)\n  at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:209)\n  at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:285)\n  at com.github.benmanes.caffeine.cache.BoundedLocalCache.performCleanUp(BoundedLocalCache.java:1106)\n  at com.github.benmanes.caffeine.cache.BoundedLocalCache.afterWrite(BoundedLocalCache.java:1027)\n  at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2104)\n  at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2020)\n  at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:112)\n  at com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:54)\n  ...\nWe again will see if we can create a sample app that shows that.. Honestly i didn't have time to try to create a project to reproduce this. :-/\nBut the good news is that we didn't have any occurence of the deadlock anymore for 10 days. So the new version of caffeine seems at least more resistant to this error then the older one... \ud83d\udc4d . ",
    "stevelaw": "Thanks for the quick response.  This helps point me in the right direction.\nThe one piece I'm unsure about is say the CacheLoader were to read from the remote tier as you mentioned.  Let's say when originally written to the remote tier it was given a 1 minute expiration, and as the CacheLoader reads the value it still has few seconds of validity remaining in the remote tier.  However, now that it's been written to the local cache, it's essentially reseting the timer to what the local cache was configured for initially. \nIf true, this likely won't be much of an issue for most of the data were concerned about, but just want to make sure this is the case or not, and if there are any best practices around this scenario.\nOr am I thinking about this incorrectly?. I'm finally getting back to working on implementing the scenario where we read and write through to a remote network cache.\nOne piece that I'm confused about is the use of both a CacheLoader along with a CacheWriter as the current wiki page for the CacheWriter demonstrates.\nThe docs mention the following:\n\nA mapping that is loaded (e.g. LoadingCache.get), reloaded (e.g. LoadingCache.refresh), or computed (e.g. Map.computeIfPresent) is not communicated.\n\nIf I'm using a LoadingCache, and relying on the loader to create the value when not present, how would I then trigger the CacheWriter.write to be called?\n. Thanks for the quick response.  So the loader would call cache.put or invoke the writer directly somehow?\nI'm assuming you meant call put explicitly.  If that's the case, does using a CacheLoader provide any benefit in this particular scenario?\nUpdate:\nNevermind...  I believe I see where you were going with this.  I'll post some pseudocode to get your feedback that I'm approaching this correctly.. I have a very early example of what I'm thinking around this topic here - https://github.com/stevelaw/caffeine-remote-cache.\nAn example usage in the readme, and this is where the bulk of the work is done - https://github.com/stevelaw/caffeine-remote-cache/blob/master/MemcacheCacheLoader.java\nI'd appreciate if you could tell me if I'm way in left field on this, and any suggestions would be appreciated.\nThanks. Thanks.  Right, good point.  I initially had memcache performing the expiration, but tried to get cute with one idea I had, but will revert back.\nThanks for the tip about CacheLoader#reload.  I'll see if that can help cleanup the code.. ",
    "rrodseth": "My bad. I was initializing with a much smaller size than I thought.. ",
    "hugares": "Thanks for answer. I will follow your advice and implement the methods directly.. ",
    "pjfanning": "Thanks Ben. We can try using the AsyncLoadingCache as you suggest.. ",
    "slovdahl": "I recently discovered that using an AsyncLoadingCache in cases like this is no silver bullet. I have seen exceptions like this a few times now:\nMar 12, 2018 8:46:49 AM com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache lambda$get$3\nWARNING: Exception thrown during asynchronous load\njava.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: Thread limit exceeded replacing blocked worker\n    at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)\n    at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)\n    at java.util.concurrent.CompletableFuture$AsyncSupply.run$$$capture(CompletableFuture.java:1592)\n    at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java)\n    at java.util.concurrent.CompletableFuture$AsyncSupply.exec(CompletableFuture.java:1582)\n    at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)\n    at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)\n    at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)\n    at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)\nCaused by: java.util.concurrent.RejectedExecutionException: Thread limit exceeded replacing blocked worker\n    at java.util.concurrent.ForkJoinPool.tryCompensate(ForkJoinPool.java:2011)\n    at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3310)\n    at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1729)\n    at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)\n    at com.github.benmanes.caffeine.cache.LocalAsyncLoadingCache$LoadingCacheView.get(LocalAsyncLoadingCache.java:400)\nI don't think there's anything caffeine can do to fix this (except maybe document the risk of it happening?). Further reading of why it's happening, in the \"Compensation Threads\" section: http://moi.vonos.net/java/forkjoin.\nIt's easy to reproduce with a synthetic test. I'm not, however, able to reliable reproduce the issue with the original code. This far, it has happened when the JVM was starting and the cache was being filled. This suggests that there might be something else using the common ForkJoinPool at the same time, so I'm not blaming caffeine at all here. Using another Executor for the cache might be one way of working around this.. Well, in hindsight that was pretty obvious. Sorry for that and thank you for the quick answer. :blush: . One of our use-cases is the following:\n- a cache for domain objects that are fairly expensive to create, where the primary key from the RDBMS is the cache key\n- when doing arbitrary queries (filtering, sorting, etc), we just fetch the primary key and get the objects from the cache in the order the primary keys were returned by the query\nHowever, resorting the result after getting it from the cache isn't a showstopper at the moment at least.. Oh joy, undefined behaviour. :smile: I just stumbled upon a strange ordering issue related to this.\nConsider the following sequence in pseudo-code for a LoadingCache<Integer, String> that describes the expected undefined behaviour:\n1. cache.get(12) loads and returns \"12\"\n2. cache.getAll(4, 12, 15) -> loads \"4\", gets \"12\" from the cache, loads \"15\", returns \"4\", \"12\", \"15\"\nThis is how Guava's cache behaves. Caffeine gets any existing cached values first, and after that uses the loader to load the missing ones, causing the following behaviour:\n1. cache.get(12) loads and returns \"12\"\n2. cache.getAll(4, 12, 15) -> gets \"12\" from the cache, loads \"4\", \"15\", returns \"12, \"4\", \"15\". Thanks for the informative answer!\nThinking of it further, it doesn't matter if there is a value or not, does it? Doing multiple refresh invocations while a value is being computed triggers equally many loads nevertheless. Wouldn't of course happen in our case, because we don't trigger any refresh if a (stale) value exists.. ",
    "kojilin": "Thanks your explain about writeTime.\nAsyncLoadingCache<Object, Object> localCache = \nCaffeine.newBuilder()\n    ...\n    .buildAsync((key, ignored) -> getFromRemote(key));\nand we only use\nCompletableFuture<Object> future = localCache.get(key);\nanother is we have localCache.synchronous().stats().hitRate() to get hitRate with metrics gauge. But I think it just get stats.\nAbout thread, we try to use server's event-loop if possible. And usually we use even-loop thread to call #get.\n```\nclass ServerOrCommonExecutorService ForwardingExecutorService {\n        @Override\n        protected ExecutorService delegate() {\n            if(RequestContext.current() != null){\n              return RequestContext.current().eventLoop();\n            }\n            return ForkJoinPool.commonPool();\n        }\n}\nbuilder.executor(new ServerOrCommonExecutorService());\n```\nNot sure if possible, but I didn't found clue that eventLoop thread has blocking or in-complete from metrics(e.g. active or running thread has 0 value during 2 days. No blocking alert..).. I can't reproduce in my local too. So still try to find the place where I can test.\nAnd curious about if\nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1297\nmay run concurrently with \nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/LocalAsyncLoadingCache.java#L121 ?\nI'm not totally figure out all the flow of caffeine, so just random thoughts.. I see, I will try to reproduce.... Let me try to remove refreshAfterWrite and test first.. Just feedback, remove refreshAfterWrite but still same result.\nat 10AM\n\nat 3PM\n\nThere is a record with same timestamp. \nBut still can't reproduce at local.\n. CentOS 6.8 and 1.8.0_92. Maybe I can test with new java version.. ah let me repost again.\nIf we run this line first, but not get now yet.\nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1338\nthen when complete runs to here and get now first.\nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1929\nthen the expire maybe wrong(because now - far future won't be negative in this case), how do you think?\nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1936\nI tried again with debug mode thread stop and step by step.\n. Sorry I didn't get your point about get too old of a write time.\nI think replace's callback will run concurrently with AddTask's run, right?\nSo if run's get now run after callback's get now\ne.g.\nAddTask got lock first at https://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1336\nand enter the if block?\n0 at \nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1929\nand 10 at\nhttps://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1338\nthen hasExpired will be \n(0L - (10L + Long.MAX_VALUE))) which will be really large. I think the condition at https://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L733 will be true?\n. AddTask got the lock and enter the block of checking in-flight future. Then replace's whenComplete got the System.nanoTime before AddTask. (I'm not sure the thread of AddTask & whenComplete must be same or not.)\nAnd about reset, I think it will return at https://github.com/ben-manes/caffeine/blob/master/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1937 \nSo it has no chance to set back to 0.. Wow yes! hope this solve the problem. Thanks!!. OK let me try and report here soon, maybe need 1 or 2 days. I tried and let it run 1 day. \nCompare to previous test(3 instance, 1 day)\n2.5.6: All servers have 1~100 entries' writeTime in far future.\n2.6.1-Snapsot: No server has far future writeTime entry.\nCurrent nanoTime is around ~34438964688472572L, and there is no < 0 data.\n\nI think the fix solve the issue.\n. I can't reproduce without pausing thread, a little hard to write test case.. I see. I tried it but looks like problem is Completable just finish but before calling BoundedLocalCache#replace, LocalAsyncLoadingCache#get -> BoundedLocalCache#computeIfAbsent need to check isComputingAsync and into block.(I mean I didn't find the place can let it await during completion of Completable and BoundedLocalCache#replace.. Thanks, didn't aware my speculation can replay like the test case.  \ud83d\ude47 \nI will try new version after new release.. Hi, just wanna give the result, seems the problem is solved. Thanks.. ",
    "gmazza": "Micrometer is probably a bit heavyweight for what I'm doing right now (that may change).  Looking at their CaffeineCacheMetrics, it would appear to be an easy matter for them to update their code to .getXXX() should you later evaluate it beneficial for Caffeine adoption to update the method names in a major release (you're a better judge than me, so refraining from providing advice here).  Either way,, I already created the JsonSerializer so no problem for me to continue using it.  Thanks!. Hi Ben, thanks for the alternative implementation suggestion!  (This resetting, user-initiated, is so rare I'm considering also just recreating the cache when it occurs.)  Closing this ticket.\n  . ",
    "liyongjun1": "yup, I just checked the answer, It helped me to understand the problem I had met. thx!. ",
    "facboy": "sorry i've not had a chance to write a unit test yet.  i observed the bug.. well this is pretty annoying, i cannot reproduce it in a unit test so started wondering if i'd imagined the whole thing.  back to our production code and when i use Long.MAX_VALUE, it definitely breaks, so i'll have to whittle back from there.. ah, the simple approach: https://github.com/facboy/caffeine-test. I don't think it's related but is there also a bug with the Expiry.expireAfterUpdate behaviour?  perhaps it's not a supported use-case but I expect an entry to expire (ie be evicted and invoke removalListener) after the delay returned from Expiry.expireAfterUpdate() has passed.  It seems that this doesn't happen if I do not further access the cache.  I didn't follow all the ins-and-outs of it but it seems that Caffeine.expiresAfterWrite() doesn't return true if Caffeine.expireAfter(Expiry) is used, so no eviction is scheduled on a write.\nPossibly the error is here in the generated LocalCacheFactory:\nif (builder.expiresAfterAccess() || builder.expiresVariable()) {\n      sb.append('A');\n    }\n    if (builder.expiresAfterWrite()) {\n      sb.append('W');\n    }\nShould it append 'W' if builder.expiresVariable() is true as well?. oh i see, i thought that was what the timerWheel did but I didn't look at it in great detail.  i guess the wheel only advances on interaction with the cache?\nEDIT: doesn't it schedule evictions etc using the ForkJoinPool?. would be interesting if this was added as an option :).  for my current use-case i don't need them to expire in a particularly timely manner, i've just added an external background thread that invokes cleanUp() periodically.. hmm.  so it fixes the test-case i gave you, but it doesn't fix the 'actual' problem i was having which is still in private code.  if it helps this is the partial stack trace of the removalListener when it is expired (the reason cause[0] is EXPIRED):\nat com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$notifyRemoval$1(BoundedLocalCache.java:283)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.notifyRemoval(BoundedLocalCache.java:289)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2080)\n    at com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2010)\n    at com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:113)\n    at com.github.benmanes.caffeine.cache.LocalLoadingCache.get(LocalLoadingCache.java:65)\nthere might be an additional issue.  the code where I first observed this (ie not the test-case) is not doing a lot of reads...probably in the low 10s at most as it's a small integration test.\n. ah great, thanks for that!. ",
    "j-baker": "Even a default size of 16 (the same as the default ConcurrentHashMap constructor would be considerably better than at present).\nObviously users can tune this - the issue is that the behavior is very pathological for pretty trivial cases, which is why I wanted to fix this upstream.. Yeah, I can do a unit test. I reckon i can convert production slowdown into unit test deadlock with a cyclicbarrier.. That said, the unit test behaviour is going to be trivially repeatable for 'some size' of use cases - e.g. with 16 threads i can deadlock 16, with 64 I can deadlock 64, so maybe the test is too like, 'a fancy way to essentially test a constant value'?\nHonestly my preference is for more than 64. I've changed it to 16 here, but thinking about it the compute() workflows suffer from this a lot more because they lock the hashmap around external code - so you'd expect to want a larger constant, no?\nHappy to defer to whatever you'd like. Thoughts?. Ok, makes sense! The use case I occasionally see of an expected very small cache of potentially very expensive to compute entries we can just increase the min size manually for this.\nTo be clear, it's not just warm-up, you'd also see this in expected-small-cache-but-bursty-accesses workloads. If a cache always stays around 20 elements, but elements are added in groups of 10, you'll likely see something queueing due to lock contention around this.\nNot sure why coverage decreased; the test is currently failing but passes for me? Looks like a flake? Not sure how to run the builds again. On the previous commit it passed but then failed while trying to publish some jars some place, which makes sense.. The FAQ seems better - I checked that, but I didn't read the full javadoc. How about\n\nContention\nA case where Caffeine might suffer from contention is when the number of entries currently being computed is similar to or greater than the max number of entries the map has ever contained, and the loading function is very expensive. This corresponds to the currently computing entries being close to the total capacity of the underlying ConcurrentHashMap, which blocks resizing the map on the loading functions completing.\nThis is expected to happen while the cache is warming up (although likely not at all). If you are seeing contention issues due to an issue like this (manifesting as threads making different requests blocking on the same monitor in ConcurrentHashMap), consider increasing your initial capacity to your expected max concurrency to compensate, or use an async cache.. \n",
    "kluever": "The drawback is that if you're passing the duration values around through various layers, you either need to pass a long,TimeUnit pair, or you need to pass a raw long and hope that people get the units correct.\nIf the durations are specified \"in-line\" with the cache builder, then sure, there's not much of a difference between the two.. Well if the user is passing around a java.time.Duration already, why make them decompose it at all? I'd rather write:\n  .expireAfterWrite(EXPIRE_AFTER)\nthan:\n  .expireAfterWrite(EXPIRE_AFTER.toNanos(), NANOSECONDS)\n.expireAfterWrite(EXPIRE_AFTER.toMillis(), MILLISECONDS)\netc. One other benefit, is that it would prevent folks like this from dropping sub-second precision (I saw 2 such instances of this in google3):\njava\n    .expireAfterAccess(ttl.getSeconds(), SECONDS)\nAnyways, I don't think overloads add much \"weight\". The main reason we didn't add these in Guava is, well cache is basically in maintenance mode, and we also haven't forked it yet for Java8. Given that Caffeine is Java8+ and under active development, I'd be +1 on adding these.. To be clear, I'm not speaking on behalf of the entire Guava team. If you're interested, I could ask other folks what they think. I do see the argument that this API really is a \"concurrency API\", but in my mind, this API is more of a user-facing configuration API, not a \"low-level\" concurrency API like j.u.c.\nI don't think dropping the sub-second precision had a negative effect in either case, but the point of Duration is so that users don't have to wonder about those types of questions to begin with.\nAnyways, I don't feel super strongly here, so feel free to close this or leave it open without doing anything (and see if it gets any additional support).. If the interfaces aren't public, then I think it's up to you which you think is easier to deal with for implementation purposes.. Ahh, OK...well adding the overloads to the interfaces can probably be done safely w/ default methods, right? If so, I think the changes should be made there as well.\nLong term, are your plans to eventually deprecate (& remove) the long, TimeUnit overloads or just leave them in place?. Yep, +1 on using Duration consistently throughout the entire API.. Thanks @ben-manes !. ",
    "kevinb9n": "Kurt and I have embarked on an effort to stamp out all date-and-time-related bugs from our internal codebase. We're doing this by driving adoption of java.time, improving API designs, and adding a ton of static analysis to ban every usage pattern under the sun that we can't prove is safe. \nFor APIs that use either (long, TimeUnit) or (long ofFixedUnit), we keep finding an absolutely endless number of bugs - basic unit mismatch, arithmetic error, etc. Part of how we want to address this globally is to drive usage of Duration as much as we possibly can. Then it becomes only the few ways of creating durations that we have to spread static checks all over.\nGiven this, I'm more or less coming to the opinion that every API that wants to accept a logical duration should always have a Duration overload. And that it's generally a positive if that's the only overload it has. I see no reason not to make this recommendation for everything in java.util.concurrent -- obviously the (long, TimeUnit) would never go away, but I think more usage of Duration should be encouraged. And when a user does have a Duration I think it's a bad thing to make them decompose it.. Please do go against our opinions when they're bad. :-). Getting off-topic, but just had to say: that's a great set of rules; we agree. In fact, we discourage ZDT (really anything where a time zone ambiguously 'stows away' inside a value) strongly enough that we made our own TimeSource class to use instead of Clock. It has just now() returning Instant, and asClock(ZoneId) for interoperation.. ",
    "jschlather": "I'm not very familiar with the benefits of method handles vs reflection. It seems like it might be safer to rely on reflection until java 10 has been released for awhile.. The phrasing on that ticket is not clear, but the fix in jdk8 and jdk9 still leaks memory. I posted on the hotspot-dev mailing list while tracking down this issue in our stack http://mail.openjdk.java.net/pipermail/hotspot-dev/2018-February/030068.html and one of the hotspot devs clarified the issue\n\nThe MemberNameTable in jdk8 used jweak as pointers to MemberNames so the \nbelow increases in JNI block memory would also be accountable to the \nMemberNameTable.   We fixed this temporarily in jdk8 and 9 to leak less \nmemory but it still leaked.  The long term non-leaking fix is in jdk10, \nwhich is pointed to by the bugs you found.. \n",
    "slandelle": "@johnou @ben-manes JDK 8's EOL is January 2019 (it's the LTS until Java 11) so I'd be very disappointed if the fix wasn't back-ported.. @ben-manes thanks for the (sad) info!. ",
    "acruise": "Makes sense, thanks!. ",
    "michaelrebello": "I ended up into thus thread when a put &refresh combination got blocked. Sorry for posting on a closed issue thread\nI was trying to replace guava with caffeine and implement totally async cache using getIfPresent and refresh only (no get() call as it loads the data inline. In my case if cache entry is not present, i call a refresh() and i continue without it but strictly dont want to block).\nSome observations i made:\nWith the usecase above i ended up calling too many refresh() calls as cache entry initially was not loaded. This might be the case only during initial load as subsequent loads are taken care by refreshAfterWrite()\nTo stop this behavior i tried to load an initial dummy value into the cache by calling put(k, dummyval) and then call a refresh(), i ended up into a put() block. Idea here was that for calls after first one, i dont end up getting a null as return for getIfPresent() and do not keep calling refresh() which i mentioned in #1 above. I assumed, refresh would ultimately replace the dummyVal.\nPseudocode #1:\nint getValue() {\nval = loadingCache.getIfPresent(k)\nif (val == null) {\nloadingCache.refresh(k)\n}\nreturn val;\n}\nPseudocode #2:\nint getValue() {\nval = loadingCache.getIfPresent(k)\nif (val == null) {\nloadingCache.put(k, dummyVal)\nloadingCache.refresh(k)\n}\nreturn val;\n}\nIs AsyncLoadingCache solution for the usecase (when will async loadcache get loaded? as i do not want to call a future.get() and blocked), i have not explored it yet as it needs some changes to make in my code which was using guava cache earlier.\n. Sure. I just deleted my comment thinking there will be late reply and i thought il post it again once i do a little more work on this. But u noticed there was reply from you and i have put my comment back after 2 of your comments so that other readers do not lose the context. Thank you for blazingly fast reply too.. Just a doubt on AsyncLoadingCache.get(), it returns me a future. Is it necessary for me to do a get on it(future) in order to load the cache? Because in my usecase, I am ok to not have a cache entry at that moment but make sure it loads some point of time later and i ultimately find it. I would not want to do a blocking call anywhere in my main thread where i lookup into cache.\nAlso, when i deleted my comment i thought il do this below change and see if i can avoid an explicit refresh() call on the cache.\nFor the first time(i have an external way to find first ever call to the cache lookup not just rely on null return for getIfPresent()), just set the dummy value to the cache and do not refresh it explicutly as refreshAfterWrite will anyway trigger async refresh). ",
    "robertotru": "Hello folks.\nI have the same issue in version 2.6.2 and I'm not sure if it is due to issues with hash codes or equality as mentioned above.\nI've create a very naive example here https://github.com/robertotru/caffeine-max-size\nThe key in this example has final fields and equals/hashCode have been autogenerated by IntelliJ using JDK7 standards.\nMaybe I'm missing something there. E.g., am I doing something wrong (see another answer here: https://github.com/ben-manes/caffeine/issues/214 )\n. @ben-manes Thanks for the answer.\nI see your point. \nGenerally speaking, the cache should be instantiated as class field and to unit test the class I should not change the way I instantiate the cache. So, probably to test it using executor(Runnable::run) I should either always use that option (which is bad in production, right?) or have some hackish solution that I don't see as clean (e.g. have a configuration option to set the executor only for testing purposes or replacing the cache via reflection). I try to avoid changing the code only for testing purposes.\nWhat's your opinion/suggestion on that?\n. Ah! Interesting. \nI totally agree with\n\nI think it wouldn't hurt to more aggressively retrigger the maintenance work if it detects a new run is needed after it completes, rather than defer to the next cache read or write.\n\nI did not check your implementation (my fault), but in principle can't one check the size at the end of a cache.get() and if it exceeds the max you re-trigger the maintenance or would this procedure be too expensive? Maybe wrapping the return in a try-finally and executing the check in the finally.\n. @ben-manes so, let's say that I will hack the code to the cache getters inside my class so that I manually retrigger the cleanup when the size reaches the fixed threshold. \nI'll stay tuned in case you will change something to make this feature more \"reliable\" in the near feature.\nThanks a lot for the quick answer.. ",
    "sjoblomjsqueed": "Thanks for the answers! Would asMap() be thread safe? Is there an example of using it somewhere?. ",
    "QuantumBear": "xml\n<dependency>\n      <groupId>com.github.ben-manes.caffeine</groupId>\n      <artifactId>caffeine</artifactId>\n      <version>2.6.2</version>\n    </dependency>\nHere is the version I use.. Thanks for you helping.  At first glance I thought asMap just return view and I should not modify the cache, but I ignore the comment says that modifications affect the cache.  . ",
    "apragya": "To explain my use case more, I want to cache same value V, by multiple keys K1, K2 and K3. I was thinking of maintaining a  separate cache per key, and a key mapper to derive each key from V and updating each of the cache's whenever any of the Keys are updated. (And its fine if these updates are eventually consistent for the different keys). \nOr, another approach might be to use same cache with a generic key interface and store all of them in the same ConcurrentHashMap, but I'm worried it might cause more contention without much benefit otherwise.\nSo, having a listener would have just made my code more organized/clean.\nBut, I agree it can be done inside the load/laodAll . And I can understand the other implications and why it might not be a great idea for this to be exposed as part of the API.\nDo you have any thoughts on what's a good approach of achieving the above?. @ben-manes Thanks some helpful suggestions. \nI actually like the idea of keeping the mappings separate as it makes things easier and explicit and no assumptions involved w.r.t size of the cache. (Our current implementation is a home-grown cache which keeps all mappings and I can see how it has lead to confusion, now that you say it)\nHowever, i'm worried how do you achieve correctness, with a loading cache here. Since I don't really have a handle for putting the  , I'm not sure if we could synchronize the indexMap and the cacheStore. I'm actually thinking a manual cache might make more sense for this purpose than a loading cache.. @ben-manes, Let me know if i have misunderstood, however, what I understand from the API java-docs is that this implementation of multi-index cache relies on the assumption that there is a possible way to get a common Derived key from each of the secondarykeys/primarykey. Using this derived key it looks up the cache and loads a value based on Key, Derived Key\n For my scenario, I would have to do a DB lookup (i.e call valueLaoder) to basically first get V from the secondary key and then derive the DK, so I already have the value for the actual cache now and it's not useful for me to make two Database calls (that would defeat the whole purpose of this).\nI need a keymapper that works on V.\n. @ben-manes \nI am already implementing something for my use-case as I was in urgent need to have a solution.\nHowever, I am using a synchronized loading cache for the main store and a concurrent Hashmap for maintaining keyToIndex mapping. I have been considering replacing it with a cache like osklar's code, (however a synchronous one).\nHowever, i was confused on the fact that using 2 synchronous loading caches can be dangerous and can potentially lead to deadlocks if I'm updating inside one's load() method the other cache. Is that correct?. Yes, even I was originally thinking that it should not be an issue, however, consider the simplified scenario:\nCache 1 (K, KeyIndex) - cache 1 is for maintaining mappings and has a loader like this\nKeyIndex load(key K) {\n     V = db.get(K);\n     KeyIndex index = keyMapper.apply(V);\n    cache2.put(index.getPK, V)\n    return index;\n}\nCache 2 (PK, Value) : cache 2 is the main store and maintains from PrimaryKey to Value and has a loader like this\nV load(key PK) {\n    V = db.get(PK);\n   KeyIndex index = keyMapper.apply(V);\n    index.getAllKeys().stream.forEach(k -> cache1.put(k, index);\n    return V; \n}\nNow, if a request simultaneously comes for a primaryKey Pk1 and its corresponding secondaryKey Sk1 and both started loading from database.  Now since they both have obtained a write lock for Pk1 and Sk1 respectively and neither of them will release until load completes. But load cannot complete unless we update the other cache (so pk1's load would be blocked on obtaining write lock for sk1 and vice-versa).\nThis seems to me like a dead-lock situation. Am i missing something?. Isn't async then the only good way to do it then outside the computation, because we cannot listen to load events? \nIf I do it inside my custom cache's get function, which in turn calls caffiene's get (and might call load), I wouldn't know if it was because of a cache miss or hit and might lead to wasteful index updates.\nIf I do it by getIfPresent(k) and then calling cache.get(k, { k -> getFromDB()}) and then selectively updating the other cache, I'm not really using the \"loading nature\" of the cache.\n. Also, really appreciate all your feedaback and reactivity to the questions :)\nYour activity on the forums is really commendable . Thanks.. Yes, technically what i need is something like a \"trylock\" and this is what i was exactly thinking. \nI'll see how i can work around and make it work. Thanks again. . @ben-manes \nI think there is one more edge case which causes incorrectness that I recently discovered in my solution, if we maintain a KeyMapping relation in the secondaryKey Concurrent map.\nSay we had a primaryKey pk, secondarykey k and value v and it got updated to pk, k', v' in the database.\n(Now if my cache has the stale value v, it would have an index mapping of k to pk but no mapping exists for k')\nIf there are two simultaneous requests fired for reading k, and k', and what if load for k' was able to update the primaryCache to v'. \nSince there is no locking done on reading secondary key, it could lead to reading the mapping as pk, even before old index was removed. So now for k, we could possibly return v'.\nI am not even sure if this could be solved by obtaining a read-write lock while reading secondary key, as what i need is a locking on primarykey pk which I don't even have till i read this mapping.\nProbably, I'm missing something but i can only think that the way to get around this is by keeping values in the other map as well instead of mappings (which has some other limitations)\n. The problem is not with staleness of data. The problem is that when querying using a secondary key, I actually get an updated value which doesn't even have that as the secondary key.\nI'll use your example/code snippets to explain it - \nInitial record : pk, k (\"abc\") , v\nUpdated Record: pk, k' (\"xyz\") , v'.\nSay request 1 for put(xyz, v'),  has reached a point where it updated the actual cache store but has not updated the indexes. Now if a simultaneous 'get' comes for old k (\"abc\"), it can read the indexes  becuase AFAIK, it doesn't check any lock status before reading the indexes or store. \nThen it means it  will read for k (\"abc\") the mapping to pk and the primary store has already been updated to pk -> v' (which actually contains \"xyz\"). So a query for a secondary key (\"abc\") the returned record has no trace of \"abc\"\npublic V get(Object key) {\n    Index<K> index = indexes.get(key);\n    return (index == null) ? null : store.get(index.getPrimary());\n}\n\npublic V put(K key, V value) {\n    Index<K> index = checkIndex(key, value);\n    return put(index, value);\n}\n\nprivate V put(Index<K> index, V value) {\n    K primary = index.getPrimary();\n    lock.lock(primary);\n    try {\n        V old = store.put(primary, value);\n        if (old == null) {\n            addIndex(index);\n        } else {\n            updateIndex(indexes.get(primary), index);\n        }\n        return old;\n    } finally {\n        lock.unlock(primary);\n    }\n}. Yeah, I think that's right.\n\nI did think about that as well, and it probably should solve it. However, I was debating as to should i add this complexity or rather go by another approach of just keeping the values in the second map (as there isn't really any increased memory footprint). It wouldn't need any atomic constraints like this one. . ",
    "osklyar": "@ben-manes Very interesting that you mention the async aspect: the cache I am using is in fact an async wrapper around the Guava LoadingCache guaranteeing non-blocking and single loading on missing values completing all the completable futures on the loading and other gets upon completing the loading stage.. @ben-manes Yes, homegrown and conceptually similar, but with a very limited scope. Effectively it is a fairly minimal implementation of BiFunction<K, Supplier<V>, CompletableFuture<V>>, where Supplier<V> is the loader, backed by Guava LoadingCache. The only serious logic is making sure that loading occurs just once. For values already in the map futures are completed on the same thread. But thanks for pointing out, I will consider moving to caffeine instead :)\n. @ben-manes My initial implementation is storing a value holder with a CountDownLatch initialised to 1 and reset upon successful load, the exception if any and the value upon the load. All other get calls will wait for the latch within CompleteableFuture.supplyAsync if the value not there and will complete either with the value or exception. If the value is already in then they will complete immediately on the same thread.\nIn any case, I will work on this in the next days and will share the final solution here, either via a gist or a new repo, just for the sake of sharing findings on an interesting challenge. . @ben-manes You've got here an amazing library, so I have already migrated some of out code off Guava. The following has been my best effort so far, still WIP, but roughly the general idea: https://github.com/teris-io/caffeinated\nWould be grateful for any critic.. Thanks for insightful comments. This is just the first (not quite finished) iteration so I will have a second and a third look at this and will come up with more realistic examples (tests).. @ben-manes So here is one real-world example essentially implementing my actual use case in test, but in a simplified manner with some dummy DAOs. The DAOs times are unfortunately not negligible in our case, thus the explicit latencies in the DAO dummy impls: https://github.com/teris-io/caffeinated/blob/master/src/test/java/io/teris/caffeinated/SessionStoreExample.java\nExceptional pathways are still to be tested, but I also simplified the API a bit and introduced JavaDocs for clarity.. Thanks a lot for the insights. On (1), will check and integrate, even though the delays are not hurting yet for the prototype. On (2), well spotted, will investigate your solution, but need to integrate a fix in any case. On (3) yeah, not ideal, so I replaced the value loader by a BiFunction with main and derived keys; in my case I do not need it, but generally it won't harm there.\nIn fact, a more irritating API decision for me was the impossibility to construct a loading cache without a loader function in the builder. More often than not I tend to provide the loader to the get method using the closure state rather than passing a generic one in the builder. I probably would prefer the means to construct the cache without the loader and get an exception if no loader is provided to the get (or exceptionally completed future, see below). The example under discussion here is actually the one where there is no single loader that can be used for all keys.\nAnother slightly irritating API decision is to permit nullable CompletableFuture returns. I would rather prefer a future completed with null or completed exceptionally depending on the situation. Effectively I would prefer to deal with nulls in the callbacks rather than on the future interface directly in cases like getIfPresent().thenApply. Thanks for a hint for the test, really brilliant. As for the behaviour here, fair enough, but what I do not quite understand why the content is actually correct (or correct much earlier), that is I could not find any temporary or incomplete entries while inspecting the cache in debugger or printing it.. Thanks @ben-manes!. ",
    "leventov": "\nIt used to be a best practice to record only incrementing values within the application, and let the aggregator perform any rollups. This is a lot simpler for all parties, and much more flexible. Is that no longer the case?\n\nYes, I agree. Then the exemplary code should probably just switch to Counter.\nRegarding the polling approach, I was concerned that getStats() is called multiple times needlessly, but the overhead of this is probably negligible.. ",
    "jwiklund": "NB I expect this to pass:\n```\n class FakeTicker implements Ticker {\nprivate final AtomicLong nanos = new AtomicLong();\n\nvoid advance(Duration duration) {\n  nanos.addAndGet(duration.toNanos());\n}\n\n@Override\npublic long read() {\n  return nanos.get();\n}\n\n}\n@Test\n  public void test() {\n    FakeTicker ticker = new FakeTicker();\n    // simulate a low nanoTime by setting the ticker to now.\n    ticker.nanos.set(System.nanoTime());\nLoadingCache<Object, String> cache = Caffeine.newBuilder()\n    .recordStats()\n    .refreshAfterWrite(Duration.ofSeconds(10))\n    .ticker(ticker)\n    .build((ignoredKey) -> \"ok\");\n\nnew CacheMetrics(metric_registry).monitor(cache, \"country-attributes-cache\");\n\nassertThat(cache.get(\"1\"), is(\"ok\"));\n\nticker.advance(Duration.ofSeconds(11));\n\nassertThat(cache.get(\"1\"), is(\"ok\"));\n\ncache.stats();\n\n}\n```\nCurrently it throws \njava.lang.IllegalArgumentException\n    at com.github.benmanes.caffeine.cache.stats.CacheStats.<init>(CacheStats.java:109)\n    at com.github.benmanes.caffeine.cache.stats.ConcurrentStatsCounter.snapshot(ConcurrentStatsCounter.java:95)\n    at com.github.benmanes.caffeine.cache.LocalManualCache.stats(LocalManualCache.java:89). ",
    "jiayeee": "ok. after i read some introduction about Caffeine and other cache components(such as guava), i understood the working mechanism of RemovalListener, thank you. ",
    "kashike": "I'm not sure what I'd consider it, but it might be wiser to stick it under a minor revision - it isn't what I'd consider a \"patch\".. | JSR 305                      | New                                                 | \n|------------------------------|-----------------------------------------------------| \n| javax.annotation.Nonnegative | org.checkerframework.checker.index.qual.NonNegative | \njavax.annotation.concurrent.NotThreadSafe and javax.annotation.concurrent.ThreadSafe have no replacements, as far as I know.\nIt's also worth noting that annotations such as org.checkerframework.checker.nullness.qual.NonNull and org.checkerframework.checker.nullness.qual.Nullable are type annotations - see https://github.com/google/guava/pull/3145.. ",
    "tbroyer": "(context: currently migrating a project away from JSR305 and looking around for what people used in replacement)\nFwiw, there are ThreadSafe and NotThreadSafe annotations in net.jcip.annotations, which you can find under Apache 2.0 license at com.github.stephenc.jcip:jcip-annotations (rather than the CC BY 2.5 of the original net.jcip:jcip-annotations). I'm not (yet?) a caffeine user so :man_shrugging: but I see you have a couple public classes that used to be annotated with @NotThreadSafe, so it might be useful to add the annotations back, if only for documentation (it also looks like Google might have a non-opensourced Error Prone checker for @ThreadSafe, and maybe one day they'll opensource it; you'll have the checks \"for free\" then :tada:). Fwiw, one other use was BloomFilter, in the simulator projector.. ",
    "alexeyOnGitHub": "that would work, thank you! \nI may have over-simplified the source code example to get my point across. we are actually using async operations to load values from external systems, and I am wondering if I can use this \"get with loader\" approach with Async Cache.\nI see there is AsyncLoadingCache that works with java futures, but it requires declaring a loader when it is created. and regular Cache class only works with sync operations.\nif there a way to have something like AsyncCache with get(key, loader) operation?\nI guess a workaround may be providing a dummy no-op AsyncCacheLoader when creating an instance using Caffeince...buildAsync()   and then use a real loader in get(key, loader) operation.\nthe downside would be that all get(key) calls will always return null and you need to remember to only use get(key, loader) for this cache. \n. I see the code I am working with also uses .getIfPresent() on that cache. with this workaround this method will always return false.\nI can change .getIfPresent()  call to get(key, dummyLoader) - the idea is to return the value if present, and return null if not found (since the loader will be a NO-OP thingy).\nimplementing AsyncCache may be useful to skip these workarounds.. this would simplify things indeed. the AsyncLoadingCache constructed this way would return values if present and null otherwise, without trying to load them from another source. good enough for now. \nthanks a lot, Ben!. @mklamra is your use-case similar to https://github.com/ben-manes/caffeine/issues/243 ? . ",
    "QubitPi": "Thanks for the very quick reply, @ben-manes ! I'll promote caffeine to my team. We are not flushing parts of the cache by eviction or updating. We are flushing everything in cache to an external source. \nWe would flush periodically; something like dumping all cache into an persistent resource readable by Caffeine cache.. In that case, does Caffeine provide an interface for easy writes and reads between the cache and the external temporary storage? It looks like we have to implement our own CacheWriter for that, is that correct?. asMap() sounds good.\nThank you!. Thanks! We'll modify our tests for that. . Ok. We'll figure out something else then. Thank you!. ",
    "mklamra": "Thanks for the explanation. I will use the dummy loader solution for now. I guess you can close this issue unless you want to keep it to track the future change.. @alexeyOnGitHub My use case is the same as yours in #243. I have an AsyncLoadingCache that caches data from another service. The cache is keyed on user ID. However, the service that I call requires that I provide a request context. The request context is provided to the method that calls the cache. The request context will always be the same for a given user ID. I could therefore use a cache loader and include the request context in the cache key, but that's a really hacky solution. As a workaround, I create the AsyncLoadingCache with a dummy cache loader that always throws an exception and instead provide a loading function when retrieving items from the cache:\ncache.get(userId, (key, executor) -> client.get(userId, requestContext));\nTo avoid the dummy loader, there would need to be a version of Caffeine.newBuilder().buildAsync that did not require a loader.. Great! Thanks a lot @ben-manes.. ",
    "yufei-cai": "@ben-manes Any plans to include AsyncCache in a release?\nMy use case is a sliding window of events, say caching all events fired in the last 5 minutes. Cache loading isn't meaningful there.. ",
    "manishonline": "It was an issue on my end itself. I was setting buffer time wrongly.. ",
    "ohadeytan": "It seems that Rapidoid removed from the source code on one of the last  commits.\nTo solve the issue just remove Rapidoid from the reference file (i.e. delete or annotate line 105) so it won't run. . Few constants here could be configurable, but I didn't want to add too many options. Do you think we should allow as much as possible configurations? \nRegarding the separation vs. abstraction of policies, I'm not sure what is the best way. IMO separation makes it simpler and clearer when you don't want to change configuration options, but when you start to go deeper and change things it is more reasonable to make an abstraction and merge policies. \nFor now, I'll merge it as is and perhaps we would reorganize few things in the future.. I extracted all the constants to the configuration file and fixed some styling and a bug I have found in minisim.\nHope it's clearer now.\nSorry for the delay.. Currently, the idea is that the Indicator is trace dependent rather than cache size dependent. But this could be debatable. For now, I leave it as is.. I just followed your FRD format. I'll fix it.. Since all of them are frd derivations I think it makes sense to leave it as is.. Done.. We could, but we should also refactor PeriodicResetCountMin4 then. That's up to you.. I extract it to the configuration file.. Done.. Yes, that is the adaptation period. No modulus required. \nI extract it to the configuration file.. ",
    "nicolas-kourtellis": "Wow, Thank you Ohad!\nThat worked and it was so simpler than what I tried.\nThank you again!\nOn Sun, Jun 10, 2018 at 11:01 AM, Ohad Eytan notifications@github.com\nwrote:\n\nIt seems that Rapidoid removed from the source on one of the last commits\nhttps://github.com/ben-manes/caffeine/commit/5851960f38b4431891a185b5105affbf1ec4228f\n.\nTo solve the issue just remove Rapidoid from the reference file\nhttps://github.com/ben-manes/caffeine/blob/master/simulator/src/main/resources/reference.conf\n(i.e. delete or annotate line #105\nhttps://github.com/ben-manes/caffeine/issues/105).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/250#issuecomment-396029390,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGhZ9wFV08u_inBiEVp3qMb-RXTNRWIoks5t7NJZgaJpZM4Uhmpu\n.\n\n\n-- \nNicolas Kourtellis\n. ",
    "jkarshin": "That is unfortunate. Thanks for the quick response.. ",
    "mehwhatever": "Got you. I want to avoid a ScheduledExecutorService since my cache would be under heavy load.\nSo when I want to implement isKeyPresentInCache, I will have to look up the keyToCacheMap as well as do a cache.asMap().containsKey on the cache it points to. Correct?. ",
    "migesok": "I think most likely cause is race between execution of user callbacks and whenComplete which removes the cached async result on the result of com.github.benmanes.caffeine.cache.LocalAsyncCache#get(K, java.util.function.BiFunction<? super K,java.util.concurrent.Executor,java.util.concurrent.CompletableFuture<V>>, boolean).\nAt least if I locally fix it in a way that the resulting CompletableFuture is completed in case of null result only when the cached result is removed, then the issue seems to go away.\nI haven't been able to run the full test suite with those changes - it just hangs at some point, even without my changes.. ",
    "omverma22": "OK Thanks for the info. I will try Infinispan.. ",
    "azagniotov": "Hi @ben-manes ,\nThank you so much for this elaborate response. It definitely helps to gain a better understanding about the shortcomings of my suggested approach.\nTo have a background maintenance worker that does periodic clean up is totally fine for my use case. Thank you again for clarifying the low level details.\n. Thanks, @ben-manes ! I do want to clarify the following:\nAccording to my understanding when an entry expires, a miss is emulated by the cache if entry is still present. With that in mind and what you said earlier, consider the following potential use case:\nA single instance of my service processes around 50 QPS. I cache query result for 30 seconds (expire after write) and my cache is limited by N size. Since (as you mentioned) you perform maintenance pretty regularly when there is activity, I am curious if there a need for a maintenance task? \nI am asking this not because I am concerned about incurring a penalty of running a thread, but whether I should just let the cache do its work. Also, what is the size of access and write queues?\nThank you and have a great weekend too!\n. Thank you, @ben-manes \nVery educative discussion . Thank you for clarifying more, @ben-manes. \nI ended up configuring an instance ScheduledExecutorService and passing it to Caffeine.newBuilder().executor(...). \nThe executor is scheduled to call cleanUp() at fixed rate using the following formulae Math.ceil(expireAfterWriteSeconds * 0.05) where 0.05 is a multiplication factor. In other words, if cache entry lifetime is 30 secs, the maintenance task will kick in every 2 secs. I just picked this number to start with.\nI am planning to run a series of performance tests and observe heap utilization with and without the maintenance thread. I can scale up my service to around 450 qps, so I am curious to see if maintenance task makes any difference in my case.\n. Thank you. Yes, in my case I am only doing a maintenance task, without other user-facing async tasks.\nStay tuned, will report shortly. The limitation that I see with the maintenance task is that it is configured with a static period. e.g.: the clean up frequency has to be manually adjusted if throughput increases/decreases over time, in order to optimize heap utilization (when it makes sense to do so).. Yes, I understand that the cache will internally perform a maintenance when queues will get full. I submit a task to trigger cleanUp() that will be invoked by executor periodically at a fixed rate. For clarity, this is my code:\n```\ndefault Cache build(final long expireAfterWriteSeconds, final long maxHeapEntries) {\n    final double multiplicationFactor = 0.05;\n    final long maintenanceFrequencySeconds = (long) Math.ceil(expireAfterWriteSeconds * multiplicationFactor);\nfinal ScheduledExecutorService maintenanceService = Executors.newScheduledThreadPool(1);\n    maintenanceService.scheduleAtFixedRate(() -> {\n        nativeCache().cleanUp();\n    }, maintenanceFrequencySeconds, maintenanceFrequencySeconds, TimeUnit.SECONDS);\n\nreturn Caffeine.newBuilder()\n            .expireAfterWrite(expireAfterWriteSeconds, TimeUnit.SECONDS)\n            .maximumSize(maxHeapEntries)\n            .executor(maintenanceService)\n            .softValues()\n            .build();\n\n}\nCache nativeCache();\n```. Thanks for reassurance, @ben-manes. I will start a few perf test runs tonight. Hello @ben-manes, I ran a two performance tests runs where in one run I was running without the maintenance thread, while in the other run I had it configured as per above.  Please see the heap utilization from both runs in the graphs below. Also, to clarify - my service is deployed across four pods in Kubernetes, that's why you see four lines in the heap graphs\nRunning with a periodic maintenance task\nGradually increased load: 200 rps, 250 rps, 300 rps, 350 rps & 400 rps\n\nRunning without a maintenance task\nGradually increased load: 200 rps, 250 rps, 300 rps, 350 rps & 400 rps\n. As you pointed out: if judging by %, it does seem that when a maintenance thread is configured, the heap utilized better. A little more context about my service: it does not actually have an explicit threadpool configured. It is an event-drive & non-blocking IO app on Netty+Vert.x stack that uses N event loop threads (configurable). I do not do any CPU-intensive operations, therefore I do not configure an explicit threadpool through Vert.x to perform those\nI can have another run with Runnable::run and see whether there is a difference. If I understand you correctly, my Cache config should be now as follows:\ndefault Cache<String, T> build(final long expireAfterWriteSeconds, final long maxHeapEntries) {\n    return Caffeine.newBuilder()\n                .expireAfterWrite(expireAfterWriteSeconds, TimeUnit.SECONDS)\n                .maximumSize(maxHeapEntries)\n                 .executor(runnable -> {\n                    nativeCache().cleanUp();\n                })\n                .softValues()\n                .build();     \n}. Alright. Stay tuned. Will report back shortly . @ben-manes I did the perf test run:\nRunning with Runnable:run\nGradually increased load: 200 rps, 250 rps, 300 rps, 350 rps & 400 rps\n\nGoing by the heap utilization percentage, it appears that in my case the heap was utilized better when running with ScheduledExecutorService. Using the Runnable:run showed better results than not using executor at all, but not as good when using ScheduledExecutorService. Hi @ben-manes ,\nThe takeaway that jumps at me after my investigation: in the case of non-blocking I/O & event-driven app that runs on a main event loop thread, it made sense to have a dedicated thread doing periodic cleanup. \nRegarding the soft references, to be honest I was not driven by any concrete data from my own testing when I configured .softValues(). I just wanted to signal to GC that it can clear soft referenced objects on response to memory demand if required. It may be a premature optimization at this stage, I admit.\nWhat do you think?. Based on this discussion that we are having, I think I should rethink whether to have this option enabled (e.g.: softValues) when I do not have an apparent reason. In other words, perhaps I am trying to solve a problem that does not exist just yet. \nI will run another set of performance tests without the softValues, but with a maintenance task and one more time with  Runnable:run. Hi @ben-manes, I did another run: \nRunning with a periodic maintenance task, without softValues option set.\nGradually increased load: 200 rps, 250 rps, 300 rps, 350 rps & 400 rps\n\nJudging by the % only and comparing with the graph in https://github.com/ben-manes/caffeine/issues/257#issuecomment-409340779 (Running with a periodic maintenance task graph), I see that it is better to have .softValues enabled. ",
    "alx696": "Thanks for your reply!\nI add a List to cache the keys, so the keys keep order. And i want to know, do i need to use Cache?\nI just Cache all, foreach all. So the gRPC method do not need to query database every time.\nCache is better than Map?. Ok, thanks very much.. ",
    "dave-fl": "Based on what I read in #254 it sounds the same.  Which means that the AsyncLoadingCache cannot be used with reactor.\nIs there a planned fix for this?  It's definitely a race condition - I intentionally put in a 2 second lag and the cache is empty.. Thank you for explaining the internals.  These intricacies should be outlined as most users might be unaware of this (the case here until running the test).  It doesn't feel right for an item to briefly be there and potentially removed, but it is good that both can be handled.\n. Shouldn't invalidate be part of the AsyncLoadingCache interface?. Had some deadlock issues with the above.  This seems to correct it.  Could you explain why that might be the case.\nThis all seems very similar to the initial blocking on Reactor, we are deferring things here though to be managed by the cache thread pool.\npublic static <KEY, VALUE> Mono<VALUE> lookupAndWrite2(\n        AsyncLoadingCache<KEY, VALUE> cache, KEY key, Mono<VALUE> mono) {\n    CompletableFuture<VALUE> future = mono.toFuture();\n    return Mono.fromFuture(cache.get(key, (k, e) -> future.whenCompleteAsync((r, t) -> {\n        if (t != null) {\n            cache.synchronous().invalidate(key);\n        }\n    },e)));\n}. For anyone who might find these writes useful.\n\npublic static <KEY, VALUE> Mono<VALUE> lookupAndWrite(\n        AsyncLoadingCache<KEY, VALUE> cache, KEY key, Mono<VALUE> mono) {\n    return Mono.defer(() -> Mono.fromFuture(cache.get(key, (k,e) -> mono.toFuture())));\n}\n\npublic static <KEY, VALUE> Mono<VALUE> lookupAndWrite2(\n        AsyncLoadingCache<KEY, VALUE> cache, KEY key, Mono<VALUE> mono) {\n    return Mono.defer(() -> Mono.fromFuture(cache.get(key, (k, e) -> {\n        CompletableFuture<VALUE> future = mono.toFuture();\n        return future.whenCompleteAsync((r, t) -> {\n            if (t != null) {\n                cache.synchronous().invalidate(key);\n            }\n        }, e);\n    })));\n}\n\npublic static <KEY, VALUE> Mono<VALUE> lookupAndWrite3(\n        AsyncLoadingCache<KEY, VALUE> cache, KEY key, Mono<VALUE> mono) {\n    return Mono.defer(() -> Mono.fromFuture(cache.get(key, (k, e) -> {\n        CompletableFuture<VALUE> future = mono.toFuture();\n        return future.whenComplete((r, t) -> {\n            if (t != null) {\n                cache.synchronous().invalidate(key);\n            }\n        });\n    })));\n}. Sorry but one last thing I am uncertain of (the size of the cache does not matter to me).  But a realistic use case.\n\nReferencing above.\nWhat happens if thread TR0 is at (step t1) and thread TR1 comes and tries to access the cache (step t0) with the same key as thread TR0.\nWill the second thread get the same object?  Would this behavior be different depending on if lookupAndWrite or lookupAndWrite2 is used?. One thing that I just noticed.  Was that none of the futures are being run on the executor pool that is passed to Caffeine.  They are being run on the main thread.  Changing lookupAndWrite3 to\npublic static <KEY, VALUE> Mono<VALUE> lookupAndWrite3(\n        AsyncLoadingCache<KEY, VALUE> cache, KEY key, Mono<VALUE> mono) {\n    return Mono.defer(() -> Mono.fromFuture(cache.get(key, (k, e) -> {\n        CompletableFuture<VALUE> future = mono.subscribeOn(Schedulers.fromExecutor(e))\n                .toFuture();\n        return future.whenComplete((r, t) -> {\n            if (t != null) {\n                cache.synchronous().invalidate(key);\n            }\n        });\n    })));\n}\n\nSeems to solve the blocking issue as it forces the code to run on Caffeine's executor.  All similar to how lookupAndWrite2 does its invalidation on another thread.  I am guessing you would expect your users to want to run the task on Caffeine's executor.\nI am not certain on the full implications of this, but I would think we lose some of the benefits of NIO, so I am not certain how much this changes things over using Mon.block() on a separate scheduler e.g.\npublic static <KEY, VALUE> Mono<VALUE> lookupAndWrite(\n        Map<KEY, Signal<? extends VALUE>> cacheMap, KEY key, Mono<VALUE> mono) {\n    return Mono.defer(() -> Mono.just(cacheMap.computeIfAbsent(key, k -> {\n                Signal<VALUE> signal = mono.materialize()\n                                .subscribeOn(Schedulers.elastic()).block();\n                return signal.isOnNext() ? signal : null;\n            })).dematerialize());\n}\n\nI think the important point here is this isn't necessarily a one size fits all solution.  There will be times where a stampede might not be likely and thus you want to enjoy the benefits of NIO and times where a stampede can take place and you want to be able to prevent that.\nThank you for the discussion Ben, you have been a great help.. Based on what I see - Mono.toFuture is fully blocking.  The future will run as soon as its called.. Check out Mono.toFuture\nEverything preceding that call will be suscribed to the newly generated MonoToCompletableFuture  The code executes as soon as the call is made.. The subscription behavior that I saw in the Mono.toFuture seems to have an issue with Spring Security.  I've opened an issue here.\nhttps://github.com/spring-projects/spring-security/issues/5690\nNot your bug obviously but something to be aware of.. ",
    "denghongcai": "I find a idea that can partly resolve my problem, https://cache2k.org/docs/latest/user-guide.html#refresh-ahead. but this will only refresh once if no query after refresh period, that means it's only make expire time twice....if expireAfter and refershAfter both exist, we can achieve our goals better.. @cruftex Thanks for your kindly remarks. \nIn my scenario, my cache behavior have to work as a CDN. It means a cache entry will be specified a ExpirayPolicy, set expire time based on HTTP Header s-maxage. s-maxage's value may vary.\nThe problem I have met is, when system QPS is high, and they are getting same cache entry, if that time point, the cache entry just expired, all requests will stall on cache loading operation at the same time and after loading finished, resume to process at the same time. Such situation will lead to higher CPU Usage and increase JVM Memory pressure or cause FullGC.\nSo I want a mechanism to ensure system have predictable performance. RefreshAhead cache2k provided have a shortage: All entries will be included to Refresh. My cache have a lot of entries, loading time typically is one seconds or more. And it's hard to select few of hot entries(some time is one, some times all of them). Precisely in my situation, only cache entry which satisfied Refreshy policy to be accessed (need configurable) need to be refreshed.\nget -> entry which satisfied Refreshy policy -> async refresh -> return old value and when refreshed, replace entry to new value\nOnly a entry not be accessed after a long time(expire time), the entry removed from cache.\n. @cruftex Thanks for your advice. \nI'm currently using L1/L2 way, just like CDN L1/L2. L1 cache size is must smaller than L2, and use Cache2K RefreshAhead to refresh. But it's hard to determine which entry should go to L1 cache. For example, a entry will being hot entry only if it's 00:00:00. To achieve this, I do a lot of job to determine which entry should go to L1 cache. \nAdvancedCacheLoader can solve my problem if combine with Cache2kBuilder.keepDataAfterExpired(boolean), but It's synchronized. IMO, I can use AdvancedCacheLoader to check if old entry should return stale data and do a async refresh or must return fresh data. If AdvancedCacheLoader can trigger background refresh, it will perfectly resolve my problem.  . ",
    "fitrah-firdaus": "\nYes, that might work. If I understand correctly, it uses a java.util.Timer to schedule a task per entry, based on its expiration time, and the thread then reloads it asynchronously. This would let you have per-entry policies at the cost of dedicated thread(s), O(lg n) writes, and entries would not disappear if unused. In Caffeine the refresh is triggered if the entry is actively used, does not conflict with expiration (as can be used in conjunction), does not use dedicated threads, and is O(1). So tradeoffs, with both implementations having their benefits and limitations.\nI think we can add this feature fairly easily, but I don't know when I'll have the time to work on it. So I think in the short-term you should choose whatever is best for you and not wait on me.\n\nHai @ben-manes Based on this thread, So the cache can't refresh automatically, when the key is expired or evicted, Right? May I request this feature? or could you give me some guidance, if I want to create PR for this feature, which file that should modified? Thank a lot for your great library. ",
    "blsemo": "I haven't signed the CLA yet - this PR might have to wait until I get the all clear from our management/legal.. Hi Ben,\nThose adaptive tests look good - they might make the manual window size configuration unneeded. I have raised https://r3-cev.atlassian.net/browse/CORDA-2562 for us to give the branch a try and double check that it delivers the performance we need in our system. If that is the case, this feature would be very good to have in an official caffeine release.\nNot sure I fully understood your last question about hiding the detail? What detail do you want to hide from whom?. ",
    "marceloverdijk": "thx @ben-manes I would like to use a Caffeine cache as I could change the configuration easily if I later find out I could not cache everything and need to implement an eviction strategy after all.\nWhat is a unbounded cache and how could I create that? Does it differ?. ah, I'm using unbound caches than already :-)\nSo I think above assumptions are correct then.\n. oke thx, I will remove the custom Weigher and maximum weight setting. Thx again for quick reply. . Oh I missed #220.\nIs this something that will be fixed in Caffeine? \nBecause of this I can't use the loadAll functionality unfortunately as I don't know the order of the keys I received... I was looking for loadAll to avoid key by key queries to a backend.. I unfortunately don't know the order on the all-site. But I will figure something out (maybe just not doing bulk loads at this time). Let's close this one and I will track #220.. I tried with single quotes but that didn't seem to have any effect. With double quotes it throws an exception. \nI will create a minimal app and will share it here.. I'm using this repo: https://github.com/marceloverdijk/hibernate-l2-caffeine-jcache\nIt's still in progress but finishing it asap.. The project is setup, and I've added minimal README.\nThe only thing is, the Hibernate L2 caches are not created... will dig into it.. I've updated the sample app https://github.com/marceloverdijk/hibernate-l2-caffeine-jcache.\nThe README.md contains some additional info.\nEspecially Note III is very interesting. In this sample app I see no caches exist in the cache manager, and in my actual app I see these caches. I cannot explain that atm.. Thank you for your help and deep analysis! After work I will get into it and update the sample as well with the findings. . @ben-manes I think the main issue was the classloader. After using CachingProvider cachingProvider = Caching.getCachingProvider(Thread.currentThread().getContextClassLoader().getParent()); everything seems to work. I also updated the actuator endpoint which is not responding with some useful info.\n2 additional questions:\n- Is there a way to define default cache settings in application.conf? I tried with key default but that didn't seem to work (https://github.com/marceloverdijk/hibernate-l2-caffeine-jcache/blob/master/src/main/resources/application.conf#L4).\n- Is it possible to define a custom name (and possible location) for the application.conf file? E.g using caffeine-jcache.conf.\nAnother thing I'm unable to unwrap the jcache to the Caffeine Cache (https://github.com/marceloverdijk/hibernate-l2-caffeine-jcache/blob/master/src/main/java/com/example/actuate/JCacheEndpoint.java#L60). I think that would be the only way to get the actual statistics?\nThx again for your help!. No worries for no way to define default settings for unnamed configurations. It's no problem to define all caches. I was just wondering.\nI see Caffeine JCache is using the typesafe Config library for configuration. I wasn't aware of this library to be honest and thought application.conf was something specifically from Caffeine.\nIn a typical Spring Boot application there is an application.properties|yml file with the application configuration. Having a application.conf as well might be confusing, but I think it is what it is. It would be nice if there would be some integration between Config and Spring Boot so e.g. Config would re-use an application.yml file. But that's beyond the scope of Caffeine :-)\nRelated to the unwrap, I was doing:\nCacheProxy cacheProxy = (CacheProxy) jcache;\ncom.github.benmanes.caffeine.cache.Cache caffeineCache = cacheProxy.unwrap(com.github.benmanes.caffeine.cache.Cache.class);\nwhich resulted in a compilation error. I should have used:\nCacheProxy<Object, Object> cacheProxy = (CacheProxy) jcache;\ncom.github.benmanes.caffeine.cache.Cache caffeineCache = cacheProxy.unwrap(com.github.benmanes.caffeine.cache.Cache.class);\nI unfortunately cannot quite follow what you mean with the statistics..\nShould com.github.benmanes.caffeine.cache.Cache not be used when using JCache / Hibernate L2 cache? Should I use something different then? The JCache Cache interface has nothing to retrieve cache statistics as far as I can seen.\n. Yes I see a difference between the Caffeine stats and JCache stats.\nE.g. I did a test with 1 miss and I did some evaluation during debugging:\ncacheProxy.statistics.misses gives indeed 1, but\ncaffeineCache.stats().missCount() gives 0\nSo I need the JCache statistics. To bad there is no programmatic way to access the CacheProxy's JCacheStatisticsMXBean statistics as it is protected.\n. ",
    "buckelieg": "I've tried the faq proposal (https://github.com/ben-manes/caffeine/wiki/Faq#pinning-entries) but this states that weight is calculated at insertion time - i.e. \"once and for all\", for my use case I want to decide if the entry should be removed right at the moment of eviction stage after some condition is met.\nSuppose you have a user session cached and cache configured.\nOnce expireAfterWrite trigger is on - the session object is evicted, but the session itself (i.e. session last accessed time - creation time is lower than its timeout) is still alive. so that it has to stay in cache.\nMay be it is meaningful to introduce such a functionality with exceptional state in the docs that its usage brakes contract and use it at own risk somehow. @Maaartinus \n\nShouldn't you use expireAfterRead instead?\n\nYes. You're right. But it changes the things slightly according to the problem.\n@ben-manes \n\nMy biggest concern, though, is how easily error-prone this feels. A simple coding mistake could cause no eviction and, if not monitoring stats, a silent memory leak. So I'm hesitant until there is a really strong argument, because I'm afraid it might cause more harm than good.\n\nI think that there is no option to totaly avoid potential user bugs on this case. Clearly state this risks at the contract level (in javadocs, particularly) with detailed explanation could be a solution.\n\nBy using weight / expiration, there is the potential for the item(s) to be ordered such that they are not evaluated.\n\nCan you please show me how can I achieve this according to the example? I do not catch how can I re-weight entry dynamically.. is compute method invoked everey time the entry is accessed?\nThank you.. ",
    "alejo-17": "Thanks for the useful information and the prompt answer!\nI will try upgrading the caffeine version to 2.6.2 and see if that fixes the issue. If not, I will get more evidences to troubleshoot this issue.\nI appreciate your help!. I have tried by upgrading caffeine version to 2.6.2 and also using computeIfAbsent style, but it looks like these options didn't fix the issue. Is it possible that if the cache size is not enough, the scheduleDrainBuffers() takes more time than the expected due to the eviction process? Should I consider adding Time-based eviction mechanism? How should that work if Runnable::run is used as an executor?\nFor example, in the first 15 minutes of a test with concurrency I see the slowness of scheduleDrainBuffers  (about 15 seconds for one request):\n\n\nPlease let me know your comments/thoughts.\nThanks.. I have validated the jar used was 2.6.2 for the last test. Actually, I realized that the first test results I had sent were performed with caffeine version 2.3.5, which contains the JCTools fix. Is there any difference between these two versions?\nBy the way, it seems like everything returns to a steady state, since the service is only taking more time to respond, but I can corroborate that with thread dumps. Do you think it's expensive to use the same thread for maintenance tasks?. Yes, my workload writes heavy. For example, for one request the cache is hit on this way: \n- putIfAbsent -> 193k times approximately \n- get -> 529k times approximately\nIn addition, I have been testing without using the same thread (ForkJoinPool) for the executor and I noticed that the performCleanUpTask is being executed less times than using Runnable:run. Is there a difference on calling this task by using a different executor? For example, for a different test, but in the same request I have the following:\n\nSame thread: 140k times approximately\nForkJoinPool: 43k times approximately\n\nI will have a sample stress test once I could replicate this out from Modeshape.\nPlease let me know your comments/thoughts.\nThanks.. Thanks for suggesting those ideas. I have a couple of questions about them:\n\n\nAbout initialCapacity, are you suggesting I should set that value with a similar value of maxSize? For example if the cache size was 50k, would I set that value to 50k or maybe 25k?\n\n\nRegarding WRITE_BUFFER_MAX, I don't understand how that is related to the drain buffer, could you please provide an example of what you're proposing?\n\n\nBy the way, I'm working on enabling stats inside Modeshape code, so as soon as I get them, I will provide you with that information.\nThanks. Thanks for the information! I'm working on test these two approaches.\nHowever, when I had tried to gather statistics, I noticed that for get/put operations they are not being recorded. I think the reason is due to a parameter is always being sent with a false value.\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L1531-L1534\nI have noticed that computeIfAbsent will get statistics, but there's an statement that I found on this issue:\nhttps://github.com/ben-manes/caffeine/issues/164\nWhich says: The asMap() view does not record stats for non-computing methods (@yrfselrahc might remember the reasons why / rules for Guava). Caffeine does record them for computing methods to capture loads, and whether that was the correct choice depends on @yrfselrahc's view. Guava's recent addition of these methods doesn't provide insight since they sporadically record stats, so it wasn't a consideration when implemented.\nModeshape is using asMap() to create the cache, so how can I manage to collect statistics?\nPlease let me know your comments/thoughts.\nThanks.\n. Thanks for the information. I'm working on getting statistics manually.\nBy the way, I have one concern regarding the optimum cache size that I should use. I got a count from my database and I saw about 2 million of rows that can be cached, since Modeshape caches all by default, and I have 50k as a max cache size. I tried by increasing it to 500k, but I started to see lots of suspension times, so the performance has decreased. Is caffeine able to work with a bigger size cache? Does it require to have more JVM memory? I have currently 2.5 GB as max configured.. I was able to get some statistics after running a single test for one features and I got the following numbers:\n- Get call: 243,244 times.\n- PutIfAbsent call: 4752 times.\nTherefore, it says that the hit rate is around 96% and 98%, which concludes that the cache is not always missing the entries. Also, in a performance test the % of writing operations is less than reading operations:\n\nSo reading operations are taking time to drain buffers (maybe due to the concurrent calls?)\nPlease let me know your comments/thoughts.\nThanks.. I'm attaching the caller breakdown of scheduleDrainBuffers operations (from a different performance test):\n\nThe class is referencing the lambda of the executor: org.modeshape.jcr.cache.RepositoryCache$$dtt$$Lambda$283.534425031\nAlso I have found a different hotspot, with a short total execution time, but I'm not sure if that is related to the issue:\n\nThe problem is that Dynatrace is not showing in which part of the execute is the bottleneck, so I don't have that precision unfourtanetly.. Thanks for the information! I will get thread dumps to see if there are blocked threads. Just to be curious, what are the conditions to skip the scheduleDrainBuffers call?\nhttps://github.com/ben-manes/caffeine/blob/f107fbc8b93883ec5513916c6c7fb5a819e6b498/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L844-L846. Thanks for the information. What I also noticed is that I have two types of execute method in my reports for only one request:\nOne represented by this class: org.modeshape.jcr.cache.RepositoryCache$$dtt$$Lambda$283, and in summary they are executed fast (89 ms in total):\n\n\nThe other one represented by this class: org.modeshape.jcr.cache.RepositoryCache$$dtt$$Lambda$283.534425031, which has the same number as the above ($283), but with a extension number 534425031, so this is taking 3.16 seconds, which is very slow:\n\n\nThe one which concerns me is the last one, but I'm not able to understand what is the difference between these two lambdas if they are supposed to be doing the same.\nPlease let me know your comments/thoughts.\nThanks.. Yes, sorry about that. It was tested with caffeine 2.3.5 since I didn't see any improvement with 2.6.2. This is the test I did with last caffeine version:\nThis is the total execution of execute(Runnable) for the whole performance test:\n\nThis is only for one request of scheduleAfterWrite:\n\n. Yes, I think I have some pending items to test. \nBy the way, I have been running the Stresser test locally, but with some changes:\n- maxCacheSize = 50,000\n- initialCapacity = 50,000\n- executor = Runnable::run\n- removed cleanUp: cache.cleanUp();\nFor reading operations:\n---------- 00:01:04 ----------\nPending reads: 29; writes: 0\nDrain status = Idle (0)\nEvictions = 998,576\nSize = 50,000 (max: 1,048,576)\nCache Size = 50000\nLock = [Locked by thread pool-2-thread-16]\nPending tasks = 0\nMax Memory = 3,817,865,216 bytes\nFree Memory = 219,395,224 bytes\nAllocated Memory = 324,534,272 bytes\n---------- 00:01:09 ----------\nPending reads: 16; writes: 0\nDrain status = Idle (0)\nEvictions = 998,576\nSize = 50,000 (max: 1,048,576)\nCache Size = 50000\nLock = [Locked by thread pool-2-thread-8]\nPending tasks = 0\nMax Memory = 3,817,865,216 bytes\nFree Memory = 219,395,224 bytes\nAllocated Memory = 324,534,272 bytes\nFor writing operations:\n---------- 00:02:54 ----------\nPending reads: 16; writes: 0\nDrain status = Idle (0)\nEvictions = 568,545,387\nSize = 50,014 (max: 4,096)\nCache Size = 50080\nLock = [Locked by thread pool-2-thread-15]\nPending tasks = 0\nMax Memory = 3,817,865,216 bytes\nFree Memory = 321,357,288 bytes\nAllocated Memory = 1,146,093,568 bytes\n---------- 00:02:59 ----------\nPending reads: 48; writes: 6\nDrain status = Required (1)\nEvictions = 586,133,639\nSize = 51,518 (max: 4,096)\nCache Size = 51299\nLock = [Locked by thread pool-2-thread-4]\nPending tasks = 0\nMax Memory = 3,817,865,216 bytes\nFree Memory = 936,253,216 bytes\nAllocated Memory = 1,538,260,992 bytes\nI'm not pretty sure how to interpret the results, but I can see locks for the executions, can you please help me to interpret these results?\nPlease let me know your comments/thoughts.\nThanks.. After testing by using the ForkJoinPool as an executor instead of Runnable::run, I'm no longer seeing long execution times on the execute method, so I'm not sure why the issue happens by using the current thread to drain. However, I still see the com.github.benmanes.caffeine.cache.BoundedLocalCache.dt_0_expirationTicker_501() as a hotspot.\n\n\nThe host where the tests are executed has only 4 cores, so I'm not sure if that number is not enough for the buffer's calculation.\nPlease let me know your comments/thoughts.\nThanks.. ",
    "Siva-R": "Ben-Manes, the picture in the first comment which Alejandro had was related to a read operation, there were no write operations in his test, but still the execute method is being called several times. Is it not an expectation that, when the execute method is called for the first time, it will free up enough nodes to avoid calling it several times?. Ben-Manes,\nCouple of questions.\n1) Is it not that when ring buffer is full, scheduleDrainBuffers() will be called? \n2) If yes, is there a possibility that because of small ring buffer size, scheduleDrainBuffers() is called frequently? \n3) Will there be an improvement if we increase the ring buffer size? Can you suggest an ideal size depending on the load?\n4) Is the ring buffer size dependent on the type of thread pool executor?. ",
    "Wiiimm": "Okay, that worked so far.\nBut when I want to execute this code, it doesnt:\n```\n        com.github.benmanes.caffeine.cache.LoadingCache cache = Caffeine.newBuilder().expireAfter(new Expiry() {\n            @Override\n            public long expireAfterCreate(String key, Long value, long now) {\n                return TimeUnit.SECONDS.toNanos(5);\n            }\n        @Override\n        public long expireAfterUpdate(String key, Long value, long now, long current) {\n            return current;\n        }\n\n        @Override\n        public long expireAfterRead(String key, Long value, long now, long current) {\n            return current;\n        }\n    }).removalListener((string, timestamp, removalCause) -> System.out.println(Duration.ofMillis(System.currentTimeMillis() - timestamp)))\n            .executor(Runnable::run)\n            .build(key -> {\n                long time = System.currentTimeMillis();\n                System.out.println(key + \" | \" + new Date(time));\n                return time;\n            });\n    for (int i = 0; i < 5; i++) {\n        cache.get(\"1\");\n        System.out.println(\"Iteration #\" + i);\n        Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n    }\n\n    //Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);\n    //cache.get(\"1\");\n\n```\nDo I first have to get the value if it\u00b4s already expired to trigger the removal listener? I thought it is triggerd after these 5 seconds, no matter what I do with the cache. Oh okay.\nI basically want to have a Cache that loads all MarketItems from a db and get\u00b4s the time when the \"MarketItem\" was created and then I want to expire it so it triggers the removeListener 24 hours (or whenever the MarketItem is loaded from the db) after the create of the MarketItem. Is there a \"workaround\"?. Well, I think the ScheduledExecutorService would do the job. But it would be nicer if the listener is triggerd when the entry is expired without having to access the cache first but as you said, it will be in a Updated version with JDK 9 soon, so I can work with that. Thank you for your support. PS: Sorry for my bad english lul. ",
    "hc-codersatlas": "I'm not so sure that VarHandles has better performance than Unsafe. Static VarHandles maybe but that's not really suitable for the use case. Just to note that in Java 10 there's also jdk.internal.misc.Unsafe which has *unaligned access methods which may enable even more performance (but it's hidden and restricted with required --add-exports / --add-opens). sun.misc.Unsafe is a wrapper around the internal version that lacks a few methods.. ",
    "phylexx": "Thanks for your quick reply.\nI read in other issues, that you only override the Executor for test purposes. In our case, Caffeine is used as an application cache for slow changing data (combined with Spring @Cacheable). But we prefer libraries do not rely on ForkJoinPool by default and would like to set a dedicated Executor for a given library. The goal is to segregate resources to avoid libraries to compete for ForkJoinPool threads.\nI agree I can achieve my goal by using the JCache/Caffeine API's, this issue is a non blocking one. The Executor is the only configuration part which is not copied from TypeSafe's config to CaffeineConfiguration. The idea of this issue is to try to close the gap since it took some time to figure out how to do it :) \n. Our first try was to use Spring Caffeine native support, but we are using a 4.1.x spring which was not providing it, we then fallback to jcache integration.\nWe are quite confident about your usage of FJP. Another point is that the need for FJP is hidden when using default Caffeine configuration,  customizing the conf will express more clearly in the app the requirement.\nBut we can definitely live without this improvement since this issue is providing 3 workarounds :\n the one in the original issue message\n yours : create cache, get conf, customize it, destroy cache and recreate it using customized conf.\n* use spring Caffeine native integration\nI let you choose,  if this is a real need for Caffeine in the future. This issue will at least document a few ways to customize the executor.\nThanks for your answers and the interisting job you have done with Caffeine.. Hi Ben,\nThank you for taking this feature request into account.\nI'm glad you found some time to release a new version of Caffeine, we will test it as soon as possible !\n. ",
    "chongyangxue": "Thanks, Caffeine.executor(Runnable::run) solved the problem. \nBut still confused of failed refresh operation when using ForkJoinPool#commonPool, even after TimeUnit.MILLISECONDS.sleep(1000);. ",
    "bojanv55": "Oh. I just tried it and it worked all the time. But I guess it was just luck?. Ok then, I guess standard one will work:\nKey key = index.get(comp.getSomeOtherVal());\n            if(key!=null){\n                cache.invalidate(key.getSomeOtherVal());\n            }\n~~or I guess this will work also~~\n~~cache.invalidate(index.get(comp.getSomeOtherVal()));~~. Btw, is there any other way (best practice) to have multikey -> value mapping? I needed this since cache.asMap().keySet().removeIf(comp::isSame) is super slow when using other attribute for comparison.. What you think about some implementation like this - should it be thread/GC safe?\n```\npublic class IndexedCache implements Cache {\n@Delegate\nprivate Cache<K, V> cache;\nprivate Map<Class<?>, Map<Object, Set<K>>> indexes;\n\nprivate IndexedCache(Builder<K, V> bldr){\n    this.indexes = bldr.indexes;\n    cache = bldr.caf.build();\n}\n\npublic <R> void invalidateAllWithIndex(Class<R> clazz, R value) {\n    cache.invalidateAll(indexes.get(clazz).getOrDefault(value, new HashSet<>()));\n}\n\npublic static class Builder<K, V>{\n    Map<Class<?>, Function<K, ?>> functions = new HashMap<>();\n    Map<Class<?>, Map<Object, Set<K>>> indexes = new ConcurrentHashMap<>();\n    Caffeine<K,V> caf;\n\n    public <R> Builder<K,V> withIndex(Class<R> clazz, Function<K, R> function){\n        functions.put(clazz, function);\n        indexes.put(clazz, new ConcurrentHashMap<>());\n        return this;\n    }\n\n    public IndexedCache<K, V> buildFromCaffeine(Caffeine<Object, Object> caffeine) {\n        caf = caffeine.writer(new CacheWriter<K, V>() {\n\n            @Override\n            public void write( K k, V v) {\n                for(Map.Entry<Class<?>, Map<Object, Set<K>>> indexesEntry : indexes.entrySet()){\n                    indexesEntry.getValue().computeIfAbsent(functions.get(indexesEntry.getKey()).apply(k), (ky)-> new HashSet<>())\n                    .add(k);\n                }\n            }\n\n            @Override\n            public void delete( K k,  V v,  RemovalCause removalCause) {\n                for(Map.Entry<Class<?>, Map<Object, Set<K>>> indexesEntry : indexes.entrySet()){\n                    indexesEntry.getValue().remove(functions.get(indexesEntry.getKey()).apply(k));\n                }\n            }\n        });\n        return new IndexedCache<>(this);\n    }\n}\n\n}\n```\nand here is quick test:\n```\n@AllArgsConstructor\n    @Data\n    @EqualsAndHashCode(onlyExplicitlyIncluded = true)\n    static class CompositeKey{\n        @EqualsAndHashCode.Include\n        Integer k1;\n        String k2;\n        Long k3;\n    }\npublic static void main(String[] args) {\n\n\n    Caffeine<Object, Object> cfein = Caffeine.newBuilder().softValues().maximumSize(200_000);\n\n    IndexedCache<CompositeKey, String> cache = new IndexedCache.Builder<CompositeKey, String>()\n            .withIndex(Long.class, ck -> ck.getK3())\n            .withIndex(String.class, ck -> ck.getK2())\n            .buildFromCaffeine(cfein);\n\n\n    for(int i=0; i<100; i++){\n        cache.put(new CompositeKey(i, String.valueOf(i), Long.valueOf(i)), \"sdfsdf\");\n    }\n\n\n    for(int i=0; i<10; i++){\n        //use equals method of CompositeKey to do equals comp.\n        cache.invalidate(new CompositeKey(i, String.valueOf(i), Long.valueOf(i)));\n    }\n\n    for(int i=10; i<20; i++){\n        //use Long index\n        cache.invalidateAllWithIndex(Long.class, Long.valueOf(i));\n    }\n\n    for(int i=20; i<30; i++){\n        //use String index\n        cache.invalidateAllWithIndex(String.class, String.valueOf(i));\n    }\n\n\n    int y = 4;\n\n}\n\n```\n. Thanks. Now just to see which data structure to use instead of HashMap, since this one occupies too much memory.... ",
    "imasahiro": "Oh, thanks for merge. Super fast! My IntelliJ inspection suggested to fix :-). ",
    "NileshChaukade": "Yes Ben, the scan is limited to my package.\nAlso, the ulimit was increased to 65K ,but Caffeine crossed that as well.\nOn Wed 14 Nov, 2018, 19:53 Ben Manes, notifications@github.com wrote:\n\nCaffeine does have many generated classes, but they should be fine as only\nthe configurations used will get class loaded. Does your system use class\nscanning that includes this library to force loading of all classes? If so,\ncan you limit it to your packages?\nYou may want to increase the FD limit as it\u2019s low by default on Linux, if\nI recall correctly.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/ben-manes/caffeine/issues/281#issuecomment-438678724,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/Aq8csDE843T0cVCm-JOu5m00HINOU9EIks5uvCd3gaJpZM4YdSvK\n.\n. \n",
    "spand": "That is fair. Skimming the code I thought it was an explicit choice looking at: \nboolean hasExpired(Node<K, V> node, long now) {\n    if (isComputingAsync(node)) {\n      return false;\n    }\n...\nor maybe handled just in time in get().\nCould we instead (or until fixed, it seems you mention forking the hash table quite a lot in the issues) have this documented explicitly in the javadoc for the refreshAfterWrite?. ",
    "khalidr": "That worked! Thanks for the quick response.. ",
    "lansuiyun": "Thanks! . ",
    "manoj13023": "Current value anyway I am getting. Wanted to reset the refresh timer. When I call getIfPresent second time after exception, it should not call reload function for next 10 sec.. That looks an easy option. Thanks. ",
    "mmmkkaaayy": "Thanks for the quick answer!\nGiven Caffeine's position as a low-level performance library, don't you think it makes sense to provide this behavior to users as a feature? Even if it's a hacky implementation, I think developers of low-latency/realtime systems may prefer the predictability and performance of immediate exception propagation.\nAlso, can please you comment on the best way to immediately invalidate the wrapped entry in the cache? If I have an Expiry implementation to specify a dynamic TTL per entry, I could just set expiry to currentTime?. Can you elaborate on how rate limiting would be a solution to this problem? I don't see how it would help achieved the desired behavior of fail-fast to all callers on an exception.\nAlso, the AsyncLoadingCache solution is not truly identical to Guava as the work should be done by a calling thread, not on ForkJoinPool/Executor. But it's definitely an alternative to consider.... Ah cool, didn't know you could set the executor like that to use the calling thread. I will experiment with the proposed solutions and see which one feels the best.\nThanks for your help!. ",
    "arkrost": "Ok, thanks for explanation. I only faced this case during unit testing and was confused with behaviour. In production code we already use retry policies, so it shouldn't be a problem. Moreover, we can fix it on our side. . ",
    "chainyu": "@Bean(name = CACHE_MANAGER_NAME)\n    public CacheManager caffeineCacheManager() {\n        SimpleCacheManager cacheManager = new SimpleCacheManager();\n    ArrayList<CaffeineCache> caches = new ArrayList<CaffeineCache>();\n    for(Caches c : Caches.values()){\n\n        caches.add(new CaffeineCache(c.getName(), Caffeine.newBuilder().recordStats().maximumSize(c.getMaxSize()).refreshAfterWrite(c.getTtl(), TimeUnit.SECONDS).build(cacheLoader())));\n\n    }\n    cacheManager.setCaches(caches);\n    return cacheManager;\n}\n\n@Bean\npublic CacheLoader<Object, Object> cacheLoader() {\n\n    CacheLoader<Object, Object> cacheLoader = new CacheLoader<Object, Object>() {\n        @Nullable\n        @Override\n        public Object load(@Nonnull Object o) throws Exception {\n            return null;\n        }\n    };\n    return cacheLoader;\n}.\n",
    "bemasc": "Thanks, that sounds like a good plan.\nAndroid does support Java 8, and indeed is running on OpenJDK 8, so perhaps you should revise the rationale there.  Caffeine currently works fine on Android API 24-27, but not 28.. ",
    "felixbarny": "Note that I'm not a caffeine user but it's probably a nice optimization for those who are :). > If I could use the trick, I'd embed the version number into the class name using some trick in the build.\nHm, that may actually work but would require some effort. You can have a look at the different net.bytebuddy.dynamic.loading.ClassInjectors. Maybe it's worth having an optional dependency on Byte Buddy and only doing the injection via Byte Buddy if it's on the classpath.. ",
    "jacek-jablonski": "Thank you for clarification!. ",
    "msridhar": "I'm still on JDK 8 as well.  I tried removing that if condition in codeQuality.gradle but still didn't see an error.  It seems that none of the error prone config in that file is getting applied to the :caffeine:compileJavaPoetJava task that actually compiles the Java code.  (I added an unused import but the RemoveUnusedImports check did not fire.)  I'm not sure why; it seems it should be a JavaCompile task and so things should work.. ",
    "avboy72": "Thanks Ben.\nI should note that we are also using the spring boot annotations.  \n@Cacheable(value = cacheName, keyGenerator = \"hashingKeyGenerator\")\n. If we used Caffeine directly... is their a way to get the cache item expiration time? Also, a way to store on the cache item if the immolation has already occurred?. basically what I mean by immolation is to sacrifice a single get on the cache by NOT returning the existing cached item that hasnt yet expired... some amount of time prior to expiration.  This way a single thread would get the resource and replace it within the cache with a new version of it before the item is actually evicted from the cache.  Allowing other threads to continue using the cached item.\nIts kind of like a zero downtime cache item refresh.. Yeah, makes sense... but then we would have to store the sensitive information in the cache so that the background Executor can refresh the item.. ",
    "JLLeitschuh": "Gradle does not verify checksums. They do use checksums to see if they need to pull an updated version from the repository.\nThis could impact users since any maliciously downloaded code is executing in the context where the releases are produced. The malicious code could have maliciously compromised the build artifacts before upload.\nPublic Disclosure\nOption 1: File for a CVE\nA project maintainer for this project should probably file for a CVE number to inform the public about this vulnerability in the build for this project. The goal is to inform the public that there was a potential for published build artifacts to have been maliciously compromised in earlier releases.\nIf a maintainer on this project works for or is associated with a CNA, please have them file it with them:\ncve.mitre.org/cve/request_id.html\nOtherwise, an open source CVE should be filed for here:\niwantacve.org\nOption 2: Manually validate the release artifacts\nIf this project's build is fully reproducible. An alternative to filing for a CVE is to go back and build the earlier releases (with the HTTPS patch applied) to confirm the artifacts were not tampered when they were built. This can be done by comparing the hashes of the artifacts built locally with the ones published. If the hashes of all previous artifacts match those that are published, you can safely assume that the releases were not tampered with.\nAgain, this assumes that the build if fully reproducible and will require significantly more work.. The gradle-witness plugin works fine with Gradle 5.1 as evidenced by the Signal-Android build using it without issue with Gradle 5.1.\nhttps://github.com/signalapp/Signal-Android\nhttps://github.com/signalapp/gradle-witness\n\nThe build is reproducible and everything appears to match.\n\nAWESOME! That takes the requirement to disclose off the table. Glad you are in the clear.. ",
    "burka": "Happy to help :-). ",
    "cschockaert": "perhaps i missed a point, you are saying that the eviction metrics i actually see is the eviction due to max cache size reached only ? \ni thought it was the sum of all eviction events, max size + expiration+ ...\nin fact i just need a metric that is reporting the eviction due to max cache size but not expiration.. I saw that RemovalListener offer the behaviour i need, i'll dev some custom metrics around it. OK thanks. Yes should be a better solution than using RemovalListener.. ",
    "bkuberek": "That's great! Thank you!. Sounds good. Thank you. ",
    "justgame": "I'm not so sure. I just configure caffeine in a springboot project, and use spring.cache.caffeine.spec=recordStats=true,maximumSize=5000,expireAfterAccess=600s, then it throws a   IllegalArgumentException with message \"record stats does not take a value\". Sorry, it's my fault. Thank you for your explanation.. ",
    "diracccc": "Thanks, the wiki helps.\n. ",
    "sheepdreamofandroids": "Actually I did an implementation using rxjava and the buffer() operator but I'm not completely happy about it.\nFirstly the buffer() operator produces useless empty batches. They can easily be ignored but it just doesn't feel right.\nSecondly it's quite easy to miss completing some futures which makes the whole thing less robust than I like. This is the main reason I like to abstract it out.\nThirdly, after a batch of keys has been processed, the times shouldn't start right away, but only when the first key is received to avoid non-optimal small batches.. ",
    "rtyley": "Ah thanks for that - I would have liked to have contributed a PR for this but to be honest I don't think I'd be confident doing that refactoring.. "
}
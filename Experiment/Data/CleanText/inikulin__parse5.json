{
    "inikulin": "Hi,\nAccording to spec parser doesn't distinguish attributes with value and attributes without value (parser should treat boolean attributes as an attribute with an empty value) :\n\nSet that attribute's name to the current input character, and its value to the empty string.\nhttp://www.w3.org/html/wg/drafts/html/master/syntax.html#before-attribute-name-state\n\nThis caused by the fact that attribute values are handled by the later stage of the page processing in browser (e.g. building render tree, when it decide to not consider  checked attribute value and just checks it's presence).  So, for the browser and parser <input type=\"checkbox\" checked> is an equivalent to <input type=\"checkbox\" checked=\"\">). If you have some specific issue caused by such behavior could you please describe it in details. I'll try to assist you to find workaround for it.\n. Yes, checked attribute value has restrictions only in the XHTML. In HTML (4 and 5) only presense of the attribute matters and it can have any value. So if you perform conversion beetween HTML and XHTML you need to handle boolean attributes manually.\n. Hi, adapters and serializer currently in the dev stage. I'll prepare new release version in the next few days.\n. New version (0.8.2) now available from npm. Check out documentation for the new API's description.\n. Hi,\nThanks for your interest in my project.\n Instantly parse5 was built to be used in node js apps. However it doesn't have any nodejs/external modules dependencies and it doesn't use any ES5/ES6 features. So, it can be used in all browsers (including older IE). However, currently it's distributed via npm as node js module. But you can easily build browser lib using e.g. browserify. \nRegarding testing - you can just delete parse5 tests due to the fact that all release versions are already automatically tested on my side. If you want to modify parse5 code and test changes you still can run nodeunit tests in browser (see doc).\nPlease, let me know if you will need any additional assistance.\n. Hi, this is awesome news. default tree adapter was an original tree format for parse5 and it was designed to fit better into W3C-proposed HTML5 parser algorithm. Later I've implemented htmlparser2 tree adapter, so parse5 can be plugged into existing solutions that uses htmlparser2 tree format. There is a little performance penalty, but it's pretty insignificant. For jsdom I suggest to keep htmlparser2 tree format because given performance benefits will not cost effort spent on the tree format change. However,  htmlparser2 tree adapter just mimics HTML Node's interfaces provided by htmlparser2, many other details (like named entity representation in text nodes, tree-hierarchy) may obviously differ from htmlparser2 due to completely different parsing algorithm (but it will almost same as in modern browsers). \n. parse5 handles framesets the way it proposed in the W3C spec. \nThere is no difference here beetwen parse5 algorithm (https://github.com/inikulin/parse5/blob/master/lib/tree_construction_stage/parser.js#L2901) and e.g. Blink (https://github.com/mirrors/blink/blob/master/Source/core/html/parser/HTMLTreeBuilder.cpp#L1277).\nSo, I need more information on this issue. Can you, please, provide me source HTML snippet and expected output.\n. @Sebmaster I'm happy to hear that issue was resolved. Can I close it for now?\n. I'm ok with it. Version 1.0.0 was published to npm\n. Hi,\nAccording to HTML5  spec:\n\nWhen the user agent leaves the attribute name state (and before emitting the tag token, if appropriate), the complete attribute's name must be compared to the other attributes on the same token; if there is already an attribute on the token with the exact same name, then this is a parse error and the new attribute must be removed from the token.\n\n(see: http://www.whatwg.org/specs/web-apps/current-work/multipage/syntax.html#attribute-name-state)\nSo, this functionality can't be achieved without spec violation and thereby can't be implemented in master branch of parse5. However, you can easily workaround this behavior for your project by forking parse5 and removing this condition, so modified code will look like this:\njavascript\nTokenizer.prototype._leaveAttrName = function (toState) {\n    this.state = toState;\n    this.currentToken.attrs.push(this.currentAttr);\n};\n. Update:\nAfter suggested change parse5 may produce browser-inconsistent tree structure in some cases. Because Noah's Ark clause used in tree construction stage havely rely on token attribute's key/value. Use it on your own risk.\n. @eGavr \nBreaking Noah's Ark clause may cause breaking of Adoption Agency algorithm which should deal with misnested content.\nConsider following HTML input:\nhtml\n<html>\n<head></head>\n<body>\n<p><font size=4 size=1><font size=4 size=2><font size=4 size=3><font size=4 size=4><p>X\n</body>\n</html>\nCurrent parse5 implementation will produce following output (which is consistent with browsers) after parsing/serialization:\n``` html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                X\n            \n\n\n\n\n\n```\nProposed change will cause the following output:\n``` html\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                    X\n                \n\n\n\n\n\n\n```\nAs you can see this change causes browser-inconsistent tree structure.\nSuch cases looks rare and complicated, but however, they can be often met on many websites around the net. JYI.\n. The are two requirements for parse5 which should be never violated:\n- To be HTML5 spec compliant\n- To be as fast as possible\nCurrent mechanism of handling parsing errors is proposed by HTML5 specification, moreover all modern browsers behave the same way.\nIf you find it hard to fork parse5 then you can take a look on forgiving HTML parser for node(e.g. htmlparser2), which are not spec-strict, so they may meet your requirements.\n. It's not a bug or a problem, it is an expected and correct behavior. parse5 can also produce htmlparser2 tree format on which many projects rely. So, in any terms this behavior can not be added to parse5, I still suggest you to create fork, it's pretty simple. \n. Reopening for now, since now we have SimpleApiParser and I'll try to provide you with workaround.\n. Add following patch for your SimpleApiParser instance and it will be able to work with duplicate attributes:\n``` js\nvar parser = new SimpleApiParser({\n    //Your handlers here\n});\n//Patch\nparser._reset = function (html) {\n    SimpleApiParser.prototype._reset.call(this, html);\n    this.tokenizerProxy.tokenizer._isDuplicateAttr = function () {\n        return false;\n    };\n};\n//Use it\nparser.parse('');\n```\n. Hi,\nstart tags and end tags are tokens in HTML5 syntax, so they are exist only on the lexical analysis stage.\nThe final product of the parsing algorithm is an abstract syntax tree which is in our case is a DOM-tree.\nDOM-tree dealing with HTMLNodes which doesn't contain low-level lexical information (such as existence of self-closing tags). So, this task can't be accomplished using DOM-tree. \n. @eGavr \nI've just rolled out new version (v1.1.0) of parse5 which now has SAX-style parser. It doesn't produce DOM-tree, so you will not face problems like this one and #13. \n. This is a correct handling of parsing error, which is consistent with spec and browsers. \nFrom chrome javascript console:\n``` javascript\n\nvar div = document.createElement('DIV')\n< undefined\ndiv.innerHTML = 'Copyright content';\n< \"Copyright content\"\ndiv.innerHTML\n< \"Copyright content\"\n```\n. Hi \nYes, we have some =): \n- whacko - my cheerio fork that uses parse5 instead of htmlparser2. Heavily used in production in TestCafe framework.\n- minidom - simple DOM level1 implementation.\n- jsdom - will start using parse5 from next release (v1.0.0). However, someone has already published dev version of jsdom which uses parse5 to npm: https://www.npmjs.org/package/js-dom.\n. Hi.\n\nIt's possible, but your task requires much more systems to be implemented (such as full DOM implementation, script executor, etc.) and can't be accomplished using just raw HTML parser. I suggest you to take a look on jsdom, since it's already have a desired functionality and it's currently using parse5. jsdom's document.write implemetation can be found here. I think it's a good starting point to figure out how it can be implemented.\n. Update: \nBut if you still feel the passion to accomplish your task, I can provide you with the following algorithm:\n1.  Create proxy for one of the parse5 tree adapters.\n2.  Proxy adapter's insertText method.\n3.  Pass this adapter to the parser's constructor\n4.  In your proxied insertText method call the original insertText method first, then check if parentNode is a <script> element. If so:\n   - store parentNode-element's parent as insertionSite\n   - run your JS executor with text\n   - intercept document.write calls, then parse provided markup with document fragment parser and pass insertionSite as a fragmentContext\n   - append documentFragment's child nodes, obtained on the previous step, to the insertionSite.\nHope this helps!\n. Important note: since parse5 was not designed to be reentrant you will need to create new instance of parser for fragment parsing within document.write calls. There may be more than one additional instance required, since document.write may cause nesting, e.g.: \njs\ndocument.write('<script>document.write(\"<div></div>\")</script>').\nPlease, let me know if you have any additional questions. \n. Yes, functional testing is a huge PITA nowadays. I can recommend you to take a look on TestCafe. It is the commercial product I'm working on. Maybe it will be useful for you.\n. Thank you! This is that happens when you use Windows machine instead of your Mac laptop for development =). P.S. I would like to revert indentation in package.json to 4 spaces instead of 2 spaces, proposed by fixpack.\n. Fixed version was published to npm\n. Yes, Windows file system is case insensitive, therefore tests didn't fail on my side. I had thoughts about adding TravisCI for a long time, but it always ended up with \"Meh, I'll do it a little bit later\". And this issue is my punishment =). Actually, I never mentioned that npm uses 2 spaces indentation untill I did npm install <package> --save yesterday for one of my projects. So, moving to 2 spaces makes sense and I like .editorconfig approach. Many thanks once again!\n. Arghhh, it's nix specific, tests passes on Windows, since it's FS is case insensitive. Will merge @slang800's pull request asap\n. Fixed with #16. New version was published to npm. Sorry for the inconvenience.\n. Thank you!\n. @eGavr The issue is that patches can be applied infinitely to make parse5 work with thing that it's was not intended to work with. You shouldn't \"hammer nails with a microscope\". Despite the fact that XML and HTML looks similar (due to common ancestor - SGML) they are quite different languages. It's just like trying to parse C++ with JavaScript parser just because they both have curly braces.  So, if you want to parse XML, then use XML-parser and If you want to parse HTML, then use HTML-parser. That's it.\n. @Sebmaster \nSorry, I accidentally deleted your comment (why the hell I can delete others comments on github?).\nParse5 currently doesn't support write chunk->parse chunk->write chunk->parse chunk->... - streaming. I can't actually figure out how to do it right at the moment, because it will require some Tokenizer inner state hybernation mechanism (since HTML5 is the context-sensitive grammar). I have a couple a thougths how to do that. The easiest way will be using ECMAScript6 generators, but since they are not available yet even in latest stable node, it's not an option. However, I have idea how to implement hybernation using ECMAScrip5 (in case of emergency I can just write generator-based code and run it through regenerator), but this requires some research. If you mean support of the write whole html->parse chunk->emit DOM subtree->parse chunk->emit DOM subtree... then it can be already achieved using tree adapter. \nShort story long. Just let me know if you need something from my side.\nUpdate - restored your comment from e-mail:\n\n@Sebmaster wrote:\nThanks for your offer to help here, we really appreciate your responsiveness with these issues.\nI'm currently planning to rewrite our htmltodom.js because it's really ugly and we have to think about >what to do with all these differing parser interfaces - some seperation has to happen at least.\nMy current plan is to switch to a streaming api where possible, if I'm informed correctly you have that >somewhere in parse5. Personally I'd go with some kind of additional event there (or put it into the >tagclose event) which provides a writeHtml function as parameter, as to not give script tags some >kind of special treatment.\nDo you think such an event would be possible?\n. @aredridel\nThe main thing I worry about here is a performance penalty. Need to dig deeper in it.\n@Sebmaster \nCurrently - no, but I'll do a research on weekend if it's required. Regarding your question about special event: tree building doesn't opperate on start tags/end tags. parse5 provides SimpleApiParser, but it's not an option for jsdom. However, you can already newly created/appended <script> elements in tree adapter (just proxy htmlparser2 tree adapter methods).\n. @Sebmaster \nWhew, I had a hard long day and my reading skills degrades =)) Missed the point of your comment in my previous answer:\n@inikulin I thought it'd be possible to use the latter possibility, but when some DOM subtree is \nemitted, check if it's a script tag, then call a writeHtml function given as a parameter to the event, >which internally would prepend that chunk of html to an unprocessed buffer.. Will that not work with the >current structure of parse5?\n\nYes, it's possible with current structure. And it will be easy to implement. writeHtml can be passed to tree adapter's appendElement and insertBefore method (need to take a look when script execution is running e.g. in Blink, since I think there may be farther DOM mutations which may cause script re-invocation). Is this approach will be fine for you?\n. @aredridel It doesn't metter, since you was a pioneer of the HTML5 parsing for node. So, kudos goes to you in any way =)\n. @Sebmaster Yes, no problem, I would like to participate. \nRegarding document.write: I just remembered I already sugested approach in #15 which will not require any changes to parse5. If it's not quite clear then I can prepare gist tomorrow.\n. @Sebmaster yep, right. So we need to use previously discussed approach. I was worried about insertBefore because it used in foster parenting. But seems like <script> tags are never foster parented, so we need to handle only appendElement.  So, to make a resume: providing writeHtml method as an argument to the appendElement tree adapter method will be a suitable solution?\n. @Sebmaster Deal! I hope I will be able get my hands on parse5 this weekend and prepare branch for you. \n. @Sebmaster @domenic Yesterday I've started the new branch that implements document.write. And the conclusion is not as simple as it seems. document.write requires script to suspend the parser. Moreover, in interactive user agents any <script> that is not marked with async attribute suspends the parser. There are 3 ways to trigger suspension:\n1)   Parser leaves text parsing insertion mode inside <script> element. <script> may have both src attribute and innerText. \n-   In we have innerText: synchronously execute the script and resume parser (this fits well into current parse5 API and I already implemented it).\n-   If we have src attribute: we should fetch script first then behave the same ways as if we have innerText. This introduces async behavior to the parser. So it's definetly require new async API in parse5. It's not a big deal actually.\n2)    If we are in <script> and we have attached new documentFragment to document. Since documentFragment created on DOM-level and it has nothing to do with the parser the following steps should be performed by DOM then documentFragment is attached to document:\na.   Search for <script>'s inside fragment and if we have one or more perform the steps below\n b.   Do not resume the parser yet.\n c.   If <script> has document.write calls then use documentWrite provided by the main document parser.\n d.   If <script> has src attribute: fetch it.\n e.   Execute the script. If it inserts new <script> elements push it to the stackOfPendingScripts\nf.     If the stackOfPendingScripts is empty then resume the parser\n g.   Otherwise pop <script> from the stackOfPendingScripts and goto step b. for it.\n3)   Because now any script execution suspends the parser and it still in incomplete state while we process scripts and parse5 parser was not designed to be reentrant you need another instance of parser handle innerHtml assignments. Then innerHtml is parsed with assisting parser it should be treated as attached documentFragment and steps described in 2) should be performed for it.\nThen document is fully parsed (document.readyState === 'interactive' || document.readyState === 'complete') things should get back to normal and everything should work as it works currently in jsdom.\nEverything that described can be easily implemented on my side. However I don't have a clue how to make a nice API for this yet and I'm not sure if it actually should fit into parser functionality (maybe separate project, e.g. jsdom-parsing-platform?) . In context of jsdom seems like it ruins the idea of the unified parser interface and I don't know how it will be possible to perform steps above for XML (since it uses htmlparser2). So more questions arise. \nPlease, let me know that do you think.\n. @Sebmaster I've baked a new API which will alow us accomplish this task. parse5 will provide new class:\nClass: InteractiveDocumentParser\nInteruptable async parser which provides <script> handling and document.write functionality. All methods of the InteractiveDocumentParser are reentrant and async environment-frendly which means that you will not need to recreate  parser everytime for fragments and document parsing. Normally, you will have just one instance of this parser.\nInteractiveDocumentParser.ctor([treeAdapter])\nConstructs new instance of the InteractiveDocumentParser.\nInteractiveDocumentParser.parseFragment(html, fragmentContext)\nJust reentrant version of parseFragment\nInteractiveDocumentParser.parse(html, callback)\nReentrant routine. Asynchronously parses given html. Returns ParserController object. callback is called when parsing is complete. and receives document node. \nObject: ParserController\nProvides routines which allows control parser's control flow.\nParserContoller.suspend()\nSuspends the parser. E.g. can be used to fetch <script> with src.\nParserContoller.resume()\nResumes the parser.\nParserController.documentWrite(html)\ndocument.write some html.\nParserController.setScriptHandler(handler)\nSets handler(document, scriptElement) which will be called each time parser encounters and parses <script> element. document is a document node in it's current state, so you can feed it to the script. scriptElement is an element which caused handler invocation.\nExample:\n``` js\nvar parser = new InteractiveDocumentParser(parse5.TreeAdapters.htmlparser2);\nvar parserController = parser.parse(html, function (document) {\n    //We done\n});\nparserController.setScriptHandler(function (document, scriptElement) {\n    if (scriptElement.attribs['src']) {\n        parserController.suspend();\n    //Fetch script\n    request(scriptElement.attribs['src'], function (err, res, scriptContent) {\n        executeScript(scriptContent, document, parserController.documentWrite);\n\n        //Let's imagine that script doesn't insert any new <script> \n        //tags with 'src' attribute, so we can resume here.\n        parserController.resume();\n    });\n\n    //Handle scripts with innerText here...\n\n    //But be careful, because <script> can also have 'src', so you need to resume\n    //only then script fetched and innerText is executed to not screw things up.\n    //Normally you should execute 'innerText' first.\n    //Then you need 'pendingScripts' stack and push scripts onto it one the one\n    //of the following cases:\n    //\n    //1)Current <script> tag has 'src' attribute.\n    //2)Executed script content caused insertion of the new <script> tag\n    //(via 'innerHTML' property or 'document.appendElement()'). This new <script>\n    //should be pushed onto stack.\n    //\n    //* If parser was not suspended and 'pendingScripts' stack is empty \n    //then suspend the parser\n    //* If after execution of the current script 'pendingScripts' stack is empty\n    //then you can resume the parser.\n    //* If it's not empty then pop script from the stack and execute it.\n}\n\n});\n```\nI'm waiting for you feedback on this, so I can start implementation.\n. @Sebmaster apppendChild is incorrect place to handle scripts. According to spec script should be executed when parser leaves text insertion mode and pops script from stack of open elements. This state is not reflected in tree adapter. If you handle it in appendChild script will not have innerText yet. I have strong believe that script handling should be pulled out from tree adapter since: a)It will be just weird to pass document e.g. in appendChild b)This breaks API consistency - tree adapter is unified interface which allows you handle your tree format. Script handling is tree format-agnostic and moreover script handling is used only by interactive parser, so it shouldn't go to interface that used by other types of parser. c)tree adapters can be passed as is and shouldn't have any events, so you will not need to extend existing adapters. \nparse5 will not suspend parser on scripts, it will just notify you what script element found (then it fully parsed) and provide you current document state. resume/suspend functions are taken out of handler, so you can control parser anywhere you want (including link rel=\"import\" handling). \n. @Sebmaster Did I get it correct: you gonna create custom tree adapter which will instantly use DOM-methods abandoning any intermidiate representation? This will be the best approach. If you still have intermidiate htmlparser2 representation, you can use the following approach to process partially built document: if script handler is invoked then convert document in it's current state to the DOM and mark each processed node with e.g. __jsdomProcessed flag. Then pass this partial DOM to the script. On next handler invocation repeat this steps, but ignore nodes with  __jsdomProcessed flag and just attach new nodes to your partially built  jsdom representation of the document. And finally then parsing is complete, repeat this operation once again to attach remainder nodes. This is that should be done, we need to invoke scripts in-place as real browser to make document.write work, but to invoke scripts in-place they should be provided with document in it's current state.\n. > I wanted to go with the first option abandoning every intermediate representation, yeah.\nThis is definetly the right way to go :thumbsup:.\nTo clarify our next steps: I will implement InteractiveDocumentParser in the separate branch. We will take a look how it fits and how it covers jsdom requirements. Then if everything is ok, I'll rebase it to master. Is this ok for you?\n. @Sebmaster Finally I was able to get my hands on this. New API is available in https://github.com/inikulin/parse5/tree/user_agent_embeddable_parser branch.\nTaking into account everything we discussed previously I've tried to design API which will fit well in jsdom.\nJsDomParser\nNew parser (it's static and doesn't need to be contructed):\njs\nvar parser = parse5.JsDomParser;\nParser exposes two methods:\njs\nvar parsing = parser.parseDocument(html, treeAdapter)\nand\njs\nvar fragment = parser.parseInnerHtml(innerHtml, contextElement, treeAdapter)\nBoth methods are reentrant, which means that you can safely call parseInnerHtml inside script handler of the document parser, or even start parsing another document.\nDocument parsing unit\nparseDocument returns parsing unit object, which have the following fields:\n-   parsing.done(callback), there callback(document) - called then parsing of document is finished.\n-   parsing.handleScripts(handler), there handler(document, scriptElement) - called then parser has a script which is ready for processing/execution.\n-   parsing.suspend() - suspends parsing process.\n-   parsing.resume() - resumes parsing process.\n-   parsing.documentWrite(html) - parser-level document.write\nExample\nAll parsing unit methods are chainable. Here is a meaningless example =):\n``` js\nvar parsing = JsDomParser.parseDocument(html, treeAdapter)\n    .done(function(document) {\n            //we are done now\n     })\n\n    .handleScripts(function (document, scriptElement) {\n            parsing.suspend();\n\n            //fetch and eval script...\n\n            //parse innerHtml\n            var someFragment = JsDomParser.parseInnerHtml(innerHtml, someContextElem, treeAdapter);\n\n            //document.write and resume\n            parsing\n                   .documentWrite(someHtml)\n                   .resume();\n     });\n\n```\nPlease, let me know how it works for you.\n. FYI JsDom parser was merged to master\n. I close this for now\n. @Sebmaster @domenic \nHi guys,\nAny news on implementing this in jsdom?\n. Sad, just let me know if you need any assitance on this. And I wish you to have a good time at holidays :santa: \n. New version 1.1.4 which includes this fix was published to npm.\n. ping @Sebmaster \n. @domenic You've asked for this. Currently I'm trying to assign tasks priority for all my projects. Do we need this for jsdom 1.0.0? Or it can be delayed for now?\n. @domenic Oh, I've missed the release, congratulations! \nRegarding templates it's definitely makes sense to implement them, medium priority sounds reasonable. I'll try get my hands on it when I finish with escodegen rewrite (hope I will be done with it within a couple of weeks). \n. Templates are landed into the master branch. I will roll out a new version within a couple of days.\n. Ping @domenic, @Sebmaster. parse5 v1.2.0 with <template>-support is in npm (changelog).\n. @azakus I've updated whacko as well (v0.17.2). Note that according to spec <template> content is wrapped into document fragment and can't be queried directly. \nE.g.:\n``` js\nvar $table = $('');\nconsole.log($table.find('tr').length); // Will print 0\nvar tmplContentNode = $table.find('template')[0].childNodes[0],\n    $tmplContent = $(tmplContentNode);\nconsole.log($tmplContent.find('tr').length); // Will print 1\n```\n. Hi,\ncurrently parse5 doesn't support streaming. However we've discussed possibility of it's implementation with jsdom maintainers in context of this thread.\nI would like to know which goal you would like to achieve using streaming? If it's primary focus is performance then I would like to warn you that I find it a little bit questionable. For streaming we need to teach tokenizer to invalidate non-emitted tokens if end of chunk was encountered. This behavior requires introduction of the tokenizer state snapshots mechanism: if we have invalidated token we need to rollback tokenizer to the last valid state and retreat preprocessor to the point there last valid token was emitted. Since tokenizer and preprocessor are the most performance-sensitive parts of parse5 this may end up with significant performance degradation, so you can lose more than you win.\nLong story short. I definetly would like to see streaming API in parse5 too. But it requires quite complex research and I'm afraid I will not be able to get my hands on this soon, since we need to land more important features (like <template> support) first and I'm already suffering from the lack of the spare time. Therefore, any PR on this task will be highly appreciated.\n. @stevenvachon Sounds interesting. Can you describe this scenario in details, please? How streaming can help here?\n. @angelozerr sax-js is not the HTML parser. \n@stevenvachon I will be able to get my hands on it not earlier than end of July/August. Any PR is still welcome. If someone would like to start implementing it, then you can count on my assistance. \n. @angelozerr No, HTML parser requires html preprocessing/tokenization/parsing algorithms. \n. Ok, good news, everyone. I finally figured out how it should be done and parse5 will receive streaming support in the near future. One thing that I would like to set for the discussion agenda is the API. Should ParserStream and SerializerStream extend node's WritableStream and ReadableStream respectively, or it should just support stream-like API like htmlparser2? How you will obtain resulting AST, should ParserStream.end() return it? Or we should give user access to the unfinished AST via the property? The unsophisticated modification of the unfinished AST can brake parser (since we don't have DOM which guarantees that AST modifications will not lead to the malformed tree). This bothers me a little bit. Any ideas or suggestions?\n. @sideshowbarker \nIt's possible. Because:\n\nIn order to do this, we have to buffer the tables.\n\nWe shouldn't. We just walk up the stack of open elements and search for the table's parent:\nhttps://github.com/inikulin/parse5/blob/master/lib/tree_construction/parser.js#L823\n\nDoing this would basically buffer the whole document in memory.\n\nNope. Since we are in body we already have html element at the bottom of the stack of open elements.\nParser don't use token lookahead. So, the the bulk of the changes goes to tokenizer. We will make snapshots of the tokenizer state after each token emission. Then if we meet end-of-chunk, we invalidate last token, rollback to the last snapshot and suspend the parser. The next call to write() resumes the parser. This is it.\n. @sideshowbarker Ah, I see there this discussion started: https://github.com/servo/html5ever/issues/149 \nMy conclusion is that you can't have full spec compilant parsing without buffering already produced DOM-tree. In our case it's not an issue. parse5 has SAX-style parser, but it behaves more like tokenizer with the simulated parser feedback (CDATA parsing flag, switch text parsing modes). \n. But I still can't figure out use cases for such approach. Using SAX parser most likely you will not need information about position of the element in the DOM-tree (It becomes even more absurd if you don't have DOM-tree). \n. Ok, here is the fundamental question: should we abandon non-streaming API and release 2.0 or keep non-streaming API as well? If we will keep non-streaming at will be messed up in my taste:\nParser\nSAXParser\nSerializer\nParserStream\nSAXParserStream\nSerializerStream\nI would like to expose Tokenizer, TokenSerializer and co-called ForgivenParser (like htmlparser2 but with the proper spec-compatible tokenization, integration points, etc.). With streaming alternatives for each of them it will be a real mess.\nPlease, let me know what do you think. \n. @stevenvachon Yep, it's should be ForgivingParser, just my typo\n. @stevenvachon nope, we will still use current tokenizer and current HTML5 lexical grammar doesn't allow such constructs.\n. @domenic Ok, I'll keep it. However, handling both document.write and streaming will be an interesting task. So, i think there will be a huge rewrite of the curent JSDOMParser code anyway.\n. > @inikulin what is forgiving like htmlparser2, then?\nHuh, seems like I'm starting to remember why I initially was against 'forgiving' parsing in parse5. Because there is no exact definition of the 'forgiving parsing'. Therefore people will always be somehow unhappy with this thing. \nConclusion: cancelled :tada: \n. Sources: https://chromium.googlesource.com/chromium/blink/+/master/LayoutTests/html5lib/resources/\n. Yep, I'm already doing it.\n. Note: sync Blink changes as well-https://github.com/html5lib/html5lib-tests/pull/67\n. @gsnedders Thank you for the clarification.\n. @jonathanong node should have parentNode field to be serialized correctly. In your case text node's parentNode should reference to it's parent <script> element.\n. @jonathanong you are welcome\n\nweird issue and solution\n\nNot actually, serialization algorithm relies on things like context element namespace, tag name and so on. So, serializer expects that at least minimal tree format is preserved. However, in this particular case serializer can avoid call to parentNode property and I think I'll rewrite it for the next release. Just keep in mind that parse5 expects that it always works with the well formed tree.\n. Hi.\nThis makes sense. If you wouldn't mind I'll rename issue, since I would like to implement it for both tree adapters, so it requires more generic title.\n. Hi, guys. \nI will try to get my hands on this as soon as possible. Meanwhile, I have question regarding this functionality: do you need just token's (and appropriate element's in case of DOM-tree parsing) start index or end index is also required?\n. @cvrebert Is it important for you to have the same property naming (startIndex) as in htmlparser2? I would like add __locations property or something like this for parse5.\n. Good, thank you.\n. New version 1.4.0 with this functionality was published to the npm (see changelog). \n@angelozerr I haven't implemented location info for the attributes in this release, since it's a quite tricky thing to do and I don't have a good idea how to implement it at the moment, so I've created separate issue for this functionality (#43).\n. @pdpi This behavior is dictated by the spec and browser behave the same way. Chrome dev console dump:\n``` js\n\nvar div = document.createElement('div')\n< undefined\nvar a = document.createElement('a')\n< undefined\na.href = 'http://example.com?hello=1&world=2';\n< \"http://example.com?hello=1&world=2\"\ndiv.appendChild(a)\n< \u200b\u200b\ndiv.innerHTML\n< \"\"\n```\n\nHowever, proposed option makes sense. \nBTW, any PR will be highly appreciated. \n. @winhamwr Thank you for the participation.\nThe first impression is that 2 is better, but I personally vote for 1.\nHere are my points:\n-  We avoid breaking change\n-  By default parse5 is pretty strict with the spec. Therefore I would like to have some extras object for the things that change parsing/serialization semantics, violate or extends the spec. E.g. the next candidate to go into such object is the flag that enable element location information (https://github.com/inikulin/parse5/issues/30). So, we will be able to separate spec-heterogenous functionality on the API level.\n. I think we should require them to pass TreeAdapter to provide extras. It's not elegant from the API standpoint, but at least it will be bulletproof and we will not have weird behavior if someone will mind to add escapeString property to the tree adapter. Regarding option name: shouldn't we name it encodeHtmlEntities or something like that? escapeString is a little bit misleading. \n. > And as far as testing this option, would it be acceptable to add a one-off test like this one?\nSure.\n. @alanclarke Thank you! In general it looks good, but there are some remarks.\n. @winhamwr @alanclarke Thank you, guys. I'll try to prepare a new release within a couple of days.\n. New version 1.3.0 with this functionality was published to npm\n. I'm not sure if this can be implemented because with non-standard tag names you will get completely different grammar.\n. I mean formal grammar. \n. Then you will end up with the parser generator. Looks like Skunks is a co-called 'forgiving' tokenizer, while parse5 was designed to be precise spec-compatible parser.\n. Spec-compatible parser is a parser that can parse HTML, \"forgiving\" parser is a parser that can \"somehow parse some subset of the HTML\". Having both in the one package doesn't makes sense for me.\n. @nylen We don't have any plans to support grammars except HTML. I recommend to stick with another project or create a fork of parse5 which will support your custom grammar.. @nylen \n\nWe plan to use HTML comments as a \"pseudo-block-tag\" to store post content, and these \"pseudo-tags\" will contain HTML content inside of them. \n\nWhy not use custom HTML elements for this purpose? E.g. <wordpress-post-content>. @nylen It would be nice to have some context before giving any advice. What's the lifecycle of these \"pseudo-block-tags\": who create them, how they processed, how their content is displayed, is there any sanity check required for content, etc.?  . > There's a lot of needed functionality/UX missing from the prototype, but the basic idea is there: to re-work editing a WordPress post into editing a series of \"blocks\". These \"blocks\" will be delimited by HTML comments. You can see how this is serialized by clicking the \"Html\" button\nSeems like I've got it. As I recall there was something similar on tumblr .. Well if those parts are always edited separately then workflow is quite simple: parsed document, get all child nodes between matching comment nodes, serialize them and dump them to editor. On save parse given fragment with parseFragment (this will automatically strip unwanted elements like <head>) perform sanity checks if necessary, then insert those nodes into parsed document and serialize it. . Or, even better:\n- Use SAXParser with location info enabled and get locations of content between two matching comments for required section. (You can stop parsing once you found what you need).\n\n\nDump found substring to editor\n\n\nOn save parse given fragment with parseFragment (this will automatically strip unwanted elements like ) perform sanity checks if necessary\n\n\nInsert new content instead of substring that was obtained earlier\n. @nylen Let me know if you'll need any assistance.. Hi, you can build client-side parse5 package using browserify. Or just use this snippet for the fragment parsing:\n\n\n``` js\nvar parseHTML = function(str) {\n  var tmp = document.implementation.createHTMLDocument();\n  tmp.body.innerHTML = str;\n  return tmp.body.children;\n};\nparseHTML(htmlString);\n``\n. Hi, I can't see any issue using it in the browser. However, you will need to [browserify](https://github.com/substack/node-browserify) it first. See also: #34 \n. @jonathanong It may use simple RE-based tokenization, so it basically fails on this piece of code. \n. Duplicate of the #30 \n. Arggh, I see. Original parse5 treeAdapter doesn't make conversion to the lower case. Therefore it returnedundefinedfor thedocumentanddocumentFragmentnodes. Which worked just fine. In jsdom's treeAdapter it fails by the obvious reason. We shouldn't switch to thenodeNamehere because it will be not quite semantically correct, but it can be easily worked around by checking text node's parent type. Thanks for the report. \n. The best solution will be to wait till I land fix to the parse5 (I'll do it tomorrow or tonight) and rollback to thetagName. Checking forundefinedlooks OK for me as well if the original issues should be solved urgently.\n. I've published update to npm.\n.Has no end tag!==Self-closing`\n. Update v1.3.2 with this fix was pushed to npm.\n. I've updated whacko as well\n. Conditional comments are not part of the HTML5 specification, so I don't think that this functionality should be implemented on the parse5 side. However, they have quite simple grammar and can be easily parsed from the comment text using regular expression (example).\n. Hi,\nQuery selection requires some DOM-like layer, while parse5 returns just bare tree. I can suggest to take a look at jsdom or whacko. They are both using parse5 as an underlying platform.\n. parse5 is the HTML parser, not DOM implementation. Why suggested libraries doesn't meet your requirements? Any specific case?\n. Then take a look at minidom lib.\n. jsdom uses nwmatcher lib for querying (see: https://github.com/tmpvar/jsdom/blob/2ff5747488ad4b518fcef97a026c82eab42a0a14/lib/jsdom/living/selectors.js). You can try to adopt it with minidom. But quick code investigation shows that it requires quite complex DOM implementation to operate properly, so seems like you will still end up with your own partial implementation of JSDOM :wink: \n. The issue is that for full-fetured CSS3 level querying you need really consider a lot of edge-cases and this thing is not as simple as it seems. This is really quite complex stuff. You can't just compare attributes, DOM is live structure, you should also consider live properties. You can't also retraverse tree on each query, because it will be just performance overkill, so you need to introduce some kind of memoization. You need query string parser after all (~45 selectors + combinations.) And... :tired_face:   Just believe me things are enormously complex and fucked up. And it's not because of twisted fantasy of the guys who loves to overcomplicate things, but because of complexity of things we have nowadays in our browsers.   But if you need just a slow implemetation of some queries, then you can basically implement them yourself, it's not a big deal: tree walker + filtering clause.\n. Closed by #92 \n. The cause of this issue is that parse5 incorrectly renders doctype.data in htmlparser2 tree adapter.\n. Updated version 1.4.2 was published to NPM. Whacko should update automtically via npm update\n. Duplicate of #39 \n. Unfortunately there is no way to do that at the moment. However, this functionality makes sense, so I keep this issue open. PRs are welcome, BTW.\n. It's a little bit different, since streaming parser will auto-suspend when it meets end of chunk. But here we want to stop parser at any moment. \n. @RReverser correct, this is exactly what I mean.\n. Hello!\nparse5 parse() and parseFragment() functions behave like browser does in case of document and fragment parsing respectively. To do parsing in \"forgiving\" manner you can use the this gist: https://gist.github.com/inikulin/bffe6fffd6942d022e11\n. You are welcome.\n. jsdom?\n. Well, seems like I don't quite understand what you want. Can you illiustrate it via some code snippets, please?\n. DOM has a much more complicated connection with the HTML parser than this. I suggest you to keep an eye on JSDOM project, which is the most complete and spec-compliant pure-js DOM implementation at the moment.\n. Parse5 provides pure HTML AST. For full-featured DOM use jsdom.\n. Hi\nYou can use \"forgiving\" parsing gist, as described in #48.\n. Yes, parse5 loads HTML entities data in memory and this is a requirement for the HTML5 parsing algorithm. BTW it's 2015, how 30mb RAM usage can be called \"excessive\" (especially for the sandboxed VM app)? Are you running it on Apple Watch or something? =)\n. @Sebmaster Seems like generator died with my previous desktop. I'm ultra-busy right now, give me a couple of days, I'll rollout the new one. The only idea regarding optimization that I have is to keep trie in the JSON form, so v8 will not generate AST representation for it. But I'm not sure we will win a lot we this approach. Morevover I have concerns about browserified version, as far as I know there is no easy way to mock FS with browserify.\n. Yeah and this is not an option. CPU performance is the priority.\n. > What if CPU load increases by less than 1% but RAM usage decreases by over 80% Would you still not implement a theoretical change that would change for proportions like those?\nNo. Let's take a look at the problem using big O notation. Trie consturcted once at startup and never modified, so in our case it's space complexity is constant - O(1).  Currently the time complexity of the trie lookup is O(m), where m is the average length of the word. If we will have time increased by 1% we will get O(1,01m). Meanwhile, if we will descrease trie size by 80% we will still get constant space complexity - O(1). Speaking clearly we will gain nothing, but we will loose in speed. Constant 22.5mb consumption (7.5mb is consumed by runtime itself for me) doesn't seems like a big deal for me nowadays.\n. @domenic Argh, yes, my bad, up to constants. Never do the math at the morning =0. \n. Taking into account my wrong math it worth making a try.\n. @Sebmaster Here is the trie generator https://github.com/inikulin/parse5/tree/master/tools\nparse5 bootstrap consumed 15Mb for me, BTW\n. @Sebmaster any progress on this? \n@domenic Do you have any complains about excessive RAM usage in jsdom. I mean, does even worth discussion and efforts?\n. I'm trying to figure out that we want to accomplish here. I mean, which memory usage can be considered non-excessive and if current memory consumption causes any real-life problems (actually it does, but it happens in quite exotic envirionments - #54 ). \n. Well, the most significant memory footprint of parse5 comes from the name entities trie. The only optimization that comes to my mind is to replace it with the Patricia tree. But I'm quite sceptical about it: it might work well with long suffixes, but it's not the case for the named entites. I don't think we can win more than 10-20%. I'm wondering if it's even worth a try. \n. Thank you for the contribution! The PR looks great, but we need to work through some minor issues: \n1. Can we move tag location information into the __location object, so we will not pollute element with the element location properties? (E.g. __startTagLocation -> __location.startTag.\n2. We definitely need some tests for this.\n. I think contentStart, contentEnd is too generic and can be misleading. The following structure looks good for me:\njs\n__location: {\n    start: Number,\n    end: Number,\n    startTag: {\n        start: Number,\n        end: Number\n    },\n    endTag: {\n        start: Number,\n        end: Number\n    }\n}\nDoctypes, comments and text nodes __location don't have startTag and endTag properties of course.\n. Thank you. LGTM. \n. I'll merge and roll out new version tomorrow.\n. Published as v1.5.0\n. > I'm trying to run Parse5 in the JVM using Nashorn and AvatarJS.\nOMG \nI'm going to rollout trie generator in context of #52. So you will be able to create your own named entities list.\n. Trie generator now available into the master branch: https://github.com/inikulin/parse5/tree/master/tools\n. I think this task goes far beyond the parse5 goals. But you can try build validator on top of the parse5. I can expose everything what is required. \n. @domenic Validation is a different beast. HTML5 can swallow any input, but the output maybe quite different from what you expect. \n. @aredridel Right. BTW parse5 already supports location info. The only issue is that there are no error message texts provided anywhere in the spec (and sometimes it's quite hard to create one). If someone from the native speakers will help me with that then it will be awesome. However, I would like to postpone this functionality for now in favor of the streaming support. \n. @sideshowbarker Wow, thanks for the helpful references.\n. I suppose implementing full-featured validation will never be in scope of parse5. The best we can do is to implement parsing errors, but it's blocking on https://github.com/whatwg/html/issues/1339 because we can't properly test them at the moment with html5lib test suite. Therefore, I'm closing it for now. Feel free to open new issue once we have error codes implemented in HTML spec if you are interested in this functionality.\n. Nope, this means that it makes no big sense to discuss this feature until we will not have standard error codes. We can use error codes from other parsers as @sideshowbarker suggested, but we will not be able to properly test it and therefore maintain them up to date.\n. > Can't it have its own error codes?\n->\n\nWe can use error codes from other parsers as @sideshowbarker suggested, but we will not be able to properly test it and therefore maintain them up to date.\n\n\n\nA wrapping library can translate them\n\nI would like to implement this feature only once =)\n. Use cases?\n. I don't think we need this function in parse5, since you can just walk the tree, escape required text nodes and then serialize with the  disabled encodeHtmlEntities option. \n. This is the classical case of the adoption agency algorithm invocation result. And it's correct.\nE.g. Chrome output:\n``` js\n\nvar div =document.createElement('div')\n< undefined\ndiv.innerHTML = '\\n';\n< \"\n\"\ndiv.innerHTML\n< \"\n\"\n``\n. Huh, haven't checked ES6 features supported by the io.js for a while. Seems like it have everything I've had in mind. Thanks, @domenic!\n. However, I'm afraid some adopters will be unhappy with this decision. As far I know currently parse5 is used in some envirionments which doesn't have this syntax support (e.g. Google Trace Viewer). BTW, how jsdom is shipped to the browsers (which are in general doesn't support new syntax) now?\n. It would be really nice to use classes, object literal extensions, etc. But as far I know this features are not shipped in browsers yet.imports/exports` is another feature, which I really love. But seems like it will make it somewhere between the Judgment Day and the moment when Edward Furlong somehow grown up into Christian Bale :weary: \n. > You could use Babel.js\n\nThis was the original intention. Babel is great and I use it in some of my projects. But for parse5 generated code may kill performance in some cases. So, after some experimentation I've abandoned this idea.\nOn the other hand, native ES6 support, as you noticed, is still in it's early ages. Moreover, olders browsers and node 10 are still target platforms for parse5. Therefore, it's also not an option.\nSo, I just keep it open till the better times...\n. Yet another option: TypeScript\nPros: same as ES6 + typing (less VM bailouts)\nCons: pretty much the same\n. Thinking of it a little bit more I came to conclusion that it will never happen: no need to fix things that's not broken. While there are some benefits they are all have just a cosmetic nature. Current codebase in ES5 is just fine (at least for now). So, it's better to put efforts in something more end-user valuable like memory performance, bug fixing, etc.\n. Wow, thank you for this! The only concerns I have is about performance since you've introduced _leaveAttrName'. Can you run regression benchmark, please?\n. Oh, and seems like it will not work for thehtmlparser2tree adapter, since it doesn't havenode.attrsand usesnode.attribsdictionary instead. Taking in the account differences between attributes representation in tree adapters I think we should move location info to the start tag's 'locationobject (for SAX parser and Tokenizer) and into the node's__location (for Parser). E.g.:\njs\nstartTag.location = {\n    start: 0,\n    end: 25,\n    attrs: {\n        'style': {\n            start: 4,\n            end: 12\n         },\n         'href': {\n          ...\n         }\n    }\n};\nand\njs\nnode.__location = {\n    start: 0,\n    end: 25,\n    startTag: {\n        ...\n    },\n    endTag: {\n        ...\n    },\n    attrs: {\n        'style': {\n            start: 4,\n            end: 12\n         },\n         'href': {\n          ...\n         }\n    }\n};\n. @sakagg Sure! Note: I've landed some v2.0-related commits recently. Could you rebase you work on top of it?\n. Closing in favor of #92 \n. Hi, thank you for the kind words.\nparse5's tree formats are far from the real DOM (but can they be used to build one, see jsdom ). However, I've tried to avoid adding new semantics to the well-known element properties, that's why nodeName is done in conformance with the DOM spec\n. @Sebmaster But should we? Where are two scenarios:\n1. parser encounters <script> and document.write is called immediately from this script. We don't need to suspend parser since it's already suspended, because you are in it's 'script' event handler.\n2. document.write is called asynchronously, then you'll be able to write only then your code which will document.write receives event loop tick. This will happen if: \n   a. parser awaits next chunk and therefore it's already suspended \n   b. parsing is accomplished, in this case you should switch to your document.write implementation which rewrites whole body.\n. It will resume once you write() or end() just like regular streams.\n. Oh, I misunderstood your question. It will just emit('script') and obviously will not resume till handler will not finish working.\n. Arghh, forgot about this use case. What if parser will auto-suspend if it has 'script' event handler?\nAnd resume function will be passed to the handler:\njs\nparser.on('script', function(scriptNode, resume) {\n    ...\n});\n. @fb55 I'd love to do that. Please, let me know if you need any assistance from me.\n. Document tree adapters as well (jsdoc?). \n. If someone from native speakers would like to help me on this, I will be very happy. Because, I'm really out of the spare time right now and this is the only v2.0 blocker. ~~And to be honest, I hate writing docs.~~\n. You can parse HTML using Parser and get AST as a result. Then walk AST and replace appropriate node content. Then convert AST back to HTML using Serializer.\n. P.S. Docs and examples are in README.md in the root of the repo.\n. @shawnXiao \njs\nvar fileContent = fs.readFileSync(includeFilePath, \"utf8\");\nvar parser = new Parser();\nvar fragment = parser.parseFragment(fileContent, entry.childNodes[0]);\nentry.childNodes = entry.childNodes.concat(fragment.childNodes);\n?\n. Can you provide code which causes this error, please?\n. Seems like html argument of the parse function is not a string. This is the cause of error.\n. > Like, the ability to define additional characters to be allowed in tag names and attribute names. If left blank/etc, it'd be spec compliant.\nI doubt it's possible without breaking grammar, too many edge cases, too many ways to shoot yourself in the head.\n. @stevenvachon \nH\u034e\u0318\u032d\u0356\u0317\u0339TM\u033cL\u03185\u032a\u0331\u035a\u034e t\u0347\u0348o\u0332\u032f\u0339\u032d\u0317\u0349\u0324k\u0345\u031d\u032f\u0324en\u0331\u0320\u0333\u034e\u032f\u0325\u0332i\u0332z\u032c\u032a\u032d\u035a\u0319\u032e\u0353e\u033c\u0353\u0331r\u0323 \u033b\u032a\u033a\u0324\u0354\u0320\u032di\u031c\u0339\u031c\u0323\u0318s\u0326 \u0319\u031d\u032et\u0348\u033b\u0349\u0345\u0339\u0354\u032co\u0359o\u031c\u033c\u0354\u032a \u033a\u0348\u0331\u0332\u0329c\u0332\u0330\u0349\u0349\u0354o\u033c\u031e\u0359\u0317\u031fm\u0339\u0348\u0354\u032e\u0316p\u033al\u0324\u0320\u031e\u0331e\u032a\u034d\u032a\u0320\u032c\u034dx\u0331\u0353\u035a\u0359 t\u0355\u0345o\u0317\u0333\u0349\u0339\u035a\u031d\u0355 \u0318\u0325\u031f\u0331b\u0324\u0353\u032c\u0353\u034e\u032f\u034ee\u031d \u0317\u0348\u032e\u032c\u032c\u0355r\u0355\u032fe\u0348\u0345\u033b\u0329\u0331\u0317\u031cp\u0316\u0324\u0333l\u0354\u0345\u0349a\u0356c\u032d\u0355\u0354\u031de\u033a\u0323\u032d\u0339\u0355\u0348d\u0354\u034e\u0317\u032f\u0325 \u0345\u0323\u0324\u033aw\u031c\u0316\u0359\u0329i\u033b\u0329t\u0345\u035a\u032ah\u032d\u0320\u0355\u0348\u0332 \u033b\u0331\u0323\u0320\u0347\u034e\u0339th\u033a\u0345\u0348\u033ce\u0317\u033c\u0320\u033b\u032e\u033b \u0320\u0332\u0339\u0320\u031cr\u032a\u031d\u0355\u034eegu\u0316\u034d\u0330\u0331\u034e\u0333l\u031d\u0319\u035aa\u032f\u033br \u031f\u032ce\u031e\u031f\u0356\u034d\u0333\u0339x\u031f\u033a\u0353\u0354\u0331\u0320\u0359p\u0354\u0355\u032f\u0318re\u0356\u0349s\u0330\u0329\u031d\u0329s\u0359\u0353\u035a\u0333\u0330i\u0355\u0325\u0329\u0333o\u032an\u032f\u032b\u0329\u0320\u032cs\n. Thank you, I'm currently switching to the jsdoc auto-generated docs. So, fix appropriate doc comments instead of README, please: \n- https://github.com/inikulin/parse5/blob/master/lib/parser/index.js#L23\n- https://github.com/inikulin/parse5/blob/master/lib/serializer/index.js#L16\n\nI think that a link to a wiki page/etc explaining just how and what non-conformities can occur would be helpful.\n\nWell, it will obviously not parse/serialize HTML entities how spec suggests.\n. Gosh, that's why I hate all this optional spec violations and other weird edge case things: it's not enough to tell people what they can mess up everything, you should describe in greater detail what could possibly go wrong. And nearly as always the answer is: anything, except your edge case scenario for which we've implemented this options.\nSo the answer is: anything, except edge case scenarios for which we've implemented this options (#31). E.g.: running serializer with encodeHtmlEntities enabled on the output of parser with the decodeHtmlEntities disabled: &amp; will be serialized as &amp;amp;\n. Thank you!\n. I have these thoughts for a long time. The only thing that stops me: for some reason we've implemented it and the worst part is what I couldn't figure out why we did it.  The origin of all this is in the #31 and after re-reading the only argument for this feature that I can find is: \"it would be cool\" . Now I have no idea why I decided to add this option, so let's just pretend I was deadly drunk at the time :beer: \nThe points that bothers me:\n- Those things are dangerous and inconsistent with spec, e.g.: https://github.com/inikulin/parse5/pull/74#issuecomment-144431330\n- I can't figure out a real life scenario for this options.\nSo, I'll keep this issue in the proposal state for a week or so to see if anyone will come up and argue for this feature. Otherwise, it will be removed from the v2.0 (MUAHAHHAHAHA :smiling_imp:)\n. I guess the best solution will be to ask guys who participated in the original suggestion and figure out their scenario and possible workaround: \\cc @alanclarke @pdpi @winhamwr\n. @alanclarke Thank you for the clarification. \n\nyou can see from the above that really neither strategy will truly replicate the browser behaviour, that's why its useful to provide users with the ability to opt out of potential side effects like that\n\nBut parse5 currently behaves the same way without flags and even without DOM implementation:\n``` js\n\na.href = \"http://blah.com/?a=b&c=d\"\na.href\n```\n\nThis is the same as if you parse some markup and then change some attribute value in the resulting AST. It will remain unmodified.\n``` js\n\na.outerHTML\n```\n\nThis is the same as serialization of the given node, it will give you the same encoded result as in the browser.\nSo, I don't see any difference here.\n. I don't get it. If you parse\nhtml\n <div data-react-props=\"{&quot;count&quot;:0}\" />\nit will be parsed as:\njs\n {\"count\":0}\nBut if you serialize it back with parse5, you will get:\nhtml\n <div data-react-props=\"{&quot;count&quot;:0}\" />\nSo what is the problem here?\n. >  It's breaking now at str.replace() inside of the escapeString() function.\nIt's unlikely the cause of this issue. Regarding disabled decoding can you explain your scenario in details?\n. >  It should be easier than HTML parsing, right?\nWell, looking at the libxml2 parser which is ~15k LOC I doubt it's easier :disappointed: . XML spec is not as clear as the HTML spec: it just gives you some BNFs. Converting them to the neat hand-written parser may take a while. Parser generators may simplify things a bit, but I doubt about their performance. \n\nThe ideal situation would be where I can use basically the same API for HTML and XHTML, including my own tree adapter and the like, but just flip one switch and parse5 does all the work of changing how it parses (and hard-erroring on encountering an error).\n\nAlso, I'm not sure about incorporating it in the parse5. Package may grow enormously and will contain 2 quite different parsers (HTML and XML will not share much code). As an option, after all, we can combine both parse5 and XML parser under some umbrella package which will provide some generic interface for the consumers. \nAnyway, implementation of this may take a while. I'm also wonder if we need polished and fast parser for the XML. I mean, how often it's used in browser nowadays and does it cost the efforts? Maybe we should just compile some existing well-tested C/C++-parser used by the browsers with emscripten and just provide nice JS fascade for it?\n. > IIRC using parse5.SAXParser for better streaming.\nI'm afraid, it's not a good idea. SAXParser provides just basic lexical information that is alone is insufficient to build correct HTML5 AST (unlike XML whose element nesting rules are quite trivial). You will need to use parse5.ParserStream which is WritableStream and has a proper scripting support (sneak peek).\n. > It would be better to have a dedicated getTemplateContents function that is only used for templates.\nMakes sense.\n\nSimilarly, <template>s are being handled specially in parsing. It appears that when parsing documents containing <template>, parse5 has a new '#root' node surrounding the template contents. When we encounter that, we need to take all of the root node's parse5 children, convert them to jsdom-children, and jsdom-insert them into the current \"parent\", which is a jsdom-template.\n\nparse5 uses DocumentFragment as the template content: https://github.com/inikulin/parse5/blob/master/lib/parser/index.js#L559. \nThis is per spec. \nThen, on insertion, we precisely follow \"appropriate place for inserting a node\":\nhttps://github.com/inikulin/parse5/blob/master/lib/parser/index.js#L534\nIf I get it right, the issue is caused by the fact that in htmlparser2 tree format both Document and DocumentFragment have the same node type: https://github.com/inikulin/parse5/blob/master/lib/tree_adapters/htmlparser2.js#L64. \nSeems like the problem is with the htmlparser2 tree format, maybe the best solution will be to step away from it and implement custom tree adapter for the jsdom?\n\nCurrently we get attributes as an object map of local names -> values, plus two other maps of x-attribsPrefix and x-attribsNamespace. I'd prefer an (ordered) array of { localName, prefix, namespace }.\n\nAgain, this is htmlparser2 legacy. htmlparser2 doesn't have attribute namespaces and prefixes implemented, and attributes collection there is a dictionary. That's why we need additional dictionaries for the namespaces and prefixes. Meanwhile, default adapter stores attributes the way they are provided by parser: https://github.com/inikulin/parse5/blob/master/lib/tree_adapters/default.js#L65. Which is array of {name, value, prefix, namespace} objects. So, again, it's one more reason to step away from the htmlparser2 tree adapter.\n. Oh, I've missed that:\n\nAs a separate case, when parsing children of a <template> context using parseFragment, we need to note that case specially, and jsdom-insert the resulting jsdom-children into the jsdom-context node's template contents, instead of directly into the jsdom-context node.\nIt would be better if there were something matching \"appropriate place for inserting a node\" more closely, which could use the treeAdapter.getTemplateContents somehow. Then parse5 could do all the work of figuring out where to place template children for us.\n\nI'm afraid I don't get it. According to the spec, fragment parsing algorithm shouldn't perform insertion into the context element. It should just return documentFragment with the parsed content. The content insertion strategy is up to the parser adopters.\nLet's summarize: the only thing that I can do here for you is provide getTemplateContents and createTemplateContents methods. In that case template content creation will be up to the tree adapter implementer, but the spec clearly says that template content should be the documentFragment.  It bothers me, because implementer could break the spec in that case.\n\\cc @domenic \n. ...or, I can implement setTemplateContent (tmpl, contentFragment) instead of createTemplateContents. Will it address your issues then?\n. @domenic Great, let's do it this way then.\n\nBTW I noticed that adapter.appendChild is not documented in https://github.com/inikulin/parse5/blob/master/docs/05_api_reference.md#TreeAdapter. I guess that is still in progress :).\n\nThanks for pointing this out. Actually, I've missed it =)\n. > Currently parse5 emits the full namespace uri of an element with a namespaceURI != SVG, HTML, MATHML\nThis is per spec:\n\nIf current node is an element in the HTML namespace, the MathML namespace, or the SVG namespace, then let tagname be current node's local name. Otherwise, let tagname be current node's qualified name.\n\nHowever, provided markup looks odd. As far as I remember, parser will produce elements only in HTML, SVG or MathML namespaces. Any other namespace can be set via DOM only. Can you provide the source code that resulted in the given markup? BTW, how browsers behave?\n. Oh, I've got it. Seems like you are implementing XML parsing for the jsdom with the SAX. And trying to serialize nodes with the parse5 serializer. Right? If so, the issue is that XML-fragment serialization differs from the HTML serialization and described in the separate normative section of the spec:\nhttps://html.spec.whatwg.org/multipage/xhtml.html#serialising-xhtml-fragments\n. > That should be localName || prefix + \":\" + localName (== tagName) then, not namespaceURI + \":\" + tagName.\nRight, thank you for pointing this out. But Node.prefix is the part of the DOM spec and should be calculated on the DOM level and has nothing to do with AST. I wonder how we can handle this gracefully. Maybe implement getNodePrefix() in the tree adapter which will return empty string in the current implementations, while jsdom's adapter will return actual Node.prefx?\n. Makes sense, thanks. Is this urgent and requires fix in 1.x or you can wait till 2.0? \n. Published as v1.5.1\n. Thanks!\n\nWith this, is it possible to recreate functionality from v1.5 where we fed a string into the SAX parser?:\n\nSure:\n``` js\nvar parser = new parse5.SAXParser();\n// ..handlers\nparser.end(str);\n```\n. @stevenvachon It's Stream API. And there is a link to it in the docs: https://github.com/inikulin/parse5/wiki/Documentation#new_parse5+SAXParser_new. Copying node docs  here doesn't make much sense.\n. Tree adapters contain the minimal API required to build/serialize AST. Everything else is up to the tree format implementers. If you want these properties in default adapter, then you can create your own adapter that will extend default adapter (or just use jsdom).\n. Thank you for discovering this!\n. Published as v2.0.1\n. @Sebmaster As far as I remember it was intended. A the time it was written parse5 was not planned for use in the full-featured DOM implementation and unlike quirks-mode limited-quirks doesn't affect parsing algorithm. So it was just removed, because it's looked unnecessary. \n\nPossible to change?\n\nYes, I'll try to take my hands on it somewhere around next week.\n. JFYI this will require introduction of the new method to the tree adapters and hence major version bump. I'm waiting for the feedback from @Sebmaster regarding scripting. So if it require any additional breaking changes, I'll be able implement them in the bunch. \n. Right, thank you for finding this. Ironically, it's not an issue with the parsing algorithm, but another case of  the #82\n. Published as v2.0.2\n. Docs in wiki are autogenerated from markdown in docs and jsdoc comments. You can modify it in repo and just issue the PR\n. Thank you!\n. @RReverser My bad, @domenic is right. You need to edit jsdoc comments in the code, since whole API reference section is auto-generated. I think, I'll gitignore docs/05_api_reference.mdto avoid confusion in the future. Regarding #86 didn't looked md-file diff, since jsdoc randomly moves members across generations. So, I'm afraid you will need to apply this changes as well. Sorry for that.\n. @RReverser to be honest, I have no idea. jsdoc generator is extremely buggy and doesn't even  work for me on OSX :cry: \n. @RReverser I'm trying to figure out the whole issue to answer your question, but looking at the diffs - they look identical for me (or my vision is too blured after working day :tired_face:). Can you explain what's wrong currently, please? Maybe I'll have some ideas.\n. @RReverser got it now. Well, seems like it's GH's markdown sanitizer - it treats <template> as the forbidden HTML tag. \nQuick workaround: wrap tag names with \"`\" in jsdoc comments, e.g.:\nSets the `<template>` element content element.\nAFAIR, I already did it in some places.\n. Fixed in fc4c510def57e75845c381257b55ca8abb62caba\n. @RReverser It's ok, thank you for pointing this out and taking time to fix it!\n. This is awesome, thank you!\n. It's the fastest spec-compliant parser. htmlparser2 is not spec-compliant. However, their performance is comparable, htmlparser2 is ~1.5x faster, but yet again, it's spec-incompatible, so it parses incorrectly in the wide range of cases.\n. Thank you for finding and fixing it! Just a little issues with some excessive code =)\n. lgtm, thanks!\n. This is awesome, thank you! Could you please update docs as well?\n. @yyx990803 Well, it's a little bit tricky. It requires new type TagLocationInfo that will extend LocationInfo, but will contain attrs as well. Meanwhile startTag and endTag in ElementLocationInfo should have TagLocationInfo type now. And we need to update references to the LocationInfo there TagLocationInfo was implied. \n. @yyx990803 My bad, forgot to tell you to update jsdoc comments instead of 05_api_reference.md since it's auto-generated. (I thought I've already removed this file from repo to avoid confusion, but seems like something went wrong =)). Really sorry for this.\n. However, nevermind. I'll take care of this. Thank you!\n. Published along with #91 as v2.1.0\n. Note: the actual version of the docs is here: https://github.com/inikulin/parse5/wiki/Documentation\n. Can you add regression test please?\n. Thanks!\n. Published as v2.1.1\n. Fixed in #95 \n. Ok, I was able to localize the issue. TransformStream designed the way that if it's ReadableStream part doesn't have consumers it's stops piping data through _transform once it's reaches highWaterMark (which is 16Kb by default). The first chunk of this file already exceeds this limit, so the remaining data will not be parsed. But once we have consumer - everything works just fine, you can try:\njs\nsource.pipe(parser).pipe(process.stdout);\nSeems like setting highWaterMark to Number.MAX_VALUE in TransformStream ctor should do a trick (since we need to consume whole data anyway). But I wonder if there are any caveats. It will be nice to have a feedback from some node streams expert. \n. Nevermind, I found workaround :wink: \n. @domenic Yep, I just pipe to /dev/null stream by default, so I can keep default highWaterMark value, but let data flow through transform if we don't have consumers\n. Published as v2.1.2\n. Hm, can't reproduce on my side with node v4.2.2 and v5.4.1. Maybe something OS specific? On which OS you are, guys?\n. Reproduced on both Windows and MacOS. Funny fact: this exact code doesn't throw in context of mocha test.\n. Finishing test on nextTick did the trick. Seems like mocha forces process termination\n. This option is not available in gulp-mocha, at least it's not documented. Anyway nextTick workaround is just fine.\n. Published as v2.1.3\n. @Sebmaster It complicates things a little bit, but I have some ideas.\n. @Sebmaster Fixed and published as v2.1.4\n. @pixelami Seems like it's completely different issue\n. https://github.com/inikulin/parse5/wiki/Documentation#version-history\n. You are welcome. Actually it's the second question of this kind, so I'm starting to doubt that documentation is obvious and accessible. I wonder if I should highlight it somehow in the README. Any suggestions?\n. @gdi2290 Thank you for the feedback. So, explicit link to the Version history in README should work I guess.\n. @domenic I like this idea, thanks.\n. Nailed it, working on fix...\n. Published as v2.1.5\n. Well, the thing is that SAXParser uses spec-compliant tokenizer under the hood. And the problem with the HTML5 tokenization is that it requires rollbacks to operate properly. So, in general there is no other way to correctly parse document, except buffering it is a whole (see similar discussion about Servo's HTML parser: https://github.com/servo/html5ever/issues/149#issuecomment-120991146). Everything else that happens in this benchmark is the effects of this buffering. \nOn the other hand, is there any real-life task that requires parsing of the 122Mb HTML document? I can fix it partially by introducing some heuristics, like limiting buffer size to 10Mb. But still, parser will have some significant memory footprint on large files. And, in theory, this may break parser in some very rare cases. \nConclusion: if you are going to parse 112Mb HTML files in production then I think htmlparser2 is the way to go. If not, then consider trying benchmarking with file sizes that you expect in real life scenario.\n. However, I have some crazy idea. Need to try and test it. Stay tuned :radio: \n. > (Parser5.SaxParser) than without streams (Parser5.DefaultParser)\nThis new info changes things drastically. Can you provide benchmarking info for parse5.SAXParser vs parse5.Parser?\n. Unfortunately, currently I'm extremely busy with other projects, so I will not be able to deliver this fast :cry:. However, if someone is willing to be an awesome person who going to implement this, here is the idea how it can be done: for streaming parsing we can remove the leading part of the buffer if it will no longer partisipate in tokenization. Once we emit a token we can slice buffer at token's end position (see location info code for the example of how the end position can be determined). The only tricky part that is left is to keep indices in sync (we need actual indices for the location info). That's it.\n. Yet another optimization: token recycling. Create pool of free token objects. Reuse tokens that are not in use anymore. This should reduce GC load and also reduce memory consumption on big inputs.\n. We have 10xInput size memory consumption here. I can't think of any object that produced in such a large scale in parser except tokens. It's just a guess, but looks like GC just can't get in time to clean up or reallocate all tokens produced by parser on large inputs, therefore producing less objects may make a trick here. \n. Yes, parse5 operates with UTF8 code points. It doesn't expect any other encoding as the input. To achieve suggested functionality it should perform decoding of the incoming byte stream as well (ideally, it will require some sophisticated implementation of the decoder that will be able to store code-point-to-byte position information to get location info in one pass). This task, however, is out of the context of this project, since It's possible to built standalone encoder and feed it's result to the parse5, then transform parse5 locations to the byte position.\n. Thank you for reporting this.\n. parse5 always behaves as an interactive user agent, so it parses noscript as a text, like browsers with enabled javascript do. If you wish to contribute, we can introduce option to switch parse5 to non-interactive user agent mode. Searching Tree Construction section of the HTML spec for \"interactive user agent\" is a good starting point.\n. Because it's per spec. \n. > As I remember browsers with enabled javascript just skip the content of tag nonscript, but not consider it as text.\nIt's considered as a text. It's just not rendered\n. Hi.\nCan you provide more info: HTML input, expected results, actual results, code snippet you are using?\n. This is incorrect HTML AST - document fragment can't be a child node of any other node (with the exception for the template elements)\n. parse5 doesn't provide DOM. For the full-featured DOM implementation, please take a look at jsdom. It uses parse5 under the hood.\n. I don't get it, what's wrong?\n. <template> adjusts it's parsing mode by the first start tag in it. In our case it's <td>, which means that parser will be switched to the in row parsing mode. Since rows in rows are not allowed, following <tr> will be omitted. \nE.g. Chrome's output:\n\n. Maybe you've got a typo in <template>? Seems like it's FF bug (IE and Chrome correctly ignores <tr> and <td> in that case):\n\n. Yes, but this functionality is not implemented in parse5, since it's a HTML parser. You will need to implement it by yourself by traversing the tree.\n. I thought the primary adopter of this will be jsdom, but they just landed encoding support on their side. However, it's missing quick encoding lookup algorithm, but I'm not sure it will give any significant performance gain in single-threaded Node.js land. Is there anyone else interested in this feature?\n. BOM + <meta> encoding detection (fast lookup algorithm from the spec) and actual decoding. string_decoder will not work for us, because we need to support all this encodings: https://encoding.spec.whatwg.org/#names-and-labels. So we will need something like inconv-lite + label converter. \n. I believe decoding shouldn't be implemented on our side, since it's not something HTML specific. Adopters will need it anyway to decode other resources and more likely they will have their implementation. The only thing that we can do on our side is Decoding prescan, but since no one showed interest in it, I'll close this issue for now. Feel free to file another issue if you are interested in this feature.\n. Hi.\nSAXParser doesn't have events for the particular tags, use startTag event instead:\njs\nparser2.on('startTag', function(name, attrs, selfClosing) {\n    if(name === 'uitag')\n        // do your stuff\n});\nAlso, note that things like this\njs\nvar body = parser.document.childNodes[1].childNodes[2];\nwon't work with SAXParser, since it's not constructing any tree.\n. >  Guess the project Wiki can include a few links to these concepts for beginners.\nIt's already does (https://github.com/inikulin/parse5/wiki/Documentation#new-saxparseroptions):\n\nStreaming SAX-style HTML parser. A transform stream (which means you can pipe through it, see example).\n. Here is the complete list of syntax differences of HTML5 and HTML4: https://html-differences.whatwg.org/#syntax. In general HTML5 parser can parse any HTML4 document, but resulting AST will not be equivalent to the one produced with the HTML4 parser. \n. Well, for now I believe it will never be implemented. It would be cool to have such thing for debugging purpose but opportunity cost for this feature is to high. So, closing for now. \n. > We want customTag or custom-tag\n\nCan I ask why you need this? AFAIK DOM API is case insensitive for HTML documents. Case matters when you deal with XML (XHTML), but you need XML parser then.\n. @RReverser But it's still missing actual rules for the mapping. E.g. how procesing instructions should be handled, how to deal with case-sensitivity\n. @songsiqi Can I close issue?\n. Tree adapters maintain the minimum set of methods required for building and serializing HTML ASTs. Is there any parse5-related case there such method should be implemented in tree adapter, why it can't be external helper method?\n. I'm afraid I still don't understand reasoning about this being a part of parse5: \n- If you need this for debugging purpose then debugger watches works just fine for nodes; \n- if you need to take a quick look at AST produced by parse5 for the particular markup you can use online playground; \n- If you need to serialize AST, you will need some circular JSON serializer anyway to preserve correct tree structure;\n- If you need to log your nodes somehow for your project then such logging should be implemented in your application, so it will meet requirements for your target output medium and format.\n. I do agree that it might be really useful. As I said before tree adapters maintain the minimal set of methods required to make parse5 work with the given tree format. Therefore I'm trying to avoid adding optional things there, since it creates additional burden for the tree adapter implementors which might not give any valuable benefits for them. Philosophy here is pretty simple: everything that could not be here - should not be here. There are couple of utils libraries for parse5: dom5 and parse5-utils. This thing could be a good addition for these libraries. Or even better you can come up with your own library (none of them support parse5 >=2.0.0 currently) and share you experience with the world.\n. BTW, you can mixin this functionality to nodes by using custom tree adapter:\n``` js\nconst parse5 = require('parse5');\nconst defaultTreeAdapter = parse5.treeAdapters.default;\nconst myAdapter = Object.create(defaultTreeAdapter);\nmyAdapter.createElement = (tagName, namespaceURI, attrs) => {\n    const element = defaultTreeAdapter.createElement(tagName, namespaceURI, attrs);\nelement.getContent = () => {/*your method code here*/};\nreturn element;\n\n};\n// Same for other node types...\n// Usage\nconst doc = parse5.parse('42', { treeAdapter: myAdapter });\ndoc.getContent();\n. This is by design behavior. Serializer returns `innerHTML` of the node. To workaround it for your particular case you can wrap content you wish to serialize with a `documentFragment`.\n. js\nconst parse5 = require('parse5');\nconst treeAdapter = parse5.treeAdapters.default;\nconst docFragment = treeAdapter.createDocumentFragment();\ntreeAdapter.appendChild(docFragment, yourNode);\nparse5.serialize(docFragment);\n``\n. I don't see how we will be able to break anything here. We need this primarily for the internal maps. The only maps exposed externally are attribute dictionaries inhtmlparser2adapters. But I don't think we can break someone's code here either.  \n. Ok, let's mark it as breaking since it's already used in the wild this way. Not a big deal to release a major, I have some pending breaking changes anyway. Thank you for bringing my attention to this.\n. No, it's impossible. Moreover HTML5 parser decisions are based on the content of the text nodes in some cases. However, you can achieve desired functionality by traversing AST and remove text nodes with only whitespace as post-processing stage.\n. So it's serializer issue (as issue title suggests) orparseFragmentissue? Did you check thatthisChartis string and not a buffer? Also, I can't make any assumption on the nature of this issue without proper small reproduction code.\n. So, I guess I can close issue now?\n. Yes, you can implement [custom tree adapter](https://github.com/inikulin/parse5/wiki/Documentation#q-i-want-to-work-with-my-own-document-tree-format-how-can-i-do-this) which will filter text nodes content on insertion into AST. \n. BTW, seems like you can achieve functionality you've asked for in #120 using this approach.\n. I guess this issue relates to [DefinetlyTyped](https://github.com/DefinitelyTyped/DefinitelyTyped) because we don't ship any.d.tsfiles.\n. Makes sense. Is there any tool that generates.d.tsfrom jsdoc comments?\n. Created separate issue: #125\n. @RReverser Can you give some examples of packages that ships TS definitions? I haven't worked closely with TS, so I would like to learn how it's usually done.\n. OK, I've tried to start working on this, using https://github.com/DefinitelyTyped/DefinitelyTyped/blob/master/parse5/parse5.d.ts as a starting point. But I've encountered a problem that it relies on node.js API definitions. How such issues are resolved in TS land: should we shipnode.d.ts` as well or we can reference it from DefinetlyTyped somehow?\n. @RReverser Oh, cool, thanks\n. I've been playing around with it whole evening and it's really cool that we've decided to implement it: I've found a couple of bugs in the docs, also our docs was a bit inconsistent regarding types. And now I'm planning to get rid of JSDOC for API reference: we'll use typedoc instead. So we'll have new nice type-consistent documentation and up-to-date TypeScript definitions. Which, I think, is pretty awesome.\n. @MeirionHughes thank you for the clarification! Already found in TypeScript Handbook \n. PR is here: https://github.com/inikulin/parse5/pull/164 Everyone interested are welcome to review.\n. @thisconnect \n\nSo new documentation is going to be under http://inikulin.github.io/parse5 ?\n\nCorrect.\n. @thisconnect Moreover, new documentation will have responsive design.\n. LGTM. I guess we will keep this PR open for now, until we will not fix failing tests and figure out what to do with those that we don't know how to fix.\n. I see lots of fails caused by #99, shouldn't be hard to fix\n. Regarding ignored token's property - it's parser intrinsic artifact and can be ignored during comparison, so we can easily fix yet another bunch of tests\n. In earlier versions of spec some end tag tokens were handled in the following manner:\n- Create fake token\n- Reprocess token in state ...\n- If token was ignored in state, do ...\nHowever, seems like someone clarified this sections because I can't find such instructions in the recent version of the spec (cool cool).  Both regular and fake tokens have same set of properties to avoid bailouts in handlers. So:\n- it has nothing to do with tokenization (so we can don't care about it in feedback simulator)\n- we can completely get rid of this property by updating appropriate handlers in parser per recent spec\n. > That would be nicer than just hacking in order to ignore them in a specific test suite.\nI agree\n. Great job! Just some minor style issues left.\n. lgtm, thanks!!\n. Everything else - LGTM\n. lgtm, thanks!\n. Yap, #102 - related\n. The thing is that we need to figure out some heuristics for flushing unused data. I'm not sure, but I have feeling that if we'll do it on every [x]DATA state performance will degrade due to string operations\n. #52 as far as I remember is totally about startup performance\n. I'll change the title then to avoid future confusion =)\n. Yes, parse5 corrects HTML just like your browser do\n\nIf yes is there any documentation I can read to become aware of these side-effects?\n\nNothing comes to my mind except the HTML specification itself.\n. @domenic Well, content-modifying proxy scenario really bothers me. I guess we can go with separate package named forgive-me-whatwg which will use SAXParser + tree adapter + black magic under the hood. But maybe someone has some better ideas?\n. @RReverser Yes, I have an idea how we can do this. \n. > Also you still need to handle the same special cases as the HTML serializer does, at least - void elements, title/textarea/style/script, plaintext (for this one you need to change the serializer also to just stop serializing before ), template, etc. But you also need to deal with foreign content, consider  - the HTML title has a text node child, the SVG title has an element x child.\nWe have ParserFeedbackSimulator which can handle all these cases. My idea was to run tokenizer + feedback simulator and just maintain simple open elements stack (end tag closes all elements in the stack up to the element with this tag name, void elements automatically popped out)\n. I'll return to this topic later with better solution. Meanwhile for such scenarios I suggest to use https://github.com/reshape/parser\n. > I thought you were the parser used by angular2. I'll start looking for their parser then.\nhttps://github.com/angular/angular/blob/master/package.json#L90\nBut seems like it's not used to parse their HTML-like syntax\n. Can you provide minimal reproduction example, please?\n. > It was working before (but that may of been due to encodeHtmlEntities: false we used to have.\nCan't tell for sure. Can you set breakpoint in escapeString function and add watch to str value when it fails? \n. Because if it's an object, it worked but worked wrong: attribute value (which seems like an object) implictily converted to the string on html concatenation. \n. OK, I guess it unlikely will be used by anyone except jsdom, so I think we don't need shorthand for it. Parser.parsePlainText sounds good to everyone?\n. @RReverser You can't emulate plain-text parsing by wrapping text with some markup: we need to wrap text with <pre> and parse it's content in plaintext tokenizer mode (<pre>'s are parsed in data mode in regular parsing algorithm).\n. I mean we should emit pre and insert text, parsed in 'plaintext' mode as it's content.\n. By plain-text parsing I mean parsing of text files\n. Spec requires to emit <pre> instead of <plaintext>:\n\nact as if the tokenizer had emitted a start tag token with the tag name \"pre\" followed by a single U+000A LINE FEED (LF) character\n. This is how it should look like:\n\nhtml\n<html>\n<head></head>\n<body>\n<pre>\n[[Parsed with PLAINTEXT tokenizer mode]]\n</pre>\n</body>\n</html>\n. @domenic Now you guys have yet another reason to switch from parse5@1.x =)\n. Can you add HTML code in issue instead of zip-archive, please?\n. > The\n\ndocument is over 800KB so I'm not sure why the ending tokens are not\nbeing emitted. I don't know where to begin in parsing it by hand to\nget rid of supposedly non-relevant stuff.\n\nUsing bisection of the input and running parser on each iteration. Worst case shouldn't take more than ~20 iterations on input of this size.\n. > Hmm, how would I effectively calculate the function value at the mid\n\npoint of an HTML file?\n\n???\n\nDo I just chop it in half then feed the first\nhalf to the parser, otherwise the second half? Anything of this sort\nand the HTML file would be malformed.\n\nJust split content in <body> tag, should work just fine.\n. OK, thank you for the simplified example. This is a by design behavior: when parser encounters <iframe> it parses it's content as a text. Here is an example: http://astexplorer.net/#/bz2uxPtBGp\nAs you can see </body> parsed as a text node. You will see similar behavior in browsers. \nThis page markup is non-conforming: seems like they forgot to close <iframe> but it doesn't have visible effects because parser impilcitly emits closing </body> and </html> anyway.\n. Awesome (just as always)! Thanks!\n. @weiweiwei256 Hi, your question is unclear.. If I get it right you want to serialize node without it's child nodes. If so, you can create document fragment using treeAdapter.createDocumentFragment method. Add node to fragment's childNodes and temporally assign [] to node's child nodes. Serialize fragment, then restore node's childNodes. \n. Just like any other AST, by performing tree traversal. Each node has childNodes, you can visit them recursively or using stack. \n. > How can I disable this function?\nWhy would you like to disable it? What's your scenario?\n. If razor is not a subset of HTML syntax, you shouldn't use HTML parser for it. Use razor parser instead\n. Hi! \nThank you for the kind words, really appreciate it! \n\nIs it possible to use parse5 for a task like this? Is there some type of parse mode that won't alter the tags, or a way for me to get the fragment parse not to strip tags?\n\nWe discussing such thing in https://github.com/inikulin/parse5/issues/132. It's still debatable, but you can give your upvote for this feature and drop your scenario as an argument there. \n\nAlso is there documentation anywhere on which tags are stripped by fragment parse mode, and/or added by full document mode?\n\nAs far as I remember this is not documented explicitly. Regarding fragment parsing, if you're use parsing without context element, <template> context will be used. In that case <html>, <head> and <body> will be stripped. \nFollowing tags will always be implicitly added by full document parsing mode:\n- <html> if missing\n- <head> if missing\n- <body> if missing\n- <p> - if </p> occurs without open tag\n- <colgroup> if <col> added directly to <table>\n- <tbody> if <td>, <th> or <tr> added directly to <table>\n- <tr> if <td> or <th> added directly to <table>\nI hope i didn't forget anything.\n. > would it be possible to work around the issue in the meantime by providing a different context explicitly, maybe something like an  element? \nYeah, you can pass <html> as context, but <body> and <head> will be generated implicitly anyway if they are missing.\n\nperfect, thank you for the quick and thorough response! Just pitched in at the linked issue. Would be happy to help out as well if someone is willing to hand-hold me a bit at the beginning, just bc this is a large and unfamiliar code base.\n\nNeed some time to figure out how it will be actually done (more likely it will be a separate package on top of parse5, but we need to expose some API first). Unfortunately, I'm extremely busy right now and stepped away from parse5 development for some time. I hope I'll be back in late August, but meanwhile maybe @RReverser could help?\n. @jescalan I'll try to release new parse5 version on Thursday which includes some great updates to our SAXParser made by @RReverser and we will try to build some basic solution on top of it.\n. @jescalan Yeah, new release will not bring any API changes, but it contains some important fixes for the SAXParser. Anyway, you can already start prototyping. The idea is quite simple: maintain own open element stack, on startTag event of SAXParser create element using tree adapter and if it's not in list of void elements put in into stack. Append nodes and elements to the top element on the stack (add document or documentFragment to the stack before you start parsing). Once you encounter end tag - pop elements up to matching element or until only document left on the stack. To deal with tree you can use one of provided tree adapters, you can find their API description in the docs.\n. @jescalan It shouldn't be related to doctype. Regarding self-closing tags: https://github.com/inikulin/parse5/wiki/Documentation#q-im-parsing-img-srcfoo-with-the-saxparser-and-i-expect-the-selfclosing-flag-to-be-true-for-the-img-tag-but-its-not-is-there-something-wrong-with-the-parser - you need to check against the list of void elements as I mentioned in comment above. \n. Hmm, it should parse plain text as is: https://tonicdev.com/57a10ab6594ef21300a7a1ad/57a11834d2ab3913009ee831\n. @stevenvachon It will not perform any tree structure correction, that's the point of this whole thread.\n. @thisconnect @jescalan Guys, I have a feeling that this discussion doesn't belong to parse5. Can you choose another medium to proceed with your conversation to not spam those who watching this repo, please?\n. Hi, thank you for the contribution, but I agree with @RReverser here. We expect correctly formed AST as an input.\n. Thank you for the notes! Fixed AAA loop condition.\n. @RReverser Feel free to merge if you don't have any additional remarks\n. Hi,\n\nCould someone clarify that SAXParser is supposed to return Attributes of the form\n\nYou are totally right.With exception that prefix present only for attributes that actually have prefix. Thank you for the find!\nCan you issue a PR, please? Tip: you need to modify JSDOC comment: https://github.com/inikulin/parse5/blob/master/lib/sax/index.js#L166\n. Closed via #148\n. Thank you!\n. Hi!\nThis is a by design behavior. parse5 has different tree adapters which can produce different tree formats. Default tree adapter was designed to get best performance for the parsing algorithm, that's why attributes presented as array there. Then you parse HTML document, more likely you will like to get it parsed ASAP and you don't care much about fast attributes lookup (in common case). Meanwhile, the only thing that you can do with location info is to perform lookup, that's why lookup table fits better there. And, therefore, for obvious reason location info for attributes has prefixed keys. Yes, this leads for API inconsistency, but this inconsistency appears only for default adapter and was introduced for good reasons.\n. Hi!\nThank you for the kind words, really appreciate it. \nSpec requires to drop spaces before <html> and <head> elements (see: https://html.spec.whatwg.org/multipage/syntax.html#writing:the-html-element-2). So this is a by design behavior..\n. @wooorm  We trying to not violate spec, even optionally. However, can you clarify your scenario, please? I guess it's some kind of html reprocessing, if so, we have related discussion going in #132. Maybe you would like to put your two cents there. Also, you may be interested in #144, @jescalan  already implemented LexicalTreeParser on top of parse5, maybe it will serve your needs.\n. Nice catch, thank you! Can you add regression test, please?\n. Awesome, thank you!\n. Published as v2.2.1.\n. If I get it right, you need some custom nesting logic, so you'll be able to incorporate markdown and parse5 ASTs. I believe SAXParser may do a trick for you. It will give you HTML tokens which you can incorporate in your AST then. You can use treeAdapter methods as an utility library to deal with the tree. Regarding position information - I think we can't do much here, except manually adjusting position information, produced by parse5. \nBTW, I'm not sure how markdown grammar works exactly. But wouldn't it be simpler to parse document as HTML first and then walk through text nodes and convert them to markdown?\n. @Jeerachee Thank you for the report. The weird thing is that locations info in full-featured parser works correct. PR is welcome.\n. Arghhh, for some reason node v6.6.0 can't install native dependency required for benchmark. I'll make it optional\n. Fixed\n. More results on bigger data:\nInput data size: 365.99 MB\nDuration:  32.85 seconds\nMemory before:  25.63 mb\nMemory after:  25.03 mb\nMemory max:  118.63 MB\nInput data size: 731.99 MB\nDuration:  1.09 minutes\nMemory before:  25.63 mb\nMemory after:  25.05 mb\nMemory max:  118.45 MB\nInput data size: 1.37 GB\nDuration:  2.03 minutes\nMemory before:  25.63 mb\nMemory after:  25.13 mb\nMemory max:  123.75 MB\n. @RReverser \nSurprisingly, it doesn't affect performance much. The difference is within an order of statistical fluctuation:  \n[16:07:11] Starting 'benchmark'...\n[16:07:11]   Running 'parse5 regression benchmark - HUGE' \n[16:07:18]     Working copy x 6.53 ops/sec \u00b19.22% (21 runs sampled)\n[16:07:24]     Upstream x 6.52 ops/sec \u00b16.73% (21 runs sampled)\n[16:07:24]   'parse5 regression benchmark - HUGE' (passed: 2, failed: 0)\n[16:07:24]     Passed:\n[16:07:24]       'Working copy' at 1.00x faster\n[16:07:24]       'Upstream' is etalon\n[16:07:25]   Running 'parse5 regression benchmark - MICRO'\n[16:07:30]     Working copy x 40.29 ops/sec \u00b19.80% (58 runs sampled)\n[16:07:36]     Upstream x 39.47 ops/sec \u00b15.95% (54 runs sampled)\n[16:07:36]   'parse5 regression benchmark - MICRO'  (passed: 2, failed: 0)\n[16:07:36]     Passed:\n[16:07:36]       'Working copy' at 1.02x faster\n[16:07:36]       'Upstream' is etalon\n[16:07:36]   Running 'parse5 regression benchmark - PAGES'\n[16:07:41]     Working copy x 134 ops/sec \u00b14.14% (56 runs sampled)\n[16:07:47]     Upstream x 137 ops/sec \u00b13.88% (57 runs sampled)\n[16:07:47]   'parse5 regression benchmark - PAGES' (passed: 2, failed: 0)\n[16:07:47]     Passed:\n[16:07:47]       'Upstream' is etalon\n[16:07:47]       'Working copy' at 1.02x slower\n[16:07:47]   Running 'parse5 regression benchmark - STREAM' \n[16:07:53]     Working copy x 102 ops/sec \u00b14.36% (73 runs sampled)\n[16:07:59]     Upstream x 104 ops/sec \u00b15.18% (59 runs sampled)\n[16:07:59]   'parse5 regression benchmark - STREAM'  (passed: 2, failed: 0)\n[16:07:59]     Passed:\n[16:07:59]       'Upstream' is etalon\n[16:07:59]       'Working copy' at 1.03x slower\n[16:07:59] Finished 'benchmark' after 48 s\n. @RReverser \n\nWaiting for answers on review questions :)\n\nI can't see any. Are you sure you've submitted the review?\n. ping @RReverser \n. Hi, yes it's possible by using browserify, webpack or any other bundler of your choice.\n. > However in firefox and chrome foo=value results in the attribute being ignored and not added to the element due to missing quotation marks.\nIt can't be true. Are you sure you parsing HTML document and not XML?\n. You are right, <title> content is always parsed as plain text. I believe the best way will be to raise an error or warning that tag accepts only text content. Here is the list of such elements: title, noscript, noframes, style, script, xmp, iframe, noembed.\n. Nice catch! Thank you for the contribution.\n. Released as v2.2.3\n. Hi Thomas.\nparse5 doesn't collapse any whitespaces (and shouldn't in this particular case). Here is the live demo with markup you've provided: https://runkit.com/580f9ad527cf550013c20537/580f9ad527cf550013c20538\n. Hi, thank you for contribution. Can you elaborate, please, what problem this PR solves. \n. Well, it's something that shouldn't be fixed on parse5 side. attrs argument is marked as required in API reference. And parser always passes this attribute. treeAdapter is not a general purpose utility library for manipulating AST, it's designed to maintain minimal API required by parser to build AST, anything else is out focus of this project. Any additional functionality on top of parse5 AST can be found in following utility libraries:\n- https://github.com/webdeps/parse5-utils\n- https://github.com/PolymerLabs/dom5\n. > rather than adding one line to the existing code (one line which has absolutely no drawbacks!)\nIt has a drawback that it reduces performance by introducing additional check that is not required by parser because it always provides attrs argument. Also, it's a question of consistency, what makes attrs argument special that it can be omitted? Should we do it for other arguments and functions? And once again: treeAdapater is not a general purpose AST manipulation library, it exists just to tell the parser how to deal with current tree format. \n\nI'm really not sure why you'd rather point people to using a wrapper library which basically no-one is using\n\nI'm just trying to help you solve your problem.\n. @RReverser Thank you!\n. @albertbuchard Hi. Can you elaborate what kind of comments and use cases will be helpful for you, please.\n. Ironically this issue doesn't contain any comments or use cases for documentation problem. However, I have some ideas on how documentation can improved.\n. Hi @rictic,\nI'll try to roll out next release on Monday or Tuesday.. @rictic just released v3.0.0 which includes this fix. Self-closing tags is an XML thing, so they are only allowed in foreign content (<svg> or <math> content), see: https://runkit.com/582b0e9ebe07a80014bf1e82/58400d2db3ef0f0013bae090. Haha, good catch. Can you regenerate docs as well by running gulp docs?. It's one-based. Thank you for pointing it out!. Hi guys,\n\nI think it's bug, but maybe owener have own look\nSo current issue is that nodes after an unclosed tag are swallowed and reported as attributes of the unclosed tag.\n\nIt's perfectly legit. The issue with HTML autocompletion is that HTML is designed to swallow any input. So if you feed parser malformed HTML it will not bailout with syntax error, but will try to re-interpret it instead, like we have in this case. As a solution you can try to implement some simple HTML parser with relaxed grammar but you will run into edge-cases anyway. The conclusion is that you can't avoid edge cases like this with HTML while working with partial input. So just\n\n. There is no canonical way to avoid it, since such serialization will not be per spec. However, you can monkey patch SerializerStream to achieve desired functionality:\n```js\nconst SerializerStream = require('parse5');\nvar s = new SerializerStream(document);\ns.serializer.constructor.escapeString = str => str;\nvar result = '';\ns.on('data', chunk => result += chunk);\n...\n``. Hi, \nThere is no canonical way to avoid it: attributes without value semantically is just a shorthand for the attributes with empty value. So, oncegets parsed there is no way to distinguish it fromin AST. HTML spec requires attributes with empty values to be serialized in explicit form. However, you can workaround it by overriding [serializeAttribute method](https://github.com/inikulin/parse5/blob/master/lib/serializer/index.js#L105) the same way as I suggested in #175. To conclude, judging from your previously open issues I guess it makes sense to implement custom Vue-specific serializer.. [jsdom](https://github.com/tmpvar/jsdom) is greate and it uses parse5 as a parser under the hood. However, since jsdom tries to implement full featured DOM it may be too heavy weight for some scenarios. For basic parse5 AST traversing you can take a look at [dom5](https://github.com/Polymer/dom5).. Well, any HTML is valid, however it can be *non-conforming* - in that case spec says to report [parse error](https://html.spec.whatwg.org/multipage/syntax.html#parse-error). I believe that having validator is a good thing for some use scenarios, e.g. having conforming HTML justifies that it safe for parse-serialize round trips, consequently making HTML instrumentation safe as well. We had this discussion before: #55. And it still blocked on whatwg/html#1339. I'll keep it open as there is a demand for the feature, however I wouldn't expect it to be implemented soon.. @diervo It would be great!. @RReverser you're right that cross browser compatibility is not an issue anymore (kinda), but it might be useful to ensure that provided markup will be interpreted as intended, because in some cases auto corrections may screw things up.. @diervo Sure, I'd love to finish this work, I've also started to do some spec work for the tree construction stage on my own. Hit me up by email and we'll try to figure out the best time to finish it.. Published asv3.0.2`. Hi,\nI don't see any bugs here:\nhttps://runkit.com/58bdb485d0de8a0014aa1f3d/58c0331fb316500013011735 . I'm not sure I understand what you're asking. Can you provide comprehensive example of what you're expecting and your usage scenario?. The thing is that html entities are not text content. HTML parser transforms them to text content as you see in your example. If you need source code that represents text content then consider using locationInfo.. As stated in documentation SAXParser is a regular node.js writable stream. This repository is not intended to provide documentation for standard node.js API. Use node documentation instead.\n\nAlso in the parser.on('text', text => {}) callback, is there any way to tell which tag the text is currently in? For example, if I'm only interested in the text between the  tag. How can I know from the on('text', text=>{}) callback that it is currently within the  tag or another tag that I'm not interested in?\n\nWhy you try to use SAXParser if you need information about nesting? This is completely opposite to what SAX parsers are usually used for. Use ParserStream instead.. No. @elgs Did you manage to navigate to link I gave you already two times? Because there is answer to your question and examples there.. > I want to output parsed result to string, as opposed to a file stream\nSAXParser doesn't produce any compound results, it just raises a number of events with chunks of information about markup. . @elgs \n1. SAXParser is a Node.js TransformStream which means it's both WritableStream and ReadableStream. The very first example in Node.js documentation for WritableStream shows how to feed it with strings:\n\nWhile specific instances of Writable streams may differ in various ways, all Writable streams follow the same fundamental usage pattern as illustrated in the example below:\njs\nconst myStream = getWritableStreamSomehow();\nmyStream.write('some data');\nmyStream.write('some more data');\nmyStream.end('done writing data');\n\nConsidering example above, here is how you can pass string to SAXParser:\n```js\nconst parser = new SAXParser();\nparser.on('text', text => { / ... / });\nparser.end('Hi there!');\n``\n2.SAXParser` doesn't return anything. You can use it as a proxy between feeder and consumer streams (e.g. response and request) to analyze content while it's gets transferred between mediums. It doesn't modify content that piped into it and passes it to consumer stream as is. Therefore, since you already have whole string in memory in your case there is no reason to pipe it anywhere: you can just simply write it to output stream (if you have one).\n\nThe thing with SAX parsers is that they are memory and computationally efficient because they don't need to track nesting information. Otherwise, you need to consider using full-featured parsing. Nesting in HTML is extremely tricky. Thus, solution from https://github.com/inikulin/parse5/issues/183#issuecomment-285699547 will give incorrect results in lots of cases. Nevertheless, if accuracy is not a big concern for you then you can try this approach on your own risk.. @FredKSchott The thing with HTML is that it's not parse-serialize round trip-proof anyway (at least unless you don't have fully conforming markup). Even adding ad hoc solution for this particular case we will not get any closer to 100% identical/correct re-serialization. The root cause of this lies in the design of HTML and it's error recovery mechanisms.. Hi,\n\nI guess you need markdown parser which provides source location information, such that you'll be able to get source location information for HTML-code markdown AST node, then get substring that contains HTML, parse it with parse5, modify it the way you like, serialize it back and then insert back into original string instead of parsed fragment. Overall, this is how I see it. However, we don't provide and don't have any plans to provide facilities to deal with markdown.. You can import interfaces, but just like you mentioned they're namespaced:\n```ts\nimport {  AST  } from 'parse5';\nfunction myFunc(doc: AST.Document) {\n}\n``. Location will still be implemented via hooks. For errors there will be combination of imperative and AOP. I don't like it either, but we can't use imperative approach for this feature since it will harm performance in basic case. Maybe, maybe I'll revisit this topic one day being fed up with current approach, but meanwhile it doesn't bother me that much.. Tree adapter will need to implement two methods:\n-setNodeSourceCodeLocation-getNodeSourceCodeLocation`\n__location property will be renamed to sourceCodeLocation.\nBoth changes are breaking.\n. We've developed quite complicated relationships with TypeScript (\ud83d\udc94 ) , therefore it will not happen in the foreseeable future.. Implemented in HTMLParseErrorWG fork: https://github.com/HTMLParseErrorWG/parse5/commit/fee10f1008efcb07fad39451ff8a94e4339adda4. Will be merged later along with the validation feature.. Will keep it open until merge.. Implemented in 2ee5b7cacfc4264e01dab736bb46af721b2f4ffa. There is no such thing as an \"invalid HTML\". There is a non-conforming HTML. What's the use case for that?. It can be covered by #180 then. There will be validate method which could be used for such checks.. @adrienboulle Element IDL definitions are handled in the DOM and the goal of this project is just to provide HTML AST. For a full featured DOM please refer to the jsdom project. . parse5 produces such AST if  write or end was never called on a stream. Basically, if stream is in initial state and haven't received any data yet. . Interesting, seems like node stream doesn't call _write on end without content. Well, that's a bug, as a fix we can write empty string to stream on construction. Care to issue a PR?. It's a bug. We give erroneous AST on empty string input (which is still a valid input). Try\n```js\nconst parser = new parse5.ParserStream(options)\n  .once(\"finish\", () => resolve(parser.document));\nparser.end();\n```. @stevenvachon That's pretty much the same.. @stevenvachon If this issue was covered by this test then why it didn't fail?. This test doesn't test that we return well formed document in that case.. We don't even need a test file in this particular case, since it's just an empty string and bug occurs in very specific scenario. Take a look at other regression tests for parser for a clue.. > @inikulin I'm asking you, since it's your library.\nidk, it's you claiming that adding new tests is redundant. And I don't understand why, since we don't have tests that cover this particular case. And no, this test shouldn't caught this case, because if you take a look at the code of test utils you'll see that we have a single chunk with '' content which is written to the parser. Therefore, it doesn't cover our case.\n\nIf we don't need a test, how are we to ensure that this fix is not undone in the future?\n\nI've never claimed that we don't need a test, I wrote that we don't need a test file like this https://github.com/inikulin/parse5/blob/master/test/data/tree_construction/tests1.dat. Since error is caused by the specific combination of API and not by the particular input you'll need to write test in the same format as e.g. this tests: https://github.com/inikulin/parse5/blob/master/test/fixtures/parser_test.js#L62. @stevenvachon seems like we've had a mutual misunderstanding. LGTM, thank you for this!. Congratulations, you've hit the darkest corner in the HTML parsing that is the Adoption Agency Algorithm. \nConsider the following markup <a>X<p>Y</a> (it's a bit modified version of your markup with text nodes added for clarity), here is how it will be parsed:\n\nParser encounters an a element that is \"split\" by a  p element. ~~a can't contain p element since the former is a block element~~[see correction below], therefore it \"squeezes out\" p out of the a element. However, according to the markup part of the p content was also in the a element content. Thus, to preserve semantics and styling parser creates an additional fake a element (with the same attributes as the original a element) inside p element that incapsulates content that was previously in both elements.\nAnd here we finally come to our issue: the element whose location you try to obtain is that fake a element. It doesn't exists in the markup, hence it doesn't have any location. However, the original a element (i.e. document.childNodes[0].childNodes[1].childNodes[0]) has location as seen from your example.\nTo conclude it's a by design behavior: we don't add location for the nodes that are not present in the markup, which IMHO makes perfect sense.\nCorrection: Actually, there is no such things as \"block\" or \"inline\" element in terms of HTML. Moreover, since HTML5 you can wrap \"block\" content with hyperlinks. The reason why p gets \"squeezed out\" is because it is in the special elements category and have specific parsing rules, whereas \"regular\" elements can be simply closed without consequences by </a>. Other formatting tags (e.g. <i>, <b>) can be closed without consequences as well without breaking semantics. In case if the end tag for such overlapping formatting element is found later in the input it is reconstructed by other algorithm, e.g. <a>X<i>Y</a>Z</i> will be parsed as:\n\n. @stevenvachon Would love to accept PR for this. Thank you!. The issue is that parse5 tree adapters are not general purpose DOM manipulation libraries. Their primary purpose is to provide a minimal low-level abstraction layer to parser that will allow it to work with a particular abstract syntax tree format. Thus, in this particular case, appendNode doesn't expand fragment's child nodes (like full-featured DOM does), it just inserts the fragment node into the tree. Therefore, serializer doesn't serialize this fragment, since fragments are only valid as tree roots or <template> content, i.e. you have malformed AST. \nI can suggest you to use jsdom or cheerio v1.0-rc1, which use parse5 under the hood for the parsing and serialization purposes, but provide high-level API for such DOM manipulation tasks.. Providing some custom handling for tokens will turn it into non-HTML grammar. And supporting any grammars other than HTML was never a goal of this project. However, I guess such incomplete input should be correctly handled by editor-focused incremental parser that is discussed in #201. Therefore, if you wouldn't mind I'll close this issue in favor of #201.. Hi,\nThank you for the suggestion. I've have thoughts about implementing language services suitable for usage in editors for some time now. The problem is that I don't have experience in editors development to figure out required capabilities for such parse5 API. \nI guess incremental parser will be a good first step towards this direction, but I'm interested in established practices that are used for such parsers for other languages. I have some ideas already, but I would like to avoid inventing a bicycle. \nAlso, I've noted that you mentioned a tree, so I guess you need a full AST and token stream will not work for you? The problem with full incremental parsing is that already parsed nodes are not invariant in HTML and can be modified by consequent input. Therefore, invalidation of the subtree can be a computationally expensive task, and hence ruin any advantages of the incremental parsing. Whereas, simple token stream don't have such limitations. \nOn the other hand, we can build simplified AST that avoids quirky error recovery mechanisms, but suitable for usage in editors, which brings us back to the idea of the LexicalTreeParser. \nTo conclude, I would really like to learn more about this domain area before jumping to the actual implementation. Any details/discussion on this topic will be definitely appreciated.. Why should we do this?. > @types/node is only used for development, and is not used when parse5 is used as a library\nIt is used in production code in .d.ts file.. @7Pass Please take a look at #235 . Yeah, it was already removed in the Parse Error WG fork. It will be merged once we accomplish tree construction errors. Is it hits you in jsdom at the moment somehow and you need a quick fix? Or you can wait till Parse Error WG fork will be merged into upstream?. Great then. Thanks for the report!. I'm actually going to use upstream html5lib tests for daily CI builds in the future, so once something will change in the spec and, thus, hopefully, in tests we will get notifications about it automatically.. Changes from parse error working group's fork has been recently merged, so closing this one.. Duplicate of https://github.com/inikulin/parse5/issues/184.. > One could go a step farther and make the interface generic over all of the subtypes\nI've initially had thoughts about it, but eventually decided to implement it in the simplified manner. If you are keen to implement this, PR is always welcome!. This issue can occur only if you feed non-string to parse5 to parse. I believe this should be issued to Angular repo.. Hi,\nIt's used to copy attributes from duplicate <html> and <body> tags to already existing elements. E.g.: https://github.com/inikulin/parse5/blob/fee401878d866ab87fbc79a4dc5f4f58e4b699c5/lib/parser/index.js#L1232. I guess you using DOMParser in XML mode (which is default). If you switch it to HTML mode you will get consistent results with parse5.. Oh, I see, sorry for misleading you. The problem is that parse5 produces AST and not full-featured DOM. HTML parser should lowercase all tag names with few exceptions for some foreign content elements. The tricky part is that tagName property is not the same thing as tagName property in Element interface.  When parser creates element it passes tagName as localName argument to element creation algorithm in DOM spec. Whereas tagName in DOM Element interface is defined as uppercased version of localName in case of HTML elements. \ntl;dr tagName in parse5 AST is actually a localName in terms of DOM spec. However, since this is a DOM-agnostic implementation of parsing algorithm and to not confuse users who are not familiar with the difference (including me 10 minutes ago) it's called tagName.. @ianstormtaylor No, I have strong believe that we shouldn't introduce such change. In addition to already provided argument about confusion for users there is a technical reason for that. This project implements HTML parsing specification which doesn't operate with term localName from DOM specification. parse5 uses tree adapters as an abstraction layer between parser and particular AST format. DOM is just one of such formats that parse5 can work with (see jsdom) and transformation of tagName in localName is AST-specific and should be performed in particular tree adapter.. @ianstormtaylor\n\nI get the decision to use the \"less confusing\" name, but it seems like it's more confusing in the end if it doesn't match casing?\n\nIn your case confusion comes from delusion that parse5 implements DOMParser functionality. When, in fact, it provides much more low level functionality. DOMParser requires at least full-featured DOM implementation working in conjunction with HTML parser. Current naming never caused confusion before, because users either use it for tasks that doesn't require full-featured DOM or use already mentioned jsdom if they need one.. Thanks!. @plantain-00 Published as v3.0.3. Hmm, creating dedicated builds for browser means that we need to setup dedicated testing browser environment and updating docs about API available in browsers. Unfortunately, I don't have time to invest into these changes. And can't it be fixed on bundler's side with some basic mocking?. We still need tests for that and documentation update.. BTW, PRs with tests and updated docs are welcome.. @dantman please take a look at #235 . HTML spec requires to lowercase tag names and attribute names (with some exceptions for foreign content). However, you can workaround it using location info API and checking source attribute string. Also, AST alone is not sufficient to perform HTML linting, since parser already performs corrections in case of invalid markup. However, AST+location information should work fine. . @afiestas If you insert <script> element in <html> tag before <head> element the parser will implicitly create <head> element and insert <script> element into it. Therefore, <head> in your markup is parsed within already existing <head> and, thus, completely ignored.. @afiestas \n\nThen, would it make sense to merge the first head tag that is parsed\n\nNo, this is a per-specification.\n\nOtherwise Information will be missed.\n\nWell, you'll always have some trade-offs if you have non-conforming markup.. Duplicate of https://github.com/inikulin/parse5/pull/145. Hi, there is no such thing as a processing instruction in HTML syntax.. > Including the xml declaration may not be the best practice but it is allowed in HTML5\nIt is not. . This is a per-spec behaviour: script tag (like any other HTML element) can't be self-closing and trailing slash in the start tag is ignored.. Hi, there is no way to make parse5 behave in any custom non-spec compliant mode and we don't have any plans to introduce one. However, feel free to fork it and implemented desired functionality in your custom fork.. It may be debatable, but TS folks recommend to include type definitions as production dependencies for libraries: https://github.com/Microsoft/types-publisher/issues/81 Considering that @type/node doesn't have any dependencies itself and it's just a few hundred lines of code I struggle to understand myself how it can be a problem as well.. @RReverser I was thinking about peer dependency, but it feels like it will introduce a lot of unnecessary hasle for users.. OK, at this point I'm totally convinced that we should keep it as a dependency, so closing.. @shepmaster @domenic Please, take a look at #235 . Hi, thank you for the PR. Unfortunately, we don't accept any changes that violate HTML specification.. Hi, I've had thoughts about this as well. But note that such transform stream will operate on tokens (like SAXParser), so information about nodes nesting will not be available.. Hi, thank you for the proposal, but we won't implement or accept PRs for any features that violate the spec or provide various \"relaxed\" parsing modes. However, feel free to fork parse5 to implement functionality required for you project.. Hi @wooorm, thank you for the report. It was introduced (seems like inconsistently) due to https://github.com/inikulin/parse5/issues/119. Maybe it shouldn't be a problem to use regular objects for location, idk. PRs and ideas are welcome.. Hi, yes: https://github.com/inikulin/parse5/issues/220. @alexandrudima Please, take a look at #235 . Hi, thank you for the contribution, however I will not accept your PR, since it's an intended behaviour. See https://github.com/inikulin/parse5/issues/220. First of all, @huntharo I'd like to say that I don't appreciate your tone in other thread. I work on this project in my spare time and if I wasn't able to answer you in timely fashion that means that something more important happening in my life at the moment. I'm not your personal support engineer. This is not the way how open source works.\nNow, back to the topic. parse5 distributes TypeScript typings, so engineers who use TypeScript as their primary language can start using it without any additional efforts. Since, parse5 inherits from some of node.js built-in classes it requires node.js typings. \nTypings are added as npm dependency without fixed version (see: https://github.com/inikulin/parse5/blob/master/package.json#L61). If you install parse5 as a dependency of your project npm will not install it's dev dependencies. This is the reason why node typings are specified as production dependency.\nFrom this perspective nodejs typings is just a subdepency of parse5. It's just a regular npm workflow. I really struggle to understand how it may break any dependent of the package. Is ReactNative uses some specific \"magic\" ways to manage it's dependencies? If so, the problem definitely shouldn't be addressed in this repo.. > So if I would be working in a Node project, I'd most likely have those types installed. But I'm not, which is why Parse with its forced import messes up the dev environment for RN developers. Does that make sense?\nNot really. If you already have typings installed npm should peek already installed typings and use it for parse5 (for that reason parse5 has * specified as node typings version).. @fictitious Thank you very much for the input. Seems like now I have a full picture of what's going on. However, it's not quite clear what will be a proper solution that will satisfy everyone. As far as I understand, if I remove node typings dependencies RN will still complain about parse5 typings since some of classes exposed by parse5 inherit from streams which are part of node API.. Can someone from folks who works closely with RN give an information if RN provides typings for other node.js APIs like streams? If so, I can make node typings a peerDependency. Otherwise, I don't understand how it's suppose to work, considering that parse5 typings rely on node APIs and it seems like RN compilation will still fail.. Despite me initially trying to follow TypeScript developers suggestion, seeing how many issues it's caused recently I'm starting to think to indeed make node typings a devDependency . \nBut the problem (if I understand it correct) is that will not fix issue for which this PR was opened: since ReactNative takes in consideration typings of dependencies it will still fail because parse5 provides it's own typings that rely on nodejs typings. (Please, correct me if I'm wrong).\nSo, the only solution is to remove typings from parse5 completely, which doesn't sound as appealing solution.\nThis problem is not specific for parse5: there are other packages in npm that provide typings and rely on node typings. My overall impression is that either I'm getting it completely wrong and it would be nice to see comprehensive guidelines from TypeScript folks of how such situation should be handled, or ReactNative does it the wrong way. \nEither way, it would be nice to hear opinion of maintainers of these projects. Does anyone care to open issue in appropriate repos, I'm from phone at the mo and already got tired typing this answer =).. Folks I've created an umbrella issue for all the TypeScript/environment related problems: #235 Let's continue discussion there.. Hi,\n\nMaybe the reason is that the parser does not directly call createTextNode--instead it calls insertText and insertTextBefore\n\nThat's correct. The reason is that treeAdapter is a minimal set of API required by parse5 to perform parsing and not a general purpose DOM manipulation utility belt. \nIf you want to manipulate parse5 AST I can suggest you to try https://github.com/Polymer/dom5 or https://github.com/webdeps/parse5-utils\n. @bary822 Thank you for the contribution. Seems like it's a part of the bigger TypeScript workflow-related problem, so I'll close this one in favour of #235 . @fictitious Thank you for handling this one! Closing since comprehensive answer was given above.. @makma Closing this in favour of #235. Hi everyone.\nI've just released v4.0.0 that implements proposed ad-hoc solution to npm. Now you should be able to bundle parse5 for platforms other than Node.js. Please let me know if you encounter any additional problems.. Hi @domenic, \n\nwe need some notification: it requires action when\n\nCan you point me to the parts of the spec that describe these notifications? It will help me to come up with a proper API for that.. Hi, thank you for the contribution, but it's an intended behaviour. Please read http://inikulin.github.io/parse5/#typescript-definitions for the details.. Unfortunately I can't come up with better workaround for this kind of issues at the moment. Splitting into separate packages is the only solution we've came up with so far. Tracking ticket for this work is https://github.com/inikulin/parse5/issues/235, so if you wouldn't mind I'll close this one as duplicate.. @Enngage This should be fine, unless you start using streaming APIs.. Closing for now, as issue seems like to be unrelated to parse5 itself. Will be happy to reopen when more info emerge. . @jescalan But as far as I understand you have custom serialiser, right? As mentioned above, parser doing the correct thing resolving entities in attributes as a part of parsing process. It's a burden of serialiser to take care of correct escaping of quotes in attributes. Links provided in https://github.com/inikulin/parse5/issues/241#issuecomment-364929967 is a good starting point.. That's correct . Hi,\nI can't reproduce this issue: https://runkit.com/embed/gy9ps7mf4564\nClosing for now. Will be happy to reopen once you provide reproducible example.. Hello, thank you for the issue, but I'm afraid I don't quite understand from the provided info what you find incorrect in parse5 behavior.\nMy only assumption is that replacement of <image> with <img> has confused you. If so, I can assure you that this behavior is per-specification and implemented in all modern browsers.\nI'll close this issue due to insufficient information. Once you provide more info, I will be happy to reopen.\n. Hi, unfortunately it's unclear from the issue what the problem is. Will be happy to reopen once you provide more information.. @suuuuuuuuperman There is no such thing as a self-closing tag in HTML (though they exist in SVG). Therefore, parser just ignores / in the end of the tag. input is a void element in HTML which means that it doesn't require end tag, so semantically markup is equivalent in both cases.. Thank you!. Closing since the problem is definitely in the way you bundle parse5 for browser. Sorry, I can't even help you with an advice since the provided information is not sufficient to make any assumptions.. Text nodes between these nodes are ignored as per specification. You can observe the same behaviour in browsers: https://jsbin.com/yakuxehebi/edit?html,js,console. If you need to modify HTML with less modifications to the original formatting, then take a look at parse5-html-rewriting-stream.. Thanks for reporting this!. Yeah, it's a typo in the variable name. PR with a fix is welcome?. Thank you!. Thank you!. Thanks!. Test data is in Git submodule. Just run:\nsh\ngit submodule update --init --recursive. @gabrielmaldi parse5 can provide source code location for each node (if it's present in the original markup, there are cases when parser generates nodes implicitly): https://github.com/inikulin/parse5/blob/master/packages/parse5/docs/options/parser-options.md#optional-sourcecodelocationinfo. @evilebottnawi \n\nwe have original code and location and we can get original text in text node, but I would not like to do such hacks \n\nI fail to see how's that's a hack, considering that having a node location and a source html you are one step away from having raw representation that you demand:\njs\nconst raw = html.substring(node.sourceCodeLocation.startOffset, node.sourceCodeLocation.endOffset);. This is per specification. You can observe te same behavior in any browser. Just escape nested closing script tag in string <\\/script>. This library targets modern node.js versions and browsers. We encourage users to perform transpilation in their setup if they target ancient platforms that don't support ES6 syntax.. Thanks!. @RReverser I wonder how idiomatic it is to have async handlers for stream events? If that's a common a practice then I'm all for including support for such a functionality into package.. @RReverser  Maybe it's worth adding your code as receipt in the docs then?. As it's not idiomatic I would rather avoid adding support for that to the API. However, I'll be happy to add @RReverser's recipe to the docs.. Keep us posted, mate.. You can easily reproduce this with any HTML that have extremely long tag name, attribute name or value. We don't have any other choice except for buffering these values even in streaming parser. Although, we can abort parsing and raise error event in such a case, but it's not implemented at the moment. \nThe other cause could be a huge HTML file that expands into AST that doesn't fit in the memory (Node.js has quite small heap size by default).. Hello, thank you for the efforts, but unfortunately I can't accept this PR as any others issued previously that try to introduce any kinds of hooks that will allow people to use parse5 for what it hasn't been intended to be used (HTML parsing in serialization).\nThough, as a workaround, I can suggest to take a look at parse5 rewriting stream which may give more fine grained control over the output for this particular case.. Hi,\nI appreciate your efforts writing down all your thoughts regarding serialization API. However, unfortunately, I'm quite limited in time to go through such a long reads during my open source work. \nCan you, please, explain in 3 bullet points (one sentence per each):\n1. What's wrong with current API in your opinion\n2. Why it's wrong \n3. What you propose instead\nIn the meantime, I'm closing this issue. Will be happy to reopen once there will be a clear explanation of the problem.. It's not quite clear if this a suggestion, question or a bug report. \n\n\nIf it's a suggestion then I'll be happy to accept any PRs that improve testing of the SAX parser.\n\n\nIf it's a question about state of things with SAX parser testing then here is the explanation: SAXParser is actually a very thin layer of API on top of Tokenizer (which extensively tested with dozen of thousands html5lib tests) and ParserFeedbackSimulator (which is, thanks to @RReverser's efforts, tested by even bigger number of tests that are generated from html5lib tree construction tests). So, the only purpose of SAXParser tests is just to check that these things are glued up together nicely, it doesn't make much sense to test parsing precisely as it's covered by unit testing of its component.\n\n\nIf it's a bug report then, please, explain in details which issue in SAXParser you've found that were shadowed by the current tests.\n\n\nIn the meantime, if you wouldn't mind, I'll close this issue. Will be happy to reopen once there will be any information from the bullet points above. . Hi, sorry for the delay in response. I'll be happy to help with the fix, however I'm a bit confused by the report. Is it possible to open separate tickets for each case where location info calculation fail with expected/actual results specified. After that we can work through each case.. Sorry for this taking that long. I'm alright with adding this change and will be happy to merge a PR for it if you mind to create one.. I recall discussing similar topic with @domenic some time ago. This puts me in a bit complicated situation: even though from spec perspective it's correct to use such terms as localName, etc. in API, I have a concern that it might be extremely confusing for casual parse5 users who just want to parse their markup and got used to term name rather than localName. . npm counts 605 dependants currently. I'm quite convinced that the vast majority of them will be confused by localName and qualifiedName terms.. Yes, that could be an acceptable solution.. Hi,\nThere is no such thing as a self-closing tag in HTML. Even though HTML doesn't complain about XML-like self-closing slash on tags (like in case with <img/>) it doesn't make any effect on the way the markup is parsed. You could try to self-close <div/> and check results in a browser: self-closing slash will be ignored. In contrary, <img> element will be automatically closed no matter it has / or not.\nHTML has a concept of void elements. These are elements that can't have any nested content and, thus, end tag doesn't required for them. <img> tag is one of them.\nTo be absolutely fair, I've lied a bit that HTML doesn't have self-closing tags. HTML markup can contain integrated foreign language fragments (such as SVG inside <svg> tag). Since SVG is XML-based it can't actually contain self closing tags (so <div/> will indeed close the <div> tag). Though, self-closing tags are just syntactic sugar for <tag></tag> which doesn't carry any useful semantic information for DOM consumer, so this information is not included in the resulting AST.  . This is an npm issue and should be reported in their bug tracker: https://npm.community/c/bugs. No need for this, it's already implemeted: https://github.com/yyx990803/parse5/blob/master/lib/tree_adapters/default.js#L166\n. Maybe it makes sense to rename it to _handleToken now?\n. I wonder if we really should do this and new lines adjustment for <textarea> and stuff? I thought we should be close to the source code as much as possible, so maybe we should just ignore those tests? However, it's debatable. What do you think?\n. Can you add a comment that we expose it just for the testing purposes, please?\n. Maybe extract this condition into boolean var to get rid of this weird formatting and give it some context?\n. Yes, makes sense\n. >  consumers might want to perform own actions on token retrieval\nDo you have any scenario in mind?\n. Let's hide it then, if someone will be interested in this feature we can always bring it back.\n. initialName? Having a verb in the var name suggests that it's a function\n. shouldInitializeName?\n. > elements in stack that are not headers \nIf elements are not headers we will need a namespace anyway to perform scoping element test: https://github.com/inikulin/parse5/pull/146/files#diff-c84848d80bdda10576c7ee69b4b5f652R295\n. As far as I remember there is only one such case: for </br>. But it's can be easily solved by immediately poping element out of the stack. For all other cases we need insertion and I believe we shouldn't introduce additional condition for this.\n. Nah, I would like to keep node 10 for now, since parse5 is still used in some browsers without proper ES6 support, also node 10 is still widely used. I would like to make sure that we don't break anything by using ES6 syntax\n. > How about reuse HIBERNATION_TOKEN for this\nI'm not sure how it can be used for this purpose, since it's emitted once we don't have insufficient data to proceed with parsing. Can you elaborate, please?\n. @RReverser Do you know any eslint plugin that restricts ES6 features usage?\n. We will need to introduce special flag that will signal that we met hibernation token which we will need to check in initial states and then reset it. So we have a lot of duplicate code or we'll need to introduce additional method call (by refactoring flag check + preprocessor.dropParsedChunk + flag reset into separate method of Tokenizer). Also, with small chunks we may have opposite effect, since we'll produce a lot of intermediate strings. Overall, I'm not sure about this approach to be honest.\n. > Just trying to think of the cases where we will still have congestion inside of the streaming parser when chunks were already fed to it, but it didn't output the text node (because it was all text).\nWe will not have such issues with text content, but we will have such issue e.g. with extremely big attribute names/values. In that case our mechanism will not work and we'll buffer huge chunk of data.\n\nIs bufferWaterline supposed to be configurable via options btw?\n\nI'm not sure if we should. It doesn't pass basic assertion for public API: how you will describe it to users? Maximum amount of data buffered by parser? It's not true, because we don't buffer exactly this amount and also we have edge cases there we can potentially buffer much more than that limit. I believe we should keep it private, since I would like to leave a space for improvement maybe we'll drastically modify this mechanism to handle edge cases in the future.\n. I've removed node .10 from test matrix and switched to native Promise\n. I've got tired of this thing being buggy in some cases and finally disabled it in all my projects. Yes, we can increase file size due to unnecessary invisible symbols, but we definitely will not break anything with it.. On the other hand, maybe it's just me being too impulsive at the end of the hard day. I've not decided yet, tbh \ud83d\ude04 . For some reason mine ignores this .editorconfig rule completely \ud83d\ude22. Reverted: fabc6d8eaba7904ae667dd20741bed9ea1a39ca0 . Or even better chunk === void 0. BTW, I believe it's always safe to write empty string here, so I guess the best way will be to eliminate this.written variable and use the second code snippet suggested by Ingvar.. The adoption agency algorithm is only one of many cases (though the trickiest one) when the parser creates elements implictly.. It's a lot of cases. I believe we shouldn't list them all in documentation either. Just give an example (https://html.spec.whatwg.org/multipage/syntax.html#an-introduction-to-error-handling-and-strange-cases-in-the-parser has a good explanation for some such cases, you can point to it).. ",
    "cburgmer": "Hi thanks for clearing this up.\nAs far as I understand checked in fact needs to be converted to checked=\"checked\" amongst others, following http://www.w3.org/TR/xhtml1/dtds.html.\nSo I guess I need to take care of that on my part.\n. ",
    "Sebmaster": "The  tag is in the obsolete features in the w3c spec. Even in HTML4 it requries a specific DOCTYPE to be valid.\n~~The underlying question is, if parse5 wants to specifically be a html5 parser or parse all html.~~ Looks like it's still defined in the standard?\n. Sorry, parse5 parsed correctly, we had the  tag wrapped in a  in our tests. htmlparser2 seems to have ignored the body tag, while parse5 (correctly) ignores the frameset in this case.\n. Yes, thanks.\n. The parsing behaviour is in line with what Firefox and IE do, so it's a Chrome thing.\n. While I do agree that we should move that kind of thing into the parser as far as possible, this problem doesn't stem from jsdom.\nI'd expect the htmlparser2 tree-adapter to produce the same output format as htmlparser2 itself, however apparently this is not the case.\n. Oh well, thanks @fb55, had more time to look into it just now. Seems like we can drop our custom HTMLDecode function and use htmlparser2/parse5 for that. Thanks!\n. @inikulin I thought it'd be possible to use the latter possibility, but when some DOM subtree is emitted, check if it's a script tag, then call a writeHtml function given as a parameter to the event, which internally would prepend that chunk of html to an unprocessed buffer.. Will that not work with the current structure of parse5?\n. @inikulin That sounds pretty good.\nI'm not sure how that'd work with insertBefore though.\nI don't know the hoising rules of html at all, nor do I know the hoisting rules of document.write (yet), so please forgive the bad example and try imagine as a bit more valid example - I wanna get the edge case here.\nIf we have this html\nhtml\n<table>\n  <script>document.write('<b>');</script>\n  <tr>\n    [...]\n  </tr>\n</table>\nI'd expect the script to get hoisted before the table like this:\nhtml\n<script>document.write('<b>Bold</b>');</script>\n<table>\n  <tr>\n    [...]\n  </tr>\n</table>\nThe order of calls would be appendElement(table), and then a insertBefore(parent, table, script).\nThe insertBefore would notice the script and write some html code, which would need to be written right after the script tag like:\nhtml\n<script>document.write('<b>Bold</b>');</script><b>Bold</b>\n<table>\n  <tr>\n    [...]\n  </tr>\n</table>\nWill you generate another insertBefore(parent, table, b) here?\nWould this be how it works? Where is my example inconsistent/wrong?\nAlso @inikulin @aredridel, since I have both of you here, @domenic and me were thinking of having some kind of standard interface for html parsers in order to make all of them more interoperable and allow switching them easier, without jsdom or similiar having to switch case all parser interfaces. Can I count you two to be on-board with that?\nThere's not been any decision in any direction here, I just wanted to get some kind of participation and discussion going somewhere at least.\n. I think this will not work with tag fragments like\nhtml\n<script>\ndocument.write('<!--');\n</script>\nI'm not actually commented\n-->\n. @inikulin I think it'd certainly be worth a try. If you create a branch with it I'm certainly gonna try it out.\n. > Because now any script execution suspends the parser and it still in incomplete state while we process scripts and parse5 parser was not designed to be reentrant you need another instance of parser handle innerHtml assignments.\nThis is currently done anyways - we create a new parser instance for every innerHTML call. We'll probably have to test how this interacts with the main document if we have to manage global state, but there're a lot of tests in the web platform tests repository for document.write.\n\nHowever I don't have a clue how to make a nice API for this yet and I'm not sure if it actually should fit into parser functionality (maybe separate project, e.g. jsdom-parsing-platform?) .\n\nSince we probably want to keep network resource loading in jsdom land we need to get stuff like <script src=\"\">. If you still want to go with something like the appendElement API we could add a the writeHTML call and additionally a callback parameter which resumes the parsing (you could check if the given appendElement function.length expects this additional parameter and if not resume parsing after emitting the event).\n\nIn context of jsdom seems like it ruins the idea of the unified parser interface\n\nThis takes a back seat for now. I'd appreciate it, if we just had a default config, which approximates all exisiting standards as closely as possible - we're going to try support all parsers somewhat, but ultimately it's up to the users to use custom parsers correctly.\n\nI don't know how it will be possible to perform steps above for XML\n\ndocument.write is illegal in XmlDocuments so this won't be a problem.\nI hope I haven't missed anything important.\n. I'm currently trying to switch jsdom over to the tree-adapter \"streaming\" model, in order to provide a correct dom/execution environment for the running script and it's going somewhat okay-ish. There're failing tests, but as far as I checked those were because we have bugs in our DOM manipulation methods and those should be fixable in a reasonable amount of time.\nI think there are some good points in your interface, however I think it might not be doable like this (since I switched to the adapter model). What I'd prefer is something like this:\nAdd pause/resume/writeHtml to the parser instance. I can then give the parser instance to the tree adapter.\nIf you then emit the appendChild into the tree adapter I can check if it's a script myself and if necessary pause the parser, load the script and then resume. I want keep the script checking in jsdom land because there might be other cases where we have to pause the parser (link rel=\"import\" maybe?) and to have some kind of unified interface for that would be best.\n. Ah, yeah I just checked, I guess you're right. Although I do think I have to do the tree-adapter as it currently stands (with having document in scope) since we have to have a finished DOM-tree (i.e. building all elements with document.createElement) by the time the script executes and can't rebuild the DOM from the virtual structure parse5/htmlparser2 uses.\nI'm not sure if we can pull off all edge-cases with that design though but I guess we'll see with time how it holds up.\n. I wanted to go with the first option abandoning every intermediate representation, yeah.\nI'm still working on a few failing tests and I think a lot of it has to do with the fact that the elements are created without a text body and the body is added afterwards via the insertText methods - I think currently jsdom does not refresh the body (changing scripts/css rules mainly). I need to check into that first.\n. Yeah sure.\n. Sadly, not yet. We'll need a bit of a larger rewrite in our initialization cycle which will break back-compat (see https://github.com/tmpvar/jsdom/pull/923), since otherwise the integration would be quite hacky, if not impossible (cleanly). I'm also currently very short on time until February, so that's a problem as well.\n. @inikulin Do you have the script for the generation of the trie still lying around/could you put it in the repo? I was thinking of playing with the structure to try and reduce the memory requirement somewhat.\n. I was thinking of trying to either move it to an array-only structure or try a trie compacting algorithm (although that'd slow down lookups a tiny bit probably).\n. Sounds good. When you emit the script event, can we pause the stream then call documentWrite sometime later and then resume the stream again?\nI'm not sure we can pause the input stream (since we don't know where the start-tag ends).\n. > parser encounters ",
    "domenic": "We don't go by W3C specs round these parts. Besides, authoring conformance criteria (\"obsolete features\") are very different from normative parser algorithm requirements.\n. Yeah, <isindex> is really old and the HTML Standard parsing algorithm has a hack that turns it into a form with a bunch of stuff. Looks like parse5 implements that. Chrome recently removed <isindex> support, and if the other browsers follow suit, then it will be removed from the standard, which would be cool. But until then, parse5 is fine.\n. The more of that kind of thing we can move into the parser, the better. I am not exactly sure how to do that, but we'll look into it...\n. HTML IS NOT XML!!!\nparse5's virtue is that it is an HTML parser. If you want an XML parser, download one of the many on npm!\n. Heh, @Sebmaster said all the things I wanted to say :). Keep the code nicely-structured and decoupled as much as possible, but don't worry about the unified parser interface. If you guys want to use this as an opportunity to work out a standard parser interface (perhaps restricted to HTML) that parse5 and html5 and any others can share, go for it, but that's of course not necessary.\n. jsdom 1.0.0 is released!! :D\nI would like this especially for people who want to parse arbitrary fragments, so we can tell them \"put it in a <template> then use .content.\" Especially because jsdom's previous html parser used to not insert <html>, <body>, etc. so people got a false impression it was good at parsing fragments.\nAlso I think it'd be cool to close gaps with the HTML standard so parse5 can maintain its claim to fame as the most compliant parser on npm ;)\nSo, medium priority, but not urgent?\n. Sweeeet! :D We should add a note to jsdom README on how to use this to do fragment parsing.\n. @inikulin I'll defer to @Sebmaster but I think we just want JsDomParser to stay around, i.e. for streaming to primarily be via document.write calls.\n. It would be better to sync with the official html5lib repo, as in some cases Blink might not be fully compliant. @gsnedders is currently looking in to Blink divergences.\n. I think that's how browsers work; you should check.\n. Hmm so to be clear I found this adapter code in jsdom:\nhttps://github.com/tmpvar/jsdom/blob/0da29317ba7893d9d9c44edc5847a981ff7096b5/lib/jsdom/browser/documentAdapter.js#L20-L21\nIt sounds like I didn't make quite the right modification. What would be the right modification?\n- Wait for parse5 to fix itself to not pass non-element nodes, then go back to tagName?\n- Check for element.tagName === undefined and if so return undefined instead of lowercasing?\n- Something else?\n. Just compare tagName to https://www.npmjs.com/package/void-elements\n. O(1.01m) = O(m)\n. I have no complaints, but one of our users does, so if that user (or someone else) wants to do a PR that helps, and you're willing to review it, it seems like it would be a nice thing to do.\n. I might be wrong but the great innovation of HTML5 is that there are no parsing errors; everything can be parsed.\n. Alternative: use the subset of ES6 supported by io.js (soon to be node.js v4). Then: no build!\n. It works only on modern browsers.\n. Basically, what you want is not a HTML Standard compliant parser, but instead for a different language. That kind of work is better off as a fork than baked into parse5.\n. @stevenvachon right, and I'm guessing @inikulin doesn't want to maintain a not-HTML parser :P\n. Haha yeah, fair enough. I agree with your concerns and think an umbrella package is indeed probably better.\nI also agree that XML mode is not very important (although we do get a surprising number of issues on it). We should probably switch to sax-js as mentioned in tmpvar/jsdom#1276, and work on that kind of umbrella package.\nI guess the thing to do here is wait for jsdom to upgrade to parse5 2.0. IIRC using parse5.SAXParser for better streaming and <script> support was the plan already. Once that's done, we can work on a parse5-lookalike wrapper on top of sax-js, and it'll take care of things hopefully.\nThanks!\n. > ...or, I can implement setTemplateContent (tmpl, contentFragment)\nI think this is the thing that makes the most sense. (Once we fix our other issues by using the parse5 tree adapter instead of the htmlparser2 one.) For parsing, the adapter needs a tree construction method that is aware of the fact that \"appropriate place for inserting a node\" sometimes requires something more complicated than appendChild(). Just like how for serialization, it needs a tree reading method that is aware that sometimes the children to serialize are more complicated than just getChildNodes().\nBTW I noticed that adapter.appendChild is not documented in https://github.com/inikulin/parse5/blob/master/docs/05_api_reference.md#TreeAdapter. I guess that is still in progress :).\n. Not sure but I think these docs are auto-generated so you need to edit the .js files instead.\n. In general the point of a highWaterMark is to prevent more than that amount of memory from being consumed. So as long as the workaround sticks to that constraint, it is in line with the spirit of streams.\n. I think filling in each release on GitHub with a link to the corresponding version history heading would be good. Project watchers also get an email each time you create a GitHub release so it's a good notification channel.\n. Hmm. I can no longer reproduce my result from last night where I could swear I got Firefox to find the tr. Weird! Thanks for setting me straight.\n. Fun fact: there is actually no spec for parsing XML into DOM structures, only for parsing it into something called an \"infoset\".\n. Yeah, I don't think this is really accurate based on reading the XML spec:\n\nit refers to XML spec which has set of rules to convert byte stream into nodes\n\nThere's no actual parser algorithm, just some kind of vague correspondence. The word \"node\" appears exactly once in the document, BTW.\n. Seems bad... very un-parse5 like\n. I guess maybe the serialization algorithm is what needs fixing for the content-modifying proxy scenario?\n. Well, a HTML parser is something that follows the HTML Standard and produces a DOM tree. I guess you might be looking for something like a HTML lexer or tokenizer, although I don't know what kind of object that would produce (not a DOM), and there's no standard governing its behavior. That's why it's fairly un-parse5-like to attempt to add such features to parse5, which is a HTML parser library.\n. Yep, this is how browsers work (and thus how parse5 works). That is why the Angular team had to invent their entire own \"HTML\" parser.\n. Woohoo, should be fun!!\n. Here is an example showing how this works in browsers: http://jsbin.com/weseduhewo/edit?html,console (not exactly the same due to needing to insert a script tag and browsers not providing a means of serializing the doctype, but it should give the idea)\n. It would be more interesting to find the bug in the code that was passing this a non-string. Parse5 seems correct.\n. There is type checking! It throws a TypeError if you pass the wrong type :)\n. Uh... you seem to be assuming all strings are C strings. Which are actually a sequence of bytes, terminated with the null (0x0) byte.\nThis is irrelevant and wrong in so many ways:\n\nThis is not a computer science principle, it is a C programming language principle. (And one widely regarded as a mistake; see e.g. this StackOverflow review.)\nC strings are not JavaScript strings\nJavaScript's null type is not a byte (or a string). It's a separate type. It has nothing to do with 0x0, which in JavaScript is more properly represented by a number, like let nullByte = 0.\n\n\"Computer science 101\" indeed, lol.. 0x0 === 0, but 0x0 !== null.. No, no rush, I was just reminded in https://github.com/whatwg/html/issues/2730#issuecomment-311779740 that we never filed a parse5 bug when removing it, and doing so is a good idea.. +1, it would be nice to omit this in jsdom.. I continue to think it's supposed to work with people who use these type-related technologies installing them themselves, separately, and leaving them out of the package that the rest of us consume.. I am happy that jsdom consumers won't get these typing dependencies installed by default. I'd be even happier if they got no types at all, just JavaScript, but I guess that would require using DefinitelyTyped, which for some reason some people believe to be \"legacy\".. See the link for \"the style element\" in the OP, where the bullet points I quoted are found.\nBased on https://github.com/jsdom/jsdom/pull/2132, an API that would work for our purposes would be two adapter functions, each taking an element, which are called when the element is pushed and when it is popped, respectively.. FWIW the serialization algorithm that should be followed is https://html.spec.whatwg.org/multipage/parsing.html#serialising-html-fragments . (See especially https://html.spec.whatwg.org/multipage/parsing.html#escapingString .) Are you using parse5 to produce the bad serialization seen in the OP, or some other tool?. Sometimes it seems like people don't really understand what a parser is.\nA parser converts raw text into a structured, canonical representation. This is inherently a lossy transformation, by nature of what canonicalization means. There are multiple inputs that all map to the same parsed representation.\nFor a JavaScript example, function(a){} parses the same as function    ( a) { } which parses the same as function (\\u{000061}) {    }. All produce a function expression with a parameter named \"a\" and an empty body. This is usually represented in a data structure known as an AST.\nFor a HTML example, <p>I will display &euro; parses the same as <p>I will display &#8364;</p> which parses the same as <P    >I will display &#x20AC;</    p>. All produce a paragraph with the contents \"I will display \u20ac\". This is usually represented in a data structure known as a DOM.\nThe whole point of a parser is to get rid of these differences in input string format, and produce a single canonical representation of the actual structure that you'll operate on.\nIf you're interested in the original raw string, you should not be parsing it in the first place. You should be operating on the string.. I'm not sure how many \"casual parse5 users\" there are :). Probably folks should be able to understand the DOM tree concepts.... Maybe you could add the proper terms for folks who know what they refer to, and add alias getters for folks who don't want to read the docs/DOM standard? Right now, for those of us who are aware of the difference between tagName, localName, and qualifiedName, the usage of name and tagName are confusing.. ",
    "fb55": "jsdom's entity decoding isn't entirely spec compliant & should be replaced with the parser-provided alternatives. htmlparser2 has a decodeEntities option, which results in the same behavior.\n. You can port css-select (which is used by cheerio & whacko) quite easily by swapping out just a few accessor functions. There is already a browser_functions.js file that can be used as a replacement of the domutils, which should do the trick, it isn't up to date, though. (At least domutils.removeSubsets is  used internally & not part of the file, which you would need to port \u2013 PRs are appreciated :) ).\n. @inikulin Awesome progress. We should probably revisit the topic of merging whacko with cheerio at some point :)\n. ",
    "eGavr": "Can you, please,  give a real example, when parse5 may produce browser-inconsistent tree structure?\n. Maybe you could make the special option for this case? For example,  ignoreDuplicateAttrs\nOhhh! Using of my own fork... Bad idea... :(\nAdding of option! Good idea, doesn't it?\nOn the one hand, you will not break the HTML spec and on the other hand, because of this option, parser will become more... more fair!\n. All in all! I think that it is necessary to add my propose without option!\nWhat is the main idea of any parser? To parse the input as it is, not for browser or for something else...\nIf the input is invalid (as in my case) - it should be parsed too! \nWhat about Breaking Noah's Ark clause may cause breaking of Adoption Agency algorithm which should deal with misnested content.? As you've said, my propose can lead to collapse, I answer this way:\nIf the user wants to parse the invalid input - OK! And parse5 have to give him fair parse of his input! Now it is unfair, because parse5 corrects the input, but it should only parse it!\n. I am using it now! It is a really good parser, I know!\nBut, in htmlparser2, these problem, I think, can't be solved whithout breaking the API, because of the structure of the DOM which it uses! Tags' attributes are presented as an object in the final DOM, so there are can't be duplicated attributes! But in your parser, tags' attributes are presented as array of objects, so it is possible to solve your problem in your parser .\n. Thank you very much! You will help me with that a lot!\nPlease, inform me when you do this!\n. Now! there is no need for me to write my own parser! ^_^\n. Thank you! It works )\n. GREAT!\nAnd probably you could add to your SAX-style parser the handling of duplicate attrs? (closed issue #11 )\nFor example,  it can be important for parsing not only HTML, but XML too.\nThis will give the opportunity to create your own parsing tree. Could you? only in Sax-style parser... \n. I think that the piece of code \nhtml\n<span class=\"copyright link\">\nshould be presented in DOM as it is presented now by parse5, but\nCopyright content</spane>\nshould be presented as a text\nNow as a text is presented only this part\nhtml\nCopyright content\n. It would be cool if you handle thus situation and your SAX-parser will become an XML parser too and not one of the many on npm, but probably the best and most convenient!\nBut, of course, if you are not going to make your parse handle it - close!\nBut, I suppose, it needn't lots of time to fix this situation and make a patch as you did with duplicate attributes)\n. :+1: \n. Thanks for the quick response!\n. As I remember browsers with enabled javascript just skip the content of tag nonscript, but not consider it as text.\nWhy do not parse a content as HTML in case of HTML by default? :)\nI mean, that the parser could detect whether a content is HTML or not and parse it in the first case, and do not parse in the second one.\n. ",
    "jackhftang": "Thanks for your helpful reply. Here is my little story. Originally, I was looking for some browser automation for my own project. I experienced a number of options, but was not satisfied. Selenium, no idea why it is so slow. Phantomjs and Slmerjs, poor error message, can only import pure javascript. And then, I go for Zombiejs. I like its approach, very flexible, but fail to be browser-like enough for my use case. \nI tried to modify zombiejs for my own use. I read through the code, then I know that the DOM implementation is based on domjs and Zombiejs add some extra web api. And then I go for domjs.  Its source code, IMO, is not so readable, no comment, patches are everywhere and variables' name are not intuitive to me. However, the separation of source code of DOM is good enough that I can reuse them. And then I think, the codebase of Zombiejs is not that large and I can borrow them and in theory a html parser + contextify + dom will be enough to be a small browser. I may be able to build one. And then I search through github and find html parsers.\nAfter reading README and issue of a number of html parsers on github, I have a feeling that parser5 is better crafted. But it is unclear to me how to prepend strings to parser. That is why I opened this issue. \nAnyway, I found another solution to my problem, That is writing a chrome extension communicating with node server via socket.io. This is a efficient (in term of running speed) and true-browser automation solution I found in my exploration. \nCurrently, I have dropped the idea of build another headless browser. =]\n. ",
    "slang800": "Thanks! Haha, never realized that windows did that, but now that travis is added, we shouldn't hit any problems like that in the future :D\nOh, and I'd advise against using 4 spaces in the package.json because npm will overwrite it every time it makes a change. For example, npm install <package> --save will change it from 4 spaces to 2 when adding the package. Also, npm version <increase> will change it and commit those changes... It's very opinionated in this regard and forces you to make your package.json the same as everyone else.\nAnd one more thing that I forgot when making this PR: you can specify the discrepancy between the rest of your code-base and the package.json by adding this to the end of your .editorconfig:\neditorconfig\n[{package.json,.travis.yml}]\nindent_size = 2\n. ",
    "jonathanong": "@inikulin ping. this repo is currently broken! it needs CI testing!\n. i realized that the text itself probably shouldn't even be a child node but instead the value or the script node or something, but that's how its getting parsed. i'm just confused right now haha\n. weird issue and solution, but thanks for the quick response!\n. Really? Fuck\n. then why is syntax highlighting correct?\n</argument>\n. ",
    "aredridel": "I just had the same request on the html5 module. I said the same thing.\n. I did some streaming parsing in the html5 module, and that's exactly what it took -- storing the state between parse chunks and it was kinda tricky.\n(I did a trick of pushing back everything that wasn't a complete state when a chunk ends, so I pretty much just stored the state I was in and the buffer of not-fully-processed material.)\n. @inikulin Yeah. html5 performance is not NEARLY as good as yours -- I don't know if that's the reason, but it's still a fact ;-)\n. Also having a DOM to start working on before all the data has arrived.\n. Actually extending streams is awesome. Then you can pipe into it and it Just Works. stream-like but not actually streams give you such fun edge cases.\n. unfinished AST is definitely useful, but could be given with a stern warning.\n. Yup. The canonical 'fix' is to emit <\\/script> inside a <script> tag to hide it.\n. Indeed -- the spec lists a whole list of validations. It's not actually that hard to put them in -- I started to in the html5 package. They are known points in the parsing where if you apply a fixup, you emit a warning. Just add location info and that's about it. Tracking the location info within the parser is the harder part.\n. This is super exciting! That's one of the few things html5 does that this didn't. \n. ",
    "azakus": "+1, this is hitting a few people using vulcanize with <table> and <select>\n. ",
    "jeromew": "the Tokenizer and Preprocessor take the html content as one big chunk. Have you considered the possibility of working on partial chunks that would be written one after the other ?\nparser.write('<div>')\nparser.write('hello')\nparser.write('</div>')\nparser.end();\n. Ok I see thanks for your feedback & for the link to the other thread. I agree that there is a big question mark on the result of all this performance wise.\nI'll start my research with understanding the caveats of the html5 spec and try to understand how the approach taken by html-tokenize is compatible or not with all this.\n. +1 for extending streams instead of home-cooked streams\nI don't know if it is possible but having a streaming solution inspired on html-tokenize that would push data as soon as is it guaranteed to be \"correctly\" understood would be nice.\nI already tried to do such a thing but was stopped in my progress. The code is just horrible and I was blocked because it needs a rewrite of html-tokenize to be more html5 conformant - https://github.com/jeromew/html-nest\nthere definitely is a problem with the html attributes because the spec says that additional html elements should push their attributes on the first one...this is not stream friendly at all ! Other than that I felt that in most scenario, we should not have to withhold tokens for too long in memory.\nthis is what I found during this experiment\n- There will always be an html element in the tree. The specification states that if an html opening tags is found in the 'in body' insertion mode, its attributes should extend the attributes of the first html element. Doing this would basically buffer the whole document in memory. A way to mitigate this could be to send provisional html tags\n- The table 'foster parenting' algorithm states that if we find elements inside a table that have nothing to do in the table, they should be reparented just before the table. In order to do this, we have to buffer the tables\n- The misnested tags are rectified by the 'adjacency adoption algorithm'. This algorithm tracks some formating elements (b, i, ..) and re-organizes locally the elements when a misnesting is detected. Sometimes the re-organization is triggered after tokens have already been processed. In order to follow this algorithm, we have to buffer tokens during formatting sections.\n. ",
    "stevenvachon": "~~Streaming would be useful in parsing HTML that contains template logic such as handlebars/etc.~~\n. @inikulin Having worked on my idea further, I realize that I was mistaken. ~~Instead, #33 makes more sense.~~\n. The biggest benefit to streaming is in memory usage and garbage collection, not parse performance. Not having to parse an entire html file in a list of thousands has great benefits.\n. Any progress on this at all?\n. ~~Unfinished AST is ideal for me, but what kind of malformed tree could we end up with? Why not delay certain elements until more information is available? An example stream could be:~~\nhtml\n<p>asdfasdfasdf\nfollowed by\nhtml\nasdfasasdf<p>asdfasdfasdf</p>\n~~The parser wouldn't yet know if the first <p> is a void element or not. Instead of emitting it, the parser could hang onto it until it has access to future closing tags (or some kind of end()).~~\nActually... that creates a problem, doesn't it, because <html> wouldn't be emitted until the whole thing is done.\n. Regarding @aredridel's mention of a \"stern warning\" -- also include technical reasons as to why there is a warning. It would do two things: 1, thwart incorrect use; 2, enlighten us on what issues to avoid.\n. I don't see why we would need non-streaming anymore. htmlparser2 is stream-only with pseudo-non-streaming via a single write() call.\nTotally awesome on the ForgivenParser! Though, perhaps rename it to ForgivingParser?\n. Totally looking forward to this. Will <{{tag}}> and <tag {{attr}}> be possible?\n. @inikulin what is forgiving like htmlparser2, then?\n. lol, ok :cry: \n. Line/column position for start/end would be awesome as well.\n. I'm not sure what you mean by \"different grammar\".\nHow about adding some flag to allow it, or perhaps going as far as making the parser customizable like skunks?\n. Ok, but what if that grammar were customizable? This brings us back to skunks.\n. Would it be silly to make parse5 both forgiving and unforgiving?\n. Ok, thank you.\nMy reasoning would be for parsing spec-compatible HTML (nested comments, checked=\"checked\", etc) but with non-spec additions. htmlparser2 is a good \"forgiving\" parser, but it has trouble in some areas. Handlebars is a good example of the need for both forgiving and spec-compatible, without having to venture into a completely custom parser.\n. @nylen I'd switched projects and haven't yet gotten around to solving this one.. K. Can we get a voidElement argument, then?\n. Ok, thanks.\n. Revisiting this. How will this library handle void elements when it supports streaming (#26)? I'd imagine that the events would be delayed until a parent element is closed. Anyhow, such a scenario could warrant a voidElement argument.\n. Definitely. If the errors encountered by parse5 were emitted/etc, that'd cover the parsing errors. Further errors such as incorrect attributes and values can be done in a validator library.\n43 will likely be necessary too.\nFor the record, I have no immediate goals to write an html validator, but am not completely against it when my other projects are complete.\n. Parsing and validating are different. For example, common Angular use of ng-something is invalid, but it does work. Technically correct use of Angular directives would be data-ng-something (which does also work).\n. Streaming +1\n. Does this mean that parsing errors are not in scope either?\n. Can't it have its own error codes? A wrapping library can translate them.\n. Precisely why I said that we could use our own codes. No need to maintain them as they'd be unique to any standard. A wrapping library (not parse5) could translate them to any standard.\n. Aye. Well, this situation sucks.\n. +1\n. +1\n. :-1: \n. Maybe a regex for such instead.\n. @domenic a fork would quickly become outdated. I do not wish to maintain an HTML parser.\n@inikulin I never said replace the tokenizer with a regexp, but simply add it as an optional step in the algorithm.\n. @domenic I know, but my thinking was that by making the non-standard tools optional, it wouldn't be a not-HTML parser.\n. Actually, I came up with another solution which will allow me to use parse5 with Handlebars.\n. This becomes necessary again if we want to parse angular2 (or similar) templates:\n``` html\n<button\n  (event)=\"value\"\n  [property.property]=\"value\"\n  [(twoWayBinding)]=\"value\"\n\n```\n\nI understand that angular2 templates also offer \"bind-\" prefixed attributes (eg. <button bind-click=\"value\">), but that's beside the point.\n. Obvious to whom? Not to me.\nCorrections made.\n. Dude, we don't all write and maintain spec-compliant HTML parsers. If we did, you wouldn't need to write one.\nThe browser decodes and encodes entities within attributes. It probably does so for text content as well so that it can render, but web inspectors re-encode them for easy reading.\nWith this new knowledge, I agree that decodeHtmlEntities should probably be removed. A note regarding manual re-encoding would be helpful to those looking to skip decoding.\n. I had thought it useful for whitespace normalization, where entities in the original text would not contribute to the concept of \"multiple spaces\".\n. @alanclarke but that should not be a global flag, but perhaps a feature specifically hardcoded to url attributes.\n. @alanclarke seems to me that resolving (or not resolving) the entities will produce a consistently valid url in the output tree. It is always possible to safely re-enode them if you want to produce valid xhtml.\n. @alanclarke wouldn't that be the responsibility of the application using the parser? The DOM (or jsdom) decides when to re-encode.\n. Ok, so the browser parses this way as well, it would seem.\n. http://stackoverflow.com/questions/3558119/are-self-closing-tags-valid-in-html5\nFor anyone else encountering this.\n. I think a similar thing is happening with non-javascript type script elements:\njs\nparse5.parseFragment('<script type=\"text/foo\"><tag attr/></script>');\nThe contents should be a string, correct? v1.5 did this. v2.0 is parsing <tag> as HTML.\n. @inikulin ^^\n. With this, is it possible to recreate functionality from v1.5 where we fed a string into the SAX parser?:\n. Ah, thanks. There is no documentation for parse5.SAXParser.prototype.end, by the way.\n. Oh yeah, stupid request; the values would need to be updated whenever the tree is mutated.\n. Hmm, but then so would parentNode, which is provided.\n. :+1:\n. Awesome! Thanks!!\n. :+1: \n. Ah, yes. Thank you.\n. Oh, hmm, I thought they were supposed to be parsed as a string. Maybe it's because I've lost sleep. Let's just say that I was wrong. Thank you.\n. @inikulin will SAXParser be on par with the regular parser in terms of \"back-checking\" DOM corrections?\n``` html\n\n\n\n\n\n```\n. > This would help prohibit [non-conforming] HTML in source code.\nAny such HTML pushed by designers would be rejected in the build step.. Hmm, \"never called\"? Am I supposed to wait for an event before sending the stream to parse5?\n```js\nconst parser = new parse5.ParserStream(options)\n  .once(\"finish\", () => resolve(parser.document));\ninput.pipe(parser);\n```\nIf nothing was received, why was \"finished\" emitted? I'm confused.. It might not be a bug. But you still may want to take some sort of action on this. I was accidentally using bhttp's discardResponse option for non-HEAD requests; which pipes to /dev/null. Do you still want to handle this with an empty string for consistent output, or is this a situation where the developer fucked up and deserves their fate?. I have tests with empty string input that correctly produce ASTs with html/head/body elements.. That's different than an empty string, though.. I just noticed that my project's empty string tests were using parse5.parse() and not ParserStream. Testing ParserStream with an empty file or a file containing only whitespace results in the same code as in the initial comment above.\nAlso, which file do you want the test in?. The issue solved by this PR should've already been tested (and caught ages ago) by https://github.com/inikulin/parse5/blob/master/test/data/tree_construction/tests1.dat#L839-L846\nCommitting my newly written tests seems redundant.. @inikulin I'm asking you, since it's your library.. It seems to assert the error instead of the document. Is this a bug in the test utils?. If we don't need a test, how are we to ensure that this fix is not undone in the future?\nThe issue also occurs with empty read streams that aren't piped to /dev/null, such as an empty file.. The confusion was \"this particular case\", where \"case\" sounded like \"this PR\". And my questions were asked to save me time, since you're familiar with the code.\nThis PR should be ready to go now.. No problem. Cheers.. Thank you. Perhaps we should mention this algorithm in the documentation as an example of when __location will be undefined. Also, the documentation describes its value as \"null\", which isn't strictly accurate.. The lint doesn't allow undefined. Could '0' be an issue if we use truthy?. If it's not too much to ask, could you list them all here? I'll try to summarize them tersely.. Done.. ",
    "angelozerr": "In my case I'm using https://github.com/isaacs/sax-js and it work's great. The feature with sax-js that I needed where: \n- speed\n- parse an HTML even if it's not valid (a XML element which is not closed for instance)\n- retrieve location of each elements, attributes\n- works with web browser (without using browserify)\nAnd sax-js provides me those features (for speed, I have not benchmark).\nI tell me if it's a good idea for parse5 to spend time to implement a new sax parser although sax-js works great. \nBut perhaps I have missed something?\n. > @angelozerr sax-js is not the HTML parser.\nYou could implement sax-js callback to create DOM node and having an HTML Document, no?\n. @inikulin I'm very interested too with this feature, because I would like to use SimpleApiParser to retrieve attributes id of an HTML DOM with location.\nIndeed, I would like to write a tern plugin http://ternjs.net/doc/demo.html to provide completion, navigation for attributes id for document.getElementById inside a JS editor : \n- test.html : \nhtml\n<html>\n  <script src=\"test.js\" />\n  <div id=\"MyId\" ></div>\n</html>\n- test.js : \njavascript\nvar elt = document.getElementById(' // Here Ctrl+Space shows MyId\nTo manage this case, I will load test.html with parse5 to retrieve attributes id.\nI need location to support navigation to the HTML file : \njavascript\nvar elt = document.getElementById('MyId' // Here Ctrl+click on MyId open the test.html and select MyId.\nSo I need location for this case.\nHope you will have time to support this issue. Thanks!\n. > I will try to get my hands on this as soon as possible.\nCool!\n\ndo you need just token's (and appropriate element's in case of DOM-tree parsing) start index or end index is also required?\n\nIn my case, I need start/end index for attribute value : \nhtml\n<div id=\"MyId\" >\n              |     |\n         start   end\nSo the token could have this structure : \njson\n{\n \"type\": \"START_TAG_TOKEN\",\n \"tagName\": \"div\",\n \"selfClosing\": false,\n \"attrs\": [\n  {\n   \"name\": {\n    \"content\": \"id\",\n    \"start\": 5,\n    \"end\": 7\n   },\n   \"value\": {\n    \"content\": \"MyId\",\n    \"start\": 9,\n    \"end\": 13\n   }\n  }\n ],\n \"start\": 0,\n \"end\": 16\n}\n. In my case I need position and not line/column. So I think it should be cool if we could customize that with a boolean option lineCharPositions\n. > @angelozerr I haven't implemented location info for the attributes in this release\nThanks for this info. I will wait for this features in a future release, if you can implement it.\n. > Hi, I can't see any issue using it in the browser. \nIn my case I would like to use to support completion for attributes id inside web browser editor with tern http://ternjs.net/doc/demo.html (and Sublime, Emacs, Vim, Eclipse which works with node) :  \njavascript\n<html>\n  <body>\n    <div id=\"MyId\" ></div> \n  </body>\n  <script>\n  document.getElementById(' // here Ctrl+Space shows MyId\n  </script>\n</html>\n\nHowever, you will need to browserify it first. See also: #34\n\nThank's for the info!\n. ",
    "sideshowbarker": "I seriously question whether it\u2019s actually possible in practice to implement a fully spec-conformant streaming parser without it needing to also buffer the entire document. It seems that no matter what partial-buffering strategy you try, you can often end up needing to buffer such a large part of the document anyway that you don\u2019t gain much versus just buffering the whole document to begin with.\n\nThe table 'foster parenting' algorithm states that if we find elements inside a table that have nothing to do in the table, they should be reparented just before the table. In order to do this, we have to buffer the tables.\n\nYeah, you always must buffer all tables entirely. And if consider how many documents out there are largely made of tables (e.g., pages that use tables for layout), in a lot of cases you\u2019re going to need to end up buffering the majority of the document anyway.\n\nThe specification states that if an html opening tags is found in the 'in body' insertion mode, its attributes should extend the attributes of the first html element. Doing this would basically buffer the whole document in memory.\n\nYeah. I don\u2019t understand how it\u2019s possible to get around that without requiring all the consumers of your streaming parser to implement some kind of special handling for html elements.\n\nA way to mitigate this could be to send provisional html tags\n\nI don\u2019t think I understand clearly what you mean by that.\n. Pinging @gsnedders (one of the html5lib devs) who might have some additional thoughts on this (though I doubt he can make time right now to respond here, and anyway my comments in https://github.com/inikulin/parse5/issues/26#issuecomment-121113301 reflect a discussion I just now had with him on #whatwg IRC ).\n. @inikulin thanks\u2014looking back at the comments in this issue I see now I misunderstood what it\u2019s about\u2026\n. > @sideshowbarker Ah, I see there this discussion started: servo/html5ever#149\nYup\n\nMy conclusion is that you can't have full spec compliant parsing without buffering already produced DOM-tree.\n\nYeah, if you're going to conform to, e.g., the adoption-agency requirements and foster-parenting requirements and the special case of the html start tag, I guess you pretty much can't do that without building some kind of full tree. It may not actually be a full \"DOM\" as such, but I guess it's going to end up being effectively the same regardless of what you call it.\nE.g., the code in https://github.com/validator/htmlparser/tree/master/src/nu/validator/saxtree provides an event-based SAX API by building something that code actually calls a \"SAX tree\"...\n. > But I still can't figure out use cases for such approach. Using SAX parser most likely you will not need information about position of the element in the DOM-tree (It becomes even more absurd if you don't have DOM-tree).\nYeah, I\u2019ve never used that API. Instead I have used the fully-streaming SAX API that code also provides. That streaming API builds no tree and produces a fatal error for any markup cases that would require the adoption-agency algorithm or foster-parenting algorithm. That streaming API is actually what validator.nu and the W3C Nu Html Checker use. Those also report all the parse errors to the end user, so doing the streaming-but-stop-with-error-message-for-fatal-parse-errors thing makes sense in that context.\n. To put this in terms of the actual language in the HTML spec, I take it this relates to what @aredridel mentions in https://github.com/inikulin/parse5/issues/55#issuecomment-119595297 \u2014that is, the fact that the HTML spec defines the concept of a parse error https://html.spec.whatwg.org/multipage/syntax.html#parse-error but the reporting of parse errors is an optional feature that an implementation of the parsing algorithm can choose to support or not, and in the case that it does do reporting of parse errors, it becomes an error-reporting parser.\nA couple of error-reporting parsers whose error-reporting code would probably be worth taking at look at:\n- @hsivonen\u2019s HTML parser https://github.com/validator/htmlparser Its error reporting is used when you do View source in Firefox (if document has parse errors, they show up in red in that view, and if you mouse over anything in red, the corresponding message for the parse error is shown).\n- The Servo HTML parser, html5ever https://github.com/servo/html5ever\n. > I might be wrong but the great innovation of HTML5 is that there are no parsing errors; everything can be parsed.\nI guess what you mean is there are no fatal parsing errors. I think it\u2019s kind of unfortunate that the spec uses the term parse error to define something that\u2019s neither a fatal error nor even an error that implementations are required to do anything with. I wish it had instead been named parse exception or parse warning or something instead. I think that would align better with what it is, conceptually. \n. > The only issue is that there are no error message texts provided anywhere in the spec (and sometimes it's quite hard to create one). If someone from the native speakers will help me with that then it will be awesome.\nI\u2019d think that mostly you could just re-use the error text from @hsivonen\u2019s parser source. See the text in:\n- err(\u2026) calls in https://github.com/validator/htmlparser/blob/master/src/nu/validator/htmlparser/impl/ErrorReportingTokenizer.java\n- errNoCheck(\u2026) calls in https://github.com/validator/htmlparser/blob/master/src/nu/validator/htmlparser/impl/TreeBuilder.java\nBut beyond that if/when you want more help with language of the error messages, I\u2019m very glad to help.\n. ",
    "gsnedders": "Note some of the Blink changes are wrong per spec, and aren't gonna get merged. I'd stick with what's on master.\n. ",
    "cvrebert": "Great! :smile: \n. I only need the start index for my use case. And I don't need any attribute-level info.\n. I don't care what you name it, so long as it doesn't overlap with any existing browser DOM property.\n. Technically Bootlint responds with that, not Parse5.\nHere's a more detailed description of the Parse5/Whacko-level issue: https://github.com/inikulin/whacko/issues/13\n. ",
    "pdpi": "In particular, the following minimal example gives surprising results:\n```\nvar p5 = require('parse5');\nvar p = new p5.Parser(),\n    s = new p5.Serializer();\nconsole.log(s.serialize(p.parse('')));\n```\n. ",
    "winhamwr": "As far as a spec for adding the ability to change this option, which API do we prefer? I can think of a few possibilities.\n1. On the Serializer in extra options argument\nThis would be easier to make backwards compatible, but feels a bit clunky.\nvar serializer1 = new parse5.Serializer({escapeStrings: false});\nvar serializer2 = new parse5.Serializer(\n    parse5.TreeAdapters.htmlparser2,\n    {escapeStrings: false}\n);\n2. On the Serializer in extra options argument\nMore consistent as far as passing options.\nvar serializer1 = new parse5.Serializer({escapeStrings: false});\nvar serializer2 = new parse5.Serializer({\n    treeAdapater: parse5.TreeAdapters.htmlparser2,\n    escapeStrings: false\n});\n3. On the Serializer.serialize in extra options argument\nAllows switching this option without creating a new Serializer, but honestly, this seems like an unlikely use case. Escaping strings is either something you want from a spec-compliance standpoint, or something that's very surprising and makes working with the serialized HTML awkward. I don't know why you'd want both in the same project.\n```\nvar serializer = new parse5.Serializer();\nvar document = parser.parse('<!DOCTYPE html>Hi there!');\n//Serialize document\nvar html = serializer.serialize(document, {escapeStrings: false});\n//Serialize  element content\nvar bodyInnerHtml = serializer.serialize(\n    document.childNodes[0].childNodes[1],\n    {escapeStrings: false}\n);\n```\nThoughts, @inikulin ? Personally, I like option 2 from an API standpoint.\nAnd as far as testing this option, would it be acceptable to add a one-off test like this one? It's not immediately obvious how Serializer options would be integrated into the generateTestsForEachTreeAdapter data-driven test framework.\nThanks\n-Wes\n. For 1, should we require that they pass in a TreeAdapter in order to provide extras, or should we attempt to test the first argument to determine if they are passing in extra or just an adapter?\n. Woohoo! :+1: \nThanks Ivan and Alan!\n. Could we add something like <a href=\u200b\"http:\u200b/\u200b/\u200bexample.com?hello=1&world=2\">\u200b</a>\u200b here so that we're also testing that this applies to attribute values as well?\n. Also, I think we should use &amp; in the src instead of a bare &. The &amp; would be valid HTML.\n. ",
    "alanclarke": "@inikulin, @winhamwr how about something like this? https://github.com/inikulin/parse5/pull/32\n. Awesome!\n. reason we did this is because it makes lossy and potentially breaking changes to the input. e.g. urls get corrupted:\n<a href=\"http://example.com?hello=1&world=2\"></a>\nbecomes:\n<a href=\"http://example.com?hello=1&amp;world=2\"></a>\nwhich is not equivalent\n. sorry for only seeing this now btw!\n. @stevenvachon consider the following case:\n\nyou can see from the above that really neither strategy will truly replicate the browser behaviour, that's why its useful to provide users with the ability to opt out of potential side effects like that \n. @stevenvachon but we'd need a parser to know which to re-encode?\n. it breaks reactjs for example:\nhttps://github.com/inikulin/whacko/pull/15\n. yeah, good point!\n. ",
    "nylen": "Hi - I'm also quite interested in this, in this case for a new iteration of the WordPress editor that we're working on.  We plan to use HTML comments as a \"pseudo-block-tag\" to store post content, and these \"pseudo-tags\" will contain HTML content inside of them.  Here's an example.\nI'm curious about your thoughts on how to parse a structure like this - it's not too different from Handlebar templates, but I agree that the robustness of parse5 would be a big benefit.  @stevenvachon what did you end up doing here?. > Why not use custom HTML elements for this purpose?\nWe want to preserve the structure of existing markup as much as possible, so that browsers will render it correctly without modification.  We also want to avoid adding extra container tags because this will break things like CSS rules that apply to specific sections of post content.\nA bit more context about the parsing specifically and what I would like to achieve there:  https://github.com/WordPress/gutenberg/issues/391. Probably the easiest way to explain that is to point you to one of our prototypes:  https://wordpress.github.io/gutenberg/tinymce-per-block/\nThere's a lot of needed functionality/UX missing from the prototype, but the basic idea is there: to re-work editing a WordPress post into editing a series of \"blocks\".  These \"blocks\" will be delimited by HTML comments.  You can see how this is serialized by clicking the \"Html\" button.  However, block delimiters have changed since then, to be more robust and look as follows:\nhtml\n<!-- wp:core/text -->\nWelcome to WordPress. This is your first post. Edit or delete it, then start writing!\n<!-- /wp:core/text -->\nIf you're interested in reading further, I'd recommend taking a look at the links in the Overview section of our project readme.. Rather than just extracting substrings, I think parsing the HTML inside of the block delimiters is part of the task.  We expect to have many different types of blocks, including implementations by third-party code via plugins.  It seems much better to me to provide a \"recommended\" way to handle parsing and verify that the markup inside of a block is actually valid for that block type, providing a fallback otherwise.\nI had hoped to achieve this in a single parsing step by extending a library like parse5, but that may not be possible. Another reason for this is that there are also other considerations - WordPress post content can contain shortcodes, yet another type of tag which needs another grammar extension.  Eventually we'd like to detect these and transparently upgrade them to the equivalent \"block\" representation.. ",
    "moll": "I was under the impression Parse5's returned tree was DOM compatible at least property-wise (sans methods). Then a query lib that could traverse that tree would be sufficient. So far I found the XMLDOM project which does the former. Still looking for that query lib. ;)\n. Thanks, though JSDOM's feels like overkill with its remote resource loading and trickery. All I'm after is a getting a basic DOM-like back (DOM-like merely to not waste time learning yet another tree structure or API) to assert on. That rules out Whacko and Parse5.\nXMLDOM seems to return a basic DOM structure. I and everyone else already knows the DOM APIs like el.getAttribute and el.tagName by heart. Trying it out now with an Xpath library to simplify querying.\n. Thanks, that seems pretty useful! If only there were a querySelector implementation I could pass a DOM from Minidom to ease finding a particular element. All of the ones I've found expect there to be a global window or document...\nYou don't happen to know one?\n. Thanks.\nJeesh. Things are so coupled in this traversing world. All you need in theory is a selector parser and traversing function capable of moving down a DOM1 tree and comparing a few tagNames values and getAttribute calls. But I guess some people like the job security they get from overcomplicating shit. ^_^\n. Well, I don't doubt emulating a browser DOM isn't hard, but in our parsing contexts the DOM's static and as always, first make it work, then make it fast. I haven't even found anything besides JSDOM's querySelector that works.\nThe Xpath libraries I mentioned above manages to do fairly well. It's not coupled to a parsing library, doesn't need a global document object and works according to a DOM spec. Perhaps a little too much to-spec as it annoys me a little with its namespaceURI pedantry, but at least it works. Not with Minidom, mind you, as its DOM 1 spec is missing a localName property and there's something with attributes that prevents Xpath from working. :P\nWhen it comes to querySelector, I'd be happy with CSS2 level querying. Anything, frankly, that's better than typing out seventy four periods to get an input[name=foo] element in a form. ;-)\n. Thanks. I ended up going with XMLDOM and XPath.\nBesides the slight annoyance of too verbose tag names (which I worked around with a simple regexp) it's a fairly composable and decoupled solution that works today. I couldn't get Minidom to work like I described above. \n. ",
    "kkirsche": "I'll close the Parse5 issue on that. Sorry about that. Thanks for the clarification!\n. ",
    "RReverser": "Looks like subset of #26 - if you get streaming-like SimpleAPIParser, you can stop parsing (and even reading original file) whenever you've found what you wanted.\n. Suspend and stop is not the same. With streaming parser, you can cancel streaming as soon as you find <meta> and thus avoid not just parsing, but even unnecessary further reading from filesystem/network/whatever.\n. @inikulin Awesome, thanks!\n. @stevenvachon It's not just about codes; parse error locations are not static and are added/removed as the spec evolves, and due to the absence of cross-implementations unit tests it will be non-trivial task to maintain them up-to-date.\n. Try to use jscodeshift + jscodemod - it allows to quickly transform codebase with little efforts.\n. @stevenvachon You might want to see a thread at https://github.com/inikulin/parse5/issues/133#issuecomment-222713109.\n. Ah thanks, that explains a lot :)\n. What's even more weird, while https://github.com/RReverser/parse5/commit/c2f8f4f538aa6c6ed60ebc38e8c28af60f11f6d7 shows incorrect diff, https://github.com/RReverser/parse5/commit/c2f8f4f538aa6c6ed60ebc38e8c28af60f11f6d7.diff returns the correct one. Apparently there is some bug with Github.\n. @inikulin You're welcome. One more thing - either due to merge or I missed smth, but table got broken :( https://github.com/inikulin/parse5/blob/master/docs/05_api_reference.md#treeadaptersetdocumenttypedocument-name-publicid-systemid\n. @domenic Also not sure about this part, but #86 was accepted by @inikulin where I edited docs manually.\nIf they are auto-generated, then I'm somewhat confused why < and > are correctly replaced in ASTNode<DocumentFragment> but not in <template> and not sure where to fix :/\n. @inikulin No worries. Doesn't explain my question though - shouldn't generator handle that as it does in ASTNode? Don't really want to put &lt; / &gt; entities in real code comments just for the sake of generated Markdown :(\n. @inikulin Same issue as in #86, apparently Github doesn't recognize specific line endings generated by jsdoc or smth :(\n\nWhat's even more weird, while RReverser@c2f8f4f shows incorrect diff, https://github.com/RReverser/parse5/commit/c2f8f4f538aa6c6ed60ebc38e8c28af60f11f6d7.diff returns the correct one. Apparently there is some bug with Github.\n\nSame here - just add .diff or .patch to PR or commit URL to see the correct raw diff. The only reliable workaround that I've found so far.\n. Thanks! Sorry, didn't find time to finish.\n. @inikulin Modern GCs are good enough to reuse small objects on their own, I doubt that will give you any significant win. But might be worth trying, I'm just a random guy on internet :)\n. Do you mean actual encoding conversion or encoding detection or replacement of specific characters? First one is already covered by built-in string_decoder, other two should be easy to do.\n. @songsiqi This is how browsers parse HTML as well - otherwise you might get ambiguity (for example, customAttr and customattr and CustomAttr are all treated by browser as a same attribute).\nI believe it's not hard to perform .toLowerCase() in own getAttr helper if all that you want is to call in getAttr('customAttr') fashion.\n. @songsiqi If you're in browser, you can go away with new DOMParser().parseFromString(str, 'text/xml'), in Node.js you might want to take a look at https://github.com/Leonidas-from-XIV/node-xml2js.\n. > An XML parser, for the purposes of this specification, is a construct that follows the rules given in the XML specification to map a string of bytes or characters into a Document object.\n\nAt the time of writing, no such rules actually exist.\n\nWhat?!\n. @domenic Well, looking deeper, it refers to XML spec which has set of rules to convert byte stream into nodes,  and HTML spec has this section:\n\nThis Document must then be populated with DOM nodes that represent the tree structure of the input passed to the parser, as defined by the XML specification, the Namespaces in XML specification, and the WHATWG DOM standard. DOM mutation events must not fire for the operations that the XML parser performs on the Document's tree, but the user agent must act as if elements and attributes were individually appended and set respectively so as to trigger rules in this specification regarding what happens when an element is inserted into a document or has its attributes set, and the WHATWG DOM standard's requirements regarding mutation observers mean that mutation observers are fired (unlike mutation events).\n\nwhich kind of describes interop, although in non-normative way.\n. @inikulin It's mainly because they are mapped pretty much 1:1 (XML names are kept in generated DOM without any extra transformations - e.g. new DOMParser().parseFromString('<XML />', 'text/xml').children[0].tagName == \"XML\"). But I agree that it should be stated more explicitly.\n. > The word \"node\" appears exactly once in the document, BTW.\nHaha, oh my... Well, we can attempt to fix it.\n. Cool, want to send a PR?\n. @inikulin I think it would be nice to ship them by the way, as their new typings approach allows libraries to ship & maintain their own typings without need to update anything in the DefinitelyTyped.\n. Well, I know they've added jsdoc support as part of https://github.com/Microsoft/TypeScript/issues/4789 meta-issue which allows to detect JSDoc types as if they were native TypeScript types, but I'm not sure if there's a tool to generate definitions out of that info.\n. Some interesting links here https://github.com/DefinitelyTyped/DefinitelyTyped/issues/2103\n. We might want to try https://github.com/fivetran/typescript-closure-tools - I suppose Closure annotations are compatible with JSDoc?\n. @inikulin Lots has changed recently, I wrote them when it was only possible to install them from DefinitelyTyped and not as part of the package so that it would be recognized. Need to check docs.\n. @fictitious Thank you for clarifications!\n\nFor compatibility with previous typescript versions, you might want to publish on DefinitelyTyped. \n\nEven in previous versions you can still pass path to definition under node_modules manually to tsconfig, so I don't think that part is needed.\n. @inikulin You just use as it's already used in that file:\n/// <reference path=\"../node/node.d.ts\" />\nThen, when definitions are published, TSD will automatically parse and add such references as dependencies. For example, when you now install npm install @types/parse5 you can see that it also installs @types/node.\n. @inikulin Sounds like next step is to revisit idea of moving to TypeScript from #58 to avoid such docs bugs in future \ud83d\ude04 \n. @pedro93 Just remove child elements before serialization, or clone only what you need into a new object. I don't think this needs to be a built-in.\n. @pedro93 More configs for all the possible use-cases is certainly not a simpler solution in any library. APIs should be as minimal as possible to cover all the basic functionality, while keeping everything else for the userland code. Anyway, glad I could help.\n. UPDATE: 55 failing now; also made adjustTokenizer private (_adjustTokenizer) and instead exposed getNextToken wrapper to be able to skip / substitute tokens more easily.\n. > I see lots of fails caused by #99, shouldn't be hard to fix\nSure, that's one of those that I referred to as\n\nSome of those we can easily fix\n\n(another was first \\n in pre / textarea)\n. Done, 28 failing left. Working on it :)\n. @inikulin Can you elaborate on this ignored thing? What is/was it used for?\n. > we can completely get rid of this property by updating appropriate handlers in parser per recent spec\nThat would be nicer than just hacking in order to ignore them in a specific test suite.\n. 13 failing left. Getting there \ud83d\ude04 \n. Okay, I added special handling for testing HTML / BODY, and added a note about why we're doing this. I guess this is the best way to handle this without breaking assumptions nor ignoring those tests completely.\n7 left.\n. Foreign content switching is a mess. Stuck on fixing those last 4 :/ Want to help?\n. Okay, looks like I've finally fixed it, but this last commit involved almost random reordering of some block until it worked, so it would be nice to have a review \ud83d\ude04 \nIn the other news, entire test suite now passes.\n. Ok done.\n. Correction: DOCTYPE-related changes were not landed on spec due to changed parse error behavior which I yet have to fix, but since we don't care about parse errors, this should be fine already.\n. And maybe partially to #52 (although obviously not the biggest source :) ).\n. Well it says \"Excessive RAM usage\" in the title, and this is also can be categorized as excessive RAM usage :)\n. Maybe better to create a separate issue since the reporter meant exactly memory usage?\n. Okay, that works.\n. >  that if we'll do it on every [x]DATA state performance will degrade due to string operations\nI suggest to just check the current state / token on hibernation.\n. >  that does not attempt to fix the html\nThis is not a \"fix\", this is the only way to parse it correctly. Otherwise you end up with useless results.\n. @inikulin Is there a possibility to do this via custom external adapter or, if not, expose APIs that could allow it to do this? (Ideally, I'd like to fix the serialization in spec, but apparently that's unlikely)\n. Closing as basically a duplicate of #116.\n. @Supamiu It's just not how HTML works. Angular 2 uses own templating language which only resembles HTML in some ways, so if you want to parse it, you most likely need to find and use the parser they do.\n. Looks like their one is here: https://github.com/angular/angular/blob/master/modules/@angular/compiler/src/html_parser.ts (plus related modules like html_lexer and so)\n. What are the differences in result with feeding a parser with '<plaintext>' + ...actual content...?\n. > You can't emulate plain-text parsing by wrapping text with some markup\nThis feels controversial to\n\nwe need to wrap text with <pre>\n. Yeah, I'm asking (rather educationally) what's the difference with emitting <plaintext> instead of <pre> (which already switches to PLAINTEXT on its own without special handling).\n. @mdhaddad Remove all the unimportant parts (such as text or even attributes) and provide minimal exampe where your issue can be reproduced.\n. You are having issues only with the end tag, so you likely shouldn't care about differences in tokens at the chunk boundaries in order to reproduce a bug.\n. > Weird. Though don't browsers have to parse iframe contents as more than just simple text if they are to render them correctly?\n\nNo. Whenever in doubt, best is to consult the spec for the correct behavior:  https://html.spec.whatwg.org/multipage/syntax.html#parsing-main-inbody\n```\nA start tag whose tag name is \"iframe\"\nSet the frameset-ok flag to \"not ok\".\n\nFollow the generic raw text element parsing algorithm.\n\n```\n\nHow come parse5 doesn't parse iframe contents as more than merely text as well?\n\nBecause it would be incorrect behavior.\n. Also, you can use a parser built-in into your browser to check the results:\n``` javascript\n\nnew DOMParser().parseFromString('', 'text/html').querySelector('iframe').textContent\n\"\"\n``\n. Oh, you squashed them. I actually left a note why I did two separate commits there - Github seems to have problems with displaying such a huge diff (it hangs the browser), so left them as separate ones. Although probably it doesn't matter as long as nobody tries to see what has changed :)\n. Yes; there is no \"processing instruction\" in HTML, it's XML thing.\n. Hm, I would say this is a problem of a consumer that passes incorrect values.\n. Omg :) I'll do tomorrow if you don't mind, that's lots of changes as for the evening.\n. Awesome work! Added some notes, but LGTM.\n. @inikulin Just back from the vacation, but good that you merged it already :)\n. Sure, will take a look today!\n. What about speed benchmarks? Given that you call that method for each character now, curious if you noticed any slowdowns.\n. @inikulin Cool! Waiting for answers on review questions :)\n. Indeed, they're in comments (as in this new review process).\n. Ok, I looked through this and the idea is awesome. Approach is slightly different than I used elsewhere, but everything looks good. Great job!\n. I don't get what you're asking for to be honest. Just custom serializer? Yes, you can implement it.. This has been answered in at least #133 and #116.. @TimvdLippe HTML spec is very explicit about each edge case, including how to restore errors, and having custom way to do so would be a violation from the spec. But if you want to autocomplete, can't you manually insert>` in the cursor position before passing to the parser?. As documentation suggests:\nJust try it in the latest version of your browser before submitting an issue.\n\nIf you try this in your browser, you will see that \\r is stripped in just the same way. This is part of HTML tokenization algorithm which normalizes any newlines to \\n.. @felixsanz I closed the issue just because it's not really an issue, but I'm happy to answer your questions. As I said, it's part of tokenization algorithm, not tree construction - that is, newlines are normalized way before you know within which tag you're.\nYou might want to check the HTML spec for explanations: Preprocessing the input stream\n\nU+000D CARRIAGE RETURN (CR) characters and U+000A LINE FEED (LF) characters are treated specially. Any LF character that immediately follows a CR character must be ignored, and all CR characters must then be converted to LF characters. Thus, newlines in HTML DOMs are represented by LF characters, and there are never any CR characters in the input to the tokenization stage.\n\nP.S. I said it's part of tokenization algorithm, but in fact, as spec suggests, it's part of \"preprocessing\" algorithm which happens even before tokenization.. As for your second question, check out this tweet from #HTMLQuiz series we did with Simon: https://twitter.com/zcorpan/status/788712392075186176 (and following answer). Documentation addresses this very problem: http://inikulin.github.io/parse5/#typescript-definitions. One more suggestion: if you want to transform some HTML nodes but preserve formatting for others, you can also use startOffest / endOffset from node's location to .slice(...) pieces from original string whenever node is unchanged. This way, you will preserve formatting on any untouched pieces, and also will likely get better performance than with serializing entire AST from scratch.. Please send a pull request, this is a simple change and PR is a much better medium for changes :). Sorry, but question is unclear.. It doesn't \"fix\" HTML, it parses it in accordance with spec. This is not a separate fixing mechanism from any other parsing, but normal parsing flow where some tags are implicit etc., but having implicit tags doesn't make HTML invalid according to HTML5 spec - in opposite, such documents are still totally valid.. Personally to me, conformance checkers just feels like a thing from the past nowadays when we had to check our HTML with online W3C tool to be sure that it will be parsed correctly (or parsed at all) by all the different browsers. Now that they all follow the same spec (apart from temporary bugs), that feels less useful, but I don't oppose it surely if there are valid use cases.. > However, boolean attributes are still a real type of attribute that the current parse5 AST cannot represent correctly.\nIt's not. parse5 represents values correctly - any attribute can have only a string value.\nWhat you're providing below is a DOM representation which converts strings to numbers (e.g. tabindex), space-separated token lists (e.g. class), booleans (e.g. hidden) and so on, but that's a completely different workflow and has nothing to do with HTML parsing itself. Your spec link points to DOM section too - not the HTML parsing.\nAs far as HTML parser is concerned (whether parse5 or in-browser), attr and attr=\"\" are exactly same, there is no way to distinguish between them via browser APIs, and further DOM conversions don't distinguish between them either:\n\nSo in the end, as pointed in the referenced issue, this is just a matter of formatting (shorthand vs canonical), and not of losing any information.. Judging from the existing structure, I'd suggest to put one into test/data/tree_construction_regression - it will be automatically tested both with regular and streaming mode by test/fixtures/parser_test.js.. Incremental parsing is indeed a very complicated task, especially in languages where single character can change entire input after that. You need to know where you're in the tree to detect whether added characters can modify the content after that or not. On another hand, once you know the user is editing e.g. inside of attribute value, it's relatively easy to detect whether you need to reparse all the following content or given characters can just change attribute value in-place.. Thanks for the report. However, I believe this is a duplicate of https://github.com/inikulin/parse5/pull/145 where the answer was given.. > Null is always a valid value for a string\n\nnull is a string\n\nWhy would you think so? null is a completely separate value, it's definitely not a string and TypeScript (which AFAIK Angular uses) with strictNullChecks or just strict mode will not allow you to pass it where a string exists.\nThis is an issue of consumer not passing the expected types, many things can go wrong in JavaScript when doing so.. > All strings are terminated with null (0x0). A string with no value is simply 0x0 but still has a pointer in memory to the initialized memory location.\n\nStrings unlike all other primative types can contain null and only null as a valid value because ALL strings are null terminated unlike all other primatives.\n\nThis is JavaScript, strings are not zero-terminated here. I think you're confusing JavaScript strings with C strings which have very different representation.\nMoreover, you're mixing null as a pointer, 0x0 as a byte and null as a JavaScript object value into one - they're all different things and different types. This is the reason why\n\nThus passing null into anything that accepts a string value is valid in all programming languages ever.\n\nis incorrect.\nBtw, JavaScript is not the only one having non-zero-terminated representation for strings, see also Rust, Lua, and many others.\nThis is why\n\nI know so because this is computer science 101.\nParse5 ignores this basic principle of cs\n\nare incorrect - you're confusing computer science with implementation details of the other language you're coming from, which is irrelevant in current context.. Back to the topic:\n\nAngular breaks with completely useless and unfixable errors (without endless trial and error) as a result of this and it isn't alone in his problem.\n\nI already explained above why escapeString doesn't have to check for types - it's responsibility of the library consumer.\nHowever, there is also one deeper reason - if escapeString is called on null, it means that the AST is malformed in the first place and you got null as a value of some attribute or a text node which it could have never been when coming just from parse5 (if it is, please do let us know and raise a separate issue!).\nSo this is just a symptop of a deeper bug in some transformer and it's indeed better for it to fail with a JavaScript error so that you could find the offending transformer and fix it, otherwise you can run into many more hidden issues than just during escaping.. @JohnGalt1717 I believe everything that could be said on-topic here was already said and explained, so I'm going to go ahead and lock this issue to prevent future misunderstandings.. And to answer the second question:\n\nThe structure of the fragment is wrapped by  natively, but with parse5 it does not (hence the need for the extra .childNodes[0] in the second example).\n\nReason is DOMParser parses provided HTML as a document not a fragment, which is just .parse in parse5.. Definitely not \"drop-in\" with defaults at least because the output formats are different (actual browser DOM nodes in DOMParser vs JS-objects-based-AST in parse5), but you can provide custom tree builders to parse5, including one that would create real DOM nodes or at least objects as similar to real DOM nodes as you need (uppercase tagnames etc.).. > or in any way useful in the browser\nI wouldn't say it's useless - fetch allows you to get a streaming response, which, in turn, can be used with parse5. Especially useful if you want to parse HTML on the fly in Service Worker or something.. @dantman Agree it might make sense, but then any bundler allows you to configure blacklists of modules - does adding \"stream\" to ignored list not cover your usecase?. @dantman \nI see... Regarding \n\nwhich you can require by require('parse5/lite')\n\nsuggestion - would using require('parse5/lib/parser') work for your use case? That's what we did in ASTExplorer and it worked just fine. Although if React Native's bundler doesn't let you override modules, that probably won't help as you said.. > since the two modules do not output the same structure (a single class vs a module of classes/functions)\nIn webpack etc. you can override to anything including local module, in which case you can reflect original structure with no issues without changes upstream. Although if you could do that, you could as well replace stream with almost empty shim and it would work in just the same way.. @afiestas It wouldn't be per spec. I'd suggest checking how your HTML is handled in a browser.. @wichert Since JSX is an extension of JS, you might want to use Acorn-JSX instead.. As far as I remember, devDependencies are not installed together with package unless you pass some special flag, as they are intended to be used only by maintainers. That way, required types would be inccessible to end users, effectively rendering them useless. . 1) It sounds like it's React Native that should be defining their typings that are compatible with Node.js ones. \n2) @ljharb It's definitely predominant convention to provide types for any popular libraries alongside the package,as it gives nice completion support for IDEs, support for TypeScript consumers etc. and end user is not supposed to search for and install or require such types separately.\n3) The problem is in fact deeper and reveals potential runtime problem too (as any typings definitions do) - parse5 indeed has mode where it relies on Node.js stream module which is the entire reason it depends on Node.js in typings - because calling streaming API of parse5 would fail on React Native in runtime as well if it doesn't provide same stream module (although I never worked with RN and don't know - maybe it does? if so, they should have common typings for such module that would be shared between RN and Node.js official typings) . DefinitelyTyped is mostly a legacy way to provide typings where author can't or doesn't want to do so or packages are too old to provide them. It's just a separate Github repo that periodically publishes typings as separate npm packages which consumer would have to install manually; however if it's a modern library, it usually has and is encouraged to have its own typings in the same npm package to reduce bloat and make it more convenient for the end user using any editor with completions or TypeScript. . Either way, as I said, types in this case, just like usually, expose a problem that would exist at runtime too should you accidentally use an API that depends on Node.js APIs and is not actually compatible with React Native. So it might make sense to either fix RN typings if they provide same API set or maybe split parse5 into engine-agnostic generic parser and another package for streaming one that would continue to depend on Node? Thoughts @inikulin? . Apparently someone just independently ran into the runtime counterpart of this issue I've been theoretizing about above - see #234 for details. . > in environments where installing node typings breaks TypeScript compilation\nThat was fixed separately when parse5 was split into core versions and one that requires Node.js. After that I don't see either why the typings had to be removed, as they're not conflicting with RN anymore.. >  v4 that was published on npm does lazy loading of node-dependent parts but they are still in the same package.\nYeah, my bad, I misunderstood what happened.. Thanks!. > This specific example makes it fairly clear why this is not always desirable.\nIt looks like the problem here is serialized representation. Parser itself does what it's supposed to do as per spec.. AFAIK that's per spec, <image> in HTML context is just an alias that gets transformed into <img>. Browser will do the same if you open such HTML and check devtools.  . It's working as intended and duplicate of previous reports as you can find here: https://github.com/inikulin/parse5/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aclosed+entities. How are you currently using it? You should transpile parse5 with Babel or some other transpiler if targetting IE11. Assuming you're bundling it with Webpack or similar and already use a transpiler, it should be a small change to the config.. > We expect they have already been transpiled and are ready for use directly.\nThat's increasingly not the case with various new libraries, as ES6 is fairly well supported across all modern browsers and devices, and often has extra benefit of being better optimisable by modern engines. So it's \"ready for use directly\" in any modern environment. Similarly, before ES6 you didn't expect all the code on npm to be transpiled from ES5 to ES3 to support IE8 and older.\nFor the cases where you target older browsers, adding a config to transpile dependencies shouldn't be too hard.. This is working as intended (per spec). Please check previously closed issues about serialisation, questions like this have been answered many times. . The issue description doesn't contain any specific question, but this is working as per HTML spec. It's also worth taking a look at some of the previous discussions. https://github.com/inikulin/parse5/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aclosed+entities. @evilebottnawi Solve what? As I said, this is working as per HTML spec, which parser strictly follows to have exactly same behaviour as browsers.. > using the parser in order to format source code; in this scenario, you don't want to modify what the developer wrote\nIsn't the whole point of autoformatting the code to... well, modify, what developer wrote into the same canonical format?. > Only euro symbol, we can use same canonical format if we have original source text.\nI don't understand. If you agree that you want to have a single canonical format as output, why do you still want the original raw text with different entity representation and not the parsed value? Arguably it would be more readable to have euro sign as-is (luckily, UTF-8 is supported everywhere nowadays), but even if not and for whatever reason you want to encode some symbols back to entities, you could do that as well from the parsed text without needing the original raw representation.. Will open another PR instead.. Same for ParserStream.. @inikulin Rebased and fixed conflicts.. I actually had a very similar issue recently and worked around it in custom subclass of RewritingStream. If @inikulin doesn't mind, I can upstream my changes.. For what it's worth, what I did to work around this is subscribed to all the events, but added rewriters to a Promise-based task queue so that ordering of the output would be still preserved. Pseudo-code (without error handling etc.):\n```js\nlet queue = Promise.resolve();\nfunction defer(fn) {\n  // adds function to the queue making sure that it runs only after all the previous have completed\n  queue = queue.then(fn);\n}\nrewriter.on('text', (_, raw) => defer(async () => {\n  let res = await myfunctionPromise(raw);\n  rewriter.emitRaw(res);\n}));\nfor (let eventName of ['startTag', 'endTag', 'doctype', 'comment']) {\n  // each event has to be subscribed to and deferred individually or it\n  // will fall back to emitting as-is synchronously\n  rewriter.on(eventName, (_, raw) => defer(() => {\n    rewriter.emitRaw(raw);\n  }));\n}\nrewriter.write(html, async () => {\n  // don't close the stream until all the tasks have finished\n  await queue;\nrewriter.end();\n});\n``. It's not common at all (unfortunately), although there have been some discussions.. It's kinda ugly due to need to iterate all events not subscribed yet manually. I wonder if it would be better to addunhandledTokenevent as a fallback instead of assuming that user wantsemitRaw` for easier control.. Thanks. Indeed, the documentation needs to be updated, but I believe that commit explains why it's done. Previous API was hiding very important yet very subtle mistakes with handling multibyte chars which you won't catch at development time, but which might cause headache in production.\nAs for upgrading code to work with new parse5, as the commit description said, change is usually as simple as adding encoding: \"utf8\" to whatever stream producer you have, e.g. for example above:\ndiff\n- const file = fs.createWriteStream('/home/google.com.html');\n+ const file = fs.createWriteStream('/home/google.com.html', { encoding: 'utf8' });\nor even, using a shorthand:\ndiff\n- const file = fs.createWriteStream('/home/google.com.html');\n+ const file = fs.createWriteStream('/home/google.com.html', 'utf8');. Would you be interested in sending a PR to update docs?. Oh, and for http:\ndiff\n+   res.setEncoding('utf8');\n    res.pipe(rewriter).pipe(file);. Note that utf8 is applicable only if, well, you're sure that response is indeed UTF-8.. What you're calling tokens here is actually nodes (DOM-like), and for nodes indeed non-consecutive items can be reordered and merged as per spec.\nWhenever in doubt, you might want to try same HTML in any browser and inspect the DOM - you should see the same behaviour as in parse5. If not, then it's indeed a bug in one or another. . Yeah, maybe. Or could even inline into this new getNextToken completely.\n. Well, for our use case I need to be as close to browser behavior as possible, as I'm going to do various streaming transformations, so if browser treats <image /> as <img /> then I should be able to see it this way too, so that any transformations that are applicable to img will be applied to image as well, any selectors that look for img, should find image too, and so on and so on.\nAs for the newlines - that might be debatable, but, personally, I'd prefer to be as consistent with the real handling as possible in order to decrease risk of running into weird bugs in future (duplicate html and body are kind of edge cases that I'm totally fine living with).\nWith the \"close to the source code as much as possible\" you could argue that we can also disable NUL-character and CRLF normalizations as they might also be seen as non-critical (especially in JS land), but that's a way too big rabbit hole to get into IMO :)\n. Btw, theoretically, if we get close enough, we could even use this simulator in between the real parser and tokenizer in order to avoid logic duplication and share more code between non- and streaming parsers.\n. Sounds good to me. Given that exact same combined condition is used in more than one place, perhaps better to create additional helper method on foreignContent itself?\n. Initially, I actually hooked into private methods of Parser in the test suite, but then decided that it might be not the best idea and consumers might want to perform own actions on token retrieval as well (similarly to treeAdapter, but token-based). If you think exposing it is a bad idea and we should only use it for testing, then I think internal hook into Parser private methods would be better as it doesn't expose anything.\n. Didn't really have any, that's why said \"might\" :) I'm totally fine with hiding it back if you wish.\n. Hm, yeah, no idea why I wrote it that way back then.\n. Ah, it's because it's a boolean and initialName would be misleading as it could suggest it's a string.\n. What should I change it to in such case?\n. Nah, just changed it to accept initialName - that's a bit cleaner anyway IMO \ud83d\ude04 \n. This might look not very clean IMO because logic of stop condition is hidden inside of the body. Perhaps better to put condition here explicitly? If it's not dependant on i, then at least nextElement !== formattingElement would do.\n. IIUC, those functions are pretty cold, and any tree adapter implementation are assumed to be fast, too, but otherwise I would probably split this into two ifs where external one calls only tag name, and if it matches, only then calls getNamespaceURI to check it too, thus creating a \"fast-case\" for any elements in stack that are not headers (basically what you did here before the change).\nBut, as I said, might be not beneficial in this particular case, dunno.\n. Wondering if this should be made conditional (depending on some option) to address cases where people don't want fake elements inserted.\n. Ah right.\n. Perhaps we can drop Node.js < 4.x for testing now and use only native Promise?\n. How about reuse HIBERNATION_TOKEN for this? In that case in streaming mode output chunks can come out with the ~ same speed as input comes in and not depend on separate \"magic\" number.\n. >  I would like to make sure that we don't break anything by using ES6 syntax\nThat sounds more like a task for linter.\n. AFAIK just setting ecmaVersion to 5 should be enough (ESLint / Espree uses Acorn under the hood, so when it passes ecmaVersion: 5, Acorn simply won't recognize ES6 syntax and won't be able to parse it).\n. I meant calling drops only when we meet hibernation token. Then, this will work naturally for streamed input, and for the regular .parse you can just feed input in chunks (basically what it does now, but on the higher level).\n. Maybe you're right... Just trying to think of the cases where we will still have congestion inside of the streaming parser when chunks were already fed to it, but it didn't output the text node (because it was all text).\nIs bufferWaterline supposed to be configurable via options btw?\n. Ok, we can revisit this in future should someone need it.\n. Why this change? Trailing spaces aren't nice :). It's not about file size, but rather about annoyance, especially when editor is configured to auto-trim them (e.g. via .editorconfig or by default like mine :P ) and then you're getting weird diffs whenever you try to commit changes.. Btw, chunk === undefined would be equivalent and simpler. Or you can even do chunk = chunk || ''.. ",
    "thisconnect": "thanks a lot for this\n. Me to on Node.js 5.4.1 parse5@2.1.2\n. I personally would opt for speed. Are there situations where you parse huge files and have limited or low memory?\nIn my application I only need speed and have plenty of memory. Wondering what situations people need low memory footprint.\n. So new documentation is going to be under http://inikulin.github.io/parse5 ?\nI like that a lot, as browsing github wiki on mobile isn't great\n. @jescalan have you considere link rel=import instead of a custom include element?\nhttp://webcomponents.org/articles/introduction-to-html-imports/\n. Sure sorry\n. ",
    "jails": "Thank you !\n. ",
    "gagern": "Adaptor for jsdom, xmldom, libxml-dom, \u2026\nIn case you wanted to suggest jsdom as an alternative: sure, it can provide similar functionality, but the fact that the latest version requires io.js makes me reluctant to use it. And parsing to jsdom just so you can import the node into some other DOM implementation you're working with doesn't look attractive either. On the other hand, another tree adaptor would be a fairly easy affair. Should I make this a pull request?\n. What I had in mind was something like this:\njs\nvar parser = new Parser(new DomTreeAdapter(new xmldom.DOMImplementation()));\nwhere DomTreeAdapter would be a new class shipped with parse5, and its constructor argument could be any object satisfying the DOMImplementation specification. But I now had a closer look and realized that on the one hand, libxml-dom apparently doesn't offer such a DOM implementation class, so the number of users would be rather small.\nOn the other hand, parse5 does object creation significantly different from what the DOM specs envision, so the effort to make things work out would be considerable. In particular, one would have to know the name (and namespace) of the root element before creating the document, and one would have to pass the document as an adapter function argument whenever constructing new elements. Probably not worth the effort just now. I thought this would be simpler, thus my suggestion.\nFeel free to close if you don't want to pursue this either.\n. ",
    "nrkn": "I had need of something like this\nhttps://github.com/nrkn/dom-treeadapter/\nI'm not actually using it with parse5 though - I've adopted the parse5 TreeAdapter interface throughout another project and I'm using it to map between two different DOM structures. Here's the mapper I'm using:\nhttps://github.com/nrkn/treeadapter-mapper\n. ",
    "vjyanand": "It's not implemented in jsdom either\n. ",
    "sakagg": "Thanks!\n. I think what inikulin mentioned is more intuitive as definition of 'content' can be ambiguous.\nOn the other hand daivdbau's method has less data repetition.\nNot sure what to do.\nAlso, in cases where starting tags are missing but content is present, we can't set contentStart location due to lack of information of the startTag(which is a fake tag) which can be misleading.\n. @inikulin @davidbau \nUpdated!\nPlease have a look\nThank You\n. Thanks! :smile: \n. Thanks :+1:\n. Sorry, I've been busy lately because of college work!\nI'll get back to this in a few days if it's okay.\n. Good thing someone finally did it!\nThanks @yyx990803\n. Yes, both of them are required.\nFirst one handles cases like\n<p> <p> </p>\nand ensures no __endTagLocation for the first <p> (because it doesn't have one)\nThe second one handles cases like\n<table>\n\u00a0\u00a0<tr>\n\u00a0\u00a0\u00a0\u00a0<td>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<p>\n\u00a0\u00a0\u00a0\u00a0</td>\n\u00a0\u00a0</tr>\n</table>\nand ensures no __endTagLocation for <p>\n. Done :+1: \n. ",
    "ghost": "What if CPU load increases by less than 1% but RAM usage decreases by over 80% Would you still not implement a theoretical change that would change for proportions like those?\n. As small as possible without compromising the main objective, that is performance.\n. ",
    "davidbau": "Consider modifying the README.md to mention your new location field on the element.\n. A naming suggestion if merging things into a single object - maybe instead of calling it the end of the startTag, we should call it the start of the content, i.e., start, contentStart, contentEnd, end.\n. Add a space between if (.\n. Are the last two clauses needed, or are they already required to be true?\nAlso, if lots of clauses are needed, the style in this file seems to be: keep short lines, so break the line after && then indent if it's going to go long.  It's important to match the prevailing coding style.\n. Cool. Maybe add brief comments with those two cases next to the clauses that check against them.\n. ",
    "philmander": "Great, thanks for the quick response.\n. ",
    "nathanhinish": "I'm using this module through jsdom. For my use, I needed ALL entities to be escaped again when serializing the document. The references go like this jsdom.serializeDocument -> domToHtml (in the jsdom module) -> parse5.TreeSerializer.\nIn order to do this currently, I am overwriting the _serializeAttributes and _serializeTextNode functions on the Serializer prototype. Then I am declaring my own escapeString function that uses the html-entities module to escape a MUCH larger set of entities.\nI created a Gist to show my custom file. https://gist.github.com/nathanhinish/acd2ef056503d861f656\n. And here's an HTML file that lists all the encoded entities that I used initially to see what was happening in my code.\nhttps://gist.github.com/nathanhinish/c8ac4d37f2fb89f121bd\n. ",
    "khrome83": "You could use Babel.js, but there is some issues with this, including compile step to transcompile to ES5. \nIf you use direct ES6, I am concerned about performance. For example, a basic loop still outperforms other looping types. I doubt it is any different in ES6.\n. ",
    "mike820324": "I misunderstood the nodeName at the first place.  I thought it is the nodeType in the Node interface. :P\nThe link is very helpful. thanks for the quick reply and reference. \n. ",
    "shawnXiao": "Thanks for your help. I have done as you said. \njs\nvar fileContent = fs.readFileSync(includeFilePath, \"utf8\");\nvar parser = new Parser();\nvar fragment = parser.parseFragment(fileContent, entry.childNodes[0]);\nentry.childNodes.push(fragment);\nI found that fragment con't be serialized to html. I have another way is traverse the fragment and then push to entry,  I think it's a little bit complex.\n. ",
    "faustman": "Hi,\nhere this:\nindex.js:\n```\nvar Parser = require('parse5').Parser;\nvar fs = require('fs');\nvar parser = new Parser();\nvar html = fs.readFileSync('./test.html');\nvar doc = parser.parse(html);\nconsole.log(doc);\n```\ntest.html:\n```\n\nHEY!\n\n```\nI think the html is a Buffer.. there is problem?\n. Oh, sorry..\nMy bad,\nshould be\nvar html = fs.readFileSync('./test.html', { encoding: 'utf8' });\n. ",
    "MarkPieszak": "@alanclarke Long time coming, but updating from 1.5.1 and we've always been using encodeHtmlEntities : false when passing into the serializer. \nI'm trying to look through parse5 to see how this could be achieved now that it has been removed in 2.0, can anyone point me in the right direction? It's breaking now at str.replace() inside of the escapeString() function.\nhttps://github.com/inikulin/parse5/blob/600449c7404408e0539fc0fd7a81050ae548a658/lib/serializer/index.js#L45\n. Taking forever, but so far it looks like it's a checkbox that blew up when it was trying to serialize the attributes on it. Still looking to get more info. (Taking eternity to debug this one)\n. It seems it has to do with a checkbox that angular2 isn't rendering properly as <input type=\"checkbox\" checked=\"checked\" />.\nRemoving those html lines actually fixed the issue. Very strange I'll have to let them know about that...\nAt the moment I have a component passing [checked]=\"item.completed\" which is either a true or false, this causes parse5 to error out here.\nWhen I changed the template logic to [checked]=\"item.completed === true ? 'checked' : '' \" this actually working.\nWas that logic potentially changed from 1.* to 2.*? It was working before (but that may of been due to encodeHtmlEntities: false we used to have.\n. ",
    "parshap": "Excellent! Thank you.\n. ",
    "yyx990803": "Haha, I didn't even check... updated.\n. @inikulin for docs, do you think it's sufficient just adding attrs to the base LocationInfo type?\n. Updated docs - my editor seems to have auto-converted all the ^M chars into \\n, hence the weird diff. Not sure if that is important though. It's probably better to preview locally.\n. cool, thanks!\n. Added test case.\n. ",
    "skreborn": "This would be the correct position to return for an attribute with no value. Consider the following. You have an attribute foo that has no value specified. The location points to the end of foo, where you might add a value by appending =\"bar\". It works the other way around, too. If you would like to remove the value of foo=\"bar\", you are in luck, as location points to the end of foo once again. You can easily remove the entire value, including the equal (=) sign.\n. ",
    "tamascsaba": "Sorry, it is in the doc :+1: \nhttps://github.com/inikulin/parse5/blob/master/docs/07_version_history.md\n. ",
    "pixelami": "I have noticed that when using the SAXParser to process a ReadableStream instance (containing latin and non latin characters) the location object passed to the 'startTag' and 'endTag' event handlers have incorrect startOffset and endOffset positions.\nDoes that seem to be a likely symptom of this issue of should I create a separate issue and investigate further?\n. I created this issue,\nhttps://github.com/inikulin/parse5/issues/103\nIs it feature or a bug, I'll let you decide....\n. ",
    "PatrickJS": "thanks\n. what's confusing are the release https://github.com/inikulin/parse5/releases after linking there it's asumed that it wasn't written so when you click on the docs when you see Version history you can also assume it's a link to releases. Version history is important enough to have on the readme even though it's in the doc\n. Parse5 is correct you don't need to merge. The code for Universal is not ensuring the property is a string when setting the attribute. The bug showed up when we started to convert DOM properties to attributes since users weren't realizing that they should be setting attributes for DOM APIs.\nsee fix https://github.com/angular/universal/pull/567 in Angular Universal\n. ",
    "ivan-kleshnin": "\nI personally would opt for speed. Are there situations where you parse huge files and have limited or low memory?\n\nI would opt for memory because if I want speed I use non-sax parsers which are both more convenient and more performant. Stream machinery is generally slower (in my experience). Anyway, you can trade one for another but now we have problems with both. I'm not well aware of HTML5-compliant parsing specifics so I can't really advice anything.\n\nIn my application I only need speed and have plenty of memory. Wondering what situations people need low memory footprint.\n\nWhen they run lots of scrapers in parallel, for example. A speed is not an important thing for scraper because you throttle requests anyway. As there are tons of use cases for HTML parsing I can't really talk for everyone.\n\nConclusion: if you are going to parse 112Mb HTML files in production then I think htmlparser2 is the way to go. If not, then consider trying benchmarking with file sizes that you expect in real life scenario.\n\nI made this benchmark after (and because) I benchmarked production scenarios and got higher memory usage with streams (Parser5.SaxParser) than without streams (Parser5.DefaultParser). That suprised me  because I expected to get the opposite results.\n. > This new info changes things drastically. Can you provide benchmarking info for parse5.SAXParser vs parse5.Parser?\nI'm not 100% sure about this one. My original use case included many libraries so I may be wrong in result intrepretation. I'll try to prepare a testbench. Maybe tomorrow, I don't have enough time today.\n. ",
    "rachardking": "node = {\n    childNodes:[\n        {\n            nodeName:'#text'\n            value:'test'\n        },\n        {\n            nodeName: '#document-fragment'\n            childNodes:[]\n        }\n    ]\n}\nwhen serialize, it can't serialize the fragment and the fragment's childNodes\n. but in  your dom operate, like this\n```\nvar appendChild = exports.appendChild = function (parentNode, newNode) {\n    parentNode.childNodes.push(newNode);\n    newNode.parentNode = parentNode;\n};\n```\nis doesn't consider the fragment node\n. but  in real dom, it's can do\n. ",
    "gameboyzone": "@inikulin \n. Thanks @inikulin.\nI figured the difference between DOM parser and SAX Parser with a bit of reading and got the fundamental point - it's an event driven parser. Guess the project Wiki can include a few links to these concepts for beginners. \n. ",
    "songsiqi": "@inikulin Any XML parser recommended?\n. @inikulin Ok\n. But I have to use ch.charCodeAt(0).toString(16). \nIf the html code is\n<text>&#xe606;&#xe605;&#xe604;&#xe603;&#xe602;&#xe601;&#xe600;content\u5185\u5bb9</text>\nHow to parse the value?. I want common ways to get text node values, which contain common characters, icon font, html entities, and so on.\n<text>&#xe606;&#xe605;&#xe604;&#xe603;&#xe602;&#xe601;&#xe600;content\u5185\u5bb9</text>\ntext value { nodeName: '#text',\n  value: '&#xe606;&#xe605;&#xe604;&#xe603;&#xe602;&#xe601;&#xe600;content\u5185\u5bb9',\n  parentNode: ...\nOr some other ways to get the text value &#xe606;&#xe605;&#xe604;&#xe603;&#xe602;&#xe601;&#xe600;content\u5185\u5bb9, not only using particular method to parse icon font.. ",
    "pedro93": "Right now, unless written by each developer, there is no way (in so far as I know) for a user of this library to get information solely about a node (without parent, sibling and children information), a level-0 obj serialization.\nI just thought that such a method (node.content()) would be helpful for developers to debug or maybe just filter-out alot of \"needless\" information. \nWhether this method should be in the tree adapter or not I do not know, I merely suggested it to be there as most node-manipulation and data-retrieval functions are there.\n. Im my case I am creating a DOM diff tool.\nDuring my algorithm's execution it modifies the AST in recursive fashion which leads to difficult debugging. \n- Adding a breakpoint in a function which has to execute 10s of times is not helpful.\n- Playgrounds don't help because the AST is quite big and modified during execution\n- I merely want the top level information of a node, no circular serialization required.\n- I agree that depending on the project, different information needs to be logged, hence my question about a wrapper and not a fixed function.\nI have come to realize that more often than not, I need to log some nodes in order to validate some internal information. This information resides at the node level, I have created a logger function specifically for this reason:\njs\nNodeManager.prototype.getNodeContent = function (node) {\n    if(treeAPI.isCommentNode(node)){\n        return node.data;\n    }\n    else if(treeAPI.isTextNode(node) ){\n        return node.value;\n    }\n    else if(treeAPI.isElementNode(node)){\n        return {type: node.type, name: node.name, attribs: node.attribs };\n    }\n    else\n        return node.type;\n};\nSomething like this is likely to be useful not only to myself but others. I am of the opinion that something like this or a wrapper, even if requiring a user created logging function would be useful\n. Thank you for the information. I will look into the libs you referenced \ud83d\udc4d \n. I did not understand how to use it with a documentFragment, could you exemplify?\n. Thank you very much!\n. I was hoping there was a simpler solution, thank you for the quick answer @RReverser \n. Just as an afterthought, are you aware of any html parsing lib which does not operate in this way, that does not attempt to fix the html?\n. ",
    "deckar01": "I'm not sure what the strategy should be here. My intuition is that any maps that are exposed by the API should be fixed, but that seems like a recipe for incompatibility. This is not a common pattern, so it will be difficult to document and support.\nIt almost feels like there needs to be a Dictionary class that adheres to the Object interface, but implements Object.create(null) as the underlying storage mechanism.\nIn any case this is effectively a major revision that breaks backwards compatibility.\n. If anyone is accessing functions from the object prototype on those attribute dictionaries, they will have to migrate to Object.prototype.call().\nExample: https://github.com/tmpvar/jsdom/pull/1444/commits/cb5addd666ccfde4f85c2b00f1f64363da8a6cd5#diff-563a86e009c6f9fbb49da85d81ad9c3bR246\nThis technically is part of the interface currently, but I understand the argument that it could be considered undefined behavior, therefore subject to change in a patch or minor revision.\n. Thanks for working on this @inikulin.\n. ",
    "noahehall": "HI @inikulin \nThanks for the quick reply\ni'll input some code on the next comment\n. Seems gitlab CI has an issue with returning a string,\nvar thisChart = helpers.chartData(cv[0],cv[1],cv[2]);\nvar document = parse5.parseFragment(thisChart);\nwhen I console thisChart its undefined, hence the error with parse5\n. Yes!\n. Just for closure, there was a module 'numeral' that was inside 'Dependencies' in package.json, but not inside 'DevDepenencies', causing a module that used it to fail\n. ",
    "Supamiu": "I'll send a PR to DefinetlyTyped then, thank you for your answer @inikulin.\n. Issue created on DefinetlyTyped: https://github.com/DefinitelyTyped/DefinitelyTyped/issues/9235\n. So it's basically impossible to preserve uppercase in html attribute using parse5? I really need this feature for html editing (adding attributes to some tags for i18n purpose).\nEDIT: Oh nvm... closed.\n. @RReverser \n\nIt has proven itself reliable in such projects as jsdom, Angular2, Polymer and many more.\n\nsource: README.md\nI thought you were the parser used by angular2. I'll start looking for their parser then.\n. ",
    "fictitious": "For current typescript version (2.0), they recommend to distribute type definitions in the same npm package. Typescript will use types field in package.json to find it.  The documentation says\n\nIf your package has a main .js file, you will need to indicate the main declaration file in your package.json file as well. Set the types property to point to your bundled declaration file.\n\nFor compatibility with previous typescript versions, you might want to publish on DefinitelyTyped. From the same document:\n\nPublish to @types\nPackages on under the @types organization are published automatically from DefinitelyTyped using the types-publisher tool. To get your declarations published as an @types package, please submit a pull request to https://github.com/DefinitelyTyped/DefinitelyTyped. You can find more details in the contribution guidelines page.\n. if you type this in the browser console:\ntypeof ''\nit prints\n\"string\"\n\nif you  type this:\ntypeof null\nit prints\n\"object\"\nif you type this:\ntypeof undefined\nit prints\n\"undefined\"\nYou see, javascript is weird, and technically, null is not a string (it's also not an object but that's irrelevant).\n\nYou can argue all you want that they should do the test for null before calling your function but the fact still remains that 0x0 is a valid string\n\nWhatever is valid is decided by whoever is defining the contract for the function in question, that is, the author of the library. Some authors like to support relaxed programming style and are content to handle whatever is thrown at them, some do not.\n. The problem is, if you have react-native typings installed in the project where node typings are installed, you get duplicate identifier errors because react-native typings define the same globals as node typings do - require, console and so on.\nIt does make sense for react-native typings to define these globals because react-native is a separate platform, on the same level as node and browser are platforms. React-native developers do not target node, they don't need node typings because their runtime environment is different, and I can understand why they say that dependency on @types/node messes up their development environment.\nThe same, just less severe issue happens when you use parse5 in a project that targets browser. Typings for node are pulled in by /// <reference types=\"node\" /> and make all the node stuff available where it shouldn't be.\nThe workaround I use in my browser projects is to have my own minimal typings for parse5, and force TypeScript to use them via paths in tsconfig.json like this:\n\"compilerOptions\": {\n    \"baseUrl\": \".\",\n    \"paths\": {\n      \"parse5\": [\"./whatever/my-custom-parse5-typings.d.ts\"]\n    },\n  . > Otherwise, I don't understand how it's suppose to work\nI don't know how to solve a problem when a library needs to have cross-platform typings, and needs to reference things that have potentially different definitions for different platforms. Well I can think of a number of ways, but each would require strictly more long-term effort to maintain.\nBut the thing is, I suppose the majority of people who are having this issue don't even need to compile anything that is directly or indirectly depends on parse5 - it comes into react-native project as dependency of jest or enzyme which are testing tools, not compile-time dependencies. For them, having @types/node as dev dependency would probably solve their problem. Having it as peer dependency would just result in useless (for them) warnings. . @shepmaster to quote from that thread:\n\nGiven that breaking consumers is a worse problem than slightly-larger packages, we've made --save the default in our documentation.\n\nWell there are way more different ways of breaking consumers than typescript developers were able to foresee. Location information is not available by default, you have to set locationInfo in ParseOptions:\nparse5.parse(src, {locationInfo: true});. It's explained here https://stackoverflow.com/questions/27545757/why-is-a-self-closing-iframe-tag-preventing-further-dom-elements-to-be-displayed , see also https://www.linkedin.com/pulse/self-closing-void-elements-vs-iframe-john-correll on why iframe is not a void tag.\n\nThe end result is that spec-compliant HTML parser must ignore the slash in <iframe /> , so closing tags for <div> and <template> are still inside the <iframe> and don't really close anything.. The workaround is to leave out parse5 parts that use streams when bundling.\nHow exactly to do that depends on which bundler are you using, and which parts of parse5 API you want to use.\nFor webpack, I think it's possible to create parse5 bundle by creating customized entry point file which is identical to https://github.com/inikulin/parse5/blob/master/lib/index.js , except with last 4 lines removed or aliased to empty module (the imports that reference stream-depending parts).\n. > I'm trying to use package which depends on parse5.\nDoes that package use functionality of parse5 that depends on streams? If yes, there is not much that you can do.\nIf no, there could be a workaround. That error message at the top seems to be produced by some kind of a tool. Is that tool configurable, and does it allow for substitution of dependencies, that is, does it allow to use your own, local version of parse5 instead of the one published on npm?. Just a wild side note about TypeScript typings. They are distributed by nature - there is no central authority that recompiles everything on every submission and enforces consistency. And until no one tried to use parse5 and ReactNative together in one project, the system was effectively partitioned.\nAccording to CAP theorem, in the presence of a network partition, one has to choose between consistency and availability.\nBut with typings, inconsistency makes them completely unusable for everyone because it stops the builds, making system effectively unavailable.\nThe outtakes from this for those who use typings,  \ud83e\udd14\ud83d\udcad I think are\n\napparent success of TypeScript is a big paradox, partially fueled by illusion of consistency it provides\ntypings distribution will always be unsolved problem (partition-intolerant, or \"immature\", if you like)\none has always to be prepared to deploy local hacks to unblock the builds\n\n. @felixfbecker Not everyone who uses parse5 needs node typings as dependency, and having it as non-dev dependency is not harmless either - it was getting installed whenever parse5 was referenced indirectly (as dependency of some other dependency) in environments where installing node typings breaks TypeScript compilation. See https://github.com/inikulin/parse5/pull/226 and https://stackoverflow.com/questions/48135164/typescript-duplicate-require-error-in-types-node. > That was fixed separately when parse5 was split into core versions and one that requires Node.js\n@RReverser I don't think this has happened yet, v4 that was published on npm does lazy loading of node-dependent parts but they are still in the same package.\nOr was a split version published under a new name? If so, could someone post a link here?\nAs I understand, splitting into separate packages is still in the future, when it happens the package(s) that unconditionally depend on node will of course include node typings.. > But why the weird requirement to map the typings in tsconfig instead of automatic resolution?\nIt's not a requirement, strictly speaking. But with automatic dependency on node typings removed, does it really make sense to have parse5 typings to be resolved automatically to something that has hard-coded node typings reference?\nAnyway, it was fine with me either way -types in package.json could probably have stayed to simplify life for TypeScript developers who are targeting node. TypeScript devs who are targeting browser and ReactNative would have to use different typings - without node typings reference - and would have to override automatic type resolution anyway. Now everyone has to do that. If this is a big problem - I don't know, maybe you guys should have spoken up a few days earlier.\n. > If the desire is to support the browser usage cleanly then imo it should be split into multiple packages where the browser package does not reference Node typings or Node API in any way.\nYes this is the agreed long-term solution (see The Big Plan bit above), it will just take some time to implement. The requirement to have weird thing in tsconfig.json is a temporary workaround.. @mhegazy your recommendation is different from what TypeScript handbook says about publishing type declarations:\n\nNow that you have authored a declaration file following the steps of this guide, it is time to publish it to npm. There are two main ways you can publish your declaration files to npm:\n1 bundling with your npm package, or\n   2 publishing to the @types organization on npm.\nIf you control the npm package you are publishing declarations for, then the first approach is favored. That way, your declarations and JavaScript always travel together.\n\nAs I understand this project has followed the suggestion in the handbook. Is the handbook going to be updated?. Yes this library has switched to dynamic require  for loading node-dependent parts on demand, instead of unconditionally loading modules that require streams API.\nYes this causes webpack warnings. Previously (before 4.0) it caused webpack to bundle shim code for node streams which was often unused and unnecessary. If these warnings are critical for you, you can use previous release until this is sorted out. (There are no functional changes in 4.0 as yet compared to 3.0, apart from that dynamic loading).\nYes there is a way to tell webpack about this lazy loading and stop issuing warnings, but looks like it's webpack-specific and will break bundling for other platforms like React Native.\nAny ideas?\n. The source seems to be in the CHARACTER_REFERENCE_IN_ATTRIBUTE_VALUE_STATE handler function. The switch to this state happens upon seeing & in the attribute value here, here and here.\nI think you could try to override handlers for all three attribute value states - ATTRIBUTE_VALUE_DOUBLE_QUOTED_STATE, ATTRIBUTE_VALUE_SINGLE_QUOTED_STATE and ATTRIBUTE_VALUE_UNQUOTED_STATE and remove the special handling for ampersand.. ",
    "MeirionHughes": "@inikulin if you're shipping the d.ts files within the npm module, and those definitions explicitly use node.js classes, then the new approach is to have a npm dependency to @types/node. At least that is my understanding. \n. ... found it; I should re-read docs before asking questions. :dancer: \n. Yup can do. \n. Don't think I can PR to the wiki.\nI've PRed that jsdoc change anyhow. https://github.com/inikulin/parse5/pull/148\n. ahh; well, technically Aurelia (templating frontend) is using the browser to parse a template html segment, and then reporting that the attribute is missing. As far as I was aware Aurelia does use the native browser for its parsing. \nand it does look like the spec says it should be fine. https://www.w3.org/TR/html5/syntax.html#unquoted\nhmm... Sorry; must be something else going\n. ",
    "zcorpan": "I think this is actually a reasonable approach for parsing things you know has been serialized. Maybe it can fatal error if it hits an unexpected end tag? Also you still need to handle the same special cases as the HTML serializer does, at least - void elements, title/textarea/style/script, plaintext (for this one you need to change the serializer also to just stop serializing before </plaintext>), template, etc. But you also need to deal with foreign content, consider <title><x></x></title><svg><title><x></x></title></svg> - the HTML title has a text node child, the SVG title has an element x child.\nIf you want to make form association survive, you could, after the first parse, check each form control that is not a descendant of a form element and doesn't have a form attribute already; if it has a form owner, set a form attribute on it that points to the form element (and set an id on that form if it doesn't have one already).\nIf you want to make the document mode survive (quirks mode, almost quirks or no-quirks), that also needs something for cases like <!doctype html />.\n. Hmm maybe shouldn't try to fix plaintext, I think it's unfixable for the foster parenting case.\n<table><tr><td>foo</td></tr>\n<plaintext>bar\n. FYI https://github.com/whatwg/html/issues/2308. ",
    "jescalan": "Hi there! Coming on over from #144 to add my support for this. Working on posthtml, which is essentially a plugin-based html transform system for html, much like (you guessed it) postcss is for css. I really like parse5, it's extremely thorough and stable, and its line location info is a huge assistance for error messages from plugins and source maps. \nBut it needs just this small amount more flexibility in order to be viable for plugin authors and users.\nRight now this is a blocking issue for me, I'm working full time trying to get to a stable release. I'm more than happy to help out with the implementation here if we could make this happen faster, if someone could give me a little walkthrough of the codebase!\n. @inikulin perfect, thank you for the quick and thorough response! Just pitched in at the linked issue. Would be happy to help out as well if someone is willing to hand-hold me a bit at the beginning, just bc this is a large and unfamiliar code base.\nAlso about what you were saying with the context, would it be possible to work around the issue in the meantime by providing a different context explicitly, maybe something like an <html> element? I feel like probably not, but worth a shot!\n. Hi, just following up quickly, @RReverser would you be able to help a little? This issue is time sensitive for me, but I am willing to put time into helping \ud83d\ude01 \n. @inikulin would be amazing. even if it's just a patch for now that's ok \ud83d\ude01\nI'm looking through the code now and there's really quite a lot to navigate. I'll keep trying though!\n. Wait I'm messing with the SAXParser right now, is there any reason that I wouldn't be able to build what I'm after here out of the SAXParser without any additional updates? It seems like it handles all tags already...\n. @inikulin Great, I have this mostly built out and it's working pretty well \ud83c\udf89 Will post here when it's entirely finished. I'm running into an issue with self-closing tags though. It seems like it will only detect them if using the closing slash like <br />. If it's missing the closing slash (which is still valid html), it doesn't mark the tag as self-closing. Is this a bug, or am I missing something?\nEDIT: It's only doing this when I don't have a doctype set in the same fragment. This is still an issue for me though, as it's possible that I'll need to parse a fragment which doesn't explicitly contain a doctype. Is there a way to set the doctype manually? I don't see one in the docs... \n. @inikulin ah, i didn't really get what you meant with the void elements at first, now it makes a lot more sense. thanks for clearing that up!\n. @inikulin Ok working well with the void tags \ud83d\ude04 One more question -- I have a test in here to see if it will parse plain text that's not inside any tags, and I'm not getting anything back from the parser on this one. Does the SAXParser parse plain text, or does it need to be contained inside a tag?\n. Working now with your method of pushing a string into the stream. I was using a different way that was not, for some reason \ud83d\udc4d \n. Just as a wrap-up, did end up getting this working in the end, thanks to the brilliant @inikulin's help. Result can be seen here: https://github.com/reshape/parser \ud83c\udf89 \n. @thisconnect absolutely, but you need to be using http/2 (preferably with server push) in order for that to make sense, and not everyone has fully made that transition yet. As soon as http/2 with push becomes more standard, the include element will probably be used much less often, if ever.\n. Yeah I think you may be right there. What I'm trying to figure out, I guess, is how a user might be able to customize the page title from the page rather than the layout. It seems like perhaps wrapping the entire title tag in a block might be the best way?\nhtml\n<block name='title'>\n  <title>custom title</title>\n</block>\n. @wooorm This is an interesting idea, thanks for the suggestion! This would be a nice additional feature I think. Either way, will take the discussion back to  https://github.com/static-dev/spike/issues/31 since I think @inikulin has given his thoughts, and we're now after workarounds.\n. Makes sense -- thanks for the clarification!. Oh sorry for the late update, but yes this is being reproduced purely with parse5, and without other tools.. Ok sorry, perhaps I didn't fully understand before. To be clear, you're saying that it is the intended behavior that the parser resolves html entities within attributes, and it's the serializer's job to re-encode them?. Got it. I mean, to be honest this doesn't seems like a reasonable behavior to me, but I assume those who wrote the spec are infinitely more knowledgable and had a good reason for it.. ",
    "wooorm": "I\u2019m also in need of the lexical tree. Plus, I want it to patch that tree with automatically inserted* elements, optionally.\nI\u2019m not sure if it\u2019s possible to unlink the two, but if it is, I don\u2019t see how it\u2019s \u201cun-parse5 like\u201d to do that if the core API can stay the same?\n* What\u2019s the proper term here?\n. Thanks for clarifying, @domenic, @inikulin, that makes sense!\nBefore I start writing code to re-insert them, would it be possible that behaviour to keep whitespace in there be added under a flag, similar to positional information? I know it\u2019s an edge case but I can see others using it as well.\n. Yes, HTML reprocessing, similar to what reshape is doing.\nI\u2019ll add a comment over there, thanks for listening :)\n. @jescalan How about an attribute on elements which loads its content?\nxml\n  <title block-name=\"title\"></title>\n. Perfect, thanks! \ud83c\udf89\n. I think <path/> is just an unknown HTML element for parse5 (as it expects HTML), or browsers for that matter. Unless it\u2019s wrapped in an <svg> tag?. This could help: https://github.com/wooorm/rehype-remark. Hi! \ud83d\udc4b\nI dunno, I personally prefer plain JSON objects (we\u2019ve been using those for decades), but those with null as a prototype (Object.create(null)) are fine as well.\nIs there a reason to have startTag as the prototype of a __location? One effect is that you can do node.__location.line instead of node.__location.startTag.line, but is that intentional?. Oh, also Sytem vs -system-!. Sure thing! Would you prefer with S (as in the html spec)?. oh, as an aside, I created fixtures for all the parse errors emitted by parse5, if you need them!. ",
    "mdhaddad": "The HTML file in question is over 800 KB. How do you want that added\ninstead of as a zip file?\nOn 6/6/16, Ivan Nikulin notifications@github.com wrote:\n\nCan you add HTML code in issue instead of zip-archive, please?\n\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/inikulin/parse5/issues/136#issuecomment-224080522\n. To be honest I'm not sure why the ending tokens for body and html are\nnot being emitted on this HTML document. It is from Bloomberg news,\npresumably all their HTML article documents are like this. The\ndocument is over 800KB so I'm not sure why the ending tokens are not\nbeing emitted. I don't know where to begin in parsing it by hand to\nget rid of supposedly non-relevant stuff.\n\nI can provide the code I used for parse5 SAXParser to reproduce this\nissue, so you can verify it on this document, but I don't know how to\nparse the text of this document by hand to produce a \"minimal\"\nexample.\nOn 6/6/16, Ingvar Stepanyan notifications@github.com wrote:\n\n@mdhaddad Remove all the unimportant parts (such as text or even attributes)\nand provide minimal exampe where your issue can be reproduced.\n\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\nhttps://github.com/inikulin/parse5/issues/136#issuecomment-224120367\n. Hmm, how would I effectively calculate the function value at the mid\npoint of an HTML file? Do I just chop it in half then feed the first\nhalf to the parser, otherwise the second half? Anything of this sort\nand the HTML file would be malformed.\n\nOn 6/7/16, Ivan Nikulin notifications@github.com wrote:\n\n\nThe\ndocument is over 800KB so I'm not sure why the ending tokens are not\nbeing emitted. I don't know where to begin in parsing it by hand to\nget rid of supposedly non-relevant stuff.\n\nUsing bisection of the\ninput and running parser on each iteration. Wort case shouldn't take more\nthan ~20 iterations on input of this size.\n\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\nhttps://github.com/inikulin/parse5/issues/136#issuecomment-224207856\n. Ok I will do that (though the HTML file would still be malformed when\nI split the content in the body tag in half, etc.)\n\nOn 6/7/16, Ivan Nikulin notifications@github.com wrote:\n\n\nHmm, how would I effectively calculate the function value at the mid\npoint of an HTML file?\n\n???\n\nDo I just chop it in half then feed the first\nhalf to the parser, otherwise the second half? Anything of this sort\nand the HTML file would be malformed.\n\nJust split content in <body> tag, should work just fine.\n\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\nhttps://github.com/inikulin/parse5/issues/136#issuecomment-224285464\n. Ok I got it down to an iframe in the content of the body tag (it happened to be the last iframe before the closing body tag) that did not have an closing iframe tag. This triggered the ending body and html tags not being emitted:\n\n<!DOCTYPE html><html xmlns:og=\"http://ogp.me/ns#\" data-view-uid=\"0\"><head></head> <body data-tracker-events=\"click,pageview\"><iframe id=\"rufous-sandbox\" scrolling=\"no\" frameborder=\"0\" allowtransparency=\"true\" allowfullscreen=\"true\" style=\"position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;\"></body></html>\n. Weird. Though don't browsers have to parse iframe contents as more\nthan just simple text if they are to render them correctly? How come\nparse5 doesn't parse iframe contents as more than merely text as well?\nIs that more work than the project is prepared to handle?\nOn 6/7/16, Ivan Nikulin notifications@github.com wrote:\n\nClosed #136.\n\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\nhttps://github.com/inikulin/parse5/issues/136#event-685082521\n. \n",
    "weiweiwei256": "How to set namespaceURI = 'http://www.w3.org/XML/1998/namespace' ?    thanks.. ",
    "vigneshshanmugam": "Thanks @inikulin .Yeah this works, But this will in turn result in broken html \nhtml\n<html></html>\n<head></head>\n<title></title> Tile of the page\nCan i provide hooks to serializer and treat some custom tags in a special way? \n. just checked the code, will subclass it and provide hooks in my code. Thanks @inikulin \n. ",
    "differui": "@inikulin Thanks for replying. I thought there is already an existing tool.\n. @inikulin I am using parse5 process .cshtml docs aka Razor which is template language of .NET MVC framework.\nIn Razor  => is something else meaning.After process of parse5 the => will be replaced into =&gt; which lead to syntax error.\nSo I am asking for preventing from parse5 replacing html entities.\n. ",
    "ChadKillingsworth": "Done.\n. ",
    "meisl": "TODO:\n did this bug materialize already at the tokenization level? Regression test here is for parser only...\n if so, add appropriate regression test to packages/parse5/test/location-info-tokenizer.test.js (see #284). related to #284?. Maybe one way to approach this is to look at possible use-cases. From my side, and for now, there'd be\n have attribute names serialized with original case by default - with or without listeners to startTag. That's a workaround, and might better be solved in the tokenizer [TODO: there's an issue re this - ref it].\n normalize whitespace/indentation under <head>, but only there.\n normalize order of attributes for certain tags\n rewrite/remove certain attributes from certain tags (e.g. make URLs relative). Ideally this would be done rather concisely in the listener function; something like \"poor-man's-jquery\".\n* generally, the default should always be to keep the original, except when I say otherwise. Particularly:\n  - self-closing tags like <meta ... /> - which are allowed (but not required) for \"html void tags\"\n  - entities in both, attribute values and text nodes (encoded or not)\n  - tag \"properties\" as in <input type=\"checkbox\" checked>\n\np.s.: for the discussion at hand, what's important is the \"how\" - NOT the \"why\". But I'll try to explain, should you really want to know.. Ok, I'll try to compress my points as you suggested. Not sure though, if I manage to get along with only one sentence each...\n. It's been quite a while, so I'll have to get into this again myself.\nAnyhow, to answer your main question: it's first and foremost a question, neither a bug report, nor a suggestion (yet). I was asking about the rationale behind mentioned tests.\nFor this you gave an answer / a few hints which sound quite reasonable to me, particularly: \"only purpose ... is to check things are glued together nicely\", precise parsing tests elsewhere).\nMy point is that I have the impression, that this very reasonable goal isn't actually quite achieved.\nBut again, right now I cannot reply with anymore details.\nSo, for the time being, I'm fine with this being closed until I can come up with more concrete questions, or maybe even suggestions (I'll dig into #284 first, though).\n. I think I've found a related issue, but actually with the location info in tokens from the tokenizer, ie. a real issue.\nIt seems it's got to do with coercing character tokens of the same type (Tokenizer.CHARACTER_TOKEN, Tokenizer.WHITESPACE_CHARACTER_TOKEN or Tokenizer.NULL_CHARACTER_TOKEN) into one token - but emitting a separate, new character token whenever this character token type changes. In these cases the end location (offset, line, col) of the preceding token comes out incorrect.\nI'm still investigating, and trying to come up with a proper test case that can actually be included as a regression test. And maybe a fix. Unfortunately, the tests are organized a bit more complicated than they should be, IMHO...\n. Well, there is indeed a problem with the location-info extension, and it materializes already in tokens emitted from the tokenizer: #284.\nHowever, since this (#282) is actually talking about the location info in nodes (from the parser, i.e. one level up), and, moreover, does not contain any of the problematic cases I have found there:\nI no longer think it's related.. Hello, thx for your feedback. In fact, I had given up on this. So, after over 2 months, I'll have to refresh my memory of this myself. For now I'll say this:\nI understand your striving to break the problem up into smaller ones. However, as mentionend, I cannot really estimate the extent of the problem yet. That is, I am even unsure about how to separate (hence: whether splitting it up makes sense or not).\nI guess the section \"The test report\" above isn't clear/accessible enough, right?\nSo maybe I should give a detailed example/walk-through of one of the failures here before I start opening several closely related new issues?\nWe could use this as a test as to how the particular problem should best be described, and only then decide whether separate issues make sense or not.\nThen I need help w.r.t. how tests are supposed to be organized in parse5. I got the impression that there are several different strategies/ideas at work for this in parallel. I reckon that's just how it is / how it grew over time. That's ok with me, but I do need help as to how I should proceed with the tests for this issue (see last 2 bullet points of \"Why didn't I make this a PR?\").\nI am talking about packages/parse5/test/location-info-tokenizer.test (or, resp. in my fork).\nI'll dig into this again, and make above things & questions more concrete. But I do need to know at least what you think about this proposal. I mean within a week, say, rather than months. No offence intended, it's just that I can't afford otherwise.. I'm sorry, but I do need at least a \"make it so\" or similar.\nGiven that I will put more effort into this, promised. But without anything - I won't. Sorry.. ",
    "teleaziz": "glad you guys figured it out at universal, I still think type checking is necessary here as well.\n. ",
    "sir-valentin": "\u0421\u043f\u0430\u0441\u0438\u0431\u043e\n\u041e\u0442\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e BlueMailhttp://www.bluemail.me/r\n\u041d\u0430 1 \u043e\u043a\u0442. 2016 \u0433., \u0432 18:01, Ivan Nikulin notifications@github.com<mailto:notifications@github.com> \u043d\u0430\u043f\u0438\u0441\u0430\u043b:\u043f\nHi, yes it's possible by using browserifyhttp://browserify.org/, webpackhttps://webpack.github.io/ or any other bundler of your choice.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/inikulin/parse5/issues/156#issuecomment-250916873, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AEWQuKQl2XFMQVGRtSyR4u53JAwybcMtks5qvnXGgaJpZM4KLxXH.\n. ",
    "thomas-darling": "D'oh!\nYou're absolutely right - thanks, and awesome work on parse5, by the way!\nThe reason for my confusion was, that the debugger in Visual Studio Code, of all things, is apparently collapsing whitespace when presenting the value of a variable - including in watches, mouse-over previews, and when just executing the variable name in the console. I'll go create a bug for them instead!\n. ",
    "pik": "Yes, currently elements returned by createElement() are not serialize-able e.g. the following will fail with:  TypeError: Cannot read property 'length' of undefined\nes6\nconst adapter = parse5.treeAdapters.default\nconst document = adapter.createDocument()\nconst element = adapter.createElement('div')\nadapter.appendChild(document, element)\nparse5.serialize(document)\nThis is because serialize expects any initialized node to have attrs set to []. Compare to the parse method which returns a usable element right away:\n``` es6\n\np.parse('')\n{ nodeName: '#document',\n  quirksMode: true,\n  childNodes: \n   [ { nodeName: 'html',\n       tagName: 'html',\n       attrs: [],\n       namespaceURI: 'http://www.w3.org/1999/xhtml',\n       childNodes: [Object],\n       parentNode: [Circular] } ] }\n```\n. Sure it's required in the specification but that doesn't make it a good choice.\n\nI'm really not sure why you'd rather point people to using a wrapper library which basically no-one is using, rather than adding one line to the existing code (one line which has absolutely no drawbacks!). \n. ",
    "rictic": "Thanks for the fix! Do you have an idea for when the next release stamp will be? This will fix some issues people have been seeing in our editor plugins.. Awesome, thanks!. ",
    "webdesus": "Yes, I can today or tomorrow make it.\n. I did it. And sorry about it. I had to check variables on use before PR. I saw that this variables not use in this place, but i don't know why I thought that the variable is used elsewhere code.. I did it \ud83d\ude0a. @inikulin its problem is observed only when <paper-|last tag in the document. If the document include after not finished tag another tags, problem not appears. I can try decide if you not against.. @TimvdLippe oh yes, i am not correct looked result my test. First element after not finished tag go to attribute, another tags work fine, like i said above\nmy test:\n```\nvar parse5 = require('parse5');\nvar document1 = parse5.parse('');\nconsole.log(document1.childNodes[0].childNodes[1].childNodes)\n```\ni see next result:\n{\n   tagName: 'paper-'\n   attrs: <p\n   childNodes:[\n      {tagName: 'div'}\n   ]\n}\nI think it's bug, but maybe owener have own look. ",
    "thanhtrixx": "@RReverser Sorry for my bad question. My English skill is not good. I've updated my question. Tks.. @michael-ciniawsky use can input full HTML or 1 fragment of HTML. I'll implement 1 method to indicate input HTML are full or not. ",
    "michael-ciniawsky": "@thanhtrixx Just use parse5.parseFragment('<div></div>') from the beginning ? \ud83d\ude1b parse5.parse expects a valid HTML Document and adds the nessecary tags if not present (spec compliant parser, like in browsers). In case I miss something maybe take a look at the SAX Parser aswell. ",
    "TimvdLippe": "In my testing the problem appears regardless if tags are after the unclosed tag.. @RReverser That would solve this edgecase, but that would then also break the case when we autocomplete for example <paper-></paper->?. @webdesus Hm yes I can confirm the above output. It is weird the node was not showing up in the editor-service, I will need to reinvestigate.\nSo current issue is that nodes after an unclosed tag are swallowed and reported as attributes of the unclosed tag.. @inikulin Okay thanks for the explanation, it does make sense. We will have to figure out a solution for our usecase.. ",
    "felixsanz": "@RReverser You closed the issue so fast, heh\nThe \\r is inside a <pre> tag and also inside a <code> one, so why is not respected? HTML tokenizer shouldn't touch this tags? Or it is impossible to represent that character in HTML?. @RReverser Thanks!. ",
    "tw93": "+1. ",
    "madbence": "@dimwell-git if you need something like document.querySelector, use jsdom or cheerio, they both provide a way to query&manipulate the parsed DOM tree.. ",
    "diervo": "What \"valid\" means? for example, per spec:\nhttps://www.w3.org/TR/html/syntax.html#syntax-elements\n\nTags are used to delimit the start and end of elements in the markup. Raw text, escapable raw text, and normal elements have a start tag to indicate where they begin, and an end tag to indicate where they end. The start and end tags of certain normal elements can be omitted, as described below in the section on optional tags. Those that cannot be omitted must not be omitted\n\nSo in this case its saying <p/> is not valid HTML, but as you pointed out, its on the spec for html5 defines how to parse such cases.\nSo is just that we are talking about different HTML spec versions?\nMy ask is to add validate as per the strict HTML spec, and so I wanted to leverage your parser to detect such cases.. We would be open to help on the standardization and implementing the changes if no one has a strong argument against it. \nWe are already working with @caridy and @domenic for some other HTML spec/questions stuff.\nLet me test the waters :). Yes we have been poking around with it, the fundamental problem is integration. The fact that is in Java adds some complexity to our integration scenarios. We will start working soon on the very first step to add validation into parse5 hopefully we can do incremental steps due to all of the possible parse errors and nuances. We have done a lot of work in the past on the HTML spec and added the proper error names and description. However we have never finished porting those to parse5 and validate optionally on this issues.\n@inikulin I haven't forget we own you still a bunch of work :)\nMaybe we should talk when would be a good time to collaborate again?. @inikulin Are you planning to use the same paradigm to inject the error types?\nI like an imperative API much better than the simple AOP hooks that exists today for location, which would be very hard to reason once you have more than one injection.\n. ",
    "caridy": "I think @inikulin has the right intuition here, it is not about validation, but about conforming, and if the parser can provide a report about the conforming aspect of the parsed document, that should be sufficient for developer to do:\n\nbe confident that a conforming document was parsed correctly.\nuse the report as a feedback loop to the end user (e.g.: show error/warning messages in a linter/IDE). . \n",
    "tmpfs": "@diervo FYI you can run the nu validator locally using java -jar it just requires that you have java 1.8 installed. I would like it if parse5 does generate a report that would help detect conformance errors but in my experience the w3c validator is excellent and well maintained.. ",
    "WilcoFiers": "@diervo @inikulin It's been a while. Has any progress been made on this feature?. ",
    "elgs": "So I have to wrap a string into a stream in order to use the SAXParser, is it correct?. So is there a way for your API to wotk with strings, as opposed to streams. I already have the HTML as a string, and I want to output parsed result to string, as opposed to a file stream? If the answer happened to be yes, can you show me some example code? My assumption is based on your answer saying warping strings into streams is not an option, which heavily confused me. . > @elgs Did you manage to navigate to link I gave you already two times? Because there is answer to your question and examples there.\n@inikulin Yes, but I didn't find anything talking about how to work with strings.\n\nSAXParser doesn't produce any compound results, it just raises a number of events with chunks of information about markup.\n\nI understand that. My question is about your example code. The example code reads data from a url, and writes results into a file. What I'm after is reading data from a string, and writing results to another string. I understand it might be off topic to SAXParser, but my question is regarding your example code.\n\nWhy you try to use SAXParser if you need information about nesting? This is completely opposite to what SAX parsers are usually used for. Use ParserStream instead.\n\nThat's not true. I am only interested in certain tags (path matters) of the the html, once I got them, I will completely ignore the rest of the html. For this question, I can use a stack/array to store such information, by pushing in the tag name on tag open and popping the same out on tag close. I don't think it is completely opposite to what SAX parsers are usually used for.. Essentially, your example code showed how the SAXParser piped an http response and a file output stream, and I'm wondering how the SAXParser will pipe two strings. You showed me Nodejs stream APIs, and at the same time, you gave me 'No' to the possibility to wrap strings into streams in order to use your API.\nAs a last resort, in order to use the API, I probably have to save my html string as a file, and upload it to a web server. . That is exactly my question. How to turn my string into a stream. I got this error:\nconst myStream = getWritableStreamSomehow();\nReferenceError: getWritableStreamSomehow is not defined\n    at repl:1:18\n    at ContextifyScript.Script.runInThisContext (vm.js:23:33)\n    at REPLServer.defaultEval (repl.js:336:29)\n    at bound (domain.js:280:14)\n    at REPLServer.runBound [as eval] (domain.js:293:12)\n    at REPLServer.onLine (repl.js:533:10)\n    at emitOne (events.js:101:20)\n    at REPLServer.emit (events.js:191:7)\n    at REPLServer.Interface._onLine (readline.js:238:10)\n    at REPLServer.Interface._line (readline.js:582:8)\nAha! I found the answer in your reply:\nparser.end('<div>Hi there!<div>');\nThis should be in the documentation. Thank you so much!. ",
    "FredKSchott": "@RReverser that's fair enough, I played around with DOMParser() a bit myself and couldn't find any meaningful information for shorthand vs. longhand boolean attribute notation either. \nI still believe that this is a useful feature for parsing & re-serializing an HTML document truthfully. Re-serializing isn't a goal of a browser's DOM parser so I can understand why this isn't something they've added support for.. ",
    "imingyu": "```javascript\nvar p5 = require('parse5');\nvar result = p5.parse('D1D2', {\n    locationInfo: true\n});\nvar body = result.childNodes[0].childNodes[1];\nvar transformResult = [];\nbody.childNodes.forEach(item => {\n    var element = {\n        tagName: item.tagName,\n        attrs: []\n    };\n    var onlyNameAttrs = [];\n    Object.keys(item.__location.attrs).forEach(name => {\n        var attr = item.__location.attrs[name];\n        if (name.length === attr.endOffset - attr.startOffset) {\n            onlyNameAttrs.push(name);\n        }\n    });\n    item.attrs.forEach(attr => {\n        if (onlyNameAttrs.indexOf(attr.name) === -1) {\n            element.attrs.push({\n                name: attr.name,\n                value: attr.value\n            })\n        } else {\n            element.attrs.push({\n                name: attr.name,\n                onlyName: true\n            })\n        }\n    });\ntransformResult.push(element);\n\n});\nconsole.log(transformResult);\n/\n[\n    {\n        \"tagName\": \"div\",\n        \"attrs\": [\n            {\n                \"name\": \"id\",\n                \"value\": \"d1\"\n            },\n            {\n                \"name\": \"show\",\n                \"onlyName\": true\n            }\n        ]\n    },\n    {\n        \"tagName\": \"div\",\n        \"attrs\": [\n            {\n                \"name\": \"id\",\n                \"value\": \"d2\"\n            },\n            {\n                \"name\": \"show\",\n                \"value\": \"off\"\n            }\n        ]\n    }\n]\n/\n```. ",
    "dwieeb": "@inikulin I've been following this issue. Could you elaborate? Is it a technical complication or something more complex? . ",
    "thomas-mindruptive": "Already found the solution: Since it is on(\"text\"), one knows it is text as a browser would display it and not a tag. Nevertheless it would be useful to get the original text. It would save two unnecessary conversion-steps, one by the parser itself and one by my own code.\nThanks!. ",
    "christophercrouzet": "I see, thanks for the explanation! And thanks for bringing forth parse5!. ",
    "7Pass": "@types/node is only used for development, and is not used when parse5 is used as a library. By making @types/node a devDependency:\n\n@types/node is installed when parse5 is referneced, even though it doesn't need to. A slightly longer installation time for an additional package that is not really used.\nFor my case, my project is using cheerio, which uses parse5, for some task during webpack bundling for a front end application with TypeScript. With this new update, @types/node is installed, causing typing conflict since our application is using front-end typings, not node typings.. You're correct, I did not think about since our project does not use parse5 directly. I'll close the pull request.. \n",
    "JohnGalt1717": "Null is always a valid value for a string and should always be correctly handled.  It means no entry and an escaped no entry is still no entry not an error since null is a string.\nWe're not talking numbers or something, we're talking a valid state of the value passed.\nPlease reconsider and properly handle this.. I know so because this is computer science 101.\nAll strings are terminated with null (0x0). A string with no value is simply 0x0 but still has a pointer in memory to the initialized memory location.\nUnassigned is different because it hasn't been assigned memory and thus would be an invalid value.\nStrings unlike all other primative types can contain null and only null as a valid value because ALL strings are null terminated unlike all other primatives.\nThus passing null into anything that accepts a string value is valid in all programming languages ever. \nParse5 ignores this basic principle of cs and isn't dealing with a valid state of a fully initialized variable.\nAngular breaks with completely useless and unfixable errors (without endless trial and error) as a result of this and it isn't alone in his problem.. @dominic etc.  You realize that JavaScript sits on top of c based software and is simply a scripting language controlling that c based software right? (Ie as is in the name, JavaScript is not a programming language it is a scripting language. Why?  Because it scripts the actions of other software. It does not interact with the processor directly ever.)\nSo in ALL cases of JavaScript, while you're not seeing the fact that the string is null terminated if you go and look in memory you will 100% see that it is null terminated.\nWhether it should be or not is irrelivant. And if you don't think it should be then consider that almost every language has come out with nullable types specifically because \"no entry\" is entirely valid which of course is what this is about.\nEscaping no entry is a valid request and of course the result back is no entry. In the case of angular this happens because the expression evaluates to a non string (ie false which is valid). So null is passed because there is no string to parse.\nYou can argue all you want that they should do the test for null before calling your function but the fact still remains that 0x0 is a valid string and your code doesn't handle a valid string and you're punting handling it upstream and causing bugs as a result.\n. ",
    "squidfunk": "This is actually solvable even easier. The best way would be to use union types and discriminators for type narrowing, which is already possible with the current design and which I do a lot in my code base because it allows for a lot of flexibility. An example:\nts\nexport interface DefaultTreeDocumentType {\n    nodeName: \"#documentType\"\n    // other fields\n}\nexport interface DefaultTreeDocument {\n    nodeName: \"#document\"\n    // other fields\n}\nexport interface DefaultTreeDocumentFragment {\n    nodeName: \"#document-fragment\"\n    // other fields\n}\nexport interface DefaultTreeCommendNode {\n    nodeName: \"#comment\"\n    // other fields\n}\nexport interface DefaultTreeTextNode {\n    nodeName: \"#text\"\n    // other fields\n}\nexport interface DefaultTreeElement {\n    nodeName: \"string\" // catch all remaining nodes\n    // other fields\n}\nexport type DefaultNode =\n  | DefaultTreeDocumentType\n  |\u00a0DefaultTreeDocument\n  | DefaultTreeDocumentFragment\n  | DefaultTreeCommendNode\n  | DefaultTreeTextNode\n  | DefaultTreeElement\nNow, if you check for the type, the TypeScript compiler will immediately use the correct type\nts\nconst node: DefaultNode = ...\nif (node.type === \"#text\") {\n  // TypeScript now knows that node is of type DefaultTreeTextNode\n  console.log(node.value)\n}. Oh okay, I wasn't aware of that. Thanks for pointing it out!. ",
    "ianstormtaylor": "@inikulin ah wow, thank you! My mistake \ud83d\ude04 \n. @inikulin actually I misunderstood your answer. I think this issue is still applicable...\nIf you compare these two invocations:\n```js\nvar parse5 = require(\"parse5\")\nvar fragment = parse5.parseFragment('')\nconsole.log(fragment.childNodes[0].tagName)\n// \"div\"\n```\n```js\nvar parser = new DOMParser()\nvar fragment = parser.parseFromString('', 'text/html')\nconsole.log(fragment.childNodes[0].childNodes[1].childNodes[0].tagName)\n// \"DIV\"\n```\n\nThe tagName returned natively is uppercase, but with parse5 it is lowercase.\nThe structure of the fragment is wrapped by <html><body> natively, but with parse5 it does not (hence the need for the extra .childNodes[0] in the second example).\n\nAm I still missing something?. @inikulin @RReverser thank you! Using .parse instead of .parseFragment makes sense.\nFor the tagName issue though, does that mean parse5 is not intended as a drop-in for DOMParser? I assumed from the docs that it was trying to be?. Would parse5 be open to implementing tagName as commonly known and putting the current tagName to the proper localName instead? I get the decision to use the \"less confusing\" name, but it seems like it's more confusing in the end if it doesn't match casing?. @inikulin very fair, thanks for explaining!. ",
    "plantain-00": "Can you release this as a patch?. ",
    "dantman": "Mocking the modern streams used by parse5 is non-trivial, the code involved has ballooned since that stream package hack was created. You'd end up adding 258kb of code to mock something that is not used or in any way useful in the browser solely to stop parse5 from generating an error in a module that isn't used.\nPackgers (webpack and browserify) just expect packages supporting node and browser to use the \"browser\" field to replace any modules that don't work in the browser with something else.\nThe change I'm describing is a relatively small change to work like how knex does it.\nA \"browser\" field in package.json pointing the modules for the stream interface to a noop.js module instead and a conditionals elsewhere to make sure the other code is ok with those modules' classes not being available.. @RReverser Ok, I suppose a small subset of users might have a use for it. I suppose it might be possible to get streams working in browser if parse5 switched to readable-stream.\nI still don't think bundling the huge pile of code you need for a streams implementation and all the unused polyfills it depends on for the browser when it's not used (which will be most cases) is a good idea.\nPerhaps that could be solved simply by adding a sub module like lite.js which you can require by require('parse5/lite') (give it a better name if you want) that just doesn't include the streaming interfaces (then those who don't want the streaming code can just require that instead).\nThen again, my real-world problem is that the next version of cheerio is starting to use parse5, and I can't control how cheerio imports parse5 so having a parse5/lite module without the streams may not actually work.. @RReverser parse5 actually uses the modules, so blacklisting things will just result in runtime errors instead. Also, sadly I'm in the one environment where you have very little control over the bundler (React Native).. @RReverser I'm using cheerio. Even if I could rewrite modules easily, rewriting parse5 to parse5/lib/parser wouldn't work since the two modules do not output the same structure (a single class vs a module of classes/functions). And I can't reasonably suggest cheerio should require parse5/lib/parser since then a publicly released package would be depending on the /lib/ structure of another package. But I could conceivably convince cheerio to import a parse5/lite version that leaves out the stream interface, since from what I can tell cheerio does not use the streaming interfaces.. ",
    "kopelli": "That makes sense. Was hoping to avoid traversing the content multiple times, but seems unavoidable.. ",
    "afiestas": "Then, would it make sense to merge the first head tag that is parsed?  Otherwise Information will be missed.. ",
    "riteshksriv": "While serializing the xml declaration in xhtml documents also gets converted as comments. Including the xml declaration may not be the best practice but it is allowed in HTML5 and extensively used in legacy documents. . ",
    "wichert": "@RReverser I did look at it, but that unfortunately has other issues: many React application use a lot of new ES features such as object spread that break Acorn-JSX badly. A combination of RReverser/acorn-jsx#75,  RReverser/acorn-jsx#32 and a bunch of plugins may work around, but the end result is still likely to be a bit fragile. And since for my use case I don't actually need to parse any JavaScript I would prefer to use a basic HTML(-like) parser and ignore all JS.. ",
    "huntharo": "So what do we have here?\nAnother open source developer who picks a technology they don't like (TypeScript in this case) and trying to read thin documentation as \"proof\" that they should actively do something useless to them that actively harms users of that technology.\nThanks @inikulin \nThis project is worthless and should be forked.. Note: parse5 doesn't seem to use TypeScript at all so I think it might actually be correct to simply remove this @types/node dependency completely.  But I need a project maintainer to confirm that.  If that's the case I can resubmit a pull request that removes the dependency completely.. I read the linked issue regarding documentation examples and I do not agree that it indisputably says that the approach taken is correct. \nSo let's set that aside for the moment and look and the benefits and drawbacks of each approach:\nAs Dependencies\n- Benefits: developers building Node projects would not need to installed Node types when they happen to also depend on parse5 because parse5 installs Node types for them???\nOk let's pause there. Is it really the responsibility of parse5 to install Node types for developers building a package that depends on Node?  No. It is not parse5s responsibility to do that. Any developer building a Node project would have installed the Node types long before depending on parse5, if they wanted them. \n\nDrawbacks: breaks all React Native-dependent projects that have types installed for React Native (requiring manually removing the Node types or finding a way to not depend on parse5).\n\nAs devDependencies:\n\n\nBenefits: All parse5 developers preserve their ability to use types for Node (but since this doesn't appear to be a TypeScript project I can't see how this is actually a benefit)\n\n\nDrawbacks: anyone using Node would have to install Node types unless they are using parse5 which already installs them... which is weird.\n\n\nNode made the choice to not force-install their types so why is parse5 making that choice for Node?\nReact Native and Node may be one of the few \"core\" dependencies where installing types like this is going to be a possibly conflict.\nCan you confirm my suspicion that parse5 is not using TypeScript at all?  If I'm correct then I would like to politely ask that you just remove references to types because it's causing pain for people that are using TypeScript and it's not helping anything. Thanks for working on your project and contributing to the open source community. . Additionally, parse5 does NOT depend on Node... so why would parse5 list a dependency on Node types (when it works just fine with React Native).\nPlease do reconsider this.. ",
    "sihorton": "ok, I understand.. ",
    "adius": "That's fine. It's enough for simple things like adding an element before the closing head or body tag, modifying links or changing scripts.. ",
    "hardcodet": "+1, that just broke TS with React Native for me - I can't follow the rationale; that's what dev dependencies are for, aren't they?. Hmm, let me make another example: I'm using a lot of dedicated libs when working with ReactNative. Those do have dependencies on RN, but still, they provide typing for their own types, but not the typing of RN. Since I work with RN, I already have those typings at hand anyway.\nSo if I would be working in a Node project, I'd most likely have those types installed. But I'm not, which is why Parse with its forced import messes up the dev environment for RN developers. Does that make sense?. To be taken with a grain of salt, but: I think the distinction to make is how you treat\n\nthe types for your own package and\nthe types of 3rd party packages you depend on (which are dev dependencies)\n\nIf a consumer of your package also needs the typings of that 3rd party package, those may be already there, too, or can be imported separately. In any case, it's not your responsibility to provide those (and not desirable either). That's how it seems to be with all the packages I'm currently working with.. ",
    "ljharb": "Complaints about typings are very preferable to making things harder for normal untyped JavaScript users, fwiw (altho obv it\u2019s great if both can be avoided). This is exactly right; it is not the job of a JS package to directly support type systems - the consumer of a JS package should not have to know that type systems even exist at all.\n@inikulin type defs should never be a production dependency because type checking is not a runtime feature; it's a build time feature. Type defs are identical in this respect to linter plugins and configs - they should only ever be dev dependencies.. @shepmaster I don't use TypeScript, so I'm not entirely sure what their challenges are. Regardless, this is a JS package on a JS platform in a JS ecosystem, and the concerns of \"not JS\" continue to need to take a back seat to the concerns of \"JS\".. @RReverser in my experience, putting types in DefinitelyTyped is the predominant convention for TypeScript; none of the typing systems have a good solution for package authors (that don't use types) to keep their type definitions in sync without having to understand and use the actual type system.. ",
    "shepmaster": "\nType defs are identical in this respect to linter plugins and configs - they should only ever be dev dependencies.\n\n@ljharb can you help connect the dots between your statement and the ones from the TypeScript developers, who say to not use devDependencies?. > a library needs to have cross-platform typings, and needs to reference things that have potentially different definitions for different platforms.\nIt feels like this is the core issue at hand here. From earlier:\n\nsome of classes exposed by parse5 inherit from streams which are part of node API.\n\nUsing a library that inherits from specific node packages on a different platform (such as React Native) seems like it's going to cause problems whenever you use these types. It feels as if the fact that you are getting \"compile time\" errors instead of runtime errors is a good thing.\nIt may be that this library is bigger than it needs to be and the Node-specific stuff should be broken out into a separate package. It may be that there needs to be some abstraction package for the Node-specific and React Native-specific code which this library can then consume. . > this is a JS package on a JS platform in a JS ecosystem, and the concerns of \"not JS\" continue to need to take a back seat to the concerns of \"JS\".\n@ljharb as I understand it, this isn't a problem if you aren't using TypeScript, so I think the \"on a JS platform in a JS ecosystem\" part isn't valid here:\n\nUsing Enzyme in React Native projects causes TypeScript builds to fail with error\n\n. > putting types in DefinitelyTyped is the predominant convention for TypeScript\nI believe that DefinitelyTyped is used for augmenting plain JS packages with types when the author doesn't want to maintain them:\n\nWhen a package bundles its own types, types should be removed from DefinitelyTyped to avoid confusion.\n\nThe TypeScript page says:\n\nThere are two main ways you can publish your declaration files to npm:\n\nbundling with your npm package, or\npublishing to the @types organization on npm.\n\nIf you control the npm package you are publishing declarations for, then the first approach is favored. . \n",
    "amcdnl": "Thanks so much!. ",
    "makma": "Thanks for your response and suggestions but I'm not sure this works for me - my original question is little misleading - I'm trying to use package which depends on parse5.. Thank you!. ",
    "felixfbecker": "Is there a reason why the Node typings dependency can't simply use exactly the range of Node versions this module supports?. I see why you would want to not have environmental typings like node as a dependency (more like a peerDependency). But why the weird requirement to map the typings in tsconfig instead of automatic resolution?. Well, it is a requirement for everyone who uses TypeScript and wants to update to the latest version.\n@types/node defines a lot of globals so you should only really have one in your project that matches the Node version you are actually using, in a way it is a loose peerDependency (unlike a library you may depend on).\nIf the desire is to support the browser usage cleanly then imo it should be split into multiple packages where the browser package does not reference Node typings or Node API in any way.\nI wasn't following this thread or project, I just got a PR from Greenkeeper to update to the latest version and the build was broken, so I had to research what changed and how to fix it. And as said the required tsconfig change seems very odd to me, I've never seen a package require that before.. ",
    "mhegazy": "Sorry if I late to the party. Our recommendation is to only include a .d.ts file in a package, iff it is auto generated from the source of the package (e.g. the package is written in typescript, and the declaration file is generated as part of the build). For all other uses, we recommend hosting the declaration file on DefinitelyTyped. The DefinitelyTyped process now allows for a change to a declaration file to make it to @types package within an hour or so. The @types package also already have a mechanism for handling dependencies, version updates, TS compiler version changes, etc.. \n@inikulin I do not believe  the solution outlined in this list is the best path forward. It is a breaking change, but still does not allow the declaration file to be hosted in DT. seems like this is the worst of both worlds. if you are willing to take a breaking change, i would say remove the file altogether, and host it on DT instead. . Thanks for the pointer, should be updated now (https://github.com/Microsoft/TypeScript-Handbook/commit/884e4e80f0868a2057234796f6a2076e27582d8d).. ",
    "Enngage": "I'm not sure what would be the best course of actions for me though... I have a client library that needs to be run in both in and outside node.js environment (so I believe I need to keep using 4.0 for that), but I'm a bit worried of possible side effects of the dynamic include warning. Currently our clients also see the warning and it seems unprofessional and just hiding it seems 'wrong'.. Thank you for the response! Sure, I understand. I do have a small question if I may since you way more familiar on the topic then I am. I've managed to fix (or hide?) the warning by using following webpack plugin:\nnew webpack.ContextReplacementPlugin(\n/(.+)?parse5(\\\\|\\/)lib(\\\\|\\/)(.+)?/,\npath.join(__dirname, 'src'), \n{} \n)\nDo you think this will have any side effects in node.js / browser environments or certain builds (AOT..)? From what I've seen so far this seems to work ok, but one never knows.. Okey, thanks a lot! We'll switch to webpack for our builds then.. ",
    "suuuuuuuuperman": "I mean when i use parse5 to parse \"input tag\" like <input class='inputtag' />, it will just become \"input tag\" like <input class='inputtag' > and lose it end tag />.  Is there some good way to help me to solve it?. ",
    "chriseppstein": "Nevermind, I got confused between the changes in 4.0.0 where the types field was removed from package.json and 5.x changes that are in-flight.. ",
    "Jocs": "@RReverser is there any way that does not convert escaped character to normal character in parse5?. ",
    "kurtinatlanta": "We do npm install, then, since we're using TypeScript, we import what we need and go.\nWe are using Webpack.\nBut we don't expect to have to transpile libraries in node_modules that we get via npm install. We expect they have already been transpiled and are ready for use directly. If that's not the case here, then we'll either have to stick with 4.0.0 or fork and build this code on our own.. Unfortunately, we can't dump IE11 (which will never be fully ES6) yet. If that's not something you care about, that's fine. I missed that 'limitation' when we upgraded to 5.0.0.. ",
    "ericmorand": "Awesome! Thank you.. ",
    "mlynch": "Thanks! Doing some tweaking of the parser for Angular templates and figured this was just not documented. \ud83c\udf7b. ",
    "evilebottnawi": "@RReverser What is workaround? How i can solve this?. @RReverser I'm not suggesting change parser. Why not provide rawData property for text node?\nLooks a lot of people want this. And just closing the issue does not seem ethical in relation to them.. @domenic \n\nSometimes it seems like people don't really understand what a parser is.\n\nWho?\n\nThis is inherently a lossy transformation, by nature of what canonicalization means.\n\nDisagree. You can parse and lose information and you can parse and don't lose information, it is your choice. First of all, the parser is written for other developers and help them, not create additional barriers.\n\nThe whole point of a parser is to get rid of these differences in input string format, and produce a single canonical representation of the actual structure that you'll operate on.\n\nI do not propose to change anything, just allow people get raw text without manipulation with original source code.\nIdeal program often differs from the program from real world. In real world you have difference technologies/libraries/tools/etc stuff and spend most of your time working on that they work well together. And this often requires additional lines of code and I've never seen a problem with it, we're all trying to create something good and help each other. . @RReverser yep, but we don't have &euro;, &#8364; or &#x20AC; :disappointed: Only euro symbol, we can use same canonical format if we have original source text.\n@gabrielmaldi we have original code and location and we can get original text in text node, but I would not like to do such hacks :disappointed: . ",
    "gabrielmaldi": "I understand Domenic's rationale.\nLet me just add what the intended use case for this would be: using the parser in order to format source code; in this scenario, you don't want to modify what the developer wrote.\nI get that it may be out of scope, but for example Roslyn provides source code location for each node. This would provide a workaround: read the file again at the given position and retrieve the original text.. ",
    "karlhorky": "\nfor whatever reason you want to encode some symbols back to entities, you could do that as well from the parsed text without needing the original raw representation\n\n@RReverser: I think what @evilebottnawi means is that this is not possible with the parsed text, because, for example, for the parsed text of \u20ac, the original representation may be either &euro;, &#8364; or &#x20AC;.\nThis is understandable from the perspective of a code formatter such as Prettier, which does not want to change the non-formatting related code that the user has written.\nNot sure if it fits the goals of the parse5 project though.. ",
    "StarpTech": "@inikulin thanks for the fast feedback. I tested it and you're right but I think the parser should handle it in a different way. It looks dangerous and wrong when the side effect is that a script tag is able to create randomly new text nodes.. The browser behaves in the same way \ud83d\udc4e so it's the way to go thanks.. ",
    "jinjilynn": "i found that the file mixin.js  in until  folder didn't compile into the es5,so it occur the problem ,only  modify the config file  can solve it. ",
    "tomas-mechura": "+1. ",
    "ijandc": "\nimg is a selfclosing tag ,but  in the object ,there is no any signs. @inikulin ok,got it. "
}
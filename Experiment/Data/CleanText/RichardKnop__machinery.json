{
    "vanhalt": ":(\n. Didn't read the whole readme... :) \n. ",
    "vjeantet": "Maybe one of you can use it to remove this sensitive information :-/ ? And change password ?\n. ",
    "RichardKnop": "This is needed for TravisCI to work. The access token is for public repos only, so it should be safe. I didn't find any other way to make TravisCI work with go get :(\nIf you guys know how to make go get work reliably on TravisCI without this hack, let me know please.\n. Thanks. I used environment variable for that ;)\n. Hi,\nThanks for spotting that. I have pushed fix to master. It should work now for these basic types:\n- int, int8, int16, int32, int64\n- uint, uint8, uint16, uint32, uint64\n- float32, float64\n- string\nThis is quite early stage project so I expect there to be more bugs.\nI also need to add support for slices and maps so you can use them as task arguments.\n. :+1: \n. :+1: \n. @m0sth8 I have added setters and getters to Server. Is this good enough for testing?\nhttps://github.com/RichardKnop/machinery/blob/master/v1/server.go#L49\n. My honest opinion is it is not ready for production environment. I have not yet used it on any production server and I would probably not recommend it.\nI have been quite busy with work couple of last weeks/months so I haven't worked on the codebase that much. It would help if somebody else contributed as well which would help move this project towards production readiness sooner.\nThere are two priorities I'd say. Sorting out dependencies via vendoring (maybe something like godep?), and finishing chords (a group of parallel tasks with a callback).\nRichard\n. @ishail I have spend some serious time finishing the remaining priorities. Chords are now supported, dependencies are managed with godep. Also fixed couple of bugs. It's definitely closer to production ready now :)\n. @denkhaus Thanks. I have merged the pull request. Also added a unit test and a section to README.md with the list of currently supported types.\n. @denkhaus No worries. Yes, feel free to contribute more :)\n. Hey,\nI am not very familiar with Sidekiq. If you can help me document how it works that would be great. Getting this useful for a broader user base is something I would like to do.\nGetting other people help would also be great as so far this has been a one man project.\nDon't worry about your Go skills. I am a generalist myself although Go has been really growing on me recently.\nI have mainly been inspired by celery when creating machinery. Not sure you are familiar with it, if not, check it out. It's one of my favourite Python libraries.\nOne thing to consider is Python and Ruby are interpreted languages so there are some very neat things you can do with them (like decorators on object methods or auto discovery of tasks) that are not possible or difficult to achieve in Go. Go has other advantages though.\n. @spinx Using memcache as a queue is probably a bad idea. I recommend RabbitMQ as a top choice followed by Redis. There is also disque which is interesting but I haven't added adapter for that yet.\n@mirzac True. But it doesn't seem to support workflows. Also only supports Redis, I find AMQP much better for a message queue solution.\n. @blalor This is definitely on my TODO list.\n. Something like celery monitor would be ideal. I will eventually get around to do that. But lot of other things that have higher priority right now.\n. It shouldn't be slow. I will have to test it over the weekend. All SendGroup does is:\n1. Sets state of all tasks in a group to pending.\n2. Sends all tasks to the broker.\nBoth steps can actually be parallelised with goroutines I guess. I will need to look at where the bottleneck is.\nThe group is just a wrapper used to send multiple tasks at once (plus also used by chords).\nWhat broker and backend are you using?\n. After I merged the last PR, SendGroup should send tasks in parallel using goroutines. Should be faster now.\n. Fixed.\n. Yes. This is something which needs to get fixed. Feel free to submit a PR if you think this is a low hanging fruit. I will take a look myself.\nYou should probably avoid publishing new tasks in the task body but still the worker needs to use concurrency properly without the need to launch multiple workers. One worker process should be capable of processing multiple tasks at the same time using goroutines.\n. I need to fix a failing integration test.\nPlus there is an issue with AMQP backend which I need to solve.\nI might get some more free time this weekend.\n. @balzaczyy Merged. Using AMQP as a result backend is a bit dodgy right now (I made it work with a clever hack but I would recommend to use Redis or Memcache as result backends if you can).\n. Fixed now.\n. Fixed with latest push to master.\n. @Requilence Hi Roman. There is no specific reason for it. You can submit a PR if you want. I'm a bit busy at the moment but performance improvements like this are needed. The way I did it was the easiest implementation but it would be much better to reuse connections.\n. Great work and investigation guys. It seems reusing channels might be the way to go.\n. @Requilence This is a bug. Thanks, I'll take a look at it.\nEDIT: Result backend should be optional for sending single tasks but I'm afraid groups and chords will not work without a way to store task states. Chains should still work without a backend.\n. @Requilence I made the result backend mandatory for now. Most workflow functionality requires a backend to store tasks states anyways.\n. This was related to another issue: https://github.com/RichardKnop/machinery/issues/27\nI chose to make result backend mandatory for now as otherwise it was easy to get errors and the documentation was a bit misleading so I fixed that too. Perhaps I should revisit that decision.\nI will be travelling this weekend so I might not be able to look at this until next week.\nEDIT: Basically, most features (sending a task and waiting for a result), chords, groups etc do not work without a result backend.\n. @balzaczyy Is your backend too customised? Ot do you think it is generic enough you could contribute it via a pull request? If it's not too specific to your use case, it can be added as another backend option.\n. @balzaczyy git pull\nIt should allow custom backends again. Sorry.\n. Thanks.\nCan you rebase against the latest code in the master branch?\nI will take a look at this soonish.\n. @vamsu Update. This has been fixed for Redis backend now. The way to fix it is using distributed locks.\nFor MongoDB backend, you could fix this bug by implementing this method: https://github.com/RichardKnop/machinery/blob/master/v1/backends/mongodb.go#L100\nSee Redis implementation for reference: https://github.com/RichardKnop/machinery/blob/master/v1/backends/redis.go#L98. Ok. Let me know if you find the cause of the problem or a way to reproduce it.\nThis is quite an early stage project so I expect there to be some issues and it will take some time to make it more robust.\n. Ok. Seems promising.\nI'm on a vacation now so I'll be online a bit less frequently over next couple of weeks.\nLet me know when you find the culprit though.\n. Found out anything?\n. :+1: \n. @wizard580 This is acceptable. Can you create a PR?\n. There will probably be a proper fix when I come back from the vacation.\n. I think this is caused by: https://github.com/RichardKnop/machinery/blob/master/v1/backends/result.go#L71\nBasically, if you call AsyncResult.Get method, it uses loop to check if the task has completed.\nYou can avoid this by not using blocking AsyncResult.Get (which is there mostly for convenience and testing/debugging) calls and just design your tasks to be completely asynchronous using callbacks.\nIt is similar to Python's Celery framework where the blocking get methods are implemented for convenience but should not be used in real applications, they rather recommend to just trigger asynchronous tasks and not wait for a result.\nFor example, if you need to do something with a result from task A, set success callback of the A task to B, which will do your business logic with values returned by B. And just trigger A without waiting for the result.\nNot sure there is a more efficient way to optimise AsyncResult.Get. Any suggestions?\n. AsyncResult.Get should not really be used in real applications, it is more of a convenience method for testing / debugging.\nJust like in Celery, you should design your tasks to be asynchronous and use callbacks to handle results from previous tasks.\n@Requilence I will test this locally in next few days and merge if all is good. On a vacation now so I might be slower than usual.\n. Merged\n. Yes, I'd like to make this compatible with Go 1.5. Will look into this.\n. @vially Can you rebase your branch against master? I merged couple of PRs and it seems there are conflicts now.\n. https://github.com/RichardKnop/machinery/pull/42\n. Thanks.\nI have created a PR with a proper fix here: https://github.com/RichardKnop/machinery/pull/40\nSo I will close this one for now. Thanks a lot for contributing though.\n. @wizard580 This is a proper fix for the issue you had.\nAnother solution (perhaps better) would be to use different queues for workers designed to process different tasks? See config.DefaultQueue parameter.\nYou could have bunch of workers processing task A on queue A and more workers listening on queue B with registered task B.\n. Thanks :+1: \n. Hi,\nFirstly, thanks for contributing!\nCI fails. I tried to re-run the Travis build but tests still fail. I will try looking at it more closely over next couple of days.\n. @TMaYaD See this PR: https://github.com/RichardKnop/machinery/pull/48\n. @rvera Hi. I'm quite busy with another project at the moment so might not have time to look at this in near future. If you want to refactor the Redis adaptor, feel free to issue a PR :)\nIt might be as simple as changing LPOP to BLPOP here: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L90\nBut it requires testing.\n. @rvera Although there is a reason why BLPOP is used. Using LPOP means multiple workers can process a task at the same time which would require tasks to be idempotent. I didn't want to force people to define idempotent tasks and instead make sure in the library each task is processed just once, hence BLPOP. If you have a different solution which works, feel free to submit a PR.\n. @roee-indegy Do you want to create a PR? :)\n. This should be solved by this PR: https://github.com/RichardKnop/machinery/pull/61\nIf it is indeed solved, I will close this issue. Let me know if it works for everybody.\n. @F21 Feel free to submit a PR that adds a Kafka broker. At the moment I am busy with another project so it is not likely I will have time to do this myself.\n. @F21 I am closing this for now but am open to a PR adding new broker.\n. Disabled RabbitMQ integration tests on Codeship for now but a proper solution would be preferable.\nIt seems the intermittent issues happen with Go 1.4, I haven't seen it happen with Go 1.5.\n. Fixed.\n. @randomandy @joeblew99 I am closing this for now but am open to a PR adding a new broker (such as NATS).\n. @roee-indegy Hi. Yes Redis has been the second broker after RabbitMQ and the implementation is a bit quick and dirty (leaving a lot to be desired). If you think this is relatively simple to fix, you can submit a PR. I'd be happy to merge it!\n. @roee-indegy Does this fix the issue?\n. @roee-indegy Alright, sorry. I will look into this more closely if I get more free time but I will be quite busy next few weeks so I can't guarantee it will be fixed soon. If you have a fix for it already, submit a PR :)\n. @ghost I am closing this for now. If you have managed to find a way to fix this please open a PR. I will reopen this if other people encounter the same issue.\n. @niedbalski Thank you for PR :)\n. @parhamdoustdar Can you open a PR and see if tests pass? It seems good on first glance.\n. Thanks :+1: \n. @parhamdoustdar Sure.\nBut if you are going to use just pure Go then there is no need for machinery at all.\nThe advantage of something like RabbitMQ or Redis is you can have workers running on multiple servers and distribute your tasks to be processed on many machines. This makes horizontal scaling easier.\nYou can also make your architecture more decoupled and run different tasks on different servers configured specifically for such job.\nThe problem is, if you want to pass messages describing tasks between different servers, you have to do it in some common format and JSON is probably the best one for that.\nJSON obviously limits what sort of arguments you can pass around (no objects or channels, for example). In my view, any public facing API should only accept simple arguments. You can have private methods that accept more complex structures such as channels but I would keep all public methods simple, for example pass a primary key and fetch the object from the database, do not pass an actual object.\nI'm not sure there would be a point in doing this as you could just implement something for you in pure Go and not use this library at all if it is an overkill and you don't need it.\n. @ritesh Yes there is.\nThis is how my ~/.bash_profile file on my Macbook looks:\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOPATH/bin\nexport PATH=$PATH:/usr/local/opt/go/libexec/bin\nexport PATH=/usr/local/sbin:$PATH\nexport GO15VENDOREXPERIMENT=1\nNotice the last line. It means Go will look for the dependencies in vendor directory and you don't need to use godep restore.\nOr, you can just do:\nmake install-deps\nWhich will instal godep and use it to get all dependencies.\n. Thank you!\nThe tests are failing for some reason :P\n. :+1: \n. Thanks a lot for the PR!\nIt looks good, merging.\n. Thank you very much! Looks good.\n. @ConnorDoyle You're welcome. The codeship build failed but I think it is some issue with how I setup CI for codeship. The first build after merging a PR branch into master always fails for some reason. I reran it and build passed. Travis doesn't have this issue.\n. Structs are difficult as all return values must be JSON serializable. Maps might be easier to support but only restricted to maps that can be represented as a valid JSON object. So the implementation might be a bit trickier.\n. This sounds like a bad design. Any tasks exposed via machinery should be quite simple and complex logic (methods that return, accept structs) should be private and called from the simple exposed public tasks.\nWhat I would probably do is have some sort of a Result struct, which has a corresponding table in your database. Just return the primary ID of result object. And then whatever code where you are handling result values should just take the primary key and fetch the object from the database.\nI would suggest something similar for input arguments, just passing primary keys as arguments and fetching from database / creating structs inside the function. This should keep the public API clean and simple. Bulk of the business logic can be inside private functions / methods.\n. @guillemcabrera You could do something \"similar\", where you can define multiple queues for different kind of tasks. And let's say you could have 4 workers consuming the important queue and 2 workers consuming the less important queue.\nThis way you could achieve at least something close to what you need, i.e. there would be more computing power for priority tasks.\nJust have multiple workers with different config.DefaultQueue defined, one set for priority tasks, and set for less important tasks.\nThis is as close as you can get to priority queues like you are describing right now.\nI am thinking about adding support for RabbitMQ priority queues but currently I am a bit busy with another project so it will have to wait. There is no plan to add this feature in immediate future.\nOf course, if you really need it and have decided to use machinery, I am open to PRs. You could implement it yourself.\n. With AMQP you could also take advantage of different routing algorithms. There is already support for that: https://github.com/RichardKnop/machinery/blob/master/v1/signatures/signatures.go#L27\n. @vkd Can you add a small description to this PR? I will review it later this week. Looks good though!\n. Right. Sorry for that. If you want to submit a patch, feel free to do so. I'll look into fixing this myself eventually.\n. @ypyf @pbrazdil Try the latest code in master. Does it fix the issue? Let me know. Also guys, feel free to submit patches. I haven't had much time to work on maintaining this library recently.\n. On Travis I set these env variables:\n- AMQP_URL=amqp://guest:guest@localhost:5672/\n- MEMCACHE_URL=127.0.0.1:11211\n- REDIS_URL=127.0.0.1:6379\n. \ud83d\udc4d Merged\n. @randomandy HI Andy. Thanks for catching this bug!\nI'm quite busy at the moment so it might take me few days to take a look. If you want, create a PR with a fix and I will try to look at it and merge it this weekend if time allows.\n. @randomandy Can you fix the tests? Then recreate the PR. I had to revert it as integration tests fail.\n. I think in order to fix this properly we would need to have two channels:\n- error channel\n- quit channel\nWhen a call is made to worker.Quit(), capture it with the quit channel and return, otherwise send to error channel and retry (the way you are doing it already).\nAs worker.Quit() is used in the integration tests and when you removed the return statement, the integration tests now timeout as they get stuck on blocking call to worker.Quit().\n. @scotwells Hi Scott. Thanks. This actually sounds like a pretty useful feature to have. I would definitely like to implement something like this.\nFor example, one project I worked on involved workflow of background tasks involving downloading / modifying / uploading very large files (hundreds of TBs). So it would be quite useful to see progress of a file download which might take few hours, for instance. Or other long running tasks. \nI haven't had much time to work on this library lately but eventually when I will have some spare time it would be something I might look at. Also if you want to submit a patch which implements something along these lines, feel free to do so.\n. @vkd Thanks! \ud83d\udc4d \n. @kcoleman731 Could you create a PR for this? I think this would be a useful method to add to broker interface. However it would need to be supported by RabbitMQ too before it can get merged.\nIf you open a PR with your Redis implementation, I would try and add RabbitMQ equivalent.\n. @kcoleman731 I have added this to the master branch with a small change. The method takes queue parameter and is named a bit differently. See: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/interfaces.go#L14\n. Thanks for the PR.\n. @evolsnow Thanks!\n. The first use case could be supported. We could add a parameter so worker would only consume a new task if it is processing less than N tasks at the moment. That seems like it could be done.\nThe second use case (with GPU or other metric) sounds more difficult to implement.\n. @evolsnow Thanks for the PR. I will take a look in next few days. From a quick glance it looks good!\n. Done.\n. \ud83d\udc4d \n. Your task needs to return second argument, an error. I believe that might be it.\n. @mehcode Would you like to propose a PR? I agree that logging needs to be done in a more sensible way. There should be a way to configure custom loggers.\n. @ryanbaer @mehcode https://github.com/RichardKnop/machinery/pull/99\n. Merged\n. @ryanbaer @mehcode documented here: https://github.com/RichardKnop/machinery#custom-logger\n. @hryx \nI like this proposal. I had the similar problem before when adding support for Redis passwords, to support both:\n- redis://127.0.0.1:6379\n- redis://password@127.0.0.1:6379\nI was thinking at a time to possibly change Config. Broker and Config.ResultBackend from strings to some sort of a struct with multiple properties instead of supporting ever more complex connection URL formats (what if in the future Redis adds concept of users).\nPerhaps like this:\n`` go\ntype Connection struct {\n  Host stringyaml:\"host\"Socket stringyaml:\"socket\"Port stringyaml:\"host\"Username stringyaml:\"username\"Password stringyaml:\"password\"Cursor stringyaml:\"cursor\"` // perhaps rename this to DB?\n}\ntype Config struct {\n  Broker          Connection yaml:\"broker\"\n  ResultBackend   Connection yaml:\"result_backend\"\n  ResultsExpireIn int    yaml:\"results_expire_in\"\n  Exchange        string yaml:\"exchange\"\n  ExchangeType    string yaml:\"exchange_type\"\n  DefaultQueue    string yaml:\"default_queue\"\n  BindingKey      string yaml:\"binding_key\"\n}\n```\nThis would probably mean more refactoring but it would be much more flexible approach as it is generic enough to support different broker / backends without having to parse X connection URL formats.\nI would prefer that instead of making ParseRedisURL more complex.\nIf you think that's too much work, we could do something like:\n- redis://password@host:port/db\n- redissocket://password@/path/to/file.soc:/db\nSo first split by @, then by :, then by /. Could work for all cases.\n. Tbh, I think this library is still quite new / experimental so I hope most people are pinning down against a specific commit in their projects and not just pulling from master (in case there are some breaking changes).\nBut I agree, the second option seems quicker and won't break anything for now.\nI might do the config modification later, it would still be under v1, not sure I want to move to v2 unless there will be some serious changes to the library.\n. Sounds good!\n. Do you use RabbitMQ broker?\nIf yes, you could use binding / routing key to route tasks to workers in different regions.\nYou could use topic exchange type and have workers in different regions running with region specific binding keys:\n- machinery.uk_tasks.#\n- machinery.us_tasks.#\nThen you can specify different routing key when sending tasks:\n- machinery.uk_tasks.foo\n- machinery.us_tasks.bar\nYou could standardise it so machinery is the exchange_name, uk_tasks and us_tasks would be queues, foo and bar a task name.\nThe tasks will then be consumed by workers in specific region.\nI have used a similar setup before with Celery as well. So I believe the above setup should work in this case too.\nMy setup was just single region but the routing was done on service bases, so tasks were routed based on binding / routing key combo to dedicated workers for different services (login, payment, ledger etc).\nThe same concept should work for distributing tasks based on region.\nThis method is used internally to set routing keys: https://github.com/RichardKnop/machinery/blob/master/v1/signatures/signatures.go#L27\nBut if you specify a routing key on the TaskSignature it will not be overwritten so geo routing should be possible already.\nHaving said that, support for headers would be useful anyways \ud83d\udc4d \n. I see. Sounds good. This looks good then.\nYour setup is probably more complex than what I was dealing with so headers make sense.\n. This can get merged once tests are updated and code cleaned up.\n. A good concept and I agree in principal. \nThis could be supported natively by AMQP. You can schedule a delayed task. Redis broker also supports delayed tasks I believe.\nSo this could be implemented in a native way with both AMQP and Redis which is good as I am trying to keep the library behaviour consistent with different brokers / backends.\n\nYou want a store a time to execute and not just the delay as you want to guarantee that the task is run as close as possible to it, If the worker crashes the delay would start again otherwise.\n\nThis would be taken care of by RabbitMQ / Redis process so machinery worker crashing and restarting should not matter as the logic will be handled by the broker. If you schedule a task to be executed at 2016-09-25 14:30:00Z and worker comes online at 2016-09-25 14:35:00Z the task will get executed immediately.\nJust need to confirm that both AMQP and Redis delayed tasks support setting a time when to free message to be consumed and not just some delay period. \n. If it requires a plugin then I wonder how would this work if we don't want to make the plugin a requirement.\nWe need to store the execution time somewhere. So if you are using AMQP for both broker and backend this is a bit of a problem.\nYou can use a hack of storing the execution in a unique queue, something like task_id_execute_at. The message is gone once you read it form queue.\nSee these comments in AMQP backend: https://github.com/RichardKnop/machinery/blob/master/v1/backends/amqp.go#L3\nI have implemented it in the same way Celery does it (a similar Python library) but it's quite hacky.\nWhat Celery actually does they have a celery-beat process which allows you to schedule tasks.\nPerhaps machinery might need something like this. A separate executable, e.g. machinery-scheduler to take care of this.\nWhat do you think.\n. @owenhaynes Agreed. Delaying tasks should be part of the main process. If you have a solid idea how to implement this, PR's welcome!\nScheduling tasks (something like CRON) could come later and probably be a separate process.\n. @kavirajk Your proposed solution makes sense! This seems like it might not be that difficult to implement. Feel free to submit a PR if you need this fast.\n. No update from me yet unfortunately.. @owenhaynes @kavirajk @tspecht @gonzalobustos I have added support for delayed tasks. Currently it only works with AMPQ broker though.\nSee here: https://github.com/RichardKnop/machinery#delayed-tasks. @chennqqi I am not sure what is causing the error. This needs to be investigated further.\nFYI: https://github.com/RichardKnop/machinery/issues/30\nThere is a bug with chords on Redis. They don't work correctly at the moment. So could be related.\n. I see. Can you make a PR to fix this?\n. \ud83d\udc4d \n. @saggit \n\nThe PRECONDITION_FAILED is because the queue has already been declared without x-message-ttl and when trying to declare it again with a x-message-ttl, the declare fails because the arguments of the declares are different. Try deleting the queue, then declare it with a int32 x-message-ttl.\n\nSource: https://github.com/streadway/amqp/issues/60#issuecomment-18119437\n. @dokterbob I agree. Do you want to create a PR? Probably another field in config so this can be configured instead of current state when it's hardcoded to 3.\n. @dokterbob Fair enough. For many use cases interfacing directly with AMQP is better than using this library. Depends on what problem you are solving :). @owenhaynes This is a good idea. Should be part of the worker processing logic. Can you make a PR?\n. @rooty123 @owenhaynes This has been fixed now: https://github.com/RichardKnop/machinery/pull/98\n. @palanglung To implement a new broker or backend, you just need to follow these interfaces:\n- https://github.com/RichardKnop/machinery/blob/master/v1/brokers/interfaces.go\n- https://github.com/RichardKnop/machinery/blob/master/v1/backends/interfaces.go\nThe current factory is not very flexible, I agree. That is one of the things I would like to refactor.\nInstead of prefixes I would like to instead make config object more robust so you can define connection details there without custom connection string formats.\n. @mdouchement If you'd like to improve / refactor to make it easier to add custom external implementations, PRs welcome :)\n. @davidje13 If you would like to contribute, this sounds like a useful feature. PR welcome!\nI might add this functionality to filter out certain logs (perhaps based on level) in the future but not sure how soon it will be as there are other issues higher on my list of things I want to do first.\nSo if you need this quickly, I'm open to merging a PR that solves this.\n. @davidje13 You can use your custom logger to filter out certain logs: https://github.com/RichardKnop/machinery#custom-logger\n. @vamsu Perhaps. I need to think about this a bit though.\nI have a feeling sticking to basic types should be enough. What I definitely want to add is support for slices and maps which is missing for now.\nNot sure about custom structs. I feel like anything which is not natively translatable to JSON should not be supported (similar to celery).\n. @oleksandr Currently this is not possible. I would like to add this feature though if I have enough time as I think it's important. Feel free to submit a PR if you have a working implementation of worker concurrency.\n. @oleksandr Cheers. Looking forward to it!. This should be possible. You can create a worker and launch it inside your application.\ngo\nworker := server.NewWorker(\"worker_name\")\nerr := worker.Launch()\nif err != nil {\n  // do something with the error\n}\nYou could launch it inside a goroutine if you wanted to.\nHowever I would probably run it as a separate process. Imagine this library like Python's celery.\nYou would run celery workers as separate process and send celery tasks to them from main application.\n. @catherinetcai There could be some deadlock going on, I am not sure. Have you had any luck debugging this to see where the issue is coming from? I will look once I have some free time.. @gofort You are correct. The configuration is one of the main areas that needs refactoring if I get enough time.\nI was thinking of different way to refactor config. What you mention seems like a good approach. Also the format of connection strings is a bit ambiguous so I was looking at refactoring that as well.\nIf you would like to contribute and submit a PR with refactor/more sensible structure, it would help this project a lot. . Yes this is currently an issue. Tasks that don't return an argument back but just error will fail.\nYou need to return some value + error from tasks. This might not be too difficult to refactor.\nThis function will probably need to become more robust:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L231\nAs currently it makes an assumption that second returned value is an error.. @saggit @egorsmkv This should be solved in this pull request:\nhttps://github.com/RichardKnop/machinery/pull/131. I am not aware of anybody using machinery in production. I know about multiple instances of people experimenting with it in development environments.\nOf course, there might be somebody out there using it in production already :)\nIt would help the project if some companies or people actually used the library in their production environments, maybe on smaller scale to see how robust it is. \nBut I can't really strongly recommend it for production usage right now unless you are confident you could fix any discovered bugs/issues and push the fixes upstream.. @hphilipps Thanks for PR. I will take a look tomorrow and merge if all is good. Looking great at first glance though! \ud83d\udc4d . @hphilipps Sorry it took so long. I was quite busy. Merged! \ud83d\udc4d . I don't think this works on a chain level, just per task.\nCurrent workaround would probably be to use RoutingKey to route tasks to specific workers. This would work with AMQP broker though. If you are using Redis not sure there is a straightforward way to achieve this.. @marcsantiago I am afraid this will not work at the moment. Machinery was inspired by celery when I originally wrote it but they are not identical, the format of messages is different.\nYou could probably rework the library to use identical format to celery in which case it might work. Not sure that's worth the effort.\nIt might be trivial to send tasks to celery from Go just using the AMQP library and hand crafting the message format so celery understands it.. I just looked at the code and it seems retry is set to true in the AMQP broker constructor:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L28\nYou can test this by launching example worker:\nsh\ngo run examples/worker/worker.go\nAnd then turning RabbitMQ off and on:\n```sh\nlaunchctl stop homebrew.mxcl.rabbitmq\nlaunchctl start homebrew.mxcl.rabbitmq\nwait a bit to test the retry fibonacci sequence will be reset to 1 after reconnection\nlaunchctl stop homebrew.mxcl.rabbitmq\nlaunchctl start homebrew.mxcl.rabbitmq\n```\nThere does not seem to be a way to turn off the retry logic, it's just hardcoded to true.\nHowever I just looked in the redis broker and it seems that retry is always false there as in the constructor it's not set:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L35\nSo a quick fix would be to set retry to true in NewRedisBroker function.\nThis could possibly be made configurable. Although I don't think anybody would ever want to disable the connection retrying (as your workers would die in case of network problem in your infrastructure which is bad). So perhaps we should just make sure it is hardcoded to true also in the Redis broker.\nThoughts?\nI have mostly used machinery with RabbitMQ broker so that one is probably better tested. Redis was added based on the interface I came up with for AMQP but it is less tested so there might be small issues like this.. Sorry for this issue. For quick fix I suggest, just set retry to true in NewRedisBroker:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L35\nThat should prevent worker from quitting if there is problem with Redis connection.\nDo you want to create a PR for this?. Sure, submit PR and I will merge it.. @sergebezborodov Hi,\nSorry for late reply. Currently there is no way of doing it.\nI am thinking about whether this is a good idea to implement such feature. I am not sure accessing server instance inside tasks is a good practice / idea.\nThis could lead to deadlocks within tasks potentially. I know you can do this with celery but it is advised against as it's a bad practice.\nIf there is a clean way to implement this, perhaps it could be added as optional feature.\nYou might be able to achieve something similar using workflows.. @kartlee There isn't a way to configure machinery to retry failed tasks at the moment. You can set callback tasks (OnSuccess task will be called when task completes successfully, OnError task will be called when the task failed - and the error string will be first argument passed to it).\nIt is on my list of TODO features I would like to add. But first there are couple of small issues and bugs I would like to resolve (as well as some refactoring) before adding more features.. @kartlee @dkiser @foobargeez I have implemented a very basic retry logic: https://github.com/RichardKnop/machinery#retry-tasks. It would not be too difficult. But I guess it would require a week or two of work. You can look at interfaces in brokers and backends directories. You just need to implement them. And I would also add unit and integration tests.. I agree this is an issue. I don't have much free time at the moment so I would accept PRs dealing with this problem. . @hieven Have you managed to solve this issue? Is it still happening?. Thanks \ud83d\udc4d . There is no way to cancel a task at the moment. You would have to stop the worker process which is executing the long running task.. @macnibblet This could be added to config object so it's not hardcoded.. @macnibblet I have added prefetch_count configuration option so you can now disable it via config (or set it to a low number such as 1 so each worker will only be processing one task at a time).\nhttps://github.com/RichardKnop/machinery/blob/master/v1/config/config.go#L31. @zfsamzfsam If you want to contribute, adding NATS broker would mean implementing this interface: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/interfaces.go#L8\nYou can take a look at AMQP and Redis broker implementations to get some idea how it's done currently.\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go\nHow would you implement delayed tasks with NATS? Currently we finally got that working with both AMQP and Redis backends so any new brokers we add would have to support this feature to stay consistent.\nAnyways, if you have time to implement NATS broker I'd gladly review it.. This looks good! Will take more detailed look tomorrow.. It should be possible to start the server without backend. This line should ignore any errors:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/server.go#L32\nHowever most functions will return error without a backend as it is required for most functionality, e.g.:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/server.go#L121\nYou can try using:\ngo\ncnf.ResultBackend = 'eager'\nThis will use eager in-memory backend which is used for testing purposes.\n. @kartlee Thanks. I have merged the PR. There was a small bug which I fixed so rebase against master.. \ud83d\udc4d . @vanng822 I have pushed a small improvement so try rebasing from master.\nI have added a timer to only ping idle Redis connections every 15 seconds instead of every time you get a connection from pool:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L403\nAnd I have also added a small delay to the go routine which is looking for delayed tasks in a ZSET as otherwise it would keep running ZRANGEBYSCORE relentlessly.\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L336\nI am not sure if these changes help but give it a try.\nI might spend some time this weekend to do some profiling to see what is causing high CPU usage. You can take a look at Redis broker and backend files in case you seem some obvious problem there which could be causing this:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go\nhttps://github.com/RichardKnop/machinery/blob/master/v1/backends/redis.go. OK. I have moved the timer up. Try it now.\nThe older revision did not have goroutine which runs ZRANGEBYSCORE. This is needed for delayed tasks functionality with Redis. It's basically looking for delayed tasks whose ETA is bigger than current timestamp and sends them over to the main queue.\nSo might be the culprit. I think this issue might be solved by placing a timer/delay to a proper place.. @coli Not right now. I would like to implement a retry functionality so you can specify how many times to retry a failed task before giving up. But that's not implemented yet.. @coli I have implemented a very basic retry logic. Hopefully it helps with your problem: https://github.com/RichardKnop/machinery#retry-tasks. What broker and backend are you using? Brokers should have redial functionality already, not backends though.. @Jolly23 I am aware of this problem but am currently very busy to really look into it.\nWhat broker and backend are you using? I don't recommend RabbitMQ as backend if you want to process large amount of tasks. I would recommend either RabbbitMQ for broker and Redis for backend in that case or Redis/Redis.\nTry using Redis/Redis broker/backend combination to see if it is faster.\nThe code which sends a group of tasks is defined here: https://github.com/RichardKnop/machinery/blob/master/v1/server.go#L177\nDo you see any obvious problem with it?\nWhat I thing the problem is: https://github.com/RichardKnop/machinery/blob/master/v1/common/amqp.go\nThe way I use amqp package it keeps opening new connection all the time. \nWhere as with redis package I use connection pooling as you can see here: https://github.com/RichardKnop/machinery/blob/master/v1/common/redis.go\nSo I think what we need to do is implement connection pooling for AMQP so we reuse connection instead of opening new connection all the time because in your case it will try to open 100-200 connections to RabbitMQ at once with current implementation.\nIf you want to contribute and implement connection pooling this would be a good place to start: https://github.com/RichardKnop/machinery/blob/master/v1/common/amqp.go. @Jolly23 You can see here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L367\nhttps://github.com/RichardKnop/machinery/blob/master/v1/backends/redis.go#L317\nThat with redis it is using a connection pool. So it keeps reusing same connections.\nBut with AMQP, this:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/backends/amqp.go#L49\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L33\nand all other places in AMQP broker and backed files where it calls b.Open or b.Connect it opens a new connection which does not scale.\nIn theory all you would need to do would be to edit a single file:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/common/amqp.go\nAnd implement some sort of simple connection pooling there so it keeps a pool of connection and reuses them. So that might be the way to solve this problem for you.\nI would start with refactoring this method:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/common/amqp.go#L96\nIf you can make that method use a connection pool instead opening a new connection all the time it might help solve your problem or at least make performance much better.. @asimashfaq FYI. @Jolly23 I did a quick test with 100 tasks and I am not getting an error. Replace example/machinery.go with this file: https://gist.github.com/RichardKnop/985d00eda8375aa4596c9d545b32538d\nAnd try to run:\nsh\ngo run example/machinery.go worker\nAnd:\nsh\ngo run example/machinery.go send\nI haven't had time to do any more detailed testing but this quick test processes group of 100 tasks and I don't get the error you are getting. Hmm.. @jsm @tingxin The redis broker has not been tested extensively with clusters. If any of you want to fix this issue (maybe improve the Redis broker, perhaps there is better library for redistribution clusters), please raise a PR.\nI will probably not have time in near to midterm future to spend time debugging and fixing this so it would be best if people using Redis broker contributed fix for this :). There is a built in support for passing context as first argument to tasks: https://github.com/RichardKnop/machinery/blob/master/v1/tasks/task.go#L21\nPossibly something similar could be done to optionally inject other objects to the task. I would need to think of some way to make this generic though so you can pass arbitrary object which might not be straightforward.\nOr you could have all your tasks import a shared package which stores API client internally and reuse the same client that way?\nSomething like this:\n```go\npackage mytasks\nimport \"github.com/clstb/project/lib/api\"\nfunc SomeTask() error {\n    // this could be reusing the same internal object \n    // so all tasks would reuse the same client\n    client := api.NewClient()\n// ...\n\n}\n```\nThe API package could have something like this:\n```go\npackage api\ntype client struct {}\ntype ClientInterface interface {\n    DoSomething(arg1, arg2 string) error\n}\nvar internalClient *client\nfunc NewClient() ClientInterface {\n    if internalClient == nil {\n        internalClient = new(client)\n    }\nreturn internalClient\n\n// eventually you could implement some client pooling logic here \n// and use round robin to return one of clients from the pool\n\n}\n``. @clstb what do you think?. That is not possible right now. Not sure how to achieve that in efficient way in a distributed system like this (which runs on multiple servers). The issue is that server (worker) doesn't really know anything about client (process which sends tasks) so it has no way to pass back information. We could get some sort of alternativeasyncResult.Get()` which is non blocking (happens in goroutine and notifies a channel when it finally gets the result).\nIn the README file it is mentioned that asyncResult.Get() is something which you shouldn't really do in your code. It's mostly for testing purposes but you should design your tasks to be asynchronous so you would never need to get a result of a task in blocking way with asyncResult.Get().\nIf your tasks are designed well and are asynchronous you won't need to do that. You can use success and failure callbacks in order to react to a task completing. For a group of tasks you can use chord if you want to react with some logic to whole group of tasks completing.. @jurre That's good to hear you find this project useful. \nThe delayed tasks are getting picked up here: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L297\nAs you can see the connection is taken from the connection pool outside of the for loop. Then it enters the for loop and it might take a long time to return from that for loop so perhaps that's why connection hangs.\nI'm going to merge this PR and hopefully it solves your issue. If not, perhaps another line we could tweak is here, maybe decrease the ping timeout to less than 15s:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/common/redis.go#L33. I have also just noticed that this function uses named return parameters but the err variable is then getting overwritten inside the function so that might be causing some issue:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/common/redis.go#L33\nI might need to take a look at refactoring that method.. @jurre I have done some refactoring of this method: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L286. @zyanho I have created a patch that fixes this bug: https://github.com/RichardKnop/machinery/pull/204\nWill test it and then merge to master.. This might have been solved by this commit: https://github.com/RichardKnop/machinery/commit/bbdc469fc5ec89871a303ef2f6e33ad3f4ba89bd\nAre you running latest code?. @saggit @alieve Can you pull from master and try now? I have pushed update which should fix this problem.\n@alieve I have managed to reproduce the bug with steps you provided. I have updated code to fix this issue. Pull from master and try it. It should work now.. Have you set keepReloading to true? That needs to be set to true for auto config refresh to work.. I'm open to PRs to refactor the config. I agree it's a bit sketchy at the moment.. @saggit @baiyunping333 I have refactored config. Now it should work better. Let me know if it's good enough.. Sure there can be an open issue. I am not planning to add support for these new backends myself though so it would need to be contributed by somebody else.\nThere still needs to be more focus on making current broker/backend implementations more robust.. @michael-younkin Is this still an issue? Have you tried with the latest code? I have committed code which fixed one interrupt signal issue so perhaps this is fixed already. If not there is another bug.. I see. Will take a look later.. Not sure that is easily possible as Go is strictly typed language. In order to properly handle arguments passed to tasks via JSON message I use reflection, as can be seen here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/tasks/reflect.go\nI think it would be very tricky to support more complex types so I would suggest to keep tasks simple and only accept simple argument types (for example you can pass integer database IDs to tasks and then more complex configuration can be lifted from database).\nOne way this could be made work would be to support something like map[string]interface{} type but I'm not sure I will focus on this any time soon.. If the task is immutable then not results from previous task should be passed.\nIf immutable is set to false then all results from previous task should be prepended (not appended) to input arguments of the callback task.\nSo if it only prepends the last argument, that's a bug. Maybe it should be like this?\ngo\n            // Pass results of the task to success callbacks\n                        args := make([]tasks.Arg, len(taskResults))\n            for i, taskResult := range taskResults {\n                                args[i] = tasks.Arg{\n                    Type:  taskResult.Type,\n                    Value: taskResult.Value,\n                }\n            }\n            successTask.Args = append(args, successTask.Args...)\nIt makes more sense to prepend the results to the argument list of the callback task, no? So add them to the beginning of the list of arguments the callback task will receive.\nThat seems more natural to me like and I think most people would expect such behavior.\nSo I think that's the reason that inner cycle was constructed in such way.\nWhat do you think?\nI'm not sure when I will have enough time to take a look at this and run some tests. So if you want to fix this yourself, feel free to submit a PR.. I think making a breaking change now should still be possible even though not ideal. This library is fairly new and I would hope most people are pinning a specific commit when they use it in their projects.\nOnce I start doing proper releases (so from release v1.0.0) no breaking changes should be introduced. I think this project is getting close to thinking about making an official 1.0.0 release as it has been in development for quite a while and lately there haven't been any big changes really, mostly just refactoring and better testing.. \ud83d\udc4d . Oh thanks \ud83d\udc4d . By default, the chain passes results of each task to its callback. So in this case I assume what's happening is that your first task returns 3 results (plus error) which get prepended to the second task that is defined as a success callback.\nThis is often useful because if you have a chain of tasks (execute A, then B, then C), the reason you cannot run them asynchronously (using a group) is probably because the order of their execution matters (for whatever reason). In such cases you would probably want to pass results of each task to the next task in the chain so that is the default behavior.\nYou can disable this behavior by making a task immutable. Try this:\ngo\ns2.Immutable = true\nThis is described in the README file if you look here:\nhttps://github.com/RichardKnop/machinery#chains\nBut also here:\nhttps://github.com/RichardKnop/machinery#signatures\n(the Immutable attribute description)\nIn essence, let's say you chained these tasks:\n\nadd(1, 1)\nadd(5, 5)\nmultiple(4)\n\nWhere tasks are defined as:\n```go\nfunc Add(args ...int64) (int64, error) {\n  sum := int64(0)\n  for _, arg := range args {\n    sum += arg\n  }\n  return sum, nil\n}\nfunc Multiply(args ...int64) (int64, error) {\n  sum := int64(1)\n  for _, arg := range args {\n    sum *= arg\n  }\n  return sum, nil\n}\n```\n(notice tasks allow any number of arguments by using ... operator).\nDefault behavior would be: \nmultiply(add(add(1, 1), 5, 5), 4)\nOr written differently:\n((1 + 1) + (5 + 5)) * 4 = 12 * 4 = 48\nJust set Immutable to true for each task where you don't want to pass in results from previous task in the chain.. That's a good idea. It would perhaps be better under a separate field.. I have merged this PR. But please pull from master as I have merged another PR with a bug before (related to passing of arguments to callbacks in a chain) so I have fixed the bug in latest commit to master.. Ok I reverted it.. Thanks \ud83d\udc4d . @xitongsys I pushed a commit to fix integration tests. For some reason Travis CI doesn't run tests in integration-tests, only unit tests. Will need to look at why integration tests are skipped by CI later.. @Blogoslov I have removed that. It used to be an option to limit number of tasks a single worker can process at a time.\nIt was moved to a specific worker attribute. See here: https://github.com/RichardKnop/machinery/pull/159. @Blogoslov Sorry for late reply. I think it would be possible to do this but it would require couple of changes to how success callbacks work now. If you are ready to support this feature feel free to submit a PR.. I don't recommend using AMQP as backend but if you must, the last commit I just added will ensure the task state queues will get deleted automatically after 1 hour.. @Blogoslov No. The tasks are being stored in a separate queue (by default called machinery_tasks but this can be changed by DefaultQueue setting). So they will wait there for hours if need to in order to be picked up by a worker.\nHowever task states are stored in task specific queues (there is no other way to do this with RabbitMQ unfortunately, therefor I recommend AMQP/Memcache or AMQP/Redis combo). These expire after 1 hour but this can also be changed via ResultsExpireIn setting.\nSee here: https://github.com/RichardKnop/machinery/blob/master/v1/backends/amqp.go#L316. @hamidfzm Adding slices here: https://github.com/RichardKnop/machinery/pull/240. The prefetch_count is different to concurrency setting (the second parameter to server.NewWorker). In your case you'd want to do:\ngo\nserver.NewWorker(\"machinery_worker\", 1)\nPrefetch count is a RabbitMQ specific thing, see here for explanation: https://www.rabbitmq.com/consumer-prefetch.html. Sorry for late reply. As @zhex has said you have AMQP settings in the config so it is trying to connect to RabbitMQ. Remove the AMQP part of config if you are using Redis and it should work.. @vidmed Thanks for finding this bug. I have made a quick PR to fix it: https://github.com/RichardKnop/machinery/pull/173\nIf you want to review it, please feel free to do so.. @Rio Sorry for late reply but I have been quite busy at my new job so didn't have much time to spend on machinery.\nThere is a partial support for contexts already. Take a look at this boolean flag:\nhttps://github.com/RichardKnop/machinery/blob/64d4c50bbd0e943e823596645c04ce98ca1c1830/v1/tasks/task.go#L21\nTherefor you might be able to use contexts already. You would need to define a task function with first argument being a context object and then in the signature have first argument of type context.\nThis should then be propagated and a new context.Background() object should be passed to the task when it will be executed.\nhttps://github.com/RichardKnop/machinery/blob/64d4c50bbd0e943e823596645c04ce98ca1c1830/v1/tasks/task.go#L72\nThis was a PR / contribution by somebody else so I am not 100% certain how it works tbh. Give it a try and let me know if it's sufficient.\nIf not, perhaps you can refactor / change this functionality to better support your use case. \nThe issue might be machinery is processing tasks on any number of remote servers so there is no way to pass context directly from where you \"send\" the task as that information is being serialised to JSON and sent over RabbitMQ / Redis to workers that are remote machines.. @seonixx Do you want to pass a specific context to a task at a point of sending it? This is difficult because machinery is distributed system so tasks are run by different processes / servers.\nWe could add support for context arguments but I am not 100% sure if contexts are JSON serialisable in a useful way (to at least keep the data associated with them). I would need to experiment with this.\nAnother way would be to allow passing specific context from worker instance (perhaps define a function to construct context for a task) but again this would be done on a remote server or in different process so not sure it's that useful.\nI'm open to PRs to improve this area if people see some straightforward ways to allow passing contexts in a useful way to tasks running on remote servers. Testing marshaling/unmarshaling contexts as JSON might be a good first step.  . There are several types of workflows supported: https://github.com/RichardKnop/machinery#workflows\n\nGroups (send N tasks at once, executed in parallel)\nChords (same as group but after all tasks in a group complete a callback can be triggered)\n3 Chains (execute A, then B, then C)\n\nIt depends on how you do the batch processing. You could use groups.. Yes, the current logic for recovering from panics is here: \nhttps://github.com/RichardKnop/machinery/blob/master/v1/tasks/task.go#L54-L68\nIt would need to be made customizable which it currently isn't. It might not be too difficult to add this functionality.. @clstb I haven't done much performance testing so it is quite possible there is some low hanging fruit that could be used to increase performance and reduce the overhead. It would require more time to investigate and research than I have right now though.. It is a concurrency setting to be used when sending group tasks to the broker. Previously they were sent in a for loop but now you can specify how many to send at the same time in goroutines.. @surendratiwari3 Done. Sorry. Had to revert this commit as it doesn't work. Integration tests are failing for some reason.. Do export MONGODB_URL=127.0.0.1:27017 and then make test. Integration tests only run if you have appropriate environment variables set. See here: https://github.com/RichardKnop/machinery#testing\nI get the following error:\n```\n--- FAIL: TestSetStateSuccess (0.00s)\npanic: interface conversion: interface {} is float64, not string [recovered]\n    panic: interface conversion: interface {} is float64, not string\ngoroutine 221 [running]:\ntesting.tRunner.func1(0xc42026ce10)\n    /usr/local/Cellar/go/1.9/libexec/src/testing/testing.go:711 +0x2d2\npanic(0x141d4c0, 0xc42005b3c0)\n    /usr/local/Cellar/go/1.9/libexec/src/runtime/panic.go:491 +0x283\ngithub.com/RichardKnop/machinery/v1/backends.(MongodbBackend).SetStateSuccess(0xc420222b60, 0xc4200c4240, 0xc420114138, 0x1, 0x1, 0x10ebc7d, 0x15862f0)\n    /Users/richardknop/code/go/src/github.com/RichardKnop/machinery/v1/backends/mongodb.go:147 +0x6e8\ngithub.com/RichardKnop/machinery/v1/backends_test.TestSetStateSuccess(0xc42026ce10)\n    /Users/richardknop/code/go/src/github.com/RichardKnop/machinery/v1/backends/mongodb_test.go:132 +0x22e\ntesting.tRunner(0xc42026ce10, 0x14aad10)\n    /usr/local/Cellar/go/1.9/libexec/src/testing/testing.go:746 +0xd0\ncreated by testing.(T).Run\n    /usr/local/Cellar/go/1.9/libexec/src/testing/testing.go:789 +0x2de\nFAIL    github.com/RichardKnop/machinery/v1/backends    0.252s\nok      github.com/RichardKnop/machinery/v1/brokers 0.012s\n?       github.com/RichardKnop/machinery/v1/common  [no test files]\nok      github.com/RichardKnop/machinery/v1/config  0.016s\nok      github.com/RichardKnop/machinery/v1/log 0.007s\nok      github.com/RichardKnop/machinery/v1/retry   0.011s\nok      github.com/RichardKnop/machinery/v1/tasks   0.012s\n```\nSo I can't merge this currently as it's broken.\nYou can also run tests inside docker-compose if you don't have locally running Mongo/etc:\nmake ci\nThe thing is the MongoDB backend has been contributed by somebody else as I have no experience using MongoDB so I am not too familiar with the code. What I don't understand this how can this work since we only support selected types of return values from tasks. See here: https://github.com/RichardKnop/machinery#supported-types\nIf would be good if you explained this change in more detail as I don't understand it.. Yes, this is correct approach. The only attribute from Signature object inside AsyncResult needed to retrieve the task state from the backend is UUID. So you can pass in otherwise empty signature - just make sure to set UUID and you should be able to poll results.\nSee here: https://github.com/RichardKnop/machinery/blob/master/v1/backends/async_result.go#L137. You would need to do that from inside the task. The worker has no way of knowing how long until a task finishes. For example if you have a long running task (hours long, for example), you should implement some tracking mechanism (using database perhaps) from inside the task. Or perhaps split it into smaller tasks and chain them.. Currently the results of previous tasks are prepended to success callbacks. This logic can be seen here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L164\nCan you give me an example of task which requires certain order of arguments where this would be a problem?\nI am open to having this as a configuration option. You want to create a PR and add a switch to reverse this behavior and append results instead.\nHowever the default behavior should stay as it is now, otherwise it would be a breaking change for people who are already using the current codebase. So I suggest making this optional. Perhaps a new attribute of a task signature struct?. @estebangarcia @siredwin I have merged this PR. I have decided this is much more logical way to do it (append arguments to next task, not prepend). This might be a breaking functionality but better do it now than later.. How are you loading the config from file? I mean where do you specify path to config.yml?. @zyanho @yang-f This could be caused by: https://github.com/RichardKnop/machinery/pull/204\nOnce I merge into master try it again and hopefully it works.. @sstarcher @SurendraPlivo I agree this is not possible with current implementation. It could be modified to allow this behavior. It's simple with some backends (Redis, MongoDB) which are persistent. It's a bit more tricky with Memcache (probably not possible) and also AMQP (possible but really inefficient, you should use a proper database to store results, not a message queue).\nI'd suggest your tasks could save data you need to hold longterm in a database. This can be totally separate from machinery and be part of your application database. But also I don't mind if you want to submit PR to make it possible to store results longterm.. Difficult to tell. How do you configure workers? Maybe they have same tasks registered.\nThis test should verify workers only consume registered tasks: https://github.com/RichardKnop/machinery/blob/master/integration-tests/worker_only_consumes_registered_tasks_test.go#L133. Even if all workers share the same queue but have different tasks registered, if task is not registered and worker receives it, the task should not be processed and re-queued instead (and not processed until it is picked up by worker that can process it). As seen here: \nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L253\nSo not sure why that would happen. More robust way is to use different queues for different types of tasks / workers though so I suggest to configure different workers and then publish tasks using routing keys so they are routed to the correct queue.. Yes this sounds like a good idea. Basically when you quit a worker this method is being called (it's implemented slightly differently for different brokers):\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/interfaces.go#L12\nSo we will need to configure it to wait for running tasks to finish or at least wait some time period.. Just looking at AMQP implementation and it seems that should already wait for tasks to complete when it receives interrupt signal. Take a look here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L165\nThe wait group's Wait method is deferred to run before returning from the function.\nEvery task which is started will increment the wait group's count by 1: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L179\nAnd again decrement the wait group once task is processed: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L184\nSo when interrupt signal is received it should trigger the stop channel:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L196\nWhich will wait for the wait group's deferred Wait method.\nEDIT: The same solution is already implemented for Redis channel: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go. I have just ran a simple test with long running task and it seems current implementation doesn't work. So it needs to be fixed.. @makkalot I submitted a PR for this: https://github.com/RichardKnop/machinery/pull/195. \n. You can configure which queue the task will be routed to by adjusting RoutingKey parameter of a signature: https://github.com/RichardKnop/machinery/blob/master/v1/tasks/signature.go#L23\nBy default, when you don't specify a custom routing key, the task will be routed to default queue, see here: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/broker.go#L51\nIf you are using AMQP, there are more options for routing as well, you can also play with different types of exchanges like direct, fanout, topic etc.. @zyanho Should be fixed in the new release.. Yes, currently you would use goroutine. Perhaps we could add an alternative launch method which will take error channel as input. This would be non blocking and you can then handle the worker process yourself via the error channel.. @takfate What about:\ngo\nfunc (worker *Worker) LaunchAsync(errorsChan chan error)\nSee here: https://github.com/RichardKnop/machinery/pull/199\nThe Launch method demonstrates the use of error channel which you can pass to LaunchAsync: https://github.com/RichardKnop/machinery/blob/8f3d35c97f2cc0e8e2f64e3521f1f481ad4c4921/v1/worker.go#L25-L33. @alexhudici For Redis I am using this library https://github.com/go-redsync/redsync/tree/v1.0.1 for connection pooling. This should work well for master / fall back nodes. When master nodes doesn't work, workers should retry to connect to one of fall back nodes. I have not tested this setup though so cannot confirm for sure it works correctly.\nChanging variables in the configuration should be enough in theory. However remember that a worker subscribes to a queue and starts consuming from it (it's a long running process) so you'd need to restart it.. @jzhbiao This should work:\nsh\nrpush machinery_tasks '{\"UUID\":\"task_1158d38e-2afb-4fbe-a08f-9e1b11e6bfa3\",\"Name\":\"add\",\"RoutingKey\":\"machinery_task\",\"ETA\":null,\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":[{\"Type\":\"int64\",\"Value\":1},{\"Type\":\"int64\",\"Value\":1}],\"Headers\":null,\"Immutable\":false,\"RetryCount\":0,\"RetryTimeout\":0,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}'. @KarlTango Can you explain this pull request in more detail?\nI cannot merge this without understanding what's the purpose.. @vano468 Yes I think the reverting of commit was a mistake. Thanks \ud83d\udc4d . @vano468 If task throws an exception / panics during processing, that should be handled fine because panics are caught here: https://github.com/RichardKnop/machinery/blob/master/v1/tasks/task.go#L53\nBut you are correct that if the worker process is killed (SIGKILL) this would be a problem so ack should be sent before processing.. Can you try with the latest release? I have fixed a bug with AdjustRoutingKey.\nIn previous release way to fix the bug you are experiencing would be to delete / comment out AMQP related settings from config file. So get rid of this subsection of config file:\namqp:\n  binding_key: machinery_task\n  exchange: machinery_exchange\n  exchange_type: direct\n  prefetch_count: 3\nThere was a bug when binding key from AMQP settings would be used also for Redis broker (basically tasks would be pushed to machinery_task queue which is the binding key from AMQP settings, instead to machinery_tasks which is default_queue generic setting). That should be fixed now.\nLet me know if you still have issue.. Glad to hear.\nBasically, this condition was being evaluated as true before even for Redis broker:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/broker.go#L100\nChanging it so it works properly and is only applied for AMQP broker fixed the issue.. Sounds good, maybe it's enough to add signal.Reset in these two places:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L60\nhttps://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L83\nhttps://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L87\nPerhaps the code can be refactored so it doesn't need to be copied 3 times.. Hi,\nCurrently with Redis, you are correct it's not possible. Each worker will only consume via BLPOP from one queue.\nTake a look at fanout exchange type for AMQP. It doesn't do exactly what you want (have one worker consume messages from multiple queues) but depending on your use case it might be a possible solution. \nI think it's more natural to do it this way, instead of having one worker accept tasks from multiple queues, send a message that needs to be processed in a different way (e.g. new order message might need to trigger fulfillment process, sending of order confirmation email, record order for reporting etc) to the exchange and that will forward it to all workers which are interested in the message.\nThe problem with having workers accept tasks from multiple queues is I have no idea at the moment how to implement it in a way which is not super convoluted. The code would get quite messy as I would suddenly need to manage a pool of subscribe connections to Redis and/or RabbitMQ. Right now workers subscribe to just one queue which makes implementation relatively straightforward. \nTo answer your questions:\n\nThere is no way to do that currently.\nI would consider it but:\n    a) It would need to be implemented for both AMQP and Redis as I want to have a uniform behaviour for all supported brokers (don't want to have Redis behave in a different way than RabbitMQ or have more features, actually RabbitMQ was the original broker this project started with so I still consider it the primary broker and Redis as secondary).\n    b) It would need to be reasonably well done so the code is readable and possible to maintain.\nAgain I would consider it but I would have to see it. It's difficult for me to imagine right now how it would work. It should be possible to implement your own broker by implementing this interface: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/interfaces.go#L9 Not sure how it is possible to allow custom brokers brokers in any more \"dynamic\" way as Go is statically typed language. I have been playing around with plugins in order to allow dynamic registering of tasks to workers via shared objects (*.so files). Plugins only work on Linux though currently afaik so I am a bit reluctant to start relying on them.\nPlease explain how you would modify the Redis broker (e.g. what Redis commands would you use instead of BLPOP, or would you run multiple BLPOPs in goroutines to fetch messages from multiple queues?).. Thank you!. Thank you again!. @WillAbides I have had issues with integration tests running on Travis before. Never had enough time to look into more closely to make tests more robust.\n\nTests on Travis are run via docker-compose - i.e. make ci but locally I just run tests with make test as I have all services installed and running on my Mac (RabbitMQ, Redis, Memcache, MongoDB).. Thanks for the PR! This will be quite hard for me to review as I haven't used Amazon SQS.. There seems to be a conflict in v1/brokers/broker.go. Could you rebase?. I will try checking out this branch locally and running tests.. Yes the tests are passing. This seems like a linter / formatting issue. Merging this & will fix formatting in next commit. . OK. Can you rebase against master? CI should be passing now.. You are probably right. That was quite a lazy implementation, your improvement makes sense.. @joelpresence You are basically correct. The reason why Redis broker URL format is expected to be:\nredis://[password@]host[port][/db_num]\nredis+socket://[password@]/path/to/file.sock[:/db_num]\n... is because Redis does not have a concept of users (e.g. there is no access control). So user:password didn't make sense to me as there is no such thing as username in case of Redis. When choosing format for Redis URL I was following format of AMQP which was the first implemented broker but I leaved out username part for the above reason so I ended up with a custom format (a mistake when looking back).\nAll you can do is use a basic authentication mechanism to protect Redis instance with a password but you can't create user spaces such as you'd be able to do when working with MySQL database, for example.\nSee here: https://redis.io/topics/security\nI'm open to PRs to fix this so URL in format redis://user:password@host:port will be accepted (the user bit would be ignored, no matter what value there is.).\nI think that's the most sensible way to do this.. @ankurs \ud83d\udc4d . I rerun CI and it passed.. \ud83d\udc4d Thanks. @Blogoslov The current logging implementation supports different log levels. See here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/log/log.go\nSo you can define INFO, WARNING, ERROR, FATAL levels.\nBasically you can define a custom logger for every level of logs by implementing this interface:\nhttps://github.com/RichardKnop/logging/blob/master/interface.go#L5\nWhich is identical to native log interface of Golang's core library.\nThe only change that would be needed would be to allow passing custom logs per level here: https://github.com/RichardKnop/machinery/blob/master/v1/log/log.go#L21\nThis could be a small PR. Feel free to modify that file so you can have different loggers per log level.. @snapercloud One way we could implement this would be with a special error you return from the task. So let's say we could define ErrRetryTaskLater and you could return something like: machinery.NewErrRetryTaskLater(24 * time.Hour). This would push the task back to the queue and reschedule it to be consumed again in 24 hours? What do you think?. @snapercloud https://github.com/RichardKnop/machinery/pull/250. @jmcarp This is a good idea. If you want to submit a PR, feel free. Sorry for ignoring bunch of issues and feature requests but time to work on machinery has been very limited last month or two. \nHow do you propose to implement this, make the backoff function used for retrying a string argument of a task signature? And have couple of functions for different use cases? Or allow users to use custom backoff functions?\nI wonder what would be an easy way to allow custom backoff functions as retries are handled in quite a simple manner now: https://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L156. @jmcarp This is a Celery like solution which might be good enough for most use cases: https://github.com/RichardKnop/machinery/pull/250\nWhat do you think?. I thought this was fixed already. It it's still an issue then this should be open.\nIf you can try to rebase your PR against master and make sure CI passes, then it would be possible to merge it.. @duyanghao Can you rebase this against master? I have just merged another PR which changed how Redis URL is parsed.. \ud83d\udc4d . @steven-zou Eager mode is only present for testing purposes. It was not designed to really be used in real scenarios. But yes, this is a bug which could be solved.. @steven-zou Not right now. Not possible unless you implement some custom logic inside your task.. \ud83d\udc4d . @apostov Thanks for this!. @mjetpax Thanks for this PR. Could you rebase against master (I have merged couple of PRs since this was created).\nAlso make sure to add the AWS DynamoDB dependencies to vendor folder? Something like:\ndep ensure -add github.com/aws/aws-sdk-go/service/dynamodb should make the CI pass?\n. @ZhenhangTung Ok will try to restart the build. \ud83d\udc4d . Can you just remove .DS_Store from .gitignore? Then I can merge.\nEdit: actually that's fine. \ud83d\udc4d . @ItsLeeOwen Yes, for some reason sometimes one of 2 builds during CI hangs. Probably related to docker somehow (I assume dependency containers like RabbitMQ/Redis don't start in correct order or something). I will need to try to debug that sometimes. It usually runs fine second time. So if one of builds is green and another one got stuck it very likely means it's ok. Bug in make ci.. @Rio Thanks. This is great!. @WingGao I think this is because JSON treats all numbers as floating point (it doesn\u2019t have int or uint type). So the conversion from uint to float and then back to uint might be causing this. Can you verify that\u2019s the case? Any suggested solutions?. @WingGao Solution here: https://play.golang.org/p/MqOaajygFqp. @WingGao Pull request here: https://github.com/RichardKnop/machinery/pull/252. Fixes: https://github.com/RichardKnop/machinery/issues/251. This needs more information. What broker are you using? Steps to reproduce? Etc..\nJust from quick glance, with AMQP deliveries are acked after processing, so if worker crashes while processing a task, task should still be in the queue and can be consumed by other worker?\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L234-L235. @steven-zou It seems to be an issue. However I am too busy to tackle this at the moment. You can submit a PR if you have a solution for this. However, since Redis does not offer proper message queue like AMQP does I am not sure if there is a simple/straightforward solution.. @rakeshbala Basically the error you see in the logs is coming from here: https://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L204-L212\nIt does not affect functionality as it's just a debug log so task results are visible in the logs. But this is still a bug we should try to fix. . @rakeshbala This has been fixed now: https://github.com/RichardKnop/machinery/pull/257. \ud83d\udc4d . @bseenu For AMQP it is not implemented as there are technical limitations, RabbitMQ is a good message queue but is not appropriate to use for storing data, therefor the AMQP backend is just a hack which can be used for development or testing but for any real usage I recommend to use AMQP broker and Memcache/Redis backend.\nFor the Redis bug, you can submit a PR to fix it if it's something obvious.. @bseenu Right, I guess you can do that via HTTP API, not sure what information you get back. Do you get back whole message payload? \nBecause I'm not using HTTP, just communicating with RabbitMQ via the AMQP library and there was no way to reading messages from a queue without consuming them (e.g. taking them off the queue) which is why this functionality is not implemented for AMQP backend originally.\nIf you think this can work well via the HTTP API, you can make a pull request :) Also for the Redis bug which seems like a simple fix :). \ud83d\udc4d . @F21 Yes, you can update the dependency and make a PR. Should be fairly small pull request.. \ud83d\udc4d . I'd probably wait for a bit before creating a new release. Perhaps more PRs will get merged so it's not a mini release.. @bseenu I will need to run tests but I am quite confident this is not the case. You can easily test that by adding some tasks to a queue and then after that launching a worker and it will consume the tasks.\nEasiest way to test it:\ngo run example/machinery.go send\nLater do:\ngo run example/machinery.go worker\nYou can even just publish JSON payloads directly to a queue and then launch worker and it will consume the tasks.\nMake sure your routing configuration is correct. Take a look at ExchangeType or BindingKey.\nFor direct exchanges, it will use the same binding and routing key: https://github.com/RichardKnop/machinery/blob/c532b2a685fe0c35d77fca82f861845ad8b8a09b/v1/brokers/broker.go#L100\nSo it will consume tasks from the same queue as your binding key and tasks will be published using routing key which is identical.\nMake sure you understand how RabbitMQ routing works: https://www.rabbitmq.com/tutorials/tutorial-four-python.html\nAnd verify your AMQP routing settings are what you want them to be (i.e. in simplest setups you will probably want to use direct exchange type and then all tasks will simply be routed to the same queue based on the binding key and when you send tasks to workers they will also be published to the same queue).. If these tasks are long running, they won't get acked by workers until they are processed. Could it be other worker is already processing them?. @georgekarrv You can fork the repo and create PR like that.. Tasks themselves, when successful, should be acked and removed from the queue. See here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L238\nNot sure why that wouldn't happen. When I tested locally, completed tasks are gone from the queue.. If we can find out the case of this issue, I'd be happy to fix it or accept PR. Currently I don't have much time to investigate this though.. @paulxuca Thanks \ud83d\udc4d . @svanburen I have pushed a commit with updated dependencies. Give it a try now.. \ud83d\udc4d . Thanks. Merged \ud83d\udc4d . Merged \ud83d\udc4d . Just one more thing I noticed. Doesn't it need to be?\n```go\n                if concurrency == 0 || (len(pool)-len(deliveries) > 0) {\n                    task, err := b.nextTask(b.cnf.DefaultQueue)\n                    if err != nil {\n                        timer.Reset(time.Second)\n                        continue\n                    }\n                deliveries <- task\n            } else {\n                    timer.Reset(time.Second)\n            }\n\n```\ninstead? Otherwise concurrency would not work properly (e.g. if we set concurrency to 4, it would take 4 seconds to start processing 4 parallel tasks). If we add the else clause, it will start processing tasks immediately until we reach the concurrency limit and then wait 1 sec.\nWhat do you think?. Oh I see. I need to look at this closer when I have time. Perhaps using time.After would be an alternative. Just want to avoid slow picking up of new tasks when there are still parallel processing slots available.. @georgekarrv See here. I think that's better: https://github.com/RichardKnop/machinery/pull/273. @georgekarrv merged. Maybe we can make the timer duration configurable as I imagine some people might want to use different value, 1s seemed a bit long to me but if you have long running tasks it's ok. Could be part of configuration object.. You can remove it from the delayed tasks queue.. @bmarini Hi. I have actually only written support for RabbitMQ and Redis brokers and for AMQP/Redis/Memcache backend. All other brokers and backends have been contributed so I am not overly familiar with that part of the codebase.\nI will need to look at the code in more detail once I have more time (perhaps this weekend). SQS support was contributed here https://github.com/RichardKnop/machinery/pull/212 by @mjetpax so perhaps you can give him a shout to get a faster answer :)\nBut in theory I agree with you that ack should happen after processing. Not very experienced with SQS though and there might be differences in behaviour compared to AMQP.. @bmarini You can submit a PR to fix this if you want :). @bmarini I have merged a fix for this issue to the master branch. . \ud83d\udc4d . @siredwin Can you try with the latest master? I have made a change to treat NULL in case of slices as an empty slice. The panic was caused because it was expecting an empty array such as [] for the value. It makes sense to treat NULL as empty slice though.. \ud83d\udc4d . @alaminopu Hey, I have fixed this myself before I noticed this PR. Thanks!. @ItsLeeOwen Could you instead just suppress logging of the error if it's equal to sqs. ErrCodeQueueDoesNotExist?. @tailingchen Yes I think this is a bug. In the AMQP reference broker, it acks message off the queue after processing whether there was error or not. See here:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L237-L239\nThen taskRetry can republish task to the queue again to be retried if the task is configured for retry attempts.\nYou can submit a PR to fix :). AMQP should be used as a reference broker, it was developed first and was tested very well. Other brokers should be based on AMQP one as closely as possible (of course there are differences as different queues work slightly differently).\nFor example, with Redis, it is simpler, because Redis queues are not very sophisticated. There is no concept of ack/nack when consuming message.\nSo there's no need to do anything after processing: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L305\nBecause BLPOP removes task from Redis queue immediately, no need to ack/nack: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L313. Some tests are failing :o \nSee here: https://travis-ci.org/RichardKnop/machinery/builds/386241189?utm_source=github_status&utm_medium=notification. Can you rebase with master branch? There're some conflicts.. @WillAbides This should be possible without breaking any backwards compatibility. I have made a quick PR and it seems to work: https://github.com/RichardKnop/machinery/pull/299\nRegarding v2, that is quite a slow process and I'd probably rethink some design choices and rewrite internals from scratch. However given lack of time to invest in this, it might be few years before that happens. For now v1 remains what I mostly work on when I get some time.. Done. I think most people are using factory functions and those didn't need to change.. @valeriano-manassero can you rebase with the master branch? I have merged another PR today and it changed some things.. @jaipradeesh @Blogoslov That's correct. The second parameter to server.NewWorker is max number of concurrent tasks being processed by this worker at a time.. @joe-mcnuggets Sorry for that. I have updated release to 1.4 instead of 1.3.7.\nI don't think this change is big enough to jump to 2.0.. @baocaixiong Hi, could you rebase with master and remove Gopkg.toml and Gopkg.lock files? They are not used anymore.. @baocaixiong Sorry for late reply. Seems the CI build is still failing. Could you check the errors?. @heww Agreed. If you want, you can make a PR :). @ofernandezcr92  \ud83d\udc4d . @mmt7172 Done. @eldadvcita This could be done, of course. If you want, you can submit PR to maintain a connection inside the server.. @alext234 Depends on your broker. There is a GetState method that can in theory be used: https://github.com/RichardKnop/machinery/blob/master/v1/backends/iface/interfaces.go#L22\nHowever that won't work with AMQP broker right now. It would be ideal to create HTTP or RPC JSON API to expose task state. I am open to pull requests on this.. https://github.com/RichardKnop/machinery/pull/344. @speza Could you rebase with master? There are some conflicts.. @PyYoshi \ud83d\udc4d . \ud83d\udc4d . You can achieve this by chaining tasks foo and bar. Should be quite straightforward. . @herpiko In a chain each task will append it's result to the next task in chain. So your second task's signature should have empty Args as the first task will pass its result as argument to next task. Also name of task needs to be the same as task registered by the worker so change name of second task to build.\n```go\npackage main\nimport (\n    \"fmt\"\n    machinery \"github.com/RichardKnop/machinery/v1\"\n    \"github.com/RichardKnop/machinery/v1/config\"\n    \"github.com/RichardKnop/machinery/v1/tasks\"\n    \"time\"\n)\nfunc main() {\n    var conf = config.Config{\n        Broker:        \"amqp://guest:guest@localhost:5672/\",\n        ResultBackend: \"amqp://guest:guest@localhost:5672/\",\n        DefaultQueue:  \"irgsh_tasks\",\n        AMQP: &config.AMQPConfig{\n            Exchange:     \"machinery_exchange\",\n            ExchangeType: \"direct\",\n            BindingKey:   \"machinery_task\",\n        },\n    }\nserver, err := machinery.NewServer(&conf)\nif err != nil {\n    fmt.Println(\"Could not create server : \" + err.Error())\n}\nbuildSignature := tasks.Signature{\n    Name: \"build\",\n    Args: []tasks.Arg{\n        {\n            Type:  \"string\",\n            Value: \"inkscape\",\n        },\n    },\n}\nrepoSignature := tasks.Signature{\n    Name: \"build\",\n             // No Args here as buildSignature result will be passed to this task\n}\nfmt.Println(\"Sending task\")\nchain, _ := tasks.NewChain(&buildSignature, &repoSignature)\nchainAsyncResult, err := server.SendChain(chain)\nif err != nil {\n    fmt.Println(\"Could not create server : \" + err.Error())\n}\n\nresults, err := chainAsyncResult.Get(1 * time.Second)\nif err != nil {\n    fmt.Println(\"Failed to get chain async result : \" + err.Error())\n}\nfor _, result := range results {\n    fmt.Println(result.Interface())\n}\n\n}\n```. @herpiko correct, you can recreate async result on a different server and fetch task data that way. Perhaps it might be a good idea to add some helper functions for this to the server.. Nothing is stopping you from implementing your own result backend based on the interface provided: https://github.com/RichardKnop/machinery/blob/master/v1/backends/iface/interfaces.go#L8\nI am trying to limit number of broker and backend implementations in this repository as it's becoming more difficult to maintain with every new addition.. Tasks with ETA for Redis are stored in a different queue, if you read the code. It's not a bug really but GetPendingTasks could be improved to also fetch data from the tasks that have ETA.. @minight Looks good!. @eldadvcita Sounds good. Do you want to make a PR?. @eldad87 \ud83d\udc4d . @gow This is probably caused by default expiration time being 1 hour: https://github.com/RichardKnop/machinery/blob/master/v1/config/config.go#L19\nPlease set it to larger value otherwise task states and group metadata will be expired. I am going to push a commit to make 24 hours default expiry time as 1 hour seems quite short.\nFor the issue 2) I am not sure why that would happen, could you debug more and find out the reason? I don't think any tasks should be lost from the queue as there is no expiration time set for messages in Redis queue Redis broker consumes from. And if task state is missing in the backend, it will get reinserted so task state being expired from the backend should not cause tasks to go missing from the queue.. \ud83d\udc4d . @fgimenez I have merged your PR but I couldn't reproduce the issue on my development machine.\nsh\ngo run example/machinery.go -c example/config.yml worker\nWorks for me also on version before the PR was merged. I don't have time to investigate further though.. \ud83d\udc4d . \ud83d\udc4d . I will merge your PR that fixes this issue over the weekend. Thank you very much for investigating and making a merge request to fix the issue!. @bharat-p done. I will restart the build, seems to be stuck (sometimes CI on Travis times out).. \ud83d\udc4d . \ud83d\udc4d . @seonixx You are using master branch of github.com/mongodb/mongo-go-driver\nIn go.mod file we define version we are using: https://github.com/RichardKnop/machinery/blob/master/go.mod#L31\nWhich accepts string as input: https://github.com/mongodb/mongo-go-driver/blob/v0.3.0/mongo/client.go#L61\nIn master branch they have changed it: https://github.com/mongodb/mongo-go-driver/blob/master/mongo/client.go#L78\nBut machinery is not using that yet, we are still at previous version.\nAre you using Go modules to handle dependencies? It's safest to use Go modules to use exactly same dependencies as CI is using for integration tests.. I think you must be using some different way to manage dependencies and your Mongo library is latest code in their master branch in your vendor folder.. @liftM I agree. Do you want to make a PR to return a friendly error? Result backend is required for machinery to work actually as it needs to store task state somewhere. There is an eager backend which is basically just storing state in memory. You can use it locally for testing or running simple tasks if you don't care about keeping the state.. @owenhaynes Restarted CI. I should probably migrate away from Travis to something like CircleCI.. This line should be:\ngo\nreturn brokers.NewAMQPBroker(cnf), nil\nThat's why CI was failing. I have merged the PR, fixed this and pushed to master.. Also rebase against master as there have been quite a lot of changes in last couple of weeks. After months of low activity I finally managed to get some free time last two weeks. So lots of refactoring and changes in the code. Plus new features.. I think this has been corrected in master branch already. Please check the code in the master branch if it is correct. I am not too familiar with the SQS broker code as I haven't written it (I know much more about AMQP and Redis which was originally written by me).. ",
    "JamesAwesome": "Personally I would see if I could encrypt it and export it as a variable and build the .netrc on the fly. It's read only but it does give us read access to all your repos...\n. Glad I could help. :+1: :smile:\n. ",
    "DamnikJain1": "vanhalt sucker, don't show attitude here idiiot just say what to do. What do you mean by \"didn't read the whole readme\" just say what you have done to solve the issue ?. ",
    "m0sth8": "Thanks, It's enough for me.\n. ",
    "ishail": "Thanks. First let me analyze the architecture then I would love to contribute on this.\n. ",
    "denkhaus": "@RichardKnop thanks so far! Will come up with some enhancements later...\n. ",
    "mirzac": "Hello... \nthere is already a project that implements workers and jobs in Go that are compatible with Sidekiq: \nhttps://github.com/jrallison/go-workers\n. ",
    "spinx": "@mirzac \nI'm aware of go-workers, but I was drawn to this solution because of groups, chords and chains as well.\n@RichardKnop OK, great. I'll start with the docs and some code next week. What are you using as a backend now ? Do you think that Memcache make sense as a queue ?\n. @RichardKnop Yeah I had backends folder open and was a bit perplexed why memcache would be used as a queue. I saw later these are results backends :)\n. ",
    "balzaczyy": "My practice is to use a cron library (like \"github.com/robfig/cron\") and feed task event accordingly. E.g.,\n``` go\nc := cron.New()\nfor _, v := range conf.CronJobs {\n    if parts := strings.SplitN(v, \" \", 2); len(parts) == 2 {\n        cmd, spec := parts[0], parts[1]\n        c.AddFunc(spec, func() {\n            server.SendTask(&signatures.TaskSignature{\n                UUID: genJobId(cmd),\n                Name: cmd,\n            })\n        })\n    }\n}\nc.Start()\nBut it work tricky in a distributed environment.\n```\n. I'm using rabbitmq for broker and wrote a elasticsearch backend to keep all the job status, which all host in a private cloud. There is one lead task, which scans 10K records, and update each record in a single task. After this initial setup, the following delta update may be far less.\nOn a second thought, maybe I should try rabbitmq in local. I will have another test today.\n. Using a local broker improves a lot. This should work for now.\nUpdated: if I set task number to be 100, instead of 20, it becomes very slow. I checked the code, and I think the state set can benefit by using more go routines. E.g., WaitGroup.\n. Still one question. What's the exact benefit to use a group, instead of sending task one by one? Is there any design consideration?\n. I managed to refine my task logic to make best use of concurrency and avoid running too long, e.g., split long task into smaller pieces. \nI also had a check on the above change. It looks quite promising. When will it be merged into the master branch?\n. Cool. Thanks. I'm using ElasticSearch as the result backend. But it's only partially implemented for my own purpose. I will try create a pull request when I finish the remaining function.\n. In Worker.Launch() loop, if retry is true, the returned error is eaten. I suspect there is some error and keep retrying won't auto-fix it. I have added a debug message to retry. It feels like time related.\nUpdates:\nUnexpectedly, the worker stopped just before reporting the result. I see \"Received message\" but never see the \"Processed %v. Result = %v\". Will try again.\n. Sorry for the late response. I'm currently traveling. And yes, I did found the root cause. It's caused by my local change that disabled the worker task concurrency. And the exiting error channel is blocking the main loop. After I changed the error channel to non-blocking, the issue is gone. It can be closed now. \n\u53d1\u81ea\u6211\u7684 iPad\n\n\u5728 2015\u5e749\u670822\u65e5\uff0c\u4e0a\u53489:40\uff0cRichard Knop notifications@github.com \u5199\u9053\uff1a\nFound out anything?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "requilence": "@RichardKnop Ok, I will test my solution and if everything works great I will create PR. \n. @wizard580 https://github.com/RichardKnop/machinery/pull/36\n. @mission-liao Also the similar approach is using in https://github.com/go-mgo/mgo. \nInitially I made the same decision (reusing connection, reinit channel) but doesn't see any performance benefits from this. I will repeat tests today and come back with results\n. In the current implementation there is no result object returned for goroutined sendTask (as you can see in the snippet). \nBut I think this can be archived by using SendGroup or by wrapping SendTask in anon func with passed chan\n. @RichardKnop Seems like channel is not safe to reuse in the current implementation. I performed some high-load test and always receive deadlock (and a lot of lost messages)\n```\nfunc main() {\ninitTasks()\nfmt.Println(\"Send in parallel without feedback\")\n\nvar wg sync.WaitGroup\nt1 := time.Now()\nfmt.Println(\"Starting...\")\n\nfor i := 0; i < 100; i++ {\n    wg = sync.WaitGroup{}\n\n    for i := 0; i < 500; i++ {\n\n        wg.Add(1)\n        go func() {\n\n            defer wg.Done()\n            fmt.Printf(\"Send task\\n\")\n            // Fetch the URL.\n            _, err := server.SendTask(&task1)\n            if err != nil {\n                fmt.Printf(\"Send task error: %s\\n\", err)\n            } else {\n                fmt.Printf(\"Sent\\n\")\n            }\n\n        }()\n    }\n    wg.Wait()\n}\nt2 := time.Now()\nfmt.Printf(\"Takes about %v\\n\", t2.Sub(t1))\n\n}\n```\nSo I made the commit to fix this. Now only connection is shared. Channel opens and closes after each publish/consume. \nAlso there is another one problem - https://github.com/streadway/amqp has a limit on total active channel count. So if you exceed maximum (looks like 65536) of active channels you will receive channel id space exhausted error. In theory we need to handle this (f.e. by waiting for channel close event)\nGenerally on my machine I have maximum publish per sec performance when using 500 concurrent sendTask. \n. ",
    "wizard580": "@Requilence \nCan you show your solution? I'll try to test it.\n. In my little investigation I see there no amqp problems, but more general issues as this issue reproducable with redis too.\nSituation:\nI have 2 workers, all running in parallel on the same server. One worker registered only 1 task \"worker1_task\" and second only 1 \"worker2_task\". \nSendGroup -> result.Get() hangs.\nPlease test such case.\nI've added some dubugging prints, and I see that my worker gets task of another worker, and then relaunch consuming with message \"[*] Waiting for messages. To exit press CTRL+C\"\n. Maybe look at this:\n\"consumeOne()\"  calls  \"taskProcessor.Process()\", then workers \"Process()\" calls:\n        task, err := worker.server.GetRegisteredTask(signature.Name)\n    if err != nil {\n        return err\n    }\nif task was not for this worker, Process() will return \"err\" and then will be \"errorsChan <- err\" at consumeOne(). So task discarded, sender/server will hang forever.\nI think in case of failed \"GetRegisteredTask\" this task should be returned to queue, or better not fetched from queue (if this possible)\n. For me it was fixed:\nv1/worker.go\nfunc (worker Worker) Process(signature signatures.TaskSignature) error {\n   task, err := worker.server.GetRegisteredTask(signature.Name)\n   if err != nil {\n      worker.server.GetBroker().Publish(signature)\n      return err\n   }\n. Is it acceptable or there will be released proper fix?\n. There not all clear with callbacks.\nCan you show example with explanations?\nAnd sure it's good to update docs.\n. as it works in each own thread, isn't there a possibility that we start all N \"send task threads\" which will be \"delayed\" a little and we come to result.Get() before any real send?\n. Yes, \"different queues for workers\" fine solution, will test.\nAnd thanks for my issue fix.\n. ",
    "mission-liao": "I've working on some go-amqp stuff (another worker queue project) and found this (issue)[https://github.com/streadway/amqp/issues/119] in that project. Quote from one discussant:\nI personally saw massive performance improvements by using a channel per go routine.\nIn current implementation of machinery, we create a channel (and connection) per 'SendTask', it seems a reasonable implementation, or maybe we would at least maintain a 'channel pool' for each request from different go routine.\n. additional dependency: github.com/stretchr/testify\n. One issue found, the type assertion of TaskResult is not correct when type is not float64\n. Ok, I will open another one later, thanks.\n. weird, I didn't touch that part\n. ",
    "vamsu": "FYI, this affects mongo backend as well. Broker: amqp.\nThis happens in the default example. It happens intermittently, there seems to be race condition somewhere.\n[machinery: amqp.go:159: Received new message: {\"UUID\":\"task_fe149098-78be-4a7a-8707-ed9618cb8a12\",\"Name\":\"add\",\"RoutingKey\":\"\",\"GroupUUID\":\"group_9af5e908-3306-4387-be9c-0d9b49650afe\",\"GroupTaskCount\":3,\"Args\":[{\"Type\":\"int64\",\"Value\":2},{\"Type\":\"int64\",\"Value\":2}],\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":{\"UUID\":\"chord_a604eb54-dcce-4872-9325-fa223d021b88\",\"Name\":\"multiply\",\"RoutingKey\":\"\",\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":null,\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}}\nINVOKED ADD: 5machinery: worker.go:110: Processed task_fe149098-78be-4a7a-8707-ed9618cb8a12. Result = 4\nmachinery: amqp.go:159: Received new message: {\"UUID\":\"task_ab92c0e2-a4c6-4d65-9a76-7cfd15094abe\",\"Name\":\"add\",\"RoutingKey\":\"\",\"GroupUUID\":\"group_9af5e908-3306-4387-be9c-0d9b49650afe\",\"GroupTaskCount\":3,\"Args\":[{\"Type\":\"int64\",\"Value\":1},{\"Type\":\"int64\",\"Value\":1}],\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":{\"UUID\":\"chord_a604eb54-dcce-4872-9325-fa223d021b88\",\"Name\":\"multiply\",\"RoutingKey\":\"\",\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":null,\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}}\nmachinery: amqp.go:159: Received new message: {\"UUID\":\"task_4820d0cd-e6a8-4501-a8c1-427aa5170319\",\"Name\":\"add\",\"RoutingKey\":\"\",\"GroupUUID\":\"group_9af5e908-3306-4387-be9c-0d9b49650afe\",\"GroupTaskCount\":3,\"Args\":[{\"Type\":\"int64\",\"Value\":5},{\"Type\":\"int64\",\"Value\":6}],\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":{\"UUID\":\"chord_a604eb54-dcce-4872-9325-fa223d021b88\",\"Name\":\"multiply\",\"RoutingKey\":\"\",\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":null,\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}}\nINVOKED ADD: 6INVOKED ADD: 7machinery: worker.go:110: Processed task_ab92c0e2-a4c6-4d65-9a76-7cfd15094abe. Result = 2\nmachinery: worker.go:110: Processed task_4820d0cd-e6a8-4501-a8c1-427aa5170319. Result = 11\nmachinery: amqp.go:159: Received new message: {\"UUID\":\"chord_a604eb54-dcce-4872-9325-fa223d021b88\",\"Name\":\"multiply\",\"RoutingKey\":\"\",\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":[{\"Type\":\"int64\",\"Value\":2},{\"Type\":\"int64\",\"Value\":4},{\"Type\":\"int64\",\"Value\":11}],\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}\nINVOKED MULTIPLY: 1machinery: worker.go:110: Processed chord_a604eb54-dcce-4872-9325-fa223d021b88. Result = 88\nmachinery: amqp.go:159: Received new message: {\"UUID\":\"chord_a604eb54-dcce-4872-9325-fa223d021b88\",\"Name\":\"multiply\",\"RoutingKey\":\"\",\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":[{\"Type\":\"int64\",\"Value\":2},{\"Type\":\"int64\",\"Value\":4},{\"Type\":\"int64\",\"Value\":11}],\"Headers\":null,\"Immutable\":false,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}\nINVOKED MULTIPLY: 2machinery: worker.go:110: Processed chord_a604eb54-dcce-4872-9325-fa223d021b88. Result = 88]\n. Opening a new one.\n. One thing I agree with you, this PR is misleading as that it says it supports custom struct. As it gives the impression that custom struct support is for structs which are not transferable to JSON. But, in fact it uses JSON to unmarshal. I could see why this could be confusing for end users.\nFeel free to close it.\n. Agreed. When you have the maps, then we can use mapstructure library to easily convert from map->struct https://github.com/mitchellh/mapstructure\nClosing this PR. Thank you.\n. ",
    "jamiecuthill": "+1 for not using godep\n. ",
    "vially": "Awesome. Thanks!\n. ",
    "TMaYaD": "Hi,\nFirst of all, let me apologise for incomplete PR with failing tests.\nThank you for working on this. #48 works. However, redis services(esp. as heroku addons) tend to use the format which is accepted by the DialURL function and I much prefer to use that instead of having to rewrite those manually for the service. However since this is no longer a show stopper, the priority is quite low.\n. ",
    "waterlink": "@RichardKnop I can't 100% understand what are you saying in your last comment. It seems like, you are advocating the use of BLPOP since it allows users of the library to not care about idempotency in their tasks.\nWhat puzzles me is that the code in question actually using LPOP, not BLPOP. Can you clarify a bit, I am, most probably, misunderstanding something.\n. ",
    "ghost": "I have encountered this issue as well. BLPOP has a timeout parameter which makes it behave similarly to LPOP. So I just changed the call to BLPOP with a timeout. The response you get is a little different but it's documented quite well here: http://redis.io/commands/BLPOP\n. @parhamdoustdar Using LPOP causes the goroutine to busy wait (constantly sampling the socket to see if there's a change) whereas using the blocking call causes the goroutine to enter a waiting state thus relinquishing its resources till the socket gets a reply from redis. It's similar to the difference between a spin lock and a regular mutex. \n. Also since the stopRecv channel is overwritten every iteration of StartConsume, if i sent a quit request during that time it won't ever happen even when a reconnecting attempt succeeds\n. Not really, the channels are blocked till the consuming goroutine reads from the channels. So the consuming goroutine will exit, but the caller to Quit will be blocked by writing to the channel.\n. I bypassed the problem by publishing the redis server's port, that way i refer to the same ip and i don't have to use quit. I played around with some patches, none of them worked so well. I'll see what i can do about it :)\n. ",
    "parhamdoustdar": "Why would LPOP increase CPU usage while BLPOP wouldn't?\n. @roee-indegy Thanks a lot for the explanation!\n. This works perfectly fine for me.\n. Here are the changes I've made to make this work for me. I'm not sure if they are acceptable though, so I'm pasting the diff here:\n```\ndiff --git a/v1/worker.go b/v1/worker.go\nindex 41d4d42..db6dbf3 100644\n--- a/v1/worker.go\n+++ b/v1/worker.go\n@@ -1,7 +1,6 @@\n package machinery\nimport (\n-   \"errors\"\n    \"fmt\"\n    \"log\"\n    \"reflect\"\n@@ -89,7 +88,7 @@ func (worker Worker) Process(signature signatures.TaskSignature) error {\n    // Call the task passing in the correct arguments\n    results := reflectedTask.Call(relfectedArgs)\n    if !results[1].IsNil() {\n-       return worker.finalizeError(signature, errors.New(results[1].String()))\n+       return worker.finalizeError(signature, results[1].Interface().(error))\n    }\nreturn worker.finalizeSuccess(signature, results[0])\n\n```\n. Pull request done. All tests pass:\nhttps://github.com/RichardKnop/machinery/pull/60\n. > The advantage of something like RabbitMQ or Redis is you can have workers running on multiple servers and distribute your tasks to be processed on many machines. This makes horizontal scaling easier.\nVery true. My use case is to only use Machinery so that I wouldn't have to implement my own library for queues and workers in Go, and that's why I use Machinery. I hadn't considered what you are saying though. Thanks for enlightening me.\n. ",
    "FelixSeptem": "any progress or plan about use kafka as a broker\uff1ffor kafka has been a popular stream system. ",
    "randomandy": "I'd also like so see a scenario with NATS. Should be quite interesting to use it with Machinery\n. Ok so I'm pretty sure it happens here:\nbroker.StartConsuming()\ninside Launch()\n. This fixes it:\n```\nfunc (worker *Worker) Launch() error {\n    cnf := worker.server.GetConfig()\n    broker := worker.server.GetBroker()\nlog.Printf(\"Launching a worker with the following settings:\")\nlog.Printf(\"- Broker: %s\", cnf.Broker)\nlog.Printf(\"- ResultBackend: %s\", cnf.ResultBackend)\nlog.Printf(\"- Exchange: %s\", cnf.Exchange)\nlog.Printf(\"- ExchangeType: %s\", cnf.ExchangeType)\nlog.Printf(\"- DefaultQueue: %s\", cnf.DefaultQueue)\nlog.Printf(\"- BindingKey: %s\", cnf.BindingKey)\n\nerrorsChan := make(chan error)\n\ngo func() {\n    for {\n        retry, err := broker.StartConsuming(worker.ConsumerTag, worker)\n        if err != nil {\n            errorsChan <- err\n        }\n\n        if retry {\n            log.Printf(\"Going to retry launching the worker. Error: %v\", err)\n        }\n    }\n}()\n\nreturn <-errorsChan\n\n}\n```\n. @RichardKnop Hey Richard. Sure thing, happy to help. Fix is in #76 \n. @RichardKnop Yes I think that'd be a good solution. I'll try to have a deeper look later this week.\n. ",
    "egorsmkv": "Excuse me, my carelessness\n. Thanks, Richard!\n. ",
    "ritesh": "Cheers. I'll try this out, thanks!\n. ",
    "pauldevelder": "Hi Richard, I've updated it.... \n. ",
    "ConnorDoyle": "Thanks for the prompt review @RichardKnop \n. ",
    "pgruenbacher": "Actually what I can do is just serialize the structs I need into strings manually myself using the encoding/json package for the time being. It seems similar questions are asked with Redis and the response is to serialize manually. http://stackoverflow.com/questions/27468458/go-how-to-save-and-retrieve-a-struct-to-redis-using-redigo\n. ",
    "vkd": "OK, I add new backend for keeping task states and results in MongoDB.\nDatabase name get from config (by default is 'tasks'), collection name is 'tasks'. \nExample task object in MongoDB:\n{\n    '_id': string,           // task UUID\n    'group_uuid': string,\n    'createdAt': time.Time,\n    'state': string,\n    'result': {\n        'type': string,\n        'value': interface{}\n    },\n    'error': string\n}\nOn initialize backend created new connection and checked two indexes on DB: by 'group_uuid' field and by 'createdAt' field with expireAfterSeconds value (index by '_id' field is default in MongoDB).\nIn GetState method if value task result is number (int..., uint..., float...), then converted value in float64, because utils/reflect.go:ReflectValue expect is float64:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/utils/reflect.go#L100\nhttps://github.com/RichardKnop/machinery/blob/master/v1/utils/reflect.go#L112\nhttps://github.com/RichardKnop/machinery/blob/master/v1/utils/reflect.go#L124\nOn v1/factories_test.go I don't use reflect.DeepEqual because every instance of MongodbBackend include new MongoDB connection.\nAlso update vendor and add some changes for travis CI tests.\n. ",
    "pbrazdil": "I don't get it as well. I am forced to use Redis as a backend.\n. Thank you Richard, seems like it's working. Btw, I plan to do some PRs in the future.\n. ",
    "kenkouot": "Good to know for future!\n. ",
    "zhulik": "You can implement this feature as user-defined function, with simple interface\ngo\ntype acceptNewTask func(currentTasksCount int) bool\nBy default it will be like this\ngo\nfunc acceptNewTask(currentTasksCount int) bool {\n    return currentTasksCount < n\n}\nAnd allow user to implement his own functions, based on his requirements\n. ",
    "hphilipps": "PR #111 added a config option to limit the number of worker instances.. @oleksandr I think PR #111 is implementing what you asked for and is merged with master.. Ah - I didn't notice that retry is already set to true for AMQP. In this case I will just create a small PR to also set it to true for Redis - because that's what I'm mostly using. I think there is no reason to disable retries at all. The fibonacci based backoff sequence is nice and should be used! \ud83d\ude04 \n. ",
    "vampirekiss": "my wrong, forgot it\n. ",
    "dokterbob": "@vampirekiss What was the problem here, in the end? I'm running into the same (probably similar) error.\n. I discovered what caused the problem, though I'm not sure whether it's a feature or a bug.\nIt seems that machinery assumes that a return argument (besides an error) is required, whereas in my application no return value is needed. Perhaps machinery should not assume there to be non-error return arguments or this could be documented better.\n. I have been considering it, but for my particular application I think I might be better off interfacing with AMQP directly - I might want to do stuff like dynamically requesting more work for slower tasks and don\u2019t need results at all.\nShould I change my mind, you can be sure to find yourself a pull request - although that would be my first contribution to a Go project. :)\n. That's indeed what I ended up doing. :)\nThanks. :). ",
    "mehcode": "Oh It's definitely something I wouldn't mind contributing to; just don't expect something fast (I don't often get time to work on things like this though I do enjoy it).\n\nI'd propose the following -- change Config  to accept a Logger StdLogger (interface type copied out from logrus -- https://github.com/Sirupsen/logrus/blob/master/logrus.go#L97-L109 ).\nThen its just a matter of propagating that everywhere that needs it ( from a quick poke I'm assuming that config is available in most places as is ).\n. ",
    "ryanbaer": "+1\n. ",
    "hryx": "Ah, modifying Config looks like a more accessible and sensible approach than the redis.DialOption suggestion I made. I like it better.\nI wouldn't mind making the changes, but the downside is that it would break existing configs and necessitate a v2 API. This change alone seems a little small to warrant a whole new API version.\nExpanding the URL parser is a bit of a cheap workaround, but it would be quicker and shouldn't necessitate a new API version. How about I do that for now, and the config change can go on a v2 wishlist?\n. Roger that. Thanks @RichardKnop, I'll get started on updating this and send a PR soon.\n. ",
    "owenhaynes": "Rabbitmq broker yes\nWe use  more then region flags hence why we do not do use the topic method\nand use headers. Headers also allow for matching as well\nOn 6 Aug 2016 12:31 p.m., \"Richard Knop\" notifications@github.com wrote:\n\nDo you use RabbitMQ broker?\nIf yes, you could use binding / routing key to route tasks to workers in\ndifferent regions.\nYou could use topic exchange type and have workers in different regions\nrunning with region specific binding keys:\n- machinery_tasks.germany.#\n- machinery_tasks.france.#\nThen you can specify different routing key when sending tasks:\n- machinery_tasks.germany.task_A\n- machinery_tasks.france.task_A\nThe tasks will then be consumed by workers in specific region.\nI have used a similar setup before with Celery as well. So I believe the\nabove setup should work in this case too.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/RichardKnop/machinery/pull/90#issuecomment-238019059,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABZ5fc7SY6qJwXOtsKdpsN1oqGoE3tk0ks5qdHCVgaJpZM4JeOOf\n.\n. I thought rabbit required installing a plugin for delayed tasks, the plugin\nthen requires a header being applied to tasks with the delay\n\nOn 6 Aug 2016 12:46 p.m., \"Richard Knop\" notifications@github.com wrote:\n\nA good concept and I agree in principal.\nThis could be supported natively by AMQP. You can schedule a delayed task.\nRedis broker also supports delayed tasks I believe.\nSo this could be implemented in a native way with both AMQP and Redis\nwhich is good as I am trying to keep the library behaviour consistent with\ndifferent brokers / backends.\nYou want a store a time to execute and not just the delay as you want to\nguarantee that the task is run as close as possible to it, If the worker\ncrashes the delay would start again otherwise.\nThis would be taken care of by RabbitMQ / Redis process so machinery\nworker crashing and restarting should not matter as the logic will be\nhandle by the broker.\nJust need to confirm that both AMQP and Redis delayed tasks support\nsetting a time when to free message to be consumed and not just some delay\nperiod.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/RichardKnop/machinery/issues/91#issuecomment-238019530,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABZ5faQTGKNh1kCiz51Hktbpwi8KCg1Zks5qdHQlgaJpZM4JeOXr\n.\n. I do not think its a good idea to have another process to be able to delay random run tasks. Only ever used redis for celery backend and AMQP for the broker. So not seen any of the flaws of AMQP as a backend.\n\nCould do something like this https://ivanyu.me/blog/2015/02/16/delayed-message-delivery-in-rabbitmq/ for AMQP backends?\nI agree that it make sense to have something similar to celery-beat if you have predefined tasks which need to be run at set times like clean up routines. A delayed task would not come under this.\n. Just trying to diagnose the index failure on the CI. Works fine locally on mongo:3.6 and mongo:4. fixed in #391 Just go around to updating to latest RC. . This supports go modules, not dep. So you will need to use a override at the moment to fix problems due to dep flaws.\nThis is what I am currently using with repos we still have which use dep.\n[[override]]\n  name = \"go.mongodb.org/mongo-driver\"\n  version = \">=1.0.0-rc2\"\nI would also recommend never doing \ndep ensure -update\nas it try to update everything and have had more problems with this, it's better todo \ndep ensure -update github.com/RichardKnop/machinery. Travis has died, could you re-kick off the build please?. Need some advise here. DeepEqual is always false now due to different properties deep in the mongo driver code. So what would be a better alternative to-do here?. ",
    "kavirajk": "I think we don't need either CRON or delayed-message-delivery-in-rabbitmq as suggested by @owenhaynes for delay task.\nWe can do the way celery does it. Point to note here is celery doesn't use celery-beat for delayed task. It uses beat only for periodic tasks (CRON).\nThe way actually cerely handles is via simple kwargs eta=. refer here http://docs.celeryproject.org/en/latest/userguide/calling.html#eta-and-countdown.\nAfter digging into the celery code, just figured out celery is using \"unacknowlege\" feature of Rabbitmq to get this done. https://github.com/celery/celery/blob/master/celery/worker/strategy.py#L115-L127\nSimple idea is, Whenever consumer consumes task from queue, it checks for key \"eta\" in the body of the message, then consumer won't acknowledge till the execution time is reached. So the task remains in the queue. \nOnce the \"execution time\" is reached, consumer acknowledges it and execute the task.\nThis seems like the good idea. I think we do the same thing. What do u think guys @RichardKnop @owenhaynes \n. @RichardKnop I think I can pick this up coming week.\n. @RichardKnop I had a second thought while implementing this feature,\nWhy don't we just use time.Timer feature to delay the task\nexample: To delay Add task to 10 seconds, we can write in the following way\n``` go\n// Add ...\nfunc Add(args ...int64) (int64, error) {\n    t := time.NewTimer(10 * time.Second)\n    <-t.C\n    sum := int64(0)\n    for _, arg := range args {\n        sum += arg\n    }\n    return sum, nil\n}\n```\n@owenhaynes what do u think?\n. ",
    "brannon": "One use for Celery delayed tasks is when retrying a task (e.g. task.retry(countdown=30)).  That could be implemented via the \"eta\" / \"unacknowledge\" approach pretty easily if you are just re-queueing the same task with an eta.\n. ",
    "gonzalobustos": "+1 \nI'm really interested in this feature.. ",
    "tspecht": "Any update on this?. Awesome, thanks a lot, exactly what I needed!. ",
    "chennqqi": "This error may be redigo's bug\n. Yeah, I use chords on Redis. I found workers 'ESTABLISHED' too many connections with broker&backend. so that's why publishing tasks  failed(wsasend: An existing connection was forcibly closed by the remote host).  Each subtask of a chord has a long term connection with broker&backend. \n. ok. I will have a try. or I sugguest using nsq or nats instand of redis...\nNow, i simply package a chord task (with a lot of subtasks) as a standard task to escape this problem.\n. ",
    "saggit": "thanks.\n. Yes, try to return (string, error) in your task define.. Have same question, For now, If I press control +C, the worker stop immediately, How to implement a feature like celery, give it a warm shutdown?. Thanks for you quick reply.\nYes, I try to solve it by use the latest code, but it still hangs, \nmore detail, the problem will happen when worker idle for days.. @RichardKnop Thanks for your hard work, But it will take me same time to verity due to #151 .. @RichardKnop I can confirm the issue was solved.\n@alieve  what about you?. I don't want the reload feature, what I need is create multi Config object with different settings \nHere are some detail, let's say I have two config file, config_a.yml and config_b.yml, each file have different value, then create Config object like this\ncnf_a := config.NewFromFile(\"config_a.yml\", true, true)\nconf_b := config.NewFromFile(\"config_b.yml\", true, true)\nthen conf_a and conf_b have same value now.\nI try to debug and found the main reason is the var configLoaded and newCnf = *cnf assignment, configLoaded will refuse to create another New Config object when there is already one and newCnf = *cnf will cause all Config object point to the same object.. ",
    "Blogoslov": "@RichardKnop Speaking friendly I don't get how the queue can be declared without x-message-ttl when server and sender before initialization has and read the same config file and set the same option 'results_expire_in:' which mentioned in the config file.\nAnd I don't understand how it can be fixed? Where and how should I declare int32 x-message-ttl? Could you please explain with small example?\nThanks  and regards.. Sorry, my false - everything is working fine with sleep function.\nPlease reject this item.. Hello yexiangyu,\nyes I also  search such functionality at 'machinery' but it seems not available. Do you have any ideas how it can be done until it does not supported by 'machinery'?. I'm using AMQP as a broker and backend.\nYes, broker has redial functionality. But this message from backend procedure.. Any help?. Sorry, but your comment very confuses me. You say that asyncResult.Get() is not recommended for usage (it is only for testing) but how on my program I will know that tasks are completed? \nFor instance - in my program, I started all necessary servers (Postgres, RabbitMQ, Redis and others), started my flow tasks and send to server.SendGroup all my thousand tasks. These tasks are working and require for example half day for completion. After it completed, my program should stop all servers (Postgres, RabbitMQ, Redis and others). How to know when I need to start my last procedure to shut down servers. Of course, this task can't be a part of server.SendTask' procedure because it will not be completed because of servers will be down.\nSo how I should proceed in this case?\nP.S. By the way - all your examples have this instruction to use block way to get results after tasks completion and no any non-block tasks run. Is it possible to see examples with the non-block run?\nThanks in advance!\n. Any thoughts about how it can be fixed? . In the broker.go file I just commented in function stopConsuming() following piece of code and I stopped see the issues\n// Stop the retry closure earlier\n        select {\n        case b.retryStopChan <- 1:\n    log.WARNING.Print(\"Stopping retry closue.\")\n    default:\n    }\nI'm sure that this is the incorrect way but for now, I see that worker completeness is much better.. As I understood from the code it is not possible at this moment and complex flow steps are not allowing.\n@RichardKnop, do you plan to support complex flow steps in near future? I'm ready to help you in this way because this is really important things.\nBest regards.\n. Richard, \nHow do you think is it reasonable to have such feature that OnError and OnSuccess callbacks can support a different kind of tasks (group, chain, single and etc)?\nAnd how do you think is it possible to do in current code implementation just make several changes? I'm ready to support this enhancement.\nThanks in advance.. Richard, does it means that if my task which is in PEND mode will not be processed by workers (the reason can be too little number of workers or too heavy tasks which take too much time for processing) then it will be deleted from the queue? Or you are speaking about only about completed tasks?\nBest regards!. I don't why but code formatting has stranges and half of code was formatted and other half was not formatted in my previous message.\nUpdate: Fixed code examples formating. will try to use redis as result backend insted of amqp to see if this affects on this case or not. . In case of AMQP broker and Redis as result backend, everything is ok and all connections are closed and I do not see any redundant connections in RabbitMQ web page.. any suggestions? or is this impossible?. For Redis bug is fixed. I see that pending tasks are correct now.. I think you are asking about this parameter:\nworker := server.NewWorker(\"worker_name\", 10)\n. ",
    "rooty123": "Any ideas how to walk around it at the present time? \nI cannot find what's the reason of the error:\nError = runtime error: invalid memory address or nil pointer dereference\n. Turned out this happens  when there are no workers responsible for the task available.. Looks like it is not processed but printed out in console, that's why it looks like it take foreign task. Sorry for misunderstanding.. ",
    "mdouchement": "I have made a prototype with Postgres here that works well.\nThe relevant points are:\n- As @RichardKnop said, its a bit difficult to use external implementation with the current factory\n- SQL databases do not fit well for tasks' broker because you need to poll the data\n. ",
    "oleksandr": "@RichardKnop OK, this is something I'm currently looking at and would love to submit PR soon.. ",
    "catherinetcai": "@RichardKnop - Thanks for the quick response! I thought that would be the case, so I set it up, but I seem to be having a few issues...\nI start a new worker inside my application and I see that it connects to Redis (using both Redis as a broker and a backend), and it hangs unless I launch it in a separate goroutine: \nmachinery: worker.go:28: - Broker: redis://localhost:6379\nmachinery: worker.go:29: - ResultBackend: redis://localhost:6379\nmachinery: worker.go:30: - Exchange:\nmachinery: worker.go:31: - ExchangeType:\nmachinery: worker.go:32: - DefaultQueue: machinery-test\nmachinery: worker.go:33: - BindingKey:\nmachinery: redis.go:86: [*] Waiting for messages. To exit press CTRL+C\nWhen I have it spun up in another Goroutine, it seems to hang once I register more than four tasks. For whatever reason, I see the messages get sent, but they get stuck in the PENDING state. Is there some sort of deadlock going on?\n. @RichardKnop - Looks like the only way I've been able to resolve this has been to create a slice of workers, and then instantiate these workers in a goroutine. . ",
    "bscott": "This project is still very young, doesn't seem to be ready for Production, but that's up to your needs and how critical they are. . ",
    "seonixx": "Any updates/change of opinion on this (almost) a year on? Looking to implement this in production for a startup.. @owenhaynes this currently just generates an empty context when a context is detected as the first argument in the task definition? So it's not actually passing the context from the caller to the worker?. Any updates on this? Can't seem to figure out how to pass the context in to the task signature.. @RichardKnop regarding the above \"partial support\" for contexts, unless I'm missing something it would appear that this simply attaches a background context to a task function definition if the first argument in that definition is of type context, however I don't see anyway you can pass an existing context to the task itself as context.Context is not a supported base type for an argument.\n@Rio your PR for send with context applies specifically to opentracing is that correct? Not sending of the serialised context as a whole to the worker? From what I can see this is attaching the span to the header, but not passing any additional context along.\nIn short, I'm trying to figure out, as the OP asked, how to send the context to the worker as an argument. Not just the opentracing component.\nI'm fairly new to opentracing so if I'm missing something, my apologies, but I'm needing to pass context to the worker for visibility of session information. Maybe this can/should be done through opentrace? Any input appreciated!. Thanks for your input! @RichardKnop you mentioned sending context is difficult because of machinery being distributed, but if the context is serialised and sent as a task argument then is this still an issue?? Just trying to work out where this becomes a problem.\n@Rio yes I also looked at the baggage option (after looking into opentracing in more detail) but thought the overhead would become a potential issue at some point.\nI'm using go-kit (https://github.com/go-kit) for microservices over HTTP-JSON which provides context threading across services via built-in instrumentation. I might try to find out more about how this works and see if could/should be applied to machinery in a similar way.\n@RichardKnop @Rio cheers for your help so far :). @Rio thanks for this!. > Early last year AWS released Redis In-Transit Encryption for Elasticache. When this is turned on, you must have TLS configured on your client in order to connect.\n\nI have added support to configure TLS for the redis backend and broker the same way you can configure it currently for mongo and amqp.\n\nHow exactly does one configure TLS on a Redis connection? Is there anything required other than providing the connection string?. There seems to be quite a long discussion about this - short story is you can do something opentrace, but context type support that was added some time ago appears to just add an empty context to the job when received.. https://github.com/RichardKnop/machinery/issues/175. @knrt10 are you using an opentrace span within the context or just a normal context type?. Actually - can see a change was made in the last 24 hours...\nhttps://github.com/RichardKnop/machinery/commit/be39d0b945df20dfff29c60ca79d91d7e452b4d2#diff-98c1e3afb4dd4a06358c9e96c3bd29aa\n@RichardKnop are you seeing any issues here?. ",
    "codepushr": "@hphilipps @RichardKnop Does this change also work on a 'per Chain' level? I would like to set an option that a worker can only process X chains (with multiple tasks) at once. The motivation comes from chains that have long running tasks and I want a whole chain to complete before another random task gets processed.\nI'm still new to Go but would collaborate if you could broadly describe what to do. :) . @RichardKnop Sorry to bother you again, but isn't MaxWorkerInstances a global config? If I understood correctly you would use a separate worker with MaxWorkerInstances=1 for the long running tasks in my chains, and a usual worker with MaxWorkerInstances=0 for all other tasks.. @michael-younkin If you are still interested have a look at this fork https://github.com/udacity/machinery/tree/babcock/sqs_broker\nThey already implemented SQS, however no idea how robust/tested it is.. I got a similar issue. When I start my service without turning on redis I can't seem to stop it anymore.\nLogs:\nWARNING: 2017/08/04 17:06:23 worker.go:51 Start consuming error: dial tcp [::1]:6379: getsockopt: connection refused\nWARNING: 2017/08/04 17:06:23 retry.go:20 Retrying in 1 seconds\nWARNING: 2017/08/04 17:06:24 worker.go:51 Start consuming error: redigo: get on closed pool\nWARNING: 2017/08/04 17:06:24 retry.go:20 Retrying in 1 seconds\n^CWARNING: 2017/08/04 17:06:24 worker.go:61 Signal received: interrupt. Quitting the worker\nWARNING: 2017/08/04 17:06:25 worker.go:51 Start consuming error: redigo: get on closed pool\nWARNING: 2017/08/04 17:06:25 retry.go:20 Retrying in 2 seconds\n^C^CWARNING: 2017/08/04 17:06:27 worker.go:51 Start consuming error: redigo: get on closed pool\nWARNING: 2017/08/04 17:06:27 retry.go:20 Retrying in 3 seconds\nEdit: going to use the latest code and check again...\nEdit2: seems like I'm on the newest commit:\n- name: github.com/RichardKnop/machinery\n  version: 46eeb95e6208e539826cc64e2f4c51e700c7cb8d. Wonderful, so it's a feature. Sorry for missing this. I had to set s1 instead of s2 to immutable for the error to resolve though.\nThanks a bunch!. Appendix\nMaybe it's only me but it might be more intuitive to append previous results into a separate field. In my opinion the args field should always be immutable as it sort of reflects the initial arguments for the signature.. @RichardKnop I think I f*cked up. I tested a few other things and it seems I persisted them on master instead of a feature branch. Commit 50fcece should not have been merged :( I'm terribly sorry, can you revert this one commit from my PR?. ",
    "kartlee": "I see a OnError callback in the task signature. I assume we can register a worker which can exclusively retry failed tasks? Is there a working example for this?. Thanks Richard. Feel free to close it.\n-Karthik. Excellent. Looking forward for the change.... Any update?. Excellent Richard. Thanks.. ",
    "dkiser": "This would be an AWESOME feature that aligns with a few key requirements of the system I'm looking to migrate away from (Celery in Python).\n. ",
    "foobargeez": "+1 retrying failed tasks will be pretty helpful.. In need of warm shutdown akin to Celery.  @RichardKnop, I would be happy to contribute but I am not currently a golang guy, unfortunately.  BTW, I know time is the issue here but if it helps, I am happy to swallow some of the development costs if you are willing to put in hours.  If you're interested, please drop me a message in gitter.  Thanks!. ",
    "davmaz": "Sorry for the bad issue report. I didn't have a backend running.. ",
    "asimashfaq": "When ever i try to send 150+ task at once it  reset my connection. ",
    "Jolly23": "I have the same problem, have you fixed it?. I use SendGroup to send my tasks, when I send 10 tasks, it works well. However, if I send 100 tasks, it tells me read tcp 127.0.0.1:56064->127.0.0.1:56379: read: connection reset by peer.\nI do not know how to solve this problem.. @RichardKnop Could you help us?. Sometimes, my project has to send tens of thousands of I/O tasks\nIt used celery with rabbitMQ before I decided to shift the language from python to Golang\nI am searching for the Golang RabbitMQ plan, I think using this package is the best way to replace the python's method.\nBut I meet this problem, I hope someone could help me.. I tried to use SendTask with a loop to send big amount of tasks, but it is very slow. \nSo I have to use the SendGroup. Thank you, now, I knew the problem.\nThanks again for your serious and responsible.\nI will try to shift the backend. . But I just found that I met this problem when using RabbitMQ for broker and redis for backend, as you said.\n@RichardKnop . ",
    "Mudpuppy12": "This seemed to work, with some digging\ntask, _ := server.GetBackend().GetState(\"task_d7e33c78-6888-4e87-8a26-18a83b98fa95\"). ",
    "paulm17": "@zfsamzfsam It's been a while, but since no-one has approached this.  I want this as well, so will be working on it.  If I'm successful, PR to follow.... Thought I would update.  I have Nats working for a Broker and I have a MemSQL cluster working as a backend, similarly to how memcache works. \nINFO: 2018/04/30 15:23:49 file.go:19 Successfully loaded config from file config.yml\nINFO: 2018/04/30 15:23:49 worker.go:43 Launching a worker with the following settings:\nINFO: 2018/04/30 15:23:49 worker.go:44 - Broker: nats://localhost:4222\nINFO: 2018/04/30 15:23:49 worker.go:45 - DefaultQueue: machinery_tasks\nINFO: 2018/04/30 15:23:49 worker.go:46 - ResultBackend: memsqlcache://root:f00b4r@tcp(192.168.0.28:3306)/machinery\nINFO: 2018/04/30 15:23:49 nats.go:45 [*] Waiting for messages. To exit press CTRL+C\n\nTomorrow, I will use NATS Streaming instead of the basic one.  As this includes things acknowledgements and might as well have it as close to the AMPQ implementation.. Thanks for the reply.  But I'm sorry, but you haven't given me enough information.  I'm missing some crucial elements.\nLet me explain what I have and hopefully you can point me to the right direction.\nI have two \"workers\" in different programs.  The job worker and a \"message queue\" worker.\nThe job worker starts the machinery worker process:\nfunc startServer() (*machinery.Server, error) {\n    cnf, err := loadConfig()\n    if err != nil {\n        return nil, err\n    }\n\n    // Create server instance\n    server, err := machinery.NewServer(cnf)\n    if err != nil {\n        return nil, err\n    }\n\n    // Register tasks\n    tasks := map[string]interface{}{\n        \"someTask\": tasks.SomeTask,\n    }\n\n    return server, server.RegisterTasks(tasks)\n}\n\nfunc worker() error {\n    consumerTag := \"machinery_worker\"\n\n    cleanup, err := tracers.SetupTracer(consumerTag)\n    if err != nil {\n        log.FATAL.Fatalln(\"Unable to instantiate a tracer:\", err)\n    }\n    defer cleanup()\n\n    server, err := startServer()\n    if err != nil {\n        return err\n    }\n\n    // The second argument is a consumer tag\n    // Ideally, each worker should have a unique tag (worker1, worker2 etc)\n    worker := server.NewWorker(consumerTag, 0)\n\n    return worker.Launch()\n}\n\nThe \"message queue\" worker is one which listens on a nats queue listening for messages, which originate from PHP.  It's the usual runTask from the example:\nfunc runTask(server *machinery.Server, event_name, event_data string) error {\n    cleanup, err := tracers.SetupTracer(\"sender\")\n    if err != nil {\n        log.FATAL.Fatalln(\"Unable to instantiate a tracer:\", err)\n    }\n    defer cleanup()\n\n    insertTask := tasks.Signature{\n        Name: event_name,\n        Args: []tasks.Arg{\n            {\n                Type:  \"string\",\n                Value: event_data,\n            },\n        },\n    }\n\n    /*\n     * Lets start a span representing this run of the `send` command and\n     * set a batch id as baggage so it can travel all the way into\n     * the worker functions.\n     */\n    span, ctx := opentracing.StartSpanFromContext(context.Background(), \"send\")\n    defer span.Finish()\n\n    batchUUID, err := uuid.NewV4()\n\n    if err != nil {\n        return fmt.Errorf(\"Error generating batch id: %s\", err.Error())\n    }\n\n    batchID := batchUUID.String()\n    span.SetBaggageItem(\"batch.id\", batchID)\n    span.LogFields(opentracing_log.String(\"batch.id\", batchID))\n\n    log.INFO.Println(\"Starting batch:\", batchID)\n\n    asyncResult, err := server.SendTaskWithContext(ctx, &insertTask)\n    if err != nil {\n        return fmt.Errorf(\"Could not send task: %s\", err.Error())\n    }\n\n    results, err := asyncResult.Get(time.Duration(time.Millisecond * 5))\n    if err != nil {\n        return fmt.Errorf(\"Getting task result failed with error: %s\", err.Error())\n    }\n    log.INFO.Printf(\"%v\", tasks.HumanReadableResults(results))\n\n    return nil\n}\n\nWhat I want to do.  Is instantiate the database object inside the job worker.  Where it's only done once and then passed to the task.\nWhy do I want to do this?  I have well over 100 tasks (100 task.go files), all of which interact with the database.  Currently, I'm opening a new connection in each task execution.  For a task queue that is going to be processing millions to tens of millions of tasks.  This is going to be a problem.\nI've also seen issue #175 and I'm also not sure if passing in the db connection via the context is a good idea or not.  It would be nice, that when a worker starts.  It can pull in a db connection from machinery itself?\nSo not sure how to solve this problem.\nMany thanks\n. Unfortunately I did not.  My work around was to build out my own message queue.  Drastic I know, but I couldn't find something open source which would satisfy all my requirements.  I'll release it when it's appropriate to do so.  But don't hold your breath.  \ud83d\udc4d . ",
    "justinhillsjohnson": "@RichardKnop \nPlease take a look at this PR as it provides support for AMQPS. I ran into issues moving our Golang app into production on Aptible and this monkey patch fixes the problem.\nOn the client side, I implemented TLS using Config as such:\nconf := &tls.Config{\n        InsecureSkipVerify: true,\n    }\n    return &config.Config{\n        Broker:             env.MqBroker,\n        ResultBackend:      env.MqResultBackend,\n        Exchange:           env.MqExchange,\n        ExchangeType:       env.MqExchangeType,\n        DefaultQueue:       env.MqDefaultQueue,\n        BindingKey:         env.MqBindingKey,\n        MaxWorkerInstances: env.MqMaxWorkerInstances,\n        TLSConfig:          conf,\n    }\nHowever, please note that tests are failing in your master branch due to unrelated changes in this PR. I can rebase against master once those issues are resolved.\n . @RichardKnop Thanks!. ",
    "vanng822": "Thank for quick response\nNot really help\nLook like that (in brokers/redis.go)\n\"ZRANGEBYSCORE\" \"delayed_tasks\" \"0\" \"1495105325993073062\" \"LIMIT\" \"0\" \"1\"\nreturns zero length items which causing error, the timer you just put in never reach.\nNot really sure why it is empty now and not on the old version (revision bae667d798a33928db0349a5e2d098d20ce3b301). Maybe something that you know by heart? need to set it up?\nAnother question is that is it sufficient to run a timer there? The CPU is actually reduce a lot if we put in a timer.. That helps thank you,\nI was thinking that delayed_tasks is nil when there are no delayed tasks. Isn't it better to stay in that for loop since it is expected that len(items) == 0, just continue instead of returning an error.. ",
    "tingxin": "when as redis as borker, still found this issue, any suggestion. ",
    "jsm": "@tingxin I don't use this anymore, but you could look into integrating this with https://github.com/go-redis/redis which has cluster support. I think this PR provides a way to custom handle panics, since panics are wrapped in errors and sent up the stack.\nhttps://github.com/RichardKnop/machinery/pull/238. @RichardKnop Bump?. ",
    "clstb": "Storing the API client internally appears to be the best solution. I forgot that this was actually a thing. I could probably do the same thing with a DB connection too.\nI\u00b4m a beginner in terms of microservices so there is another question you could help me out with. \nThe API I query returns simple JSON. Do you think I should go for the approach you advised in https://github.com/RichardKnop/machinery/issues/70 and store everything in a external DB right away or only my processed data?\nI feel like since there already is a ResultBackend it should be used and pass the result back serialized. So I dont have to create a table for every endpoint I query.. I think we can close this by now. I figured that using MongoDB as ResultBackend and general persistent storage is the most convinient way to handle data for my application.. ",
    "jurre": "Thanks @RichardKnop, also for the quick response! I'm going to deploy these changes tomorrow, and then after the weekend I should know if this fixes our issues, if not, I'll report back.\nAppreciate your help!. By the way, gometalinter could have found those overwritten errs, how would you feel about running linting on CI? I wouldn't mind setting it up, it's pretty strict and not easy to ignore certain warnings, but I've found it helpful in other projects though.. ",
    "zyanho": "@RichardKnop . machinery could not handle different task with retry\nif use redis, RoutingKey not work, because redis only have one default key for delay task name \"delayed_tasks\". redis could not use task, all delay task shoud send to delayed_tasks, if there 2+ queue worker\nthen all worker should got block, the delay task could not handle by the right worker\n149 . machinery could not handle different task with retry\nif use redis, RoutingKey not work, because redis only have one default key for delay task name \"delayed_tasks\". #149 . ",
    "alieve": "It seems worker can not restore connection to Rabbit in case of loosing it and go to idle state. \nSteps to reproduce:\n1. Run worker.go worker with configuration above, make sure rabbitmq-server is running\n2. Stop rabbitmq-server\n3. Start rabbitmq-server\n4. Try to strike Ctr+C or send message by worker.go send. Nothing happens, worker in idle state and can be closed by killing process only.. ",
    "baiyunping333": "```go\nimport(\n...\n\"gopkg.in/yaml.v2\"\n\"github.com/RichardKnop/machinery/v1/config\"\n\"github.com/RichardKnop/machinery/v1/log\"\n...\n)\nfunc NewFromYaml(cnfPath string) config.Config {\n    var newCnf config.Config = &config.Config{}\ndata, err := config.ReadFromFile(cnfPath)\nif err != nil {\n    log.FATAL.Printf(\"File not exists, %s\", err)\n}\n\nif err := yaml.Unmarshal(data, newCnf); err != nil {\n    log.FATAL.Printf(\"Unmarshal YAML error: %s\", err)\n}\n\nreturn newCnf\n\n}\n```\nin the way:\ngo\ncnf = NewFromYaml(*configPath)\nmay help.\n. keepReloading is chicken ribs\uff01. Is it necessary\uff1f I  can give a patch, but i don't think this is a good way.. ",
    "mbyio": "Nice! I wish I had seen this sooner.\nUnfortunately, we needed something a little more robust, and so far we don't need all the features provided by machinery, so we ended up writing our own little framework. We might have time to revisit it in a few months though!. ",
    "KirinDave": "@michael-younkin @codepushr We're using it in production. Right now we only support fifo queues but we're about to add non-fifo (as we've discovered that we actually don't care about the fifo nature for the jobs we run at all). \nWe've also got a port in Typescript that we're gonna open source Real Soon Now(tm). ",
    "devmark88": "Is there any update? I still have this issue. I run the workers in the different routines in Gin web application.\ngolang\nfor i := 0; i < n; i++ {\n    worker := server.NewWorker(fmt.Sprintf(\"worker_%v\", i), 10)\n    go func() {\n        worker.Launch()\n    }()\n        // Just for having an instance of workers in a slice\n    workers = append(workers, *worker)\n}\nand after that I simply run my gin server\ngolang\nr.Run(\":9080\")\nServer Log:\nlog\n[GIN-debug] Listening and serving HTTP on :9080\nINFO: 2019/02/18 00:46:48 worker.go:46 Launching a worker with the following settings:\nINFO: 2019/02/18 00:46:48 worker.go:47 - Broker: amqp://guest:guest@localhost:5672/\nINFO: 2019/02/18 00:46:48 worker.go:49 - DefaultQueue: machinery_tasks\nINFO: 2019/02/18 00:46:48 worker.go:53 - ResultBackend: redis://localhost:6379\nINFO: 2019/02/18 00:46:48 worker.go:55 - AMQP: machinery_exchange\nINFO: 2019/02/18 00:46:48 worker.go:56   - Exchange: machinery_exchange\nINFO: 2019/02/18 00:46:48 worker.go:57   - ExchangeType: direct\nINFO: 2019/02/18 00:46:48 worker.go:58   - BindingKey: machinery_task\nINFO: 2019/02/18 00:46:48 worker.go:59   - PrefetchCount: 0\nINFO: 2019/02/18 00:46:48 amqp.go:94 [*] Waiting for messages. To exit press CTRL+C\nAfter sending SIGNINT signal (ctrl+c) I've got this log:\nlog\n^CWARNING: 2019/02/18 00:46:56 worker.go:89 Signal received: interrupt\nWARNING: 2019/02/18 00:46:56 worker.go:94 Waiting for running tasks to finish before shutting down\nWARNING: 2019/02/18 00:46:56 broker.go:101 Stop channel\n^CWARNING: 2019/02/18 00:46:58 worker.go:89 Signal received: interrupt\n^C^C^C^C^C^C^C^C^C^C^C^C^C\nand it keeps running unless I send kill signal like below:\nbash\nlsof -n -i4TCP:9080 && kill -9 {{PID}}\n. ",
    "praveenchourasia": "Any pointers on this? . ",
    "StTymur": "I'm sorry, I meant 'prepended' in my first comment. Yes, sure, the intended behaviour explained in the docs is the most natural one. It's just a little bug, that makes onSuccess callback to prepend only last argument. I think I could make a PR, some time soon. However, if someone used this exact behavior (relied on getting only last result as an argument) this would lead to breaking compatibility which would not be compile-time recognizable. What do you think about this?\nThanks for answer!\n. Great! \nI submitted PR for this!\nhttps://github.com/RichardKnop/machinery/pull/156\n. ",
    "codecov-io": "Codecov Report\n\nMerging #156 into master will decrease coverage by 0.05%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #156      +/-\n==========================================\n- Coverage   36.12%   36.07%   -0.06%   \n==========================================\n  Files          25       25            \n  Lines        2444     2445       +1   \n==========================================\n- Hits          883      882       -1   \n- Misses       1467     1471       +4   \n+ Partials       94       92       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/mongodb.go | 54.4% <0%> (-0.52%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6508cc5...4d513cc. Read the comment docs.\n. # Codecov Report\nMerging #157 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #157   +/-\n=======================================\n  Coverage   36.07%   36.07%         \n=======================================\n  Files          25       25         \n  Lines        2445     2445         \n=======================================\n  Hits          882      882         \n  Misses       1471     1471         \n  Partials       92       92\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 96a1c04...eebb0a7. Read the comment docs.\n. # Codecov Report\nMerging #159 into master will decrease coverage by 0.04%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #159      +/-\n==========================================\n- Coverage   36.07%   36.02%   -0.05%   \n==========================================\n  Files          25       25            \n  Lines        2445     2448       +3   \n==========================================\n  Hits          882      882            \n- Misses       1471     1474       +3   \n  Partials       92       92\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 70.58% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/tasks/workflow.go | 30.23% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/eager.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 26.24% <0%> (-0.19%) | :arrow_down: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/amqp.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 46eeb95...50fcece. Read the comment docs.\n. # Codecov Report\nMerging #161 into master will decrease coverage by 0.24%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #161      +/-\n==========================================\n- Coverage   36.15%   35.91%   -0.25%   \n==========================================\n  Files          25       25            \n  Lines        2442     2456      +14   \n==========================================\n- Hits          883      882       -1   \n- Misses       1465     1482      +17   \n+ Partials       94       92       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 24.18% <0%> (-2.06%) | :arrow_down: |\n| v1/backends/mongodb.go | 54.4% <0%> (-0.52%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7a3a118...2c15a10. Read the comment docs.\n. # Codecov Report\nMerging #173 into master will decrease coverage by 0.05%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #173      +/-\n==========================================\n- Coverage   35.51%   35.45%   -0.06%   \n==========================================\n  Files          25       25            \n  Lines        2520     2524       +4   \n==========================================\n  Hits          895      895            \n- Misses       1531     1535       +4   \n  Partials       94       94\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/server.go | 23.56% <0%> (-0.62%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 57cfb50...6a971bd. Read the comment docs.\n. # Codecov Report\nMerging #174 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #174   +/-\n=======================================\n  Coverage   35.45%   35.45%         \n=======================================\n  Files          25       25         \n  Lines        2524     2524         \n=======================================\n  Hits          895      895         \n  Misses       1535     1535         \n  Partials       94       94\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/server.go | 23.56% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0247ab8...f236d6b. Read the comment docs.\n. # Codecov Report\nMerging #182 into master will decrease coverage by 12.71%.\nThe diff coverage is n/a.\n\n\n```diff\n@@             Coverage Diff             @@\nmaster     #182       +/-\n===========================================\n- Coverage   35.45%   22.74%   -12.72%   \n===========================================\n  Files          25       19        -6   \n  Lines        2524     1420     -1104   \n===========================================\n- Hits          895      323      -572   \n+ Misses       1535     1054      -481   \n+ Partials       94       43       -51\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/backends/amqp.go | | |\n| v1/backends/redis.go | | |\n| v1/backends/memcache.go | | |\n| v1/backends/eager.go | | |\n| v1/backends/async_result.go | | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0601e25...6eaa1c0. Read the comment docs.\n. # Codecov Report\nMerging #183 into master will increase coverage by 0.27%.\nThe diff coverage is 75%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #183      +/-\n==========================================\n+ Coverage   35.45%   35.73%   +0.27%   \n==========================================\n  Files          25       25            \n  Lines        2524     2544      +20   \n==========================================\n+ Hits          895      909      +14   \n- Misses       1535     1539       +4   \n- Partials       94       96       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/backends/mongodb.go | 56.33% <75%> (+1.41%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2523544...254f5cc. Read the comment docs.\n. # Codecov Report\nMerging #188 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #188   +/-\n=======================================\n  Coverage   35.73%   35.73%         \n=======================================\n  Files          25       25         \n  Lines        2544     2544         \n=======================================\n  Hits          909      909         \n  Misses       1539     1539         \n  Partials       96       96\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 68f4317...c6c7084. Read the comment docs.\n. # Codecov Report\nMerging #195 into master will decrease coverage by 0.15%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #195      +/-\n==========================================\n- Coverage   35.73%   35.57%   -0.16%   \n==========================================\n  Files          25       25            \n  Lines        2544     2555      +11   \n==========================================\n  Hits          909      909            \n- Misses       1539     1550      +11   \n  Partials       96       96\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/backends/async_result.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/amqp.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f290825...b330812. Read the comment docs.\n. # Codecov Report\nMerging #199 into master will decrease coverage by 0.29%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #199     +/-\n=========================================\n- Coverage   35.87%   35.58%   -0.3%   \n=========================================\n  Files          26       27      +1   \n  Lines        2570     2591     +21   \n=========================================\n  Hits          922      922           \n- Misses       1552     1573     +21   \n  Partials       96       96\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/result.go | 0% <0%> (\u00f8) | |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/async_result.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 362bcfb...72dd392. Read the comment docs.\n. # Codecov Report\nMerging #203 into master will increase coverage by 0.04%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #203      +/-\n==========================================\n+ Coverage   35.44%   35.49%   +0.04%   \n==========================================\n  Files          27       27            \n  Lines        2567     2564       -3   \n==========================================\n  Hits          910      910            \n+ Misses       1563     1561       -2   \n+ Partials       94       93       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/factories.go | 63.35% <\u00f8> (+1.41%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c3ad28f...8f2f1a7. Read the comment docs.\n. # Codecov Report\nMerging #204 into master will increase coverage by 1.96%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #204      +/-\n==========================================\n+ Coverage   35.44%   37.41%   +1.96%   \n==========================================\n  Files          27       28       +1   \n  Lines        2567     1986     -581   \n==========================================\n- Hits          910      743     -167   \n+ Misses       1563     1149     -414   \n  Partials       94       94\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/env.go | 38.09% <\u00f8> (+4.76%) | :arrow_up: |\n| v1/config/file.go | 42.42% <\u00f8> (+4.12%) | :arrow_up: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/errors.go | 0% <0%> (\u00f8) | |\n| v1/brokers/amqp.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/tasks/task.go | 70.58% <0%> (-2.27%) | :arrow_down: |\n| v1/brokers/broker.go | 53.33% <0%> (-1.93%) | :arrow_down: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/tasks/validate.go | 100% <0%> (\u00f8) | :arrow_up: |\n| v1/retry/fibonacci.go | 100% <0%> (\u00f8) | :arrow_up: |\n| ... and 21 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c3ad28f...a9f4dad. Read the comment docs.\n. # Codecov Report\nMerging #205 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #205      +/-\n==========================================\n- Coverage   37.41%   37.39%   -0.02%   \n==========================================\n  Files          28       28            \n  Lines        1986     1987       +1   \n==========================================\n  Hits          743      743            \n- Misses       1149     1150       +1   \n  Partials       94       94\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/amqp.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 252e464...48a1bc6. Read the comment docs.\n. # Codecov Report\nMerging #207 into master will increase coverage by 8.82%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #207      +/-\n==========================================\n+ Coverage   37.39%   46.21%   +8.82%   \n==========================================\n  Files          28       23       -5   \n  Lines        1987     1573     -414   \n==========================================\n- Hits          743      727      -16   \n+ Misses       1150      752     -398   \n  Partials       94       94\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/factories.go | 64.07% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/backend.go | 40% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/eager.go | | |\n| v1/brokers/errors.go | | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 60bc4ff...f0e7c02. Read the comment docs.\n. # Codecov Report\nMerging #211 into master will decrease coverage by 8.66%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #211      +/-\n==========================================\n- Coverage   46.21%   37.55%   -8.67%   \n==========================================\n  Files          23       28       +5   \n  Lines        1573     1997     +424   \n==========================================\n+ Hits          727      750      +23   \n- Misses        752     1153     +401   \n  Partials       94       94\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/eager.go | 0% <0%> (\u00f8) | |\n| v1/brokers/errors.go | 0% <0%> (\u00f8) | |\n| v1/brokers/amqp.go | 1.11% <0%> (\u00f8) | |\n| v1/brokers/broker.go | 52.5% <0%> (\u00f8) | |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d94e515...1484b2a. Read the comment docs.\n. # Codecov Report\nMerging #212 into master will increase coverage by 1.94%.\nThe diff coverage is 60.49%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #212      +/-\n==========================================\n+ Coverage   37.55%   39.49%   +1.94%   \n==========================================\n  Files          28       29       +1   \n  Lines        1997     2157     +160   \n==========================================\n+ Hits          750      852     +102   \n- Misses       1153     1202      +49   \n- Partials       94      103       +9\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/factories.go | 62.85% <33.33%> (-1.23%) | :arrow_down: |\n| v1/brokers/aws_sqs.go | 60.89% <60.89%> (\u00f8) | |\n| v1/brokers/broker.go | 66.66% <66.66%> (+14.16%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 07db002...2910386. Read the comment docs.\n. # Codecov Report\nMerging #216 into master will increase coverage by 0.05%.\nThe diff coverage is 66.66%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #216      +/-\n==========================================\n+ Coverage   39.59%   39.64%   +0.05%   \n==========================================\n  Files          29       29            \n  Lines        2157     2159       +2   \n==========================================\n+ Hits          854      856       +2   \n+ Misses       1201     1200       -1   \n- Partials      102      103       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/aws_sqs.go | 61.39% <66.66%> (+0.49%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update df149a9...e0e8f13. Read the comment docs.\n. # Codecov Report\nMerging #233 into master will decrease coverage by 1%.\nThe diff coverage is 80%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #233      +/-\n==========================================\n- Coverage   40.45%   39.45%   -1.01%   \n==========================================\n  Files          30       29       -1   \n  Lines        2348     2162     -186   \n==========================================\n- Hits          950      853      -97   \n+ Misses       1276     1203      -73   \n+ Partials      122      106      -16\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/factories.go | 65% <80%> (-0.75%) | :arrow_down: |\n| v1/tasks/reflect.go | 52.17% <0%> (-11.16%) | :arrow_down: |\n| v1/tasks/task.go | 70.58% <0%> (-3.49%) | :arrow_down: |\n| v1/backends/memcache.go | 56.94% <0%> (-1.73%) | :arrow_down: |\n| v1/backends/redis.go | 67.51% <0%> (-1.2%) | :arrow_down: |\n| v1/backends/eager.go | 83.9% <0%> (-0.37%) | :arrow_down: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/config/config.go | 80% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/eager.go | 0% <0%> (\u00f8) | :arrow_up: |\n| ... and 7 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 58783d8...84d5edf. Read the comment docs.\n. # Codecov Report\nMerging #237 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #237   +/-\n=======================================\n  Coverage   39.56%   39.56%         \n=======================================\n  Files          29       29         \n  Lines        2171     2171         \n=======================================\n  Hits          859      859         \n  Misses       1205     1205         \n  Partials      107      107\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/broker.go | 66.66% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 163f8a1...a15824f. Read the comment docs.\n. # Codecov Report\nMerging #238 into master will decrease coverage by 0.12%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #238      +/-\n==========================================\n- Coverage   43.51%   43.38%   -0.13%   \n==========================================\n  Files          31       31            \n  Lines        2721     2729       +8   \n==========================================\n  Hits         1184     1184            \n- Misses       1403     1411       +8   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f7ddc8...9e8a1b8. Read the comment docs.\n. # Codecov Report\nMerging #239 into master will increase coverage by 4.52%.\nThe diff coverage is 75.97%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #239      +/-\n==========================================\n+ Coverage   39.31%   43.84%   +4.52%   \n==========================================\n  Files          30       31       +1   \n  Lines        2373     2703     +330   \n==========================================\n+ Hits          933     1185     +252   \n- Misses       1320     1384      +64   \n- Partials      120      134      +14\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/factories.go | 64.54% <0%> (-1.2%) | :arrow_down: |\n| v1/tasks/state.go | 18.18% <0%> (-0.57%) | :arrow_down: |\n| v1/backends/redis.go | 68.9% <100%> (+0.19%) | :arrow_up: |\n| v1/backends/memcache.go | 58.94% <100%> (+0.27%) | :arrow_up: |\n| v1/backends/mongodb.go | 54.18% <100%> (+0.25%) | :arrow_up: |\n| v1/backends/dynamodb.go | 76.85% <76.85%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dc677f0...74258f1. Read the comment docs.\n. # Codecov Report\nMerging #242 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #242   +/-\n=======================================\n  Coverage   41.11%   41.11%         \n=======================================\n  Files          29       29         \n  Lines        2274     2274         \n=======================================\n  Hits          935      935         \n  Misses       1217     1217         \n  Partials      122      122\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/state.go | 18.75% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 60713a1...d0c9692. Read the comment docs.\n. # Codecov Report\nMerging #244 into master will increase coverage by 0.64%.\nThe diff coverage is 53.84%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #244      +/-\n==========================================\n+ Coverage   40.41%   41.06%   +0.64%   \n==========================================\n  Files          29       29            \n  Lines        2284     2282       -2   \n==========================================\n+ Hits          923      937      +14   \n+ Misses       1245     1219      -26   \n- Partials      116      126      +10\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/aws_sqs.go | 59.63% <53.84%> (+4.44%) | :arrow_up: |\n| v1/backends/mongodb.go | 58.53% <0%> (+4.6%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d51d5d...4e055b2. Read the comment docs.\n. # Codecov Report\nMerging #246 into master will decrease coverage by 0.45%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #246      +/-\n==========================================\n- Coverage   41.11%   40.66%   -0.46%   \n==========================================\n  Files          29       29            \n  Lines        2274     2270       -4   \n==========================================\n- Hits          935      923      -12   \n- Misses       1217     1231      +14   \n+ Partials      122      116       -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/aws_sqs.go | 55.19% <100%> (-6.2%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 60713a1...e38f4dd. Read the comment docs.\n. # Codecov Report\nMerging #248 into master will decrease coverage by 0.69%.\nThe diff coverage is 4.65%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #248     +/-\n=========================================\n- Coverage   40.45%   39.76%   -0.7%   \n=========================================\n  Files          30       30           \n  Lines        2348     2389     +41   \n=========================================\n  Hits          950      950           \n- Misses       1276     1315     +39   \n- Partials      122      124      +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/signature.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 21.98% <0%> (-3.64%) | :arrow_down: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/tasks/task.go | 64.51% <20%> (-9.56%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 58783d8...6ac404c. Read the comment docs.\n. # Codecov Report\nMerging #250 into master will increase coverage by 2.05%.\nThe diff coverage is 14.28%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #250      +/-\n==========================================\n+ Coverage   40.35%   42.41%   +2.05%   \n==========================================\n  Files          29       30       +1   \n  Lines        2292     2452     +160   \n==========================================\n+ Hits          925     1040     +115   \n- Misses       1247     1281      +34   \n- Partials      120      131      +11\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/errors.go | 0% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/tasks/errors.go | 33.33% <33.33%> (\u00f8) | |\n| v1/tasks/task.go | 74.07% <33.33%> (+3.48%) | :arrow_up: |\n| v1/config/config.go | 77.77% <0%> (-2.23%) | :arrow_down: |\n| v1/brokers/aws_sqs.go | 60.08% <0%> (+6.38%) | :arrow_up: |\n| v1/backends/mongodb.go | 61.08% <0%> (+7.15%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c5e0570...600760d. Read the comment docs.\n. # Codecov Report\nMerging #255 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #255   +/-\n=======================================\n  Coverage   43.84%   43.84%         \n=======================================\n  Files          31       31         \n  Lines        2703     2703         \n=======================================\n  Hits         1185     1185         \n  Misses       1384     1384         \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/signature.go | 0% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/amqp.go | 1.1% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update faefd1d...1923cbc. Read the comment docs.\n. # Codecov Report\nMerging #258 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #258   +/-\n======================================\n  Coverage    39.6%   39.6%         \n======================================\n  Files          30      30         \n  Lines        2353    2353         \n======================================\n  Hits          932     932         \n  Misses       1302    1302         \n  Partials      119     119\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a249f4...6848dcd. Read the comment docs.\n. # Codecov Report\nMerging #260 into master will decrease coverage by 0.29%.\nThe diff coverage is 11.76%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #260     +/-\n=========================================\n- Coverage    39.6%   39.31%   -0.3%   \n=========================================\n  Files          30       30           \n  Lines        2353     2373     +20   \n=========================================\n+ Hits          932      933      +1   \n- Misses       1302     1320     +18   \n- Partials      119      120      +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/signature.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 21.52% <0%> (-0.46%) | :arrow_down: |\n| v1/tasks/workflow.go | 23.8% <16.66%> (-7.23%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4e01207...dfb0640. Read the comment docs.\n. # Codecov Report\nMerging #263 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #263   +/-\n=======================================\n  Coverage   39.31%   39.31%         \n=======================================\n  Files          30       30         \n  Lines        2373     2373         \n=======================================\n  Hits          933      933         \n  Misses       1320     1320         \n  Partials      120      120\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dc677f0...b1b45d2. Read the comment docs.\n. # Codecov Report\nMerging #266 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #266   +/-\n=======================================\n  Coverage   43.84%   43.84%         \n=======================================\n  Files          31       31         \n  Lines        2703     2703         \n=======================================\n  Hits         1185     1185         \n  Misses       1384     1384         \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/backends/dynamodb.go | 76.85% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 450b514...da41349. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@55e9fa0). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #269   +/-\n=========================================\n  Coverage          ?   43.84%         \n=========================================\n  Files             ?       31         \n  Lines             ?     2703         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1185         \n  Misses            ?     1384         \n  Partials          ?      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/broker.go | 66.66% <\u00f8> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 55e9fa0...b931858. Read the comment docs.\n. # Codecov Report\nMerging #273 into master will decrease coverage by 0.12%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #273      +/-\n==========================================\n- Coverage   43.77%   43.64%   -0.13%   \n==========================================\n  Files          31       31            \n  Lines        2707     2715       +8   \n==========================================\n  Hits         1185     1185            \n- Misses       1388     1396       +8   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 66ad5cb...cad80b2. Read the comment docs.\n. # Codecov Report\nMerging #277 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #277      +/-\n==========================================\n- Coverage   43.64%   43.63%   -0.02%   \n==========================================\n  Files          31       31            \n  Lines        2715     2716       +1   \n==========================================\n  Hits         1185     1185            \n- Misses       1396     1397       +1   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/aws_sqs.go | 53.65% <0%> (-0.33%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update baf17b7...ccc3b9c. Read the comment docs.\n. # Codecov Report\nMerging #279 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #279      +/-\n==========================================\n- Coverage   43.63%   43.61%   -0.02%   \n==========================================\n  Files          31       31            \n  Lines        2716     2717       +1   \n==========================================\n  Hits         1185     1185            \n- Misses       1397     1398       +1   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/aws_sqs.go | 53.33% <0%> (-0.33%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2d6e01e...74f2b63. Read the comment docs.\n. # Codecov Report\nMerging #280 into master will decrease coverage by 0.04%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #280      +/-\n==========================================\n- Coverage   43.63%   43.58%   -0.05%   \n==========================================\n  Files          31       31            \n  Lines        2716     2719       +3   \n==========================================\n  Hits         1185     1185            \n- Misses       1397     1399       +2   \n- Partials      134      135       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/aws_sqs.go | 52.69% <50%> (-0.97%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2d6e01e...49f76ce. Read the comment docs.\n. # Codecov Report\nMerging #281 into master will decrease coverage by 0.04%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #281      +/-\n==========================================\n- Coverage   43.56%   43.51%   -0.05%   \n==========================================\n  Files          31       31            \n  Lines        2720     2723       +3   \n==========================================\n  Hits         1185     1185            \n- Misses       1400     1403       +3   \n  Partials      135      135\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/aws_sqs.go | 51.46% <0%> (-0.92%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 70d0573...40ec6d2. Read the comment docs.\n. # Codecov Report\nMerging #286 into master will decrease coverage by 0.01%.\nThe diff coverage is 76.19%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #286      +/-\n==========================================\n- Coverage   43.51%   43.49%   -0.02%   \n==========================================\n  Files          31       31            \n  Lines        2721     2722       +1   \n==========================================\n  Hits         1184     1184            \n- Misses       1403     1405       +2   \n+ Partials      134      133       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/state.go | 17.64% <0%> (-0.54%) | :arrow_down: |\n| v1/backends/mongodb.go | 54.18% <80%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1f7ddc8...13eeca3. Read the comment docs.\n. # Codecov Report\nMerging #288 into master will decrease coverage by 0.07%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #288      +/-\n==========================================\n- Coverage   43.36%   43.29%   -0.08%   \n==========================================\n  Files          31       31            \n  Lines        2730     2735       +5   \n==========================================\n  Hits         1184     1184            \n- Misses       1413     1418       +5   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/aws_sqs.go | 50% <0%> (-1.47%) | :arrow_down: |\n| v1/factories.go | 64.76% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 21.52% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 371133a...a172465. Read the comment docs.\n. # Codecov Report\nMerging #289 into master will not change coverage.\nThe diff coverage is 50%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #289   +/-\n=======================================\n  Coverage   43.36%   43.36%         \n=======================================\n  Files          31       31         \n  Lines        2730     2730         \n=======================================\n  Hits         1184     1184         \n  Misses       1413     1413         \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/redis.go | 68.9% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 371133a...316ae80. Read the comment docs.\n. # Codecov Report\nMerging #290 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #290   +/-\n=======================================\n  Coverage   43.36%   43.36%         \n=======================================\n  Files          31       31         \n  Lines        2730     2730         \n=======================================\n  Hits         1184     1184         \n  Misses       1413     1413         \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/server.go | 21.52% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/backends/redis.go | 68.9% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/worker.go | 0% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/factories.go | 64.76% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/backends/memcache.go | 58.94% <0%> (\u00f8) | :arrow_up: |\n| v1/config/file.go | 37.83% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/dynamodb.go | 76.85% <0%> (\u00f8) | :arrow_up: |\n| v1/config/env.go | 32% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/async_result.go | 0% <0%> (\u00f8) | :arrow_up: |\n| ... and 1 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 371133a...bcb60e1. Read the comment docs.\n. # Codecov Report\nMerging #292 into master will increase coverage by 0.39%.\nThe diff coverage is 52.11%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #292      +/-\n==========================================\n+ Coverage   43.36%   43.76%   +0.39%   \n==========================================\n  Files          31       31            \n  Lines        2730     2790      +60   \n==========================================\n+ Hits         1184     1221      +37   \n- Misses       1413     1435      +22   \n- Partials      133      134       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/backends/mongodb.go | 56.06% <52.11%> (+1.87%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 783c60a...14cee41. Read the comment docs.\n. # Codecov Report\nMerging #296 into master will decrease coverage by 0.04%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #296      +/-\n==========================================\n- Coverage   43.76%   43.71%   -0.05%   \n==========================================\n  Files          31       31            \n  Lines        2790     2793       +3   \n==========================================\n  Hits         1221     1221            \n- Misses       1435     1438       +3   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6c3ca9d...3f11fb5. Read the comment docs.\n. \n",
    "maxpowel": "At this moment I'm calculating the next execution at the end of the task:\neta := time.Now().UTC().Add(time.Second * 5)\nsignature.ETA = &eta\nAnd then send this task to the server. I need to define a package variable with a reference to the machinery server instance so I can use it inside of the task for send tasks inside a task.\nYou should control onError and onSuccess events, imagine that your task raises an error and you dont enqueue your task, you should use onError to know this (in my case I have a dynamic scheduling, depending to the result the task will be re executed in a few seconds or in hours, like google crawler). ",
    "rvolykh": "Wow, thanks, very interesting idea.\nPossibly, it could fit our needs too \n. ",
    "xitongsys": "Thank you and your machinery\ud83d\udc4d\ud83d\udc4d\ud83d\udc4d\ud83d\udc4d\ud83d\udc4d\ud83d\udc4d, I will use it in my project. ",
    "mymtw": "@RichardKnop what to do if I need to pass map[string]string into my function, which will be called by signature?. yes, I already installed from commit 0ca0c2cf9e6f8d79757fdc089a34c854ad557138. I even don't know, is project should be fixed, with ability to use new API, or close this issue? Because govendor not cloning e.g. machinery/vendor folder, govendor install all deps that he will find in import statements.. ",
    "deanchou": "Good job.\nI'm following up too.. ",
    "zhex": "This is because you have AMQP settings in config, this will cause the worker search AMQP.bindingKey instead of defaultQueue. task object could not be found, That's the reason why it is always in pending state.. ",
    "MaJiangBo": "thanks very much. I have a task to copy file,like this:\nfor i=0;i < count;i++{\n        off :=i*bufferSize\n        , err := srcFile.ReadAt(buf,off)\n        if err != nil {\n            return 0,err\n        }\n        ,err =dstFile.WriteAt(buf,off)\n        if err != nil {\n            return 0,err\n        }\n    }\nwhen I copy 1M success,I want return a state.\nlike celery   update_state(). ",
    "vidmed": "Yeah, that`s ok. Thank you.. ",
    "Rio": "I'm just running into this myself. We're evaluating machinery for workflows but one of the requirements is context propagation for opentracing and maybe access tokens. If anybody has an already working solution to this we'd all like to know. Before we start doing double work ;). I've just noticed signature.Headers, might be an idea to add it to the context if not empty? Task should get a reference to the signatures then.. Another way would be to extend Signature with Context that is populated by the various Server.Send* methods that propagate the contexts in the case of SendChain.\nSorry, just brain dumping.. I didn't expect any reply soon so that's ok. We're all busy ;)\nI've seen the UseContext field and was thinking of loading the contents in Signature.Headers into the new context that's passed to the function. This way the payload structure doesn't need to be changed unless you foresee some issues with AMPQ with which I'm not familiar.\nWe could also scope the context within the headers with it's own key and type. This way we can ensure that what ever's put into the context is json serializable.\nThen when sending the tasks we could have SendTaskWithContext equivalents for all those functions.\nI'll have a go at it and we'll see where it ends up.. Oh yeah, if we could add a context type argument to the signatures that would also work.\nMissed your remark about that.. I've actually worked on this last week. It's functional but only for passing opentracing contexts. All other values are currently ignored. I'll make a PR but I'd like to put in some tests, haven't come around to that yet and the readme needs work.. PR is here: https://github.com/RichardKnop/machinery/pull/248. I think having your loggers, metrics and tracers as a dependency is generally a good idea. I think this should be added to the config.Config struct that defaults to GlobalTracer if not set.  . yes the main problem would be that any datatype can live inside a context and we have no way of quickly knowing if they can be serialized to json and if they can be it might not make sense when they get deserialized back on the worker side. This is the reason that contexts for now are opentracing specific as it defines a standard way of serializing/deserializing.\nA solution could be that we have a machinery specific type that we know how to serialize and deserialize just like the span context so we can guarantee that it will reach the workers.\nYou could, for now, use the baggage feature of opentracing to get data across. How ever that baggage will travel with the trace for as long as the trace is alive, and depending on the size it could add a significant network cost. Also binding your business logic to opentracing is generally not something that you'd want to do.. For example the go libraries for gRPC use a MD or metadata structure that they know how to serialize and deserialize when sending it over the network. Maybe we could have a MachineryMD?. I've omitted the actually updated vendor directory as I wasn't sure what the policy is around it and it introduced quite a lot of diff.... So added the vendor folder so the tests would at least run but it's not really clear to me why it simply stopped running. Any help here would be appreciated.. Closing and reopening PR in an attempt to get travis to restart the job.. go travis go!. Any time!. Looks good! But I'm wondering if we should give those hooks access to the context, at least in the pre task handler. I can imagine somebody wanting to inject something into the context before starting the task. What do you think @RichardKnop?. ",
    "cherubmiki": "This PR helped us too. However, it would be great if we could set the tracer to the server not just use the global tracer.\nWhat do you think about it?. ",
    "cool14521": "maybe i should change some code.. ",
    "simonoff": "@RichardKnop thank you for reply! But I don't need to execute them in parallel.\nI need to collect params as Map or similar structure and insert one request into DB instead of 1000 for example.\nAs I understand I can use a channel with Chords and after in trigger make an insert into DB.\nIs it simple to Chord a 1000 same tasks?. @RichardKnop also I want to push tasks one by one instead of creating a new queue inside app via channels. ",
    "surendratiwari3": "added support for mongodb result value to be in json. @codecov-io please let you know more in details.. @RichardKnop please merge this code as i tested this with load.this is required when someone want the result value should be inserted as json into mongodb for further processing. after changes i am able to insert as json into mongodb like this.\ndb.tasks.find({_id: \"1505906984389049\"} ).pretty()\n{\n    \"_id\" : \"1505906984389049\",\n    \"state\" : \"SUCCESS\",\n    \"results\" : [\n        {\n            \"value\" : {\n                \"FreeSWITCH-Hostname\" : \"ip-172-31-29-181\",\n                \"FreeSWITCH-Switchname\" : \"ip-172-31-29-181\",\n                \"Up-Time\" : \"0 years, 5 days, 4 hours, 14 minutes, 20 seconds, 13 milliseconds, 785 microseconds\",\n                \"Max-Sessions\" : \"1000\",\n                \"Event-Calling-Line-Number\" : \"75\",\n                \"Event-Name\" : \"HEARTBEAT\",\n                \"Session-Since-Startup\" : \"7515\",\n                \"Event-Date-Local\" : \"2017-09-20 11:29:44\",\n                \"Event-Calling-Function\" : \"send_heartbeat\",\n                \"Session-Per-Sec-Max\" : \"26\",\n                \"Event-Calling-File\" : \"switch_core.c\",\n                \"Event-Info\" : \"System Ready\",\n                \"Event-Date-GMT\" : \"Wed, 20 Sep 2017 11:29:44 GMT\",\n                \"Event-Date-Timestamp\" : \"1505906984389049\",\n                \"Session-Per-Sec\" : \"30\",\n                \"Session-Peak-Max\" : \"26\",\n                \"Session-Peak-FiveMin\" : \"1\",\n                \"FreeSWITCH-IPv4\" : \"172.31.29.181\",\n                \"FreeSWITCH-IPv6\" : \"::1\",\n                \"Event-Sequence\" : \"1142811\",\n                \"FreeSWITCH-Version\" : \"1.9.0+git~20170911T194756Z~2aea0c329b~64bit\",\n                \"Session-Per-Sec-Last\" : \"0\",\n                \"Idle-CPU\" : \"94.766667\",\n                \"Core-UUID\" : \"a475b338-99e5-11e7-8b36-3bdaa2eeeccc\",\n                \"Uptime-msec\" : \"447260013\",\n                \"Session-Count\" : \"0\",\n                \"Session-Per-Sec-FiveMin\" : \"1\"\n            },\n            \"type\" : \"Json\"\n        }\n    ]\n}\n. @RichardKnop please add this pull request this will help to many programmer to store the result as json in mongosb.. @RichardKnop can you suggest me to make some changes.as it is required by most of people.. i think i only made changes to backend mongo.might be integration test needs to be changed to get this scenario done.\n. @RichardKnop now every check is passed with all integration test. will you please elaborate your use case?. ",
    "SurendraPlivo": "@RichardKnop  please merge this request so we can take the latest master and work with it.. @RichardKnop i will resolve this issue. thanks mate for pointing the right thing. @RichardKnop this will work like this if we sent json in the string args.like \n\"{\\\"name\\\":\\\"surendra\\\",\\\"designation\\\":\\\"voip developer\\\"}\" after processing this in mongodb previously we are not able to save this as json.we are just sending this as string to result in mongodb.but using my patch we can successfully identify weather result is the json string or string.if it is json string we are creating proper json format to insert into mongodb.so using field name anyone can get the field value into mongodb.. this task is received by worker {\"UUID\":\"1506990890097305\",\"RoutingKey\":\"machinery_task\",\"Name\":\"add\",\"ETA\":null,\"GroupUUID\":\"\",\"GroupTaskCount\":0,\"Args\":[{\"Type\":\"string\",\"Value\":\"{\\\"Event-Name\\\":\\\"CHANNEL_DESTROY\\\",\\\"Core-UUID\\\":\\\"7786a18a-a5ce-11e7-a95d-2bdd1d3c2dfa\\\",\\\"FreeSWITCH-Hostname\\\":\\\"ip-172-31-29-181\\\",\\\"FreeSWITCH-Switchname\\\":\\\"ip-172-31-29-181\\\",\\\"FreeSWITCH-IPv4\\\":\\\"172.31.29.181\\\",\\\"FreeSWITCH-IPv6\\\":\\\"::1\\\",\\\"Event-Date-Local\\\":\\\"2017-10-03 00:34:50\\\",\\\"Event-Date-GMT\\\":\\\"Tue, 03 Oct 2017 00:34:50 GMT\\\",\\\"Event-Date-Timestamp\\\":\\\"1506990890097305\\\",\\\"Event-Calling-File\\\":\\\"switch_core_session.c\\\",\\\"Event-Calling-Function\\\":\\\"switch_core_session_perform_destroy\\\",\\\"Event-Calling-Line-Number\\\":\\\"1563\\\",\\\"Event-Sequence\\\":\\\"1111337\\\",\\\"Channel-State\\\":\\\"CS_REPORTING\\\",\\\"Channel-Call-State\\\":\\\"HANGUP\\\",\\\"Channel-State-Number\\\":\\\"12\\\",\\\"Channel-Name\\\":\\\"sofia/internal/889@18.221.70.239\\\",\\\"Unique-ID\\\":\\\"a45b76da-a7d2-11e7-bab1-2bdd1d3c2dfa\\\",\\\"Call-Direction\\\":\\\"inbound\\\",\\\"Presence-Call-Direction\\\":\\\"inbound\\\",\\\"Channel-HIT-Dialplan\\\":\\\"true\\\",\\\"Channel-Call-UUID\\\":\\\"a45b76da-a7d2-11e7-bab1-2bdd1d3c2dfa\\\",\\\"Answer-State\\\":\\\"hangup\\\",\\\"Hangup-Cause\\\":\\\"WRONG_CALL_STATE\\\",\\\"variable_direction\\\":\\\"inbound\\\",\\\"variable_uuid\\\":\\\"a45b76da-a7d2-11e7-bab1-2bdd1d3c2dfa\\\",\\\"variable_call_uuid\\\":\\\"a45b76da-a7d2-11e7-bab1-2bdd1d3c2dfa\\\",\\\"variable_session_id\\\":\\\"5425\\\",\\\"variable_sip_from_user\\\":\\\"889\\\",\\\"variable_sip_from_uri\\\":\\\"889@18.221.70.239\\\",\\\"variable_sip_from_host\\\":\\\"18.221.70.239\\\",\\\"variable_video_media_flow\\\":\\\"disabled\\\",\\\"variable_audio_media_flow\\\":\\\"disabled\\\",\\\"variable_text_media_flow\\\":\\\"disabled\\\",\\\"variable_channel_name\\\":\\\"sofia/internal/889@18.221.70.239\\\",\\\"variable_sip_call_id\\\":\\\"1724400310-1378422954-1814834803\\\",\\\"variable_ep_codec_string\\\":\\\"CORE_PCM_MODULE.pcmu@8000h@20i@64000b\\\",\\\"variable_hangup_cause\\\":\\\"WRONG_CALL_STATE\\\",\\\"variable_hangup_cause_q850\\\":\\\"101\\\"}\"}],\"Immutable\":false,\"RetryCount\":0,\"RetryTimeout\":0,\"OnSuccess\":null,\"OnError\":null,\"ChordCallback\":null}\ni processed this task and get following result \n{\"Event-Name\":\"CHANNEL_DESTROY\",\"Core-UUID\":\"7786a18a-a5ce-11e7-a95d-2bdd1d3c2dfa\",\"FreeSWITCH-Hostname\":\"ip-172-31-29-181\",\"FreeSWITCH-Switchname\":\"ip-172-31-29-181\",\"FreeSWITCH-IPv4\":\"172.31.29.181\",\"FreeSWITCH-IPv6\":\"::1\",\"Event-Date-Local\":\"2017-10-03 00:35:34\",\"Event-Date-GMT\":\"Tue, 03 Oct 2017 00:35:34 GMT\",\"Event-Date-Timestamp\":\"1506990934437245\",\"Event-Calling-File\":\"switch_core_session.c\",\"Event-Calling-Function\":\"switch_core_session_perform_destroy\",\"Event-Calling-Line-Number\":\"1563\",\"Event-Sequence\":\"1111353\",\"Channel-State\":\"CS_REPORTING\",\"Channel-Call-State\":\"HANGUP\",\"Channel-State-Number\":\"12\",\"Channel-Name\":\"sofia/internal/889@18.221.70.239\",\"Unique-ID\":\"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\"Call-Direction\":\"inbound\",\"Presence-Call-Direction\":\"inbound\",\"Channel-HIT-Dialplan\":\"true\",\"Channel-Call-UUID\":\"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\"Answer-State\":\"hangup\",\"Hangup-Cause\":\"WRONG_CALL_STATE\",\"variable_direction\":\"inbound\",\"variable_uuid\":\"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\"variable_call_uuid\":\"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\"variable_session_id\":\"5426\",\"variable_sip_from_user\":\"889\",\"variable_sip_from_uri\":\"889@18.221.70.239\",\"variable_sip_from_host\":\"18.221.70.239\",\"variable_video_media_flow\":\"disabled\",\"variable_audio_media_flow\":\"disabled\",\"variable_text_media_flow\":\"disabled\",\"variable_channel_name\":\"sofia/internal/889@18.221.70.239\",\"variable_sip_call_id\":\"347149456-2004223162-1626098735\",\"variable_ep_codec_string\":\"CORE_PCM_MODULE.pcmu@8000h@20i@64000b\",\"variable_hangup_cause\":\"WRONG_CALL_STATE\",\"variable_hangup_cause_q850\":\"101\"}\nin mongodb i can see > db.tasks.find({_id: \"1506990934437245\"} ).pretty()\n{\n    \"_id\" : \"1506990934437245\",\n    \"state\" : \"SUCCESS\",\n    \"results\" : [\n        {\n            \"type\" : \"Json\",\n            \"value\" : {\n                \"FreeSWITCH-IPv6\" : \"::1\",\n                \"Event-Date-Timestamp\" : \"1506990934437245\",\n                \"Channel-State\" : \"CS_REPORTING\",\n                \"Channel-State-Number\" : \"12\",\n                \"Unique-ID\" : \"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\n                \"variable_audio_media_flow\" : \"disabled\",\n                \"variable_text_media_flow\" : \"disabled\",\n                \"variable_ep_codec_string\" : \"CORE_PCM_MODULE.pcmu@8000h@20i@64000b\",\n                \"Core-UUID\" : \"7786a18a-a5ce-11e7-a95d-2bdd1d3c2dfa\",\n                \"FreeSWITCH-Hostname\" : \"ip-172-31-29-181\",\n                \"Event-Date-GMT\" : \"Tue, 03 Oct 2017 00:35:34 GMT\",\n                \"Event-Sequence\" : \"1111353\",\n                \"Answer-State\" : \"hangup\",\n                \"variable_direction\" : \"inbound\",\n                \"variable_call_uuid\" : \"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\n                \"Event-Calling-Function\" : \"switch_core_session_perform_destroy\",\n                \"Channel-HIT-Dialplan\" : \"true\",\n                \"variable_session_id\" : \"5426\",\n                \"variable_sip_from_user\" : \"889\",\n                \"variable_sip_call_id\" : \"347149456-2004223162-1626098735\",\n                \"Channel-Call-State\" : \"HANGUP\",\n                \"variable_video_media_flow\" : \"disabled\",\n                \"variable_channel_name\" : \"sofia/internal/889@18.221.70.239\",\n                \"Presence-Call-Direction\" : \"inbound\",\n                \"Channel-Call-UUID\" : \"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\n                \"FreeSWITCH-Switchname\" : \"ip-172-31-29-181\",\n                \"Event-Calling-Line-Number\" : \"1563\",\n                \"Hangup-Cause\" : \"WRONG_CALL_STATE\",\n                \"variable_uuid\" : \"bec8db84-a7d2-11e7-bab3-2bdd1d3c2dfa\",\n                \"variable_hangup_cause\" : \"WRONG_CALL_STATE\",\n                \"Event-Name\" : \"CHANNEL_DESTROY\",\n                \"Event-Date-Local\" : \"2017-10-03 00:35:34\",\n                \"Channel-Name\" : \"sofia/internal/889@18.221.70.239\",\n                \"Call-Direction\" : \"inbound\",\n                \"variable_sip_from_host\" : \"18.221.70.239\",\n                \"FreeSWITCH-IPv4\" : \"172.31.29.181\",\n                \"Event-Calling-File\" : \"switch_core_session.c\",\n                \"variable_sip_from_uri\" : \"889@18.221.70.239\",\n                \"variable_hangup_cause_q850\" : \"101\"\n            }\n        }\n    ]\n}\n. @RichardKnop so using this changes anyone can save the result as json into mongodb.so here like if i want to get Event-Date-Timestamp, i can easily get it using results.Value.Event-Date-Timestamp. previously it was not possible because values are saved as string only.now it worked with all the datatypes you defined as well as for json result storage in mongodb.. @RichardKnop if you have any doubt about this feature let me know so i can better explain you.. @sstarcher right now its not implemented in the machinery. but we can change the behaviour of ResultsExpireIn to never expire the result. i started working on this. i will be coming back with solution.. ",
    "rakeshbala": "Alright. I realized that I can re-create the AsyncResult if I specify a UUID for the signature and then use a signature with same UUID while creating the new AsyncResult.\n```\n    uuid := uuid.NewV4()\n    signature := &tasks.Signature{\n        Name: \"add\",\n        Args: []tasks.Arg{\n            {\n                Type:  \"int64\",\n                Value: 1,\n            },\n            {\n                Type:  \"int64\",\n                Value: 2,\n            },\n        },\n        UUID: uuid.String(),\n    }\n_, err = server.SendTask(signature)\n\n```\nand then later \nsig := &tasks.Signature{\n        Name: \"add\",\n        UUID: uuid.String(),\n    }\n   asyncResult := backends.NewAsyncResult(sig, server.GetBackend())\ngives me the right result. \nI'll leave the issue open for a couple of days to see if you have any comments.. I think this is because:\nhttps://github.com/RichardKnop/machinery/blob/master/v1/tasks/reflect.go#L152\nvalue.([]interface{}) returns false for an interface{} type pointing to []string\nHere is a an example snippet https://play.golang.org/p/HqIvwMqRa4N\nHowever, I haven't checked how it is working while retrieving the results.. CI build failure seems to be due to other reasons and not related to this PR.. ",
    "siredwin": "Golang naturally supports using variadics with known arguments first and arbitrary trailing arguments. It would be nice if callback appended the results to its back.\nI am also running into the same problem. \nTask1 returns a link after a long process.\nTask 2 takes that link and sends email with it. Task 2 has 4 other mostly variable parameters plus the link.\nWhat is the status on this?. @RichardKnop , perfect!\nI can use the following now as a callback task with no problems at all.\nfunc sendEmail(sender, to, subject, template string, message ...string) (string, error){}. This will probably be a breaking change but it is something that can help in the future.. I won't be able to test it but I will take the fix. It only occurred in production but not development, and I don't want to break the code again.\n\nThe way you described it sounds like what was happening. If someone else comes across it, they can comment.. Thanks, I will look into it.. ",
    "makkalot": "That was quick @RichardKnop, thanks giving it a try :). ",
    "alexhudici": "Thats awesome thank you @RichardKnop. I will have a look at the library and for the workers, I figured I would have to create a new machinery server and register new workers to it since the server is where the redis IP gets registered. Unless it is safe to switch the IP of the existing server struct and just restart it since the queue would have the same name.. ",
    "RainbowMango": "@alexhudici Have you tested master-slave scenario?  Does worker retry to slave when master doesn't work? . @RichardKnop . Someone asked a similar question.  https://github.com/RichardKnop/machinery/issues/200\nI checked the code carefully.  Update Redis broker's URL can't take affect automatically.\nWhen worker need connect redis, just get one from pool. broker.pool will not rebuild even all connection become invalid in the pool.\n// open returns or creates instance of Redis connection\nfunc (b *Broker) open() redis.Conn {\n    if b.pool == nil {\n        b.pool = b.NewPool(b.socketPath, b.host, b.password, b.db, b.GetConfig().Redis)\n    }\n    if b.redsync == nil {\n        var pools = []redsync.Pool{b.pool}\n        b.redsync = redsync.New(pools)\n    }\n    return b.pool.Get()\n}\n@RichardKnop \nCan you confirm about this?\nSo, can we set broker.pool with nil when we found connect failed with redis and reload conf?. Bad news. After Redis failover from master to slave the connection cannot be re-build. \nI get the following errors:\"Set state pending error: READONLY You can't write against a read only slave.\"\nI used https://github.com/go-redis/redis that can switch connection to new master. . I need to update broker's URL runtime, I tried following steps:\n1. worker.Quit()\n2. Update server's config   server.config.Broker = newURL;  as well as server.config.ResultBackend = newURL.\n2. server.SetBroker(broker)  //the broker comes from BrokerFactory(server.GetConfig())\n3. server.SetBackend(backend) //the backend comes from BrokerFactory(server.GetConfig())\n4. server.GetBroker().SetRegisteredTaskNames(server.GetRegisteredTaskNames())\nI really works...but, i wonder if there is a easy way for doing this?\n@RichardKnop . @RichardKnop \nI'm trying to fix this issue. And i think the 'host'\u3001'password'\u3001'db' should be removed. Since the three info could get from broker.cnf, seems it breaks some design patterns.  Even though configuration changed outside, the there info never be updated. \n` // Broker represents a Redis broker\ntype Broker struct {\n    common.Broker\n    common.RedisConnector\n    host              string\n    password          string\n    db                int\n    pool              redis.Pool\n    stopReceivingChan chan int\n    stopDelayedChan   chan int\n    processingWG      sync.WaitGroup // use wait group to make sure task processing completes on interrupt signal\n    receivingWG       sync.WaitGroup\n    delayedWG         sync.WaitGroup\n    // If set, path to a socket file overrides hostname\n    socketPath string\n    redsync    redsync.Redsync\n}\n// New creates new Broker instance\nfunc New(cnf *config.Config, host, password, socketPath string, db int) iface.Broker {\n    b := &Broker{Broker: common.NewBroker(cnf)}\n    b.host = host\n    b.db = db\n    b.password = password\n    b.socketPath = socketPath\nreturn b\n\n} `. @eldad87 What's your problem?  For my project, I restarted worker for workaround.. How to re-trigger CI?\nI checked the CI output, not related to my submission.\n@RichardKnop . CI unstable. @RichardKnop . \n. Please have a try without any redis configuration, just use the default. Then share your result. . ",
    "jzhbiao": "I want to send task just use redis command  in redis, then machinery listen it , is that a proper way?. I try to rpush like this:\n127.0.0.1:6379> rpush machinery_tasks \"[123 34 85 85 73 68 34 58 34 116 97 115 107\n 95 51 55 101 57 101 48 55 57 45 100 99 57 101 45 52 55 48 101 45 56 98 51 100 4\n5 56 50 102 101 50 101 48 99 99 56 101 57 34 44 34 78 97 109 101 34 58 34 99 97\n108 95 110 117 114 101 114 121 95 100 97 116 97 34 44 34 82 111 117 116 105 110\n103 75 101 121 34 58 34 34 44 34 69 84 65 34 58 110 117 108 108 44 34 71 114 111\n 117 112 85 85 73 68 34 58 34 34 44 34 71 114 111 117 112 84 97 115 107 67 111 1\n17 110 116 34 58 48 44 34 65 114 103 115 34 58 110 117 108 108 44 34 72 101 97 1\n00 101 114 115 34 58 110 117 108 108 44 34 73 109 109 117 116 97 98 108 101 34 5\n8 102 97 108 115 101 44 34 82 101 116 114 121 67 111 117 110 116 34 58 48 44 34\n82 101 116 114 121 84 105 109 101 111 117 116 34 58 48 44 34 79 110 83 117 99 99\n 101 115 115 34 58 110 117 108 108 44 34 79 110 69 114 114 111 114 34 58 110 117\n 108 108 44 34 67 104 111 114 100 67 97 108 108 98 97 99 107 34 58 110 117 108 1\n08 125]\"\nand it show: WARNING: 2017/11/28 10:17:38 worker.go:60 ****: json: cannot unma\nrshal number into Go value of type tasks.Signature\n. thank you for reply. ",
    "regaw-leinad": "This is not a clean PR, especially with the addition of IDE metadata.\nI agree there is an issue here, though.  Right now the code is checking the Backend config protocol to determine what Broker to create.  A better change here would be to change cnf.ResultBackend to cnf.Broker, since we still want to check for an amqps broker.\nUPDATE: Looks like this was fixed as a part of @RichardKnop 's latest PR https://github.com/RichardKnop/machinery/pull/207\nThis should be closed. A quick example... I group the handlers in related structs, then create a new \"handler\" and pass the db context to its initialization function.  The worker can then access it's db field while executing.\nFor example:\n```go\ntype TaskHandler struct {\n    db *DB\n}\nfunc NewTaskHandler(db DB) TaskHandler {\n    return &TaskHandler{db: db}\n}\nfunc (h *TaskHandler) DoThing1(a, b string) error {\n    // Do stuff with database in here\n    h.db.Query(\"...\")\n    return nil\n}\n```. ",
    "KarlTango": "@regaw-leinad Thanks for your information.  . @RichardKnop . As @regaw-leinad has said, right now the code is checking the Backend config protocol to determine what Broker to create . I just want to fix this bug simply. Now this bug  has been fixed by you. So it's OK now.  . ",
    "vano468": "UPD:\nI see merged PR for fixing this problem: https://github.com/RichardKnop/machinery/pull/45/files\nUnfortunately @RichardKnop you're reverted it back in this commit: https://github.com/RichardKnop/machinery/commit/89b4824ead892abedb223c3559bb7cc8e3029eb8#diff-64062d33aaac544f83f3791cac8fbb7c\nWas it reverted by mistake?. @RichardKnop thanks for quick response. ",
    "mavidser": "Solved. Ah. I did face this issue earlier, but I proceeded with changing the config file to fix the issue, like you suggested earlier.\nThough, this particular issue actually turned out to be #208. On further thought - it's better to call signal.Reset() after quitting, and add a note about this behavior on the Readme. . ",
    "ahlusar1989": "@RichardKnop It looks like this still needs to be implemented, if I am not mistaken?. @RichardKnop Is this something that I can help with (resolving the merge conflicts?). ",
    "WillAbides": "Thanks for taking the time to provide detailed answers to all my questions.\n\nTake a look at fanout exchange type for AMQP\n\nUnless I am misunderstanding fanout, I don't think it will work for us because we need each task to be processed by exactly one worker. Also, adding RabbitMQ to the stack could be a tough sell. Not that there's anything wrong with RabbitMQ. There's just a lot of effort involved in supporting a new service.\n\nI would consider it but:\na) It would need to be implemented for both AMQP and Redis \n\nThis will be challenging for me because I haven't worked with AMQP before. I would need to partner with somebody for that portion.\n\nb) It would need to be reasonably well done so the code is readable and possible to maintain.\n\nOf course all my code is perfectly readable and maintainable. ;-) Some local preschools even read it aloud at story time.\n\nIt should be possible to implement your own broker by implementing this interface: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/interfaces.go#L9\n\nThanks for pointing this out. This should be all I need on that front.\n\nPlease explain how you would modify the Redis broker (e.g. what Redis commands would you use instead of BLPOP\n\nI would use a single BLPOP with multiple keys.  nextTask might end up looking something like this:\ngolang\nfunc (b *RedisBroker) nextTask(queues ...string) (result []byte, err error) {\n...\n    blpopArgs := make([]interface{}, len(queues) + 1)\n    for i, q := range queues {\n        blpopArgs[i] = q\n    }\n    blpopArgs[len(queues)] = 1\n    items, err := redis.ByteSlices(conn.Do(\"BLPOP\", blpopArgs...))\n...\n}\nnextDelayedTask would be trickier because ZRANGEBYSCORE doesn't accept multiple keys like BLPOP does.  That would make it O(n) for the number of queues the worker is watching. That could possibly be mitigated by having one goroutine watch the delayed queues on behalf of all workers and forward new jobs to whichever worker has been waiting longest.  Another option would be a goroutine that regularly checks delayed queues and moves tasks to the regular queue when their time comes.\nHowever, my current project doesn't have a need for delayed tasks, so I wouldn't implement it in a custom broker unless I intend for it to be merged upstream.. We ended up changing the design of the project that was going to require this change.  Instead we are using machinery in a more conventional way.\nBefore the design change, I did start on a broker that would allow a worker to work multiple queues.  As @RichardKnop said, I was able to implement brokers.Interface to create a custom broker in my own package.  I couldn't use machinery.NewServer with my custom broker because BrokerFactory doesn't know about it, but that wouldn't have been necessary anyway.. Thanks for doing this so quickly.  I was taking a strict interpretation of backward compatibility since the methods like NewRedisBackend are exported, but if they aren't used outside the machinery in practice, then it isn't really a backward compatibility problem.. ",
    "mjetpax": "Okay, so I fixed the conflict. But, the build does not pass CI. I think this is due to the circular vendoring. I'm not sure how to bypass this, the new broker requires certain dependencies be present in the vendor folder to run correctly and to pass CI. But those requirements have not been merged with the project yet.. Also, Amazon SQS is very user friendly, a couple of clicks and everything is all set up. The only gotcha is IAM permissions (if you're not already familiar with the AWS IAM).. ",
    "ZhenhangTung": "I believe the problems are these two:\n1) Errors were thrown when running make lint\n2) Something went wrong when running tests in Go v1.9\nI'm now taking care of these problems.. On my local env, whose go version is go version go1.9.2 darwin/amd64, all tests pass. Maybe it's other reasons or we could rerun CI after those errors are thrown by make lint being fixed.. I've found a bug would sometimes break the tests. Another PR would be requested later, after @mjetpax  finish his review.\nWe appreciate your work and you really helped our team. @RichardKnop . I think this time I've fixed the problem shown in before, by adding one more channel called firstStep to control go routine.  But this time the CI still failed, so I wonder if you could rerun it again? @RichardKnop . Sorry for this, I was gonna PR this to my company's fork for review, but mischose the origin master.\n  . Please fix the errors thrown by make lint :) @duyanghao . Sure! I'm working on it.\nI think there is one key point we need to discuss in this PR, which is CreatedAt being introduced. Check: https://github.com/RichardKnop/machinery/pull/239/files#diff-aaae9e6274d067f485799ecfc4cdc75aR37\nFrom my perspective, I think this could help users to debug and trace job's history better.. @RichardKnop Could you rebuild our latest build in TravisCI, for we don't have the write access. Seems like there is a timeout error. Thanks!. ",
    "joelpresence": "Actually, I don't even think redis://password@host:port is a legal URL is it?  I think the spec requires the user info to either be user or user:password, but just password is not legal because it's ambiguous - does the plain text value without a colon represent a password or a user.. ",
    "ankurs": "Thanks. ",
    "gimmeshelter": "This is because the dev o the uuid project checked in an API breaking change.  You need to use commit 063359185d32c6b045fa171ad7033ea545864fa1 which was the last commit before the change . ",
    "BaSO4": "pls check your rabbitmq or other mq . ",
    "snapercloud": "That's a good idea. I would be good to keep an error count (or the number of times the task tried to execute so far) for inspection. Maybe allow combining timed retries with a limit of attempts. How would this work with a chain? It should fire the remaining tasks only when the failed task suceeds.. ",
    "jmcarp": "I think that registering a map of named retry functions and including the map key in the signature would work. We could also use an api more like celery's and allow users to return custom retry errors from a task that include a delay: return NewRetry(5 * time.Second). WDYT?. Looks good. Thanks!. ",
    "ItsLeeOwen": "closed for PR 230. closed for PR 230. creating a new PR from master. going to make individual PRs per feature change. @RichardKnop  sorry was the issue report too poor?  I can recreate w/ standard issue templating; expected behavior, actual result, steps to reproduce if that's helpful.. Fix Merged https://github.com/RichardKnop/machinery/pull/247. PR - https://github.com/RichardKnop/machinery/pull/242. PR - https://github.com/RichardKnop/machinery/pull/244. PR - https://github.com/RichardKnop/machinery/pull/246. added sqs integration test.  sqs connection vars are passed in from environment.. @RichardKnop   I haven't been able to track it down, but there is some condition where integration_tests can sometimes hang.. @RichardKnop  I thought about that, are there other errs in mind that might occur in this block where sleep would be undesirable?  I was thinking any consumption err might be worthy of a delay or even max retry backoff but am uncertain.. @tailingchen  @ahlusar1989  you could pull master, merge it into your branch, and resolve conflicts in the branch.. ",
    "duyanghao": "@RichardKnop Done for Rebase.. ",
    "WingGao": "I found these code https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L113-L116\nAnd https://github.com/RichardKnop/machinery/blob/master/v1/brokers/redis.go#L258-L261\nwill shrink the uint64 value int the Signature.Args.. @RichardKnop You're right.\nHere is the test.\ngo\nfunc TestNewSignature(t *testing.T) {\n    var uint64Val uint64 = 185135722552891243\n    sig := NewSignature(\"test\", []Arg{\n        {Type: \"uint64\", Value: uint64Val},\n    })\n    msg, _ := json.Marshal(sig)\n    sig2 := &Signature{}\n    json.Unmarshal(msg, sig2)\n    sig2Val := sig2.Args[0].Value.(float64)\n    t.Logf(\"%f\", sig2Val)\n    if uint64(sig2Val) != uint64Val {\n        t.Errorf(\"uint64 value should be %d but get %f\", uint64Val, sig2Val)\n    }\n}\nIt will convert to float64.\nMy current solution is  change the uint64 to string in the my sender code.\nI think it's not a universal method, but is the easiest.. ",
    "steven-zou": "@RichardKnop Is it true?. Thanks a lot for your quick reply @RichardKnop, here are more details:\nBroker and backend are redis\n```\n\nbroker: 'redis://...:6379'\ndefault_queue: machinery_tasks\nresult_backend: 'redis://...:6379'\nresults_expire_in: 3600000\n```\nUse the example code to start 2 workers\ngo run example/machinery.go -c example/config.yml worker\nCreate an example long running task\n```\n// LongRunningTask ...\nfunc LongRunningTask() error {\n    done := make(chan bool)\n    err := make(chan error)\n    //sig := make(chan os.Signal, 1)\n    //signal.Notify(sig, os.Interrupt, syscall.SIGTERM, os.Kill)\n    go func() {\n        tk := time.Tick(5 * time.Second)\n        for {\n            select {\n            case t := <-tk:\n                log.INFO.Printf(\"Long running task: %s\\n\", t.String())\n            case <-time.After(5 * time.Minute):\n                done <- true\n                return\n            }\n        }\n    }()\n  <-done\n  return nil\n\n}\n```\nSend the task for executing\n```\n//With command:\n//go run example/machinery.go -c example/config.yml send\nasyncResult, err := server.SendTask(&longRunningTask)\n    if err != nil {\n        return fmt.Errorf(\"Could not send task: %s\", err.Error())\n    }\nresults, err := asyncResult.GetWithTimeout(time.Duration(time.Second*60), time.Duration(time.Millisecond*1))\nif err != nil {\n    return fmt.Errorf(\"Getting task result failed with error: %s\", err.Error())\n}\nlog.INFO.Printf(\"Task returned = %v\\n\", tasks.HumanReadableResults(results))\n\n```\nThe task will be executed on one worker. Terminate/Kill that worker node, the running task will not be put to another worker as a failure recovery task.\nThe task result kept on redis becomes partial data\n{\n  \"TaskUUID\": \"task_4730820e-fa28-41bf-80c9-e8caa0dab147\",\n  \"State\": \"STARTED\",\n  \"Results\": null,\n  \"Error\": \"\"\n} \n. @RichardKnop any answers?. @RichardKnop Thanks for your reply. I will consider the PR option at proper time.. ",
    "mrjoes": "I want to chime in and ask a related question: is this applicable to SQS backend too? EDIT: Did a quick code review - message is ACKed before the task is executed.\nJust in case, gocraft/work (redis) does it with a small Lua script that moves a task from a pending queue to a \"currently running queue\" atomically. If the task is complete - it's removed from a running queue. If it failed - it's moved back to the pending queue. If a worker is restarted - tasks from the running queue are moved back to the pending queue.. Isn't it a bug? It would only delete the task if task processor failed.. ",
    "bseenu": "Richard,\nThanks for responding, I am checking the pending message count in the queue\nmessages_url :=http://localhost:15672/api/queues/%2f/machinery_taskreq, _ := http.NewRequest(\"GET\", messages_url, nil)\n       client := &http.Client{}\n       http_resp, _ := client.Do(req)\n        defer http_resp.Body.Close()\n        var data map[string]interface{}\n        rbytes, _ := ioutil.ReadAll(http_resp.Body)\n        json.Unmarshal(rbytes, &data)\n        if data[\"messages\"] == float64(0) {\n                 fmt.Printf(\"No more pending tasks\")\n             } else {\n                fmt.Printf(\"Pending tasks %g\\n, data[\"messages\"]\n           }\nThough this will not take care of the cases where messages are being re-tried as they go to seperate queue, but is it possible to check for all the possible queues to determine the outstanding tasks. Also any reason why you want to put the retry in a seperate queue instead of same one?. Looks like this is only push based model, the worker is waiting for the event to pick up the task - https://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L172-L203\nI think the model should be to connect to the queue directly and start consuming which will take care of the cases where the worker is added later on too. My settings are identical and i am using the direct exchange type as well\nI have started some long running tasks yesterday with 4 workers and the messages were not getting de-queued as expected so to make it faster i will bring up 4 additional workers with the same settings as earlier ones and they just keep waiting there even though when they are message\nI am able to reproduce this only the workers which were adding during pushing of tasks are working. Possible, but i had many messages in the queue in hundreds. Even i am seeing this issue and this has caused duplicates, I am trying to process millions of files asynchronously using machinery, And my team is asking me to rewrite this in python using celery. ",
    "F21": "Thanks @RichardKnop. Can you tag a new release? Since this has a breaking change, maybe a major release?. I upgraded to 1.5.0 and noticed a bunch of Stop channel warning messages in my logs. Is there a reason why Stop channel is classed as a warning?. ",
    "georgekarrv": "I am seeing similar behavior with Redis as the queue broker.. so we are spinning up 10 docker containers that each launch 1 worker. using redis as the broker it works fine for some time and then it will stop pushing pending jobs through and when i jump onto the redis image and run 'dump machinery_tasks' the value is empty. There seems to be an issue where the key machinery tasks gets clobbered.. @RichardKnop I would like to address this issue with a PR that limits the worker from grabbing a task off the queue unless it has a pool available but I do not have permissions to create a branch to base the PR off of.. At least for redis I simply moved the pools creation to StartConsuming and monitor len(pool) - len(deliveries) > 0 to allow pulling off the queue\nThis works very well for me.\n```\ndiff --git a/v1/brokers/redis.go b/v1/brokers/redis.go\nindex afa46e6..9ea6e40 100644\n--- a/v1/brokers/redis.go\n+++ b/v1/brokers/redis.go\n@@ -70,6 +70,14 @@ func (b *RedisBroker) StartConsuming(consumerTag string, concurrency int, taskPr\n// Channel to which we will push tasks ready for processing by worker\ndeliveries := make(chan []byte)\n\n\npool := make(chan struct{}, concurrency)\n+\n// initialize worker pool with maxWorkers workers\ngo func() {\nfor i := 0; i < concurrency; i++ {\npool <- struct{}{}\n}\n\n}()\n// A receivig goroutine keeps popping messages from the queue by BLPOP\n// If the message is valid and can be unmarshaled into a proper structure\n@@ -85,12 +93,16 @@ func (b RedisBroker) StartConsuming(consumerTag string, concurrency int, taskPr\n        case <-b.stopReceivingChan:\n            return\n        default:\n-               task, err := b.nextTask(b.cnf.DefaultQueue)\n-               if err != nil {\n-                   continue\n+               // If concurrency is limited, limit the tasks being pulled off the queue\n+               // until a pool is available\n+               if concurrency == 0 || (len(pool) - len(deliveries) > 0) {\n+                   task, err := b.nextTask(b.cnf.DefaultQueue)\n+                   if err != nil {\n+                       continue\n+                   }\n+\n+                   deliveries <- task\n            }\n-\n-               deliveries <- task\n        }\n    }\n}()\n@@ -125,7 +137,7 @@ func (b RedisBroker) StartConsuming(consumerTag string, concurrency int, taskPr\n    }\n}()\n\n\nif err := b.consume(deliveries, concurrency, taskProcessor); err != nil {\n\nif err := b.consume(deliveries, pool, concurrency, taskProcessor); err != nil {\n        return b.retry, err\n    }\n\n@@ -214,16 +226,7 @@ func (b RedisBroker) GetPendingTasks(queue string) ([]tasks.Signature, error)\n// consume takes delivered messages from the channel and manages a worker pool\n // to process tasks concurrently\n-func (b RedisBroker) consume(deliveries <-chan []byte, concurrency int, taskProcessor TaskProcessor) error {\n-   pool := make(chan struct{}, concurrency)\n-\n-   // initialize worker pool with maxWorkers workers\n-   go func() {\n-       for i := 0; i < concurrency; i++ {\n-           pool <- struct{}{}\n-       }\n-   }()\n-\n+func (b RedisBroker) consume(deliveries <-chan []byte, pool chan struct{}, concurrency int, taskProcessor TaskProcessor) error {\n    errorsChan := make(chan error, concurrency*2)\nfor {\n\n```. #271 but this is a solution for the redis broker. something similar would have to be done for the other brokers.. with that a 'successful' dequeu of a task where you push a task onto the deliveries channel would not reset the timer and you would never return to this select in the loop,\nyou could do this so long as on the line after deliveries you have another timer.Reset(time.Millisecond) or some such.. ya that looks good \ud83d\udc4d . agreed. ",
    "dmora": "i'll be spending some time today in this issue. For a quick glance it seems it could be related to the retry logic when using AMPQ as broker. I'll keep you posted.. ",
    "marconn": "I was able to reproduce the issue described.\nLooking in here https://github.com/RichardKnop/machinery/blob/master/v1/brokers/amqp.go#L238\nmessages are acknowledged, but only in the consumer queue. They remain in the publisher queue if autoAck is set to false (which is ok, we don't want to lose any task if the server goes down :) ).\n@RichardKnop I would like to try and develop a fix, could you please provide me a hint to know where to start or what should i take into account?\nThanks. ",
    "svanburen": "This worked, thanks so much!. ",
    "1yzz": "using CLI tools just like in this repo's example. ",
    "s4kibs4mi": "@RichardKnop Can you please give an example or any link with example ?. ",
    "mjfeller": "An example would be very helpful. I've tried\n- backend.SetStateSuccess()\n- backend.SetStateFailure()\n- backend.PurgeState()\nNo luck as of yet. I'll keep digging in the mean time though.\n. ",
    "niranjan94": "@RichardKnop ping \ud83d\ude04 A simple example would be helpful. I'm not having any luck figuring a built-in way out for this. Thanks !. ",
    "bmarini": "Thanks for the info @RichardKnop that helps!. ",
    "tailingchen": "@RichardKnop My bad. The original logic is right! I think I have misunderstood on this. \nIf the task need to retry, the worker will re-send the task and return nil error if re-sending task successfully, then the old task will be deleted. Otherwise, the old task will be kept in SQS if failed to re-send task.\nhttps://github.com/RichardKnop/machinery/blob/master/v1/worker.go#L189\n. @RichardKnop The fail reason is some channels are initialed in New function, so the pointers of these channels are not nil anymore. It can not pass reflect.DeepEqual. My opinion is to define a initWaitGroup or mutex to avoid calling StopConsuming before instances does not complete initialization yet.. ",
    "edwardmp": "@paulm17 Running into the same issue. Did you ever find a proper solution. If so, can you share it?. ",
    "casoetan": "Same question here.. ",
    "wisp888": "me too!. ",
    "valeriano-manassero": "@RichardKnop I have no clue on why Travis build is failing. Can you help me please?. Rebased and Dep updated.\nIt seems to me Dep update doesn't only update logging but also broke some dependency.. @RichardKnop Since a dep ensure gives me errors, I tried to manually import the right file from logging and checks are now good.\nI'm not sure this is good enough for you, waiting for your feedback.. @RichardKnop Can you help me on this issue please?\nOr, if there's no solution, what about referenced PR?\nThanks!. @RichardKnop after rebasing and adding the right unit test checks are now ok.. ",
    "codecov[bot]": "Codecov Report\n\nMerging #300 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #300      +/-\n==========================================\n- Coverage   47.68%   47.66%   -0.02%   \n==========================================\n  Files          29       29            \n  Lines        2548     2549       +1   \n==========================================\n  Hits         1215     1215            \n- Misses       1199     1200       +1   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/log/log.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 480748c...9929af9. Read the comment docs.\n. # Codecov Report\nMerging #304 into master will increase coverage by 0.24%.\nThe diff coverage is 53.84%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #304      +/-\n==========================================\n+ Coverage   47.89%   48.13%   +0.24%   \n==========================================\n  Files          29       29            \n  Lines        2537     2549      +12   \n==========================================\n+ Hits         1215     1227      +12   \n  Misses       1189     1189            \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 28.85% <100%> (+7.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e255d32...9fe4e9d. Read the comment docs.\n. # Codecov Report\nMerging #308 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #308   +/-\n=======================================\n  Coverage   47.89%   47.89%         \n=======================================\n  Files          29       29         \n  Lines        2537     2537         \n=======================================\n  Hits         1215     1215         \n  Misses       1189     1189         \n  Partials      133      133\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e255d32...57772d3. Read the comment docs.\n. # Codecov Report\nMerging #312 into master will increase coverage by 0.16%.\nThe diff coverage is 75%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #312      +/-\n==========================================\n+ Coverage   47.89%   48.05%   +0.16%   \n==========================================\n  Files          29       29            \n  Lines        2537     2547      +10   \n==========================================\n+ Hits         1215     1224       +9   \n- Misses       1189     1190       +1   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/backends/redis/redis.go | 70.11% <75%> (+1.21%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6cef439...2a3cdf9. Read the comment docs.\n. # Codecov Report\nMerging #313 into master will increase coverage by 0.13%.\nThe diff coverage is 44.18%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #313      +/-\n==========================================\n+ Coverage   46.95%   47.09%   +0.13%   \n==========================================\n  Files          29       29            \n  Lines        2645     2682      +37   \n==========================================\n+ Hits         1242     1263      +21   \n- Misses       1269     1280      +11   \n- Partials      134      139       +5\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/tasks/signature.go | 0% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/tasks/validate.go | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/sqs/sqs.go | 50.86% <0%> (-0.6%) | :arrow_down: |\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/factories.go | 56.7% <54.28%> (+0.89%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 21f6dd0...24d847a. Read the comment docs.\n. # Codecov Report\nMerging #318 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster    #318   +/-\n======================================\n  Coverage    48.3%   48.3%         \n======================================\n  Files          29      29         \n  Lines        2559    2559         \n======================================\n  Hits         1236    1236         \n  Misses       1190    1190         \n  Partials      133     133\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 55e0829...94cbfba. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@86c9fc7). Click here to learn what that means.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #321   +/-\n=========================================\n  Coverage          ?   48.26%         \n=========================================\n  Files             ?       29         \n  Lines             ?     2561         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1236         \n  Misses            ?     1192         \n  Partials          ?      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <\u00f8> (\u00f8) | |\n| v1/brokers/amqp/amqp.go | 4.25% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 86c9fc7...f6e3bbe. Read the comment docs.\n. # Codecov Report\nMerging #331 into master will decrease coverage by 0.05%.\nThe diff coverage is 62.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #331      +/-\n==========================================\n- Coverage   46.92%   46.86%   -0.06%   \n==========================================\n  Files          29       29            \n  Lines        2647     2650       +3   \n==========================================\n  Hits         1242     1242            \n- Misses       1271     1272       +1   \n- Partials      134      136       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/backends/dynamodb/dynamodb.go | 76.14% <62.5%> (-0.71%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 98564b6...bea598f. Read the comment docs.\n. # Codecov Report\nMerging #333 into master will decrease coverage by 0.36%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #333      +/-\n==========================================\n- Coverage   47.71%   47.35%   -0.37%   \n==========================================\n  Files          29       29            \n  Lines        2603     2623      +20   \n==========================================\n  Hits         1242     1242            \n- Misses       1228     1247      +19   \n- Partials      133      134       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/factories.go | 55.81% <0%> (-10.25%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a928be...6dd8b1f. Read the comment docs.\n. # Codecov Report\nMerging #338 into master will increase coverage by 0.04%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #338      +/-\n==========================================\n+ Coverage   48.34%   48.38%   +0.04%   \n==========================================\n  Files          29       29            \n  Lines        2565     2567       +2   \n==========================================\n+ Hits         1240     1242       +2   \n  Misses       1192     1192            \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/server.go | 29.8% <100%> (+0.94%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 07b9cdc...1c898a9. Read the comment docs.\n. # Codecov Report\nMerging #343 into master will decrease coverage by 0.15%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #343      +/-\n==========================================\n- Coverage   48.38%   48.23%   -0.16%   \n==========================================\n  Files          29       29            \n  Lines        2567     2575       +8   \n==========================================\n  Hits         1242     1242            \n- Misses       1192     1200       +8   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/worker.go | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 759f2c7...372c7ca. Read the comment docs.\n. # Codecov Report\nMerging #345 into master will decrease coverage by 0.61%.\nThe diff coverage is 1.72%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #345      +/-\n==========================================\n- Coverage   48.38%   47.76%   -0.62%   \n==========================================\n  Files          29       29            \n  Lines        2567     2600      +33   \n==========================================\n  Hits         1242     1242            \n- Misses       1192     1225      +33   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/amqp/amqp.go | 3.61% <1.72%> (-0.64%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 72d106f...4eeeed5. Read the comment docs.\n. # Codecov Report\nMerging #347 into master will decrease coverage by 0.05%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #347      +/-\n==========================================\n- Coverage   47.76%   47.71%   -0.06%   \n==========================================\n  Files          29       29            \n  Lines        2600     2603       +3   \n==========================================\n  Hits         1242     1242            \n- Misses       1225     1228       +3   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/common/broker.go | 38.63% <0%> (-2.83%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 75b0e4f...dc773ef. Read the comment docs.\n. # Codecov Report\nMerging #350 into master will decrease coverage by 0.05%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #350      +/-\n==========================================\n- Coverage   47.71%   47.65%   -0.06%   \n==========================================\n  Files          29       29            \n  Lines        2603     2606       +3   \n==========================================\n  Hits         1242     1242            \n- Misses       1228     1231       +3   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/amqp/amqp.go | 3.57% <0%> (-0.05%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a928be...4f34aae. Read the comment docs.\n. # Codecov Report\nMerging #351 into master will decrease coverage by 0.07%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #351      +/-\n==========================================\n- Coverage   47.71%   47.64%   -0.08%   \n==========================================\n  Files          29       29            \n  Lines        2603     2607       +4   \n==========================================\n  Hits         1242     1242            \n- Misses       1228     1232       +4   \n  Partials      133      133\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/server.go | 29.03% <0%> (-0.77%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a928be...1c819a5. Read the comment docs.\n. # Codecov Report\nMerging #353 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #353   +/-\n=======================================\n  Coverage   47.71%   47.71%         \n=======================================\n  Files          29       29         \n  Lines        2603     2603         \n=======================================\n  Hits         1242     1242         \n  Misses       1228     1228         \n  Partials      133      133\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6a928be...bed8369. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@06b0913). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster     #362   +/-\n=========================================\n  Coverage          ?   47.29%         \n=========================================\n  Files             ?       29         \n  Lines             ?     2626         \n  Branches          ?        0         \n=========================================\n  Hits              ?     1242         \n  Misses            ?     1250         \n  Partials          ?      134\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 06b0913...60603b4. Read the comment docs.\n. # Codecov Report\nMerging #363 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #363   +/-\n=======================================\n  Coverage   46.95%   46.95%         \n=======================================\n  Files          29       29         \n  Lines        2645     2645         \n=======================================\n  Hits         1242     1242         \n  Misses       1269     1269         \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/validate.go | 100% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 21f6dd0...4084b81. Read the comment docs.\n. # Codecov Report\nMerging #366 into master will decrease coverage by 0.03%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #366      +/-\n==========================================\n- Coverage   46.95%   46.92%   -0.04%   \n==========================================\n  Files          29       29            \n  Lines        2645     2647       +2   \n==========================================\n  Hits         1242     1242            \n- Misses       1269     1271       +2   \n  Partials      134      134\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/tasks/signature.go | 0% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/brokers/sqs/sqs.go | 50.86% <0%> (-0.6%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1a643dd...c4ab49a. Read the comment docs.\n. # Codecov Report\nMerging #369 into master will increase coverage by 0.18%.\nThe diff coverage is 63.54%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #369      +/-\n==========================================\n+ Coverage   46.97%   47.15%   +0.18%   \n==========================================\n  Files          29       29            \n  Lines        2659     2600      -59   \n==========================================\n- Hits         1249     1226      -23   \n+ Misses       1274     1239      -35   \n+ Partials      136      135       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/factories.go | 56.15% <100%> (+0.33%) | :arrow_up: |\n| v1/backends/mongo/mongodb.go | 61.45% <62.36%> (+5.38%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ed2891...8e1c55e. Read the comment docs.\n. # Codecov Report\nMerging #370 into master will increase coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #370      +/-\n==========================================\n+ Coverage   46.86%   46.88%   +0.01%   \n==========================================\n  Files          29       29            \n  Lines        2650     2649       -1   \n==========================================\n  Hits         1242     1242            \n+ Misses       1272     1271       -1   \n  Partials      136      136\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/amqp/amqp.go | 3.47% <0%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 26de5d8...021431d. Read the comment docs.\n. # Codecov Report\nMerging #373 into master will increase coverage by 0.04%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #373      +/-\n=========================================\n+ Coverage   46.86%   46.9%   +0.04%   \n=========================================\n  Files          29      29            \n  Lines        2650    2652       +2   \n=========================================\n+ Hits         1242    1244       +2   \n  Misses       1272    1272            \n  Partials      136     136\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/backends/eager/eager.go | 84.78% <100%> (+0.33%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 26de5d8...43e752f. Read the comment docs.\n. # Codecov Report\nMerging #374 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff          @@\nmaster   #374   +/-\n=====================================\n  Coverage      47%    47%         \n=====================================\n  Files          29     29         \n  Lines        2657   2657         \n=====================================\n  Hits         1249   1249         \n  Misses       1272   1272         \n  Partials      136    136\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 77c8be0...ce91f94. Read the comment docs.\n. # Codecov Report\nMerging #375 into master will decrease coverage by 0.03%.\nThe diff coverage is 16.66%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster     #375      +/-\n==========================================\n- Coverage      47%   46.97%   -0.04%   \n==========================================\n  Files          29       29            \n  Lines        2657     2659       +2   \n==========================================\n  Hits         1249     1249            \n- Misses       1272     1274       +2   \n  Partials      136      136\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/common/redis.go | 0% <0%> (\u00f8) | :arrow_up: |\n| v1/backends/redis/redis.go | 70.11% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 77c8be0...ef813c8. Read the comment docs.\n. # Codecov Report\nMerging #385 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #385   +/-\n=======================================\n  Coverage   47.08%   47.08%         \n=======================================\n  Files          29       29         \n  Lines        2604     2604         \n=======================================\n  Hits         1226     1226         \n  Misses       1241     1241         \n  Partials      137      137\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/brokers/sqs/sqs.go | 51.09% <0%> (\u00f8) | :arrow_up: |\n| v1/server.go | 29.03% <0%> (\u00f8) | :arrow_up: |\n| v1/brokers/amqp/amqp.go | 3.46% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c632cfa...b6e3d13. Read the comment docs.\n. # Codecov Report\nMerging #387 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #387   +/-\n=======================================\n  Coverage   47.08%   47.08%         \n=======================================\n  Files          29       29         \n  Lines        2604     2604         \n=======================================\n  Hits         1226     1226         \n  Misses       1241     1241         \n  Partials      137      137\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c632cfa...2be3a56. Read the comment docs.\n. # Codecov Report\nMerging #388 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #388   +/-\n=======================================\n  Coverage   47.08%   47.08%         \n=======================================\n  Files          29       29         \n  Lines        2604     2604         \n=======================================\n  Hits         1226     1226         \n  Misses       1241     1241         \n  Partials      137      137\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 22b2f6e...17c04a6. Read the comment docs.\n. # Codecov Report\nMerging #391 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #391   +/-\n=======================================\n  Coverage   47.08%   47.08%         \n=======================================\n  Files          29       29         \n  Lines        2604     2604         \n=======================================\n  Hits         1226     1226         \n  Misses       1241     1241         \n  Partials      137      137\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| v1/config/config.go | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| v1/backends/mongo/mongodb.go | 61.45% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 196eace...77f61b5. Read the comment docs.\n. \n",
    "joe-mcnuggets": "@RichardKnop thank you, but Semver tells that minor versions should be backwards-compatible. I guess that the best solution for this changes is to wait for a major release \nBtw, 1.3.7 tag is still availiable. @RichardKnop it still doesn't work with dep:\ndep ensure -update\nhangs with subprocess calling \nssh git@github.com git-upload-pack 'mongodb/specifications.git'\nstrace on this subprocess says \n--- stopped by SIGTTOU ---\n@owenhaynes can it be fixed without changing version constraints in dep to RC branches?\nBut it can be my mistake, of course. @owenhaynes thank you!. Tracking master branch is bad because of potential unexpected breaking changes. I see that you use your own package, so I recommend to start versioning it properly and lock dependency on the minor version.. I think it's better to avoid untranslated words in documentation.. ",
    "eldad87": "@RainbowMango, is there any updated with your issue?. It simply means that you better isolate your infrastructure and provide Machinery with its own Redis.\nAnyway, its hard to diagnose your issue without going through your code and configuration.\nTry using SQS as a broker and Redis as a result backend. If that doesn't work and Redis is still loaded, reduce the amount of works. In addition, avoid using the AsyncResult object, as it connect to your Result backend. Or simply upgrade your Redis instance.. If I understood correctly, your app require the following steps:\n1. Queued / Received - Should be implemented in your application controller / HTTP handler (Validation pasted, message sent successfully to the message broker etc.).\n2. In Progress - Fork and merge this PR. Use the Pre-task hook.\n3. Done / Error - use The signature.OnSuccess, signature.OnErrror.\nHope it helps.. Please merge this branch, It will help with instrumentation.\nThanks!. Nice work! Thank you!\n. Its up to you.\nMachinery can run as a producer and consumer on the same process.. Checkout my boilerplate (WIP). There is no such thing as github.com/RichardKnop/machinery/v1/errors, errors can be find in github.com/RichardKnop/machinery/v1/tasks. This variable define the goroutine pool size, in turn, this pool is used to send your tasks to the broker.\n0 means unlimited, in order words the pool size will be equal to the amount of tasks you're about to send.\nValue of 0 will send your messages the fastest (probably), 1 will keep the messages in order etc.\n. Did you tried to set the Signature.ETA?\n. You need to apply that limitation directly into your task implementation:\n```\nfunc Add(args ...int64) (int64, error) {\n  // Implement timeout\n}\nserver.RegisterTask(\"add\", Add)\n. It's just an example. int64 is just an argument. Anyway, you can do something like:\nimport \"github.com/eapache/go-resiliency/deadline\"\nfunc MyTask() error {\n  dl := deadline.New(1 * time.Second)\nerr := dl.Run(func(stopper <-chan struct{}) error {\n    // do something potentially slow\n    return nil\n  })\nswitch err {\n  case deadline.ErrTimedOut:\n    // execution took too long, oops\n    return tasks. NewErrRetryTaskLater(\"some error\", 4 * time.Hour)\n  default:\n    // some other error\n  }\n}\nserver.RegisterTask(\"myTask\", MyTask)\n```\nHope it helps.. No, but you can implement it using:\n```\n    server.SetPreTaskHandler(...) // Issued before you produce a task\nworker.SetPreTaskHandler(...) // Issued before a task is executed\nworker.SetErrorHandler(...) // Issued when a task is result in an error\nworker.SetPostTaskHandler(...) // Issued after a task finished running\n\n```\nI expect to release a PR for this one during the next few month, as it required for my boilerplate project.. ",
    "Rizbe": "Removing aws from vendor file seemed to do the trick, saw this issue in the aws-sdk project:\nhttps://github.com/aws/aws-sdk-go/issues/634. bump. ",
    "koron": "the type of SQSConfig.Client is *sqs.SQS.\nbut sqs package is vendored by machinery,\nso values of *sqs.SQS type won't be injected by library users (out of machinery pacakge).\nso current SQSConfig.Client never work for users,\nor too hard to make it work.. ",
    "shinsuke-nara": "This issue must be resolved. Now machinery does not vendor sqs package.. ",
    "ljesparis": "i did add this fastest solution(not the best one)\n1) add global variables\ngo\nvar once = &sync.Once{}\nvar conn *amqp.Connection\nvar channel *amqp.Channel\nvar confirmsChan <- chan amqp.Confirmation\n2) create connection once\ngo\nonce.Do(func() {\n        conn, channel, _, confirmsChan, _, err = b.Connect(\n            b.GetConfig().Broker,\n            b.GetConfig().TLSConfig,\n            b.GetConfig().AMQP.Exchange,     // exchange name\n            b.GetConfig().AMQP.ExchangeType, // exchange type\n            signature.RoutingKey,            // queue name\n            true,                            // queue durable\n            false,                           // queue delete when unused\n            b.GetConfig().AMQP.BindingKey, // queue binding key\n            nil, // exchange declare args\n            nil, // queue declare args\n            amqp.Table(b.GetConfig().AMQP.QueueBindingArgs), // queue binding args\n        )\n    })\nI did this to test my point and results from my server are:\nwith one connection:\n[GIN] 2018/07/25 - 11:08:08 | 200 |   29.280761ms |       ...\nwithout one connection:\n[GIN] 2018/07/25 - 11:10:11 | 200 |  124.820257ms |      ...\n. ",
    "baocaixiong": "@RichardKnop hi bro, I had fixed it. could you merge my branch to master? or have other questions?. ",
    "eldadvcita": "For now, based on my PR: https://github.com/RichardKnop/machinery/pull/338\nI just implemented a blank Result backend:\n```\npackage blank\nimport (\n    \"github.com/RichardKnop/machinery/v1/backends/iface\"\n    \"github.com/RichardKnop/machinery/v1/common\"\n    \"github.com/RichardKnop/machinery/v1/config\"\n    \"github.com/RichardKnop/machinery/v1/tasks\"\n)\n// Backend represents a \"blank\" result backend\ntype Backend struct {\n    common.Backend\n}\n// New creates blank Backend instance\nfunc New() iface.Backend {\n    return &Backend{\n        Backend: common.NewBackend(new(config.Config)),\n    }\n}\n// InitGroup creates and saves a group meta data object\nfunc (b *Backend) InitGroup(groupUUID string, taskUUIDs []string) error {\n    return nil\n}\n// GroupCompleted returns true if all tasks in a group finished\nfunc (b *Backend) GroupCompleted(groupUUID string, groupTaskCount int) (bool, error) {\n    return false, nil\n}\n// GroupTaskStates returns states of all tasks in the group\nfunc (b Backend) GroupTaskStates(groupUUID string, groupTaskCount int) ([]tasks.TaskState, error) {\n    ret := make([]*tasks.TaskState, 0, groupTaskCount)\n    return ret, nil\n}\n// TriggerChord flags chord as triggered in the backend storage to make sure\n// chord is never trigerred multiple times. Returns a boolean flag to indicate\n// whether the worker should trigger chord (true) or no if it has been triggered\n// already (false)\nfunc (b *Backend) TriggerChord(groupUUID string) (bool, error) {\n    return true, nil\n}\n// SetStatePending updates task state to PENDING\nfunc (b Backend) SetStatePending(signature tasks.Signature) error {\n    return nil\n}\n// SetStateReceived updates task state to RECEIVED\nfunc (b Backend) SetStateReceived(signature tasks.Signature) error {\n    return nil\n}\n// SetStateStarted updates task state to STARTED\nfunc (b Backend) SetStateStarted(signature tasks.Signature) error {\n    return nil\n}\n// SetStateRetry updates task state to RETRY\nfunc (b Backend) SetStateRetry(signature tasks.Signature) error {\n    return nil\n}\n// SetStateSuccess updates task state to SUCCESS\nfunc (b Backend) SetStateSuccess(signature tasks.Signature, results []*tasks.TaskResult) error {\n    return nil\n}\n// SetStateFailure updates task state to FAILURE\nfunc (b Backend) SetStateFailure(signature tasks.Signature, err string) error {\n    return nil\n}\n// GetState returns the latest task state\nfunc (b Backend) GetState(taskUUID string) (tasks.TaskState, error) {\n    return nil, nil\n}\n// PurgeState deletes stored task state\nfunc (b *Backend) PurgeState(taskUUID string) error {\n    return nil\n}\n// PurgeGroupMeta deletes stored group meta data\nfunc (b *Backend) PurgeGroupMeta(groupUUID string) error {\n    return nil\n}\nfunc (b *Backend) IsAMQP() bool {\n    return false\n}\n```\nAnd I used it like this:\n```\nconf = ...\nmachineryBroker, err := machineryServer.BrokerFactory(&machineryConf)\nif err != nil {\n    return nil, err\n}\nmachineryBackend := machineryBlankBackend.New()\nserver := machineryServer.NewServerWithBrokerBackend(&conf, machineryBroker, machineryBackend)\n```\nHave fun!. Please close the issue.\nThanks!. ",
    "chawco": "I'm also running into this issue -- @RichardKnop any chance we could get a v1.0.2 tag on your fork of redsync to address this?. ",
    "speza": "Newer PR created. @RichardKnop should be sorted now. Branch had been sitting around a while! . ",
    "bharat-p": "Maybe we disable this check when a special environment variable e.g. DISABLE_STRICT_SQS_CHECK is set? @RichardKnop  if you are ok, i can create a PR for this change. @feelinc Issue you are facing with SQS fifo queue is described here : #365 and a fix has been made to address it: #366. @RichardKnop thanks for merging the fix,  could you please tag a new release with this fix?. @RichardKnop  while above PR fixed the issue with worker not listening on custom queue specified, it surfaced another issue, which is Machinery worker when started to with NewCustomQueueWorker is not able to delete the task from SQS, this is because code which handles job deletion is still using Default queue name when making request to AWS to delete the task, instead of custom queue. Will create a PR to fix this issue as well.\n2019-01-29T14:27:59.375-0800    INFO    v1/worker.go:71 Broker failed with error: ReceiptHandleIsInvalid: The receipt handle \"AQEBWKQM6bk1XAs3PpOt3NU9PiV9GbBNjX2l0C+d3y6QSn8OnsT9husBn1kbnRqlMqJkvCDETQCPVl9DNFyW7wCS93QN+omYX4QgNMpchPTi5ZdjulyqQNyxHEYnyoFip9kvgk8bqOBKlLTzf14sRt19RQ1hFrNJdTbgTbfunEQG1tjAXK5oqL2BOaQQPTKjM1tEpxlXWmsMJe7036/DLeULk5k4pvPglRAS4OHrjGlLvDN+VBgICVYVFMEhZCrAylaqZz8l44Ou9FR8UtWuLvZWZi0hvszAZAeXZog9dwj8LqalYntl+FnmosMds0VjAEq6gCBqN3aANmEeCEDMh2AIf3ZRyXCnCuNZdtQuwvsFylkNZlbuwi9MtCd5+FlU741ah3Xy8ZT59Fq7YL55DtQ/0w==\" is not valid for this queue.\n    status code: 404. https://github.com/RichardKnop/machinery/pull/377 to fix task deletion issue. cc @RichardKnop this fixes #367 . Closing for now, as I need to do some local testing. ",
    "alext234": "Yes,  why not. Thank you. . ",
    "PyYoshi": "Thank you so much :satisfied:. ",
    "herpiko": "I've tried to run make ci on master and all are passed. It must be my code that is wrongly written. Adding this ( https://github.com/herpiko/machinery/commit/20b2e53986d083d747f2629799b64c7625c5e8fa ) solves my chain issue but it breaks the unit testing. Is there any clue what is wrong with the code above?. Solved.\n```\n    chain, _ := tasks.NewChain(&buildSignature, &repoSignature)\n    _, err = server.SendChain(chain) // the ChainAsyncResult are not used\n    if err != nil {\n        fmt.Println(\"Could not create server : \" + err.Error())\n    }\n// Recreate the AsyncResult instance using the signature and server.backend\ncar := result.NewAsyncResult(&buildSignature, server.GetBackend())\ntaskState := car.GetState()\nfmt.Printf(\"Current state of %v task is:\\n\", taskState.TaskUUID)\nfmt.Println(taskState.State)\n\n```. @RichardKnop Can I pick this task? I'll send PR ASAP.. ",
    "feelinc": "I have check the v1/brokers/redis/redis.go source code. The tasks stored in \"delayed_tasks\".\nThe tasks added by \"ZADD\" command, but the GetPendingTasks is using \"LRANGE\" command, so redis will return error \"WRONGTYPE Operation against a key holding the wrong kind of value\", which result empty.. I have similar issue worker.go:69 Broker failed with error: Group completed error: redigo: nil returned\nBroker : SQS\nBackend : Redis\nQueue : FIFO\nThis happen if I set GroupUUID in task.. ",
    "theobouwman": "Okay, tnx. Where can I find an example of this?. ",
    "gow": "There was another event that matches the timeline. Few of the tasks in a group resulted in error. These tasks have a retry count (10) and backoff time defined. So they went into delayed_tasks queue. I'm not sure if it's correlated. Just adding more information here.. Ok. After debugging further, I discovered a couple of issues that might be related to this.\n1. The group result TTL is not set correctly.\nIssue: I had the default result expiry time (1 hour) and had tasks execute in a chord. This set the group's result to expire in 1hr when it was created. But some tasks in the group took longer than 1 hour to complete because of retries and exponential backoff. So, by the time all the individual tasks finished, the group's result had expired and the framework didn't know how to proceed and the chord was not executed completely.\nFix: Set the group's expiration time to Max(tasks.ETA) + ResultsExpiryTime\n2. The above issue somehow causes the workers to drop other tasks from the queue.\nIssue: While testing locally, I noticed that when the worker encountered the Broker failed with error: Group completed error: redigo: nil returned error, it dropped all other tasks currently existing in the queue. It did process newly enqueued jobs though. I think this is a bigger issue.\nFix: Don't know\n3. Wrap the errors with useful information.\nIssue: The errors found in the logs were not descriptive. \nFix: Add useful information like the Group ID that was not found etc.\n. @RichardKnop yes. Setting the default timeout to 24hr could provide some relief. I changed our config to 24hrs as well. But someone will definitely run into this problem again at some point. All they have to do is to have retries + backoff configured such that it the task/group exists longer than the configured ResultsExpiryTime. I think the right fix is to extend the TTL of the result object whenever a task is put back into the queue due to a retry.\nRegarding issue 2): I too found it strange. But something to that effect did happen. I'll investigate it further when I get some time.\nWhile you are at it, can you also address issue-3?\nA more descriptive error message would have saved a bit of time and also pointed me towards the right issue sooner. Something like Broker failed with error: Group [Group-ID] completed error: redigo: nil returned would be more helpful. Take a look at https://github.com/pkg/errors. It makes it easier to wrap errors. I'll be happy to help and send a PR as well.. ",
    "risav1994": "It can also happen if you have set two different db number for two different tasks in a group.. Its not abt ETA. It will just the delay the task to be published.\n I dont have how many tasks will arrive before the task that is to be done after the success of all tasks. So I dont know that time before hand. So thats what I am trying to solve.. And in this \nfunc Add(args ...int64) (int64, error) {\n  // Implement timeout\n  return tasks. NewErrRetryTaskLater(\"some error\", 4 * time.Hour)\n}\nwhat should be there in place of (int64, error)?. Thank you very much.. ",
    "toutouastro": "Following this article: http://masnun.rocks/2016/11/01/distributed-task-processing-in-golang/\nThe code does not run showing the above error!\nI think the article should be updated (because the folder errors is already removed maybe it existed in a past version)!. ",
    "fgimenez": "Thanks @RichardKnop! You would probably have garyburd/redigo v2.0.0 already in the cached modules from a build when it was available, try again after deleting (or renaming) ${GOPATH}/pkg/mod and you should get the error.\nCheers!. @jwilner if you are using modules this replace can fix things on your end:\n$ go mod edit -replace=github.com/garyburd/redigo=github.com/garyburd/redigo@v1.6.0+incompatible\nHope this helps.. There might be a potential problem with this approach, if a task not registered by any of the workers is received, the message would be requeued forever. Some possible solutions:\n\nDefine a MaximumRequeues setting for AMQP and keep track of requeues for each UUID, once the maximum is reached drop the message.\nAdd a random sleep timeout before requeuing, the message would be kept forever but the workers won't spend too much time with it.\nKeep the code as it is and use multiple servers, each with a different DefaultQueue and RoutingKey for each set of registered tasks. This would require updating the documentation to describe this situation. IMO this solution is not optimal, the resulting system would be much more complex.\n\nAm i missing something? Any thoughts more than welcome :). ",
    "jwilner": "This is breaking builds for us; we would very much appreciate having the fix merged in ASAP.. Thanks @fgimenez! We got around this by downgrading machinery, but that command is helpful -- still a lot to learn about go mod!. For posterity, I'll also note that because of https://github.com/golang/go/issues/28680, it seems that go clean -modcache will also fail in go 1.11 if you've got the problematic dep; GO111MODULE=off go clean -modcache (as mentioned in the link) seemed to get the cache back to a valid state though.. ",
    "knrt10": "Thanks @seonixx I solved it using #293. Actually, I did not want to pass a context. I had a model for a struct, I needed to pass that. Context was just an example of what I wanted.. ",
    "Zambiorix": "go get -u github.com/RichardKnop/machinery/v1\nbanging head against wall\n. ",
    "Huangsir": "same problem. ",
    "jtherin": "Many of us use opentracing, I don't agree with choice 1.. ",
    "Azuka": "Agree with @jtherin. It would probably be easier to provide pre and post hooks with the context both for pushing and receiving tasks. \nThat way, it'd be possible to swap/implement chainable adapters for OpenCensus, Opentracing, or some other trace standard (e.g. I use both opentracing and elastic-apm) that lets the user set custom headers on outgoing tasks and retrieve them from incoming ones as well as mutate the context for downstream usage.\nIt'd probably be achievable if the signatures for #351 and #343 had a context as the first argument.. ",
    "dhui": "Oh, it does appear that way! I believe this should be if err == nil!. Not sure why the diff is misleading.\nCode was changed here: https://github.com/RichardKnop/machinery/commit/74f2b63fd3304bfaa55e39a3b285b5b839394c57\nAnd code in the master branch looks fine: https://github.com/RichardKnop/machinery/blob/master/v1/brokers/sqs/sqs.go#L197\n. "
}
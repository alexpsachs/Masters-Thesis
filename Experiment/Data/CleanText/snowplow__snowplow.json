{
    "alexanderdean": "Done in v0.3\n. Done in v0.3\n. Done in v0.3\n. Done in v0.3\n. Done\n. Done\n. Closing as we are unlikely to ever do this now (and I've never seen another analytics company do this).\n. Closing this as unlikely to get round to this - niche demand\n. Done in the next version of the serde (will be 0.4.7)\n. Try the isolated operation modifier to see if that helps...\n. There are about 6 tests being skipped currently.\n. Fixed in 0.7.6 (serde versioning unchanged)\n. Closing as will support this when Elasticity supports this - https://github.com/rslifka/elasticity/issues/34\n. This is roadmap not an issue per se. Closing\n. Field will be called tstamp\n. Assigning to Yali to get his thoughts...\n. Closing this as we have a much cooler and more general solution in mind...\n. Thanks - fixed in my comment! I like your logic Simon for figuring out when to chop up the event into multiple GETs\n. This ticket is very difficult to implement:\n1. We need to come up with a new payload format to support multi-GET sends\n2. We need to add an additional (expensive aggregation) phase to the start of the Enrichment to reconstitute multi-GET sends\n3. For Hadoop, we need to add a cache so we can reconstitute multi-GET sends which end up spit across two batch runs\nThe implementations will be totally different for the Hadoop flow and for the Kinesis flow. Closing given we will have POST support soon:\n- https://github.com/snowplow/snowplow/issues/187\n- https://github.com/snowplow/snowplow/issues/543\n- https://github.com/snowplow/snowplow-javascript-tracker/issues/168\n- https://github.com/snowplow/snowplow-python-tracker/issues/70\n. Thanks Richo!\n. Thanks @richo, much appreciated. Now merged. For my reference, the merge commands:\ngit fetch git://github.com/richo/snowplow-wiki.git doc_fixes:doc/richo-fixes-x\ngit merge doc/richo-fixes-x\ngit push\nI'll leave this ticket open - if/when you have more updates for me to make, just leave me a new comment like \"pull request\" and I can take it from there.\nThanks again @richo - you're added to the Contributors page!\n. Done in the next version of the serde (will be 0.4.7)\n. Note that achieving this isn't currently possible with our Hive serde-based row-level deserialisation. We would need either:\n1. A stream-based ETL process which cached querystring hashes for X minutes to check for uniqueness, or:\n2. An aggregation step following our current ETL process which dropped the duplicates\n/cc @yalisassoon \n. Yep that could well work... Hopefully once we have built the caches required by the OLAP cube, the approach to deduplication will become clearer.\nOne thing we should probably do at some stage is move from storing the txn_id for a row to storing the generated event_hash (which will include the txn_id as well as everything else on the querystring).\n. To handle dupes cleanly, we need to store the hashes for all messages from the last batch, which could be duplicated in our time period. We can store these in a table in HBase:\nhttp://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hbase.html\n. To handle dupes cleanly, we need to store the hashes for all messages from the last batch, which could be duplicated in our time period. We can store these in a table in HBase:\nhttp://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hbase.html\n. One of our oldest open tickets now.\nIn a stream processing world, you have to make an assumption that all dupes will come from the same shard key (IP address?), and then keep all event_ids in your shard's local storage for N minutes. (Otherwise you would have to re-partition based on incoming event_id, pretty painful.)\nThere could be an interesting abstraction in Scala Common Enrich where as well as an EnrichmentRegistry, it is supplied with read/write access to a KV store - the specifics of the KV store are abstracted (Samza's KV store impl? Storehaus?) but Common Enrich can use it to manage dupes, sessionization etc.\n/cc @yalisassoon @fblundun \n. Renaming from Scala Common Enrich: to Unnamed new module:, because in a stream processing world, any stage can introduce natural copies. So we have to embed this module in each storage sink that cares about duplicate event_ids.\n. We have done some further brainstorming on this today. First we drew out some different event scenarios:\n\n. Then we came up with this taxonomy:\n| event id | ur event | possible explanations | strategy |\n| --- | --- | --- | --- |\n| same | same | * At least once processing * 2 trackers, 1 event Robots | Delete all but first |\n| same | different | * User reusing event ID * Event ID algo collision * Robot | Assign new event ID to all but first, preserving original event ID as 'parent event ID' |\n| different | same | * 2 trackers, 1 event * Behavioral collision | Do nothing |\n. We quickly realized that it is essential to define the ur event (ur meaning most primitive) correctly. @yalisassoon to add his whiteboard photo...\n. Further reading:\n- Dealing with duplicate event IDs \n- New data model: deduplicate in R69\n- New deduplication queries in R72\n- De-deduplicating events in Hadoop and Redshift [tutorial]\n- Event de-duplication in Hadoop Shred in R76\n. For existing code: https://github.com/snowplow/snowplow/commit/8c09aded04d78eae51827b8baee8771b27b74da5\n. Thanks, good spot. I noticed that AWS had updated their workflow when I refreshed the snowplow.js hosting documentation recently.\n. Assigned to Yali. Need to synchronize rolling out this update with changing the JavaScript tracker to call /i not /ice.png\n. Can we close this one now @yalisassoon ?\n. Thanks! Fixed. It's trackEvent BTW.\n. Many thanks! Fixed.\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/9\n. See also #35\n. Update: have added in i to the 2-collectors/cloudfront-collector/static\n. Will be in next release (0.6.1), leaving open till documentation is updated.\n. This is all complete now, just need documentation around i not ice.png to be updated. Assigning to Yali - please close this once documentation updated.\n. This has now been done in v0.4.3\n. CC'ing @yalisassoon again\n. Will be in next release (0.6.1), closing.\n. We're going to call this applicationid in the output Hive table to avoid confusion\n. Implemented in upcoming 0.4.9\n. Cool!\n. Looks good! Holler if you need any help with the test cases\n. Thanks @larsyencken - your help is much appreciated!\n. Closing this as this will be released in the imminent 0.4.9 (currently being tested).\nAdded some additional tests (TransactionTest and TransactionItemTest) too...\n. Fixed in version 0.4.8\n. Yali will tackle this as part of his wiki clean-up this afternoon (a lot of the current content has moved to the website - the wiki is going to be more for developers and community contributors...)\n. This is roadmap not an issue per se.\n. Hi @svanzoest - many thanks for raising this question.\nOur launch version of SnowPlow uses the AWS stack (CloudFront -> S3 -> Elastic MapReduce), but SnowPlow is designed as a loosely-coupled system, not tied to any single platform or vendor stack.\nFor example, instead of the CloudFront/S3-based event collector, you can use a node.js-based collector which you can run on your own servers, called SnowCannon. Both collectors are listed here:\nhttps://github.com/snowplow/snowplow/tree/master/2-collectors\nSnowCannon also currently logs to Amazon S3, but you could certainly change it to log to Walrus instead.\nOnce you have the data in Walrus, you would need to perform the SnowPlow ETL step on it. We currently have an Apache Hive-based ETL process, and are developing an alternative Hadoop-based ETL process too. Neither of these processes are specifically tied to AWS - you could run the ETL process on your own Eucalyptus private cloud instead.\nWe'd love to get help from people in the community to run SnowPlow on private cloud technology like Eucalyptus. If you're keen to get involved, ping an email to our user group and we can discuss the specific steps involved in some more detail:\nhttps://groups.google.com/d/forum/snowplow-user\nThanks again for your question,\nAlex\n. Hi @svanzoest - just a quick update on this: @shermozle's SnowCannon collector has now been updated to support outputting SnowPlow events to Fluentd. Fluentd is a lightweight log collector which supports multiple different output plugins.\nIn order to log SnowPlow events to Walrus, you need the following setup:\n- Deploy SnowCannon\n- Configure SnowCannon to output events to Fluentd\n- Configure Fluentd to output events to Walrus\nI have had a quick search, and couldn't find a fluent-plugin-walrus, to output data from Fluentd to Walrus. But, if Walrus is API-compatible with S3, it should be easy for you to:\n1. Fork https://github.com/fluent/fluent-plugin-s3 as fluent-plugin-walrus\n2. Update the code (maybe just the endpoint?) to support Walrus as necessary\n3. Use fluent-plugin-walrus with SnowCannon and Fluentd to collect SnowPlow events in Walrus\nI'm sure the Fluentd team would love to have a Walrus plugin contributed!\nI hope the above makes sense - feel free to ping an email to the SnowPlow Google Group if you would like to brainstorm further.\nClosing this issue as SnowPlow now officially supports non-S3 storage for SnowPlow events (via SnowCannon + Fluentd).\n/cc @shermozle @yalisassoon \n. Thanks for clarifying @ramn! @mtibben, apologies, this is our bad - the ice.png self-hosting guide is a bit out-of-date - there's a ticket to update this:\nhttps://github.com/snowplow/snowplow/pull/39\nI'm doing some updates to the Hive serde now anyway (to deal with the new CloudFront format*), so I'll add in this pull request - it will be helpful for anyone else who has Forward Query String = yes.\n* Deployed by Amazon on 12 Sept 2012. Blog post coming soon...\n. Duplicate of #549\n. Will be in next release (0.6.1), closing.\n. Fixed in 0.5.1\n. We're going to leave this one for now. Should be able to achieve something similar by setting different application IDs on a per-tracker basis\n. Will be in next release (0.6.1), closing.\n. See also this article: http://blog.protonet.info/post/9620971736/exception-notifier-javascript\n. Make the fields first-class citizens in the output structure\n. See http://help.airbrake.io/kb/api-2/notifier-api-version-22 for a more maximalist definition of an error event\n. Also add isFatal flag...\nRename message to description, URL to file.\nAlso: consider adding custom variable and custom JSON support to errors, so more data can be sent...\n. Adapting from Airbrake, also add:\nclass - the class name or type of error that occurred.\ncomponent - the framework, sub-system, or MVC controller in which the error occurred\naction - class method, function, or 'controller action' in MVC parlance\nAll optional.\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/16\n. Whoah - @mtibben this is awesome! Huge thanks for putting this together. The idea of a processing bucket makes a whole lot of sense, and adding in the Elasticity jobflow config stuff (instances etc) is really neat.\nI'm going to merge this now; in parallel, there's a few misc things that would be good to chat to you about - I think point 2 below is particularly important:\n/1. S3 libraries - at the moment we have:\ngem.add_dependency 'fog', '~> 1.6.0'\ngem.add_dependency 's3', '0.3.11'\ngem.add_dependency 'aws-s3', '0.6.3'\nOnce @rslifka updates Elasticity, we should be able to use his sync() and drop the 's3' (qoobaa/s3) dependency. What is the new 'aws-s3' dependency for - is it for something not easily done in Fog?\n/2. Boundary errors - I believe the new flow in your pull request is:\n1. For the date range, find files in CloudFront with those dates in their filenames\n2. Move them into processing bucket\n3. Run Hive on them, with the ETL'ed rows going into date partitioned files\n4. Move the processed files into archive bucket\nThis flow makes sense to me - but I believe there's a boundary issue with it (CC'ing @yalisassoon here as he's being doing a lot of the work with Hive):\n- We do a processing run for 26/09/2012\n- A CloudFront log timestamped 26/09/2012 00:01am may contain events from the 25th\n- Those events from the 25th will not be loaded into Hive, because of the HiveQL filter: WHERE dt='${DATA_DATE}';\n- That file will be moved from processing to archived with those boundary events unprocessed\nI'm wondering if we could just take out the WHERE dt='${DATA_DATE}'; - it's not clear to me what value that is adding, @yalisassoon @mtibben any thoughts on this?\n/3. Naming - I'm a big fan of descriptive names, but EMR ETL Runner and particularly snowplow-emr-etl-runner.gemspec seem a bit wordy. If you had any ideas on a catchier name, that would be great!\n\nGreat, that's everything, merging this pull request now but keen to brainstorm the above - either in this thread, in the Google Group or by email (alex@, yali@ snowplowanalytics.com) as you prefer!\n. Cheers @rslifka - have fun in Europe and look forward to catching up in due course. This ETL runner wouldn't be possible (well, it would be a massive ballache ;) without Elasticity!\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/12\n. Thanks @mtibben, merged!\n. Cool! Merging... We can git rm daily-etl.q now right?\n. To me, yes, many thanks @mtibben. I've almost finished working on improved test coverage, cleaner test syntax and a bug fix in the Serde (https://github.com/snowplow/snowplow/tree/feature/serde-test-upgrade) so will roll this out at the same time, and also add in #33.\n@ramn any thoughts from you or good to go?\n. This pull request has been merged into https://github.com/snowplow/snowplow/tree/feature/serde-test-upgrade\nI have added a couple more tests too (TransactionTest and TransactionItemTest).\nWe'll test the updates on some live data and then push to master & version bump...\nThanks for all your help guys\n. Fixed in 0.5.1, thanks @mtibben\n. Cool thanks @mtibben, merging...\n. Thanks Simon! It was an upload glitch, now fixed. Thanks again.\n. Okay cool - thanks Simon! I will merge this in and cut a release with versioning next week...\n. This has been manually merged into feature/js-upgrade, closing...\n. Thanks @rgabo - confirmed, this was done in 0.8.6*, not sure how it slipped through.\n* https://github.com/snowplow/snowplow/blob/master/CHANGELOG#L51\n. Fixed in 0.5.1\n. I think that would achieve something a little different (not including a step at all vs assuming it's already been done and starting from a later step) - but an interesting idea too...\n. Added --skip staging and --skip emr to version 0.5.2. --skip skips up to and including the specified step. So --skip emr only does the archiving step.\n. Won't fix - instead we're moving to the rolling approach...\n. Hey @mtibben - many thanks for putting this together! It looks good. The whole ETL process not dying in the case of a single bad row of data is definitely the direction of travel we want to be moving SnowPlow ETL in.\nTwo quick things:\n1. I had a question above about a missing break;\n2. Where does the Commons Logging actually log to in a Hive/EMR world?\nIf you could fix up the rest of the tests that would be awesome, many thanks Michael.\nI'm going to merge EmrEtlRunner into master very soon (cutting a new release), so will merge this pull request into a new feature branch once I've done this... Leaving this open in the meantime.\n. Many thanks for the test cases, and thanks for raising useragent issue Mike! On the useragent parsing - we probably need to move from the current library to one which uses a regularly-updated 'database' (YAML file, basically) of useragent strings. I've raised a ticket for this: https://github.com/snowplow/snowplow/issues/62\n. Many thanks @mtibben - this is now merged into feature/ice-compat, which will be the next release...\n. Fixed in 0.5.1\n. To move this forward, someone needs to:\n1. Have a play with ua-parser\n2. Grok which fields ua-parser will generate\n3. Figure out how they map onto our existing fields - any renames, deletes, additions\n4. Add ua-parser as a dependency\n5. Remove the old library in hadoop-etl/lib\n6. Replace the old library code with ua-parser code\n7. Update the Redshift table definition\n8. Provide a migration script from old Redshift table definition to new one\n9. Update the Postgres table definition\n10. Provide a migration script from old Postgres table definition to new one\n11. Update the Analytics Cookbook recipes as appropriate\n. Hey @kingo55 - agreed, we'll do that in the next release: https://github.com/snowplow/snowplow/issues/416\n. @pkallos had a play around with this - see #576 for details. Unfortunately ua-parser takes away things (e.g. browserType), even as it gives us enhanced accuracy.\nSo rather than a direct replacement, maybe we leave till we have pluggable enrichments, and then we offer both different UA parsing options\n. Ticket depends on: https://github.com/snowplow/iglu-central/issues/96, https://github.com/snowplow/iglu-central/issues/94\n. Will be in next release (0.6.1), closing.\n. Thanks Yali! 'ccing @mtibben as I know he'll be interested...\n. All logging in the new Hadoop ETL is now first-party (i.e. we write out errors, validation errors etc to buckets), so we can close this requirement.\n. Looks good! Thanks @mtibben. Merging... Final tests on 0.5.1 tomorrow.\n. Thanks @mtibben! Now in feature/storage-loader, which will  be the next release.\nBTW - 99designs:master is out of date now. I manually transferred your commit into the current s3_tasks.rb.\n. Will be in next release (0.6.1), closing.\n. Thanks! Looks very useful.\n. @yalisassoon - the timezone library suggested by @mtibben can pull back a IsDaylightSavingsTime flag as well as the TZ. What do you think - shall we add that in as a field to store as well?\n. Yep it looks really good!\nOn 14 Nov 2012 00:14, \"Michael Tibben\" notifications@github.com wrote:\n\nThe big advantage of this lib is that it gives you back a Olson tz key\n(e.g. Europe/Berlin) rather than a UTC time offset, which can be skewed\ndue to daylight savings etc\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/69#issuecomment-10349957.\n. Thanks for the library suggestion @mtibben - jstimezonedetect is now integrated into feature/js-upgrade and working a treat... IsDaylightSavingsTime function didn't work as I thought it did, so not including that.\n. Will be in next release (0.6.1), closing.\n. See http://programmers.stackexchange.com/questions/122372/is-browser-fingerprinting-a-viable-technique-for-identifying-anonymous-users\n. This is now added into the branch feature/js-upgrade - thanks @moncaubeig ! Added to contributors list, and also raised #85\n. Will be in next release (0.6.1), closing.\n. Hi @testower ! Apologies - the template config.yml (in the GitHub repo and in the wiki) had an out-of-date version for the Hive deserializer. Can you edit your config.yml and change:\n\nyaml\n...\n:snowplow:\n  :serde_version: 0.4.9\n...\nto:\nyaml\n...\n:snowplow:\n  :serde_version: 0.5.0\n...\nThat's been fixed in the repo and wiki now too, many thanks for raising. Will leave this issue open for now.\n. Excellent! Closing now...\n. Confirmed fixed in 0.5.2 (new release).\n. Hmm, very odd. It sounds like the line:\nfile_match = file.key.match('([^/]+)$')\nis not returning a match for you (file_match[1] is nil) - which should never happen. I'll ask @mtibben if he can think what might be happening, as he wrote the S3 download code.\nHave you definitely set all five bucket names correctly in the config file? What version of Ruby are you running?\n. Ah - many thanks - I think I can guess what it is. The file loop is including your sub-directory /processing as an entry in its own right, and then its regexp match is returning nil (because the /processing folder key doesn't include a filename). So your fix is the right one to skip sub-folder entries. My bad - we didn't test using subfolders on the Processing Bucket.\nMany thanks for flagging - will fix and cut a release on Monday.\n. It's an odd one. I reproduced the problem locally. The problem doesn't happen with the move to processing, just with the archive.\nAnyway, confirmed this is fixed in 0.5.2 (the new release).\n. @yalisassoon can you take a look at this one on Monday?\n. No worries, cheers Tom!\n. We didn't go for custom variables in the end - we built custom contexts, now implemented.\n. Merry Xmas!\nYes agreed, we want to make our link tracking as good as we can.\nThanks for the link, that's very helpful. We already had element ID\ntracking planned, so that's a definite. On GA's ability to follow the\nredirects behind links - this is something we need to look into further!\nDefinitely cool though.\nNo rush on this feature - let's keep discussing it in this thread, and\npotentially do a spec in a Gist when the time comes!\nOn Dec 25, 2012 3:40 AM, \"Robert Kingston\" notifications@github.com wrote:\n\nThe fact that it's broken could be quite the opportunity though. We could\nmake clicktracking a bit more useful (similar to how GA uses its new link\nattribution). What do you think about this, @alexanderdean :\nhttp://analytics.blogspot.com.au/2012/11/announcing-enhanced-link-attribution.html\n\u2014\nReply to this email directly or view it on GitHub.\n. +1 for this feature on Sunny's behalf.\n. Won't fix in Snowplow core - link tracking will be handled via unstructured events.\n. Not a priority, closing\n. Ah, we'll do it, why not.\n. Closing this as we are moving StorageLoader to the transient EMR cluster anyway. Fixed in 0.5.3 (coming soon)\n. Problem seems to be in JavaScript feature detection, for all Internet Explorer user agents.\n. Not a bug: https://github.com/snowplow/snowplow/wiki/Troubleshooting#wiki-ie-features\n. Done in 0.8.0.\n. Duplicate of #76\n. Fixed in SnowPlow 0.7.0\n. Hey @mtibben - thanks, that's good to know. I'm going to leave this ticket open for a while to monitor what's going on.\n\nI've been doing a historic run of 3 months' CloudFront logs for a pretty small site - it's been quite disappointing how much time the S3 file moves to Processing and Archiving have gone. Over 12 hours each (vs 2 hours for the EMR job with 5 m1.smalls)...\n. That's definitely interesting! Is the Python script pulling the files down from S3, aggregating and re-uploading?\n. Thanks guys. This definitely sounds interesting - could be a good approach for big sites using the CloudFront collector. Would you be able to share a suitably anonymised version of the Python script e.g. in a Gist?\n. Many thanks for sharing Lars! The script looks good. So the script is building 2012-11-03/01.gz etc in your EBS volume attached to EC2, with the datestamps taken from the CloudFront filenames.\nA couple of questions:\n1. What's your process for then feeding those aggregates into EmrEtlRunner? Presumably you move the files up into an S3 bucket for EmrEtlRunner to operate on - but how do you know when a file like 2012-11-03/01.gz is 'finalised' - given that CloudFront access logs can take some time to arrive?\n2. It looks like you are leaving the ingested files in the CloudFront log bucket. Is that going to scale okay? In a couple of months, aren't you going to have to parse a million filenames in the bucket to find the new filenames? Or am I missing something?\n/cc @yalisassoon \n. Thanks for clarifying @larsyencken ! Keep us posted on what you find out - we're going to explore a few different options ourselves over the next month or so...\n. Closing this as it hasn't recurred since CloudFront logging was refactored by AWS a few years ago. Looks great @mtibben! --skip staging,emr,archive syntax is where I wanted to go next after #81\nI will add this into the next release...\n. This has now been pulled into feature/js-upgrade, closing. Many thanks @mtibben, much appreciated!\n(Preview of what is coming in the next release:\nhttps://github.com/snowplow/snowplow/blob/feature/js-upgrade/CHANGELOG )\n. Big thanks @richo! Now added (and Troubleshooting page moved into Setup guide)...\n. Assigning to @yalisassoon as he will be tidying up the Ruby installation configuration/instructions.\nThis will be the Version 0.6.7 release.\n. Fixed in 0.7.1\n. Also make it possible to disable user fingerprinting.\n. Note some duplication between user fingerprinting and the old Piwik code that generates the user ID. Can we simplify it so there's only one piece of fingerprinting code?\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/7\n. Removed in 0.7.3\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/11\n. Closing this now as added into next release (0.6.1).\n. Because you don't always want to rely on the IP address in the collector's access logs.\nEssentially, this ticket means adding ETL support for &ip= in querystring\n/cc @ramn\n. Because you don't always want to rely on the IP address in the collector's access logs.\nEssentially, this ticket means adding ETL support for &ip= in querystring\n/cc @ramn\n. Nobody's asked for this again... Closing given there's a ton of more important stuff to do.\n. Added in 0.7.3\n. Clarified! Also added a placeholder for social referrers into our attribution library:\nhttps://github.com/snowplow/attlib/issues/1\n. Done in 0.8.11\n. Fixed in SnowPlow 0.7.0\n. Hopefully not required (can make Tomcat conform to CloudFront format exactly), closing...\n. Let's do this in 0.8.1 (when we are arguably adding our first enrichment).\n. Done in 0.8.1\n. Done in 0.8.0. Will potentially come back to this and attach run IDs more widely (e.g. to processing bucket) later.\n. Many thanks @mtibben, good catch! Merged into feature/js-fixes along with #102. Will be deployed shortly, closing.\n. Great stuff - let us know how you go trying out the fingerprinting. It's very new!\n. Taking a look at this now...\n. Thanks @mtibben . I think I know what this is, give me a minute...\n. Basically - I think the bug is one I spotted when working on the feature/better-events branch - tracker.js has lots of references to addEventListener which are not properly prefixed - they should be SnowPlow.addEventListener. (/ccing @ramn as I believe this was introduced when snowplow.js was split into multiple files.)\nI have fixed this in feature/better-events so that page pinging (to track ongoing browsing activity within a page) works again. The puzzle is why you are getting WRONG_ARGUMENTS_ERR yet I'm not seeing that with the version on ://d1fc8wv8zag5ca.cloudfront.net/0.8.1/sp.js. I'll install YUIC 2.4.7 on Monday (currently using 2.4.2) and see if that's the reason for the divergence.\nAnyway I'll take a look on Monday and confirm if that's the problem - if it is, I will backport the fix from feature/better-events into a 0.6.2 release with a js-0.8.2, because better-events won't be ready for a week or so...\nIf in the meantime you want to try prepending SnowPlow. onto the addEventListener calls and see if that solves it for you, please go for it!\n. Hi Michael, I've tried YUIC 2.4.7 (same as your version), and I've tried making the change to prepend SnowPlow. and I cannot reproduce your errors. Can you share your minification process - does it diverge in any way from the steps laid out in:\nhttps://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/snowpak.sh\nEdit: could it be that one of the files in dependencies.txt is not getting included prior to your minification? Make sure to keep that trailing line at the end of dependencies.txt...\n. Hey Michael,\nSure thing - whenever we release a new version of the tracker, we upload the newly minified version to CloudFront. You can find details on this page:\nhttps://github.com/snowplow/snowplow/wiki/Hosted-assets\nI think my earlier comment about SnowPlow. (capital P) is a red herring... I think something must be going wrong with your minification. Here's my recommendation:\nStep 1\n$ git clone git@github.com:snowplow/snowplow.git\n$ git checkout 0.6.2\n$ cd snowplow/1-trackers/javascript-tracker\n$ chromium-browser examples/web/async.html\nAnd check that you don't get any browser issues with the hosted JS version.\nStep 2\nDownload and install YUIC 2.4.2, then:\n$ cd js\n$ ./snowpak.sh /path/to/yui/yuicompressor-2.4.2\n$ wget http://d1fc8wv8zag5ca.cloudfront.net/0.8.1/sp.js\n$ diff -s sp.js sp.js.1 \nFiles sp.js and sp.js.1 are identical\nStep 3\nEdit examples/web/async.html and repoint it to the sp.js you just created.\nCheck it still works fine in Chrome\nStep 4\nRepeat steps 2 & 3 but with YUIC 2.4.7...\nDoes that help?\n. A test suite would be great - but as a first step I'd like to be able to manually reproduce the errors you are seeing.\nI can see the regressions in the code but I literally can't get a browser to complain about js-0.8.1 - and I'm guessing @ramn didn't either when he was writing the code. There has to be something different about your setup that is making these bugs error in the browser... Testing of js-0.8.1 was done on recent versions of Chrome, Firefox and IE. Chrome and Firefox on both Windows and Ubuntu.\nCan you share any command-line instructions to reproduce the browser errors you're seeing?\nEdit: I think I've squashed all the regressions (although can't test as cannot reproduce #103). The minified 0.8.2 is here:\nhttp://d1fc8wv8zag5ca.cloudfront.net/0.8.2/sp.js\nLet me know if that version works for you @mtibben...\n. Thanks @mtibben, let me know how it goes. I'm going to have a think about \"static\" analysis options as well as unit tests, and also about implementing #45 for some Muscula-style error logging...\n. Thanks Anthon - that's a great idea! Creating a separate ticket to track that:\nhttps://github.com/snowplow/snowplow/issues/106\n. Fixed in version 0.6.3 (JavaScript tracker version 0.8.2), which was just released. Closing...\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/6\n. Take a look at: https://github.com/backchatio/rl\nIt might let us clean up aka normalize our URLs really nicely...\n. Assigning to 0.7.2 because I think this is important sooner rather than later...\n. Added in 0.7.3\n. Thanks Simon - there's a ton of interesting stuff there. I've left a comment/question on @stoja's blog too and I found his project here:\nhttps://github.com/stoja/WebAnalyticsBeacon\nI wonder if we could we wire a basic custom collector into Selenium, so that we can use Selenium's asserts directly to check that the beacon is getting the right stuff.\nEdit: I think a way to do this would be:\n1. Write a basic collector that runs locally and stores pinged querystrings to SQLite\n2. In your tracker code, you setCollectorUrl to localhost\n3. In Selenium, pull the querystring out of SQLite and check it matches what's expected\nThis is I believe pretty similar to how @robocoder does it for integration testing piwik.js (minus the Selenium integration).\n. Thanks @stoja - I hadn't heard of Selenium's proxy injection mode before:\nhttp://blog.qaevangelist.com/?p=317\nhttp://blog.eviltester.com/2010/05/a-selenium-capturenetworktraffic-example-in-java.html\nThis seems like it could be great for our purposes:\n- Put Selenium into proxy mode\n- Run a test\n- Analyse the collected traffic to check that the correct data was sent to the beacon\n. Look at http://theintern.io/ as an alternative testing stack, and read the chapter in Third-Party JavaScript (Manning) on testing.\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/5\n. Fixed in 0.6.4\n. Kicking over the fence to @yalisassoon...\n. Or consider using: http://www.maxmind.com/en/organization\n. We should use MaxMind organization, it's super simple:\n```\n/ OrgLookupTest.java /\nimport com.maxmind.geoip.*;\nimport java.io.IOException;\n/ sample of how to use the GeoIP Java API with GeoIP Organization and ISP databases /\n/ This example can also be used with the GeoIP Domain and ASNum databases /\n/ Usage: java OrgLookupTest 64.4.4.4 /\nclass OrgLookupTest {\n    public static void main(String[] args) {\n    try {\n        LookupService orgl = new LookupService(\"/usr/local/share/GeoIP/GeoIPOrg.dat\");\n        LookupService ispl = new LookupService(\"/usr/local/share/GeoIP/GeoIPISP.dat\");\n        System.out.println(\"Organization: \" + orgl.getOrg(args[0]) +\n                   \"\\tISP: \" + ispl.getOrg(args[0]));\n        orgl.close();\n        ispl.close();\n    }\n    catch (IOException e) {\n        System.out.println(\"IO Exception\");\n    }\n    }\n}\n``\n. Depends on https://github.com/snowplow/scala-maxmind-geoip/issues/14\n. Fixed in 0.9.6, closing\n. As an alternative, consider: http://www.maxmind.com/en/netspeed\n. Let's do this via MaxMind rather than adding more code to the JavaScript Tracker.\n. @kingo55 what are your thoughts on #308?\n. @frutik - which idea are you referring to? igvita, MaxMind or episodes.js? :-)\n. @frutik - cool! I think it's something we're going to explore soon... /cc @yalisassoon \n. Thanks @ramn! Some of those we had missed infeature/fix-103` - I have added the ones we missed into that branch.\nThis commit is also quite relevant: https://github.com/snowplow/snowplow/commit/f54decb6b8b0e8830deedea5a46cd1b3fc120031\nClosing - I hope to release the fixed JS from feature/fix-103 later this week...\n. Fixed in 0.6.4\n. Many thanks Mike! Merged into feature/fix-103, which will be the next branch released. Closing\n. Hey Rob!\nThis is a really interesting idea. I agree - it could work as well for media sites too (recommended articles etc).\nI think the way it would work in the JavaScript tracker would be the same as with the ecommerce item tracking: each offer in the array would be expanded out into its own row sent to the collector. I imagine this would make querying easier too.\nI'm not sure about the word \"offer\" - it feels a bit too salesy?\nOn a related point, currently tracking a page view only supports an optional custom title for the page view. I wonder if it should also support an optional content/product ID. This would make it easier to join up those pageviews to the \"offers\".\nAs a final point, if one of the outputs here is calculating CTR, then we would need to make sure anything here tied up with our (roadmap) link tracking. This makes me wonder if e.g. it's better to track offer locations only positionally or also with e.g. div IDs.\nCCing @yalisassoon @shermozle @larsyencken @ramn for their thoughts...\n. Hey @kingo55 - glad you're excited about this feature! We are too.\nOn your specific questions:\n\nitem ranking - If this is missing, will this be inferred based on the order of items passed through the trackItemView() > and will the order of item locations also play a part in determining ranking?\n\nWe cannot guarantee any ordering once the item views get to the collector/ETL, so any ordering information needs to be set in the tracker. We might be able to come up with an implicit ordering based on the order in which the trackItemView() calls are added to the async tracking queue (snaq), but this feels a bit 'magical'/hairy to me. Probably best just to make this ordering explicit with the item ranking field.\n\nWill each product create a new trackItemView() and request for the pixel?\n\nYep.\n\nOne general comment I think is important is that I have a few ideas about things we can track through pageviews.\nShould we include some mechanism to switch certain functionality off and on? e.g. What if someone wants to use\nthe slots we're using for this feature for something else?\n\nI would say a couple of things here:\n1. Do please share your ideas of what we should be tracking as first party data through pageviews!\n2. We are working on our custom variable approach, and current thinking is that (among other types of custom variable), the user will get 10 \"slots\" for custom page-level variables. The idea is that you set these once per page and then they are transmitted with every event sent on that page. So our current thinking is that, if someone wants to track something else outside the \"first class\" pageview fields, then they can always use those page-level variables. But again, interested to hear your thoughts - this is all still up-in-the-air!\n. Hey @kingo55 - that's a good point re. 100+ products per page. It would nice to have some sort of bulk send with an array like you say.\nUnfortunately... a major limitation of the current row-level ETL process (which uses a Hive row deserializer) is that each input row can only generate 0 or 1 output rows, not N output rows. So that basically means that we cannot currently expand a querystring with an array into multiple events. However, this limitation should go away when we release the pure Hadoop (no Hive) version of the ETL in a couple of months.\nThe other thing to be careful of with a big payload of 100+ item views is the IE URL length limitation - see #20 for some workaround ideas here...\n. Yes - probably best to wait. In fact the Hadoop ETL is higher priority anyway (we will be starting on it as soon as the Clojure Collector is released).\nBy the way Rob, are you guys using SnowPlow actively yet (or still evaluating?). If so, it would be great if we could add you to https://github.com/snowplow/snowplow/wiki/Our-users - let us know...\n. Makes sense @kingo55 - if we can do anything to help you get more comfortable with SnowPlow, just let us know!\n. Hi @sdepablos - thanks for raising that! I have updated that page, and also updated the roadmap with our current status:\n- https://github.com/snowplow/snowplow/wiki/Hadoop-ETL\n- https://github.com/snowplow/snowplow/wiki/Product-roadmap\nIn short: the Hadoop ETL is quite far progressed; we are just waiting for the Redshift release to finish work on it. It's a high priority.\nAny other questions, just let us know... /cc @yalisassoon \n. Hey Sergi,\nGood question - actually we've discovered that Redshift, Infobright and\nMySQL all share the same technical limitation: namely that rows have to fit\nwithin 65k bytes. By this measure, our events table is almost full up, so\nwe will need to look at another approach (i.e. more than just one 'fat\ntable') to store new event types such as item views, add to baskets etc.\nBut the good news is - yes, whatever approach we take will work equally\nwell for Infobright or Postgres or MySQL.\nOn Feb 23, 2013 12:36 AM, \"Sergi de Pablos\" notifications@github.com\nwrote:\n\nThanks for your answer @alexanderdean https://github.com/alexanderdeanAny technical limitation to make it work with Infobright instead of\nRedshift? I was thinking that for most small sized ecommerces Redshift may\nbe overkill but at the same having information on how the product listings\nwork should be a must for all of them.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/113#issuecomment-13981475.\n. Yes, we were surprised to see it too... I think it's something to do with Redshift's querying technology, rather than the storage system itself. You can see details here:\n\nhttp://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_TABLE_usage.html\n. I found some code to handle element-on-page visibility:\nhttp://stackoverflow.com/questions/123999/how-to-tell-if-a-dom-element-is-visible-in-the-current-viewport\nOur current thinking is: we should attach some kind of viewabilityDetector to each item on the page - when the item realises that it's now in-view, then it should fire a trackItemView.\n. Closing as this will be implemented in the JavaScript Tracker (doesn't require any updates to core).\n. /cc @talkspoon\nThis bug had in fact already been fixed in the feature/better-events branch, which will be released in late Dec...\n. Building on the above - an event without an ev_action is essentially an 'untyped' event - you could supply a label,  value or property but you wouldn't fundamentally know what that label or value is describing. This would be fine as long as your app only generated one type of event, of course.\nMy gut feel is that making a field like ev_action compulsory for SnowPlow would be less about easy support for out-of-the-box event analytics (like Sky) or charts, and more about stopping people from shooting themselves in the foot in the tagging phase by creating lots of untyped events that they can't distinguish later...\nI guess with all of these things, we have to strike a balance between giving users lots of power versus making it easy for them to do something sensible that gives them good results...\n. We also need to think about the value field... I think currently we support string, int or float for this field. For typed storage targets, should we forking this value field into value_string, value_float and value_int based on its contents?\n. Closing - was decided that category and action must be provided, and value must be numeric. See #199 for details.\n. Fixed in SnowPlow 0.7.0\n. Fixed in SnowPlow 0.7.0\n. Fixed in SnowPlow 0.7.0\n. Closing this given we are moving away from this (per #3132). Have added a branch for this:\nfeature/ruby-install\nMake the documentation changes last (#122) to prevent out-of-sync conflict between code and documentation.\n. Fixed in 0.7.1\n. Great stuff @jasonlynes - we should be doing a small release soon which tidies all this Ruby installation stuff up once and for all...\n. @jasonlynes that's a super interesting idea Jason!\nAt the moment, the only entrypoint into EmrEtlRunner is the CLI interface. I think we would need to change it so that the core flow of EmrEtlRunner is also callable as a function, one which is supplied with a hash of all arguments (rather than needing to read a config file). I guess EmrEtlRunner could then be pushed onto Rubygems as a gem which is embeddable in other applications.\nDoes that make sense? Feel free to ping an email to snowplow-user@googlegroups.com if you want to open the discussion up a bit. I'm totally in favour of making this change BTW.\nEdit: ticket created to track/discuss this, #128\n. Fixed in 0.7.1\n. Done, closing...\n. Created branch for this:\nfeature/no-js-tracker\n. Added in 0.7.2\n. As per @kingo55's idea, we should integrate this with pagepings, so each pageping contains information on how far down the user has scrolled.\nNote: each user obviously has a different viewport and browser, so we will also need to track viewport (#94) and total page height (TBC how do we do this - @kingo55 any ideas?)\nScheduled for milestone 0.7.2.\n. Great thread! I think capturing the raw data points and then calculating scroll reach in the ETL is probably the most robust approach.\nWe're going to be capturing viewport height_width on all events logged to SnowPlow, so don't worry too much about that one. I think we should be capturing document height_width on all web events too - I've updated #94 accordingly.\nBased on this, am I right in thinking then that all we need to log with the page ping is:\n1. Max scroll left during last ping period\n2. Max scroll right during last ping period\n3. Max scroll up during last ping period\n4. Max scroll down during last ping period\n. Added in 0.7.3\n. Done in 0.9.3, closing\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/13\n. /cc @ppcanalytica\n. Current thinking: come up with a new name=value pair for the querystring, e.g. utm_share=copypaste and then extract this in our ETL into either a new column or an existing marketing column, TBC.\n. Let's kick this down the road. Here is the current implementation:\n```\n// How do we identify dark social copy-paste?\nconfigDarkSocialAppend = '?utm_share=copy-paste',\n/\n * Enables dark social tracking.\n * Only works on newer browsers\n * that support push state.\n \n * Based on:\n * - https://plus.google.com/u/0/109159729997909875172/posts/Jk8n5nhqC6c\n /\nenableDarkSocialTracking: function () {\n    if (SnowPlow.windowAlias.history.pushState && \n      !SnowPlow.endsWith(SnowPlow.windowAlias.location.href, configDarkSocialAppend)) {\n        var newUrl = SnowPlow.windowAlias.location.href + configDarkSocialAppend;\n        SnowPlow.windowAlias.history.pushState(null, SnowPlow.documentAlias.title, newUrl);\n    }\n},\n```\nThe main issues are:\n- Risk of adding 2+ querystrings and fragging the URL\n- After enableDarkSocialTracking is called, all of our events and page pings for that page going forwards have a URL which includes the ?utm_share=copy-paste cruft on the end.\nCurrent thinking is:\n- Switch to AddThis-style # based format\n- Check for an existing # and only add the copy paste # if not found\n- Use the same AddThis-style (#randnumber.cp) - but add a vendor prefix (#sp.randnumber.cp)\n- Ask AddThis to add vendor prefixing to theirs too\n- Update ETL to extract the # for us, but also for AddThis too\nAll in all, a fair bit of work for not much gain currently - pushing back in the roadmap...\n. Closing this (leaving the JS Tracker issue open). This is a massive chunk of work and at best improves the capture for only 1 type of SnowPlow event (page views), and at worst muddies the event stream, because the Apache logs could include lots of non-page views (e.g. form submits and redirects). Closing...\n. Fixed in 0.7.3\n. Note: this has been partially done in 0.7.3\n. This will be completed in Milestone 0.8.1\n. Implemented in 0.8.3\n. Thanks Simon! Fixed - I went with option :one:\n. Fixed in 0.7.1\n. Thanks, both of these fixed. Mysteriously the clojure-collector-0.1.0-standalone.war file has gone missing from that bucket - created a separate ticket for @yalisassoon to re-upload. #137\n. Re-uploaded. No idea what happened to the file before.\n. Fixed in 0.7.3\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/10\n. Cool! What was it?\nOn Jan 17, 2013 8:57 PM, \"Yali\" notifications@github.com wrote:\n\nFixed!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/140#issuecomment-12391235.\n. Dropping this feature for two reasons:\n1. Will potentially break the no-javascript tracker in some browsers - because there is no longer a transparent pixel to load\n2. Harder to debug because e.g. Chrome Developer Tools shows 204 responses in red with status \"cancelled\"\n\n\nWon't fix.\n. Scheduling for 0.8.13.\n. Hi @frutik ! At current development velocity, I expect mid-December for 0.8.13\n. Note that the code to add unstructured events support is currently in branches:\n- feature/unstruct-events-etl (which will need merging for this ticket)\n- feature/improve-etl (which should be merged into master as part of 0.8.11)\n. Done, closing.\n. Fixed in 0.7.3\n. Fixed in 0.7.3\n. Nice idea! To implement this we would introduce the idea of \"named collectors\". It would look something like this:\njavascript\n_snaq.push(['addCollectorCf', 'd3rkrsqld9gmqf', 'backup']);\n_snaq.push(['addCollectorUrl', 'sp.mydomain.com', 'main']);\n_snaq.push(['attachUserId', false, 'main']);\nStill TBC which other configuration needed to be per-collector versus global - e.g. potentially:\njavascript\n_snaq.push(['enableActivityTracking', 10, 10, 'main']);\nNote that this is a big restructuring of the tracker - everything about the current code assumes there is only one collector to configure and send to. Would need to wait on us having a JavaScript test suite. \nBecause this functionality does not exist, currently on snowplowanalytics.com we use a tag manager (GTM but Qubit works too) to send events to multiple SnowPlow collectors.\nBy way of comparison, here are the strengths and weaknesses of the tag manager approach:\n- -ive - have to duplicate all event firings\n- -ive - have to load sp.js twice\n- +ive - allows each collector to work with a different version of sp.js (which is great for testing new versions of sp.js without losing data if there are issues)\n- +ive - approach works today\n/cc @yalisassoon for any additional comments...\n. See this excellent blog article on Universal Analytics' new tracker namespacing capabilities:\nhttp://www.blastam.com/blog/index.php/2013/05/ga-universal-analytics-properly-namespaced/\nNote that the cookies are namespaceable too.\n. Migrated to https://github.com/snowplow/snowplow-javascript-tracker/issues/4\n. Cool, this becomes the new 0.7.2. (Fix in separate branch then ping this\nthread and I'll merge & release.)\nOn Jan 26, 2013 9:52 AM, \"Yali\" notifications@github.com wrote:\n\nI think this is a simple fix - will action ASAP:\nhttp://stackoverflow.com/questions/6842245/converting-date-time-to-24-hour-format\nOn Sat, Jan 26, 2013 at 9:26 AM, Alexander Dean notifications@github.comwrote:\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/146>.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/146#issuecomment-12733314.\n. Great stuff! Will release tomorrow with Simply Business's fix too...\nOn Jan 27, 2013 9:55 AM, \"Yali\" notifications@github.com wrote:\nThe fix is ready to be merged - the branch is\nclj-collector/fix-24-hour-clock\nOn Sat, Jan 26, 2013 at 9:54 AM, Alexander Dean notifications@github.comwrote:\n\nCool, this becomes the new 0.7.2. (Fix in separate branch then ping this\nthread and I'll merge & release.)\nOn Jan 26, 2013 9:52 AM, \"Yali\" notifications@github.com wrote:\n\nI think this is a simple fix - will action ASAP:\n\nhttp://stackoverflow.com/questions/6842245/converting-date-time-to-24-hour-format\n\nOn Sat, Jan 26, 2013 at 9:26 AM, Alexander Dean \nnotifications@github.comwrote:\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/146>.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/146#issuecomment-12733314>.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/146#issuecomment-12733329>.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/146#issuecomment-12752075.\n. Fixed in 0.7.2\n. Hi Angus,\n\nBig thanks for this - that's a great bug spot. We are doing a bug fix release on Monday morning so will be sure to add this in.\nI've checked and we inherited this bug when we forked Piwik's JavaScript tracker almost a year ago (https://github.com/piwik/piwik/blob/master/js/piwik.js#L1093). So I will let them know too.\nObviously this bug meant cookieSecure was always false, which meant that if an all-new user hit a HTTPS page first, then they were assigned a SnowPlow User ID cookie without ;secure indicated.\nWhat I am having trouble understanding is, why was this causing the problems that Phil reported to me - i.e. a separate User ID cookie was generated when the user subsequently navigated to an HTTP page? Perhaps it's just a vagary of HTTP behaviour, but do you have any ideas? Edit: I have just done some testing and the presence or absence of ;secure doesn't seem to be causing any issues (i.e. same cookie is shared across HTTPS and HTTP pages regardless of which is hit first). \nFinal point - unit/integration tests would be amazing and very much appreciated! There is some discussion of different strategies and a link to the Piwik prior art on this ticket:\nhttps://github.com/snowplow/snowplow/issues/106\nBig thanks again,\nAlex\n. Hi Angus,\nThe blog post about Selenium network traffic capture sounds great, look forward to reading it.\nI think you're right - the user ID cookie in snowplow.js is tied to the site ID, so if you were:\n1. Using the CloudFront collector, and:\n2. Were setting multiple different site IDs on the same domain,\nthen each site would be setting different user IDs. Maybe this is what you are running into? Look forward to discussing on Tuesday...\n. Thanks, closing as now merged into feature/no-js-tracker (which will be the next release)...\n. Fixed in 0.7.2\n. Note: this has been reverted in 0.7.5: all cookieSecure functionality has been removed as it was causing user ID cookies to be overwritten. See #181 \n. Exactly, yes - this is another feature we inherited from Piwik alas. No idea why Piwik made the decision - absolutely no benefit taking a user ID already tied to a domain and potentially further restricting it to a single site ID within that domain...\nAngus have you tracked Phil's reported issue down to this ticket?\n. Good point @GuiSim. @yalisassoon could you update 2.1.2 Date / time fields in the Canonical Event Model to reflect the fields currently available in Redshift/Postgres/S3?\n. Update:\n- Let's set nuid and duid (network and domain user IDs) based on the cookies\n- Let's make uid settable with a new setUserId() function\n- Let's remove the attachUserId() method as no longer required\n. Duplicate of #75!\n. Closing as out of date with current Snowplow approach...\n. Closing as out of date with current Snowplow approach...\n. Let's finally do this one now we have the JSON Schema stuff.\n. De-scheduling as not pressing\n. Closing, this would be handled in Iglu Central / tracker repos these days.... Hi @SyedWasiHaider - please discuss on our forums:\nhttps://discourse.snowplowanalytics.com/. Let's finally do this one now we have the JSON Schema stuff.\n. I like the idea of a play ping running every X seconds. Stateful collectors are not a good idea - there lies the path to server affinity which is a world of a pain. I think there are three options:\n1. Send all the 1 second play pings into Redshift and do the calculations there\n2. Create a Kinesis app which groups up the play pings, detects a lapse and generates its own \"viewer stops watching video\" event, which then gets sent on for enrichment and storage\n3. Process the play pings into viewing summary stats in Hadoop, then load summary stats into Redshift (a shame because then Redshift isn't seeing the raw video event flow)\nThoughts @yalisassoon @shermozle \n. Sounds like a plan!\n. De-scheduling as nobody has asked for this.\n. Migrated to JS Tracker, closing\n. A couple of thoughts here:\n1. Internal search could fit quite nicely with tracking item views - essentially treating each search result item as an item view. Quite interesting to see a) how frequently things come up in search, and b) how often they're clicked on.\n2. I like the idea that a SnowPlow user can configure an internal search engine for their site, which is fed into our referer parser alongside the external referers. So in the same way we log referer=search, keywords=cheap,handbags we could log referer=internal-search, keywords=gucci,bag\n. Closing, moved to https://github.com/snowplow/snowplow-javascript-tracker/issues/241\n. Added in 0.7.6\n. Added in 0.7.6\n. @yalisassoon could you do this one into a branch?\n. Note: let's create this table in a schema called atomic, i.e. atomic.requests.\n. Table def merged into feature/pg-support and branch pg-table-def (or similar) deleted.\n@yalisassoon please make any changes to the table def straight into feature/pg-support...\n. Done in 0.8.8.\n. We probably need to load off the hive-format, because the non-hive-format won't include the JSON.\n. Update: Redshift is based on ParAccel which basically copied the Postgres API (although it's not technically Postgres under the hood). So potentially the Redshift output will load into Postgres with no or little modifications.\n/cc @shermozle\n. Done in 0.8.8.\n. Fixed in 0.7.3\n. Awesome - big thanks for sharing @featureless !\nScheduling to add that into the next release (which is a JavaScript-centric release)!\n. Update: error is on line (when snowpak'ed without compression) 2468:\n!!window.sessionStorage,\n. Implemented as part of 0.8.7\n. Duplicate of #175\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/17\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/8\n. Part of a \"gang of three\" cross-row ETL processes, with #20 and #24\n. Hey Simon - the visit counter is stored in a first-party domain-tied cookie. So each domain gets a different counter.\nAre you asking why we don't add a session counter as a third-party cookie into e.g. the Clojure Collector? We have thought about this - and we renamed the current counter domain_sessionidx to make \"space\" for a potential network_sessionidx in the future.\nOr are you asking something else?\n. Sure thing - yes we thought we should leapfrog network_sessionidx and go straight to sessionization in the ETL.\nSessionization has a big dependency on: #210 Add support for derived/canonical user_id\nHere's a quick algo for sessionization in a Hadoop world before I forget:\n1. Process events as standard\n2. Group events by canonical user id and order by time\n3. Join events by user id to a list (generated by last batch) of: user id; current session index; last event time\n4. Determine session index:\n   - If no last event time, create new session index of 1.\n   - If last event time <-> first event time in this batch < session expiry length, then attach current session index to first event\n   - Else increment session index and attach\n5. For all subsequent events, if previous event time <-> this event time < session expiry length, then attach current session index, else increment session index\n6. Final step: write out updated list of: user id; current session index; last event time\n. We can potentially adapt the Pig UDF for Sessionization which is part of LinkedIn's datafu library:\nhttps://github.com/linkedin/datafu/blob/master/src/java/datafu/pig/sessions/Sessionize.java\n. We can use HBase to hold the latest event times for all users seen in the last batch period.\nhttp://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hbase.html\n. Hey @rgabo - that rings a bell, I think that's something Omniture apply too - maybe @yalisassoon would know?\n. The thinking here has moved on a long way with event data modeling - closing this ticket.\n. Assigning to Yali...\n. Added in 0.7.6\n. Added in 0.7.6\n. Added in 0.7.6\n. Added in 0.8.1\n. Added in 0.8.1\n. Done! 0.8.0\n. Done in 0.8.0\n. Fixed in 0.8.4\n. Hey Frutik,\nSorry to hear you've been having problems. I've taken a look - I think the problem is due to a couple of pieces of documentation being out-of-date:\n1. The self-hosting guide refers to an old version of the tracker. The current version is 0.11.0 (see https://github.com/snowplow/snowplow/wiki/Hosted-assets for details)\n2. The JavaScript tracker's manual hasn't been updated since the last release (last Friday). The blog post about this release is here: http://snowplowanalytics.com/blog/2013/02/15/snowplow-0.7.3-released/ If you scroll down on the blog post to \"Breaking changes and deprecations\", I think the problem is probably:\nBreaking change: setAccount() removed, use setCollectorCf() instead\nAdditionally, note that if you are not using CloudFront for the collecting pixel, then you can use setCollectorUrl instead of setCollectorCf.\nApologies if any of this is unclear - and apologies about the out-of-date documentation. We will get the docs updated tomorrow morning and ping you in this thread when it's done!\n. Fixed in 0.7.5\n. Can you paste in the :jobflow: section of your config.yml please?\n. Actually no need - https://forums.aws.amazon.com/thread.jspa?threadID=53963\nSo use something larger than a t1.micro\n. Yes, will do! Thanks @frutik , keep your Qs coming :flashlight:\n. Can you paste in the relevant part of s3n://bk-processing-logs/j-3KGAB1C3MNNTD/steps/2/stderr?\n. Weird:\ncannot find dir = s3n://news-processing/2013-02-25-20-42-00-D32F5F45CE4096CB\nWhat is this news-processing bucket? I can't see any reference to it in your config.yml\n. Right, we think we know what this is. Hive on EMR has a problem with writing and reading from the root of an S3 bucket. We already warn about the writing problem, but we don't warn about the reading problem.\nYour fix: add a sub-folder onto your processing bucket config eg:\n:processing: s3n://news-processing/events/\n(Please let us know if that works.)\nOur fix: https://github.com/snowplow/snowplow/issues/183\n. Okay, look for an *Exception in the files contained within:\ns3n://bk-processing-logs/j-************/task-attempts/job_201302261138_0001/task_201302261138_0001_m_000000_*\n. I'm taking a look...\n. It's failing before the parsing phase - the ETL can't recognise your input data as a valid CloudFront row.\nCan you paste in an example CloudFront row from your logs? It doesn't have to be the exact one that failed if it's hard to find...\n. That's not anything like the CloudFront access log file format - http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html#LogFileFormat\nThere's random text in there e.g. \"snowpoow-pixel\" sic.\nHow are you generating this file? Which of our collectors have you deployed?\n. Okay no worries! That's already in the documentation. Good we checked anyway :feelsgood:\nLeaving open as we still need to add the original warning.\n. Fixed in 0.7.6\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/15\n. Hey Simon - yes we're thinking the same way! We currently store (in Redshift - in Infobright it's the same data but split into more fields):\n- collector_tstamp - the timestamp when the event was received by the collector. Stored to the second\n- dvce_tstamp - the timestamp as reported by the client device when the event happened. Stored to millisecond so if two events happen really close to each other, you know which one happened first\nAs you say - dvce_tstamp is really unreliable, but it's good for user-specific funnel analysis, i.e. knowing a) what order events happened in on the device, and b) the exact elapsed time between events on a given device.\n. Moving this into the Mobile trackers support placeholder milestone\n. Rescheduling this as we will prioritize POST support in the Clojure Collector.\n. Note Kinesis' 50kb per message limit...\n. Moving up!\n. Implemented in 0.8.3\n. Hi Richo - you're right, we're just marking the Ruby version in .rvmrc. If we should move to .ruby-version for this, a PR would be great!\n. Many thanks @richo, this has been merged into feature/scalding-etl (which will be the next release). Closing...\n. Redshift can't load useragent strings > 1,000 due to field restraint. @shermozle just saw a 1,602 character useragent.\n. Thanks Simon, added.\nhttps://github.com/snowplow/snowplow/blob/feature/scalding-etl/3-etl/hadoop-etl/src/main/scala/enrichments/EnrichmentManager.scala#L226\n. Done in 0.8.0\n. Assigning to the performance milestone (currently 0.8.6)\n. Fixed in 0.8.6\n. Done in 0.9.3, closing\n. Agree - ue_ should be the prefix for unstructured events, short and to-the-point.\n. Ah - just realised the Tracker Protocol says usc, hence your confusion! I have updated the Tracker Protocol to say ue now.\n. Implemented as part of 0.8.7\n. Thanks guys! Look forward to having a read through tomorrow.\nOn your Base64 idea - I think that makes a lot of sense (especially given that #20 is quite a long way off). I think the downside (lack of greppability) is worth the upside (less likely to run out of room). Interested to hear anyone else's thoughts too!\n/cc @yalisassoon \n. Yes would be neat to do wiki changes at the same time! But GitHub treats a Wiki as a separate repo strictly (you can manually fork the wiki under Wiki > Git Access).\nA couple of things that would be worth brainstorming:\n1. Dates. I suggest we follow Keen.io and mandate ISO-8601 Format: https://keen.io/docs/event-data-modeling/event-data-intro/#iso-8601-format This should make it relatively painless to detect dates in the ETL\n2. Lists. I think we can support lists okay with the Redshift approach discussed yesterday. Do you have a view on if/how you could support them with Postgres hstore?\nPS Yali says if you're happy to append to the Tracker Protocol in your wiki fork that would be great!\n/cc @yalisassoon \n. Yep the example for trackUnstructEvent looks great!\nOn validation - ultimately we should validate in both places, in the tracker and in the ETL:\n- The tracker should throw an error message to the browser's JavaScript console and not send the event\n- The ETL should reject the row and store it in the bad rows processing folder with the reason why the event payload was bad (this functionality already exists in the Scalding ETL branch)\nValidation in the JavaScript tracker doesn't exist yet - e.g. for trackStructEvent, category and action are required but this isn't checked yet:\nhttps://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/tracker.js#L678\nCompare to the Arduino tracker, where they are validated:\nhttps://github.com/snowplow/snowplow-arduino-tracker/blob/master/SnowPlowTracker.cpp#L291\nI'm creating tickets to add validation to trackStructEvent on both sides:\nhttps://github.com/snowplow/snowplow/issues/199\nhttps://github.com/snowplow/snowplow/issues/200\n. With dates, I think we have two options:\n1. Put dates in \"raw\" and the ETL checks every string to see if it's parseable as a date\n2. Insist that dates are wrapped like this:\njavascript\n\"when_clicked\": {\n            \"tstamp\": \"2012-12-14T20:24:01.123000+00:00\"\n        }\nThis is definitely more fiddly on the tracker (and more verbose in payload terms), but should make validation more robust, and make the ETL more performant (because it's not checking every single string to see if it's a date).\nFinally, we could use this approach to introduce other data types later, like coordinates:\njavascript\n\"tweet_location\": {\n            \"coord\": [-88.21337, 40.11041]\n        }\nThis would be the only \"nesting\" supported, and obviously would be stripped out in the ETL (because it's not true nesting, just nesting to flag data types).\nThoughts? /cc @rgabo @yalisassoon \n. Good point @rgabo - I had forgotten about JS Date. So yes the user simply sends in a Date object and we detect it and expand it to:\n{\"tstamp\": \"XXX\"}\nSo it's invisible to the end user. With coordinates or other non-1st class data types the end user would have to specify the type envelope manually but I think that's survivable.\n. Hey @hzarka - thanks for sharing!\nI like the type suffix idea a lot. As well as the advantages you cite, it also reduces the chance of an accidental collision with a user-defined property.\nOn UTC unix timestamp - in fact we already use these in SnowPlow (https://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/tracker.js#L515), so yes not at all averse to using it here too. As well as the space saving, it also feels a little more robust/harder to get wrong than ISO8601.\nOn milliseconds - it's one of those things where you don't normally need them, but when you do need them, you really need them (the main use case that keeps coming up is funnel analysis when two events happened in the same second - which happened first?). So yes we either keep ms in always for a small concision hit or split it into two distinct types (maybe :dt versus :ts).\nAnyway thanks again for your ideas, this is a really fruitful brainstorm...\n/cc @yalisassoon @rgabo \n. Yes - #20 is quite hairy but increasingly key as we create fatter event envelopes going forwards. #20 is part of a \"gang of three\" cross-row features which now become feasible (if not exactly easy) with the Scalding ETL. The other two are: #24 and #169. We'll be tackling #24 first as it's easiest and will improve existing data quality...\n. Agree - #20 is orthogonal.\nAgree - let's go with Unix Epoch, and support :dt accurate to seconds, :ts accurate to milliseconds.\nMy view - default for { \"time\": new Date() } should always be non-lossy, so for JavaScript that's :ts (with milliseconds) rather than :dt.\nBase64 - @tarsolya raises a good point. Proposal:\n- ue_pr for non-encoded\n- ue_px for Base64-encoded\nSensible default is to use ue_pr (i.e. don't encode payloads), but we can have a encodeUnstructEvents(true) which changes the setting to Base64-encode and populate ue_px instead of ue_pr.\nA bit more complicated for those needing compression but simpler for those who don't.\n. Yep :dt is the weakest part left I think. Anyone else have any ideas? :ts versus :tms?\n. Looks good - except why are we reintroducing ISO8601 for :tms? Isn't it just:\n{ \"timestamp:ts\": new Date()} => { \"timestamp:ts\": 1364471251}\nand\n{ \"timestamp\": new Date()} => {\"timestamp:tms\": 1364471251378} (where 378 is the milliseconds)\n. Great stuff, I think that's everything for the upstream definition!\nOn Mar 28, 2013 4:49 PM, \"Gabor Ratky\" notifications@github.com wrote:\n. Hey @rgabo, @tarsolya,\nMany thanks - the PR looks great. My feedback is as follows:\n- Validation - I say let's create a separate ticket and add validation (e.g. nulls, arbitrary suffixes) as a v2 (we can release when we add validation for trackStructEvent too)\n- Examples - can we add a couple of different timestamp flavours into the example code in (a)sync.html?\n- SnowPlow.base64encode - please move into its own file in src/lib/base64.js and add the new file to dependencies.txt. This separation makes third-party \"dependencies\" easier to manage\nFinal question: how comfortable are we with \":\" as the suffix separator? As a character it's such a core part of JSON, it feels like it could easily lead to accidents when people (or indeed libraries) don't quote their JSON keys (as currently in (a)sync.html). I don't know if there's another character which is a) very distinctive and b) still works with unquoted JSON keys, but wanted to raise this for suggestions.\n. Hi @rgabo - great, many thanks for making those changes.\nAgree on leaving the date conversion as-is for arbitrary suffixes for the v1. I have added a ticket to capture the validation as a phase 2: https://github.com/snowplow/snowplow/issues/209\nOn : - I agree it's a visually nice separator, the trouble is it's already in use between keys and values, and many users are lazy about putting quotes around JSON keys (and I fear some libraries in different languages might be too).\nJust noodling around:\njavascript\nJSON.stringify({ created_at:ts: new Date() })\nSyntaxError: Unexpected token :\nJSON.stringify({ created_at!ts: new Date() })\nSyntaxError: Unexpected token !\nJSON.stringify({ created_at|ts: new Date() })\nSyntaxError: Unexpected token |\nJSON.stringify({ created_at%ts: new Date() })\nSyntaxError: Unexpected token %\nJSON.stringify({ created_at@ts: new Date() })\nSyntaxError: Unexpected token ILLEGAL\nJSON.stringify({ created_at$ts: new Date() })\n\"{\"created_at$ts\":\"2013-04-02T13:05:38.193Z\"}\"\nBased on this, I reckon we should go with a dollar instead - let me know if anyone objects!\n. Cheers - yes I discussed with @yalisassoon and he was happy with \"$\" too. A little weirder initially than \":\" but suspect it will save us from hard-to-debug pilot errors down the line...\n. Looks great! Nice work @tarsolya \nScheduled for new milestone 0.8.1 (next-but-one release).\n. Hey @tarsolya - happy for you to create a separate issue for vim modelines at the bottom of the js source files. Normally I'm not keen on editor-specific stuff getting into repos but happy to make an exception for js+vim.\nAgree the lack of consistent formatting is a bit of a pain - there's an uneasy mix of tabs and spaces across the files. We should agree and action some guidelines at some point - two I like the look of are:\nhttps://github.com/rwldrn/idiomatic.js/\nhttps://github.com/airbnb/javascript\n. Brilliant - many thanks @rgabo!\nPS you can check out the Avro evolution in #213\n. Hey @rgabo - thanks for the updated examples. Here's some feedback on your last 2 comments:\nTime/dates\n- collector_dt and collector_tm are both set using the clocks on Amazon's servers, and recorded in UTC\n- device time is set using the client's clock, and is recorded in UTC (see 1, 2)\n- On $dt - I would think you take the .getTime() and / 8640000 to give # days before/since epoch? I think it's fine to add that in to the JavaScript tracker if you want, and we'll unpick it in the ETL (i.e. change it back to ms-since-epoch)\n1: https://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/tracker.js#L515\n2: http://stackoverflow.com/questions/4577823/if-javascript-new-date-gettime-is-run-from-2-different-timezones\nTyped property tables\nOur thinking has moved on a little since we last spoke on this - and it sounds like yours has too. We're planning on putting together a blog post in the next couple of weeks setting out our new plan. In the meantime, it would be good to catch up and get your team's perspective on typed property tables via a quick Skype chat or call if that works for you. Let us know when would suit!\n. Hey @rgabo,\nWhile porting the unstructured code to the new Lua tracker, I started reading the base64 spec: http://tools.ietf.org/html/rfc4648\nNote the section on Base 64 Encoding with URL and Filename Safe Alphabet: http://tools.ietf.org/html/rfc4648#page-7\nIt looks like your current approach is to use the non-URL-safe variant of base64, and then URL-encode the resultant string (with add()).\nWould it be more compact/cleaner to use the URL-safe variant and then just add it to the querystring non-URL-encoded (i.e. with addRaw())? Interested to hear your thoughts.\nEdit It looks like URL-safe decoding in Java/Scala is pretty easy:\njava\nimport org.apache.commons.codec.binary.Base64;\npublic static String base64UrlDecode(String input) {\n    String result = null;\n    Base64 decoder = new Base64(true); // The true means \"url safe\"\n    byte[] decodedBytes = decoder.decode(input);\n    result = new String(decodedBytes);\n    return result;\n}\n. Hey guys, another piece of feedback which came up from my work porting the above to the Lua Tracker:\nCan we rename encodeUnstructEvents() to encodeBase64() please.\nThis is because encodeUnstructEvents() is at once too specific and too vague:\n1. encode... is too vague - in fact all fields (apart from addRaw()ed ones) are encoded: either URI-encoded or Base64-encoded. encodeUnstructEvents doesn't tell you what type of encoding is being activated\n2. ...UnstructEvents is too specific, because in due course we will be adding property JSONs to all event types (as custom variables aka metrics/dimensions), and so the Base64-encoding toggle shouldn't be specific to unstructured events.\n. Hey chaps - apologies for the delay, we have our CLA ready now:\nhttps://github.com/snowplow/snowplow/wiki/CLA\nAny questions, please let me know.\n. @rgabo - this all looks great, thanks very much for all your hard work on this! And thanks for getting the CLA back to us too.\nThis all looks great - we're going to get this all merged and start testing it. We're only going to make one change - which is defaulting configEncodeBase64 = true. This feels like the more sensible default, given Base64 encoding is what other providers (e.g. Mixpanel) do, and false is more for debugging purposes.\nRight going to merge this shortly and start testing...\n. Merged into feature/unstruct-events for imminent release, big thanks all, closing.\n. Implemented in 0.8.3\n. Scheduled as our first validated track()\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/14\n. Fixed in 0.8.0\n. This is out of date - fixed a long time ago actually.\n. Added in 0.8.1\n. Hey Max, it looks like you have copied over all of the EMR assets to your own bucket, s3://snowplow-etl-assets-ajaila. Can you confirm that this bucket contains all of the required assets (mysql-infobright-etl-0.0.8.q, snowplow-log-deserializers-0.5.5.jar etc) on the right paths?\n. Hey Max,\nAh, you must have changed your config.yml to point to an empty bucket. You can change that line back to:\nyaml\n:assets: s3://snowplow-emr-assets\nand your ETL will work.\nYou can check the template config.yml here:\nhttps://github.com/snowplow/snowplow/blob/master/3-etl/emr-etl-runner/config/config.yml\nThanks, not a bug w SnowPlow so closing...\n. Thanks @frutik, good spot. Have added into feature/scalding-etl which will be the next release.\n. Pushing back into 0.8.3 (taking longer than expected)\n. Assigning to the performance milestone (currently 0.8.6)\n. Note - this is dependent on @alexanderdean finishing the filecrush PR:\nhttps://github.com/edwardcapriolo/filecrush/pull/2\n. Not in scope but capturing for the future: as a phase 2 for filecrush we can potentially save the data in a binary format rather than text.gz.\n. Fixed in 0.8.6\n. Hi @frutik - I suspect about a month from now we'll be releasing the Postgres support. It's the next-but-one release.\nBy the way - I don't think we have you yet on Our users page: https://github.com/snowplow/snowplow/wiki/Our-users\nIf you could email me (alex@snowplowanalytics.com) with the name and logo of the company using Snowplow plus a brief description that would be awesome. More profiled users means more commercial users means faster feature development :sailboat: \n. Done in 0.8.8.\n. This refers to unstructured events - not a dupe of #199\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/2\n. That's clever! I suppose the scope of this derived/canonical ID is wider, as it could potentially include strategies for cross-device-stiching too...\n. See also: http://lucb1e.com/rp/cookielesscookies/\n. The thinking here has moved on a long way with event data modeling - closing this ticket.. /cc'ing @bamos so he's aware that this ticket is well-out-of-scope for current work. scala-kinesis-enrich should simply output a TSV String back onto the stream (i.e. do nothing more than stringify the CanonicalOutput). Not particularly pretty but has the benefit of being very easily ingestable into Redshift.\n. We won't do this ticket, but assigning to milestone for reference purposes.\n. This won't be done, but leaving open for reference.\n. Assigning to milestone for reference purposes.\n. The thinking here has moved on a long way with event data modeling - closing this ticket.. Fixed in 0.8.1\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/18\n. Fixed in 0.8.2\n. Fixed in 0.8.2\n. Fixed in 0.8.2\n. Nice! Seems like you found a workaround for https://github.com/weavejester/lein-ring/issues/55 Mark.\n. Creating a new Milestone 0.8.2, scheduling this into it along with all the other Clojure Collector improvements, thanks @butlermh \n. Unfortunately this doesn't work - the three files from .ebextensions end up in the root of the war, not in an .ebextensions sub-folder. They must be in an .ebextensions sub-folder for Elastic Beanstalk to correctly pick them up:\n\nA pity, we're back waiting for https://github.com/weavejester/lein-ring/issues/55.\nClosing.\n. No worries Mark - thanks for giving it a try!\n. Ah interesting - reopening...\n. Closing as we now have #225\n. Yes sounds like a plan.... Wow - looks great Mark! That Robert Hooke library is a really interesting find.\nI will have a play with this shortly. Scheduling this for the next release.\nBTW we are working on a CLA to be signed by contributors - https://github.com/snowplow/snowplow/wiki/CLA . I will ping you a link when they are finalised (and before merging). Hope this is okay.\n. Hi Mark - apologies for the delay.\nThe individual CLA is here: https://docs.google.com/a/snowplowanalytics.com/forms/d/1J1FNYq9538ndzzcBdlCbxPo1yFiOY4mwalhDTSl1pgg/edit\n. Big thanks Mark, this has now been merged into feature/clj-collector-upgrade, which will be the next release...\n. Thanks @butlermh, lein aws is working a treat. Closing this and prepping the next release.\n. Fixed in 0.8.4\n. Fixed in 0.8.4\n. @frutik - agreed. Our Avro URL-definition makes sure to keep a raw URL field:\nhttps://github.com/snowplow/snowplow/blob/feature/avro/4-storage/avro-storage/src/main/avro/simple_types/URI.avdl\n. See also: https://github.com/michaelklishin/urly\n. Hi @frutik - I'm sorry for the confusion: the 0.8.x series (0.8.0, 0.8.1...) does not support Infobright as a storage target. You can find an explanation of why in our 0.8.0 release blog post:\nhttp://snowplowanalytics.com/blog/2013/04/03/snowplow-0.8.0-released-with-all-new-scalding-based-data-enrichment/#limitations\nYou have three options:\n1. Continue to use the Hive-based ETL process for now. You won't have access to new 0.8.x features such as validation, referer parsing, geo-IP lookups\n2. Migrate to using Redshift with the 0.8.x series (rerunning your ETL process over all of your historic data to populate Redshift)\n3. Wait probably 6 weeks or so for Postgres support to be added, and use Postgres instead of Redshift\nSorry for the confusion - I appreciate the above is a little frustrating.\n. @frutik - good idea!\n. Fixed in 0.8.4\n. Fixed in 0.8.4\n. See #234, #235. Still trying to identify what went wrong with the other two rows...\n. Looks like the other two rows were #235 as well. (Weird bug where Redshift didn't capture the whole row in its stl_load_errors reporting, but rows look clean on disk).\nClosing as we will tackle the specific two issues now.\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/3\n. Fixed in 0.8.4\n. Fixed in 0.8.4\n. Fixed in 0.8.4\n. All releases are tagged, see the branch: dropdown in the GitHub UI for details.\n. Implemented in 0.8.3\n. Implemented in 0.8.3\n. Which collector and ETL are you using?\n. Okay great, thanks for clarifying @mfu0. We'd love to have an IIS collector contribution - it sounds like you're doing some clever stuff!\nSo - an undocumented feature of the JavaScript tracker is that actually it already supports a custom path:\nhttps://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/tracker.js#L637\ni.e., you can already pass in:\njavascript\n_snaq.push(['setCollectorUrl', 'my-own-snowplow-collector-endpoint.com/snowplow']);\nA couple of points on this:\n- Passing in the protocol isn't supported - in any case it's unwise as you could easily end up with Mixed Content errors in the browser\n- The final collector URL will always have /i appended to it - it's important to have a \"signature value\" here as I'll explain below...\nThe reason this feature is undocumented is that a custom path does not currently work with the ETL. Because the logs from the CloudFront and Clojure Collectors can contain non-Snowplow requests, we have a very proscriptive filter to make sure that we only try to process rows which we are sure come from the JavaScript Tracker. Otherwise our validation bucket would fill up with \"favicon isn't valid request\" etc.\nThe filter in the ETL looks like this:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/inputs/CollectorLoader.scala#L77\nWe can potentially loosen this check up a bit to support paths as you suggest. But are you wedded to ice.png - is a pixel called i not an option?\nEdit: I'm thinking a quicker solution might be:\n1. Rename your pixel from ice.png to i (assuming IIS lets you)\n2. Use setCollectorUrl's undocumented ability to pass in the path\n3. In your IIS W3C -> Cf-format log converter module, rewrite /snowplow/i to /i\nLet me know your thoughts!\n. Great stuff! Let us know how you go.\n(Another thought I had: eventually if/when you opensource the IIS collector, we can potentially add IIS W3C as a standard input format for the ETL, which should remove the need for your second module...)\nClosing now, let us know if you have any other issues...\n. Looks good to me! Merging...\n. Hey @rgabo - that all sounds good. I merged to make some further updates - the core extraction loop is now \ndecoding and validating ue_p(x|r).\nI think we're on the same page: I don't see feature/unstruct-events-etl as a candidate for eventual merging into master, but it is a good place to sync up on, discuss and evolve some of the functionality we are going to need down the line, such as JSON parsing and validation.\nOn the other issues you referenced - I wouldn't worry too much about syncing up with the Avro release for now: Avro is only scheduled for the 0.8.x series, and we're only half way through the 0.7.x series now; the 0.7.x series is full of things that Snowplow users are already asking for (geo-IP, dealing with the Hadoop small files problem, an alternate target to Redshift), whereas Avro is more where we see we need to take the product architecturally for H2 2013.\nIn any case: 0.8.0 will be releasing an Avro target only; Avro->Redshift will be in a later 0.8.x release (we're massive believers in small batches at Snowplow). Once we have an Avro target in 0.8.0, it should be easier to involve the community in how they want Avro->other storage targets to work (shredding, custom schemas etc).\nSo, I think for you guys it makes most sense as you say, to continue in your fork to process ue_json into the custom table that you need, and subsequently backing the raw JSON out of the Redshift schema. I think the main points of collaboration here can be:\n- Short-term: discussing JSON extraction and processing\n- Medium-term: seeing if there's any learnings you've made in your fork which should affect our Avro schema design for unstructured events\n- Longer-term: looking at how the custom schema approach is working out for you and how we can start to generalize that (and whether that should be the focus for e.g. 0.8.1 or whether we should start with a shredding approach)\nDoes that make sense Gabor?\n/cc @yalisassoon as he is leading on the Avro schema design...\n. Fixed in 0.8.4\n. Hey @frutik - I'm sorry to hear you've been having problems with your downgrade. We need to get better at explaining when it makes sense to upgrade, versus when a user should stick with the current version - sorry about that. Keep us posted in this thread with how you get on and if we can help...\n. Glad to hear it all got fixed @frutik! On head of master - this will only ever contain documentation/non-functional deltas from the last tagged version (i.e. we never have orphan functionality existing un-tagged in master). So pulling master should be functionally indistinguishable from pulling the last tagged release.\nHaving said this - I agree it's a good idea to make tarballs of stable tags available, not least so you don't have to have git available on your deployment box. I've created a ticket for this: https://github.com/snowplow/snowplow/issues/246\n. We moved to Bintray. Implemented as part of 0.8.7\n. As an example: you might want to tune your ETL so that malformed URLs only generated warnings (i.e. the row otherwise processed correctly).\n. Closing, duplicate of #351\n. Scheduled for 0.8.4.\n. Hey @ehsmeng,\nI'm not sure I understand what is going wrong? I can see JavaScript from Struq incorrectly shown on the page. When I go View Source, I can't find any reference to Snowplow in the code.\nEdit: it looks like the site is using a mix of Google Tag Manager plus lots of tags (GA, Kissmetrics etc) written directly into the page. So I guess you might be running Snowplow inside of GTM... Taking a look at the Network traffic...\nCan you clarify this bug?\nThanks,\n. Edit 2: I can't see any load of sp.js in the Network Traffic, attached.\n\n. No worries @ehsmeng ! Thanks for clarifying.\n. @rgabo - hmm, sounds likely. That narrows the problem down to either a) the flatfiles being written by Scalding or b) the COPY process into Redshift. A grep on the part_* files should tell us which.\n. This should be fixed by #1403, please re-open if not.\n. No idea what this is, ticket is 2 years old, closing.\n. Fixed in 0.8.6\n. Superceded by #263.\n. Very good questions! Have moved to the snowplow-user group and posted an answer to open up to a wider discussion...\nhttps://groups.google.com/d/msg/snowplow-user/APNFdobN2ac/HD3s9i3Jqf8J\n. Fixed in 0.8.5\n. Fixed in 0.8.5\n. Where can I download a VM from with IE8?\n. Problem looks to be this code:\n```\nvar x = offsets[0];\nif (pageXOffset < minXOffset) {\n    minXOffset = pageXOffset;\n} else if (pageXOffset > maxXOffset) {\n    maxXOffset = pageXOffset;\n}\nvar y = offsets[1];\nif (pageYOffset < minYOffset) {\n    minYOffset = pageYOffset;\n} else if (pageYOffset > maxYOffset) {\n    maxYOffset = pageYOffset;\n}\n```\nThis code should be making use of the x and y variables. Referring to pageXOffset and pageYOffset will error on any browser that doesn't have these as properties of the window object.\n. Implemented as part of 0.8.7\n. See EmrEtlRunner implementation for details.\n. Won't fix, we are replacing the JRuby StorageLoader with @chuwy's Scala re-write.. Not a priority. See accepted answer ( not highest scoring answer) here:\nhttp://stackoverflow.com/questions/703396/how-to-nicely-format-floating-types-to-string\n. Fixed in 0.8.5\n. Filing as an ETL bug and putting into 0.8.5 (misc ETL bug fixes).\nIf you have a view on why the JS Tracker is sending such a huge number as the screen resolution, please file as a separate JS Tracker bug!\n. No worries - that code is used to extract viewport and document size as well, my hunch would be that's where the huge numbers are being generated, not screen resolution itself...\n. Now fixed in feature/etl-bug-fixes. Approach taken: catching the error and reporting it as a Validation Failure.\n. Fixed in 0.8.5\n. Hey @joshspivey - agreed, it would be very cool to have a way of still passing the referer back to Snowplow when within an iframe.\nMany thanks for sharing that code. To get this added into the JavaScript tracker, please:\n1. Sign one of our two CLAs - https://github.com/snowplow/snowplow/wiki/CLA\n2. Create a fork of the Snowplow project\n3. Add the above code into it, plus any additional wiring code you need\n4. Smoke-test it to check that it works as you expect\n5. Submit a pull request back to this project, including a quick overview of the new functionality - for example, it's not obvious to me what best practice is for how to set the ?referrer= in the iframe's querystring\nMany thanks - let me know if you have any questions!\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/1\n. Hey Peter - this looks like a good idea, thanks for contributing! Two things:\n1. Have you signed our CLA yet? We have an individual one and a corporate one: https://github.com/snowplow/snowplow/wiki/CLA\n2. Shouldn't we bump up the size of refr_urlport too? One page's page_urlport becomes the next page in the journey's refr_urlport\nThanks - hopefully we can get this into the 0.8.5 bug fix release this week...\n. Nice one - cheers Peter! Will merge this tomorrow...\n. Fixed in 0.8.5\n. Hmm - thanks @mfu0, will take a look!\n. Hey @mfu0 - I've taken a look now. I can see two issues:\n1. https://github.com/snowplow/snowplow/blob/master/1-trackers/javascript-tracker/js/src/tracker.js#L606\n^^ This should be using currentUrl\n\\2. https://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/enrichments/web/PageEnrichments.scala#L95\n^^ When 1. is fixed, this line in the ETL will then cause problems. Basically: if the rewritten custom page URL was shorter than the page URL determined from the collector's referer URL, then the ETL would assume that the url= field in the querystring had been truncated (due to URL length), and default back to using the collector's referer URL to determine the page URL.\n. Great stuff - thanks for checking @mfu0!\n. Implemented as part of 0.8.7\n. Scheduled for 0.8.6\n. Implemented - I am just checking why this change has caused 34 extra rows (0.04%) of our test data set end up in the bad bucket...\n. Problem has been diagnosed - not an issue with this release.\nBasically PBZ used a version of the tracker in Jan which accidentally sent urls in incorrectly double-escaped on the querystring - e.g.\nhttp%253A%252F%252Fwww.psychicbazaar.com%252F2-tarot-cards%252Fgenre%252Fnative%2520american%252Ftype%252Fall%252Fview%252Flist%253Futm_source%253DGoogleSearch%2526utm_medium%253Dcpc%2526utm_term%253Dnative%252520american%252520tarot%252520cards%2526utm_content%253D28722334288%2526utm_campaign%253Duk-tarpt--native-american%2526gclid%253DCKH9l9bU6LQCFSbMtAodbUsA9A\ninstead of:\nhttp%3A%2F%2Fwww.psychicbazaar.com%2F2-tarot-cards%2Fgenre%2Fnative%2520american%2Ftype%2Fall\n268 caused validation to fail for these corrupted rows, because we switched to using the corrupted url= in preference to the collector's (uncorrupted) referer URL.\nSo not a bug with this release.\n. Fixed in 0.8.6\n. Scheduling for 0.8.10...\n. Should we rename to be more clearly focused on an out-of-band compaction process for Very Big users?. Note: this is currently blocked by the lack of a --flatten flag for S3DistCp, which is required to flatten the Clojure Collector's logging sub-folders.\nhttps://forums.aws.amazon.com/thread.jspa?threadID=132614\n. This is ongoing with specific tickets, closing. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Scheduled for 0.8.7\n. Brilliant thanks Gabor!\n. No longer adding EMPTYASNULL as a configuration option which can be switched off as we always want EMPTYASNULL on our load. It's never correct to load null strings as \"\" into our events table.\nThis as a configurable option would only make sense if the Enrichment phase wrote out nulls as e.g. \\000 and we set NULL AS '\\000' - then the option to not add EMPTYASNULL would make sense.\n. Done in 0.8.8.\n. Duplicate of #395, closing\n. See https://github.com/snowplow/sluice/issues/2\n. Sluice is going away. Blocked by #1775 \n. Hey @BenFradet - TBH I am happy to drop the --end and --start arguments - we have never used these (although I know a few in the community did) and I think they have outlived their purpose and are unnecessarily complicated.\nI don't think the renaming is essential either. We just need to be careful that the sub-folder structure is preserved through the pipeline to prevent accidental overwrites.. Right - but the sub-folder structure needs to be persisted when archiving the raw files out of staging.... Done in 0.8.8.\n. Note that the Gzipping will need to be optional so as not to break loading into Postgres.\nQuestion to @yalisassoon : does Hive on EMR support querying gzipped files? I suspect it does...\n. Excellent!\n. Moving this to shredding milestone as this will make the shredding much more performant\n. See also: #610\n. Done in 0.9.5, closing\n. Good idea!\n. Question for @rslifka - does Elasticity support the debug option for EMR? I believe this is the -enable-debugging option in the EMR CLI tool.\n. Ah brilliant - thanks @rslifka !\nEdit - hmm, that's odd because we always set log_uri as part of our job flow, and yet it looks like @rgabo is seeing a screen telling him that debug wasn't enabled.\nCould there be an additional level of debugging beyond the log_uri level that we're not switching on Rob?\n. Ah - actually looks like we enable debugging by adding a custom job step to our flow:\nhttps://forums.aws.amazon.com/message.jspa?messageID=189264\n/cc @rslifka as this would be a nice first-class feature in Elasticity (if we don't get to it first ;-)\nRefs from the Ruby EMR CLI client:\n- https://github.com/tc/elastic-mapreduce-ruby/blob/master/commands.rb#L1795\n- https://github.com/tc/elastic-mapreduce-ruby/blob/master/commands.rb#L322\n- https://github.com/tc/elastic-mapreduce-ruby/blob/master/commands.rb#L244\n. Thanks @rslifka - good idea, will do...\n. Awesome - cheers @rslifka ! Scheduling for next EmrEtlRunner release...\n. Depends on #345\n. Done in 0.8.11\n. We need to move from character-length truncations to byte-length-based ones.\n. Why not just move to TRUNCATECOLUMNS on the Redshift COPY command?\n. The good news is that this is fixed for Redshift users in 0.8.11 which is code complete and currently in test. Expect this to be released next week.\nIn the meantime, if you can't wait, then:\n1. Edit your local copy of https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/lib/snowplow-storage-loader/redshift_loader.rb#L34\n2. Add the word TRUNCATECOLUMNS into the COPY statement\nSee this commit for details: https://github.com/snowplow/snowplow/commit/7871cab88363a5bf10073a36256ed71fb9b63d1a\n. Done, closing.\n. Done in 0.8.8.\n. Hi @mfu0 - thanks for raising this. My preference would be:\n-- Tracker sends two page URLs: The raw document.URL and the (possibly) overridden \"page URL\".\nThere are a couple of reasons why this cannot be implemented for a while:\n1. There is no space in the fat event row for an additional URL (in fact we don't even currently have space to store the raw page and referer URLs alongside the URL components)\n2. (Less important) Passing an additional custom page URL increases the likelihood of the querystring exceeding supported browser length. Much safer to do this when #20 has been implemented\nBy the way - can we add you guys to our users page? https://github.com/snowplow/snowplow/wiki/Our-users \nAdding companies to Our users raises the profile of Snowplow which directly drives investment in the platform...\nWe would need company name, logo and brief description. Thanks!\n. Cheers @mfu0 - let me know how you get on!\n. Migrated to: https://github.com/snowplow/snowplow-javascript-tracker/issues/19\n. Hi @sriv - the reason that the current Hadoop ETL does not currently work with Infobright (or MySQL) is that the flatfile format required for loading by Infobright/MySQL is slightly different from the one required by Redshift/Postgres.\nFrom a roadmap perspective, it makes more sense for us to add Postgres support next (as it requires no change to the ETL), rather than adding in Infobright/MySQL (which would require work to the ETL - work which we would throw away when we launch Avro support in 0.9.x).\nIn other words, the current ETL flow is:\nraw events > ETL > Redshift/Postgres-format flatfiles\nWhere we will move to later this year:\n> Hive\n                                           > HBase\nraw events > ETL > binary Avro event files > MongoDB\n                                           > more ETL > Redshift/Postgres-format flatfiles\n                                                      > MySQL/Infobright-format flatfiles\n                                                      > other-format flatfiles\nTo answer your question: yes the ETL could be forked & patched to load into Infobright - it would require tweaking a few data types. This could work as a stop-gap until MySQL/Infobright support is added back in later this year:\nraw events > forked ETL > MySQL/Infobright-format flatfiles\n. Done in 0.8.8.\n. Done in 0.8.8.\n. Done in 0.8.8.\n. Ah okay - we will keep it in then! If it's useful, it's useful. Only rationale for removing is simplification, nothing else (see other tickets added in 0.8.8)...\nBTW @rgabo can we get SSP added to Our users page? https://github.com/snowplow/snowplow/wiki/Our-users \nWe would need logo and brief description of how you are using Snowplow. alex@snowplowanalytics.com\n. Cheers @rgabo !\n. Done in 0.8.8.\n. Depends on #293 and #294 \n. @yalisassoon has added into branch: https://github.com/snowplow/snowplow/tree/update/hive-table-def\n. (Re-opening until 0.8.8 is released)\n. @yalisassoon Can you test the new Hive script against the files in s3://snowplow-events-archive-emrtest Yali? These folders match the new Hive-required format.\n. I'm hopeful Hive won't barf on the empty _SUCCESS files...\n. Done in 0.8.8.\n. Note this was done directly into master - will formalise this into 0.8.7 (the next release).\n. Implemented as part of 0.8.7\n. Implemented as part of 0.8.7\n. No need to do this - run= is added by EmrEtlRunner, and StorageLoader then just faithfully transfers this over.\n. Unfortunately we're waiting on Scalding for a new release here:\n- https://github.com/twitter/scalding/commit/ddf5974fc8ce78d1d8f6477cd8638a0e2729bbf7#commitcomment-3646638\n. Done in 0.8.8.\n. Implemented as part of 0.8.7\n. 2013-07-28 00:04:39,288 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Running with args:    [Ljava.lang.String;@2ba11b\n2013-07-28 00:04:41,998 FATAL com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Failed to get source file system\njava.io.FileNotFoundException: No such file or directory 's3n://snowplow-emr-processing/xxx'\n. Done in 0.8.11\n. Done in 0.8.10\n. Done in 0.8.10\n. Done in 0.8.8.\n. Done in 0.8.8.\n. Done in 0.8.10\n. Done in 0.8.10\n. Done in 0.8.10\n. Removing this from 0.8.10 as low priority.\n/cc @yalisassoon \n. Closing, nobody has asked for this ever.\n. Done in 0.8.8.\n. Removed in 0.8.8.\n. Done in 0.8.8.\n. Migrated to https://github.com/snowplow/snowplow-javascript-tracker/issues/125\n. Done in 0.8.11\n. Hey @rgabo !\njava.lang.VerifyError may mean that you have a conflict between a library you are compiling against versus the one you are using at runtime. Suspect it might be related to Scalaz given the type signature reported: scalaz/Validation has changed a bit in different Scalaz versions.\nThis commit was a while back so hard to see how this is the problem:\nhttps://github.com/snowplow/snowplow/commit/5af9f769216a6350a065c1aeffe6f0f8cd19c705#3-enrich/hadoop-etl/project/Dependencies.scala\nThere were a lot of version changes going from 0.3.1 to 0.3.2:\nHadoop ETL: bumped to 0.3.2\nHadoop ETL: bumped Scalding to 0.8.5\nHadoop ETL: bumped Scala version to 2.10.0\nHadoop ETL: bumped scala-maxmind-geoip to 0.0.5 to work with Scala 2.10.0\nHadoop ETL: bumped SBT from 0.12.1 to 0.12.3\nHadoop ETL: bumped Specs2 to 1.14\nHard to see what's going wrong - maybe something in your local build process is not quite right...\n. Hey @rgabo - have you tried assembling the jar on a \"clean\" (new) VM to guarantee it's not a dirty SBT/Maven/Ivy cache issue?\nAm super-snowed under at the moment with Prof Services and Snowplow Care - hoping I get time to look at this this weekend...\n. Ah okay - good news! There were very few changes to the Scala codebase in 0.3.3, so a bit strange... Keep us posted.\n. Assuming this is fixed - must have been some dependency hell or dirty cash.\nIf it's still happening, please re-open with steps to reproduce.\n. Basically turn the current storage section of YAML into an array. Probably each storage target should be given a user-definable name.\n. Moving this into 0.8.8 to make it easier for people to run their data into both Redshift and Postgres if they want...\n. Done in 0.8.8.\n. Hey @dominiclovell - thanks for filing this. You're having a problem with the S3DistCp phase at the start of the Enrichment job - we run S3DistCp to take your CloudFront files from S3, compress them and store them on the EMR cluster's local HDFS before running the Enrichment MR job. This makes the job much faster to run.\nI haven't seen this issue before - but I would say that 90% of Enrichment phase errors we see are configuration related. Could you email your full config file minus AWS creds to alex@snowplowanalytics.com?\n. Thanks Dominic,\nYour config.yml looks good. The invocation that's failing is:\n| Elasticity Custom Jar Step | FAILED | 2013-07-28 17:28 EST | 2013-07-28 17:29 EST | /home/hadoop/lib/emr-s3distcp-1.0.jar | - |   --src s3n://<<bucket>>/processing/ --dest hdfs:///local/snowplow-logs --groupBy .*\\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\\..* --targetSize 128 --outputCodec lzo --s3Endpoint s3.amazonaws.com |\nThere's very little in that call to be causing the error... Let's check a few things:\n- s3n://<< bucket >>/processing/ definitely exists, is readable by your AWS creds and is definitely in us-east-1 ?\n- Your << keypair >> keypair was created in the correct region - us-east-1?\nThere's not much else to check - clearly S3DistCp is running, it's just having some kind of permissions error on your EMR cluster...\nLet me know re. the above and we'll figure out next steps.\n. Thanks for clarifying @dominiclovell ! For anyone seeing something similar, the S3DistCp step reported the following error:\n2013-07-28 12:01:08,156 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Skipping key 'processing/' because it ends with '/'\n2013-07-28 12:01:08,156 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Created 0 files to copy 0 files \n2013-07-28 12:01:08,302 INFO org.apache.hadoop.mapred.JobClient (main): Default number of map tasks: null\n. Hi @jakewilliamson88 - please go through the troubleshooting steps here: https://github.com/snowplow/snowplow/wiki/Troubleshooting\nIf that doesn't get it working, then please send an email to the snowplow-user email group.\n. Not really needed - just set continue_on_unexpected_error to true and look in the error rows bucket.\n. Re-opening as this might still be useful, after we have delivered #278\n. Let's do this in this release.\n. Done in 0.9.5, closing\n. Update: the bad referer was:\n&refr=http%253A%252F%252Fbigcommerce%252520wordpress%252520plugin%252F\nUnescaped this referer URL is:\nhttp://bigcommerce%20wordpress%20plugin/\nWe need to update referer-parser to handle a bad referer URL like this - creating ticket...\n. Solution: upgrade Hadoop ETL to referer-parser 0.1.1 (which fixes this).\n. Done in 0.8.8.\n. Hi @energyfirefox - apologies for the confusion, I've only just got back online.\nBasically - unrelated to the 0.8.8 release, we have been tidying up our Amazon S3 account. As part of this tidy-up, we deleted the legacy S3 bucket s3://snowplow-emr-assets. The still-supported bucket holding our hosted Hadoop, Hive etc is called s3://snowplow-hadoop-assets.\nUnfortunately, this new bucket has a slightly different internal format from s3://snowplow-emr-assets did. But the good news is, to fix your problem, I have temporarily added in the old Hive files to the old path structure within the new bucket. So for example, the missing file:\ns3://snowplow-emr-assets/hive/hiveql/mysql-infobright-etl-0.0.8.q\nis now available as:  \ns3://snowplow-hosted-assets/hive/hiveql/mysql-infobright-etl-0.0.8.q\nTherefore, please:\n1. Revert to the old version of your EmrEtlRunner\n2. Revert to the old config for your EmrEtlRunner\n3. Update the line: :assets: s3://snowplow-emr-assets to :assets: s3://snowplow-hosted-assets\nThat should get you running again. (Please do migrate to an updated version of Snowplow when you get a chance, as we won't be keeping this temporary file structure live in s3://snowplow-hosted-assets forevever.)\nAnd apologies for the confusion caused!\n. Ah good - I'm glad.\n@frutik asked on the blog: Is it possible to re-import logs collected by snowplow 0.7 (i.e. last version with infobright support)?\nMy response was:\nHi Frutik! The MySQL/Infobright flatfile format written out by Snowplow 0.7 is slightly different from the Postgres/Redshift flatfile format written by more recent Snowplow versions. You can either manually adjust your existing event files using Unix command line tools (or maybe some Pig or Hive or Cascalog on EMR), or simply re-run the Hadoop ETL process across all of your historic logs. I would recommend the latter, especially given all the new data points (e.g. geo-IP and referer parsing) available in the recent versions of the Hadoop ETL.\nIf you have any further Qs about this - do please email snowplow-user@googlegroups.com\n. Hi @EZWrighter - have followed up on this in the Google Group.\nFor other readers: please note that the fix I set out in the comment above in https://github.com/snowplow/snowplow/issues/316#issuecomment-22149016 was a temporary fix; please contact us on the Google Group if you need more long-term access to these legacy Hive assets.\n. Also ref. #313 \n. I think what is happening is:\n- S3DistCp is respecting the datestamped folders in the grouping copy from S3 to HDFS\n- Hadoop is throwing its standard complaint about sub-directories in the input dir\nSo the simplest fix would be somehow collapsing the archive folders in the S3DistCp group-by statement:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_jobs.rb#L86\nWill need some experimentation to come up with a regex that works for archive and doesn't break regular (non-datestamped folders).\nIf this proves impossible for some reason, I think our two options are:\n1. Politely ask S3DistCp team to add support for this (much preferred)\n2. Add an additional Sluice step to strip the sub-folders (nasty)\n. Thanks for sharing @rgabo . I'm tempted to ask the S3DistCp team to add a --flatten option for sub-dirs. What do you think?\n. @rgabo done! Here is the thread: https://forums.aws.amazon.com/thread.jspa?threadID=132614&tstart=0\nA +1 there would be great...\n. Awesome - lots of support!\n. Closing, this ticket has been superceded by events. Assigning to Alex\n. Done in 0.8.10\n. Note: we are hoping that Amazon implements --flatten for S3DistCp in preference to merging this PR. See this thread for the full discussion: https://github.com/snowplow/snowplow/issues/317\nPlease +1 the Amazon forum thread if you want this issue resolved.\n. @rgabo - it looks like the new --srcPrefixesFile in S3DistCp in AMI 3.8 might be a better solution than this PR now? http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html#UsingEMR_s3distcp.options \n. Okay great - I am going to close this PR... Note that r66 Oriental Skylark is AMI 3.6+ (Hadoop 2.4).\n. @rgabo - we need to check how this new option handles conflicts eg:\nprefix-1/part-00001\nprefix-2/part-00001\n. Done in 0.8.11\n. Thanks @rgabo - this makes a lot of sense. Let's schedule it in with --skip vacuum I think.\n. Actually - in most cases users won't want to use vacuum, so it makes more sense for it to be off by default, switched on with:\n--include vacuum\nIf we are adding the idea of --include, let's add COMPROWS support too...\n. Done in 0.8.11\n. Bump Elasticity to 2.6\n. Depends on #345\n. Closing as Elasticity functionality is insufficient.\n. Done in 0.8.11\n. Done in 0.8.11\n. Scheduling for 0.8.11\n. Done in 0.8.11\n. Thanks for confirming @frutik, scheduling...\n. Could it be related to this https://github.com/snowplow/sluice/issues/5 ? I will investigate.\n. This is caused by this one: https://github.com/snowplow/sluice/issues/9\n. Fix: bump Sluice to 0.1.5, which fixes this bug.\n. Done in 0.8.11\n. Done in 0.8.12, closing.\n. Thanks Rob, I will take a look...\n. Hi Rob,\nThanks for this and for your pull request! I think your approach is the\nright one - basically changing the escape character so that e.g. \\t can't\nbreak the COPY statement.\nHowever, we can't guarantee that \\t won't naturally occur in a field\neither... So I think we need to replace the escape character with something\nguaranteed not to appear in a field - e.g. a control character explicitly\nremoved by:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/utils/ConversionUtils.scala#L89\nI will take a look when I'm back next week!\nA\nOn 24 August 2013 03:09, Robert Kingston notifications@github.com wrote:\n\nHey Alex,\nI managed to work around this issue by using the following query to load\nevents:\n\"COPY #{target[:table]} FROM '#{f}' WITH CSV ESCAPE E'\\' DELIMITER\n'#{EVENT_FIELD_SEPARATOR}' NULL '#{NULL_STRING}';\"\nIt works on the data in my table without error. Mind you my SQL is pretty\nbad though...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23198658\n.\n. Hmm - it might do, I need to check! I'll add in some unit tests for\nfixTabsNewlines in the next release so we can decide exactly what it a safe\nescape character for Postgres...\n\nhttps://github.com/snowplow/snowplow/issues/332\nA\nOn 29 August 2013 12:48, Robert Kingston notifications@github.com wrote:\n\nHi Alex,\nYou're welcome. It's the least I can do.\nWouldn't this effectively escape \\t anyway?\nOn 29/08/2013 7:44 PM, \"Alexander Dean\" notifications@github.com wrote:\n\nHi Rob,\nThanks for this and for your pull request! I think your approach is the\nright one - basically changing the escape character so that e.g. \\t\ncan't\nbreak the COPY statement.\nHowever, we can't guarantee that \\t won't naturally occur in a field\neither... So I think we need to replace the escape character with\nsomething\nguaranteed not to appear in a field - e.g. a control character\nexplicitly\nremoved by:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/utils/ConversionUtils.scala#L89\nI will take a look when I'm back next week!\nA\nOn 24 August 2013 03:09, Robert Kingston notifications@github.com\nwrote:\n\nHey Alex,\nI managed to work around this issue by using the following query to\nload\nevents:\n\"COPY #{target[:table]} FROM '#{f}' WITH CSV ESCAPE E'\\' DELIMITER\n'#{EVENT_FIELD_SEPARATOR}' NULL '#{NULL_STRING}';\"\nIt works on the data in my table without error. Mind you my SQL is\npretty\nbad though...\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23198658>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23477933>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23478131\n.\n. I have done some digging (see below), and I think we can safely use any control character as the escape character. This is because the Hadoop ETL removes all control characters, as per:\n\nhttps://github.com/snowplow/snowplow/blob/feature/improve-etl/3-enrich/hadoop-etl/src/test/scala/utils/conversionUtilsTests.scala#L72\nSo if we do something like this:\nruby\n\"COPY #{target[:table]} FROM '#{f}' WITH CSV ESCAPE E'\\x02' DELIMITER '#{EVENT_FIELD_SEPARATOR}' NULL '#{NULL_STRING}';\"\nThen that control character will never appear in a field value, so the delimiter will never be accidentally escaped...\nDigging:\n- http://www.postgresql.org/docs/9.2/static/sql-copy.html\n- http://stackoverflow.com/questions/935/string-literals-and-escape-characters-in-postgresql\n- http://grokbase.com/t/postgresql/pgsql-general/06bhc57xeg/copy-command-to-load-data-into-a-postgresql-db\n. Tweaked the test data to:\nvfalearning web 2013-08-16 02:00:07.000 2013-08-16 02:00:12.615 page_view   com.snowplowanalytics   1949fd22-3a92-4b29-a4fd-ebd8fa8afdf2    990584  js-0.12.0   cloudfront  hadoop-0.3.3        111.111.111.112 3424596119  6d0b098cf04d2eba    1       AU  7   Torquay     -38.330795  144.32639   VFA Learning | Personal Training Courses | Massage & Childcare Courses  http    vfalearning.vic.edu.au  80  /           http    www.google.com.au   80  /url    sa=t&rct=j&q=vfa&source=web&cd=1&ved=0CC0QFjAA&url=http://vfalearning.vic.edu.au/&ei=oIcNUpKwJ6SXiAfZq4DIAw&usg=AFQjCNGRRiq0FpqDXp8PQYObmDOHyq32Cw&bvm=bv.50768961,d.aGc    search  Google  vfa\\                                                                                                                        Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1 Firefox 4   Firefox 4.0.1   Browser GECKO   en-US   1   0   1   0   1   0   0   0   1   1   16  1366    664 Windows Windows Microsoft Corporation   Australia/Sydney    Computer    0   1366    768 UTF-8   1349    1297\n. Done in 0.8.11\n. Many thanks @kingo55 ! Have you signed our CLA - https://github.com/snowplow/snowplow/wiki/CLA\n. Merged into feature/improve-etl. See also: #329\n. Many thanks Simon. Have you signed our CLA yet - https://github.com/snowplow/snowplow/wiki/CLA\n. Cheers Simon!\n. Scheduling for 0.8.11\n. Done in 0.8.11\n. @kingo55 - thanks for adding to the thread, I'm keen that Amazon understand that unannounced changes to the Cf file format are very disruptive.\n@rgabo - I have started on the fix, https://github.com/snowplow/snowplow/commit/073ba6f15b0908dfa7203966fcb85832db54b556, hope to have something in test today and then do the release tomorrow.\n. A quick update - Amazon have decided to reverse the change. That of course still leaves ~3 weeks of Cf files in the wrong format.\nSnowplow 0.8.9 with the fix is code complete (in branch https://github.com/snowplow/snowplow/tree/feature/cf-format-fix) and in test now.\n. Done in 0.8.9\n. Scheduling for 0.8.13\n. Duplicate of #395 and 0.8.11 fixes.\n. Scheduling for 0.8.11\n. Done in 0.8.11\n. See https://github.com/snowplow/snowplow/issues/3332 for impact of this on Redshift load.. Done in 0.8.11\n. Done in 0.8.11\n. Done in 0.8.11\n. Duplicate of #96.\n. Done in 0.8.11\n. Many thanks @gregakespret ! Have you signed our CLA yet?\nhttps://github.com/snowplow/snowplow/wiki/CLA\n. Many thanks @gregakespret, scheduling...\n. Thanks Grega!\n. Done in 0.8.11\n. Done in 0.8.11\n. @yalisassoon the problem on this specific row is utm_term=bohemian gothic tarot - note the spaces. From doing a lot of testing, it looks like this breaks the URI parsing in a way which is unrelated to the Amazon CloudFront format changes or our fix.\nFrom yesterday's PBZ bad rows, here's another one:\nProvided URI string [http://www.psychicbazaar.com/2-tarot-cards/genre/celtic/type/all?n=24&utm_source=GoogleSearch&utm_medium=cpc&utm_campaign=uk-tarot--celtic-tarot&utm_term=pagan tarot cards&utm_content=29272174408&gclid=COGGut_J7bkCFYyWtAod6QoA3g] violates RFC 2396\nDoes PBZ have AdWords campaigns with spaces in the utm_term? From using the GA URL Builder, it looks like words in utm_term should be \"+\" separated.\nThis does not solve the wider problem, which is that a lot of services generate URLs which use spaces rather than %20 or +. Even if a Snowplow user can fix the problems on her own site and utm campaigns, she does not have control over badly-formatted referers. Here is an example of a badly formatted Amazon referer:\nProvided URI string [http://www.amazon.co.uk/s/ref=nb_sb_noss_2?url=search-alias=aps&field-keywords=tarrot cards] violates RFC 2396\nTherefore I am going to update this ticket, to defensively fix raw spaces in incoming URLs.\n. @chandru9279 0.3.5 as per https://github.com/snowplow/snowplow/blob/feature/improve-etl/CHANGELOG\n. Note: I no longer think this is the underlying issue, but we might as well keep it in 0.8.11 as it handles an edge case which could still happen occasionally.\n. Done in 0.8.11\n. Done in 0.8.11\n. Done in 0.8.11\n. First place to do this:\nextractMarketingFields\nBecause a bad URI should not stop the row being loaded.\n. Current thinking:\n1. Switch from validation failures being Strings to ProcessingMessages which contain a log level\n2. Add a flag to the bad rows output which indicates whether the row was written as an enriched event or not\n3. Add ability to configure what log level will fail the row - WARN, ERROR or FATAL\n4. Add ability to configure the log level for a given enrichment (with sensible defaults provided)\n. Priority is supporting warnings instead of failures for URIs, because:\n\nalthough a Snowplow user can make sure all their URLs are valid according to the RFC standard, they > have no control over referer URLs.\nBecause so many of these referers aren't valid acccording, many first lines in a session (because\nthat's the line with the external referer) are lost. This is a real pain because there's a disproportionate \namount of data in the line. (It reveals where the user came from and what page they landed on).\n\n/cc @yalisassoon \n. Added to 0.9.10! 0.9.10 will actually end up being 0.9.12 (after webhooks and Kinesis r2). It's become a hideous grab-bag so let's do a review of what's in it soon...\n. Talking to @chuwy about this. This becomes more important as we layer on more enrichments with external dependencies (e.g. weather, APIs, DBs), which could fail in some way, but don't necessarily mean that the whole event should fail.\n. One approach which I quite like is to attach the warnings (\"API enrichment failed\") as contexts to the event. Then you always have the partial failure of an enrichment joined to the event affected. Better than creating a whole new set of buckets/streams to manage just for warnings...\n. /cc @jbeemster \n. Good idea @rgabo, adding event type into main issue description!\n. I'm leaning towards doing this by the Scalding ETL executing a piece of JavaScript which can reference any of the fields in our CanonicalOutput and must evaluate to true or false.\nIt should be the most flexible approach...\n. Scheduling for 0.8.14\n. Maybe syntax like this:\nyaml\n:enrichments:\n  :dynamic_filter:\n    :enabled: true\n    :script: ./scripts/filter-ips.cljs\n. Note - we should divide into:\n1. pre-filter - on raw events in log files\n2. post-filter - on full Snowplow events before writing out\npre-filter is useful if e.g. a user has got some kind of uptime monitor (e.g. Pingdom) hitting their Snowplow collector.\n. Updated title as this would be much better done as a new job in the flow than as a set of filters in Common Enrich. If done in Common Enrich, we will be in the situation where rows are being silently filtered with no audit trail...\n. We will implement this as a new Kinesis app, kinesis-tee...\n. Hi @dideler - yes we have to require a CLA for all changes to the repo. There's no way with Apache licensing to distinguish \"big\" changes from \"small\" changes. Many thanks for the pull request!\n. Many thanks! Scheduling for next release...\n. Thanks @yalisassoon that's v helpful. I'm thinking we can get away with just checking if all steps have status CANCELLED. Do you think that's adequate or should we also regexp \"LastStateChangeReason\"?\n. It looks like this:\nSHUTTING_DOWN [On the master instance (i-xxx), bootstrap action 1 timed out executing] ~ elapsed time n/a [ - ]\n - 1. Elasticity Scalding Step: Enrich Raw Events: CANCELLED ~ elapsed time n/a [ - ]\n - 2. Elasticity S3DistCp Step: Enriched HDFS -> S3: CANCELLED ~ elapsed time n/a [ - ]\n - 3. Elasticity Scalding Step: Shred Enriched Events: CANCELLED ~ elapsed time n/a [ - ]\n - 4. Elasticity S3DistCp Step: Shredded HDFS -> S3: CANCELLED ~ elapsed time n/a [ - ])\n. Here's another:\nOn the master instance (i-xxx), after bootstrap actions were run Hadoop failed to launch] ~ elapsed time n/a [ - 2015-04-10 10:16:22 UTC]\n - 1. Elasticity S3DistCp Step: Raw S3 -> HDFS: CANCELLED ~ elapsed time n/a [ - ]\n - 2. Elasticity Scalding Step: Enrich Raw Events: CANCELLED ~ elapsed time n/a [ - ]\n - 3. Elasticity S3DistCp Step: Enriched HDFS -> S3: CANCELLED ~ elapsed time n/a [ - ]\n - 4. Elasticity Scalding Step: Shred Enriched Events: CANCELLED ~ elapsed time n/a [ - ]\n - 5. Elasticity S3DistCp Step: Shredded HDFS -> S3: CANCELLED ~ elapsed time n/a [ - ]):\n. Oh good catch Fred! Agree, let's check for bootstrap string...\n. Hey @fblundun - there is another valid failure string for a bootstrap failure:\nSnowplow data pipeline: SHUTTING_DOWN [Master instance startup failed due to an internal error] ~ elapsed time n/a [ - ]\nIt would be great to support retry for this one too...\n. Thanks Fred!\n. Hi @soplakanets - support issues should go to our snowplow-user Google group, not GitHub bug reports.\nCan you pop an email to snowplow-user@googlegroups.com ?\nThanks,\nAlex\n. Potentially add to:\n- collector_tstamp\n- device_tstamp\n- event\n- domain_userid\n- page_path\n. Taking a look at http://www.postgresql.org/docs/9.1/static/indexes-types.html, it feels like we could start with:\n1. B-tree indexes on the _tstamp fields\n2. Hash indexes on event and domain_userid\nI reckon let's leave page_path out for now (create a separate ticket to consider that index later?)\n. Rescheduled to 0.8.13\n. Hey @mrwalker - any thoughts on this one?\n. De-scheduling\n. Done in 0.8.11\n. Manual workaround:\nWhen the Ruby code waiting for the EMR job dies, you first need to check on the EMR job status (either in the AWS UI or using the API). If the job succeeded, then rerun EmrEtlRunner with:\n--skip staging,emr\nIf the job failed (much less likely), then fix the issue and resume EmrEtlRunner with:\n--skip staging\n. This is happening a lot at the moment - bringing forwards...\n. Pushing back...\n. This should be as simple as catching  IOError (Connection reset by peer) as well as the existing SocketError\n. Here's another one: Errno::ECONNREFUSED (Connection refused - Connection refused)\n. Done, closing.\n. Done in 0.8.11\n. Whoa - thanks for the heads-up @rgabo! That's hugely exciting.\nGreat timing too. /cc @yalisassoon \n. Of course - Redshift isn't designed for unstructured querying in the way that say BigQuery is.\n\nWith this piece of information, how do you imagine the unstructured events would be laid out in the events schema?\n\nWhat events schema?\n. Makes sense. I need to investigate and see how much space we have left to play with in Redshift...\n. Done, closing.\n. Done, closing.\n. Dependent on #426\n. Closing, encapsulated in #359 \n. Dependent on #426\n. Closing, included in scope of #361 \n. Dependent on https://github.com/snowplow/snowplow-javascript-tracker/issues/34 being implemented.\n. This ticket is about checking that the supplied currency is a valid one...\n. This is superceded by the work in r63.\n. Duplicate of #857 \n. No longer required following 0.9.6, closing.\n. This is being worked on by @AALEKH\n. Hi @frutik - thanks but this has been done already in feature/recipe-views.\n(0.8.10 is 80% done in feature/recipe-views. 0.8.11 is 70% done in feature/improve-etl)\n. Not scheduled currently.\n. @kingo55 will be similar in approach to the work Yali is doing on pre-pack cubes (0.8.10 release):\nhttps://github.com/snowplow/snowplow/tree/feature/recipe-views/4-storage/redshift-storage/sql/cubes\n. Done in 0.8.11\n. Would make #402 much harder to do\n. Closing because:\n1. This will make #402 much harder to do\n2. It feels like premature Enrichment - doing querystring parsing in the JS Tracker, rather than in the Enrichment\n3. We don't want to be adding more fields to the Tracker Protocol now\n. Won't fix - this can be replaced by custom context.\n. @jrobgood not sure if I quite follow - but you can pass the end user's IP on the Tracker Protocol with the &ip= parameter. This was added to support proxying.\nActually, we don't have an equivalent for end user's useragent. We should add that - see #381.\n. @jrobgood - correct:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/enrichments/EnrichmentManager.scala#L135\nI think it's an undocumented feature currently - we'll get it added to the documentation.\n. Updated design for this: we don't want to mutate the enriched event from JS, but rather to allow the addition of custom contexts generated from JavaScript.\n. Done in 0.8.11\n. Done in 0.8.11\n. Scheduled for 0.8.14\n. Scheduling for 0.9.9. This is a two step process:\n1. Edit the Snowplow Tracker Protocol\n2. Action this ticket and deploy the new schema to Iglu Central: https://github.com/snowplow/iglu-central/issues/25\n. This is done. Not for CHANGELOG, so closing\n. Added to 0.8.12, the EMR stability release.\n. Let's move this back - get the basic retryable code in first.\n. Agree - updated ticket title. Suggested algo:\n- Get list of available AZs for the region\n- Try one\n- If it fails, try another one\n- Repeat\n. Closing in favour of #187\n. Done in 0.8.11\n. Move back so that this release can be Ruby app only.\n. Needed for #538, moving into that release /cc @pkallos\n. Superceded by #848, closing\n. Also interesting to try and detect that steps like setup debugging are taking \"too long\".\n. See also: https://groups.google.com/forum/#!topic/snowplow-user/4XfTL3ungrI\n. This isn't a problem any more (and if it was, we would fix in Dataflow Runner).. Unscheduled\n. Haven't seen EMR clusters stuck in pending for years, closing\n. For reference see EmrEtlRunner's:\n--process-bucket BUCKET', 'run emr only on specified bucket. Implies --skip staging,archive\n. Design needs further work but there's something useful in it...\n. Add initial contracts around the new loading shredded files\n. Done in 0.9.5, closing\n. Done in 0.9.3, closing\n. Hey Rob,\nUnfortunately the PG COPY command doesn't seem to support a TRUNCATECOLUMNS style option. So we are left doing manual truncation in the ETL:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/enrichments/EnrichmentManager.scala#L277\nCan you update this ticket with all of the fields you are having trouble with, and I will add them in?\n. I have to do the 0.8.11 release now, so adding this in as se_label only. If there are other fields, please create additional tickets.\n. Done in 0.8.11\n. This is the offending change: https://github.com/snowplow/snowplow/commit/22c6c7ef6cc8c58811e73b0199fdfb0b717e23f2#diff-954411b7cd08f867b29a2fe7ba0684ac\n. Done in 0.8.11\n. On disk, run ids look like this: run=2013-08-22-11-20-46\nI'm wondering if we should store the run id in Redshift as:\n1. string, run=2013-08-22-11-20-46\n2. string, 2013-08-22-11-20-46\n3. timestamp, 2013-08-22 11:20:46\n3 Might be most flexible for querying... Maybe we call it etl_ts rather than run_id.\n@yalisassoon what do you reckon?\n. Moving this back so that 0.9.1 can be a Ruby-app-only release...\n. Missing from CHANGELOG\n. Fixed in 0.9.6, closing\n. Done in 0.8.11, thanks for flagging @shermozle\n. Thanks @jrobgood that's really interesting. Could you clarify a little what these are:\nsegment ID -- a compound key of my Segment and Group IDs\nWhat's a segment and a group in this context?\n. Thanks @jrobgood ! Very helpful. BTW here is the Sendgrid doco on their event definitions:\nhttp://sendgrid.com/docs/API_Reference/Webhooks/event.html\nRe. email address hashing - it sounds like best practice is to hash the email address before you embed it in the events? Is there any documentation on best practices here?\n. Makes sense - please @shermozle or @jrobgood add a row to this thread...\n. Cheers Simon, provisionally scheduling this fix for 0.8.12 (although likely 0.8.12 will probably break into two).\n. This is worth fixing ASAP - moving forwards to 0.8.12.\n. Done in 0.8.12, closing.\n. Done in 0.8.12, closing.\n. Done in 0.9.3, closing\n. To be followed eventually by #436 (but much lower priority)\n. Note: this ticket covers the batch (Hadoop) flow only.\n. Assigning this to Fred.\n. Hi @fblundun! Can you update the YAMLs in this ticket to be JSONs? That would be helpful for me...\n. Thanks Fred!\n. Duplicate (?!) of #402\n. Done in 0.8.12, closing.\n. Done in 0.9.3, closing\n. Add Zookeeper notification support too...\n. Don't want to be adding complexity, closing.... e.g. value \"2\" -> 1\n. Done in 0.8.12, closing.\n. Done in 0.9.3, closing\n. Done in 0.8.12, closing.\n. Done in 0.9.5, closing\n. Many thanks! BTW have you guys signed our CLA:\nhttps://github.com/snowplow/snowplow/wiki/CLA\n. Cheers Matt!\nOn Dec 2, 2013 9:27 PM, \"Matt\" notifications@github.com wrote:\n\nHey @alexanderdean https://github.com/alexanderdean, we have not, but\nI'll do that shortly.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/pull/414#issuecomment-29659261\n.\n. Many thanks @mrwalker - scheduling for next release.\n. Note: this bug isn't found in the Redshift DDL, just Postgres.\n. Many thanks @kingo55 ! @yalisassoon can you action in the website then close?\n. Done in 0.8.12, closing.\n. Done in 0.8.12, closing.\n. Dependent on https://github.com/rslifka/elasticity/issues/49\n. Not likely ready for 0.9.1, postponing...\n. Duplicate of #622, closing\n. Done in 0.8.12, closing.\n. Done in 0.8.12, closing.\n. Done in 0.8.12, closing.\n. Done in 0.8.12, closing.\n. Done in 0.8.12, closing.\n. Hi @tomgrim1 - thanks for raising this. You are right - there is a bug here. It is on our radar and we have a community contributed fix (#414) that will go into the next version.\n\nI'll close this as a duplicate of #414. Let us know if you spot anything else!\nThanks, Alex.\n. Dependency on https://github.com/rslifka/elasticity/issues/59\n. ruby\njobflow.tags                              = {name: \"app-name\", department: 'marketing'}\n. Done, closing.\n. Let's address this soon as it comes up from time to time and fails a load.\n. To clarify why this happens: it's because the OS clocks on users' PCs are very unreliable. Sometimes people set them to weird times to get around software trial expiry dates, sometimes the BIOS chips go haywire. This is why we recommend using client_timestamp only for the flow of events belonging to a single user (events sharing the same clock); for all other analyses use dvce_timestamp.\n. To confirm: we can fix this in Redshift with: TIMEFORMAT 'auto' in our COPY command.\n. To confirm: we can fix this in Redshift with: TIMEFORMAT 'auto' in our COPY command.\n. Done, closing.\n. It would be great to add to CHANGELOG and action... Thanks Fred!\n. Fixed in 0.9.6, closing\n. Done.\n. Done.\n. This one is done, closing.\n. This is done, closing. Will re-open if we spot anything during testing.\n. Hey Brandon - I would say initial tests just checking HTTP responses. Then if there's time, we could write an IRecordProcessor-based check. But this latter sounds hard - because we would have to wire this into the middle of a Specs2 test...\n. This looks great bamos. A couple of things:\n1. We should probably change \"store the expected event as a serialized Thrift object in Kinesis.\" to \"store the expected event as a serialized Thrift object in the enabled sink.\"\n2. Can we assert the payload too (even if it's empty)?\nBut yes this looks great - something similar for the Kinesis-based enrichment would be awesome!\n. \"\" -> null feels right to me... Otherwise we are using \"\" as a flag value for emptiness?\n. Yes please - \"coming soon\" everywhere sounds good. We have quite a backlog of roadmap and contributing pages to write! Things should be a bit easier now we have the jvm-dev-environment now tho...\n. Hey Brandon - I would probably split it out a bit separately:\nGuide 1: setting up the Scala stream collector\n1. Download from Snowplow Hosted Assets\n2. Write config file\n3. java -jar CLI options\nGuide 2: building the Scala stream collector yourself\n1. Create Scala/SBT\n2. ...\n. Thanks @bamos - I have updated both sections\n. Many thanks Brandon, I have gone through these now and have rationalized and updated. I think they're looking good, closing the ticket. @yalisassoon will probably make some updates next week when he goes through the guides himself.\n. Hi @dtheodor - thanks for your question, it's an excellent one. We are at the earliest stages of adding the ability to customize the business logic of the Enrichment process. We agree with you that it's very valuable functionality!\nThe introduction to this blog post explains a little bit of our thinking around customization:\nhttp://snowplowanalytics.com/blog/2013/10/21/scripting-hadoop-part-1-adventures-with-scala-rhino-and-javascript/\nWe also have some stubbed tickets for the functionality:\n- #378 \n- #352\nThese tickets however will take some time - in the meantime, as the blog post mentions, the other solutions are typically to fork the Snowplow Enrichment job, or to add another job downstream of the current Enrichment job. There are pros and cons to both approaches.\nHowever - your specific customization is a commonly requested one - so we have a dedicated ticket for it, which hopefully we can action sooner:\nhttps://github.com/snowplow/snowplow/issues/402\nTo implement this, we really need to understand how the SiteCatalyst variable works:\n- Is the variable always called cmpid, or is it user-configurable?\n- Is the value always utm_source, utm_medium etc, pipe-delimited, or is that a custom format you came up with yourself?\n. Also paging @shermozle for his experiences here...\n. Hey Simon - thanks for this, super-helpful explanation and recommendations. Factoring in what you've said and Dimitris, how about a new enrichment in the config.yml like this - standard Google-style settings:\nyaml\n:enrichments:\n  :campaigns:\n    :enabled: true\n    :mapping: static\n    :fields:\n      :mkt_medium: utm_medium\n      :mkt_source: utm_source\n      :mkt_term: utm_term\n      :mkt_content: utm_content\n      :mkt_campaign: utm_campaign\nTo use it in the way you suggest:\nyaml\n:enrichments:\n  :campaigns:\n    :enabled: true\n    :mapping: static\n    :fields:\n      :mkt_medium:\n      :mkt_source:\n      :mkt_term:\n      :mkt_content:\n      :mkt_campaign: cid\nAnd eventually, we could add support for Dimitris' version, like so:\nyaml\n:enrichments:\n  :campaigns:\n    :enabled: true\n    :mapping: script # Note 'script', not 'static'\n    :fields:\n      :mkt_medium: cmpid.split('|')[0] # This is JavaScript\n      :mkt_source: cmpid.split('|')[1]\n      :mkt_term: cmpid.split('|')[2]\n      :mkt_content: cmpid.split('|')[3]\n      :mkt_campaign: cmpid.split('|')[4]\n@yalisassoon , @shermozle , @dtheodor - what do you guys all think?\n. @dtheodor - exactly right:\n1. Fork the repository\n2. Edit AttributionEnrichments.scala\n3. Assemble and upload the new jar file\n4. Update your EmrEtlRunner's config.yml to reference your new bucket\n. Closing this ticket as duplicate of #352, #378, #402 and new #436 \n. This will probably get pushed back, but scheduling anyway.\n. Ah pulling from Maven is much better. Sorry didn't realize it was available on AWS!\n. Hey Brandon - the only issue I had was the AWS dep ( #437 ), so yes that should all be fixed. Great stuff.\n. Hmm - I like it! Good idea Brandon - otherwise we are just inventing Yet Another Serialization FormatTM...\n. That looks great Brandon - that's a wrap!\n. Ah right - the one to follow is one of our libraries, like:\nhttps://github.com/snowplow/snowplow/blob/feature/enrich-improv/3-enrich/scala-common-enrich/project/BuildSettings.scala#L70\nPublishing to Maven is just a matter of adding a section like that. If you let me know when that's done, I can hit the publish command.\n. Once you make these changes, a neat change is from:\nscala\npackage com.snowplowanalytics.scalacollector\nto:\nscala\npackage com.snowplowanalytics.collectors\npackage scalastream\nThis will let you pull in the Thrift POJO with just import thrift.XXX\n. Hey Brandon - apologies, I've been working on the Hadoop ETL and noticed the package is:\ncom.snowplowanalytics.snowplow.enrich....\nThe additional .snowplow. is in there to indicate this belongs to the core Snowplow product, rather than other projects we will open source later.\nSorry, could you add this in to yours too?\ncom.snowplowanalytics.snowplow.collectors....\nApologies, promise this is the last change :-)\n. Thanks :-)\n. Nice!\n. I think Spray is very tightly coupled to Akka's actors, but that's about as far as my knowledge goes :-)\n. Done in 0.8.12, closing.\n. Moving back so that 0.9.1 can be Ruby app only.\n. We can do this in 0.9.3, as we will be changing all these pieces in that release.\n. Won't fix, we are moving to Spark Enrich anyway in R89. Hey Brandon - that sounds like a good approach. Let me know how you get on!\n. No that looks like a wrap to me!\n. Let's bump version to 0.2.0 with this change.\n. Published!\n. Thanks, this is done now, closing.\n. Closing this one as we have userId in here, correctly set. 0.4.0 is published too.\n. Many thanks for this - @yalisassoon can you please fix these?\nBTW Brandon I have tweeted about your new tool! https://twitter.com/SnowPlowData/status/419820663537684480\n. Assigning to Fred's enrichment phase 3\n. Hmm can't think of one!\n. Added child ticket: #462\n. New branch is feature/kinesis-enrich\n. Hi Brandon, thanks for raising, it's a very good question.\nLong-term the plan is do a re-interpretation of the CanonicalOutput into Thrift - see #211 (and #212).\nBut this will take a long time to do. The advantages of just serializing a byte array which is a TSV String for now are:\n1. It should be a lot faster to implement\n2. It should be possible to directly ingest this stream into Redshift using kinesis.connectors.redshift without additional coding\nAgree it's a bit hideous, but probably better than creating an intermediate Thrift representation which we'll only throw away later...\nDoes that make sense?\n. De-scheduling\n. JSON is just easier for most analysis purposes, closing.. Depends on #475\n. Hmm, good point. Of the two of these, thrift.TrackerPayload is the more permanent, so I guess we can import inputs.TrackerPayload => ITP or similar?\n. thrift.TrackerPayload => ThriftTrackerPayload sounds good to me...\n. For sure - will do!\n. Assigning to Alex to review.\nIn the meantime, have published current as a snapshot:\ninfo] Main Scala API documentation successful.\n[info] Packaging /vagrant/snowplow/3-enrich/scala-common-enrich/target/snowplow-common-enrich-0.2.0-SNAPSHOT-javadoc.jar ...\n[info] Done packaging.\n[info]  published snowplow-common-enrich to /var/www/maven.snplow.com/prod/public/snapshots/com/snowplowanalytics/snowplow-common-enrich/0.2.0-SNAPSHOT/snowplow-common-enrich-0.2.0-SNAPSHOT.pom\n[info]  published snowplow-common-enrich to /var/www/maven.snplow.com/prod/public/snapshots/com/snowplowanalytics/snowplow-common-enrich/0.2.0-SNAPSHOT/snowplow-common-enrich-0.2.0-SNAPSHOT.jar\n[info]  published snowplow-common-enrich to /var/www/maven.snplow.com/prod/public/snapshots/com/snowplowanalytics/snowplow-common-enrich/0.2.0-SNAPSHOT/snowplow-common-enrich-0.2.0-SNAPSHOT-sources.jar\n[info]  published snowplow-common-enrich to /var/www/maven.snplow.com/prod/public/snapshots/com/snowplowanalytics/snowplow-common-enrich/0.2.0-SNAPSHOT/snowplow-common-enrich-0.2.0-SNAPSHOT-javadoc.jar\n[success] Total time: 26 s, completed Jan 9, 2014 4:42:26 PM\n. This is looking great. I will add a few more tests when I get a chance but let's treat this ticket as complete now.\n. Unscheduled as blocked by #461 which is also unscheduled.\n. Cheers Brandon!\n. Duplicate of #2650. Thanks for the kind words @naomiwells ! If you have any specific Qs just let us know.\n. Hi Naomi - sure: if you git clone this repository you can see the whole structure of the wiki on disk:\ngit clone https://github.com/snowplow/snowplow.wiki.git\nThen check out the user guide for Gollum (technology underlying GitHub wikis):\nhttps://github.com/gollum/gollum/wiki\nHope this helps!\n. Hi Brandon - I would say, if these headers are introduced by the collector rather than coming from the tracker, then yes it's probably a good idea to filter them out and not add them into the headers list in the SnowplowRawEvent.\nThanks, good idea...\n. Hey Brandon - yes could you look into that cookie further in a new ticket? It looks a little odd.\n. Hey Brandon,\nHmm, as a general rule we shouldn't be setting top-level Thrift fields by looking inside the payload. Also there's a bit of a chicken-and-egg there, insofar as, what encoding should be use to parse the payload to get the cs field? :-)\nI guess there is a follow-on question - is the encoding field:\n1. related only to the payload's encoding, in which case why isn't it in the payload rather than top-level in the Thrift?\n2. related to the whole Thrift object (i.e. everything but the client-encoded payload), in which case how much value is it adding when we could just state (or hard-code ;-) that SnowplowRawEvents are UTF8?\nI was wondering - is there an HTTP header which tells you what encoding the incoming request is in? Request being URI and body etc.\n. No worries Brandon - I am going to take this one out of the milestone as it's not obvious to me what we should do here. It's fine to hard-code to UTF8 for release.\n. This hasn't cropped up again, closing. Duplicate of #426 \n. Released in 0.8.13\n. Is this a big job?\n. Add back in as/when a new release of SnowCannon is compatible with the Snowplow Enrichment process.\n. This is done.\n. Done in branch feature/kinesis-enrich\n. This is done, closing.\n. Okay cool - so the cups cookie is a non-Snowplow one...\n. Good catch Brandon! I saw '365d' and thought, \"oh that's cool, didn't know you can express days like, but I didn't think to check!\" :-)\n. Cheers Brandon - just added an in: {} wrapper to future proof in case we add further input streams (e.g. for control data, dimension widening) in the future.\n. Looks good to me\n. Ouch, good spot. octets is the right term, I'm not sure why I introduced quartets! Brain-freeze.\nSuggest closing this, and creating a new ticket, I'll handle this today: #491 #492\n. This is done. We will bump version from -SNAPSHOT when the time comes.\n. See #480 for a fuller discussion of this idea.\n. Closing. This will be handled by simply restarting the app #1250\n. This will be done with Serf, see e.g. #1250\n. Not sure this is such a good idea actually. It's quite nice that the Enrichment is not tied to a Thrift-specific object. What if we add in e.g. Avro or Protocol Buffers support in due course? We would have to reverse this work... Closing.\n. See #493 for successor ticket.\n. Done, closing.\n. This is done.\n. This is done.\n. Done.\n. Re-opening as actually the TSV output is value only (i.e. value\\tvalue\\tvalue not name\\t\\value\\tname. This is for direct loading into Redshift (Redshift TSV loading is positional i.e. fieldname-agnostic).\n. That's a wrap! Thanks, closing...\n. Hi @luxerama - no reason, we probably just chose COPY because we were in a Redshift frame of mind.\nSuggest we change to \\copy unless anybody can give a good reason why not, scheduling...\n. This doesn't work, de-scheduling, removing from CHANGELOG and codebase and closing.\nhttps://www.ruby-forum.com/topic/183322\n. Hi Vincent, no worries at all! If you could create a PR that would be great. I'll then combine your work with the PR from Radico (#624)...\n. Hi @luxerama - just following up on the above, could you create a PR for us and sign our CLA[1]?\nThere is a big refactoring of the StorageLoader coming soon so I am keen people don't get stuck on old forks.\nhttps://github.com/snowplow/snowplow/wiki/CLA\n. This isn't core to 0.9.4, pushing back.\n. If you wanted to have a go at this that would be awesome, many thanks Brandon!\n. Renamed ticket. Lower priority than any code work.\n. Thanks for that @bamos - moved those todos to their own ticket, #520. I will do a pass on the Enrichment section of the wiki prior to releasing 0.9.0.\n. This is done.\n. Nice!\n. Nice work Brandon!\n. Hey @bamos - totally agree with everything you say.\nSuggestion - how about we:\n1. Add the sink configuration idea used in the collector to this Enrichment\n2. Add a new sink which is a test sink, for integrating into Specs2 or another test suite\nThis is kind of similar to how the Scalding project has the idea of 'local mode', where an ETL job is run locally rather than on a Hadoop cluster. This is how our scala-hadoop-enrich tests run.\n. Done.\n. Done, closing.\n. Duplicate of #497\n. Hi @ihortom - could you prioritize this page this week? It is getting quite a lot of interest, and I think writing it will be a good introduction to how the collectors work...\n. I have tidied up the ticket description - it was out of date. Here are some notes for the updated page:\n```\n    CloudFront access log format\n        Documentation: http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html\n        Supports single events sent via GET only\n        No support for network_userid\n        Example: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-hadoop-enrich/src/test/scala/com.snowplowanalytics.snowplow.enrich.hadoop/good/TransactionCfLineSpec.scala#L39\n    Tomcat access logs\n        A custom textual format based on Apache logfile format\n        Supports GET but also multiple events sent via POST\n        Supports network_userid\n        Example: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-hadoop-enrich/src/test/scala/com.snowplowanalytics.snowplow.enrich.hadoop/good/CljTomcatTp2MultiEventsSpec.scala#L39\n    Snowplow Thrift Raw Event:\n        Thrift IDL is here: https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/CollectorPayload/thrift/1-0-0#L10\n        Build for Java/Scala is here: https://github.com/snowplow/snowplow/tree/master/2-collectors/thrift-schemas\n        Note that these events are packaged in a Twitter Protobuf: https://github.com/snowplow/iglu-central/blob/master/schemas/com.twitter/SerializedBlock/protobuf/1-0-0 (a format output by https://github.com/snowplow/kinesis-s3)\nThese are just the message envelopes - the event data itself will be encapsulated within the GET querystring or the POST response object. If it's inside the POST response object, then the events will be contained within this JSON:\nhttps://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/payload_data/jsonschema/1-0-3\nWith either GET or POST, the lowest level will be a set of name-value pairs that respect the Snowplow Tracker Protocol: https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol\n```\n. Looks good! Have pushed. A few things:\n- \"Kineses-S3 sink.\" is misspelt\n- \" Kinesis-S3 reads records from an Amazon Kinesis stream, compresses them using splittable LZO, and writes them to S3 \" misses the Protocol Buffer array layer of the encoding that we discussed. Can you talk to @fblundun to get the details of this and add it in?\n- Would be great to have a diagram of the various layers of the onion (Thrift, PB, LZO, gzip)\n. Nice work! Closing\n. Done, closing.\n. Sister ticket of #495\n. There is no code referring to Cloudfront in hadoop-enrich, closing.\n. Depends on:\n- #496 \n- #493\n- #498 \n- #499\n. Done, closing.\n. Done, closing.\n. Not required.\n. Awesome thanks - reassigned.\n. Yes - this is exactly the change I was thinking you might suggest.\n. I have just published a common-enrich snapshot - if you make this change, I'll then publish a new one...\n. Actually I can do it. I'll do it now...\n. Done, 428f649, closing. Have published SNAPSHOT too.\n. Yep is failing because the Base64 is out of date with current schema...\n. Added #503\n. Hey @bamos - awesome, thanks. Re. line 119: that's exactly where I am writing tests/reviewing functionality at the moment. Leave it with me - will push something soon!\n. @bamos - test suite having some issues:\n[error] /vagrant/snowplow/2-collectors/thrift-raw-event/src/test/scala/com.snowplowanalytics.snowplow.collectors.thrift/SnowplowRawEventSpec.scala:33: type mismatch;\n[error]  found   : com.snowplowanalytics.snowplow.collectors.thrift.TrackerPayload\n[error]  required: String\n[error]     val event = new SnowplowRawEvent(0L, payload, \"collector\", \"encoding\")\n[error]                                          ^\n[error] /vagrant/snowplow/2-collectors/thrift-raw-event/src/test/scala/com.snowplowanalytics.snowplow.collectors.thrift/SnowplowRawEventSpec.scala:39: type mismatch;\n[error]  found   : com.snowplowanalytics.snowplow.collectors.thrift.TrackerPayload\n[error]  required: String\n[error]     val event = new SnowplowRawEvent(0L, payload, \"collector\", \"encoding\")\n[error]                                          ^\n[error] /vagrant/snowplow/2-collectors/thrift-raw-event/src/test/scala/com.snowplowanalytics.snowplow.collectors.thrift/SnowplowRawEventSpec.scala:46: type mismatch;\n[error]  found   : com.snowplowanalytics.snowplow.collectors.thrift.TrackerPayload\n[error]  required: String\n[error]     val event = new SnowplowRawEvent(0L, payload, \"collector\", \"encoding\")\n. Sorry - another thing - can we make TrackerPayload the first optional field, not the last?\n. Is having two 40s allowed?\n. BTW - I just realised, we have been using 0.1.0, 0.2.0 etc on SnowplowRawEvent when we really should have been doing 0.1.0-SNAPSHOT. Can you change the version from 0.5.0 to 0.1.0-SNAPSHOT?\n. Looks good!\n. Working a treat! I have now published 0.1.0-SNAPSHOT to Maven.\n. Done, closing.\n. It's okay - I'll do this one. I prefer the current approach as it doesn't feel like we are testing much if we serialize a Thrift object (e.g. a bug in the JVM Thrift de/serialization code wouldn't be spotted)...\n. This is done.\n. I was just wondering that! What about invalid events to stderr? It sort of works because we only have two output streams :-)\nGreat stuff, excited about this feature...\n. Perhaps the sink in that case should be sink= \"stdouterr\"\n. Or we could call both the input and output sinks in this case by the more general family name \"stdio\". Not sure which is nicer... Your call Brandon!\n. @bamos this is so cool! The piping basically gives us a single-thread-single-node implementation of Snowplow collect & enrich & store, which is something we've never had before, and should be awesome for doing more end-to-end integration tests. Great great work!\nI think this ticket is a wrap. Also note I have re-assigned #470 to me as I don't think we'll figure out the answer this week...\n. Just discussing with Yali - another thing we can do is use this to embed end-to-end tests in our trackers. Especially once the CanonicalOutput is changed from TSV to Thrift. In e.g. the Python Tracker:\n1. Fire up a process which runs collector | enricher\n2. Fire a track_event from the Python Tracker\n3. Python-Thrift-deserialize the output of collector | enricher\n4. Validate fields are set as expected\n. Assigning to @yalisassoon to document his work with the Amazon connectors in this ticket.\n. Perfect, thanks for clarifying all that Yali. Re-assigning back to me for scheduling.\n. Label added for collectors! Creating an S3 Sink for a stream would be great, thanks Phil.\nif I understand correctly an S3Sink would be useful for the raw Thirft data?\nConfirmed.\ndesign that allows for several sinks to be used simultaneously, so the raw data can be directed to Kinesis for realtime processing and to S3 for archiving and ETL\nEdit: we get this design with Kinesis for free without additional code. A key feature of unified log tech like Kinesis and Kafka is that we can have multiple different apps reading from the same Kinesis stream (i.e. they can each maintain their own 'cursor' position, and they do not conflict or block each other).\n. There is some additional discussion of the design for this ticket on this PR: #536 \n@pkallos is going to have a stab at this one - thanks Phil!\n. Duplicate of #546 \n. Don't worry - I've witnessed this too. Intermittent code failures is almost always a Spec2s / PermGen memory issue. Normally if I quit sbt and start it again, the test starts working...\n. Yes - that's great. Please put this in a branch for next release...\n. Done in 0.9.4, closing.\n. e.g. Troubleshooting EMR\n. Duplicate of #1674\n. Thanks for raising Brandon! You're right, this was something I was thinking about. Good to have it as a ticket - it's not obvious to me if we should add functionality like this inside of kinesis-enrich, or whether we should have something external which updates it, and then just (gracefully) restarts kinesis-enrich. Swings and roundabouts to both approaches! Thanks for raising.\n. My gut feel now is that separation of concerns means this belongs in Ansible or similar - and we should focus on having a way to gracefully restart kinesis-enrich.\n. Closing as this is a deployment problem not a stream processing problem\n. Now bumped to 0.13.1\n. This is done now.\n. This is done now.\n. Dependent on unstructured events support.\n. Done, closing.\n. Yep this one is important. Scheduling for 0.9.1.\n. Done, closing.\n. Assigning to Fred...\n. Completed by Fred, nice work.\n. Hopefully this will go away with the new enrichments in 0.9.6 - let's check.\n. Close this one as a dupe if already done!\n. That sounds great! Please close this ticket or merge it or rename it...\n. Thanks! Have you signed our CLA? https://github.com/snowplow/snowplow/wiki/CLA\n. Scheduling for 0.9.1\n. Thanks @kinabalu . How are you getting these updates to scala-common-enrich into production? Are you publishing to a local Ivy repo and then updating scala-hadoop-enrich to point to that local copy?\n. Can you itemize step 4? It should look something like:\n4a. Run sbt assembly in scala-hadoop-enrich\n4b. Upload the generated fat jarfile to an S3 bucket in the correct folder structure\n4c. Update EmrEtlRunner config.yml hosted-assets path to point to the new S3 bucket\n4d. Re-run EmrEtlRunner\nSomething like that?\n. @kinabalu - awesome, glad that worked. I have added a page to the wiki which explains how to self-host:\nhttps://github.com/snowplow/snowplow/wiki/4-Self-hosting-Hadoop-Enrich\n. Assigning to Yali and adding documentation tag.\n. Re-assigning to Fred...\n. Assigning to Yali. Note that JS Tracker 1.0.0 hasn't been released yet.\n. Fred - please action this one.\n. Assigning @BenFradet, @ihortom and @jbeemster to liaise and come up with an improvement here.. Is this done?. Awesome, closing.... Note: this behaviour only started a couple of months ago, which makes us think that Redshift started erroneously reporting an informational message as an error into http://rubygems.org/gems/pg\n/cc @shermozle\n. Should reduce likelihood of job failure.\nAlso helpful as HBase can store lookup data across multiple runs\n. @yalisassoon have you seen anyone doing this before?\n. @yalisassoon - prototyping would be awesome!\n. Awesome - assigning\n. Awesome improvement, thanks @pkallos ! Scheduling\n. I'm not sure I understand this one. Why would you write the raw events to multiple Kinesis streams, when you could just have multiple apps reading from the same Kinesis stream?\nEdit to clarify, if you have 10 different use cases for a stream, you just have 10 different apps reading from the same stream, no need to write the data into 10 different streams\n. No worries :-)\n. Right - so I'm not convinced adding an S3 sink to the Scala stream collector is the right approach. As I wrote in one of the email threads:\n\nWhat we need is a new Kinesis app which stores (\"sinks\") to S3 the raw Kinesis stream generated \nby the Scala Stream Collector, not some kind of new S3 sink option built into the Scala Stream \nCollector. We will at some stage add a Kafka sink to the Scala Stream Collector as a Kinesis \nalternative, but S3 has none of the characteristics of Kinesis and Kafka that make it appropriate for \na unified event log (vs long term archival)...\n\nDoes that make sense?\n. Hey @pkallos - awesome, v exciting to have you tackling this!\nThe right place for the app for now would be:\nsnowplow / snowplow / 4-storage / kinesis-s3-sink\nIf it ultimately turns out to be 100% generic in the end (i.e. no Snowplow assumptions made) then we can extract it to its own repo later.\n100Mb chunks gzipped sounds good. The Hive-ish year=YYYY/month=MM/day=DD/hour=HH/ is a nice idea but unfortunately it won't work with the Snowplow batch Enrich process currently. This is because Amazon has not yet actioned this feature request:\nhttps://forums.aws.amazon.com/message.jspa?messageID=479023#\n(S3DistCp is the first - and essential - step in the Snowplow batch Enrich process.)\nSo would suggest just storing in a flat folder structure for now, how about:\ns3://s3-bucket/some/s3/path/thrift-raw-events.YYYY-MM-DD-HH.N.gz where N is the index of the file written within that hour. This format is very similar to the CloudFront S3 filename format*, which has worked pretty well for us.\n* E1AZFVPZEZT1LD.2013-10-21-10.S3yqcoQu.gz\n. I'm closing this one for now - we can continue the discussion over in #506. BTW Phil - I have restructured the next 0.9.1 release to move things forward on the Kinesis + EMR side of things...\n. Duplicate of #1003 \n. Currently blocked by #558.\n. Hi @pkallos - I'm going to write some notes in here:\n- 0.9.1 development is ongoing in here https://github.com/snowplow/snowplow/tree/feature/thrift-emr\n- Running the test suite gives a couple of failures related to #558 \n- The first step on this ticket (538) is to add a few tests into https://github.com/snowplow/snowplow/tree/feature/thrift-emr/3-enrich/scala-hadoop-enrich/src/test/scala/com.snowplowanalytics.snowplow.enrich.hadoop/good - these tests should contain a single row of Thrift data and check the expected output\n- To get these tests running will likely require some Scala-type-based work in EtlJob\nI hope this helps!\n. As another note to @pkallos - I am partially blocked by not having visibility of what the output of Kinesis S3 Sink looks like on disk - are we talking binary or base64 strings or?\n. Hi @pkallos - that makes sense. In that case this will probably prove handy: https://groups.google.com/forum/#!topic/cascading-user/XmVobBznZMI\n. Split out the tests into their own ticket: https://github.com/snowplow/snowplow/issues/559\n. Hey @pkallos, this should fix it: https://github.com/snowplow/snowplow/commit/9b18957e3fd0a0c327460244579726c8cae58aac#diff-ff2f985832493a683e4ed9e7bd50ad91\n. Correct diagnosis - this is a side effect of 944be1e, and the errors you're seeing are as I mentioned above:\nRunning the test suite gives a couple of failures related to #558\nCoarsening the type signature to : Any should be a good workaround for now... The bigger problem I see coming is making the EtlJob work with either a String Tsv input OR a binary Thrift input...\n. You are doing great work Phil!\n. Nice!!\n. This is done, closing.\n. Actually we should remove all tracking pixels because of: https://github.com/blog/1766-proxying-user-images\n. Thanks for the PR, closing in favour of #720 which is now done, see ticket for commit.\n. Not required, this will be handled automatically by Snowplow custom context support.\n. We are using custom contexts for this, closing.\n. Renamed ticket. Do the work into feature/mobile-context\n. Done, closing\n. This is incorrect - it's not an additional processing phase in Hadoop. It's just a little bit of a shuffle in Scalding to end up with multiple rows.\n. De-scheduling as we don't need this for 0.9.7\n. Taking this out of the initial Mobile milestone. @jonalmeida @azsmith do you have any ideas for cool mobile-specific enrichments? This means some additional processing in the Enrichment process which derives some additional data (e.g. new context) from the incoming raw mobile event. Good web examples are referer parsing and marketing campaign extraction.\nIf you can think of any cool mobile-specific enrichments, just add them into this ticket.\n. Woah - that is a really neat idea @jonalmeida! I think I remember meeting a startup in Bristol that was doing something like that. It's not a row-level enrichment - but we could have a process to mine the historical event data (ML?) and figure out likely locations...\n. This isn't how we are implementing mobile support - we are using custom contexts. Closing\n. Many thanks @pkallos for putting all this together! I had a quick read through of this summary earlier. I will try and do some initial code review later this evening to unblock you...\n. No, thank YOU @pkallos - we really appreciate the contributions!\n. Hardcore! Great work Phil...\n. This has been merged into feature/kinesis-sinks for further work.\n. Huge thanks Phil!\n. Hey @kazjote - the work on this ticket is being done in this branch: https://github.com/snowplow/snowplow/tree/feature/kinesis-s3-sink That should have updated dependencies already.\n\nI am setting up snowplow Kinesis workflow and really need data on S3 to use with Spark.\n\nGuessing here but I suspect this sink won't do what you want: this is a sink for storing the raw Snowplow events (as stored by the collector) to S3 (in an optimized LZO-block-Thrift format), ready for the Snowplow Hadoop Enrichment to process into enriched events.\nIf you are setting up the Kinesis workflow and want to use Spark, then I'm guessing you want to use Spark on the enriched events. There is a later milestone to work on a sink that stores enriched events to S3: https://github.com/snowplow/snowplow/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Fourth+Kinesis+Release%22\nI think you have a few options:\n1. Wait for the later milestone which will write enriched events to S3, ready for you to process in Spark\n2. Use Spark Streaming's Kinesis support, and read in the enriched events from the Kinesis enriched event stream\n3. Use the Snowplow batch flow, which writes enriched events to S3, ready for you to process Spark\nOptions 2 and 3 are ready today; 1 will involve waiting probably 4-5 weeks. If you don't want to wait for option 1, the PRs to work with are #689, #996.\n. Hey @kazjote - good point, we really should rename kinesis-s3-sink to make it clear it's writing raw events aka collector payloads. /cc @fblundun - Fred also note that the exclusions may still need tweaking.\nKeep us posted how it goes @kazjote !\n. This should go away with the other changes in this milestone, but assigning to Fred just to be sure.\n. Add to CHANGELOG\n. Fixed in 0.9.6, closing\n. Update We will do this by extending the existing guide. The documentation to update: https://github.com/snowplow/snowplow/wiki/Setup-IAM-permissions-for-users-installing-Snowplow\n. See also: #1490\n. This is done, closing as it's not for CHANGELOG.\n. See also:\n- https://github.com/shermozle/SnowCannon/issues/23\n- https://github.com/snowplow/snowplow/issues/40\nAnd from @shermozle:\n\nTo give a bit more background, you can set up these redirects in Omniture so that they collect a beacon using the standard syntax, but also then redirect to the supplied forward URL. For example, I wrote this one-pager for a company I worked at to ensure campaign clicks are captured after repeatedly failing at getting the concept of checking that the target site had our Omniture code on it. So instead they started using this tool to build links and it always got captured.\nhttp://dl.dropboxusercontent.com/u/62533350/Omniture_redirects/index.html\n. 400 for Bad Request - https://github.com/snowplow/snowplow/blob/feature/uri-redirects/2-collectors/clojure-collector/java-servlet/src/snowplow/clojure_collector/responses.clj#L83\n. If that's easily doable, yes please...\n. Do this in a branch called feature/new-protocol\n. Closing as not for CHANGELOG\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Subsumed into #556, closing\n. Subsumed into #556, closing\n. Done in 0.9.5, closing\n. Replaced with #590, #556, #601 \n. Replaced with #590, #556, #601 \n. Looks like yes - https://github.com/rslifka/elasticity#2---specifying-options\n\nGreat idea, let's schedule...\n. Done, closing.\n. Depends on https://github.com/snowplow/sluice/issues/13\n. Closing, we are moving all file moves to S3DistCp.. Assigning to Fred - it's fine to document in TP before releasing 2.0.0 (in fact, this is technically more correct, implementation follows specification)\n. Duplicate of #1461, closing\n. Duplicate of #574\n. Done, closing.\n. Thrift deserialization is already done in scala-hadoop-enrich for the Hadoop flow. Thrift deserialization is not already done in scala-kinesis-enrich though:\n- https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/inputs/ThriftLoader.scala#L57\n- https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala#L86\nThe Kinesis approach is nice because if our binary data in a given record is somehow corrupted, we can handle this as part of our monadic Validation code. Equally I totally understand the Hadoop approach, where we get LZO decompression and Thrift unmarshalling together at the same time from the LzoThriftScheme[] funkiness.*\nCan we find a way of making both approaches play nicely together? Maybe something like this:\nhttps://gist.github.com/alexanderdean/9588012\n* Although if the Thrift records are somehow corrupted on disk, they will not flow through into our bad bucket in S3, which is quite a shame...\n. Scheduling as we will need to do something here\n. Note this is unscheduled\n. We are doing this in 0.9.4; see relevant tickets. Closing\n. We believe that an empty querystring parameter (never generated by a well-behaved tracker) causes an exception, when it should just be treated as a null value.\ne.g. &tr_af=boatconfigurator&tr_tx=&tr_sh=0\n. Testing a workaround where we just rename the file...\n. Long since superceded by the geolocation enrichment being configurable...\n. Good catch, scheduling...\n. @fblundun could you add this into your imminent Kinesis release? I think it might be in already.\nIf it's difficult, push to Kinesis 2.\n. Thanks Fred\n. It will also make Redshift loads much faster! Scheduling...\n. Phil - awesome implementation. Your approach let's us release this without waiting on #578. Great work! (Unscheduling #578 from 0.9.3.)\n. Requires #677\n. No worries - thanks for the heads-up!\n. Hey @pkallos - did you get over the issues you had with this one?\n. No worries! I am thinking of skipping this and actually just applying compression using S3DistCp. Cheap and cheerful...\n. Need to figure out how this flows through, as we use URIs everywhere, e.g. https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/enrichments/web/AttributionEnrichments.scala#L103\n. Is this superceded by #1172?\n. Thanks Fred, keep me posted!\n. Okay - let's de-schedule from this release but leave open in case of further community feedback\n. Super-weird, scheduling... @yalisassoon have you ever seen this before?\n. Two things I can think:\n1. We bumped the version of our useragent library recently\n2. Maybe the Scala Stream Collector is having some kind of issue recording user agents\n. Agree\n. Thanks Phil - we expected there would be a mismatch between the two UA parsers - bit of a shame though that ua-parser loses the convenient browserType field. Will add a comment to #62\n. Plus the underlying bug was fixed in user-agent utils 1.12, which @pkallos pulled in via #662. So yes let's close.\n. Cleared milestone too\n. \u279c  Development  git clone https://github.com/Cascading/cascading.git\n\u279c  Development  cd cascading\n\u279c  cascading git:(2.5) grep -r \"makeTempPath\" .\n./cascading-core/src/main/java/cascading/util/Util.java:  public static String makeTempPath( String name )\n./cascading-hadoop/src/main/shared/cascading/tap/hadoop/util/Hadoop18TapUtil.java:    makeTempPath( conf );\n./cascading-hadoop/src/main/shared/cascading/tap/hadoop/util/Hadoop18TapUtil.java:  public static void makeTempPath( JobConf conf ) throws IOException\n./cascading-hadoop2-mr1/src/main/shared/cascading/tap/hadoop/util/Hadoop18TapUtil.java:    makeTempPath( conf );\n./cascading-hadoop2-mr1/src/main/shared/cascading/tap/hadoop/util/Hadoop18TapUtil.java:  public static void makeTempPath( JobConf conf ) throws IOException\n. Aha! Looking in: ./cascading-hadoop/src/main/shared/cascading/tap/hadoop/util/Hadoop18TapUtil.java I found:\n``` java\n  /* used in AWS EMR to disable temp paths on some file systems, s3. /\n  private static boolean writeDirectlyToWorkingPath( JobConf conf, Path path )\n    {\n    FileSystem fs = getFSSafe( conf, path );\nif( fs == null )\n  return false;\n\nboolean result = conf.getBoolean( \"mapred.output.direct.\" + fs.getClass().getSimpleName(), false );\n\nif( result )\n  LOG.info( \"output direct is enabled for this fs: \" + fs.getName() );\n\nreturn result;\n}\n\n}\n```\n. I think Cascading is trying to create temporary files before writing to the final destination - perhaps because the file size is large (so it will composite on its side or something like that). It tries to create a temp directory, that directory ends up in _$folder$, there are no write permissions there, so it fails.\nFor $folder, see: http://hadoop.apache.org/docs/r2.2.0/api/org/apache/hadoop/fs/s3native/NativeS3FileSystem.html\nIt looks like maybe setting mapred.output.direct.NativeS3FileSystem=true would fix this...\nI found this: https://github.com/haodeng/hadoop_pig_ec2/blob/master/job_201305291613_0001_conf.xml#L157\n. Would delivering this fix it? #278\nI believe so - because then the output filesystem for the Cascading (Scalding) job would be HDFS, not S3. So Cascading could make any temp folders and files it wanted to without tripping over IAM perms...\n. Closing as we are moving away from Cascading in R89.... Postponing from 0.9.3 as @pkallos' implementation of #574 allows us to release 0.9.3 w/o #578, without breaking Postgres.\n. Hey @esurdam, the recommended way to load Redshift or Postgres from the Kinesis pipeline is a lambda architecture: http://snowplowanalytics.com/blog/2015/06/16/snowplow-r66-oriental-skylark-released/#lambda\n. Wow nice! /cc @bamos as I'm sure he'll be interested in this improvement. Scheduling...\n. To clarify: according to @shermozle's latest email, the above will not fix the problem. See:\nhttps://groups.google.com/forum/#!msg/snowplow-user/cZphPy-QGRw/t5PyqexmBHoJ\n. Scheduling for 0.9.1\n. Depends on #587\n. Done, closing.\n. Fred please action this one...\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. This is a non-starter as we reject invalid JSONs (which is a good thing anyway). Descheduling and closing.\n. Actually this is workable - e.g. removing extra spaces from the JSON, turning 3.0 into 3. Adding back in.\n. Done in 0.9.5, closing\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. Duplicate of #142 \n. Duplicate of #426\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Not needed for CHANGELOG, closing\n. Done, closing.\n. https://github.com/snowplow/snowplow/blob/feature/0.9.2/4-storage/redshift-storage/sql/com.snowplowanalytics/ad-def.sql\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Will be done in Scalding now that #609 is complete\n. Done in 0.9.5, closing\n. This is almost working - but need help with https://groups.google.com/forum/#!topic/cascading-user/w24BMQ6ZG10\n. This is done - we submitted to Scalding, they included in 0.10.0, so we were able to remove this from our release. Closing as technically this is not part of the release (it's part of Scalding's release).\n. Dupe #278 \n. This was done in 0.9.5, closing.\n. nice one\n. Done, closing.\n. Hi @smugryan -\n\nwe aren't sure if they were processed or not.\n\nCan you check in the archive and see if the files which are leftover in the processing bucket are also to be found in the archive? There is no such thing as an S3 file move - under the hood it's actually an S3 copy plus S3 delete, so it's important to figure out which of those ops is going wrong.\nIt may be an issue with Sluice, or it may be an S3 eventual consistency problem...\nEdit or could it be the next run starting before the last few files have been archived from processing?\n. Hmm - this is what a file move looks like:\n(t3)    MOVE snowplow-xxx-output/events/events/run=2014-04-02-23-16-35/part-02308 -> snowplow-xxx-archive/events/events/run=2014-04-02-23-16-35/part-02308\n      +-> snowplow-xxx-archive/events/events/run=2014-04-02-23-16-35/part-02308\n      x snowplow-xxx-output/events/events/run=2014-04-02-23-16-35/part-02308\nThe +-> signifies the copy succeeded, the x signifies that the delete succeeded.\n. I have yet to see this one occur, with many 1000s of jobs run, so I'm closing...\n. There doesn't seem to be anything to update here, closing.\n. Done, closing.\n. Hey @ihortom - can you look at this and fix it as necessary please.. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Huge! Many thanks Matt. Really nice approach. Scheduling...\n. Pushing back as not core for 0.9.4\n. This was done in 0.9.5, closing.\n. Moving back to 0.9.4 as I know how to implement this now.\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Closing as I don't believe this is still relevant.. This doesn't make sense to me, closing\n. Following #62\n. Yep we should upgrade it.\n. I think we can close this as a won't fix given the move to JRuby.\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. We are moving to a world where there is going to be lots of shredding, so doesn't make sense to spend time making this a configurable option.\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Moving this into @BenFradet's milestone.... That sounds good to me @BenFradet !. You could migrate over the ScaldingStep too.... Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. We would need a database of bot IP addresses. Any suggestions @tclass ?\n. Hey @fblundun - I think this has been done, please close if so!\n. Aha - we need to update this to be more permissive - see the Clojure Collector:\nhttps://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/java-servlet/src/snowplow/clojure_collector/core.clj#L44\nNote how a tracker or webhook can post or get to any {api-vendor}/{api-version} ?\n. Thanks Fred!\n. You could support any webhook that supports GET with the CloudFront Collector simply by adding the relevant pixel at the relevant path... We could think about adding these...\n. Need to check too if we need to update the CloudFrontLoader...\n. Depends on #658\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. This has been done, thanks Fred, closing\n. Thanks Phil! Scheduling for the next release where we will update the Scala Common Enrich\n. Moving up to Useragent phase 1\n. Moving this to Fred's milestone as looks to be the next-but-one release\n. Fixed in 0.9.6, closing\n. This is done, closing as not for CHANGELOG.\n. Normally I would say a bug with field order, but:\n1. I have had a quick check and can't see any bugs in the DDL\n2. You would see a lot more than a few load errors\n3. Those definitely sound like epoch timestamps and we don't write out any epoch timestamps to the TSV files\nSo something else is going on - very puzzled as to what...\n. Fixed the problem (if not the underlying cause, which I don't think we uncovered) in:\nhttps://github.com/snowplow/snowplow/issues/1788. Note: need to bump to 3.0.4 for #629\n. This was done in 0.9.3\n. We won't be doing this now.\n. We won't be doing this now.\n. We won't be doing this now.\n. Awesome - thanks @pkallos ! Scheduling for 0.9.3...\n. Note 0.2.3 is not released yet. Needs releasing with latest referers.yml\n. No idea what this was. Done in 0.9.3, closing\n. Superseded by #3023 . Done in 0.9.3, closing\n. I'm not sure how possible this is... Postgres support in JRuby seems to be somewhat old and untouched: https://github.com/headius/jruby-pg\n. I think we will have to re-write a chunk of the StorageLoader in Java, using JDBC...\n. Storing configurations in S3 is clunky, closing\n. Is this a duplicate @fblundun ?\n. Can we start with just two events:\n1. Job started\n2. Job finished (may be failure or success)\n. Hey @fblundun - these look good. I am tempted to say that we create this as a context and add it to all three events. Obv the last 2 can be nil...\n. I can believe that!\n. Looks good\n. Let's start with some super basic tracking - just load failures and load successes.\n. Sounds good!\nOn 8 Jul 2015 9:36 am, \"Fred Blundun\" notifications@github.com wrote:\n\nThe load_failure can have an error field. I don't know if there are any\nfields we want in the load_success event. We can add add an\napplication_context to both events as well...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/679#issuecomment-119496726.\n. Hey Fred - good catch. It should be possible to make this pretty watertight - just search for CREDENTIALS '...' and replace with CREDENTIALS ''. This is what Redshift does in the console when reporting loads.\n. Sounds good. Where does the >=7 constraint come from? \n. Sounds good to me!\n. @fblundun - can we take the above approach and apply it to the StorageLoader's stdout too? In other words, the credentials-scrubbing code should be applied to stdout/stderr/Snowplow... Please create another ticket in this release as necessary...\n. Dupe of https://github.com/snowplow/snowplow/issues/1029\n. De-scheduling as not a priority...\n. This is a dupe of https://github.com/snowplow/iglu-central/issues/27, closing\n. Dupe of https://github.com/snowplow/snowplow/issues/1030\n. Hey @alienware - it's pretty straightforward using the Snowplow Tracker Protocol: https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol\n\nMake sure to setup a JavaScript Tracker on a friendly website to get a better handle on what valid payloads look like...\n. Is this going to happen in this release @yalisassoon ?\n. Moving back\n. Okay cool - yes separate release sounds good, trying to reduce scope of 0.9.4 as much as possible...\n. Done in 0.9.4, closing.\n. Looks great @bugant and @potomak - huge thanks for putting this together! Scheduling for 0.9.3...\n. Hmm - I can't see you both in the https://github.com/snowplow/snowplow/wiki/CLA responses (either individual or company). This is a prereq for merging, thanks.\n. Thanks @bugant! \n. Awesome - thanks!!\n. Hi @pkallos - yes certainly, we can host up the amazon-kinesis-connector libs no problem. I'll create a ticket.\n. Hey @pkallos - don't have a super strong view but I would probably override the emit method rather than tack on a \\n to the kinesis-enrich output. Simply because the newline is only of use when sinking to S3/HDFS - it's not an intrinsic part of the enriched event.\n. For sure - values in the enriched event are tab-delimited. On disk we need a \\n as record delimiter, but in Kinesis we get record delimiting \"for free\".\nDoes the S3Emitter assume that rows to sink already have newlines on them? That seems a bit strange given you could easily be wanting to sink e.g. JSONs, which obviously wouldn't be transmitted with a \\n at the end.\n. This has been merged into the branch feature/kinesis-sinks for further work. Thanks for everybody's work on this!\n. This code now lives in here: https://github.com/snowplow/snowplow/tree/kinesis-redshift-sink\n. Done in 0.9.5, closing\n. Done, not for CHANGELOG so closing\n. Closing\n. Done, closing as not for CHANGELOG.\n. Done, not for CHANGELOG so closing\n. Scheduling for 0.9.3\n. Not sure what this one is, pushing back\n. Done in 0.9.4, closing.\n. Done in 0.9.5, closing\n. As per @shermozle's suggestion, let's make the regexp more tolerant in case future fields are added again.\n. Done, closing.\n. Also includes: 345522f07c603db0feb9eb3908ffa4ea05ec6a93\n. Done, closing.\n. Assigning to a new 0.9.3 release which prioritizes Clojure Collector fixes\n. Rescheduling to 0.9.4. Hopefully can close as a dupe of #717 following 0.9.3 release.\n. Closing as a dupe of #717. Derk please let me know if the issue has not been resolved.\n. Done in 0.9.3, closing\n. Done, closing.\n. Done, closing.\n. Done, closing.\n. Scala common enrich isn't assembled, closing\n. Hey Rob, thanks for flagging. In the short-term, our plan is to switch to the more permissive java.net.URL, #575.\nLonger-term, we want to make URL parsing failures a warning rather than a bad row...\n. Duplicate of #698 \n. Closing, we won't be adding this field.\n. Done in 0.9.2, closing.\n. Done in 0.9.2, closing.\n. Done in 0.9.2, closing.\n. Update the hosted asset paths too.\n. Weird - I can see milestone was moved to Kinesis 1, but it's registering as still Kinesis 3 in GitHub. Trying to change again...\n. Okay I think I moved this now...\n. Thanks @pkallos ! I think yours is the right solution to the geoIP issue discussed elsewhere (I will add a reference so everybody is on the same page).\n. Scheduling for 0.9.4\n. Thanks, scheduling\n. @fblundun - I think this is a dupe, but assigning into your milestone to to make sure...\n. The SendGrid events are pretty straightforward. If anyone is interested in picking this one up:\n1. Read this guide to writing a Snowplow webhook adapter: https://github.com/snowplow/snowplow/wiki/How-to-integrate-a-webhook-into-Snowplow\n2. Review the existing email webhook implementations: Mandrill and Mailchimp (google for \"Snowplow Mandrill\" and \"Snowplow MailChimp\")\n. Thanks for raising @smugryan - we will look into a fix and get that out ASAP.\n. Assigning to a new 0.9.3 release which prioritizes Clojure Collector fixes\n. Hi @smugryan - would you be able to check and see if #700 is an issue for you too? In other words, can you identify events (based on &tid= and &dtm=) in the _var_log_httpd_elasticbeanstalk-access_log-* which are not making it into either of your sets of Tomcat log files.\nI'm trying to understand if we are dealing with two separate issues or if Derk's is a misdiagnosis of your issue (i.e. #700 is a dupe of #717).\n. Thanks @smugryan !\n. Hi @smugryan - that's okay - no need to spend the time now! I think we will focus on #717 and then ask Derk to verify if that's fixed his issue too. I agree with you - I think it's likely they are the exact same issue.\n. Depends on: https://github.com/snowplow/sluice/issues/19\n. Confirmed this implementation is working:\nD, [2014-05-20T18:10:46.385000 #5222] DEBUG -- : Staging CloudFront logs...\n  moving files from s3n://eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/ to s3n://sn-bucket/snplow2/processing/\n(t1)    MOVE eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-13aabd52/_var_log_tomcat7_localhost_access_log.txt-1400601661.gz -> sn-bucket/snplow2/processing/i-13aabd52-_var_log_tomcat7_localhost_access_log.txt-1400601661.gz\n(t2)    MOVE eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-13aabd52/_var_log_tomcat7_localhost_access_log.txt-1400605261.gz -> sn-bucket/snplow2/processing/i-13aabd52-_var_log_tomcat7_localhost_access_log.txt-1400605261.gz\n(t0)    MOVE eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-13aabd52/_var_log_tomcat7_localhost_access_log.txt-1400598061.gz -> sn-bucket/snplow2/processing/i-13aabd52-_var_log_tomcat7_localhost_access_log.txt-1400598061.gz\n      +-> sn-bucket/snplow2/processing/i-13aabd52-_var_log_tomcat7_localhost_access_log.txt-1400598061.gz\n      +-> sn-bucket/snplow2/processing/i-13aabd52-_var_log_tomcat7_localhost_access_log.txt-1400605261.gz\n      +-> sn-bucket/snplow2/processing/i-13aabd52-_var_log_tomcat7_localhost_access_log.txt-1400601661.gz\n      x eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-13aabd52/_var_log_tomcat7_localhost_access_log.txt-1400598061.gz\n      x eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-13aabd52/_var_log_tomcat7_localhost_access_log.txt-1400601661.gz\n      x eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-13aabd52/_var_log_tomcat7_localhost_access_log.txt-1400605261.gz\n(t3)    MOVE eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-ec19d9af/_var_log_tomcat7_localhost_access_log.txt-1400605261.gz -> sn-bucket/snplow2/processing/i-ec19d9af-_var_log_tomcat7_localhost_access_log.txt-1400605261.gz\n(t4)    MOVE eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-ec19d9af/_var_log_tomcat7_localhost_access_log.txt-1400601662.gz -> sn-bucket/snplow2/processing/i-ec19d9af-_var_log_tomcat7_localhost_access_log.txt-1400601662.gz\n      +-> sn-bucket/snplow2/processing/i-ec19d9af-_var_log_tomcat7_localhost_access_log.txt-1400601662.gz\n      +-> sn-bucket/snplow2/processing/i-ec19d9af-_var_log_tomcat7_localhost_access_log.txt-1400605261.gz\n      x eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-ec19d9af/_var_log_tomcat7_localhost_access_log.txt-1400601662.gz\n      x eb-bucket/resources/environments/logs/publish/e-bgp9nsynv7/i-ec19d9af/_var_log_tomcat7_localhost_access_log.txt-1400605261.gz\nAlso: the regular differences in timestamp (...-1400598061 vs ...-1400598062) would explain why Derk was only losing 40% of data, not 50%\n. Done in 0.9.3, closing\n. Good point!\n. Assigning to a new 0.9.3 release which prioritizes Clojure Collector fixes\n. As explained by AWS:\n\nWhen logging inbound connections from behind an Elastic Load Balancer, it is important to look at \nthe X-Forwarded-For HTTP header, as this will contain the source IP address.\nMore information on this can be found here:\nhttp://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide?/TerminologyandKeyConcepts.html#x-forwarded-headers\n. Done in 0.9.3, closing\n. Done in 0.9.3, closing\n. Done in 0.9.3, closing\n. I've added into the wiki page now: https://github.com/snowplow/snowplow/wiki/SnowPlow-Tracker-Protocol#event2\n. Bringing forward and renaming this ticket as we should support this in 0.9.4\n. https://github.com/snowplow/snowplow/issues/722\n. Moving into Mobile milestone\n. Fred I've added this one into your release! Can you add to CHANGELOG please.\n. Fixed in 0.9.6, closing\n. Stay classy\n. Hi @hslee16 - thanks for this PR, but we can't merge it. We stopped supporting the Hive-based ETL process a year ago. Compare:\n\nhttps://github.com/99designs/snowplow/tree/master/3-etl\nto:\nhttps://github.com/snowplow/snowplow/tree/master/3-enrich\n. Duplicate of #1139\n. Pushing back as we can get a long way with SSL termination at the load balancer.\n. Won't fix, put a load balancer in front!. Before implementing, be aware of: https://groups.google.com/forum/#!topic/snowplow-user/o_p2Ikr5ViE\n. This is done, closing\n. Done, closing\n. This is out of date, closing\n. Nice\n. Fred has done this one, closing\n. Fred has done this, closing\n. It's silly to overspecify the payload as either elements or properties or whatever. How about:\njson\n{\n  \"header\": {\n    \"type\": \"xxx\",\n    \"vendor\": \"com.snowplowanalytics\",\n    \"version\": \"x.y.z\"\n  },\n  \"data\": <>\n}\nwhere data is any valid JSON.\n. So the payload would have something like &ue_pr={\"header\":<>, \"data\": <>}\n. Another example:\njson\n&co={                            // Inception level 1 (us_contexts.json)\n  \"header\": <>,        \n  \"data\": [\n    {\"header\": <>, \"data\": <>},  // Inception level 2 (user-defined contexts)\n    {\"header\": <>, \"data\": <>},\n    {\"header\": <>, \"data\": <>}\n  ]\n}\n. Note the ticket name change @fblundun \n. Sorry, I confused vschema.json with self_desc.json. We need to come up with a better name than vschema.json for what this is describing\n. Done in 0.9.5, closing\n. Placeholder for now\n. Near-duplicate of #543 - and we wouldn't replace the current GET approach (as that would break event archives) - instead we would offer this alongside.\n. Let's see if we can introduce a self-describing field to JSON Path files, that would be nice as it would let us version the JSON Path files.\n. What we have is good enough.\n. Done, not for CHANGELOG, closing.\n. Closing as contexts is now fully namespaced. No serious risk of collision.\n. Done in 0.9.5, closing\n. Sure thing - just clone our wiki and have a play around:\ngit clone https://github.com/snowplow/snowplow.wiki.git\nAll the structure etc is there.\nThanks!\n. For consistency, we should probably rename osType to osFamily\n. Please create this in a new branch: feature/mobile-context\n. Moved ticket to Mobile trackers support release\n. Good point!\n. Whoops fingers\n. This is done, closing.\n. This is done, closing.\n. For the full list of magic macros you can use:\nhttps://support.zendesk.com/entries/20203943-Zendesk-placeholders-reference\n. Hi @fblundun - how is this progressing?\n. Remember we can start with a subset of all the available fields, as long as it's a useful subset and we can grow it with SchemaVer ADDITIONs.\n. Migrated to https://github.com/snowplow/iglu-central/pull/625/files. Done in 0.9.5, closing\n. Thanks Peter! I just checked and couldn't see Simply Business having signed the CLA - https://github.com/snowplow/snowplow/wiki/CLA Could you get that signed and let me know?\n. Thanks Stewart!\n. That looks awesome, thanks guys. Scheduling for 0.9.3\n. Done in 0.9.3, closing\n. I think we can close this as a won't fix given the move to JRuby.\n. Not for CHANGELOG, closing\n. Duplicate of #754, closing\n. Done in 0.9.3, closing\n. Thanks Joao! Have you signed our CLA: https://github.com/snowplow/snowplow/wiki/CLA\n. Paging @joaolcorreia \n. Awesome, thanks Joao\n. Done in 0.9.5, closing\n. The behavior you've captured is very odd - and would go some way to explaining why the results from user-agent-utils have historically disappointed (vs e.g. ua-parser, which we will move to eventually).\nRelated issues:\n- https://github.com/snowplow/snowplow/pull/662\n- https://github.com/snowplow/snowplow/issues/62\nScheduling for 0.9.5 (re-test after version bump of user-agent-utils)\n. See this thread for further discussion with @danrama and @shernozle https://groups.google.com/forum/#!topic/snowplow-user/tRgPJytLbYY\n. Depends on #758 \n. This was done as part of #758, so closing.\n. Bringing forward...\n. Moving back, wasn't done\n. Done in 0.9.5, closing\n. Done in 0.9.4, closing.\n. Done in 0.9.4, closing.\n. Done in 0.9.4, closing.\n. Done in 0.9.4, closing.\n. Done in 0.9.4, closing.\n. Done in 0.9.4, closing.\n. Not sure what dashboard this is, closing. The Schema is written. Assigning to Fred to write tests\n. Done in 0.9.5, closing\n. Paging @yalisassoon to do this one in the next day or two. Into feature/shredder would be great.\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Happy to discuss further on Tues\n. ~~NFCCWD~~ Reassign to Alex when done.\n. Done, closing\n. Cool! Can you raise a PR and sign the CLA?\n. Thanks @gregorg ! How would you compare the functionality/performance to this PR: https://github.com/snowplow/snowplow/pull/624/files\n. Wow, thanks for taking the time to performance check these!\n. Okay cool, agreed, closing\n. Add geographic data into Redshift so we can take advantage of Looker's geo plotting capability\n. Done in 0.9.4, closing.\n. Done in 0.9.4, closing.\n. Labelling and assigning to Yali\n. Can't find this in the file anymore: https://github.com/snowplow/snowplow/blob/master/5-data-modeling/looker/lookml/sessions_technology.view.lookml\nClosing; please re-open if this is still a thing.\n. Not required, is hosted fine elsewhere\n. This seems like make-work - we can just append contexts to the existing array.\n. This seems like make-work - we can just append contexts to the existing array.\n. This seems like make-work - we can just append contexts to the existing array.\n. This seems like make-work - we can just append contexts to the existing array.\n. This is done, was in wrong milestone\n. Fixed in 0.9.6, closing\n. TODO: move this ticket to Iglu Central\n. Moved, closing: https://github.com/snowplow/iglu-central/issues/94\n. Depends on: https://github.com/snowplow/iglu-central/issues/95\n. This is partially done - we have published schemas for ecommerce and enhanced ecommerce...\n. Migrated to Iglu Central\n. JSON Paths migrated to Iglu Central\n. Sounds good\n. Done, thanks, closing.\n. Thanks, assigning to Fred\n. Thanks Fred\n. Compare to https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/atomic-def.sql\n. thanks\n. @fblundun needs updating with a enum for the database files\n. Close this once changes have been made to Iglu Central\n. Closing, code lives in iglu-central now.\n. Based on YAML from #402\n. Closing, code lives in iglu-central now.\n. I can't quite parse what you said Fred... You say it should be object rather than an array, unless we do something which looks like an object to me, not an array.\n. Ah I think I get what you're saying. I think you meant:\njson\n[\n    {\n        \"schema\": \"iglu:com.snowplowanalytics.snowplow/anon_ip/1-0-0\",\n        \"data\": {\n            \"enabled\": true,\n            \"parameters\": {\n                ...\n             }\n        }\n    }\n    ]\n. Ah no\n. Did you mean:\njson\n{\n   \"schema\": \"iglu:com.snowplowanalytics.snowplow/enrichments/jsonschema/1-0-0\",\n    \"data\": [\n    {\n        \"schema\": \"iglu:com.snowplowanalytics.snowplow/anon_ip/1-0-0\",\n        \"data\": {\n            \"enabled\": true,\n            \"parameters\": {\n                ...\n             }\n        }\n    }\n    ]\n}\n. It should be possible to write quite a proscriptive JSON Schema for enrichments no?\n. Question: should we drill down one layer further or not? I.e. in this JSON Schema could/should we specify that each enrichment should have an enabled: field and then a parameters:\nNot sure either way, thought I would ask the Q\n. Whoa - I think you just made self-describing JSON Schemas compose Fred!\nInteresting indeed. Let's discuss on Monday.\n. Closing, code lives in iglu-central now.\n. Adding validation to the calling code (EmrEtlRunner, Kinesis flow) is nice-to-have but if we validate in Scala Common Enrich we get both for free.\n. If any enrichment fails validation, then we should throw a FatalEtlError.\n. Good news Fred - there is now some prior art here:\nhttps://github.com/snowplow/snowplow/blob/feature/shredder/3-enrich/scala-hadoop-shred/src/main/scala/com.snowplowanalytics.snowplow.enrich/hadoop/ShredJobConfig.scala#L76\n. Fixed in 0.9.6, closing\n. As per: https://github.com/snowplow/snowplow/blob/feature/shredder/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb#L186\n. @fblundun update ticket name in CHANGELOG\n. Fixed in 0.9.6, closing\n. Add to CHANGELOG\n. Fixed in 0.9.6, closing\n. It's here - https://github.com/snowplow/snowplow-javascript-tracker\n. The minified code is generated by grunt, it's not under source control. You can also find it here:\nhttps://github.com/snowplow/snowplow/wiki/Hosted-assets\n. The unminified version is here https://github.com/snowplow/snowplow-javascript-tracker/tree/master/src/js\n. Grunt + Browserify\n. Updated these\n. Fixed in 0.9.6, closing\n. This is done and not for CHANGELOG, so closing\n. Done in 0.9.5, closing\n. Done in 0.9.5, closing\n. Yep there are a few tickets about this:\n- https://github.com/snowplow/snowplow/issues/354\n- https://github.com/snowplow/snowplow/issues/382\n- https://github.com/snowplow/snowplow/issues/387\n. No worries at all, GitHub search is pretty terrible!\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. This wasn't done in time for the release, de-scheduling\n. It would be nice to download these summary counts to local, parse the files and emit a Snowplow event with the counts.\n. We can potentially implement this as a Spark job which embeds a Snowplow Tracker and just fires the summary counts to a collector on completion...\n. See #826 for config file update\n. Superceded by our latest thinking around Dataflow Runner. Bringing this forward. Talk to Alex when ready to do this one - I have some ideas!\n. Yes please Fred\n. - In R71 we are moving to new-style bad rows everywhere\n- Until R73 we don't have a formal mechanism for loading any bad rows in ES for the Hadoop pipeline\n- For a while we have had a formal mechanism for loading bad rows into ES for the Kinesis pipeline, this only loaded old-style bad rows because the new style were previously only in Hadoop Shred\n- If we only support new-style bad rows then the only issue will be for Kinesis pipeline users who have old-style bad rows already loaded in ES - for these users, they will have to setup a new index in ES for the new-style\nThereforce: this is only a Kinesis pipeline upgrade issue - and within the context of this ticket, just supporting new-style is fine.\n. Some dupes is acceptable for release - can you add the alternatives into a new ticket called \"Explore options for ...\"\n. Note that we are currently trying to get this working with AWS Elasticsearch Service - but that is blocked currently, see https://github.com/elastic/elasticsearch-hadoop/issues/565#issuecomment-146244371\n. Depends on https://github.com/snowplow/ansible-playbooks/issues/29\n. When we do this, we will likely emit the aggregates as Snowplow events rather than tie to a specific storage target...\n. There's another question here which is around how we express which type (sorry) of data (enriched events or bad rows) is being stored in the target. It feels like we should be explicit about this. Let's brainstorm all this when I'm in...\n. Agree, let's add. Let's use _s not .s to be consistent with ssl_mode.\nAt some point we need to move these targets out into properly typed self-describing JSONs, one per databasebut that can wait a couple more releases...\n. Agree, let's add. Let's use_s not.s to be consistent withssl_mode`.\nAt some point we need to move these targets out into properly typed self-describing JSONs, one per database` but that can wait a couple more releases...\n. Can we add an example elasticsearch target into the example config.yml please?\n. Thanks!\n. This is done and not for CHANGELOG, closing\n. Depends on #817\n. Thanks Simon - it's a great suggestion. It's already being worked on by Fred. Closing as dupe of https://github.com/snowplow/snowplow/issues/396\n. Let's do this in 0.9.5...\n. Please don't attempt @yalisassoon  till we have merged the other branch\n. Granular tickets for Scala Hadoop Enrich, Scala Common Enrich, Postgres, Redshift. Expect four tickets or so\n. Closing now other tickets have been created - #833, #834, #835, #836\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. This should be in \"Scala Common Enrich\" methinks\n. Oops sorry, please ignore above comment\n. I think it will primarily be the tests in Scala Hadoop Enrich that need updating (removing expected fields)\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Kevin this is awesome! Thanks so much for contributing this.\nI don't think you've signed a Contributor License Agreement yet - could you sign one? Here is the link:\nhttps://github.com/snowplow/snowplow/wiki/CLA\nMany thanks!\n. Huge! Thanks Kevin, I will get this prepped for release\n. I've merged this into a new repo: https://github.com/snowplow/snowplow-java-tracker\nJust doing some tidying up now. Closing this PR!\n. Done in 0.9.5, closing\n. This is missing from CHANGELOG\n. This is no longer required, closing. @fblundun please remove from CHANGELOG!\nRedshift: fixed sortkeys for atomic.events (#844)                                       - TODO by Alex\n. As yet unscheduled\n. https://github.com/bcgit/bc-java/blob/master/prov/src/test/java/org/bouncycastle/jce/provider/test/AESTest.java\n. I now think - rather than do it at collection time, do it in the S3 and Redshift sinks\n. https://github.com/bcgit/bc-java/blob/master/prov/src/test/java/org/bouncycastle/jce/provider/test/AESTest.java\n. Still unscheduled.\n. Closing this in favour of https://github.com/snowplow/snowplow/issues/3247. Superceded by #911\n. Fred we will do this one very last minute - after 0.9.5 has been released. So don't worry about this one for now.\n. BUT @fblundun this is still missing from CHANGELOG - please add\n. Fixed in 0.9.6, closing\n. You should be good to go on this one. It involves simply removing any %3D decoding from this file:\nhttps://github.com/snowplow/snowplow/blob/feature/json-enrichments/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/utils/ScalazArgs.scala#L95\ni.e. remove the method and calls to it.\n. Fixed in 0.9.6, closing\n. Done in 0.9.5, closing\n. Whoops wrong milestone\n. Please add a link to this page on the wiki when ready\n. Looks great Fred. Some feedback, all minor:\n- Let's call the page \"Configuring enrichments\"\n- Common capital C in the ToC\n- \"The Enrichments\" -> \"Individual enrichments\"\n- I think in Common we should show a \"template\" JSON before describing the fields. A bit confusing otherwise\n- \"Its\" not \"It's\"\n- Links to JSON Schemas should go to IgluCentral.com for consistency with how the code works\n- In Introduction, I would link to the page configuring the EmrEtlRunner\n- I would have a section where you link to the folder containing \"sensible defaults\" in GitHub\n. My last point refers to this folder https://github.com/snowplow/snowplow/tree/feature/json-enrichments/3-enrich/emr-etl-runner/config/enrichments\n. Also: please add this page into the _Sidebar.md (you'll see it if you check out the wiki from GitHub), and push users to it heavily from all the related (calling pages).\nAlso: please slap a warning that this page is not to be used until 0.9.6\n. Nice, closing\n. Closing in favor of #2260. Hi,\nI've patched clojure collector, just set %{X-Forwarded-For}i instead of %a and bump version to 0.6.0b. Iain tou can download it here till we found a permanent fix :\nhttp://dl.free.fr/g5Ft2je4Q\ndiff --git a/2-collectors/clojure-collector/project.clj b/2-collectors/clojure-collector/project.clj\nindex 4178947..03ef7fa 100644\n--- a/2-collectors/clojure-collector/project.clj\n+++ b/2-collectors/clojure-collector/project.clj\n@@ -13,7 +13,7 @@\n ;;;; Copyright: Copyright (c) 2012-2013 Snowplow Analytics Ltd\n ;;;; License:   Apache License Version 2.0\n-(defproject snowplow/clojure-collector \"0.6.0\" ;; MUST also bump version in server.xml\n+(defproject snowplow/clojure-collector \"0.6.0b\" ;; MUST also bump version in server.xml\n   :license {:name \"Apache Version 2.0\"\n   :url \"http://www.apache.org/licenses/LICENSE-2.0\"}\n   :description \"A SnowPlow event collector written in Clojure. AWS Elastic Beanstalk compatible.\"\ndiff --git a/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml b/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml\nindex 397923a..89d4a72 100644\n--- a/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml\n+++ b/2-collectors/clojure-collector/war-resources/.ebextensions/server.xml\n@@ -135,10 +135,10 @@\n              Note: The pattern used ensures that the access log format matches that produced by the Cloudfront Collector. (So that the same ETL process can be employed for both collectors.) -->\n         \n- pattern=\"%{yyyy-MM-dd}t&#9;%{HH:mm:ss}t&#9;-&#9;%b&#9;%{X-Forwarded-For}i&#9;%m&#9;%h&#9;%U&#9;%s&#9;%{Referer}i&#9;%{User-Agent}I&#9;%q&amp;cv=clj-0.6.0b-%v&amp;nuid=%{sp}C&#9;-&#9;-&#9;-\" />\n </Host>\n\n\n\n  -\n  \\ No newline at end of file\n  +\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Move back to next milestone.\n. This is much more important now that we have costly enrichments such as API lookups.\n. I think there are two scenarios:\n1. The recent Cascading upgrade does this optimisation for free\n2. The test suite isn't reflecting how a real Hadoop cluster will run this\nOne way of investigating would be to compare job times with and without\nthis .forceToDisk...\n. Fantastic news!\n. What is publicProperty privateProperty - is it a JSON Schema thing?\n. oh being dense. yes that's what I mean!\n. can you add random annotations like pii in JSON S?\n. Interesting alternative!\n. Trouble with doing it in the trackers:\n1. Implementation x10\n2. Cannot safely have an AES key in the trackers, so would have to erase PII rather than transform it. Defeats the point of adding the fields into the JSON Schemas in the first place\n3. Would have to retrieve the JSON Schemas to know which fields to erase\nSo as an alternative, we could consider moving this enrichment forwards into the Kinesis sinks - i.e. both the raw S3 sink and the enrichment app would both apply this scrubber, so no PII ever touches disk.\n. https://github.com/bcgit/bc-java/blob/master/prov/src/test/java/org/bouncycastle/jce/provider/test/AESTest.java\n. I agree that PII scrubbing (or hashing or encryption) in the trackers is preferable, as anything we do in the enrichment process is basically \"too late\" from the raw logs perspective.\nBut I'll keep this open because it would be nice to have this available, especially for applying retrospective scrubbing in the case that something sensitive slipped through a tracker...\n. Renamed so that it follows on from https://github.com/snowplow/snowplow/issues/3472. One of the nice things about this idea is that the pii: true hint would be enough for Iglu when generating Redshift etc tables to make sure these columns are wide enough to take the hashed value.\nIt also just means that the work to identify that e.g. com.acme.email/send_email's email_recipient property is PII is just done in one place (at the time of schema authorship), rather than every user having to configure their own PII Enrichment.. Depends on https://github.com/snowplow/scala-maxmind-geoip/issues/14\n. Fixed in 0.9.6, closing\n. Depends on #863\n. Bumped to 0.2.1 to fix Scala dependency issue\n. Need 0.2.2 to get this working on Hadoop (see https://github.com/snowplow/referer-parser/issues/76)\n. Fixed in 0.9.6, closing\n. If the only incomplete test is #919, then we can de-schedule and close this one.\n. Cool!\n. Not in changelog, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Thanks Fred\n. Thanks Fred!\n. Pushing back\n. Josh is tackling this one...\n. This is done\n. @fblundun can you prioritize this one please\n. Done in 0.9.5, closing\n. Superseded by #3023 . Done in 0.9.5, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Duplicate of #892, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. Fixed in 0.9.6, closing\n. I like option 2 - I think we should assume there are IP addresses out there that MaxMind can't support (but shouldn't die)...\n. Done in 0.9.6, closing\n. Thanks Fred!\n. Looks like @fblundun forgot to do this one\n. Renamed to bumping to 0.4.0 (which is now available)\n. Done in 0.9.6, closing\n. @fblundun - this is quite an important bug to fix\n. Fixed in 0.9.6, closing\n. @fblundun if you could add this one to the CHANGELOG with a TODO, I will todo it.\n. Thanks Fred\n. Actually show 2 examples:\n1. The configuration which is an exact substitute for the behavior in 0.9.5\n2. A maximalist example assuming user has purchased all commercial dbs\n. Looks good!\n. Updated title\n. This is some weird environmental issue - it's an intermittent bug. I've seen it myself. I'm not sure what causes it. Sometimes the contracts across the whole of EmrEtlRunner just don't get checked. I will /cc @egonSchiele but he will probably be as stumped as us.\n. Many thanks for checking up on it @egonSchiele !\n. I believe that the combination of #1498 and #1438 should fix this, closing...\n. This is not required - superceded by #1438\n. This is not required - superceded by #1438\n. Closing this as we are going to move to Spark in time\n. Duplicate of #1947\n. Fixed in 0.9.6, closing\n. Hi @hslee16 - can you raise support issues on the snowplow-user email group?\n. Cheers Alex\n. Solution is here: http://stackoverflow.com/a/14602089/255627\n. Fixed in 0.9.6\n. Hey Rob - yes, we need to extend our truncation code to cover most or all of the fields. This has been low-priority as it's a Postgres-only issue (Redshift does auto-truncate of the fields)...\n. Right - yes that makes a lot of sense. Just as a note - at the moment validation of typed events against their schemas only happens in the shredding step. We will eventually add it into the enrich as well, but in the meantime when you launch a new event type you probably want to be shredding a sample just to make sure that your instances fit your schema...\n. Also added #928 based on use case ^^\n. Thinking about this some more. If a user does --skip enrich, implying that they want to run the shred but not the enrich, then we need to decide where to get the input for the shred step from. Two options would be:\n1. The processing location in S3 - but this is really where raw events are found, not enriched events\n2. The enriched event location in S3 - again a little confusing, as the user probably doesn't want to shred all enriched events, just the run that failed somehow\nWe also need to decide: does --skip enrich imply --skip staging,archive as well?\n@yalisassoon what would be your preferred behaviour for running shredding without enrichment?\n. Let's do this in 0.9.7\n. Sounds good @yalisassoon ! Let's do it that way...\n. Let's add idea of whitelist and blacklist, as may sometimes be easier to specify which ones you don't want to shred (e.g. because they are being fed into your systems monitoring platform instead), rather than the ones you do.\n. Or filter based on app_id - this is driven by the insight that a Snowplow user may have some apps with huge event volumes (e.g. in-game events, ad impressions) that they want to keep out of Redshift, but some apps with manageable event volumes (e.g. ecomm transactions, admin UI usage) that they want to analyze in Redshift.\nGiven these various use cases, it would probably be best to support arbitrary filtering logic (JavaScript based?).. Dupe of #904\n. Duplicate of #1712 \n. Only makes sense with #936 \n. Make sure we do #1199 too\n. This is complex and can be done anyway using an orchestration wrapper (with better monitoring to boot). Pushing back\n. Covered by #1173?\n. Hi @ngocthanhit - my suggestion would be to add some startup code into Scala Hadoop Enrich and Hadoop Shred to clear out those file paths. You should be able to use something like fullyDelete: https://hadoop.apache.org/docs/r1.2.1/api/\n. Yep, please open a PR when you have it working! This code will give you a flavor of how to do one-time setup tasks in our Hadoop jobs:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala#L72\n. Actually - what I suggested won't work... What I suggested will delete precisely the files that you want to process - the ones that S3DistCp just copied into hdfs for this run.\nAlternative suggestions:\n1. Update the hdfs pathing in EmrEtlRunner to include the run ID, or\n2. Add a scripting step after enrich and shred to delete everything from hdfs\nIn fact, with option 1 you would still need 2, because otherwise your HDFS cluster will run out of space. So I would jump straight to step 2.\nResources:\n- http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-hadoop-script.html\n- http://stackoverflow.com/a/16797599/255627\n. You don't need to write any Scala. Instead, you:\n1. Write a Bash script which uses the hadoop hdfs command line tools to empty the HDFS path\n2. Upload the Bash script to a public S3 bucket somewhere\n3. Update the EmrEtlRunner to add a new job step which invokes your Bash script using Amazon's  script-runner.jar step\nAll as per the links I've shared.\n. Sounds good to me! Thanks @ngocthanhit \n. Bringing forwards\n. Note that the presence of some bots (anti-virus, child protection, content blocking) in the data source has becomes more obvious now that we assign an event_id on event creation.\n. Updated ticket name, please update commit etc. @rzats - I think your spec above for the enrichment is excellent, really well done. Just a few points:\n\nAre there any failure states / error handling where we need to document how the enrichment behaves (if not, still add a section making that clear)\nIn your example JSON, wouldn't it make more sense for one of the paths to be HTTP, given that you are supplying the HTTP basic auth keys\nCan you mention the authentication approach used to access the S3 buckets\nCan you put a warning that the database files are commercial and proprietary and are not to be stored publically (e.g. on unprotected HTTPS) anywhere\nCan you clarify what happens if an unprotected HTTPS path is used - do we error? . /cc @BenFradet . This is pretty important actually...\n. Hey @fblundun - I think the only important case is the one you cite - splitting out a >50kb payload_data into multiple smaller payload_datas...\n. We will also need to support an event being a) oversized and b) failing splitting, so necessitating a bad stream...\n. We still need this, it just splits at 1Mb...\n. Done, closing\n. Done, closing as not for CHANGELOG\n. Thanks @gisripa - good catch! Have you signed our CLA yet - https://github.com/snowplow/snowplow/wiki/CLA\n. This is section 4 isn't it: https://github.com/snowplow/snowplow/wiki/4-Loading-shredded-types#4-creating-and-uploading-a-json-paths-file\n. Hi @gisripa - you shouldn't need to upload JSON Path files for the Snowplow-authored JSONs. See this method: https://github.com/snowplow/snowplow/blob/master/4-storage/storage-loader/lib/snowplow-storage-loader/shredded_type.rb#L84\n\nThis method will try to locate the JSON Path file in first your own hosted assets path, and then in the Snowplow one if not found. Are you saying that this code wasn't working for you? What error were you getting? I will rename this ticket...\n. Hi @gisripa - that sounds like a separate issue. I've created a ticket for that here: #958.\nAre you still having the problems specific to this ticket, #952?\n. Okay thanks, I'm going to close this one. If anybody has any issues reading the Snowplow-authored JSON Path files, please flag in this thread and we will re-open.\n. This is a great idea. It's somewhat painful to implement, because it means changing all the hosted JSON Path files without breaking the shredding process for people on 0.9.6. Something to be mindful of.\n. This was implemented in #1977, closing. Thanks @bamos! Scheduling for 0.9.7...\n. Thanks @OAGr ! You are right, those links were broken. Can you sign our CLA if you haven't done so already? https://github.com/snowplow/snowplow/wiki/CLA\n. Many thanks Ozzie - scheduling for 0.9.7...\n. This is done, closing\n. Good catch - thanks @kujo4pmZ !\n. @fblundun has fixed this, closing\n. Good points, agree that fixing downstream is the way to go... Do you want to have a play around and come up with a regexp that you're confident will work? \n. Some thoughts, hope they're helpful:\n- In 0.9.8 (coming soon), the Clojure Collector's regexp will be separated from the CloudFront Collector's. See: https://github.com/snowplow/snowplow/blob/feature/0.9.8/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/loaders/CljTomcatLoader.scala#L38\n- Note that the Clojure Collector actually uses a proper tab-delimited character to separate fields: https://github.com/snowplow/snowplow/blob/feature/0.9.8/2-collectors/clojure-collector/java-servlet/war-resources/.ebextensions/server.xml#L138 This should make it easier to tweak the regexp to distinguish between tabs and \", \"s in the IP\n. Although I agree it would be nice to fix this downstream, in practice a) there's nowhere in the enriched model to put proxy IPs and b) there's an opportunity to fix this now (0.9.9) given that we also need to fix #992.\nSo we will drop any intermediate proxy IP addresses in the access-valve.\nIn other words, if the access-valve sees:\n%{X-Forwarded-For}i=64.60.1.2, 54.239.3.4\nthen it will only record in the log file:\n64.60.1.2\n. Note: from this ticket: https://github.com/cloudfoundry/java-buildpack/issues/75\nI believe that we can fix this without changing the access-valve, but instead updating the server.xml value from:\n%{X-Forwarded-For}i\nto:\n%{org.apache.catalina.AccessLog.RemoteAddr}r\nTesting now...\n. Hey Grant @gkushida - I think I have a solution for this. Can you test it out in a dev env?\nIt's here:\nhttp://d2io1hx8u877l0.cloudfront.net/2-collectors/clojure-collector/clojure-collector-0.8.0-standalone.war\n. Super-thanks for the detailed testing @gkushida , it's very much appreciated. Part of the pain here is that there's an additional dimension to the testing - which is whether or not the Beanstalk cluster is inside a VPC; that changes a lot of things.\nLet me digest this and come back to you...\n. Hi @gkushida - okay I have had another attempt at this. This is a pretty maximalist implementation: https://github.com/snowplow/snowplow/commit/bde20ff617a5e6cec93862f81f8fe19d0027097d#diff-575d7f188e15b831d5275012c7cea71cR254\nI have confirmed this is working okay in a non-VPC, 2-instance Beanstalk. Would you be able to check this too? The updated file is here:\nhttps://snowplow-hosted-assets.s3.amazonaws.com/2-collectors/clojure-collector/clojure-collector-0.8.0-standalone.war\n. Okay great, thanks @mortenstarfly. Let's see what Grant says!\n. Great news!\n. There is some merit in the kitchen sink approach :-)\n. Hi @mortenstarfly - okay, we're scheduling this for 0.9.10: #655, #656\n. Hey @mortenstarfly - thanks for raising. I wonder what could be causing those ones... Feel free to open a new ticket!\n. Awesome work! Thanks Sam\n. Thanks Nic, we will fix those\n. Re-assigning to @ihortom to check if this is still live. Thanks. Hey Greg! Thanks for sharing. What would be the use case for writing the enriched events out to gzipped files in /tmp?\n. Those are great use cases - but the plan isn't to add N different monopolistic sink options into the scala-kinesis-enrich. Instead, sinks are being written as dedicated Kinesis apps which can consume the enriched event stream.\nFor examples:\n- https://github.com/snowplow/snowplow/tree/feature/kinesis-sinks/4-storage/kinesis-s3-sink\n- https://github.com/snowplow/snowplow/tree/feature/kinesis-sinks/4-storage/kinesis-redshift-sink\nSome other possibilities (not yet written):\n- kinesis-hdfs-sink\n- kinesis-elasticsearch-sink\n- kinesis-vertica-sink\nMain advantages of this approach:\n- Support N sinks in parallel: a given user can run the S3 sink, the Redshift sink, the ElasticSearch sink, the Vertica sink all at the same time, all reading off the same enriched event stream. With your PR, if you wanted to sink to gzipped /tmp and Redshift, you wouldn't be able to\n- Avoid code duplication: maintain sink code in one place, avoid duplicating sink code across lots of different Kinesis apps\nSo under this approach, you would write a kinesis-logfile-sink based on kinesis-s3-sink but taking enriched events from the stream and writing them to local disk rather than S3... Or better still, create a kinesis-hdfs-sink, which could put the events direct onto HDFS and so avoid the need to introduce a logfile collection technology like Scribe or Flume or Fluentd...\nDoes that make sense Gregory?\n. Hey Gregory, have you considered Kafka instead of Redis? We are planning to add Kafka support to Snowplow alongside Kinesis for users who don't want to run Snowplow inside the AWS cloud.\nKinesis is extremely close to Kafka conceptually - in fact you can think of Kinesis as Amazon's hosted version of Kafka. Like Kinesis, Kafka has:\n- Clustering support out of the box\n- A persistent architecture, reducing the chance of losing events\n- A sequential, append-only architecture, making it easy to have multiple apps consuming the same stream (and different apps can maintain separate positions in the stream)\nWhat we have sketched out for Kafka would look like this:\nHTTP --> scala-stream-collector -> Kafka raw event stream -> scala-enrich-kafka -> kafka enriched event stream -> Kafka HDFS sink\nFor the Kafka HDFS sink, you could likely reuse https://github.com/linkedin/camus\nFor scala-enrich-kafka, we would likely write a Samza application which looked conceptually pretty similar to scala-enrich-kinesis: http://samza.incubator.apache.org/\nAdding Kafka support to the Scala stream collector would be the first job - it shouldn't be hugely difficult.\nWe haven't written any Kafka code for Snowplow yet, although I've been working with Kafka quite a lot in my new book: http://manning.com/dean/\nLet me know if a Kafka-based flow would be interesting to you. It's definitely some new technologies to learn but the plus is that we plan on supporting Kafka as a first-class architecture in Snowplow, and we would love to get you contributing!\n. > however I'm new to scala, java ecosystem and Kafka. This PR is my first scala program ... So I'm scared about doing something in the wrong way\nHi Gregory - don't worry at all, we're very happy to support you in all this.\n\nI have a question, why use raw events in the kafka stream and not thrift events ?\n\nSorry for confusion - I meant Thrift events. So the Scala stream collector would write out Thrift raw events as per the existing schema, into a Kafka topic.\n\nDoes it make sense to create a new branch ?\n\nDone! https://github.com/snowplow/snowplow/tree/feature/kafka\n. Hi Gregory - it's a very good question! You are right - functionally, scala-kafka-enrich and scala-kinesis-enrich would be extremely similar. scala-kinesis-enrich does leverage the Kinesis Client Library to handle stream processing, but this usage is limited to the Kinesis sink and source - i.e. you can run scala-kinesis-enrich without touching Kinesis-specific code.\nSo I think you are right - it would be better to add a Kafka source and sink to scala-kinesis-enrich. If that works well, we can potentially rename that project to scala-stream-enrich to make its scope clearer.\nIf we need more sophisticated stream processing capabilities in scala-stream-enrich later, we can always add Samza to it.\nBTW for a simple Java app I wrote reading from one Kafka topic and writing to another, check out: https://github.com/alexanderdean/Unified-Log-Processing/tree/master/ch03/3.5/weatherenrich\n. Great stuff - let's keep the loosely coupled architecture and add the HDFS sink as a separate app, kafka-hdfs-sink. Especially because users will want to sink both a) raw events and b) enriched events to HDFS.\nDo we definitely need to create kafka-hdfs-sink? What about just running Camus: https://github.com/linkedin/camus\n. Yep, it will look something like this:\n-> Storm\nHTTP -> ssc -> Kafka raw -> scala-stream-enrich -> kafka enriched -> Camus -> HDFS\n                                                                  -> Spark Streaming\n                                                                  -> Secor -> S3\nSecor: https://github.com/pinterest/secor\n. > So I have to add a kafka sink which will be sourced by camus, right ?\n@gregorg remember that one kafka topic (aka stream) can be read by many different apps.\n. AWESOME!\n. Looks great! Merging now...\n. Hero! Merging now\n. Coolio, merging\n. Internet Explorer lives on in Spray HTTP! Thanks for raising Yuval.\n. Workaround:\n``` json\nspray.can.server {                                                                                                                                                                                                                                                          \n  # To obtain the hostname in the collector, the 'remote-address' header                                                                                                                                                                                                    \n  # should be set. By default, this is disabled, and enabling it                                                                                                                                                                                                            \n  # adds the 'Remote-Address' header to every request automatically.                                                                                                                                                                                                        \n  remote-address-header = on                                                                                                                                                                                                                                                  \nuri-parsing-mode = relaxed                                                                                                                                                                                                                                                \n  raw-request-uri-header = on                                                                                                                                                  \n  # Adding spray server config. to redefine max request length (default = 2048)                                                                                          \n  parsing {                                                                                                                                                                                                                                                                  \n    max-uri-length =4096                                                                                                                                                                                                                                                    \n  }                                                                                                                                                                                                                                                                          \n} \n```\n. Awesome, please bump the config up to something like 16384\n. @fblundun - any reason why there isn't a commit linked to this ticket? Probably just an errant commit message...\n. This has been superceded, closing\n. Note: this isn't actually urgent, as the mistake was overwriting the %h field, which is not used in the Snowplow Enrichment process. Still not ideal but does not need an rush fix.\n. Proxy IPs are found in the headers, which we capture already. Closing\n. Yep please\n. This is awesome work @ilyakava - thank you! @fblundun is about two weeks away from focusing full time on the Kinesis flow, so hopefully we can start to feed these PRs back into core very soon...\n. @ilyakava have you signed the CLA yet - https://github.com/snowplow/snowplow/wiki/CLA ?\n. Confirmed - thanks Ilya!\n. GitHub auto-closed this PR - the code now lives in here: https://github.com/snowplow/snowplow/tree/kinesis-redshift-sink\n. Duplicate of #2072\n. Makes sense!\n. 2.4.8 is a dependency on jobflows with >256 steps\n. Updated ticket as this has been found to occur on 2.4.3, 2.4.5, 2.4.8. I've also created this ticket:\nhttps://github.com/snowplow/iglu-scala-client/issues/24\nMy current thinking is that something in 2.4.3 updated the Hadoop classpath resource loading code, which is causing this code to break:\nhttps://github.com/fge/jackson-coreutils/blob/master/src/main/java/com/github/fge/jackson/JsonLoader.java#L74\n. So 2.4.3 promotes to Java version to 7u60 (early access release):\n- http://www.oracle.com/technetwork/java/javase/2col/7u60-bugfixes-2202029.html\n. Here is the line that is throwing the exception:\n- https://github.com/snowplow/iglu-scala-client/blob/master/src/main/scala/com.snowplowanalytics.iglu/client/repositories/EmbeddedRepositoryRef.scala#L147\n. I have raised a support ticket with AWS to try and get more insights into what changed between 2.4.2 and 2.4.3...\n. This issue is being investigated by AWS now...\n. #1652 is the hopeful fix for this.\n. Update: #1652 wasn't the fix for this. The good news is that updating our code to Hadoop 2 seems to work - so we can bump the AMI version to 3.6, which does work!\n. Closing as a won't fix - there are just too many bugs in too many aspects of the 2.4.x series AMIs to make this achievable. See #1651 instead.\n. When you get to this milestone, please break this ticket out accordingly. Redshift, Postgres, Scala Common Enrich, Scala Hadoop Enrich tests...\n. Fixed by Fred, thanks. De-scheduling from 0.9.9\n. Hi Pawel, thanks for raising.\nI just looked in to this - according to the UUID spec, a to f should be lowercase:\nhttp://www.ietf.org/rfc/rfc4122.txt\nTherefore this is a bug in the iOS library. Closing this ticket and opening one over here:\nhttps://github.com/snowplow/snowplow-ios-tracker/issues/71\nWe'll get this fixed very soon, thanks a lot for raising.\n. From a closer reading of the RFC, actually upper-case characters in UUIDs are valid on read, they are just invalid on write.\nIn other words: the write implementation in the ios-tracker is wrong, but equally the read/write implementation in snowplow/snowplow is wrong. So I'm reopening this ticket and scheduling.\n. Hi @lekki, the updated iOS Tracker 0.1.2 is already available. This fix will go out in 0.9.9, probably next (this) week.\n. Duplicate of #1800\n. Duplicate of #1801\n. Duplicate of #1802\n. @jbeemster please create companion tickets for SQL/JSON Paths/JSON Schema in appropritate projects.\n. Here is the doco: https://www.olark.com/help/webhooks\nThe two interaction types are so similar (offline vs online) that I would suggest one event type...\n. Pushing back\n. Bringing forwards - we are going to go for 5 webhooks in this release...\n. Update: 4 webhooks\n. Please do this one before Marketo\n. Hmm... Interesting! We'll put this in the first Kinesis release, which Fred will be working on from about a week's time...\n. Nice! Keep us posted...\n. Thanks for all this digging Ilya!\n. Okay great - sorry this took up your time. Things will get easier with the Kinesis flow once @fblundun  starts work on this next week; bringing the collector and kinesis-enrich up-to-date is his first priority.\n. This is done, closing.\n. For reference see: https://bitbucket.org/abcdevelop/redback\n. Right, this code is correct:\nruby\n        unless @args[:skip].include?('staging')\n          unless S3Tasks.stage_logs_for_emr(@args, @config)\n            logger.info \"No Snowplow logs to process since last run, exiting\"\n            exit 0\n          end\n        end\nThe problem is this wrapper:\nruby\nrescue SystemExit => e\n  exit 1\nThis should respect the parent exit code thus:\nruby\nrescue SystemExit => e\n  exit e.status\n. Oh sorry Fred - I neglected to mention that we have an algo for snake_casing the Redshift table names from the JSON Schemas. See for an example:\nhttps://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/org.schema/web_page_1.sql\nDoes that make sense?\n. Exactly - many thanks!\n. Thanks Fred, good idea making this a dedicated ticket\n. Hey @michaelpryor - good spot, thanks for raising. @fblundun can you fix?\n. Based on incorrect understanding of AD-X Tracking's webhooks, closing\n. This was based on a misunderstanding of how AD-X Tracking works, closing.\n. Hey Dani,\nThe URI in the log line isn't correctly encoded - it's only once encoded. The URI should be twice encoded - once by the website which should generate valid URIs, and once by the CloudFront logging itself. So the error message is correct: once you strip off CloudFront's encoding, you find a URI with raw pipes, which in Snowplow's admittedly rather-too-fussy implementation, is invalid.\nCheers,\nAlex \n. It is double-encoded? See:\nrefr=http%253A%252F%252Fwww -> refr=http%3A%2F%2Fwww -> refr=http://www\n. Thanks @michaelpryor - @fblundun can you take a look?\n. Assigning to Fred, let's see how this plays with his Kinesis release\n. Remember @jbeemster - one commit per ticket, and commit message must auto-close the ticket on merge to master.\n. This was based on a misunderstanding of how AD-X Tracking works, closing.\n. Renaming and adding back in\n. For launch, GET only\n. This was done, closing.\n. Ace, thanks\n. @yalisassoon - actually this isn't a regression, it's just rare because normally the trackers don't let empty fields to be sent through. I've changed the name of this, and the name of the prior ticket (#570), and both will be fixed in 0.9.9\n. This one is done too, commit message missed \"closes\".\n. For reference on how to do this: https://github.com/snowplow/snowplow/commit/475ea12a3dcd7cdb73b177e6f57799191cd60c77\n. Actually we can make this generic - in the campaign_attribution config, set the querystring params you want to check for the click id. Suggested defaults: gclid, mscklid\nhttp://advertise.bingads.microsoft.com/en-us/dom\n. Yep - so you could have a list of click id property names to look up against...\n. Sorry typo, the MS one is: msclkid. http://advertise.bingads.microsoft.com/en-us/blogpost/6276/bing-ads-blog/introducing-adcenter-clickids\n. DoubleClick click ID is dclid: https://plus.google.com/107594353522876672934/posts/EbWDKtXDyFL\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Fred can you take a look?\n. Moved to https://github.com/snowplow/snowplow-javascript-tracker/issues/273\n. Please add tickets related to the Snowplow JavaScript Tracker to that project!\n. On second thought - given this is a question not an issue, please email it to the snowplow-user Google Group.\n. Hi @Kimblebrook - oh sorry yes, typo!\n. Thanks for raising! Scheduling\n. Closing, superceded by #1282\n. TBC what path should be used for redirects. Maybe something like\ncom.snowplowanalytics.snowplow/uri_redirect\n. Or something shorter given this is going to be embedded in URIs.\n. s/r\n. The redirect will speak tp2 so that needs to be flagged - how about:\nr/tp2\n. Thanks\n. Superceded, closing\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. My theory is that the VPC proxying has one fewer layers of URI encoding on the useragent than the alternate approaches, and this somehow spoils the browser version without affecting the other browser fields. That's the only thing I can think of...\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Nice - can go in Kinesis 1 or Kinesis 2, your call!\n. Moving this into 0.9.12 as it's nearly good to go\n. Moving this into 0.9.12 as it's nearly good to go\n. Closing given #2548. All file moves are moving to S3DistCp, closing. Good thinking, this will make things more robust.\n. Moving back to 0.9.9...\n. This is in production now in 0.9.9 and seems to be working a treat:\n```\n, [2014-10-27T14:07:53.781000 #12183] DEBUG -- : Initializing EMR jobflow\nF, [2014-10-27T14:07:58.505000 #12183] FATAL -- :\nSnowplow::EmrEtlRunner::DirectoryNotEmptyError (Cannot safely add shredding step to jobflow, s3n://xxx/shredded/good/ is not empty):\n    /opt/snowplow-0.9.9/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:250:in initialize'\n    org/jruby/RubyMethod.java:140:incall'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/contracts.rb:230:in call_with'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/decorators.rb:157:ininitialize'\n    /opt/snowplow-0.9.9/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:59:in run'\n    org/jruby/RubyMethod.java:124:incall'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/contracts.rb:230:in call_with'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/decorators.rb:157:inrun'\n    file:/opt/snowplow-0.9.9/emr-etl-runner!/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in (root)'\n    org/jruby/RubyKernel.java:1081:inload'\n    file:/opt/snowplow-0.9.9/emr-etl-runner!/META-INF/main.rb:1:in (root)'\n    org/jruby/RubyKernel.java:1065:inrequire'\n    file:/opt/snowplow-0.9.9/emr-etl-runner!/META-INF/main.rb:1:in (root)'\n    /tmp/jruby8025908767391820260extract/jruby-stdlib-complete-1.7.16.jar!/META-INF/jruby.home/lib/ruby/shared/rubygems/core_ext/kernel_require.rb:1:in(root)'\n``\n. Thanks Fred\n. Good spot!\n. No worries...\n. Moving to a release requiring other Clojure Collector changes\n. Bringing forward so that MailChimp webhooks work (MailChimp does a GET check before sending the POSTs).\n. Pushing back\n. Have to make sure this doesn't break the enrichment process...\n. Pushing back as we're not touching Shred in 0.9.10 or 0.9.11\n. @yalisassoon is checking to see if the JSON output of the shredder contains \"\" or null...\n. Hmm - this seems to be some kind of regression around #942. We need to isolate if this is a bug in the RedshiftCOPYcommand as it looks like StorageLoader is doing its job (not puttingEMPTYASNULL` on the JSON copies).\n. Okay this seems to be a bug in Redshift. I will file a ticket to get this investigated once I can reproduce the issue.\n. That's a great idea Ryan - thanks for raising...\n. A couple of tips:\n1. Upgrade just to R64 as that's the first one with IAM role support\n2. You'll have to upgrade to Hadoop Enrich 0.14.1 for compatibility with the latest EmrEtlRunner\n3. You'll have to upgrade from Redshift 0.3.0 straight through to 0.6.0\nHope that helps\n. Great to hear it! Thanks for taking the time to document this Eric.\nAlex\nOn 30 Jun 2015 11:15 pm, \"Eric Zimmerman\" notifications@github.com wrote:\n\nOk...upgraded and running normally again.\nCouple notes on Migrating from 0.9.2 --> r64\n1. Update to r64 codebase\n2. Migrate Postgres from 2-->3-->4-->5 with scripts in storageloader s\n3. Update cloudfront collector pixel.\n4. Update config file based on the link I added above to Alex's\n   translations of old config\n5. Add --skip shred since I am using postgres and it doesn't seem to\n   do anything with the emr processed shred files.\n6. Change out all buckets to remove any periods in their names.\n7. Success :-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1136#issuecomment-117360244.\n. Pushed: https://github.com/snowplow/snowplow/wiki/Upgrading-Steps\n\nThis is great - keep it coming @ihortom !\n. I wouldn't go past v0.9.0 - diminishing returns\n. Thanks - pushed!\n. Thanks, I've repushed your local copy\n. Great job, closing\n. Not sure what this was meant to be, closing\n. Hey @dominickendrick ! Thanks for this. We actually have a Kinesis 0.2.0 coming out in a few days with this fix and lots of others. Not sure if the Scalazon bump is in the new release - assigning to Fred to take a look...\n. http://discourse.snowplowanalytics.com/t/authentication-authorization-of-tracker-event-streams/572\n. Another idea: in the case that an end user has to login to a website (i.e. authenticate using JTW or similar), could we pass that authentication information with their events and thus verify that they are who they say they are? This would prevent spoofed events from logged in users...\n. November 2014! Where does the time go... Closing.\n. November 2014! Where does the time go... Closing.\n. Closing as this join is fairly inexpensive in Redshift and in Hadoop we need a more comprehensive solution (as there are lots of other fields as important as app_id that it would be nice to embed to avoid a join).. @fblundun can you \"get ahead of this issue\" in the Elasticsearch Sink by using the correct name already...\n. Pushing back to webhooks 3 as haven't had a chance to test\n. This is done, the ticket was just missing\n. Pushing back as dependent on #1149\n. Migrated to https://github.com/snowplow/iglu-central/issues/524. To clarify: the problem here is that if we upgrade the JSON Path file and somebody doesn't upgrade their link_click table, then their load process will crash because of the mismatch in number of fields.\n. Migrated to https://github.com/snowplow/iglu-central/issues/521. Depends on #1152 \n. Moving forwards because this issue is a real pill\n. We have to update EmrEtlRunner in this release anyway...\n. Hey @kazjote - please let us know:\n1. What processing tool you want to use with the enriched events in S3 (Hadoop / Spark / Hive / something else)\n2. What format you would like the enriched events stored in (sequence files? parquet?)\n3. What folder structure you would like the enriched events partitioned into\n. I think for starters we just follow the Secor project and write new line delimited raw text files for enriched events. This will let them be picked up by Spark and Spark Streaming.\n. This is probably a dupe, please close if so...\n. @jbeemster can you double check all the JSON Schemas for missing fields which MailChimp might send through... \n. Guess they must do - let's try to catch any other ones on Monday...\n. Closing this, this has been fixed through:\n- https://github.com/snowplow/iglu-central/commit/7816d0c5482d2423dc306a21b1ec6d79635bf70d\n- https://github.com/snowplow/snowplow/commit/8241c2f8cefe8fa0261edee62d21015570fc7482\n. @jbeemster please create companion tickets for SQL/JSON Paths/JSON Schema in appropritate projects.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. Done in 0.9.11, closing. Please make sure to have a commit message which will auto-close each ticket in future.\n. @jbeemster please create companion tickets for SQL/JSON Paths/JSON Schema in appropritate projects.\n. Let's push this back until it's supported by Elasticity too.\n. Hey @fblundun - this one isn't working I'm afraid. What is happening is:\n- The tag field names are being converted into symbols\n- The symbols are then failing the Contract check\nExpected:\n:monitoring=>{:tags=>#<Contracts::HashOf:0x32442dd0 @value=String, @key=String>\nGot:\n:monitoring=>{:tags=>{:client=>\"snowplow\", :job=>\"snplow2\"}\n. Thanks Fred!\n. @jbeemster please create companion tickets for SQL/JSON Paths/JSON Schema in appropritate projects.\n. Pushing back to webhooks 3 as haven't had a chance to test\n. Oh cool thanks Josh - was weighing this up right this moment! That makes things much easier for the 0.9.14 release...\n. Lol no worries\n. Thanks\n. Good catch\n. Closing this, superceded\n. Good suggestion. Is this a \"fail-fast\" scenario or would we be collecting all failing indexes (preferable)?\n. Awesome - can we move up into Webhooks 2 (i.e. do it in the MandrillAdapter for launch)?\n. Okay cool - you can close this ticket when done, no need for this to go into CHANGELOG etc\n. done = implemented\n. Thanks for this! Leave it with me, we will add a version of this into Snowplow...\n. Hey @fblundun - I'd like us to add a modified version of this PR into your 0.9.12 as it's a super-useful feature for people on the Hadoop flow (in fact it justifies the release for them, really).\nMy review notes:\n1. I'd like us to remove the two-pass aspect of the process. I can't see why we wouldn't go straight to using the NetAPorter URI (let's call it NAPURI) parsing\n2. Round tripping from NAPURI to Java URI can be done using NAPURI's toURI method, doesn't have to go via a String\n3. Would be great to have more tests. I'd like to be much clearer on just how permissive NAPURI is. Does it support spaces, pipes, forward slashes in querystrings?\nMany thanks!\nAlex \n. Hi @fblundun - good spot. %2B is the code for a +, which isn't what we want... Any idea why NAPURI does this, and if it can be switched off? It's not a reasonable assumption for NAPURI to make\n. In this case - I'm happy to stick with the two-pass approach...\n. This is really cool! Thanks for contributing this guys.\n. WARNING Before merging, remember that the contents of HDFS [1] [2] is not cleared out between \"runs\" in a persistent cluster, so you could easily end up reprocessing the same data...\n[1] hdfs:///local/snowplow/enriched-events/\n[2] hdfs:///local/snowplow/raw-events/\n. Moving to Persistent EMR cluster milestone\n. This is still a neat idea @rupeshmane but we are gradually moving to Dataflow Runner, which already supports targeting an existing EMR cluster. So this is a won't merge.. Oh drat. Thanks Yali. @jbeemster please fix these in a branch, then we'll figure out if we have to do a whole release to fix this... Always ask if you need something for testing (like a Redshift login)...\n. This will go into webhooks 2 release...\n. @jbeemster - please work with @chuwy to get this feature into Anton's release branch!\n. Closing, won't fix.. Nope it's fine to leave out\n. This was done\n. Closing as MailChimp's envelope contains \"data\".\n. Hey Fred - I was thinking about this - you are exactly right, we need to come up with a definitive deterministic approach to this.\nIt's also not helped by the fact that the code that does this is split across two langs currently - Scala and Ruby (StorageLoader).\nI like your proposed update to enable \"abcDEF\" -> \"abc_def\" too. \n. This isn't going to happen\n. As a workaround till this is done, http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-timestamp-field.html#mapping-timestamp-field-format\n. /cc @yalisassoon \n. Great idea!\n. Wow! Awesome work Fred.\n. This is superceded by the other PR right?\n. Great, closing\n. Pushing back to webhooks 3 as haven't had a chance to test\n. Pushing back to webhooks 3 as haven't had a chance to test\n. Note that when this is done, an upgrading user must switch to a new index in ES for their new-style bad rows, because https://github.com/snowplow/snowplow/issues/824#issuecomment-138937853\n/cc @fblundun @jbeemster \n. Makes sense...\n. Note that when this is done, an upgrading user must switch to a new index in ES for their new-style bad rows, because https://github.com/snowplow/snowplow/issues/824#issuecomment-138937853\n/cc @fblundun @jbeemster \n. Also will need a ticket to update Hadoop Enrich's bad job tests\n. And a ticket to write out the update bad rows in Hadoop Enrich's Scalding job\n. Closing\n. Confirmed! Thanks\n. Awesome - we were thinking of doing something like this! Have you signed our CLA: https://github.com/snowplow/snowplow/wiki/CLA\n. Thanks Fred! Good sleuthing\n. This is one of the most painful (albeit rare) failure states we have currently...\n. Completely agree!\n. This is done, closing\n. Thanks, yes I saw that 1.15 issue too recently, guess it's still not published\n. Naming convention TBC\n. Good idea\n. /cc @fblundun \n. Blocked by https://github.com/cloudify/scalazon/pull/14\n. Was originally EmrEtlRunner: allow config.yml to be passed in on stdin. But this is impractical because e.g. the enrichments cannot be passed in on stdin too.\n. Was originally StorageLoader: allow config.yml to be passed in on stdin. Updated based on change of approach in #1228\n. @fblundun - can you update a Hadoop test to check this change worked?\n. Depends on #758 \n. Duplicate of #275 \n. Dupe of #275 \n. Hmm - I think let's leave the sharding as-is...\n. Pushing back...\n. I've been thinking about this and I think I have a better way of doing this, closing...\n. @fblundun - I don't think there's anything further to check here, do you?\n. Cool, closing\n. Yes - good point Fred!\n. Good point!\n. Serf event will have this name: user:enrich:shutdown\n. The serf event should contain a reason, e.g. \"maxmind reload\".\n. We should do this outside of the application using a Consul watch which invokes a kill via daemontools\n. Moving back\n. Lower priority than the other heartbeats because we get a lot of this for free via the ELB\n. Thanks Denis! Can you guys sign our CLA please: https://github.com/snowplow/snowplow/wiki/CLA\n. @fblundun - please go with @pkallos' suggestion. From a code-merge perspective, please note that @pkallos has signed the CLA, @denismo hasn't.\n. Thanks, yep we have a few screenshots that are out of date.\n. We are planning a re-write of the StorageLoader into Scala which should make that component much easier to package (because we can assemble it). We can already successfully package the EmrEtlRunner using JRuby - made available (experimentally; undocumented) here: https://bintray.com/snowplow/snowplow-generic/snowplow/0.9.14/view/files\nOnce the StorageLoader is ported over to Scala and assembled, a Bintray zipfile will become the standard way of downloading and installing the EmrEtlRunner and StorageLoader. Going beyond this into creating OS packages is a nice-to-have - the priority is to simplify the runtime artifacts...\n. Something we just need to be aware of: any OS-level packaging would still need to make it possible to deploy and run multiple versions of Snowplow on the same box (important for testing and rollback scenarios).\n. It's totally feasible to run multiple Snowplow pipelines via one box - we do this at Snowplow for the Managed Service. What is the issue you are seeing?\n. Closing this - deployment from Bintray is fine, and makes it much easier to run multiple different versions of Snowplow CLI alongside each other.\n. This is a cool idea - pull request most welcome!\n. Sister ticket: https://github.com/snowplow/sluice/issues/31\n. I'm pretty sure we will need to update Sluice to support IAM roles too...\n. At this point, any dev cycles we could put into improving EmrEtlRunner, we could instead put into the migration to Dataflow Runner, so most likely this won't happen. But added: https://github.com/snowplow/dataflow-runner/issues/34. Looks great! @danisola can you get yourself added to SB's CLA response please (currently only Pvdb and Kimblebrook listed) Many thanks!\n. Note for self: let's add a .upcase in get_compression_format\n. Confirmed! Your PR is scheduled for Monitoring 1, which despite the boring name has some quite cool stuff in it, all related to EmrEtlRunner and StorageLoader:\nhttps://github.com/snowplow/snowplow/milestones/Monitoring%201\n. Looks good!\n. Scheduling for Core Refresh...\n. This has been merged, closing\n. @yalisassoon - can you confirm your 6 commits in this branch:\nhttps://github.com/snowplow/snowplow/commits/feature/sql_data_models\nrelate to this (and only this) ticket?\n. Thanks @MrCurtis ! Squashed down to one commit and added as a branch here: https://github.com/snowplow/snowplow/tree/feature/kinesis-bigquery-sink\n. Love it! Possible idea: instead of hanging, we have state=retrying, consecutive_retries=6 - so that the healthcheck itself can decide # retries == hanging...\n. Bringing forwards...\n. Completely agree! This ticket was a rename from a Serf/Consul \"what is the current state\" ticket? The rename was confusing, I should have just closed the ticket...\n. I'm hugely excited about this...\n. Depends on https://github.com/snowplow/iglu-central/issues/106\n. Depends on https://github.com/snowplow/iglu-central/issues/106\n. Thanks for spotting this Fred!\n. @Fred - please update the script to include your new Kinesis job...\n. And @fblundun please merge this branch into your gestalt branch...\n. This is business-level data - it would be great to have it sitting in DynamoDB as it makes it super-easy to update without re-deploying servers...\n. This would actually be really useful for the Hadoop flow too, because there are issues in the Hadoop flow with trying to pass too much configuration data as Hadoop command-line arguments.\nSo we should probably be thinking of this as a Scala Common Enrich thing with hooks into Scala Kinesis Enrich and Scala Hadoop Enrich.\n. Bringing forwads. The Scala DynamoDB library is ready to go now: https://github.com/piotrga/async-dynamo\n. @fblundun - I have updated the desc as per our chat - over to you!\n. Hey Fred - when you say all items, do you mean all items or all keys?\n. Okay - I can't see a way around this without restricting all configurations to total less than 400kb. Let's go ahead with your approach. The table will be called something like snowplow-configuration and thus won't contain a lot of extraneous items.\n. ZK adds complexity - not sure if we will ever do this\n. ZK adds complexity - not sure if we will ever do this\n. ZK adds complexity - not sure if we will ever do this\n. Migrated to https://github.com/snowplow/kinesis-s3/issues/80. FIxing milestone...\n. We need to rethink this when we have more clarity on the persistent cluster option.\n. A workaround until this is implemented would be to run the StorageLoader with a --skip load argument, so it just performs the archiving.\n. This is a bit Homer's Car, closing.\n. Duplicate of #935 \n. Nice idea!\n. Adding this back in causes some weird contracts issue I don't have time to look at now:\nActual: :silencer\n    Value guarded in: Module::mattr_reader\n    With Contract: Hash => NilClass\n    At: /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/core_ext/module/attribute_accessors.rb:53\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/contracts-0.4/lib/contracts.rb:214:in `block in call_with'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/contracts-0.4/lib/contracts.rb:209:in `times'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/contracts-0.4/lib/contracts.rb:209:in `call_with'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/contracts-0.4/lib/decorators.rb:157:in `mattr_reader'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/core_ext/module/attribute_accessors.rb:208:in `mattr_accessor'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/logger_silence.rb:7:in `block in <module:LoggerSilence>'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/concern.rb:120:in `class_eval'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/concern.rb:120:in `append_features'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/logger.rb:7:in `include'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/logger.rb:7:in `<class:Logger>'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/logger.rb:6:in `<module:ActiveSupport>'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/logger.rb:5:in `<top (required)>'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support.rb:27:in `require'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support.rb:27:in `<top (required)>'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/all.rb:1:in `require'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/activesupport-4.1.8/lib/active_support/all.rb:1:in `<top (required)>'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/time_diff-0.3.0/lib/time_diff.rb:2:in `require'\n    from /home/vagrant/.rvm/gems/ruby-1.9.3-p551/gems/time_diff-0.3.0/lib/time_diff.rb:2:in `<top (required)>'\n    from /vagrant/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:25:in `require'\n    from /vagrant/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:25:in `<top (required)>'\n    from /vagrant/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner.rb:23:in `require_relative'\n    from /vagrant/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner.rb:23:in `<top (required)>'\n    from bin/snowplow-emr-etl-runner:19:in `require'\n    from bin/snowplow-emr-etl-runner:19:in `<main>'\n. Pushing back - @fblundun please remove from Bee Hummingbird CHANGELOG...\n. Thanks Fred - do you have enough ---------s in the header for this release...\n. Duplicate of #1346\n. This isn't a great approach - it's better to double-down on the DynamoDB approach as that is portable to Kinesis...\n. This isn't a great approach - it's better to double-down on the DynamoDB approach as that is portable to Kinesis...\n. Ping @alexanderdean before starting on this one as I have some code to help with this...\n. This is done, closing.\n. Bringing forwards...\n. Yep, specifically that... Related to recent snowplow-user thread: https://groups.google.com/forum/#!topic/snowplow-user/4Di915aRLI4\n. Hey @natilivni - how is this: http://astracker.snplow.com/crossdomain.xml\nLet me know if that's correct or needs tweaking...\n. No problem @natilivni - how about now? http://astracker.snplow.com/crossdomain.xml\n. Hmm - not really. If the response == the Clojure Collector's response, then that's good enough I think...\n. Looks good to me! Thanks for checking Fred\n. As in to stdout/err? Sounds fair...\n. This is done, closing\n. Hey @AALEKH - please change the commit message to be: \nScala Common Enrich: converting transactions from given currency to \"home currency\" (closes #370)\n. Closing this PR as @fblundun has merged all this into his branch. Thanks @AALEKH !\n. Thanks for raising @mlively - we've seen intermittent problems both with this and with the attempted fix. We'll schedule this back into the next release.\n. Superceded by #1310, closing.\n. No let's leave it in existing ones, but make a mental note to change going forwards...\n. Great thinking!\n. This is done\n. I think this may be related to not setting the encoding configuration option in the server.xml:\n\nCharacter set used to write the log file. An empty string means to use the system default character set. Default value: use the system default character set. \n\nhttp://tomcat.apache.org/tomcat-8.0-doc/config/valve.html\n. I think for GET we need this too: http://stackoverflow.com/questions/138948/how-to-get-utf-8-working-in-java-webapps\n. Update: the character encoding issue turned out to be in Scala Common Enrich (https://github.com/snowplow/snowplow/issues/1403). However, these updates to \"lock\" the Clojure Collector's encodings to UTF-8 can't hurt, so suggest we keep them in.\n. Thanks @sr-ix - have assigned the ticket. As a workaround for now, just add in a dummy kinesis section...\n. This was fixed in snowplow-stream-collector-0.2.0: https://github.com/snowplow/snowplow/blob/master/CHANGELOG#L78\n. Oh sorry, that's fixed now! From the next release onwards all the Kinesis apps will be bundled in a single zipfile so should be a bit easier to get started. Preview is here:\nhttps://bintray.com/snowplow/snowplow-generic/snowplow/r60-bee-hummingbird/view?sort=&order=#files\n. This isn't needed - once we take a SSC instance out of the ELB pool, it will quickly run out of events to sink, and then it can be shutdown gracefully anyway.\n. This probably adds too much magic to the process\n. Hey @duncan! This looks cool, thanks for the contrib. Have you signed our CLA - https://github.com/snowplow/snowplow/wiki/CLA\nThis will get scheduled into Kinesis #5 which will add a lot of bells and whistles to the Kinesis Enrich... \n. Hey @pkallos - how come you didn't need this already? I thought you were running behind ELB...\n. Cheers Duncan. No plans currently to pull anything other than the trackers out into their own repos - it would be just an unmanageable number of repos and git submodules (we are adding new apps into the repo at a steady rate - two more Kinesis apps coming shortly). Have you looked at Peru? It's pretty good at pulling down subsets of repos... https://github.com/buildinspace/peru\n. CLA confirmed! Yes, the most annoying thing about the single repo approach is that Travis still doesn't have proper testing for multiple projects inside the same repo. But yes 25 more repos in the snowplow org is a worse nightmare...\n. Bringing this forward...\n. @duncan - we've brought this forward into r60, so should be out in a couple of days...\n. That was my guess ;-)\n. This is done, closing.\n. Yes, healthcheck events to /i will end up in the bad bucket as querystring-less, but agree it's just another thing to debug...\n. Thanks @smugryan !\n. Ah I remember why this bug was introduced: it's because a VACUUM cannot be done in a transaction, and so the ANALYZE was incorrectly done first. Needs fixing.\n. Yes, should have scrolled down! Thanks, closing\n. This is very cool! Thanks for contributing @danisola.\n. Hey @danisola . I took another a look at this. Imagine a Snowplow user where:\n- Your nightly job takes about 3 hours\n- You set the failure threshold to 4 hours to be safe\n- You set the job to run at 5am so your analysts have up-to-date data when they get in\n- You want to try three times in the case of a bootstrap failure\nHere are a couple of scenarios:\n| Scenario | Old approach | PR approach |\n| --- | --- | --- |\n| Bootstrap failure | Job fails to start. Engineer fixes when he wakes up | Job starts on second attempt. Data is ready 10 minutes later than usual |\n| Failure in shredding process | Job fails 2 hours in. Engineer fixes when he wakes up | Job fails 2 hours in, 3 times in a row. EmrEtlRunner is still running at 10am. Engineer goes into AWS Dashboard and realizes what is going on |\nI think rather than attempting blanket restart, we should detect a failure to launch (bootstrap failure) as per this ticket #354 and only retry in this special case.\n. Hey @danisola - thanks, that's helpful background. Next scenario: you have a volume spike one day and the processing time takes you over your threshold, then the job gets killed and you have to start over once you realize what's happened. That puts your processing doubly-behind schedule. Have you had this happen?\n. Okay, that makes sense. I've brainstormed it through with Yali and the team. Our verdict:\n- The only jobflow failure that we have seen under which it is always safe & always fruitful to restart a jobflow is bootstrap failure. Bootstrap failure can always be unambiguously identified. With any other failure, a human review is needed\n- Killing and restarting a job which has taken longer than some expected fixed duration is a very crude mechanism. What we would expect to do instead is send alerts if a job is taking longer than expected. Then a human operator can review the job and identify whether it is a job step hang, a larger-than-usual event volume, a missing schema from Iglu or whatever\nTherefore, our plan is to adapt your PR to remove the jobflow-killing aspects and add in the check that a jobflow failure is a bootstrap failure before attempting a restart.\n. Also I wanted to check: have you filed a bug with AWS about the S3DistCp hangs?\n. Okay great stuff, thanks @danisola !\n. Thanks @danisola - have you seen this just on job submission or also during wait_for?\n. Closing as relevant parts have been pulled in. Thanks Dani!\n. Makes sense! Thanks for the contrib. Can't see you guys in the CLA yet - https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed @kazjote - thanks so much!\n. We're not going to do this, closing\n. See also https://github.com/snowplow/snowplow/issues/1373 (unscheduled)\n. This can wait till r62 as doesn't impact SSC.\n. We have a cookie extractor in R72, so this is lower priority...\n. Better still - if we make the insertId the event ID and event fingerprint, we can stop all duplicates. /cc @bogaert \n. This is an implementation detail (does not need to be a separate ticket/commit when completed).\n. Thanks Fred!\n. That's brilliant!\n. Bringing this forward as there are other filename-related tickets in this release...\n. Note: we should append, not prepend, to make sorting by timestamp easier. /cc @yalisassoon \n. This is basically a way of giving Kinesis streams \"optional type annotations\"\n. Hey nice! Scheduling...\n. Duplicate of #1403 \n. Thanks @MrCurtis ! Squashed down to a single extra commit and added to this branch: https://github.com/snowplow/snowplow/tree/feature/kinesis-bigquery-sink-errhandling\n. Not a duplicate - see titles\n. The syntax is here:\nhttps://github.com/softwarepublico/sgf/blob/a0fb3ab0f09e3611b2cc14f616d5d3318304c221/WebContent/META-INF/context.xml#L8\n. Sure thing, https://github.com/snowplow/snowplow/issues/1404\n. The suggested server.xml addition of fileDateFormat=\"yyyy-MM-dd-HH\" won't work - this is because that configuration option covers Tomcat's built-in log file rotation option, but in fact on Beanstalk we are using Amazon's own rotation scripts. So we would have to change Amazon's own rotation scripts to accomplish this\n. From further discussions with @jbeemster - the number already in the access log filename is the rotation time as a Unix secs since epoch. All we need to do is convert this timestamp to yyyy-MM-dd-HH and we should be good to go.\n. Thanks, moved to: https://github.com/snowplow/snowplow-javascript-tracker/issues/321\n. Reopening and scheduling.\n. Thanks @r-com , @fblundun can you fix these? Plus anything else in the Kinesis doco which has changed with the r60 release...\n. See also: #1379\n. Hey @danisola! Thanks for this, it looks great. Question: should this be an array rather than a singular one?\n. Thanks @danisola !\n. Bringing forwards...\n. Merged, closing! Thanks so much.\n. This one is done, closing\n. 1-0-1 was removed from Iglu Central - see instead #2068 \n. Unscheduled for now\n. I don't think this can be reopened...\n. Nice! Is there a JSON Path file in there somewhere too...\n. This is looking good! Something I remembered - CloudFront access log files start with two header rows:\n```\nVersion: 1.0\nFields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem sc-status cs(Referer) cs(User-Agent) cs-uri-query cs(Cookie) x-edge-result-type x-edge-request-id x-host-header cs-protocol cs-bytes time-taken\n```\nIt would be good if these were swallowed (=> None) rather than turned into bad rows...\n. Thanks Fred...\n. Also note that the exact contents of line 2 will be somewhat different across the different access log formats.\n. Looks great Fred! Beyond the comment about an upper-case f, can you do two things:\n- Create a ticket for the Hadoop test you added\n- Remove any CHANGELOG changes to prevent merge conflicts when I pull into the release branch\nOnce those 3 things are done, I will merge into pygmy-parrot...\n. Nice - thanks Fred!\n. Sounds essential - scheduling into Kinesis #5...\n. Blocked by https://github.com/cloudify/scalazon/pull/14\n. Closed in favor of individual tickets\n. Removed \"text/plain\" from ticket name\n. Great thanks...\n. Good thinking\n. Please do this into this branch: https://github.com/snowplow/snowplow/tree/feature/r66-common-enrich\n. Cool! Scheduling...\n. How should a failing JSON be identified?\n. Okay makes sense...\n. Remember that Scala Common Enrich will need to tolerate content type-less events for this to work (create a separate ticket when the time comes - and check if content type-less is \"\" or \"-\")...\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Hey @christiangda - first off, if it's version 0.3.0 of the SSC that you're using, then you'll want the first option, not the second:\ndata = LOAD '/home/devuser/ltv/data/snowplow/*.lzo.index' \nUSING  com.twitter.elephantbird.pig.load.ThriftPigLoader('com.snowplowanalytics.snowplow.CollectorPayload.thrift.model1.CollectorPayload');\nSecond, can you share the error you experienced? You only shared informational/warning messages. I can't see an error.\n. Hmm. We don't use Pig so it's hard to say. My suggestion is: process the events using EmrEtlRunner (i.e. using Scalding/Hadoop/EMR) first, and confirm that's all working. Then it's a case of comparing the Scalding code to your Pig code to figure out the difference\n. Good luck!\n. Moved to: https://github.com/snowplow/kinesis-s3/issues/3\n. Duplicate of https://github.com/snowplow/snowplow-javascript-tracker/issues/287\n. Thanks for raising!\n. Thanks @waterlink!\n. Not going to do this\n. Not going to do this per se.\n. Depends on #1440\n. We will do #1481 instead\n. Migrated to https://github.com/snowplow/iglu-central/issues/528. Migrated to https://github.com/snowplow/iglu-central/pull/478. Migrated to https://github.com/snowplow/iglu-central/issues/527. See the milestone description for why these cannot be merged into master currently\n. Looks good to me\n. Note that the refr_ prefix is slightly wrong, because these fields are derived from the page_uri not from the referer_uri. But spiritually they are coming from the referer (because it's the referer that sets these on the outbound link) so I think it's okay.\n/cc @yalisassoon\n. Duplicate of #1481\n. I think it should just happen automatically - there's no harm in it being run.\n. Closing this legacy PR as @chuwy is working on this enrichment now...\n. Also note that there is a bug in the stream creation code, which means that the shard count is not used:\nhttps://github.com/snowplow/snowplow/blob/master/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/sinks/KinesisSink.scala#L104-L131\nThis code is copy-pasted into all the Kinesis apps!\nThis is another good reason why our Kinesis apps shouldn't be in the business of creating streams...\n. Sounds good Fred - what is the status/origin of callProcessRecordsEvenForEmptyList?\n. I think we should be okay - checkpointing is against DynamoDB which is pretty responsive...\n. This is a dupe of a ticket in r62.\n. This would also be nicer for people building KCL, Spark Streaming, Storm et al apps. The first thing those people have to do is convert the TSV into something sensible like a JSON anyway...\n. /cc @yalisassoon \n. See also: #1630, where the conversation is going into some detail...\n. Duplicate of other tickets in this milestone, but leaving open for reference...\n. Connector is a bit vague...\n. Bringing forwards...\n. Duplicate of #1534\n. It's looking great! I have left some initial comments.\n. Awesome - I will have another read through on Sunday!\n. Hey @fblundun - sorry for the late change. All references to session_id in the PR (including tickets and CHANGELOG) need to be updated to domain_sessionid. I'll update the blog post now...\n/cc @yalisassoon \n. Thanks @fblundun !\n. Unscheduled\n. This is also a newer version than Scalding uses :disappointed: \n. This is now achievable given we are moving to Hadoop 2.4!\n. This is done, closing as it's not for CHANGELOG.\n. Please create equivs for any other configs missing a default explanation\n. Nothing to do: https://github.com/snowplow/snowplow/blob/master/4-storage/kinesis-elasticsearch-sink/src/main/resources/application.conf.example#L19\n. Sounds good\n. Hey @sparrovv ! Thanks for the contribution - I'm afraid we have a fix ready to go in this commit:\nhttps://github.com/snowplow/snowplow/commit/2cae3679a01ff965e5729a7fb947aa0ea01be7c7\nAppreciate the PR though!\n. This is needed by Warbler, seems a shame to remove it\n. Pinned to prevent Contracts warnings\n. Make sure to include timestamp\n. Depends on https://github.com/snowplow/iglu-central/issues/151\n. This is really crucial!\n. I've been thinking about this and I think I have a better way of doing this, closing...\n. where RecordThreshold and ByteThreshold reflect current Kinesis PutRecords limits?\n. Sounds great!\n. Yep I was going to ask about that! Sounds good!\n. @fblundun - this is an old ticket, but I believe the intent is to harmonize the SSC with the ES Sink.\nSSC options:\n# The following are used to authenticate for the Amazon Kinesis sink.\n      #\n      # If both are set to 'cpf', a properties file on the classpath is used.\n      # http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/ClasspathPropertiesFileCredentialsProvider.html\n      #\n      # If both are set to 'iam', use AWS IAM Roles to provision credentials.\n      #\n      # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\nES Sink options:\n# The following are used to authenticate for the Amazon Kinesis sink.\n  #\n  # If both are set to 'default', the default provider chain is used\n  # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)\n  #\n  # If both are set to 'iam', use AWS IAM Roles to provision credentials.\n  #\n  # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\nDoes that make sense?\n. See #1518 for rationale.\n. Reopening till merge to master\n. @yalisassoon has this been done in a branch?\n. Okay let's do this one last...\n. Worth running a check across the repo for \"5-storage\" to prevent broken links...\n. Maybe we make the log level a proper (i.e. Argot) argument then?\n. I think that's a great idea!\n. Strange because collector version is pretty hardcoded: https://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/java-servlet/war-resources/.ebextensions/server.xml#L138\n. Even if the %v failed, I would expect the collector version to be \"clj-1.0.0-\"\n. First plausible explanation I've seen: https://groups.google.com/forum/#!topic/snowplow-user/R6npMyc8xIE\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Pushing back\n. Dedicated argument should be --resolver, and point to a single file containing a self-describing resolver JSON.\n. This functionality shouldn't live in a collector, just put an nginx infront...\n. That's a good point. Hmm. Even without this, by definition, multiple threads means multiple copies of the LRU cache, which will be less efficient (more lookups)...\n. Interesting!\n. @fblundun - yes, this seems like a big win... At some point we will need to express some kind of DAG/dependency graph between the enrichments, eg:\nip-address -> [ip lookups enrichment] -> +geo-location -> [weather enrichment] -> +weather\n. @fblundun - can you take a look please?\n. Also weirdly I can't see geo_location in here: https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/elasticsearch_enriched_event/jsonschema/1-0-0\n. TBC if there are any implications when the user is setting the dvce_created_tstamp themselves - I don't think there are...\n. Consider adding derived_tstamp local as well...\n. Hey @fblundun ! I think they are already in atomic.events?\n. Good catch\n. Thanks Fred\n. Moved forwards\n. /cc @yalisassoon \n. Thanks Fred!\n. Hi @gabhi! Which Scala project are you trying to build?\n. Hey @gabhi - I just tried from a fresh Vagrant and it all worked:\nhost$ git clone https://github.com/snowplow/snowplow.git\n host$ cd snowplow\n host$ vagrant up && vagrant ssh\nguest$ cd /vagrant/3-enrich/scala-common-enrich\nguest$ sbt test\n...\n[info] Total for specification UserAgentParseSpec\n[info] Finished in 0 ms\n[info] 1 example, 0 failure, 0 error\n[info]\n[info] Passed: Total 141, Failed 0, Errors 0, Passed 141\n[success] Total time: 409 s, completed 29-Mar-2015 11:21:44\nSometimes that UserAgentUtils library has slightly unreliable hosting - so I'd suggest trying again!\n. Good to hear it @gabhi !\n. Hi @anishdevasia - I'm not sure I follow. snowplow_kinesis_r60_bee_hummingbird sets up the Kinesis pipeline, which doesn't use StorageLoader.\nCan you add a comment providing more detail on your exact setup?\n. Hi @anishdevasia - ah, I understand, that makes a lot more sense.\nWould you mind posting it into the forum? I'll then answer it there.\n. Migrated to https://github.com/snowplow/iglu-central/issues/522. Migrated from https://github.com/snowplow/iglu-central/issues/523. Hi @ngocthanhit - afraid not. Our Hadoop jobs are all built on Scalding, which is still Hadoop 1.x only.\n. Cool, thanks for clarifying!\n. Hey @fblundun - can you create a ticket to reflect this in Hadoop #1? We'll put this into the next Hadoop release, but it won't probably be the same release as Kinesis #6...\n. Badrow timestamp should be a JSON-Schema compatible datetime\n. Good catch Fred!\n. Aha! Of course.\n. Pinned to prevent Contracts warnings\n. @fblundun - this is all looks good! (Sorry, have only just got round to reviewing). Minor things:\n- Is there a reason why the DynamoDB table has id lowercase versus JSON all caps? Seems a bit idiosyncratic.\n- Your tests have a mix of blah: {} and blah {} in your embedded hocons\n. Nice one! I'll push the updated Kinesis apps to Bintray now and then let @jbeemster know that the \"json\" key is now lowercase.\n. The latest commit fixes that Contracts violation...\n. Depends on https://github.com/rslifka/elasticity/issues/81\n. Hey @falschparker82 - I am working on a PR into Elasticity now...\n. Thanks for the kind words @falschparker82 ! Bit of a roadblock with this AWS security change but Rob is back in the office on April 13 so hopefully we can get it resolved then. \n. @falschparker82 - this branch has now been updated with the latest Elasticity capabilities...\n. Commit is in this thread. This branch/PR should do it too: https://github.com/snowplow/snowplow/pull/1609\n. Just realized I don't think you have seen this thread: https://groups.google.com/forum/#!topic/snowplow-user/R9q1Jzpj3sw\n. Oh brill @falschparker82 - did the job start running?\n. Hey! Interesting idea. A few notes:\n- Editor-specific ignores should be put in your global .gitignore, not in project-specific ones\n- Replacing ASCII operators (=>) with unicode ones (\u21d2) is something we'll never do at Snowplow, sorry\n- There is already a PR to bump the Scala version - this is a few releases off - https://github.com/snowplow/snowplow/pull/1381\nCheers!\n. Cool! Have you signed our CLA? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed CLA signed! Thanks John. Assigning to Fred...\n. Hi @ferrlin - yes sure, we have ScalaCheck tests elsewhere in Snowplow, they are pretty cool...\n. @ferrlin at some point we may embed encypted AWS credentials in the Travis file, but it's unlikely and in any case you wouldn't be able to use those credentials when testing locally. Hope this answers your question.\n. Oh nice! Putting into the next main EmrEtlRunner release...\n. @bogaert - heads' up: we will be changing this field in r64.\n. Thanks for letting us know @danisola ! @fblundun let's set the field to 128 characters.\n. Thanks Fred!\n. Here is a value which is illegal: \"-0025-01-01 23:01:01.000\"\n. Sounds good to me\n. Yes sounds wise!\n. Reopening till merged to master...\n. Hi @falschparker82 ! We have heard from community members of occasional S3DistCp hangs - it's not something we've witnessed across any of our customer accounts, but some users do seem to be affected. I'd recommend pinging the above details to the snowplow-user@ user group to see if you can get any thoughts from those affected. I'd also definitely bring it up with AWS Support...\n. Before I close this - definitely consider increasing the instance types for core, that can't hurt...\n. Good idea\n. Yes good idea Fred - this should be a Snowplow event being fired...\n. Bringing forward to r66 following this convo https://groups.google.com/forum/#!topic/snowplow-user/3tPsz4e2wt4\n. Yep, this is a confusing gotcha with S3. A folder has no semantic meaning in S3, so a glob for the prefix:\ns3://my-processing-bucket/keyname\nwill match:\ns3://my-processing-bucket/keyname/filename\ns3://my-processing-bucket/keyname-someotherstuff/filename\nThere might be a fix in Fog for this now - not sure. Feel free to raise a ticket in Sluice.\n. Sounds good...\n. Hi Neeki,\nIt sounds a lot like this one:\nhttps://groups.google.com/forum/#!topic/snowplow-user/JuNn4BIz_EM\nCheers,\nAlex\nOn Tue, Apr 21, 2015 at 2:16 PM, Neeki Patel notifications@github.com\nwrote:\n\nHello I started seeing my map reduce jobs failing. I look into the job\nlogs and found the following error:\n2015-04-21 12:37:25,907 INFO org.apache.hadoop.util.NativeCodeLoader\n(main): Loaded the native-hadoop library\n2015-04-21 12:37:26,109 INFO org.apache.hadoop.mapred.TaskRunner (main):\nCreating symlink:\n/mnt/var/lib/hadoop/mapred/taskTracker/distcache/-2643132834156240618_-1421377178_1542957072/hdfs/cache/GeoLiteCity.dat\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/./ip_geo\n2015-04-21 12:37:26,118 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/job.jar\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/job.jar\n2015-04-21 12:37:26,120 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/.job.jar.crc\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/.job.jar.crc\n2015-04-21 12:37:26,122 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/META-INF\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/META-INF\n2015-04-21 12:37:26,124 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/.options\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/.options\n2015-04-21 12:37:26,126 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/ASL-2.0.txt\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/ASL-2.0.txt\n2015-04-21 12:37:26,127 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/DebugProtoTest.thrift\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/DebugProtoTest.thrift\n2015-04-21 12:37:26,129 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/LGPL-3.0.txt\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/LGPL-3.0.txt\n2015-04-21 12:37:26,131 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/LICENSE_rhino-1.7R4.txt\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/LICENSE_rhino-1.7R4.txt\n2015-04-21 12:37:26,133 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/PropertyList-1.0.dtd\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/PropertyList-1.0.dtd\n2015-04-21 12:37:26,135 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/address_book.thrift\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/address_book.thrift\n2015-04-21 12:37:26,136 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/bin.tgz\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/bin.tgz\n2015-04-21 12:37:26,138 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/build.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/build.properties\n2015-04-21 12:37:26,140 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/cascading\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/cascading\n2015-04-21 12:37:26,142 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/ch\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/ch\n2015-04-21 12:37:26,143 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/com\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/com\n2015-04-21 12:37:26,146 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/compiler.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/compiler.properties\n2015-04-21 12:37:26,148 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/contribs\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/contribs\n2015-04-21 12:37:26,150 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/core-default.xml\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/core-default.xml\n2015-04-21 12:37:26,151 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/decoder.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/decoder.properties\n2015-04-21 12:37:26,153 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/digesterRules.xml\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/digesterRules.xml\n2015-04-21 12:37:26,155 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/draftv3\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/draftv3\n2015-04-21 12:37:26,157 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/draftv4\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/draftv4\n2015-04-21 12:37:26,159 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/eu\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/eu\n2015-04-21 12:37:26,160 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/hdfs-default.xml\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/hdfs-default.xml\n2015-04-21 12:37:26,162 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/hsqlServlet.class\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/hsqlServlet.class\n2015-04-21 12:37:26,164 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/iglu-client-embedded\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/iglu-client-embedded\n2015-04-21 12:37:26,166 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/images\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/images\n2015-04-21 12:37:26,167 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/javax\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/javax\n2015-04-21 12:37:26,169 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/jdtCompilerAdapter.jar\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/jdtCompilerAdapter.jar\n2015-04-21 12:37:26,171 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/joptsimple\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/joptsimple\n2015-04-21 12:37:26,173 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/junit\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/junit\n2015-04-21 12:37:26,175 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/library.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/library.properties\n2015-04-21 12:37:26,177 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/licenses\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/licenses\n2015-04-21 12:37:26,179 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/log4j.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/log4j.properties\n2015-04-21 12:37:26,181 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/mapred-default.xml\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/mapred-default.xml\n2015-04-21 12:37:26,183 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/minlog-1.2.jar\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/minlog-1.2.jar\n2015-04-21 12:37:26,184 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/objenesis-1.2.jar\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/objenesis-1.2.jar\n2015-04-21 12:37:26,186 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/org.codehaus.commons.compiler.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/org.codehaus.commons.compiler.properties\n2015-04-21 12:37:26,188 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/org\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/org\n2015-04-21 12:37:26,190 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/plugin.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/plugin.properties\n2015-04-21 12:37:26,191 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/plugin.xml\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/plugin.xml\n2015-04-21 12:37:26,193 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/properties.dtd\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/properties.dtd\n2015-04-21 12:37:26,195 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/referers.json\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/referers.json\n2015-04-21 12:37:26,197 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/referers.yml\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/referers.yml\n2015-04-21 12:37:26,198 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/reflect.properties\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/reflect.properties\n2015-04-21 12:37:26,200 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/reflectasm-1.07-shaded.jar\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/reflectasm-1.07-shaded.jar\n2015-04-21 12:37:26,202 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/riffle\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/riffle\n2015-04-21 12:37:26,203 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/rootdoc.txt\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/rootdoc.txt\n2015-04-21 12:37:26,205 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/scala\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/scala\n2015-04-21 12:37:26,207 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/scalaz\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/scalaz\n2015-04-21 12:37:26,209 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/shapeless\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/shapeless\n2015-04-21 12:37:26,210 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/stylesheet.css\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/stylesheet.css\n2015-04-21 12:37:26,212 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/templates\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/templates\n2015-04-21 12:37:26,214 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/test.thrift\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/test.thrift\n2015-04-21 12:37:26,215 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/thrift\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/thrift\n2015-04-21 12:37:26,217 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/ua_parser\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/ua_parser\n2015-04-21 12:37:26,219 INFO\norg.apache.hadoop.filecache.TrackerDistributedCacheManager (main): Creating\nsymlink:\n/mnt1/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/jars/webapps\n<-\n/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201504211230_0003/attempt_201504211230_0003_m_000000_1/work/webapps\n2015-04-21 12:37:26,394 WARN\norg.apache.hadoop.metrics2.impl.MetricsSystemImpl (main): Source name ugi\nalready exists!\n2015-04-21 12:37:26,488 INFO org.apache.hadoop.mapred.MapTask (main): Host\nname: ip-172-31-63-60.ec2.internal\n2015-04-21 12:37:26,507 INFO org.apache.hadoop.util.ProcessTree (main):\nsetsid exited with exit code 0\n2015-04-21 12:37:26,513 INFO org.apache.hadoop.mapred.Task (main): Using\nResourceCalculatorPlugin :\norg.apache.hadoop.util.LinuxResourceCalculatorPlugin@5a232b4\nhttps://github.com/org.apache.hadoop.util.LinuxResourceCalculatorPlugin/snowplow/commit/5a232b4c\n2015-04-21 12:37:27,188 INFO cascading.tap.hadoop.io.MultiInputSplit\n(main): current split input path: hdfs://\n172.31.57.87:9000/local/snowplow/raw-events/2015-04-210.lzo\n2015-04-21 12:37:27,200 INFO\ncom.hadoop.compression.lzo.GPLNativeCodeLoader (main): Loaded native gpl\nlibrary\n2015-04-21 12:37:27,202 INFO com.hadoop.compression.lzo.LzoCodec (main):\nSuccessfully loaded & initialized native-lzo library [hadoop-lzo rev\n049362b7cf53ff5f739d6b1532457f2c6cd495e8]\n2015-04-21 12:37:27,207 WARN\norg.apache.hadoop.io.compress.snappy.LoadSnappy (main): Snappy native\nlibrary is available\n2015-04-21 12:37:27,207 INFO\norg.apache.hadoop.io.compress.snappy.LoadSnappy (main): Snappy native\nlibrary loaded\n2015-04-21 12:37:27,218 INFO org.apache.hadoop.mapred.MapTask (main):\nnumReduceTasks: 0\n2015-04-21 12:37:27,236 INFO cascading.flow.hadoop.FlowMapper (main):\ncascading version: 2.5.5\n2015-04-21 12:37:27,236 INFO cascading.flow.hadoop.FlowMapper (main):\nchild jvm opts: -Xmx1096m\n2015-04-21 12:37:27,487 INFO cascading.flow.hadoop.FlowMapper (main):\nsourcing from: Hfs[\"TextLine[['offset',\n'line']->[ALL]]\"][\"hdfs:/local/snowplow/raw-events\"]\n2015-04-21 12:37:27,487 INFO cascading.flow.hadoop.FlowMapper (main):\nsinking to: Hfs[\"TextDelimited[['app_id', 'platform', 'etl_tstamp',\n'collector_tstamp', 'dvce_tstamp', 'event', 'event_id', 'txn_id',\n'name_tracker', 'v_tracker', 'v_collector', 'v_etl', 'user_id',\n'user_ipaddress', 'user_fingerprint', 'domain_userid', 'domain_sessionidx',\n'network_userid', 'geo_country', 'geo_region', 'geo_city', 'geo_zipcode',\n'geo_latitude', 'geo_longitude', 'geo_region_name', 'ip_isp',\n'ip_organization', 'ip_domain', 'ip_netspeed', 'page_url', 'page_title',\n'page_referrer', 'page_urlscheme', 'page_urlhost', 'page_urlport',\n'page_urlpath', 'page_urlquery', 'page_urlfragment', 'refr_urlscheme',\n'refr_urlhost', 'refr_urlport', 'refr_urlpath', 'refr_urlquery',\n'refr_urlfragment', 'refr_medium', 'refr_source', 'refr_term',\n'mkt_medium', 'mkt_source', 'mkt_term', 'mkt_content', 'mkt_campaign',\n'contexts', 'se_category', 'se_action', 'se_label', 'se_property',\n'se_value', 'unstruct_event', 'tr_orderid ', 'tr_affiliation', 'tr_total',\n'tr_tax', 'tr_shipping', 'tr_city', 'tr_state', 'tr_country', 'ti_orderid',\n'ti_sku', 'ti_name', 'ti_category', 'ti_price', 'ti_quantity',\n'pp_xoffset_min', 'pp_xoffset_max', 'pp_yoffset_min', 'pp_yoffset_max',\n'useragent', 'br_name', 'br_family', 'br_version', 'br_type',\n'br_renderengine', 'br_lang', 'br_features_pdf', 'br_features_flash',\n'br_features_java', 'br_features_director', 'br_features_quicktime',\n'br_features_realplayer', 'br_features_windowsmedia', 'br_features_gears',\n'br_features_silverlight', 'br_cookies', 'br_colordepth', 'br_viewwidth',\n'br_viewheight', 'os_name', 'os_family', 'os_manufacturer', 'os_timezone',\n'dvce_type', 'dvce_ismobile', 'dvce_screenwidth', 'dvce_screenheight',\n'doc_charset', 'doc_width', 'doc_height', 'tr_currency', 'tr_total_base',\n'tr_tax_base', 'tr_shipping_base', 'ti_currency', 'ti_price_base',\n'base_currency', 'geo_timezone', 'mkt_clickid', 'mkt_network', 'etl_tags',\n'dvce_sent_tstamp', 'refr_domain_userid', 'refr_dvce_tstamp',\n'derived_contexts', 'domain_sessionid',\n'derived_tstamp']]\"][\"hdfs:/local/snowplow/enriched-events\"]\n2015-04-21 12:41:07,052 ERROR cascading.flow.stream.TrapHandler (main):\ncaught Throwable, no trap available, rethrowing\ncascading.pipe.OperatorException:\n[com.twitter.scalding.M...][com.twitter.scalding.RichPipe.each(RichPipe.scala:471)]\noperator Each failed executing operation\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:107)\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\nat cascading.flow.stream.SourceStage.map(SourceStage.java:102)\nat cascading.flow.stream.SourceStage.run(SourceStage.java:58)\nat cascading.flow.hadoop.FlowMapper.run(FlowMapper.java:130)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:441)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:377)\nat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\nat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.lang.NullPointerException\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.stripInstanceEtc(JsonUtils.scala:240)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.extractJson(JsonUtils.scala:204)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.validateAndReformatJson(JsonUtils.scala:189)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$$anonfun$1.apply(JsonUtils.scala:59)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$$anonfun$1.apply(JsonUtils.scala:58)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$$anonfun$4.apply(EnrichmentManager.scala:103)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$$anonfun$4.apply(EnrichmentManager.scala:103)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$$anonfun$1.apply(MapTransformer.scala:158)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$$anonfun$1.apply(MapTransformer.scala:155)\nat\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nat\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)\nat\nscala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)\nat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nat scala.collection.AbstractTraversable.map(Traversable.scala:105)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$.com$snowplowanalytics$snowplow$enrich$common$utils$MapTransformer$$_transform(MapTransformer.scala:155)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$TransformableClass.transform(MapTransformer.scala:132)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$.enrichEvent(EnrichmentManager.scala:193)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:81)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:80)\nat scalaz.NonEmptyList$class.map(NonEmptyList.scala:29)\nat scalaz.NonEmptyListFunctions$$anon$4.map(NonEmptyList.scala:164)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:80)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:78)\nat scalaz.Validation$class.map(Validation.scala:114)\nat scalaz.Success.map(Validation.scala:329)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:78)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:76)\nat scala.Option.map(Option.scala:145)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:76)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:74)\nat scalaz.Validation$class.map(Validation.scala:114)\nat scalaz.Success.map(Validation.scala:329)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$.processEvents(EtlPipeline.scala:74)\nat\ncom.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:172)\nat\ncom.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:171)\nat com.twitter.scalding.MapFunction.operate(Operations.scala:58)\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\n... 11 more\n2015-04-21 12:41:07,054 ERROR cascading.flow.stream.SourceStage (main):\ncaught throwable\ncascading.pipe.OperatorException:\n[com.twitter.scalding.M...][com.twitter.scalding.RichPipe.each(RichPipe.scala:471)]\noperator Each failed executing operation\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:107)\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\nat cascading.flow.stream.SourceStage.map(SourceStage.java:102)\nat cascading.flow.stream.SourceStage.run(SourceStage.java:58)\nat cascading.flow.hadoop.FlowMapper.run(FlowMapper.java:130)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:441)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:377)\nat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\nat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.lang.NullPointerException\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.stripInstanceEtc(JsonUtils.scala:240)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.extractJson(JsonUtils.scala:204)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.validateAndReformatJson(JsonUtils.scala:189)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$$anonfun$1.apply(JsonUtils.scala:59)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$$anonfun$1.apply(JsonUtils.scala:58)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$$anonfun$4.apply(EnrichmentManager.scala:103)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$$anonfun$4.apply(EnrichmentManager.scala:103)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$$anonfun$1.apply(MapTransformer.scala:158)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$$anonfun$1.apply(MapTransformer.scala:155)\nat\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nat\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)\nat\nscala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)\nat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nat scala.collection.AbstractTraversable.map(Traversable.scala:105)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$.com$snowplowanalytics$snowplow$enrich$common$utils$MapTransformer$$_transform(MapTransformer.scala:155)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$TransformableClass.transform(MapTransformer.scala:132)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$.enrichEvent(EnrichmentManager.scala:193)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:81)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:80)\nat scalaz.NonEmptyList$class.map(NonEmptyList.scala:29)\nat scalaz.NonEmptyListFunctions$$anon$4.map(NonEmptyList.scala:164)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:80)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:78)\nat scalaz.Validation$class.map(Validation.scala:114)\nat scalaz.Success.map(Validation.scala:329)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:78)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:76)\nat scala.Option.map(Option.scala:145)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:76)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:74)\nat scalaz.Validation$class.map(Validation.scala:114)\nat scalaz.Success.map(Validation.scala:329)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$.processEvents(EtlPipeline.scala:74)\nat\ncom.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:172)\nat\ncom.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:171)\nat com.twitter.scalding.MapFunction.operate(Operations.scala:58)\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\n... 11 more\n2015-04-21 12:41:07,062 INFO org.apache.hadoop.mapred.TaskLogsTruncater\n(main): Initializing logs' truncater with mapRetainSize=-1 and\nreduceRetainSize=-1\n2015-04-21 12:41:07,081 INFO org.apache.hadoop.io.nativeio.NativeIO\n(main): Initialized cache for UID to User mapping with a cache timeout of\n14400 seconds.\n2015-04-21 12:41:07,081 INFO org.apache.hadoop.io.nativeio.NativeIO\n(main): Got UserName hadoop for UID 105 from the native implementation\n2015-04-21 12:41:07,082 WARN org.apache.hadoop.mapred.Child (main): Error\nrunning child\ncascading.pipe.OperatorException:\n[com.twitter.scalding.M...][com.twitter.scalding.RichPipe.each(RichPipe.scala:471)]\noperator Each failed executing operation\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:107)\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\nat cascading.flow.stream.SourceStage.map(SourceStage.java:102)\nat cascading.flow.stream.SourceStage.run(SourceStage.java:58)\nat cascading.flow.hadoop.FlowMapper.run(FlowMapper.java:130)\nat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:441)\nat org.apache.hadoop.mapred.MapTask.run(MapTask.java:377)\nat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\nat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.lang.NullPointerException\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.stripInstanceEtc(JsonUtils.scala:240)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.extractJson(JsonUtils.scala:204)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$.validateAndReformatJson(JsonUtils.scala:189)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$$anonfun$1.apply(JsonUtils.scala:59)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.JsonUtils$$anonfun$1.apply(JsonUtils.scala:58)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$$anonfun$4.apply(EnrichmentManager.scala:103)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$$anonfun$4.apply(EnrichmentManager.scala:103)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$$anonfun$1.apply(MapTransformer.scala:158)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$$anonfun$1.apply(MapTransformer.scala:155)\nat\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nat\nscala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\nat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)\nat\nscala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)\nat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\nat scala.collection.AbstractTraversable.map(Traversable.scala:105)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$.com$snowplowanalytics$snowplow$enrich$common$utils$MapTransformer$$_transform(MapTransformer.scala:155)\nat\ncom.snowplowanalytics.snowplow.enrich.common.utils.MapTransformer$TransformableClass.transform(MapTransformer.scala:132)\nat\ncom.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$.enrichEvent(EnrichmentManager.scala:193)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:81)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:80)\nat scalaz.NonEmptyList$class.map(NonEmptyList.scala:29)\nat scalaz.NonEmptyListFunctions$$anon$4.map(NonEmptyList.scala:164)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:80)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:78)\nat scalaz.Validation$class.map(Validation.scala:114)\nat scalaz.Success.map(Validation.scala:329)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:78)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:76)\nat scala.Option.map(Option.scala:145)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:76)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:74)\nat scalaz.Validation$class.map(Validation.scala:114)\nat scalaz.Success.map(Validation.scala:329)\nat\ncom.snowplowanalytics.snowplow.enrich.common.EtlPipeline$.processEvents(EtlPipeline.scala:74)\nat\ncom.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:172)\nat\ncom.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:171)\nat com.twitter.scalding.MapFunction.operate(Operations.scala:58)\nat\ncascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\n... 11 more\n2015-04-21 12:41:07,086 INFO org.apache.hadoop.mapred.Task (main):\nRunnning cleanup for the task\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1625.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7881 622 925\n@alexcrdean https://twitter.com/alexcrdean\n. Can you create a sibling ticket in the referer-parser project @fblundun ?\n. Thanks for flagging, added a Trello card to track this. Hey @andrioni ! Actually yes we do have plans to work on this this summer. We have started sketching out an evolution of the enriched events format using Apache Avro. (We are keeping the old Avro branches around for reference, but are unlikely to use them again.)\nBy way of related information, have you seen this JSON Schema for the enriched events as they are shredded ready for Elasticsearch: https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/elasticsearch_enriched_event/jsonschema/1-0-1\nThinking about how to model an enriched event in Avro is super interesting.\nIgnoring the hard-coded \"legacy\" TSV fields for the minute, a Snowplow enriched event fundamentally consists of a collection of snapshotted business entities, all helpfully self-annotated with their JSON Schema. How do we represent this in Avro? One idea is to let each company's enriched event schema in Avro (or each company's app's enriched event schema) evolve based on the entities observed in the stream. In other words, something like this:\n- For a games company, acme.com, with their game Zombie\n- For each new entity observed in JSON in an incoming Zombie enriched event,\n- Convert the entity definition to Avro using json-schema-avro\n- Upload the entity definition to Iglu\n- Fetch the current Avro definition for enriched_event_com_acme_zombie\n- Issue a new version of enriched_event_com_acme_zombie which includes the new entity definition\n- Upload the entity definition to Iglu\nThis is just back-of-the-envelope thinking - if you have any other ideas, please add them to this ticket! The basic goal is to reconcile an emergent, organic (but still internally schema'ed) event model in JSON Schema in the front half of the pipeline with a definitive, strongly typed (but still evolvable) Avro event schema in the second half of the pipeline.\n. Putting into milestone...\n. Some further thinking: we may be able to model a Snowplow enriched event as an array of the union of all the different record types. In other words something like this:\njson\n{\n  \"type\":\"record\",\n  \"name\":\"enriched_event_com_acme_zombie\",\n  \"fields\":\n  [\n    {\"name\":\"SomeCommonData\",\"type\":\"string\"},\n    {\"name\":\"MoreCommonData\",\"type\":\"float\"},\n    {\"name\":\"CompositeRecord\",\"type\":\n      [\n        {\n          \"type\":\"record\",\n          \"name\":\"ZombieLevel\",\n          \"fields\":\n          [\n            {\"name\":\"x\",\"type\":\"string\"},\n            {\"name\":\"y\",\"type\":\"long\"}\n          ]\n        },\n        {\n          \"type\":\"record\",\n          \"name\":\"ZombiePlayer\",\n          \"fields\":\n          [\n            {\"name\":\"z\",\"type\":\"int\"},\n            {\"name\":\"w\",\"type\":\"float\"},\n            {\"name\":\"m\",\"type\":\"double\"},\n            {\"name\":\"l\",\"type\":\"boolean\"}\n          ]\n        }\n      ]\n    }\n  ]\n}\n. /cc @chuwy\n. Hmm, that needs to be fixed as it will break Redshift loading too...\n. Can probably just use this: http://avro.apache.org/docs/1.6.2/api/java/org/apache/avro/data/Json.html\n. @danisola will work on this.\n. From an architecture perspective, my suggestion is to:\n1. Extract the current JSON Validation logic from Scala Hadoop Shred into Common Enrich\n2. Use Common Enrich's JSON Validation logic in Scala Hadoop Shred, Scala Hadoop Enrich and Scala Kinesis Enrich\n. Duplicate of #1947 \n. Follows on from #1630\n. /cc @andrioni, @yalisassoon \n. Dupe of #1640 \n. Duplicate of #1710\n. Ah that's great research - thanks Josh!\n. Hey Josh - yes I was trying to understand this and couldn't quite. Is the subscribe URI the same as the notification URI, or do we have the possibility of using a separate one?\n. Ah! I just meant, is the URI which is used to request a subscription the same URI as the one which is used to send notifications to, if the subscription request is approved? i.e. we have to use the collector URI for both?\n. Or later? We could just have an AWS Lambda app which auto-confirms the subscriptions (i.e. reads the enriched events from Kinesis and curls the confirmation URI)...\n. Sure thing - the Kinesis pipeline will need some different logic, because unlike Hadoop, it doesn't get s3:// protocol support \"for free\"...\n. Why is this not an issue with pipelines we have running existing Kinesis Enrich, which are downloading geoip fine? /cc @jbeemster \n. Hey @ngocthanhit - can you post to the Snowplow user group? Make sure to include your campaign_attribution json.\n. Actually the error we get is slightly different:\nCaused by: Status Code: 403, AWS Service: Amazon S3, AWS Request ID: F6003FA50CD80573, AWS Error Code: null, AWS Error Message: Forbidden, S3 Extended Request ID: ...\nI have tried renaming the files to have shorter names and that didn't work. I'm going to try the CloudFront files next...\n. The difference of error was because we were using an S3 file in enrichments which we didn't have access to. With that removed, we get the exact same error as Michael and Anthony. This is good because we are running in a VPC subnet, and Michael isn't, so that rules out it being somehow VPC-related.\nI'm investigating...\n. This is the error I get:\njava.lang.IllegalArgumentException: AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively) of a s3n URL, or by setting the fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties (respectively).\n    at org.apache.hadoop.fs.s3.S3Credentials.initialize(S3Credentials.java:66)\n    at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.initialize(Jets3tNativeFileSystemStore.java:49)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n    at org.apache.hadoop.fs.s3native.$Proxy7.initialize(Unknown Source)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:216)\n    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)\n    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)\n    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)\n    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)\n    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)\n    at com.twitter.elephantbird.mapreduce.input.MultiInputFormat.determineFileFormat(MultiInputFormat.java:179)\n    at com.twitter.elephantbird.mapreduce.input.MultiInputFormat.createRecordReader(MultiInputFormat.java:88)\n    at com.twitter.elephantbird.mapred.input.DeprecatedInputFormatWrapper$RecordReaderWrapper.<init>(DeprecatedInputFormatWrapper.java:251)\n    at com.twitter.elephantbird.mapred.input.DeprecatedInputFormatWrapper.getRecordReader(DeprecatedInputFormatWrapper.java:118)\n    at cascading.tap.hadoop.io.MultiInputFormat$1.operate(MultiInputFormat.java:253)\n    at cascading.tap.hadoop.io.MultiInputFormat$1.operate(MultiInputFormat.java:248)\n    at cascading.util.Util.retry(Util.java:762)\n    at cascading.tap.hadoop.io.MultiInputFormat.getRecordReader(MultiInputFormat.java:247)\n    at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.<init>(MapTask.java:197)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:418)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\nIt's the exact same as Anthony's. Michael's is odd because it doesn't look like the error is caused when trying to read the lzo files (Michael's doesn't have com.twitter.elephantbird.mapreduce.input.MultiInputFormat in the stack trace).\n. Right so I believe the problem is AWS's recent IAM roles policy - see this thread:\nhttps://github.com/awslabs/emr-bootstrap-actions/issues/33\n\nWhen you use IAM roles, core-site.xml will not have these things:\nfs.s3n.awsAccessKeyId\n\u2028fs.s3.awsAccessKeyId\nfs.s3bfs.awsAccessKeyId\n\nAnd it looks like the hadoop-lzo code needs those properties to read the lzo files. I'll update the ticket name.\n. Here is more information on this: http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-iam-roles-calling.html\nWe can probably fix this by adding an S3DistCp step in front of the lzo files, so that hadoop-lzo doesn't have to attempt S3 access...\n. Right so I have the S3DistCp in place. The lzo filenames were causing some trouble with S3DistCp - apparently the version of S3DistCp on AMI 2.4.2 has:\n\nsome difficulties with copying S3 objects that may have similar paths (prefixes). AMI 2.4.3 and later\nhas a fixed version of S3DistCP. \n\nTrying 2.4.3 gave me an error in the Iglu code (something related to Jackson parsing, maybe a Jackson dependency changed between 2.4.2 and 2.4.3). I am trying 2.4.5 now. If that fails, I am going to attempt renaming the lzo files and seeing if that fixes it.\n. So I can't promote to 2.4.3 because of this issue: https://github.com/snowplow/snowplow/issues/1001\nI'm going to experiment with a different filename format for the lzo files and see if that works with 2.4.2...\n. So I tried another filename format (UUIDs) and that didn't work either - the problem is that whatever filename format I use, there are always two files with almost identical names (.lzo) and (.lzo.index)...\nSo I think the only solution is to fix #1001.\n. /cc @fblundun \n. Aha!\n. Thanks Fred!\n. Fred to check if this is already done.\n. Depends on https://github.com/snowplow/iglu-scala-client/issues/25\n. Pushing this back...\n. Thanks for raising!\n. Will be fixed by #1669, closing\n. Hey @ihortom - I think I forgot to close it back in May last year...\n. @fblundun - yep, it's a very common oversight in Java land...\n. This has been done, closing\n. @bogaert @dilyand can we close this? Presumably there's nothing here we need in the Spark web data model.. Thanks, closing. Hey @yuvalherziger ! This is a really interesting ticket. The hard-coded 15000 is designed to ensure that no truncated JSONs will get loaded into atomic.events. Even a single truncated JSON could break Redshift's JSON functions.\nYou raise a very good point: if you have shredding setup, then you are much more interested in having oversized JSONs successfully shredded thru to Redshift, and you won't care if the raw JSON fields in atomic.events are truncated.\nThe underlying problem here is that Snowplow's enrichment process is somewhat complected with the Redshift-specific storage code: the truncation code you mention should really be in a Redshift-specific storage job (if it exists at all), not in the storage-agnostic enrichment job.\nI'm updating the ticket name so that we can give this further thought...\n. Hi @yuvalherziger - yes, the 50kb limit on Kinesis is a bit of a pain, especially as we can't subdivide an event further without making things hugely complicated...\n. My preference is to remove the truncation code in its entirety, and bring it back on a per-db basis as needed in the future. This may lead to some breakages on the Postgres side but would be much better for Redshift (which supports automatic truncation of columns on load).\n. See also: #1731 \n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Need further tickets for Redshift, Postgres etc.\nDepends on: #1550 \n. Updated to records over 1Mb\n. Hey Fred - giving the VM more cores sounds like a good idea!\n. Thanks Fred, could you rename this ticket to something like \"Update Vagrant to give access to all CPU cores\" and close the sibling tickets?\n. Genius!\n. Thanks Vincent!\n. Yep that page is behind - we will bring it up to-date Vincent!\nYou are right, contributing to GitHub wikis which are collaborator-write-only is a little difficult - but there are some instructions here: https://gist.github.com/larrybotha/10650410\n. Awesome, closing\n. Thanks!\n. Thanks Melodie! Are there any particularly bad/confusing examples we should prioritize?\n. This is really nice @ihortom ! How do you do the yellow highighting?\n. Pushed to master\n. Hi @ihortom - great, that's been pushed now. Closing!\n. @uniuser - please follow up in the blocker ticket too\n. The blocking issue is fixed, so we can bring this forward now.\n. Ah @fblundun - this one should move to snowplow/kinesis-s3...\n. This is done, closing.\n. Agree\n. Looks good!\n. It should have some entropy no?\n. This is just for the partition key right? We can still query for missing IP addresses in Elasticsearch or Redshift... Entropy is to avoid hotspotting on one shard. Let me know if I'm missing something...\n. No the session ID would be separate - would span multiple page views...\n On 15 May 2015 12:22 pm, \"Fred Blundun\" notifications@github.com wrote:\n\nWould this be covered by the domain_sessionid field\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/outputs/EnrichedEvent.scala#L230?\n(It isn't currently populated by any tracker.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1684#issuecomment-102373102.\n. Correct\n. Would be good to test with the new JS Enrichment:\n\n``` json\n{\n    \"schema\": \"iglu:com.snowplowanalytics.snowplow/javascript_script_config/jsonschema/1-0-0\",\n\"data\": {\n\n    \"vendor\": \"com.snowplowanalytics.snowplow\",\n    \"name\": \"javascript_script_config\",\n    \"enabled\": true,\n    \"parameters\": {\n        \"script\": \"ZnVuY3Rpb24gcHJvY2VzcyhldmVudCkgew0KDQogIHZhciBhcHBJZCA9IGV2ZW50LmdldEFwcF9pZCgpOw0KICANCiAgaWYgKGFwcElkID09IG51bGwpIHsNCiAgICByZXR1cm4gW107DQogIH0NCg0KICAvLyBVc2UgbmV3IFN0cmluZygpIGJlY2F1c2UgaHR0cDovL25lbHNvbndlbGxzLm5ldC8yMDEyLzAyL2pzb24tc3RyaW5naWZ5LXdpdGgtbWFwcGVkLXZhcmlhYmxlcy8NCiAgdmFyIGFwcElkVXBwZXIgPSBuZXcgU3RyaW5nKGFwcElkLnRvVXBwZXJDYXNlKCkpOw0KDQogIHJldHVybiBbIHsgc2NoZW1hOiAiaWdsdTpjb20uc25vd3Bsb3dhbmFseXRpY3Muc25vd3Bsb3cvcmVtb3ZlX2Zyb21fY2FydC9qc29uc2NoZW1hLzEtMC0wIiwNCiAgICAgICAgICAgICAgIGRhdGE6IHsgc2t1OiBhcHBJZFVwcGVyLCBxdWFudGl0eTogMiB9DQogICAgICAgICAgIH0gXTsNCn0NCg==\"\n    }\n}\n\n}\n```\n. @Aerlinger - this file being uploaded to all snowplow-hosted-assets-* buckets now, should be there in a few mins...\n. That's done\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Couldn't we just add a Spam: section into the existing referer-parser database?\n. Bringing forwards as pretty straightforward add\n. Dupe of #1685\n. Hey Fred! This is all I came up with for the EmrEtlRunner:\nhttps://github.com/snowplow/snowplow/commit/3ef384705a93f261fccf3109af4ec7ee1d0681bd\nI think collector_uri is probably preferable to endpoint or collector...\nAgree, I think tna is not needed.\nOn the hocon section - I think monitoring { snowplow { might be more futureproof than tracking {...\n. That sounds very wise!\n. Bringing forward as this will save some pain!\n. We don't commit to release dates but it should be in around 3 releases' time.\n. See https://github.com/snowplow/sluice/issues/30\n. Hey @gincor - I'm sorry this one has been hanging for so long. Bringing it forwards...\n. Thanks @gincor - you are right, you can work around the limitation with some usage of aws s3... All the best with it!\n. Pushed! Closing.\n. Many thanks @jasonbosco ! CLA signed; this is going into the next release...\n. This is done, closing!\n. Moving to Kinesis #6.\n. Thanks for exploring this Fred!\n. @fblundun - this would be a great one to explore first...\n. Added iam:PassRole, closing\n. Let's push this one back..\n. How about option 2 but with a count against each code?\n. Nice\n. This is already done, closing\n. Update from @yalisassoon - these fields are in fact v useful for debugging bad rows. Therefore we can say that this ticket is blocked by this ticket: #824, which provides an alternative way of debugging bad rows...\n. Yes, an invalid context (say) will still be present in atomic.events (today), as long as the JSON is valid JSON.\n. Makes sense...\n. Note that following https://github.com/snowplow/kinesis-s3/pull/43 this might not be super-necessary.\n. Here is some sbt-fu to replace an old version of a transitively bundled dependency with a newer one: https://github.com/snowplow/snowplow/blob/feature/r66-hadoop2/3-enrich/scala-hadoop-enrich/project/Dependencies.scala#L58\n. This ticket will only work if the subset of the Java API we use is compatible from 1.2.1 to 1.4.4\n. This is done and tested and working\n. Thanks for raising @grzegorzewald ! Great announcement by AWS there.\n@fblundun - I remember us making the Kinesis record size hocon-configurable, but can't seem to find it in the collector hocon. Please advise?\n. Yeah, a lot of work has been done on that stuff in Kinesis #6... Fred is back next week, he will update us...\n. Thanks @fblundun - I would add make maximum record size configurable in Kinesis #8. You're right, not a priority as 1Mb is large and they are unlikely to update soon...\n. StackLead is dead no? http://stacklead.com/blog/\n. Good Q! Making the period smaller definitely works as a workaround. Let's push back this ticket and think about it some more...\n. This is Kinesis #7 so already pushed back...\n. https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-kinesis-enrich/src/main/resources/config.hocon.sample#L52-L53\n. Pushing back...\n. Won't fix, Hadoop Enrich has been replaced and we are long past AMI 3.x. Moving forwards!\n. Hey Fred, good question but no that's deliberate - it means no data to process since last time, which is not in itself a failure\n. Hey @fblundun - can we move this up to R67 too? It will be super-helpful for @yalisassoon upgrading users' Clojure Collectors.\n. Thanks Fred!\n. This isn't the strategy any more, closing. Ah - thanks Fred. Second option sounds good...\n. Support questions to the support group please!\n. Yep I think sounds good!\n. A whitelist could look something like this:\n- com.snowplowanalytics.snowplow/link_click\n- com.snowplowanalytics.snowplow/mobile_context\n- com.mandrill/*\n- com.acme/*\n. No it would be the whole file...\n. Looks great Fred! Great work\n. Won't do, see #1959\n. Whether we can accomplish this for Clojure Collector will depend on whether there are potential filename conflicts.\n. Duplicate of #1977, closing. This is doubly difficult because the shredded files are in folders, which will cause S3DistCp to die.\n. S3DistCp will happily copy a folder structure I believe...\n. I think there are two ways we could implement Azure support:\n1. It should be straightforward to run the planned Kafka+Samza version of Snowplow on Azure boxes. Then we could just implement support for SQL Data Warehouse when that's available\n2. We could create an \"Azure idiomatic\" version of Snowplow using EventHub\nHas anyone successfully used EventHub in the wild? It looks pretty immature compared to Kinesis and Kafka...\n. Hey @jpoon ! EventHub looks interesting but I could only find .NET examples for it; we would need a JVM SDK for it to make it work with Snowplow.\nThe Kafka+Samza version is still currently on the drawing board - although we do have a branch with Kafka support for our Scala Stream Collector.\nOne potential architecture for Azure could be like this:\ntrackers -> Scala Stream Collector -> Kafka -> Kafka-Blob sink -> an Azure-friendly version of our existing Hadoop flow\nI guess questions I would have:\n- Is there an equivalent of Pinterest's Secor but for sinking from Kafka to Azure Blob Storage instead of S3?\n- Our EmrEtlRunner Ruby (soon to be JRuby) app does a lot of Hadoop orchestration on Elastic MapReduce, using Elasticity. What is the equivalent for programmatically orchestrating a Hadoop job on Azure? We would need a Ruby or JVM SDK here too\n. Hey @jpoon ! Thanks for the summary of options, very helpful. Some comments:\n- EventHub. The hdinsight/eventhubs-client looks promising. Potentially we could embed that into the Scala Stream Collector, and then Snowplow raw events could be written straight to EventHub\n- Kafka or EventHub -> Azure Blob Storage. Shame there isn't something already - it feels like this would be a good thing for Microsoft to build to encourage Azure adoption. A bit like Amazon's amazon-kinesis-connectors library. We have written kinesis-s3, somebody needs to write kafka-azureblob or eventhub-azureblob :-)\n- SQL Data Warehouse. Yes, having a horizontally scalable analytics database available is essential for a Snowplow user. How do we get on the private beta? :-)\n- PostgreSQL. Yes, PostgreSQL support is available. It will struggle with any serious event volumes, but it can be a good way of testing things\n- HDInsight. Let me know when you know about orchestration tools. The other thing we lean on heavily is AWS's S3DistCp, which is an Amazon fork of the regular Hadoop DistCp tool, which makes it easy to move data S3<>HDFS. I wonder if there's an equivalent in HDInsight?\n. I've done some digging, and it looks like there is a .NET library for automating Hadoop jobs on HDInsight:\n- https://hadoopsdk.codeplex.com/\n- https://azure.microsoft.com/en-gb/documentation/articles/hdinsight-submit-hadoop-jobs-programmatically/#submit-mapreduce-jobs-using-hdinsight-net-sdk\n. As per the above, it's not possible currently...\n. Hey @jpoon, @thomaswrenn  ! No worries at all. Answers to your questions/comments:\n\nHave you found that a lot of people persist the raw events from Kinesis straight to S3?\n\nYes - at the moment we don't do loading of Redshift from Kinesis (only from Hadoop), so currently we recommend people sink the raw events to S3 and then run our Hadoop pipeline to load into Redshift as per our latest release blog post.\nIn any case - archiving raw events is important to facilitate re-processing later; I explore this in chapter 9 in my book.\n\nWhat is the difference between Amazon's version vs snowplow?\n\nAmazon's version is more of a toolkit (albeit with a sample application) - kinesis-s3 is a pre-built app which just needs a configuration file. Also, kinesis-s3 writes lzo-index files which are very Hadoop-friendly.\n\nHDInsight. HDInsight has native support for Azure Blob Storage (ie. no need to move it to an explicit HDFS store).\n\nAh okay. EMR has native support for S3 too - but moving to the cluster's HDFS with S3DistCp first makes processing much faster. Is that not the case with HDInsight?\n\nOtherwise, as HDInsight deploys the Hortonworks Hadoop stack there is Ambari and WebHCat out of the box.\n\nRight - and now that the Snowplow Hadoop job runs on Hadoop 2.4.0 / YARN, we could probably submit the job using the YARN Client?\n\nquickest path to supporting Azure ... it might mean adding support for kafka or event hub and then using the persisting that data to PostgreSQL?\n\nMy gut says:\n1. Scala Stream Collector with EventHub support via eventhubs-client\n2. EventHub -> Azure Blob Storage sink app\n3. Snowplow CLI app can submit Hadoop job to vanilla YARN cluster (#1792)\n4. We extend our Redshift loading engine to also work with SQL Data Warehouse\nThis is the minimum number of moving parts which is still very horizontally scalable.\nPostgres would be less work but it would be much less impactful - because nobody with serious event volumes can use Postgres with Snowplow. Also a powerful part of Snowplow is the custom shredding process which loads our event JSONs into dedicated tables in Redshift...\n. These are probably quite a good place to start:\n- http://snowplowanalytics.com/blog/2014/07/09/snowplow-0.9.5-released-with-json-validation-shredding/\n- https://github.com/snowplow/snowplow/wiki/6-Configuring-shredding\n. Hmm - streaming analytics doesn't look like you can execute arbitrary code. We need a runtime (whether Hadoop, Samza or something else) where we can embed all our enrichment and validation logic :-)\n. Potentially @philbritton ! But these sorts of managed cloud ETL services tend to be too opinionated to make a port of Snowplow straightforward... \n. Hey @rgknp - we would love to see Snowplow on Azure too! How do we make this happen? Storm/Spark Streaming is a nice idea but we always want support for our standard batch flow so that we can enable users to perform full-archive re-processing (e.g. when we introduce new enrichments or update the enriched event format).\nSo we need to support at a minimum:\ncollector -> event hub -> azure blob storage -> hadoop -> azure sql data warehouse\nWith this in mind - has Azure made any progress in the last 12 months with providing a standard component for sinking an EventHub into Azure Blob Storage (think Secor but for Azure)? We are only a small team and don't have the bandwidth to build these kinds of generic components for Azure at this time...\n. Hey @rgknp - the EventProcessorHost looks pretty low-level. AWS provide the kinesis-connectors project and Kinesis Firehose to make things like Kinesis->S3 straightforward. What's the Azure equivalent?\n. Thanks @markusahlstran - that's helpful to know. I read somewhere that Stream Analytics has to deserialize the individual records before working with them - isn't that rather inefficient if we just want to write them to Blob Storage?\n. That sounds good @rgknp - does it work with Thrift? How are records written to Blob Storage - presumably N incoming records are written to each file - what is the file format for storing N records?\n. Great find @rgknp !\n. No timeframe, sorry - getting Kafka support for Snowplow is our first priority.\n. Hi @rgknp ! It's not yet on the roadmap. We are currently still at the point of trying to identify internal sponsors at Azure for our port. But did have a great meeting @ MS last week in London where we went through Azure SQL DW - very cool tech...\n. Sounds good!\n. Discussion migrated to http://discourse.snowplowanalytics.com/t/porting-snowplow-to-microsoft-azure/1178. Duplicate of #1793\n. Scheduling into atomic.events update\n. Per:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/outputs/EnrichedEvent.scala#L66\nThe underlying field is a Java Integer, values from -2147483648 to 2147483647, inclusive per:\nhttps://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.2\nThis maps onto a Redshift INTEGER, INT, or INT4, values -2147483648 to +2147483647, per:\nhttp://docs.aws.amazon.com/redshift/latest/dg/r_Numeric_types201.html#r_Numeric_types201-integer-types\n. Correct!\n. /cc @danisola \n. /cc @danisola \n. I thought about that idea, but I think there's too much scope for pilot error... Or to put it another way, I think there's a lot of overlap in the functionality but it's better to have a new enrichment which guides the user in how to do this (even if it uses a lot of the same functionality under the hood).\nAlso I am leaning towards implementing the 1. Configuration option first, as again it should reduce the complexity of the enrichment for users...\n. Also - the DynamoDB enrichment is somewhat easier because of the JSON support. For the HBase or Cassandra or MySQL cousin enrichments we would have to do a lot more hand-holding...\n. Duplicate of #2345, closing\n. Fixed! Thanks.\n. https://github.com/snowplow/snowplow/wiki/Clojure-collector\n. @danisola will work on this\n. Oh - I misunderstood you. We don't do tickets for a piece of software's own\nself-version bump, only its deps.\nOn 23 Jun 2015 4:40 pm, \"Josh\" notifications@github.com wrote:\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1806.\n. This is so that a downstream app can monitor the bucket for new runs which are safe to process\n. This is done\n. This can wait till R69\n. Yep please revert - let's keep this release Kinesis-only...\n. Thanks, scheduled\n. It's good thinking! Let's discuss on Monday\n. Thanks, see also: #1817. Please sign our CLA for contributing: https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed - thanks!\n. The stack trace is incomplete - it will continue on into the wait_for...\n. Here's another one @fblundun : \n\nIOError (Connection reset by peer): org/jruby/ext/openssl/SSLSocket.java:677:in `sysread_nonblock' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/shared/jopenssl19/openssl/buffering.rb:174:in `read_nonblock' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/protocol.rb:141:in `rbuf_fill' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/protocol.rb:122:in `readuntil' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/protocol.rb:132:in `readline' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/http.rb:2571:in `read_status_line' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/http.rb:2560:in `read_new' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/http.rb:1328:in `transport_request' org/jruby/RubyKernel.java:1270:in `catch' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/http.rb:1325:in `transport_request' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/http.rb:1302:in `request' /opt/snowplow-r64-palila/emr-etl-runner!/gems/rest-client-1.7.3/lib/restclient/request.rb:270:in `net_http_do_request' /opt/snowplow-r64-palila/emr-etl-runner!/gems/rest-client-1.7.3/lib/restclient/request.rb:418:in `transmit' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/1.9/net/http.rb:746:in `start' /opt/snowplow-r64-palila/emr-etl-runner!/gems/rest-client-1.7.3/lib/restclient/request.rb:413:in `transmit' /opt/snowplow-r64-palila/emr-etl-runner!/gems/rest-client-1.7.3/lib/restclient/request.rb:176:in `execute' /opt/snowplow-r64-palila/emr-etl-runner!/gems/rest-client-1.7.3/lib/restclient/request.rb:41:in `execute' /opt/snowplow-r64-palila/emr-etl-runner!/gems/rest-client-1.7.3/lib/restclient.rb:69:in `post' /opt/snowplow-r64-palila/emr-etl-runner!/gems/elasticity-4.0.5/lib/elasticity/aws_request.rb:34:in `submit' /opt/snowplow-r64-palila/emr-etl-runner!/gems/elasticity-4.0.5/lib/elasticity/emr.rb:17:in `describe_jobflow' /opt/snowplow-r64-palila/emr-etl-runner!/gems/elasticity-4.0.5/lib/elasticity/job_flow.rb:161:in `status' /opt/snowplow-r64-palila/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:352:in `wait_for' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts/method_reference.rb:46:in `send_to' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts.rb:305:in `call_with' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts/decorators.rb:159:in `common_method_added' /opt/snowplow-r64-palila/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:296:in `run' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts/method_reference.rb:46:in `send_to' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts.rb:305:in `call_with' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts/decorators.rb:159:in `common_method_added' /opt/snowplow-r64-palila/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:60:in `run' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts/method_reference.rb:46:in `send_to' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts.rb:305:in `call_with' /opt/snowplow-r64-palila/emr-etl-runner!/gems/contracts-0.7/lib/contracts/decorators.rb:159:in `common_method_added' file:/opt/snowplow-r64-palila/emr-etl-runner!/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in `(root)' org/jruby/RubyKernel.java:1087:in `load' file:/opt/snowplow-r64-palila/emr-etl-runner!/META-INF/main.rb:1:in `(root)' org/jruby/RubyKernel.java:1071:in `require' file:/opt/snowplow-r64-palila/emr-etl-runner!/META-INF/main.rb:1:in `(root)' /tmp/jruby2886024548669569113extract/jruby-stdlib-1.7.19.jar!/META-INF/jruby.home/lib/ruby/shared/rubygems/core_ext/kernel_require.rb:1:in `(root)'\n. Definitely something to explore! Created #2492, #2493\n. Do you have SSL validation switched on for your cluster? This PR: https://github.com/snowplow/snowplow/pull/1980\nhas been merged into R71. If you want to try out R71, there's a rc build already available on Bintray:\nhttps://bintray.com/snowplow/snowplow-generic/snowplow/r71-stork-billed-kingfisher/view#files\n. @fblundun - given that 3.8.0 is problematic and 3.6.0 is deprecated, maybe in 71 we change the recommendation to 3.7.0?\nAfter that, we'll leapfrog 3.8.0 and move to the 4.x series in due course.\n. Aha! Good to know...\n. This is now done - note the new version number\n. Thanks for raising Josh. Question - would this have been detected as stream lag? It sounds like it would have...\n. Pushing back to R69\n. This will ensure better type safety...\n. Hmm - how would the EtlPipeline as a class sit with Kinesis Enrich? Given that the output for Common Enrich won't be Redshift-formatted in a few releases' time anyway, I'm not too stressed about the conversion (in other words, soon we will have lots of conversions anyway).\n. Thanks for clarifying - yep - I think let's just make it a DateTime...\n. This is done, closing\n. This is done\n. That's a great idea. I suggest we ANALYZE COMPRESSION on a few users' atomic.events and then merge the results (because any one user won't be using all the fields)...\n. Scheduling\n. There is (currently) a sister ticket, #1849, in this milestone which will remove the JSON fields including contexts. So it's looking like this milestone will massively reduce Redshift space requirements...\n. Cool! This milestone should lead to big Redshift space savings...\n. Thanks for sharing @vceron !\n. Can we close this, superseded by R93?. Hi! This is the ticket: #1731. @fblundun - please let @danielzohar know when you would be starting on this (and thus whether it's worth him getting started on this or not).\n. See #24 for a detailed treatment of the problem\n. Sounds good!\nOn 6 Jul 2015 1:02 pm, \"Christophe Bogaert\" notifications@github.com\nwrote:\n\nThanks @alexanderdean https://github.com/alexanderdean.\nI discussed this with @yalisassoon https://github.com/yalisassoon this\nmorning, and the idea was to have this as a SQL Runner step after each\nload. Natural copies get deduplicated, while synthetic copies get moved to\na separate events table.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1866#issuecomment-118833774.\n. Link dead, closing.. Hey @fblundun - it's not a bad idea at all... My nervousness is that if we make the catch too expansive and there is some unrecoverable issue and the EmrEtlRunner just sits there retrying for ever.\n\nWhat about a ticket for the next CLI release where we:\n1. Catch all nonfatal\n2. Keep a counter of non-fatals in a row and die after N non-fatals in a row\n. Hey @jbeemster - I know you've looked into this. Would this work as something inside the sink itself, as @sdboer78 suggests? I suppose my concern is that the index is singular whereas there are N instances of the sink...\n. This sounds great - let's bring this forwards ;-)\n. Bringing forwards\n. We should auto-determine shard count for new index\n. What if we build instead a CLI wrapper around Curator: https://www.elastic.co/guide/en/elasticsearch/client/curator/4.0/about-origin.html We can still do the cool stuff like assess event volumes and use this to auto-determine shard count...\n. Stubbed https://github.com/snowplow/elasticsearch-housekeeper\n. Hey @christoph-buente - what you're saying makes sense. If we have an ES with an index being actively partitioned, then it makes sense for the Kinesis ES sink to be \"partition-aware\" and load into the correct partition.\nWould you be able to share the partitioning rules that you are using with curator? Are they defaults or customized by you?. This has been merged I believe\n. Good call\n. Thanks Fred\n. @fblundun - in the meantime (i.e. while waiting for the Elasticity 6 release), please get EmrEtlRunner upgraded to the latest Elasticity release (5.0.3), so that we can fix any Ruby dependency issues in good time.\n. Hey @fblundun - thanks for diving into this. I'll /cc @rslifka in this thread so that he is aware of the 5.0.3 issue we are having...\n. Cool, thanks @fblundun . BTW - I have pinged Rob to ask if there's any way we can help out on the Elasticity 6 release to get it out sooner...\n. Looks great Fred!\n. Thanks so much @rslifka!\n. /cc @bogaert \n. Nice!\n. Hmm, actually an API is just a server isn't it.\n. Agree - also we have the automated script so we can generate the array pretty easily. Great idea!\n. This can be done in R68 or R70 as you prefer @fblundun !\n. Could use HTTP or gRPC or Serf...\n. Hey Fred - good question. I think we should standardize on ProcessingMessages as they are richer...\n. Hey Eric - yes, sometimes S3DistCp hangs indefinitely. It's very unpredictable who it happens to (we have very rarely seen it among our Managed Service customers; there are some community members who experience it regularly). Please capture the logs for the hung jobflow and share it with your AWS support contact. Closing as this is a S3DistCp issue.\n. No problem, always good to raise things like this.\n. I think you're right - this has already been done, closing...\n. Sure, let's close. That's a really good point - with a mobile network's adult content detector, the first event is the synthetic copy, it's the second event which is the \"true\" event. But the first event will have already passed through and been e.g. sunk to Redshift. Needs further thought!\n. > What type of de-duplication window are you exploring? 2 mins? 2 hours?\nWe haven't looked at the time duplication of dupes yet, but it's pretty easy to imagine a pair of dupes which are 14 hours apart (send event, get on SF->LHR flight, land, send dupe).\nThe trouble with using Spark Streaming for de-duplication (vs using it for e.g. identity stitching or sessionization) is that we have to write the de-duplicated output somewhere, and if we write it to Kinesis then we are immediately introducing dupes again. My hunch is that de-duplication needs to be available as a library to be embedded inside any Kinesis app which actually cares about duplicates.\nFor example: the hypothetical Kinesis Redshift Sink would embed this de-duplication library so that no duplicates are written to Redshift...\n. Only loosely related, but we have actually implemented our first dedupe code here: #2246\n. Blocked by https://github.com/rslifka/elasticity/issues/121\n. Huge thanks for the contribution @denismo ! Please can DigDeep sign our CLA:\nhttps://github.com/snowplow/snowplow/wiki/CLA\nScheduling...\n. No, this is pretty stale now. Snowplow R89 (the next release) will port Hadoop Shred to Spark Shred. A release shortly after that will rewrite StorageLoader into Scala. Then we'll merge the two together, and after that we'll add a Kinesis source to the merged app. That's our dripfeeding strategy for Redshift - closing this PR.... Can we close this, superseded by R93?. Thanks\n. Sounds wise\n. Completely agree - I think it's odd having example files in resources...\n. Just #1865!\n. Hey @jramos! Thanks for this. Two quick things:\n- We have a strict 1 ticket - 1 commit - 1 changelog entry setup for Snowplow commits these days, and tickets can't straddle multiple Kinesis apps. For this PR to go in, create a ticket for each piece of functionality in it per app and make sure there is a single commit in your PR which maps onto it. Check out this page of the commit history to see the approach: https://github.com/snowplow/snowplow/commits/master?page=2\n- Can you sign our CLA please - https://github.com/snowplow/snowplow/wiki/CLA\n. Thanks @jramos !\n. This has been done in R70: https://github.com/snowplow/snowplow/blob/feature/r70/3-enrich/emr-etl-runner/config/config.yml.sample#L40\n. Depends on a new iglu-ruby-client: https://github.com/snowplow/iglu-ruby-client\n. Storage target linting dependent on release of R88 (@chuwy's). Hey @BenFradet - that sounds good I think. Do we add lint all to simplify things for a CI/CD script?. Yep that would be nice. @danisola - this looks great. Thanks so much for putting this together!\n1. @danisola - could you move the shredder into utils.shredder, just so the association between the Shredder and the TypeHierarchy is more obvious\n2. @fblundun - can you take a look at this PR and let us know any feedback!\n. Hey @danisola - ah yes, I think those schema-less contexts are from before we came up with self-describing JSON. So yep those tests just need to be updated to use any simple schema that is available in Iglu Central. Here is a super-simple schema I just published today:\nhttps://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/uri_redirect/jsonschema/1-0-0\n. Interesting idea but JSON validation is such a key part of the platform, I don't see that as workable in the medium term. If we can't validate people's JSONs, very soon we will be able to literally do nothing with their event payloads.\n. I wanted to get more of a handle on the historical raw data as raised by @fblundun:\n- 14 May 2013 we released a JS Tracker which supported unstructured events (name and properties), but no support for processing those unstructured events\n- 11 Apr 2014 we released a Snowplow version which wrote ue_name and ue_properties to atomic.events\n- 3 July 2014 we released a JS Tracker which replaced name+properties with self-describing JSON\n- 9 July 2014 we released a Snowplow version with automatic shredding into Redshift. This is essentially the version we support today\nSome observations:\n- The window where we had an operational, endorsed pipeline for unstructured events in the old format was actually very short (11 Apr - 3 Jul 2014)\n- We silently dropped ue_na from the Tracker Protocol v2 (not good practice)\n- We removed ue_na support from Common Enrich without any kind of migration path to self-describing events - presumably because we knew that a ue_na value wouldn't be a valid Iglu schema URI\nBased on this analysis, I think we go ahead with the change as planned. I think if somebody has a large historical archive of schema-less unstructured events which they can no longer re-process, the solution is a manual EMR job to turn those legacy JSONs into self-describing JSONs. If this is very widespread, we could even add a SnowplowAdapter which does that (it would be functionally similar to a MixpanelAdapter or KeenAdapter anyway).\n. I think this branch is fine for all of 'em!\n. @danisola - yep those migration versions sound good. Remember that because the new fields are at the end, the migration scripts can just use ALTER TABLE ADD COLUMN statements (would put them all in a single transaction)...\n. Legend! Thanks so much Dani\n. Good catch!\n. Right - this will be fixed in R72.\n. I think it's probably a good idea!\n. I think this is fine - these exceptions should be very rare, and they should always be fixed in a subsequent Snowplow release (i.e. from unexpected exception -> typed exception), so should be rarer still...\n. Updated title\n. Can you link to the cascading-user group thread?\n. This is a great idea! I would say this should be a new enrichment which outputs a dedicated new JSON. We need to come up with a good hashing algorithm. Assigning to @fblundun - Fred if we can get this into R71 that would be amazing...\n. Update: in the short-term this needs to go into atomic.events itself. This is because currently deduping is done in Redshift, and if this is in a separate table (a shredded context) then we have a chicken-and-egg situation where we have to join to dedupe, but we can't join because we have dupes.\n. Thanks @fblundun - with user_ipaddress and page_url, how difficult would it be to access the original QS values when generating the hash. I.e. think of it as a very \"early\" enrichment...\n. Why would you exclude dvce_sent_tstamp?\n. Makes sense\n. That sounds right to me. Note that if the same event with same event ID is sent by two different tracker instances*, there is a good chance the fingerprints will not match (because e.g. different tnas, or different auto-attached contexts), but think this is unavoidable...\n* https://github.com/snowplow/snowplow-android-tracker/issues/59\n. For this enrichment, I think let's have a couple of configuration options:\n\"hashAlgorithm\": \"xxx\"\n  \"excludeParameters\": [\"stm\"]\nObviously we will only support one hash algorithm for launch.\n. Migrated to https://github.com/snowplow/iglu-central/issues/525. Migrated to https://github.com/snowplow/iglu-central/issues/526. I think it would be a new field and fed into derived_tstamp\n. Note on this:\n\nThere should be a flag in a given tracker to specify whether a user-specified timestamp should be the true timestamp or device-created\n\nWhen a tracker has an event builder pattern, we can make the specification of true timestamp versus device-created on a per track() basis, which will be more elegant.\n. The problem is determining if an event is \"unauthorized\" or not. Our view is that you are better off determining that downstream where you have an audit trail for the events and if you make a mistake in your categorization, you can correct it (because you still have the raw events archived in S3).\nSee also: #1139\n. I think that's a good idea!\n. This becomes more important as more contexts move into derived_contexts (hence milestone).\n. Pushing back\n. Split this ticket into individual tickets for each tracker that is getting bumped.... Duplicate of #1777. Removing from milestone to prevent confusion\n. Awesome - thanks so much @dennisatspaceape! Scheduling.\n. This is done, closing.\n. @fblundun - can you review this click tracking PR please?\n. Now in the release branch\n. Agree, would be nice...\n. Updated ticket name to reflect this should live in Snowplow CLI...\n. Hey @fblundun - thanks for checking in on this. I think given that we can do a alter table atomic.events\nrename dvce_tstamp to dvce_created_tstamp;, we should go ahead and clean this up.\n(If Redshift didn't support col renames, I would have pushed this back to the non-additive Redshift milestone, https://github.com/snowplow/snowplow/milestones/atomic.events%200.8.0)\n. Phew! What a release Fred. Have now gone through the whole of it. Great work - feedback as above.\n. Thanks @fblundun !\n. Merged!\n. Not yet - if you could add that would be great. I said to @ihortom yesterday that we could mark all trackers as prod ready apart from AS3 and Unity, which we should flag as beta...\n. Awesome - thanks for sharing Eric!\nOn 26 Aug 2015 2:47 pm, \"Eric Pantera\" notifications@github.com wrote:\n\nHey @alexanderdean https://github.com/alexanderdean\nseems that the following approach is an elegant way to tune JVM;\n1) Create a custom bootstrap script file hosted in S3\neg:\ns3://viadeo-snowplow-processing/bootstrap-actions/snowplow-emr-customize-heap-size.sh\nwith content :\n!/bin/sh\n/usr/share/aws/emr/scripts/configure-hadoop -m mapred.child.java.opts=-Xmx2G\n2) Declare the custom bootstrap action script in the snowplow config file\n:emr:\n  ...\n  :bootstrap:\n    - s3://viadeo-snowplow-processing/bootstrap-actions/snowplow-emr-customize-heap-size.sh\nSimple and non intrusive - works well for us.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2000#issuecomment-135026196.\n. Good idea! How easy do you think this will be to do?\n. What's your idea here?\n. Note that when this is done, an upgrading user must switch to a new index in ES for their new-style bad rows, because https://github.com/snowplow/snowplow/issues/824#issuecomment-138937853\n\n/cc @fblundun @jbeemster \n. Thanks! Can you sign our CLA? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed - thanks so much!\n. Great idea! Here's the wiki page to get you started: https://github.com/snowplow/snowplow/wiki/How-to-integrate-a-webhook-into-Snowplow\n. (Note that adding webhooks to Snowplow is much, much simpler now that Schema Guru exists.)\n. Hey @christoph-buente - my suggestion would be to setup a Snowplow Clojure Collector in Beanstalk, then you'll get all the files landing in S3.\nA nice trick is to do the POST to <<collector_uri>>/com.shopify/v1, because that means that later you can use these same log files to test the webhook adapter running inside your Snowplow code.\n. Probably no need to have a separate URI for each webhook action - check out Schema Guru's schema-by CLI argument, which lets you use a property of each JSON instance to determine which JSON Schema this specific instance should help to derive.\n\nAnd then i strip out the column with base64 encoded JSON payload and pass it to the schema guru? \n\nYes you will need to do that. Depending on volumes, you could write a shell script or Hadoop job to extract the payloads...\n. Correct - you'll have a schema for each webhook event type. Once you've got your schemas ready to go, open the PR into Iglu Central and we can then arrange some time on the phone to talk you through the Scala webhook adapter part of what's needed.\n. How's it going @christoph-buente - have you had any luck schema'ing the Shopify webhooks?\n. Good question! It really depends on how complex the webhooks look to be, and how many edge cases they might have...\n. Hey @christoph-buente - almost always, a webhook event payload will contain some field in it which tells you the event type. I've decoded your attached Base64 payload, and I can't see the event type. This is very unusual.\nDo you agree that Shopify event payloads don't seem to specify what type of event they are? In that case, we may need to revert to an approach where you specify the path to each event type separately, eg:\n<<collector_uri>>/com.shopify/v1/add-to-cart\n<<collector_uri>>/com.shopify/v1/create-customer\n<<collector_uri>>/com.shopify/v1/...\nWhat do you think @christoph-buente ?\n. Yes, sorry - that is a little unusual of the Shopify guys (haven't seen it before)...\n. Hey @christoph-buente - this is an excellent start! The best next step is to open a PR into Iglu Central with all of these schemas, so we can review them.\nThe most important thing to do will be for you to manually inspect the JSON Schemas and tune them by hand - basically if you don't have a huge sample size of JSONs to feed into Schema Guru, then the JSON Schemas generated will tend to be a bit over-prescriptive. For example:\njson\n          \"vendor\" : {\n            \"type\" : \"string\",\n            \"minLength\" : 17,\n            \"maxLength\" : 29,\n            \"enum\" : [ \"Bauer Henrik Staar\", \"Bauer Bernd Schulz\", \"Bauer Jasper Metzger Petersen\", \"B\u00e4uerin Kirsten H\u00e4nsel\", \"Bauer Hanno Offen\" ]\n          },\nYou probably want to relax this to become:\njson\n          \"vendor\" : {\n            \"type\" : \"string\",\n            \"maxLength\" : 256\n          },\nBut yes - this is a great start. Exciting!\n. Looks like some of the payloads might be getting truncated - we have a ticket to investigate this, here #2201\n. Hmm - are you saying it was just a problem with your prior Base64 decoder?\n. Interesting - the first part makes sense: the collectors are wired to expect /foo/bar, not /foo/bar/bar, so you will get a 404. But the logging is orthogonal to the return code, so I wouldn't expect the 404 to result in a corrupted Base64 entry.\n\nNeither Linux+Mac commandline, openssl nor ruby clients could decode it successfully\n\nMost of these probably aren't using Url-safe Base64 decoding, which might account for the discrepancy.\n. Hey @christoph-buente ! It looks good. Could you give us a sense of how shallow/deep a set of source instances you were able to use to build these?\n. Hey @christoph-buente - this is a great dataset to get us started! I think we can progress with this - but we will probably want to wait until we have \"schema matching\" in place before we release it.\nI am going to write an RFC on schema inference (of which schema matching is the slightly primitive first step) hopefully soon.\n. Thanks @christoph-buente !. No this should be an app-wide config of a working directory. It's Kinesis\npipeline specific and orthogonal to any specific enrichment.\nA\nOn 1 Sep 2015 10:46 am, \"Fred Blundun\" notifications@github.com wrote:\n\nShould the MaxMind database configuration live in the ip_lookups\nconfiguration JSON\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/config/enrichments/ip_lookups.json\n?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2009#issuecomment-136652434.\n. Hey @fblundun - the %3D decoder is an artifact from when Scalding couldn't handle = signs in incoming arguments - it's fine to exorcise.\n. So elegant!\n. Bringing forwards\n. @fblundun - has this one been done?\n. This branch has been merged now!\n. That's odd...\n. Good thinking...\n. Thanks! Can you sign our CLA please? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed! Thanks so much\n. Bringing forward as this is happening quite frequently...\n. This is happening quite frequently now. It tends to happen more on the archive_enriched step than the archive_raw, but this may just be a result of larger #s of file moves at that step.\n. Note that restarting the job from the failed step always works, so I'm pretty sure it's just an intermittent failure bug.\n. We no longer use Fog for file moves, closing. Believe this is a dupe and can be closed...\n. What is the \"size\" field - do you have a link to it @fblundun ?\n. Initial format likely to be something like:\n\njson\n{\n  \"schema\": \"xxx\",\n  \"data\": {\n    \"version\": \"x.x.x\",\n    \"value\": \"xxx\"\n  }\n}\n. Suggest splitting in two: 1) remove JSON length checks, 2) move truncation code to shredder\n. Validating contexts in Common Enrich is important - it's part of our move\nto fully validating our raw event early/eagerly. It's not storage target\nrelated. Remember that our enriched event in S3 is becoming our canonical\nformat, so needs to be fully validated.\nOn 9 Sep 2015 9:59 am, \"Fred Blundun\" notifications@github.com wrote:\n\nr72 will move some shredder logic into common enrich to get the event\nname, vendor, etc from unstructured event JSONs. As a side effect this will\ncheck whether unstructured events are valid JSONs. Should we continue to\ncheck whether contexts are valid JSONs in common enrich, or should that be\nthe responsibility of later parts of the pipeline (currently hadoop shred\nand kinesis es sink)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2041#issuecomment-138840200.\n. Logic will be moved out of Common Enrich as it is storage-target specific (actually Postgres-specific - Redshift COPY supports TRUNCATECOLUMNS).\n. Duplicate of https://github.com/snowplow/snowplow-android-tracker/issues/167\n. Great idea\n. One way we could do this:\n\nWhen we create our own JSON Paths file format (i.e. outgrowing the Redshift format), each entry could be a tuple2, where the first item is the JSON Path, and the second item is the default.\n. Name's a bit cryptic?\n. Agree!\n. Makes sense\n. Wow - okay!\n. @yalisassoon, @fblundun, @chuwy - any feedback on this enrichment specification? It is subject to change when we get feedback from a potential user this/next week, but would be good to get any initial feedback...\n. Can you give an example @chuwy ?\n. Right - this enrichment is designed for much simpler, probably RESTful, APIs. Anything with a complex interaction pattern like exchange rates or weather, yes they will still need dedicated enrichments...\n. How to handle cache expiry X minutes after write:\nhttp://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/cache/CacheBuilder.html#expireAfterWrite%28long,%20java.util.concurrent.TimeUnit%29\n. @chuwy I updated the section on auth:\n\nCurrently the only supported authentication option is http-basic: provide a username and/or a password for the enrichment to use to connect to your API using basic access authentication. Some APIs overload only the username or password field with an API key, so we support only supplying one of these; set the other to null.\n. Changed the schemaCriterion to:\n\n\"schemaCriterion\": \"iglu:com.snowplowanalytics.snowplow/client_session/jsonschema/1-*-*\"\n. @chuwy - I don't see a commit referencing this ticket yet...\n. Cool idea - create a separate ticket to explore this!\n. Renamed to API Request Enrichment\n. @chuwy - can we change \"http-basic\" to \"httpBasic\" for consistency?\n. I've updated it to say httpBasic\n. This is for the input keys right? I agree - we should use java.net.URLEncode everywhere...\n. R79 should be out sometime in March... Hadoop release to start with, as usual.\n. Hopefully not too long after - we are trying to pick up the pace a bit on the Kinesis releases, especially given that the Kinesis pipeline will power Snowplow Mini...\n. Sneak peak: https://github.com/snowplow/snowplow-mini\n. Open question: do we want to allow a wildcarded MODEL version for an input's schema criterion? I found I wanted it in my tutorial (because both com.snowplowanalytics.snowplow-website/trial_form_submitted/jsonschema/1-0-0 and com.snowplowanalytics.snowplow-website/trial_form_submitted/jsonschema/2-0-0 in fact have an email field.\nI think we probably want to support this...\n. Yes please @chuwy! Let's implement it. I need to do a new Iglu Central release for the Clearbit tutorial anyway...\n. Great first spike! Feedback:\n- We don't want to encourage event mutation in place - enrichments should add to the derived contexts array. The \"urge\" to mutate will decrease as more and more of the fat jar moves out into JSONs anyway. Eventually we will allow a special class of enrichment which is allowed to destroy/mutate an event (e.g. the AnonIpEnrichment)\n- Part of the appeal of this enrichment is that you can write it in any fatjarable JVM language and include new dependencies, so it would be nice to have test implementations in Java and Clojure which use an unusual dependency. Also there shouldn't be an assumption of json4s in the API anywhere - JsonNode should be the lingua franca\n- It would be nice if the enrichment supported arguments being passed from the config to the method, rather than having to hard-code all config in the method. Perhaps the method has a second argument which is a JsonNode (remember, shouldn't be Scala-specific), and then in the JSON config you can specify the argument parameters\n. Nice. Can we express this as a Java interface rather than Scala trait as\nthe lowest common denominator between Java/Clojure/Scala/Kotlin?\nOn 14 Sep 2015 11:25 am, \"Fred Blundun\" notifications@github.com wrote:\n\nThat all sounds good... So IUserEnrichment would look like this:\ntrait IUserEnrichment {\n  @throws[Exception]\n  def configure(config: JsonNode): Unit\n@throws[Exception]\n  def createDerivedContexts(event: EnrichedEvent): Array[JsonNode]\n}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2054#issuecomment-140030051.\n. All good suggestions! This problem should diminish over time as EnrichedEvent becomes more strongly typed and Avro-ish, but agree we would need to do something in the short term.\n\nI think the first suggestion is fine - as the EnrichedEvent is steadily shorn, there will be less and less that an enrichment can do without using the existingContexts, so any option to ignore contexts will become less and less relevant. Similarly, I think asking the user to do deserialization isn't the right direction of travel, given that we already have a deserialized version of the contexts available in Common Enrich (because we are validating them). \n. Note that calling Clojure from Java even with a well-defined interface is somewhat involved:\nhttp://stackoverflow.com/questions/8614734/how-do-i-implement-a-java-interface-in-clojure\nTherefore, we will explicitly exclude Clojure support from this enrichment in favour of #2056. Scala, Java and Kotlin support (http://kotlinlang.org/docs/reference/java-interop.html#calling-kotlin-code-from-java) should all still be fine.\n. Hey @fblundun ! A couple of thoughts on this:\n- Can we come up with a simple example that flexes all the functionality we're going to support?\n- Can we add this into: https://github.com/snowplow/snowplow-java-example-enrichment\n. Good catch!\n. Awesome - thanks so much @fblundun !\n. I think in a Docker world the idea of having the adapter or enricher connected over the network at runtime is much nicer than modifying the build artifact.. I think you're right...\n. Hey @bogaert - my inclination would be to remove that page until as/when we have a Snowplow Event Studio that it can be a part of - but see what @yalisassoon thinks...\n. Thanks @bogaert!\n. It's a good question! If bad rows were events (which they will become one day), then I would say that failure_tstamp would become the true_tstamp for the event, rather than the etl_tstamp. I agree however that it would be very helpful to have the etl_tstamp in the bad row event as well.\nTherefore, I would not set failure_tstamp to etl_tstamp, but I think there is a good case for:\n1. Accelerating turning the bad rows into proper events, and/or\n2. Attaching etl_tstamp to the bad rows and renaming failure_tstamp to true_tstamp\n. Thanks! Have you signed our CLA? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed! Thanks so much\n. Duplicate of #1967 \n. Duplicate of #1443\n. Duplicate of #1445 \n. Migrated to https://github.com/snowplow/iglu-central/issues/520. Duplicate of #1444 \n. Duplicate of #1966\n. Good thinking\n. This looks good! Thanks so much @kazjote. Do we want to support some kind of wildcard \"*\" option for people who want to capture all cookies?\n. Hi @kazjote - okay cool. I've created a ticket to re-consider this wildcard option at some future point: #2075\n. This has gone into the branch for release, closing.\n. Looks great - feedback is just a few new comments to add...\n. Ace - thanks\n. Buggy openjdk?\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Is this a regression in Contracts (or somehow JRuby related)? It used to at least print out the failing hash and associated contract...\n. Agree - shall we do a PR into Contracts?\n. Sounds good!\n. I think we can probably get away with just \"instance_identity\" - i.e. the document is just a way of requesting the JSON version?\n. Okay cool - go with \"instance_identity_document\" then.\n. @yalisassoon - yes indeed, nuid is currently being used. Your explanation of why we should exclude it makes perfect sense - I am updating the release and blog post to exclude nuid...\n. Removed from the example config, closing...\n. Agree, let's disable cv. I've added this to the blog post... @fblundun please update the example JSON config and rebase...\n. Thanks!\n. The event_id (\"eid\") is pretty definitely not an input. I think this is fine - for snplow2 it would be very rare to have an event which is duplicated prior to the attachment of the event ID. Closing...\n. Interesting... Thanks @bogaert\nOn 8 Oct 2015 3:09 pm, \"Christophe Bogaert\" notifications@github.com\nwrote:\n\nJust as an FYI. There are rare cases where events have the same\nevent_fingerprint but a different event_id. I came across events sent in\nby this bot:\nMozilla/5.0 (compatible; Yahoo! Slurp;\nhttp://help.yahoo.com/help/us/ysearch/slurp)\nMost fields (including dvce_created_tstamp) were NULL, except for\nip_address and useragent (thus producing the same fingerprint).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2088#issuecomment-146557013.\n. Agree!\n. Is this ready for release @bogaert ?\n. Cool, thanks for the update\n. Great stuff!\n. Hey @xkidro - what's the particular problem you are having? We haven't used the Elasticsearch Service yet...\n. No, it's still planned, but not yet scheduled...\n. Duplicate of https://github.com/snowplow/kinesis-s3/issues/64\n. Thanks! @bernardosrulzon - have you signed our CLA? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed! Thanks so much.\n. I like it! Less magic/secret knowledge in the Snowplow CLI is the right direction of travel...\n. Thanks @mattmueller ! We'll get this fixed...\n. Bringing forwards as pretty straightforward add\n. Interesting!\n. Closing, brain freeze.\n. One advantage of using lzo index everywhere is that we could do a run over both the compacted archive and the recent (post-archive) files from Kinesis S3.\n. Thanks @pkallos, that's good to know. How did you disable the writing of the index files? Are the files still processable by Hadoop Enrich without them?\n\nIt feels like the lzo.index files are useful if e.g. you have a compacted archive with only one file per hour, but much less useful as originally generated by Kinesis Enrich...\n. Good to know, thanks! And does Hadoop Enrich happily process the format if the index files are missing, or did you have to fork that, or were you using a different process?\n. Thanks @pkallos - that's helpful. I am going to rename this ticket to upgrade elephant-bird to 4.10, and I'll create a separate ticket in kinesis-s3 to add a lzo mode (i.e. lzo not lzo-index)...\n. These have been cherry-picked in, closing...\n. /cc @chuwy\n. Particularly want to check the EC2 context is working well...\n. /cc @chuwy for investigation\n. Is there a commit for this?\n. Right - so we de-schedule and close as dupe of #2024\n. Hey @bogaert - can you rebase this off master please (it's currently 70-odd commits behind master). @fblundun can show you how to rebase if unclear...\n. Good catch\n. Sounds fine!\nOn 21 Dec 2015 7:24 p.m., \"Fred Blundun\" notifications@github.com wrote:\n\nWe decided that EER and SL aren't going to be combined in a single app\nafter all, meaning that it won't be necessary to wait for all the SL code\nto load before EER does anything. So the startup time is not going to be as\nproblematic as we thought. I think we can postpone this ticket.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2117#issuecomment-166364821.\n. Closing as not important. Hi @xkidro!\n\nIn terms of better bot detection - the people to speak to are the IAB/ABC, who maintain a list: http://www.iab.com/guidelines/iababc-international-spiders-bots-list/ It would be cool to have a Snowplow Enrichment which used the IAB/ABC bots and spiders list.\n\nFor me having bots in the stats seems pretty pointless since I need to exclude them, they shouldn't be there in the first place.\n\nLots of users don't want the bots in their event stream, but of course some do - in fact some are using Snowplow only to detect bots.\n\nit would be better if it was possible to reject or send them to the bad kinesis stream\n\nWe have thought about adding filtering capabilities to Kinesis Enrich, but it a) makes the event stream harder to audit and b) breaks the Unix philosophy of small stream workers which do one thing well. Instead, we are planning on creating a dedicated forker/filterer for Kinesis, here: https://github.com/snowplow/kinesis-tee\n. Duplicate of #937, closing. Note - there is a performance optimization to be done in this upgrade script, which is to drop the JSON columns before doing the INSERT INTO SELECT.\n. Love it!\n. Hi @jramos - the records in the bad stream will tell you why they are failing! The JSONs contain the raw event and the failure messages. You can load these into Elasticsearch using the kinesis-elasticsearch-sink for analysis. There is nothing meaningful logged to stdout/err around individual event failures.\n. That's odd - thanks for raising. We don't use PG much so it's quite possible a regression has slipped in. Assigning to our new CLI release, please add more info when you have it.\n. Thanks\n. Interesting! What's the rationale for disabling the truncation?\n. Ah - got it.\nThings are a bit different in Snowplow - the columns you cared about would be modeled in JSON Schema as either events or custom contexts, and you should certainly put maxLength constraints on those (and Schema Guru would use those maxLength constraints to auto-generate the appropriate varchar(x) in Redshift). Then if an outsize value came in, the failure would be caught much earlier - in the Enrichment process when JSONs are validated against their JSON Schemas, before reaching any database.\nA couple of other points on this:\n- We plan to migrate the remaining \"legacy\" fields in atomic.events out into new JSON Schemas over time, so over time all fields should be enforceable with maxLength\n- Even if we added this, the fields would be truncated anyway by the Postgres truncation code, which is applied to all events (because we can't distinguish between storage targets in Hadoop Shred yet) [1]\nSo I don't think it makes sense to merge, but leaving open for others' thoughts...\n[ 1 ] https://github.com/snowplow/snowplow/pull/2074/files#diff-12ac1a3e8840ae401be7e730e9ebd085R135\n. Closing as there was no follow-up discussion on this one.... Thanks @chuwy !\n. Thanks for capturing all this @chuwy !\n. Augmenting the setup guide with aws cli commands is a great idea - assigning to @ihortom ...\n. Awesome - pushed! Thanks so much @ihortom \n. Thanks for these @fblundun ! Could you rerun against Schema Guru 0.4.0 and just check things are the same / improved? https://bintray.com/snowplow/snowplow-generic/schema-guru/0.4.0/view#files\n. Closing as merged into release branch now...\n. Okay - in the future, snowplow-user is the place for these support requests.\n. Duplicate of #2089\n. Hey @epantera - thanks for this. Your use case makes a lot of sense and I can see how this setting will work for you with your current setup.\nThe difficulty with using JSON Paths files being present as a determinant of what to load into Redshift is that we are slowly re-architecting towards auto-generation of JSON Paths files (and Redshift DDL) in Iglu server, on demand as required. This step will massively simplify the setup and maintenance of a company-wide self-describing JSON dictionary; it will also make the load process less fragile and finally make it possible to drip-feed Redshift robustly from Kinesis.\nYour use case of wanting to not load all JSON entities into a given Redshift database is a 100% valid one. But my hunch is that the right long-term approach to this is some kind of whitelist and/or blacklist filters which can be configured against the storage target...\nDoes that make sense?\n. Hi @epantera - all sounds good.\nI don't have any strong thoughts on the filtering implementation yet - but a couple of points:\n1. We will be working on the new Redshift/JSON Paths functionality in Iglu in a set of internal (\"ghost\") releases - you can see the first one here: https://github.com/snowplow/iglu/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Scala+repo+server+0.2.0-g1%22\n2. We are fairly quickly outgrowing embedding our storage target configurations in the config.yml. The plan (not yet written down anywhere) is to move these out into individual JSON configurations, similar to how we configure the enrichments\n. Update: I'm not sure the TCP keep-alive settings in the kernel make a material difference...\n. That would be seriously cool! Using actors would be a great approach to handling async event emitting to Snowplow.\nI have stubbed the repo: https://github.com/snowplow/snowplow-erlang-tracker\nAnybody interested in working on this - please follow up in this thread and we can arrange permissions on that repo for you...\n. Won't implement (0.4.0 to 0.8.0 instead).\n. Thanks @miike ! Can you sign our CLA: https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed! Thanks so much.\n. Sorry @chuwy could we bring this one forwards into R79? It should be a one-line fix.\n. Changed the title!\n. I think this makes sense - I think the missing piece is optimizing for the scenario where many events have the exact same values for latitude and longitude and relatively low variance of event time.\nFor example, it is highly likely that a user will have the same IP address for all events in a session, and thus will have the same longitude and latitude for that whole session.\nIn this scenario, it seems a shame to be potentially checking 27 keys per event just to retrieve the same value across 10s or 100s of events.\n. To put it another way: we need to make sure we optimize the cache intra-user as well as inter-user.\n. I think that's survivable, because the event fingerprint doesn't include derived contexts... (Therefore deduplication won't be affected.)\n. Can you follow up on this in the Kinesis Connectors Library and cross-link the tickets?\n. Hey @xkidro - using the Kinesis Firehose instead of the kinesis-s3 app is an interesting idea! It's not something we have the bandwidth to investigate at this time.\nIf you get a chance to experiment with this approach, please update this ticket with your findings!\n. Hi @bcharp - using Kinesis Firehose instead of kinesis-s3 is not something we are actively exploring.\nI don't believe Kinesis Firehose can work with binary formats like Thrift - if I am wrong please let me know. In the absence of that, we would have to re-engineer the Stream Collector, Hadoop Enrich and Stream Enrich to move to a textual format like JSON for raw events, and this would of course increase the size of our raw events in Kinesis and S3, plus slow down enrichment (because JSON de/serialization is slow).\nRelated, our kinesis-s3 output format is optimized for fast Hadoop processing - again we would lose this in the switch to Kinesis Firehose.\nA Snowplow real-time topology ends up with a lot of different moving parts - porting to Firehose to remove one moving part doesn't feel like a win given the compromises we would have to make. But certainly happy to hear a counter-view...\n. > As snowplow is always using the most standard way to do things with AWS\nOne of the nice things about AWS is that there isn't \"the most standard way to do things\" - instead AWS typically offers a variety of different strategies, which tend to vary based on ease of use vs configurability.\nThe trend with Snowplow is often to start with the most-packaged approach, and then move towards the most-configurable approach as we outgrow the most-packaged approach. You can see this with collectors:\n1. CloudFront Collector, super easy to use, very inflexible\n2. Clojure Collector, still easy to deploy using Beanstalk, more functionality but no control over logfile rotation and some Beanstalk behaviors are a little mystical\n3. Stream Collector - very flexible, not even AWS specific but can be deployed on AWS using ELB and an ASG\nIn terms of streaming events from the real-time pipeline into Redshift - this is something we are working on. We are doing a lot of work to extend Iglu and StorageLoader so that they can handle automatic table creation, table migrations etc. These are tough problems but will be very exciting to solve!\n. Thanks for sharing @johnrobertfitz !\n. Thanks for sharing @fingerco !. 18 months later and Kinesis Firehose is still only available in 3 regions, closing. That's right @jdub - Snowplow S3 Loader continues to be our focus.. Thanks for raising @timelf123 ! Scheduling...\n. Cool, bringing forwards...\n. Here are some sample events:\n```\n {\"line\":\"2016-03-08\\t18:12:22\\t-\\t-\\t54.205.100.220\\tPOST\\t54.205.100.220\\t/io.statusgator/v1\\t200\\t-\\tRuby\\t&cv=clj-1.1.0-tom-0.2.0&nuid=9bcdd07a-be54-4379-9934-6292712396de\\t-\\t-\\t-\\tapplication%2Fx-www-form-urlencoded\\tc2VydmljZV9uYW1lPVhlcm8mZmF2aWNvbl91cmw9aHR0cHMlM0ElMkYlMkZkd3hqZDljZDZyd25vLmNsb3VkZnJvbnQubmV0JTJGZmF2aWNvbnMlMkZ4ZXJvLmljbyZzdGF0dXNfcGFnZV91cmw9aHR0cCUzQSUyRiUyRnN0YXR1cy5kZXZlbG9wZXIueGVyby5jb20lMkYmaG9tZV9wYWdlX3VybD1odHRwcyUzQSUyRiUyRnd3dy54ZXJvLmNvbSZjdXJyZW50X3N0YXR1cz1kb3duJmxhc3Rfc3RhdHVzPXVwJm9jY3VycmVkX2F0PTIwMTYtMDMtMDhUMTglM0ExMiUzQTIxJTJCMDAlM0EwMA\",\"errors\":[{\"level\":\"error\",\"message\":\"Payload with vendor io.statusgator and version v1 not supported by this version of Scala Common Enrich\"}],\"failure_tstamp\":\"2016-03-09T07:32:22.579Z\"}\n{\"line\":\"2016-03-08\\t18:17:27\\t-\\t-\\t54.196.158.208\\tPOST\\t54.196.158.208\\t/io.statusgator/v1\\t200\\t-\\tRuby\\t&cv=clj-1.1.0-tom-0.2.0&nuid=44b89d12-e006-401e-84f8-51284b0efc99\\t-\\t-\\t-\\tapplication%2Fx-www-form-urlencoded\\tc2VydmljZV9uYW1lPVhlcm8mZmF2aWNvbl91cmw9aHR0cHMlM0ElMkYlMkZkd3hqZDljZDZyd25vLmNsb3VkZnJvbnQubmV0JTJGZmF2aWNvbnMlMkZ4ZXJvLmljbyZzdGF0dXNfcGFnZV91cmw9aHR0cCUzQSUyRiUyRnN0YXR1cy5kZXZlbG9wZXIueGVyby5jb20lMkYmaG9tZV9wYWdlX3VybD1odHRwcyUzQSUyRiUyRnd3dy54ZXJvLmNvbSZjdXJyZW50X3N0YXR1cz11cCZsYXN0X3N0YXR1cz1kb3duJm9jY3VycmVkX2F0PTIwMTYtMDMtMDhUMTglM0ExNyUzQTI2JTJCMDAlM0EwMA\",\"errors\":[{\"level\":\"error\",\"message\":\"Payload with vendor io.statusgator and version v1 not supported by this version of Scala Common Enrich\"}],\"failure_tstamp\":\"2016-03-09T07:32:22.634Z\"}\n{\"line\":\"2016-03-08\\t22:06:38\\t-\\t-\\t54.196.158.208\\tPOST\\t54.196.158.208\\t/io.statusgator/v1\\t200\\t-\\tRuby\\t&cv=clj-1.1.0-tom-0.2.0&nuid=35b2266e-db75-4eb3-86ec-914432b7d461\\t-\\t-\\t-\\tapplication%2Fx-www-form-urlencoded\\tc2VydmljZV9uYW1lPUFtYXpvbitXZWIrU2VydmljZXMmZmF2aWNvbl91cmw9aHR0cHMlM0ElMkYlMkZkd3hqZDljZDZyd25vLmNsb3VkZnJvbnQubmV0JTJGZmF2aWNvbnMlMkZhbWF6b24td2ViLXNlcnZpY2VzLmljbyZzdGF0dXNfcGFnZV91cmw9aHR0cCUzQSUyRiUyRnN0YXR1cy5hd3MuYW1hem9uLmNvbSUyRiZob21lX3BhZ2VfdXJsPWh0dHAlM0ElMkYlMkZhd3MuYW1hem9uLmNvbSUyRiZjdXJyZW50X3N0YXR1cz11cCZsYXN0X3N0YXR1cz13YXJuJm9jY3VycmVkX2F0PTIwMTYtMDMtMDhUMjIlM0EwNiUzQTI4JTJCMDAlM0EwMA\",\"errors\":[{\"level\":\"error\",\"message\":\"Payload with vendor io.statusgator and version v1 not supported by this version of Scala Common Enrich\"}],\"failure_tstamp\":\"2016-03-09T07:32:56.406Z\"}\n{\"line\":\"2016-03-08\\t21:17:29\\t-\\t-\\t54.196.158.208\\tPOST\\t54.196.158.208\\t/io.statusgator/v1\\t200\\t-\\tRuby\\t&cv=clj-1.1.0-tom-0.2.0&nuid=b0f816f2-a0d2-4217-9a41-f224fe3ff62f\\t-\\t-\\t-\\tapplication%2Fx-www-form-urlencoded\\tc2VydmljZV9uYW1lPUFtYXpvbitXZWIrU2VydmljZXMmZmF2aWNvbl91cmw9aHR0cHMlM0ElMkYlMkZkd3hqZDljZDZyd25vLmNsb3VkZnJvbnQubmV0JTJGZmF2aWNvbnMlMkZhbWF6b24td2ViLXNlcnZpY2VzLmljbyZzdGF0dXNfcGFnZV91cmw9aHR0cCUzQSUyRiUyRnN0YXR1cy5hd3MuYW1hem9uLmNvbSUyRiZob21lX3BhZ2VfdXJsPWh0dHAlM0ElMkYlMkZhd3MuYW1hem9uLmNvbSUyRiZjdXJyZW50X3N0YXR1cz13YXJuJmxhc3Rfc3RhdHVzPXVwJm9jY3VycmVkX2F0PTIwMTYtMDMtMDhUMjElM0ExNyUzQTE2JTJCMDAlM0EwMA\",\"errors\":[{\"level\":\"error\",\"message\":\"Payload with vendor io.statusgator and version v1 not supported by this version of Scala Common Enrich\"}],\"failure_tstamp\":\"2016-03-09T07:33:44.774Z\"}\n{\"line\":\"2016-03-08\\t09:12:06\\t-\\t-\\t54.145.71.82\\tPOST\\t54.145.71.82\\t/io.statusgator/v1\\t200\\t-\\tRuby\\t&cv=clj-1.1.0-tom-0.2.0&nuid=5388014d-54e9-4f57-a339-e1bf894e1189\\t-\\t-\\t-\\tapplication%2Fx-www-form-urlencoded\\tc2VydmljZV9uYW1lPUFtYXpvbitXZWIrU2VydmljZXMmZmF2aWNvbl91cmw9aHR0cHMlM0ElMkYlMkZkd3hqZDljZDZyd25vLmNsb3VkZnJvbnQubmV0JTJGZmF2aWNvbnMlMkZhbWF6b24td2ViLXNlcnZpY2VzLmljbyZzdGF0dXNfcGFnZV91cmw9aHR0cCUzQSUyRiUyRnN0YXR1cy5hd3MuYW1hem9uLmNvbSUyRiZob21lX3BhZ2VfdXJsPWh0dHAlM0ElMkYlMkZhd3MuYW1hem9uLmNvbSUyRiZjdXJyZW50X3N0YXR1cz11cCZsYXN0X3N0YXR1cz13YXJuJm9jY3VycmVkX2F0PTIwMTYtMDMtMDhUMDklM0ExMSUzQTQ1JTJCMDAlM0EwMA\",\"errors\":[{\"level\":\"error\",\"message\":\"Payload with vendor io.statusgator and version v1 not supported by this version of Scala Common Enrich\"}],\"failure_tstamp\":\"2016-03-09T07:34:05.649Z\"}\n{\"line\":\"2016-03-08\\t07:31:40\\t-\\t-\\t54.145.71.82\\tPOST\\t54.145.71.82\\t/io.statusgator/v1\\t200\\t-\\tRuby\\t&cv=clj-1.1.0-tom-0.2.0&nuid=55120e08-748a-4372-b1c0-7997de876d13\\t-\\t-\\t-\\tapplication%2Fx-www-form-urlencoded\\tc2VydmljZV9uYW1lPUFtYXpvbitXZWIrU2VydmljZXMmZmF2aWNvbl91cmw9aHR0cHMlM0ElMkYlMkZkd3hqZDljZDZyd25vLmNsb3VkZnJvbnQubmV0JTJGZmF2aWNvbnMlMkZhbWF6b24td2ViLXNlcnZpY2VzLmljbyZzdGF0dXNfcGFnZV91cmw9aHR0cCUzQSUyRiUyRnN0YXR1cy5hd3MuYW1hem9uLmNvbSUyRiZob21lX3BhZ2VfdXJsPWh0dHAlM0ElMkYlMkZhd3MuYW1hem9uLmNvbSUyRiZjdXJyZW50X3N0YXR1cz13YXJuJmxhc3Rfc3RhdHVzPXVwJm9jY3VycmVkX2F0PTIwMTYtMDMtMDhUMDclM0EzMSUzQTIzJTJCMDAlM0EwMA\",\"errors\":[{\"level\":\"error\",\"message\":\"Payload with vendor io.statusgator and version v1 not supported by this version of Scala Common Enrich\"}],\"failure_tstamp\":\"2016-03-09T07:34:46.762Z\"}\n```\n. Wow! Looks great Ed...\n. Hi @bernardosrulzon - no, this PR will silently drop those. I'll create a ticket to discuss this: #2202\n. Duplicate of #2798\n. Moving this ticket into Ed's release\n. Update: we have had a customer report that they have successfully had 2Mb POSTs (from Mandrill) landing in their Clojure Collector, but equally they seem to be experiencing some truncations at 72k (could be truncated by Mandrill?).\n. Assigning to @jbeemster - I'd like this issue bottomed out as part of the Avalanche work...\n. Worth testing this on Scala Stream Collector too - if a max post size is found there, it could be suggestive of an ELB limitation...\n. Please share!\n. Yep that's fine.\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. [16:47] Ed Lewis: @Deano do you have something in mind for https://github.com/snowplow/snowplow/issues/2202 ?\n[16:52] Alex Dean: Hello @ed!\n        So it's not super easy because the unique args are not well enveloped\n        they are in the root of the events!\n[16:53] Ed Lewis: yeah! They come out as just extra keys tagged on\n[16:53] Alex Dean: I suspect to support this will be a big job\n        It will involve having a config for the adapter\n[16:53] Joshua Beemster: We could enforce a prefix?\n[16:54] Alex Dean: where the user configures which unique args should be looked for and added into custom contexts\n        at some point, it's easier just to add a separate layer of metadata\n        than to make people contort their additional data so it works in Snowplow\n[16:54] Joshua Beemster: Fair\n[16:55] Alex Dean: it will be quite powerful though\n        and in line with the configs we will be adding for other adapters like Segment\n        but yes definitely out of scope for now\n. Hey @andrioni - no current plans. Are you thinking core.async and clj-http?\nIf you are interested, let me know and I can create a Snowplow repo and give you perms...\n. Stubbed! Good luck with it\nhttps://github.com/snowplow/snowplow-clojure-tracker\n. Thanks, scheduling...\n. This is an easy one @ihortom \n. Please continue on your local fork @ihortom - we'll work out a way of merging them in once you have a set of tickets done...\n. Fixed by @ihortom, closing\n. Duplicate of #2155\n. Right! Thanks Fred\n. Hi @derdeka - thanks for raising. We are already using almost the latest version of Elasticity (6.0.3, with 6.0.5 under test internally as part of R73), but there may be a stray EMR API call or two which need updating. Assigned to R74 to investigate; we'll make sure to get R74 out before the AWS deadline of 12/14/2015.\n. @rslifka - any idea what the legacy API calls could be?\n. I don't think there are any legacy calls - neither in EmrEtlRunner nor Elasticity. I believe if you are running Release 68 Turquoise Jay or above, the deprecation on 12/14/2015 will not affect you.\nKeeping this ticket open until we confirm this.\n. See https://github.com/rslifka/elasticity/issues/88\n. Thanks for confirming @rslifka! We are checking some of our internal monitoring tools for any cheeky DescribeJobFlows :-) I think we can safely close this anyway.\n. Hi @bcharp - sure thing, we will upgrade the version as part of #1568\n. Cheekily bringing forwards...\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Thanks @fblundun, great infosec catch!\n. Thanks @fblundun, great infosec catch!\n. Hey @chuwy - has this been done in the re-write?. So the RDB Loader doesn't use prepared statements?. Sure, closing!\n. /cc @danisola \n. Thanks @iaingray !\n. @ihortom can you please update?. Hey @fblundun - one thing: the fatjar created by assembly is called snowplow-hadoop-elasticsearch-sink-0.1.0.jar, but EmrEtlRunner specifies it as hadoop-elasticsearch-sink-0.1.0.jar to EMR.\n. Sounds good, thanks Fred...\n. This is a won't fix, the collector_tstamp should not be used for analytics anyway (use the derived_tstamp).. This would allow eg:\n\nWrite a JavaScript function that inspects the error messages on the bad rows, and where it sees error messages that indicate that a given field caused an issue, mutate the given field to a safe value\n. Thanks for diagnosing all this @fblundun . @mitulshah44 - any further support requests to the Snowplow user group snowplow-user@ please. Closing\n. Thanks @iaingray !\n. Pushed!\n. Hey @fblundun - neat trick re. ERB templating!\n\nAgree, let's use \"iam\" to signify getting the roles from IAM.\n. Neither of those solutions will work for the use case we want, which is\ninvoking StorageLoader inside Hadoop via script-runner.jar - fundamentally\nwe have to pass the config as an argument to a jobflow step via the AWS\nAPI, so there is no notion of a config file at that point.\nWhy don't we add an alternative to the existing --config arg which is\n--inline-config and is followed by the encoded string?\nOn 21 Dec 2015 8:27 p.m., \"Fred Blundun\" notifications@github.com wrote:\n\nIt is currently possible to do this outside StorageLoader like this:\n./snowplow-storage-loader --config <(base64 --decode encodedconfig)\nor\nbase64 --decode encodedconfig | ./snowplow-storage-loader --config -\nThis is a bit indirect since it requires wrapping SL in a script.\nHow about adding a new on/off --base64-decode-config switch?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2227#issuecomment-166382731.\n. Sounds good!\n. I think it's 100-128k bytes...\n. A good collection of enrichment config JSONs make it through okay so we should be fine...\n. Dupe of https://github.com/snowplow/snowplow/issues/2348\n. https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/outputs/EnrichedEvent.scala\n\nin TSV format\n. /cc @ihortom \n. https://github.com/ihortom/snowplow-wiki-temp/wiki/Stream-Enrich\n. Done, thanks, closing\n. https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/s3_tasks.rb#L41\n. TODO: refresh this branch using 74-onto-73 (to get e.g. #2090)\n. TODO: remove the data-modeling/spark folder from this branch\n. Good idea, thanks for raising!\n. Totally agree - this is a relic from an earlier age. Let's remove.\n. @ihortom - one of our interns, @bamos, wrote a tool a while back: https://github.com/bamos/github-wiki-link-validator\n. Fixed by @ihortom, closing\n. You are a hero for writing these tests @ninjabear ! Nice work\n. (Once your tests have stabilized, create ticket(s) for them in your milestone and then rebase the commits onto those tickets)\n. The Base64 implementations for the JVM are notoriously fickle across different JVMs/classpaths etc!\n. This looks great @ninjabear - next step for me is to build this as a release candidate and get it running for some existing jobs to confirm no regressions...\n. At the moment, I think your best approach would be to use the JS Tracker's setCustomUrl and manually add them to that - @fblundun do you think that would work?\nIn the future, the utm parameter fields will be moving out of atomic.events into a context, so it should be possible to just manually construct that context in the browser. But that's a way off.\n. Closing as there's nothing for us to action here...\n. I think you could well be right @ninjabear - as I remember it, the GroupBy is notoriously finicky...\n. Great summary @fblundun !\n. If we use S3DistCp for the Thrift files, there is an open question whether or not we can make use of the .lzo.index files - and indeed whether it's necessary. .lzo.index is really designed for massive files - not the kind of small files that the Kinesis-S3 sink typically generates. So maybe we discard the .lzo.index files (and ultimately disable the indexing in Kinesis-S3).\n. Looking good! Can you change this commit message to be past tense:\nScala Common Enrich: add weather enrichment (close #456)\nto:\nScala Common Enrich: add*ed* weather enrichment (close #456)\n. Hey @chuwy - this PR gives a good idea of the various things required to get a release ready...\n. This ticket also needs to factor in the dependency graph between enrichment instances. We have a couple of options here:\n\nCrude dependency tree: EnrichmentC depends on EnrichmentB and EnrichmentA\nPrecise dependency tree: EnrichmentC depends on EnrichmentB.outputs.user.ip_address and EnrichmentA.outputs.derived_tstamp. As a quick update - I am leaning strongly towards the precise dependency tree here. I think it's achievable and much preferable.. Hi @snowindy - sorry for the confusion. The shred step is required for both Redshift and Postgres now. You should not be running --skip shred with Postgres any longer.\n. Re-opened with new title\n. Thanks Fred!\n. Okay cool - please add a commit that changes the hadoop enrich version to 1.5.0-rc1 in a new commit...\n. Added as branch release/75, closing PR...\n. Won't fix, we are doubling down on the SSC:\n\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. /cc @ninjabear \n. Hi Eugene,\nFor a quick fix:\n1. Precede your StorageLoader run with an $ aws s3 cp command to pull\n   down the files locally\n2. Run your StorageLoader with --skip download\nIn terms of the underlying problem: StorageLoader uses a library we wrote\ncalled Sluice, which is a multi-threaded wrapper around the Fog library for\nworking with S3. The bug could be in either Sluice or Fog.\nNote that the Postgres load architecture for Snowplow is pretty archaic at\nthis point: all enriched events are downloaded from S3 to the orchestration\nbox running StorageLoader, and then those event files are streamed into a\nPostgres instance. It's the only place where there is actual dataflow\nthrough the orchestration box (the Redshift load process is fully remote).\nA much cleaner architecture would be to load the events straight from HDFS\ninto Postgres from the EMR cluster itself. We have a lot of refactoring\nplanned on our database load architectures for next year.\n. Agree, let's close this and focus efforts on #2260. I've added a comment to that ticket about dependency graphs and closed #853. Thanks for contributing! Assigning to @bogaert to review...\n. Nice! Scheduling...\n. @bernardosrulzon has signed our CLA\n. This is merged into the release branch now, closing...\n. Done, closing\n. This was fixed a while back - basically there is always something to copy now, because Hadoop Shred outputs the /atomic-events/ folder too.... Closing this as we haven't reproduced.. Edited to 2 hours to ensure all logs rotated\n. Edited to 2 hours to ensure all logs rotated\n. Assigning into sprint, please liaise and decide who will do this.. Thanks, @jbeemster can you review?. Think this is not just specific to AMI 4.1.0, but all AMI 4.x.x?\n. Cool - can we refine the ticket name please...\n. This isn't finished - but assigning to you prior to our 1:1...\n. This is ready for review now - @yalisassoon, @bogaert, @chuwy let me know your thoughts!\n. Yes - we can warn users not to use this with Redshift. I have removed the explicit reference to Redshift support from the ticket.\n. Added warning\n. Pushing back as API lookup is getting ready for the release runway\n. Changed the schemaCriterion to:\n\"schemaCriterion\": \"iglu:com.snowplowanalytics.snowplow/client_session/jsonschema/1-*-*\"\n. Renamed to \"SQL Query Enrichment\"\n. Hmm - prepared statements are best practice from an info-sec perspective. It's very easy for string manipulation of SQL queries to facilitate SQL injection. Why do you say that prepared statements are useless in our case?\n. Why can't we just take the type from either the relevant POJO property or the JSON Path property, then use that type to switch on setBoolean versus setString etc? So the whole thing is strongly typed versus just stringly typed...\n. Hey @chuwy ! Thanks for these points, they all make sense as alternative approaches. Here are my thoughts:\n- expectedRows - agree, let's move to output\n- query.sql versus other approaches. Although it is tempting to move the SQL into a top-level query or into the database, I think keeping it separate is cleanest and most futureproof. We can always revisit this in the future\n- propertyNames - I agree that this is not functionally necessary due to AS alias, but this functionality protects us in the case that column names are inconsistent and the user forgets to fix them with aliasing (e.g. user just writes SELECT *) \n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. We will probably fix this in the loader. Scheduling this for Ben to confirm that, and if so, de-schedule and change ticket name.. Okay, understood. Tweaked title\n. Many thanks @ninjabear ! Will get this fix merged in...\n. Tweaked title\n. Closing as merging into release branch now\n. The default will be random sharding - but if you need events sharded by ipAddress (so that users are approximately tied to one shard), you can still switch that on.\n. See #1924\n. It's not required, fixed in #2740. https://zapier.com/zapbook/webhook/\n. Warnings in your VirtualBox boot are not in our control to fix, closing\n. Hey @0xABAB - for support questions (your second paragraph - your first is fine), please ping the user group, https://groups.google.com/forum/#!forum/snowplow-user\n. @ihortom - not sure if this refers to wiki documentation or repo (README) documentation...\n. You're right - thanks @ihortom . Putting this one in the appropriate milestone...\n. Thanks @fblundun !\n. Fixed by @ihortom, closing\n. Renamed to DynamoDB Read Enrichment, as Query has a specific meaning in the DynamoDB API.\n. Hi @chuwy - this is ready for your review now, please let me know any feedback!\n. Hey @chuwy - can you review and feedback on this on Monday please?\n. Thanks for raising @luochenhuan !\n. Great, closing\n. Does that mean that all the continue_on_unexpected_error and errors: bucket paraphernalia is no longer used?. Let's create a ticket to track removing this from the product, even if 90% of this will go \"for free\" when we move to Dataflow Runner.... Right - let's open up a ticket to discuss this.. Thanks for raising @iaingray - assigning to @ihortom ...\n. Hey @ihortom - thanks for this! If you change your commit message to:\nUpdate Scala Hadoop Bad Rows documentation to use AWS CLI EMR (close #2348)\nthen this ticket will automatically be merged when we merge this commit to master... \n. That's fine - it's in a PR now:\nhttps://github.com/snowplow/snowplow/pull/2396\nIt takes time for PRs to be merged to master...\n. Thanks for raising! Assigning...\n. We get all notifications into our internal chat!\n. Thanks @ihortom - will leave this open till we re-write the section. We should probably also change these instructions to assume the user is working with an AWS-purchased certificate to simplify things.\n. This would be complex given the architecture of the Clojure Collector - where the actual logging is performed by the Tomcat AccessLogValve, rather than by the collector itself.\nGiven that we plan on moving away from the Clojure Collector, I don't see us committing R&D time into this.\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. @ihortom if you read through the Snowplow release blog posts you should get an idea of how we recommend installing EmrEtlRunner now. You can also talk to @chuwy and @ninjabear about this today...\n. It still needs some work - I have updated the title now!\n. Really nice @ihortom - I have pushed to master, closing\n. Would be good to update this section with a link to RFC 6265 (April 2011):\nhttps://tools.ietf.org/html/rfc6265\nThere is a sub-section which explains cookie domain behaviors.\n. Nice, pushed, closing\n. @ihortom - worth talking to @yalisassoon about this one...\n. Done by @ihortom !\n. Actually, this should be slightly different - I've changed the title and re-opened\n. Re-pushed, closing\n. https://github.com/bamos/github-wiki-link-validator\n. Thanks Fred!\n. Thanks so much for doing this @ihortom - I have now pushed. @fblundun could you re-run?\n. Thanks - I've re-pushed the latest to master. @fblundun could you re-test next week?\n. Done by @ihortom !\n. Thanks! Putting this into the Blocked schemas milestone, as we cannot unfortunately merge this into master without breaking pipelines in the wild...\n. Thanks! Putting this into the Blocked schemas milestone, as we cannot unfortunately merge this into master without breaking pipelines in the wild...\n. You are very welcome to open the PR @digitaltouch - thanks...\n. Migrated to: https://github.com/snowplow/iglu-central/issues/534. Thanks! Putting this into the Blocked schemas milestone, as we cannot unfortunately merge this into master without breaking pipelines in the wild...\n. Thanks! Putting this into the Blocked schemas milestone, as we cannot unfortunately merge this into master without breaking pipelines in the wild...\n. Migrated to https://github.com/snowplow/iglu-central/issues/533. Migrated to: https://github.com/snowplow/iglu-central/issues/532. Let's do this - but for safety, let's always make sure we are quoting our attributes anyway, thusly:\nhtml\n<img src=\"http://collector.com/i?e=pv&amp;p=web\">\n. Ouch, good catch\n. Thanks @rbkn !\n. Assigned to @jbeemster - can you take a look at this after the Avalanche release goes out?\n. I would add the mock HTTP server into a new sub-folder off the repo root, called integration-tests\n. Cool idea! See https://github.com/andimarek/graphql-java\n. Nice @chuwy !\n. Pushed!\n. Thanks! Can you sign our CLA please: https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed! Thanks so much\n. @dukex has signed the CLA\n. That elastic-mapreduce CLI tool is very out of date now. You can use the emr sub-command in the AWS CLI instead. Closing this ticket in favour of a new ticket: #2380\n. Nice, closing\n. Blocked because the AMI 4.x.x series is still using Java 7 (@fblundun just confirmed this)\n. Unblocked: https://aws.amazon.com/blogs/aws/amazon-emr-4-4-0-sqoop-hcatalog-java-8-and-more/\n. https://github.com/snowplow/snowplow/wiki/JavaScript-script-enrichment\n. Nice catch @ihortom !\n. Really nice, merged, closing\n. This looks great Fred! Can you add the CHANGELOG in too?\n. @jbeemster, can you update the CHANGELOG with all your commits?\n@chuwy, can you update the CHANGELOG with your commits when done?\n. Thanks both!\n. I think we're getting very close with this branch - @fblundun can you move this commit 60d0ba065ed0dbb7e8d541b46e516c9fba2d4cc6 into the two preceding commits which have the correct issues?\n. Thanks!\n. @fblundun - any ideas why Travis is not passing?\n. Yep agreed\n. @fblundun I fixed the missing credentials in the arg call\n. Really nice! Some suggested additional terms for the first batch:\n- Tracker\n- Collector\n- Enrichment\n- Storage\n- Data modeling\n- Analytics\n- Huskimo\n- Sauna\n- Shredding\n. Looks awesome! Closing for now...\n. Thanks - GitHub Pages recently upgraded to Jekyll 3. We are working on a fix.\n. Old ticket from hackathon, closing\n. Hey @jgautheron - we didn't return to Druid after our hackathon I'm afraid. It remains an interesting database though!. I honestly haven't looked at it in a while. This would be a great discussion topic on our forums - why not ask the community! http://discourse.snowplowanalytics.com/. Thanks for clarifying @fblundun - closing...\n. Please re-open in the website repo...\n. See https://github.com/snowplow/snowplow.github.com/issues/266\n. Duplicate of #2505\n. Dupe of #1148\n. No longer required, Steven found a way round this\n. Yes, this is a good suggestion! We originally planned to use something like kwalify, but I think a more Snowplow-esque course of action would be:\n1. Move out the storage targets into their own configuration files as planned (i.e. similar to enrichments)\n2. Create a self-describing schema for the remainder\n3. Validate that self-describing schema on startup\n. Work has started on this:\n\nhttps://github.com/snowplow/snowplow/issues/2991\nhttps://github.com/snowplow/snowplow/issues/2992. Makes sense, scheduling!\n. Brought forwards to help Snowplow Mini\n. I like it! @ninjabear does that work for your use case (Snowplow Mini)?\n. Hey @ninjabear - I can't think of any burning requirement for that right now; if it comes up in the future we can always extend the config I guess...\n. The alternative would be to store the network_userid on read from the request (rather than logging on the initial response), but then we would only get the network_userid from the second event, not the first...\n. Cleared milestone\n. Some users may prefer the following algorithm to avoid paying for OWM's historical bolt-on:\nIf the event is within the last X hours, use the real-time weather lookup\nIf the event is more than X hours ago, don't add any weather\n\ni.e. some users may prefer slightly stale weather or no weather instead of paying for accurate weather. \n. This is a blocker on the R77 release.\n. Good question! I think at this point we are unlikely to reimplement the Enrichment process in another language, so there's no informational benefit to keeping the prefix (unlike with a collector, where we might easily write a Rust Stream Collector or Elixir Stream Collector one day).\nSo I think we could call the component \"Stream Enrich\" and artifacts would be called \"snowplow-stream-enrich\" to avoid confusion...\n. Yep, we don't reset version numbers when components are renamed\n. Closing this. Exciting! Have you signed the CLA? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed - thanks so much! Assigning to @chuwy to review...\n. Can we create a ticket in Elasticity and link back to this one please?\n. Hey @chuwy - could you take a look at this one please ASAP? If you can get a fix into R77 (branch r77-rebase) that would be awesome.\n. Changed ticket name to match commit message\n. Ah, good to know\n. Yes - the behaviour is as you observed: when building each bad row, we link it back to the source line which produced the bad row, and that line could contain multiple source rows (not all of which are bad).\nThe way around this would probably to have an intermediate form of the event, and this intermediate form would be stored alongside the errors, rather than the source line.\nIt would be a lot of work to fix this behaviour, but it would make sense to do it when we refactor the pipeline to emit bad rows as well-structured Snowplow events rather than as a separate pipe of JSONs...\n. I am treating this as a data loss issue, because this design decision (report the full collector payload for each event in that payload which fails) can cause OutOfMemory issues which blow up at least the batch pipeline:\n\nIf you have POSTs of 100 events, and 50% of the events are bad, then\nSnowplow will try to write out 50 rows to bad, with each of the 50 rows containing the POST of 100 events\nThis massive multiplicative effect causes OutOfMemory issues\n\nWhen we get these OOMs, our only options are:\n\nFail forwards, effectively discarding the problematic batches (data loss)\nIf we are lucky, patch the offending schema (if there is one), bringing error rates down to a manageable rate\nWrite a one-off clean up job in Spark to fix the underlying problem(s)\n\nTreating this ticket as data loss given the first scenario is relatively common.. This is important but can't be fixed in this milestone - it's a bigger topic.. Nice idea...\n. Anything unexpected should still be captured within the type system - we can't expect users to explore stdio logs.\n. Thanks @fblundun \n. Closing as will be covered by #2449\n. Closing as will be covered by #2449\n. See also: #2095, Consider adding an array_index field into shredded tables\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Note @ihortom that you can find the recommended AMI version from the sample config.yml file for that release\n. Agreed...\n. Note: this idea is borrowed from the Kafka guys, who suggest co-locating checkpoints alongside data in a storage target\n. Cool! Have left some comments. Where is the code that generates the DataWorks load schema based on the events table?\n. Thanks @ironsideshu . Can you also add the atomic.events table definition for DashDB into this PR? You can create a folder in here: https://github.com/snowplow/snowplow/tree/master/4-storage\n. As part of this PR, we are going to need documentation that we can add into the wiki on how to setup DashDB, DataWorks etc ready for this load process. Would you be able to add a comment on this PR which starts to list the salient points of this setup process @ironsideshu ?\n. Hi @ironsideshu - sounds good, I would go with the Enterprise level, and add a comment into the file explaining the differences to make (manually) if you are using the Entry level.\n. Hey @ironsideshu - I have just built a version of Scala Hadoop Shred which outputs CSV with all values \" enclosed. To try it out:\n1. Upgrade to Snowplow R77 - http://snowplowanalytics.com/blog/2016/02/28/snowplow-r77-great-auk-released-with-emr-4.x-series-support/\n2. Update your config.yml like so:\nyaml\nversions:\n  hadoop_enrich: 1.6.0\n  hadoop_shred: 0.8.0-csv         # WAS 0.8.0\n  hadoop_elasticsearch: 0.1.0\nLet me know if you have any issues...\n. Bringing forwards\n. Improved title\n. Focused this on SSC given we are migrating away from Clojure Collector. Interesting. As the collectors are stateless, this would have to be a new cookie that registers that the nuid must not be set.. @BenFradet can you estimate the work involved here?. Merged!\n. /cc @fblundun @ninjabear \n. Thanks @ninjabear - good points. Rather than complect the fallback switch into the Scala Stream Collector, I think it would be better to do this out-of-band using the healthcheck. In other words:\nOn each collector instance:\n- Have a process checking the collector healthcheck\n- If backpressure gets too great, then restart the collector with the fallback config (which writes to S3)\nAnother thing to consider is that whether we want to enable the fallback on a box-by-box basis, or globally for the whole collector cluster. It might be more coherent to do the latter.\nHave also added: #2466\n. This is cool...\n. Yes, I think that's orthogonal and we should be doing that anyway - we don't want to be in a situation where we can't bounce a box without losing data...\n. Note: not suggesting putting RocksDB on the \"hot path\", but as soon as we are having write failures, we start using RocksDB...\n. Hey @fblundun - this ticket isn't about putting the send to Kinesis in the hot path (i.e. in the request/response transaction). Rather, it is about distinguishing the behaviour in scenarios 3 and 6 below:\n| # | Tracker | Collector | Response | Intention |\n| --- | --- | --- | --- | --- |\n| 1 | Cache | No problems | 200 | OK |\n| 2 | Cache | Dead | 4xx-5xx | Try later |\n| 3 | Cache | Backed up (e.g. Kinesis outage) | 503 | Try later |\n| 4 | No cache | No problems | 200 | OK |\n| 5 | No cache | Dead | 4xx-5xx | Give up |\n| 6 | No cache | Backed up (e.g. Kinesis outage) | 200 | Hope collector catches up |\nTo distinguish these, the collector needs to know the tracker's capabilities (i.e. has cache or not).\n. Yes! That would be cool. It feels like we need to send more information in both directions:\n1. A tracker should tell the collector what its capabilities are (particularly important for very primitive trackers like pixel or Arduino)\n2. The collector should tell a tracker what is currently happening (particularly useful for sophisticated trackers like JS/iOS/Android)\nWe just need to remember that any data from collector to tracker will just relate to the single collector instance that the tracker is talking to via the ELB - it's still useful though...\n. > So under extreme stress the collector would potentially refuse events that can be stored on their host device but accept events that cannot be kept.\nThis is a nice point - it basically implements a priority system, where \"goldfish\" :fish: trackers take priority over \"elephant\" :elephant: trackers...\n. Is this still the case @bogaert ?. Thanks, done. Hey @ihortom - this is great! I will have a detailed readthrough, but the first piece of feedback is that we decided yesterday to move from talking about \"event dictionaries\" to talking about \"schema registries\". So we are encouraging a company to set up a \"schema registry\". Reasons for the change:\n1. \"schema registry\" is becoming a mainstream term\n2. \"schema registry\" is a better fit for our work with Iglu\n3. A \"schema registry\" can contain contexts and aggregates as well as events\n. I'm happy with that, got to start somewhere...\n. I think the question is whether there really is a separate layer of screenshots etc (the \"event dictionary\") maintained ahead of / alongside uploading the schemas into the registry...\nSeparate point: @ihortom in your guide can you please replace any references to \"... [schema] repository\" (e.g. Iglu repository, schema repository) with \"schema registry\", so e.g. Iglu schema registry, schema registry.\n. Sounds good to me!\n. Great work! Pushed\n. These are great - pushed! Are there any left @ihortom ?\n. Merged! Closing\n. Great idea @ihortom !\n. Thanks for the contribution @kazjote ! @fblundun can you review please?\n. Hey @chuwy - why hasn't this ticket been implemented?\n. Completely makes sense - good point @chuwy ! \n. I think let's make the change in R80 - @chuwy can you please do a commit directly into the release branch for R80 with the change?\n. Fixed title\n. @chuwy - can you rename HeaderExtractorEnrichment... to HttpHeaderExtractorEnrichment... for consistency with the schema name and ticket name? Please update the commit name too.\n. @chuwy can you capitalize the \"e\" in the commit message and changelog: Scala Common Enrich: added API Request Enrichment\n. Thanks @chuwy - yes please, please create a CHANGELOG in a separate commit...\n. @fblundun - can you do full code review on this PR please?\n@chuwy - can you ensure that we have full wiki documentation for both of the new enrichments in this release:\n- The API Request Enrichment needs to have all the granular information from the original ticket\n- Both pages should have a warning at the top that they are dependent on an as-yet-unreleased version of Snowplow\n- Please ask @ihortom to review your new documentation as soon as it's ready!\n. /nudge @fblundun \n. Hey @chuwy - that one looks good to me!\n. Sounds like a promising idea for a future release - please create a ticket!\nOn 29 Mar 2016 4:16 p.m., \"Anton Parkhomenko\" notifications@github.com\nwrote:\n\nGood idea.\n{\n    \"api\": {\n      \"http\": {\n        \"method\": \"GET\",\n        \"retries\": {\n          \"5xx\": 5,\n          \"4xx\": 0\n        },\n        \"uri\": \"http://api.acme.com/users/{{client}}/{{user}}?format=json\",\n        \"timeout\": 5000,\n        \"authentication\": {}\n      }\n    }\n}\n@alexanderdean https://github.com/alexanderdean what do you think?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/2483#issuecomment-202948417\n. This looks great. Very minor feedback and nothing stopping me from doing an initial rc. Two things:\n1. Please can you bump the versions to -rc1 and -M1 in a separate commit that can be a) mutated when we are ready for -rc2 / -M2 and b) deleted when we are ready for release\n2. Is it possible to add a test to Hadoop Enrich which uses the API Request Enrichment? I would feel more comfortable with a test of the enrichment against a \"real\" enriched event\n. Hey @chuwy - that's fine. I'll build and push and start testing now! Thanks\n. No nothing I've seen...\n. @chuwy - there is a bug in the error handling. If the lookup fails, the error looks like this:\n\njson\nOiIxNDYyMDIxMTk1MjAwIn1dfQ\",\"errors\":[{\"level\":\"error\",\"message\":\"com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.apirequest.ValueNotFoundExceptioncom.snowplowanalytics.snowplow.enrich.common.enrichments.registry.apirequest.ValueNotFoundException\"}],\"failure_tstamp\":\"2016-04-30T13:46:06.501Z\"}\nThe contents of the ValueNotFoundException is not printed - just the name of the case class...\n. Current thinking is to leave the page as-is, because it attaches the data as contexts, whereas it is structured as track... events in the new JS Tracker API...\n. If @yalisassoon is happy to leave this page as-is, I think this is a wrap for 2.6.0 release...\n. Cool idea!\n. Bringing this forwards as this is a blocker on Docker support...\n. Thanks @danielzohar ! That would be great...\n. Hey @danielzohar - first off thanks for doing all this, it's appreciated! I've caught up with @jbeemster and this is our plan:\n- From R78 onwards we will support Docker for Snowplow\n- We will roll in other projects over time too (e.g. Schema Guru Web, Iglu Server, Kinesis S3)\n- Semantic versioning doesn't make sense with lots of orthogonal artifacts in the same repo, so we will cut the releases as Release 1, Release 2 etc\nI hope that all makes sense - I've created a repo here: https://github.com/snowplow/docker-snowplow\nPR very welcome!\n. Just to let you know that R78 is now out @danielzohar \n. Hey @danielzohar - the apps do have separated JARs. Do you mean separate artifacts on Bintray?\n. Sure thing @danielzohar - bringing forwards: https://github.com/snowplow/snowplow/issues/1707\n. This work is being done in a separate repo now: https://github.com/snowplow/snowplow-docker/. @fblundun - can you review please?\n. @fblundun - how does starting at a specific sequence number work? Wouldn't you have to specify the sequence number on a per-open-shard basis?\n. Has this been done @chuwy @BenFradet ? . Duplicate of #3054. Thanks @ihortom !\n. Hey @ihortom - can you cherry-pick this into a branch in snowplow/snowplow (rather than your own fork)?\n. Okay @chuwy - closing... @digitaltouch please re-open if there's something specific missing.. We are moving away from this - Spark data modeling is done using Dataflow Runner, not by overloading EmrEtlRunner.... We are moving away from this - Spark data modeling is done using Dataflow Runner, not by overloading EmrEtlRunner.... Difficult because the paths are pretty different.. This is still valid, but I doubt we'll do this given we are moving to Dataflow Runner in the mid-term.. I just tested this with a clean vagrant up and it works, closing\n```\n\u2717 vagrant ssh\nWelcome to Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-43-generic x86_64)\n\nDocumentation:  https://help.ubuntu.com/\n\nSystem information as of Tue Mar  8 08:38:38 UTC 2016\nSystem load:  0.97              Processes:           80\n  Usage of /:   2.8% of 39.34GB   Users logged in:     0\n  Memory usage: 3%                IP address for eth0: 10.0.2.15\n  Swap usage:   0%\nGraph this data and manage this system at:\n    https://landscape.canonical.com/\nGet cloud support with Ubuntu Advantage Cloud Guest:\n    http://www.ubuntu.com/business/services/cloud\n0 packages can be updated.\n0 updates are security updates.\nSetting up Ansible to run out of checkout...\nPATH=/vagrant/vagrant/ansible/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\nPYTHONPATH=/vagrant/vagrant/ansible/lib:\nANSIBLE_LIBRARY=/vagrant/vagrant/ansible/library:/usr/share/ansible/\nMANPATH=/vagrant/vagrant/ansible/docs/man:\nRemember, you may wish to specify your host file with -i\nDone!\nvagrant@snowplow:~$ cd /vagrant/3-enrich/scala-common-enrich\nvagrant@snowplow:/vagrant/3-enrich/scala-common-enrich$ sbt test\nGetting org.scala-sbt sbt 0.13.2 ...\ndownloading http://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.2/jars/sbt.jar ...\n    [SUCCESSFUL ] org.scala-sbt#sbt;0.13.2!sbt.jar (1523ms)\ndownloading http://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.3/scala-library-2.10.3.jar ...\n    [SUCCESSFUL ] org.scala-lang#scala-library;2.10.3!scala-library.jar (5316ms)\ndownloading http://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/main/0.13.2/jars/main.jar ...\n    [SUCCESSFUL ] org.scala-sbt#main;0.13.2!main.jar (3139ms)\n``\n. It suggests your priorvagrant updidn't work (maybe it got cancelled?). If youvagrant destroy, then remove the following folders in/vagrant:oss-playbooks,ansible,.peru, you should be able to runvagrant upagain.\n. It takes over >30 minutes, and requires you to have a high-quality internet connection, but it does work in the end... I can't re-open this because there's no bug to be fixed here.\n. If it keeps failing @juanstiza - you can always just deploy JVM/Scala/SBT into your host environment (i.e. your raw machine). But it's nicer to work from inside Vagrant if possible.\n. This proposal seems sane - the&nuid=was invented to let a server-side tracker pass-through a client-side nuid to the collector, but you are right, the cookie value sent back is not influenced by the&nuid=`. So in the case that you want to do a one-time sync and for that to dictate the nuid for all further behavior, yes you will need this ticket...\n. Ah! @fblundun no longer works at Snowplow, so first action on a ticket will unassign him!\n. I'll bring this forwards...\n. Overridden by #2654\n. No worries - looks like it's coming along!\n. Bringing forwards...\n. Hey @kala725 - in the future please direct questions to the Snowplow forum - snowplow-user@googlegroups.com In this case, I can restructure your ticket into a feature request...\n. Actually it already exists - closing as a dupe of #928\n. Thanks - let's wait!\n. Hi @ihortom - which pages are updated for this ticket?\n. Thanks @ihortom - the best people to review this are @yalisassoon and @fblundun - assigning to Yali to review. I can push once finalized.\n. Merged!\n. Re-pushed\n. @fblundun - the release badge on the README is out of date\n. Alright! Let's wait for the build to go green, then hopefully we are good to go\n. Got a red build, can we try again\n. Good catch, I think we should set to \"/\"\n. Bringing forwards (R82 is in test already)\n. Thanks for flagging @gyurist - created #2739 to explore this...\n. Blocked by https://github.com/awslabs/amazon-kinesis-connectors/pull/63\n. Correction: not blocked (because we don't use the Connectors code for talking to ES), bringing forwards\n. MaxMind is famously pretty inaccurate in Brazil... Maybe take a look at https://geocoder.opencagedata.com/ ?\n. Thanks! Scheduling\n. Thanks so much @christoph-buente - scheduling!\n. Bringing forwards (R82 is in test already)\n. Thanks @christoph-buente - this did get somewhat forgotten. Have given it a concrete release now.\n. I think @ihortom will have a view on this - suspect he will want to \"grandfather\" the old page somewhere...\n. Thanks @fblundun!\n. That comment can be deleted...\n. Awesome - thanks @ihortom!\n. Thanks! Pushed, closing\n. Thanks, appreciate the rich tests! Assigning to @fblundun to review...\n. Thanks @christoph-buente, did you make the updates following @kazjote's feedback?\n. Makes sense!\n. Done!\n. Duplicate of https://github.com/snowplow/snowplow/issues/2950\n. Hey @kala725 - thanks so much for this! Very exciting contribution to Snowplow.\nI have a couple of additional pieces of feedback:\n- The command-line arguments like --skip are really more about controlling the overall jobflow DAG (i.e. which steps to run), rather than the specifics of how each step should operate (as e.g. which shredded types to include/exclude). Therefore, it would be better to include the skip options in the config.yml than at the command-line. Especially because you will probably make a decision on which types to load or not load and then re-use that list each run\n- If we are not loading a shredded type into a database, there is no point paying the \"AWS tax\" of processing it through to S3 etc. Therefore over time we will most likely move the type exclusion code out of StorageLoader into Hadoop Shred. We don't need to do this now though - but I've created a ticket to track this: https://github.com/snowplow/snowplow/issues/2540\nHere's a suggested format for config.yml for further discussion:\nyaml\n  targets:\n    - name: \"Snowplow REDSHIFT Database - us-east-1\"\n      type: \"redshift\"\n      shredded_types:\n        include:\n          - com.acme/...\n        OR:\n        exclude:\n          - com.acme/\n      ...\n. Hey @kala725 - on point 2, I'm just suggesting moving the equivalent whitelist/blacklist code upstream into Hadoop Shred. You would still see entries in atomic.events for events which were excluded... If I have misunderstood your point, let me know!\n. Hi @kala725 - yes sure, he signed the Individual CLA\n. Thanks! I've left some feedback. As background on my last comment - I'm keen that we white/blacklist explicit JSON Iglu types (e.g. com.acme/shopping_cart/...), rather than table names.\nThis is because table names are an implementation detail (may change one day; may have different naming rules per database we support), whereas the JSON Iglu types are the formal representation of the entities themselves.\n. @fblundun could you help out @kala725 here? I think it will be about taking the last four sub-folders off of the s3_objectpath...\n. Thanks @kala725 - @fblundun can you take a look?\n. That would be nice...\n. CLA signed confirmed! Thanks so much.\n. Hey @kala725 - it's functional as it stands, but we may want to revisit the implementation.\nThe problem with the approach in this PR is that it performs the filtering \"late\", by excluding files in S3 from being loaded. The issue is that if I exclude com.acme/checkout, although the com_acme_checkout_1 table will not be populated, those checkout events will still be present in atomic.events. In other words, it's not a comprehensive filtering, just a partial filtering.\nWhy does this matter? Because if I have an event type which is say 100x the volume of other events and I want to exclude it from loading into Redshift, then this PR as it stands will still load that huge volume into atomic.events...\nFor this reason, we probably want to jump straight to implementing filtering in our Hadoop Shred component, rather than 'late' in the StorageLoader.\nSorry for the late change of view on this... Open to your thoughts. \n. Thanks for sharing that @kala725 - helpful input!\n. Descheduling this for now. @chuwy can you comment on this PR?. Agreed, closing. Thanks again @kala725 for your work here - it's appreciated. Although it hasn't been merged in this form, it will provide a lot of guidance for when we do implement it.. Hi @mariussoutier - it's awesome that you are working on Kafka support for Snowplow! It sounds like you are having trouble with your enrichment process - presumably you've taken Stream Enrich and added Kafka support. Open a PR containing the problematic code and we'll be happy to take a look! \n. Hi @mariussoutier - it's awesome that you are working on Kafka support for Snowplow! It sounds like you are having trouble with your enrichment process - presumably you've taken Stream Enrich and added Kafka support. Open a PR containing the problematic code and we'll be happy to take a look! \n. There's a lot of logic in Common Enrich to deal with all sorts of edge cases in the incoming event stream - your best bet is to use that component... It shouldn't be too difficult to integrate Kafka support into Stream Enrich, which embeds Common Enrich.\n. Can you share an example of the problem? Ideally something like:\n- When visiting aaa page URI from bbb referer URI\n- The page URI/referer URI is incorrect in the rawEvent: ccc\n- The page URI/referer URI is incorrect in the enriched event: ddd\n. Ah - thanks for clarifying!\n. Yep I think page URL parsing needs to become an independent enrichment - as a first draft it can obviously still populate the legacy columns (so it will be a \"configurable enrichment populating legacy fields\" in @ihortom's parlance).\nHaving a standalone URL enrichment is similar to how Keen do it: https://keen.io/docs/api/#url-parser\n. /cc @yalisassoon \n. Hey @ihortom - can you update the specific references? We will leave this ticket open afterwards as it refers to a wider point (automatic generation of new docs)...\n. Pushed!\n. Pushed and re-assigned\n. Hey @ihortom - you are right, we agreed earlier to get rid of that section.\nHowever, we have realized that to support data modeling steps written in Spark (see the other tickets in this milestone), we will need to let users install Spark on their cluster. We can do this using this Elasticity functionality:\nhttps://github.com/rslifka/elasticity#emr-applications-optional-needs-release_label--400\nI think to avoid user confusion, we will leave the config.yml.sample with software: {} so new users aren't tempted to install apps on the cluster that they don't need... \n. Renamed. /cc @ihortom \n. Yes, hopefully it would reclaim some space mid-run...\n. Moving forwards as this would really help a couple of clients... /cc @yalisassoon \n. Bringing forwards given this help avoid some shred failures.. Have split this out into multiple tickets. This is a really difficult ticket to have code against, given that it represents the absence of code in your port.... Moved into #3023 . Thanks, assigning\n. Instead it should crash out. Let's also add a longer timeout to bind to the socket.\n. Can we update this ticket name to match the commit please!\n. Hey @bernardosrulzon - thanks for this. We agree that it's quite confusing what resolution you should attempt if the job fails - @ihortom is currently working on a flowchart mapping the job steps to their corresponding data moves, and will start documenting different resolutions for the different cases.\nWe can't merge your specific suggestion here, because the solution Try running with --skip staging,enrich won't be correct in all cases. For example, at Snowplow, the DirectoryNotEmptyError is almost always because a prior run has not yet completed, rather than because the corresponding prior archive step has failed (which your message implies).\nHowever, I think your idea of friendly error messages makes a lot of sense. My suggestion is that once @ihortom has finished his troubleshooting guide, we link to that guide from these error messages. Thoughts?\n. Awesome - I will push once the GTM page has been reviewed...\n. This is super-interesting! Related tickets: #2457, #953\nSome open questions:\n- Do we just do atomic.events, or also all shredded tables, or some shredded tables? \n- Day tables, week tables or month tables? Probably configurable\n- Redshift has a limit of 9,900 tables per cluster (Redshift limits) to factor in\n- Do we partition based on etl_tstamp or derived_tstamp? If the latter, then any given load could obviously be loading into many many tables (because a given load can have derived timestamps from multiple days)\n- Note that Redshift doesn't have an equivalent of BigQuery's table wildcard functions, but we could add this as a pre-processor in a Snowplow SQL Analytics SDK\n. Hi @Biogasbottle - please direct support requests to the user group, https://groups.google.com/forum/#!forum/snowplow-user\n. Hi @Hamitamaru - support questions to the Snowplow user group please.\n. Sounds sensible\n. Thanks @ninjabear! Scheduling into EMR data modeling\n. Descheduling this PR as has been merged into the release branch now\n. Thanks for this @sparrovv ! You are not listed on Simply Business's corporate CLA submission - can you get a new submission done? https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed - thanks so much! Scheduling...\n. Good idea @chuwy \n. Sounds good to me...\n. Hey @ninjabear - I agree with everything you've said and this is 100% how I'd like the SQL Runner integration to work: essentially you have a CLI argument on SQL Runner to spit out the playbook as a Factotum DAG which can then be composed into the parent Factotum DAG. The SQL Runner is then delegated to run each individual step one by one (in other words, a playbook with 20 SQL scripts will translate to 20 individual SQL Runner invocations).\nWith EmrEtlRunner, I had thought this wasn't going to be possible because the EMR portion of the DAG is stateful - i.e. all of the EMR steps have to be run on the same cluster in sequence. So running individual steps isn't going to work.\nHowever, thinking about it some more, I think your above approach can work - the trick is that we need to bring forward #936, and think of a 5-step EMR job flow as 5 EmrEtlRunner invocations on an existing cluster. That removes the stateful-ness requirement.\nExciting!\n. This is an interesting idea - remember though that we have N independent instances of EmrEtlRunner running at any time on a given server... \n. Makes sense @ninjabear - I really like your syntax too:\nemr-etl-runner batch up\nemr-etl-runner batch run \"first step\"\nemr-etl-runner batch run \"second step\"\nemr-etl-runner batch halt\n. Closing but this design lives on https://github.com/snowplow/dataflow-runner. Here are some ideas:\n- scala-kv-conf\n- scala-confhaus (inspired by storehaus)\n- scala-config-kv\n. That's definitely it - thanks Josh. (And a good example of how we need to dig into these bugs when they are originally spotted!)\n. Closing\n. There are a few options:\n1. Run the delete as part of the initial move to processing step (optionally with a new config.yml option to enable/disable these deletes)\n2. Add the delete as a new (Clojure Collector only) step immediately after move to processing\n3. Include the delete as a housekeeping step in EmrEtlRunner which can be invoked from the command-line but isn't part of the regular flow\nThey all have pros and cons - the difficulty with 3. is it's another separate process to schedule. The trouble with 1. and 2. is that they slow down the run time for non-essential housekeeping.\nThere is a further risk with 1. and 2., namely that we upgrade a long-term Snowplow user and then Sluice (which is not fast) gets stuck deleting thousands of files before EMR can even start. So it feels like we need the option of switching this functionality off and also the option to run just this functionality - for these reasons I am leaning towards option 2...\n. I think you're right - option 3 is the least disruptive one and it can always be added to the regular DAG if somebody wants to delete events as they go along...\n. Won't fix, we are doubling down on the SSC:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. /cc @ihortom \n. Assigning to @chuwy to bring this into his release branch. Once this has been brought in, please de-schedule the PR from the milestone and close it...\n. Hi @pj-moviestarplanet , I've added a recovery guide here: http://discourse.snowplowanalytics.com/t/problems-with-vagrant-and-our-github-repositories/122\n. CI/CD bot says:\n\nBuild started!\n. CI/CD bot says:\nArtifact published! https://bintray.com/artifact/download/snowplow/snowplow-generic/snowplow_kinesis_r80_southern_cassowary_rc1.zip\n. @chuwy can you do code review on this? I know the Kinesis apps are new to you - feel free to grab @fblundun on a Hangout to go through together...\n. Yep we are moving to imperative, so the ticket name is identical to the commit message...\n. CI/CD bot says:\nBuild started!\n. CI/CD bot says:\nBuild failed:\n\n[info] + return an invisible pixel\n[info] x return a cookie expiring at the correct time\n[error]    -10000 is not close to 0 +/- 2000 (CollectorServiceSpec.scala:143)\n[info]\n[info] + return the same cookie as passed in\n[info] + return a P3P header\n[info] + store the expected event as a serialized Thrift object in the enabled sink\n[info] + report itself as healthy\n[info]\n[info]\n[info] Total for specification CollectorServiceSpec\n[info] Finished in 25 seconds, 877 ms\n[info] 6 examples, 1 failure, 0 error\n[info] PostSpec\n[info]\n[info] Snowplow's Scala collector should\n[info] x return a cookie expiring at the correct time\n[error]    -3000 is not close to 0 +/- 2000 (PostSpec.scala:138)\n[info]\n[info] + return the same cookie as passed in\n[info] + return a P3P header\n[info] x store the expected event as a serialized Thrift object in the enabled sink\n[error]    1461314749929 is not close to 1461314755000 +/- 1000 (PostSpec.scala:179)\n[info]\n[info]\n[info]\n[error] Failed: Total 18, Failed 3, Errors 0, Passed 15\n[error] Failed tests:\n[error]     com.snowplowanalytics.snowplow.collectors.scalastream.PostSpec\n[error]     com.snowplowanalytics.snowplow.collectors.scalastream.CollectorServiceSpec\n. [info] Snowplow's Scala collector should\n[info] + return a cookie expiring at the correct time\n[info] + return the same cookie as passed in\n[info] + return a P3P header\n[info] x store the expected event as a serialized Thrift object in the enabled sink\n[error]    1461322125408 is not close to 1461322132000 +/- 1000 (PostSpec.scala:179)\n[info]\n[info]\n[info]\n[info] Total for specification PostSpec\n[info] Finished in 30 seconds, 730 ms\n[info] 4 examples, 1 failure, 0 error\n[error] Failed: Total 18, Failed 1, Errors 0, Passed 17\n[error] Failed tests:\n[error]     com.snowplowanalytics.snowplow.collectors.scalastream.PostSpec\n. Trying again! The build didn't seem super-slow to me, maybe I missed that...\n. The rc2 deployment will be handled by CI...\n. Great work! Released\n. It's probably cleanest if we design a new (small) context for this for the trackers and add some \"legacy migration\" code into Snowplow to take an existing string like 'js-2.6.0' or 'java-0.5.0-rc1' and reverse generate the context...\n. In the release where we did this, it would be kind to also release SQL migration scripts to backfill a Redshift database's tracker_context table based on v_tracker.\n. Duplicate of #2092, closing\n. Hi @chuwy - can you look at this one and let me know what has happened?\n. My current theory is that something was overwritten in the force push which caused something to change so that this commit did not close, although I believe that the commit is now in master. I am nervous about closing this ticket and tagging the release until I head back from @chuwy however.\n. Okay - so it sounds like:\n- The feature was code complete 24 days ago\n- 3 days ago you made a single change: fix the commit message to reference the correct ticket\n- My force push undid the commit message fix\nThe solution: I rebase and reword the commit message, then force push again. Agree?\n. Not sure - but if the feature was code complete 24 days ago, I'm happy to fix per the above...\n. Bringing forwards\n. Thanks, fixed\n. Moving this into R85. This would be a good one for @ronnyml to do next\n. Ah, much better idea, let's do that! I was overthunking it. Thanks Josh.\n. Nice\n. This has been done - @fblundun, @ninjabear, please proceed to getting R80-rc2 into Bintray on Monday morning...\n. /cc @ihortom \n. Thanks @adamlwgriffiths, assigning to @ihortom \n. Thanks pushed!\n. Thanks both - this is coming along nicely. I agree that tag names are the way to go. I agree that it should be the committer's responsibility to ensure BuildSettings versions are correct.\nThe two extensions around the tags are interesting. I was pondering an alternative approach, where the CI/CD can figure out what needs to be pushed. I guess it would have to compare versions in BuildSetttings with what is available on Bintray. This means that the CI/CD needs some \"business understanding\" of this repo's artifacts, but I don't think that's avoidable in snowplow/snowplow due to the complexity of the layout (and the various release scenarios we need to support).\nThoughts?\n. Thanks both - this is coming along nicely. I agree that tag names are the way to go. I agree that it should be the committer's responsibility to ensure BuildSettings versions are correct.\nThe two extensions around the tags are interesting. I was pondering an alternative approach, where the CI/CD can figure out what needs to be pushed. I guess it would have to compare versions in BuildSetttings with what is available on Bintray. This means that the CI/CD needs some \"business understanding\" of this repo's artifacts, but I don't think that's avoidable in snowplow/snowplow due to the complexity of the layout (and the various release scenarios we need to support).\nThoughts?\n. Hey @ninjabear - lots of good points there.\n\nI think we'd be starting to work against the grain though, traditionally one repository compiles to one artifact, even if that artifact is composed of N other artifacts - e.g. \"snowplow rXX is clojure-collector 0.1.0, stream-collector 0.2.0..\". \n\nThis has never really been the direction of travel in snowplow/snowplow - instead it consists of more and more individual apps and libraries (Scala Hadoop Enrich, Scala Common Enrich, Stream Enrich etc) which are independent (though possibly with cross dependencies in the case of Scala Common Enrich), published as standalone artifacts and orchestrated by separate technology (EmrEtlRunner; Tupilak).\nIf we accept that direction of travel, then the challenge for CI/CD has to be how to decide what artifacts to publish at a given point in time. The CHANGELOG illustrates how different releases will only ever update a subset of artifacts:\nhttps://github.com/snowplow/snowplow/blob/master/CHANGELOG\nI think there are only two strategies here:\n1. We use git tag \"hinting\" to express the artifacts which are required\n2. We ask the CI/CD to perform an \"artifact diff\" to determine what needs pushing\nI think both options have merit. My concern with option 1 is that it introduces the possibility for human error - for example, somebody performing the final merge to master forgets to tag hint that Scala Common Enrich needs to be published before Scala Hadoop Enrich is re-assembled.\nThe idea of CI/CD determining what needs pushing and in what order - I agree it's more intelligence than you would normally see in a CI/CD process, but Snowplow is a complex project so that logic is always going to have to live somewhere, regardless of repo layout. I think that having that logic crystallized in a build process has some merit.\nThoughts please!\n. Hey @ninjabear -\n\nhowever if we could split the repo into the collection of artifacts that we do update together (real-time/batch?) it would give us a couple of advantages downstream\n\nWe already have this with the Kinesis pipeline (push kinesis publishes a zipfile containing three separate stream processing apps) - but in fact users want us to move away from this because they prefer to interact with our individual binaries through an intermediary like DockerHub:\n- https://github.com/snowplow/snowplow/issues/2493\n- https://github.com/snowplow/snowplow/issues/1707\nTupilak will ultimately promote this pattern too.\nSimilarly, with the Hadoop pipeline there's no user demand to download a zipfile which contains all of our Hadoop jars - instead we publish these jars to S3 (s3://snowplow-hosted-assets) and EmrEtlRunner mediates use of these.\nAnother way to think of it is that any given release of Snowplowis a bit like a Linux distribution: it's a rubber-stamped version of Snowplow where everything is guaranteed to work together; unlike a Linux dist however, a Snowplow release isn't shipped in a monolithic zipfile - instead there are configuration files like config.yml which assert the new versions to use.\n(So why is Snowplow shipped from a single repository? Because it would be incredibly difficult to hold the whole system in your head for a new release if it fragmented into 20-30 repos with cross-dependencies.)\n\nI think if we don't split the project up with submodules (or other) then we should dig in and start a build script inside the snowplow/snowplow repo.\nI think this because if we have some non-standard build logic we need to capture it as neatly as possible, and keep it outside the build tool where we can. This means we'll remain fairly build tool agnostic, and we'll never lose the ability to build our product (surprisingly this does happen).\n\nAll sounds good!\n\nPython is often used in this capacity - realistically we have a choice between python or ruby (using rake would be nice since build tools understand it). (We could also feasibly use gradle and a fair chunk of groovy - this has the advantage of having a bintray plugin provided by bintray). I'd like to avoid bash\n\nAgree re bash. I am not a huge fan of Groovy so would lean towards Ruby with Rake.\nI am a bit hazy on your last two paragraphs Ed:\n\nI think the way to look at the process deploying with crafted tags is, not that it's a button to deploy a release - but that the tag defines the release\n\nBut the example was r80-rc2:sce;she,shs - that only describes the release as a delta from R79; it doesn't describe the release of the \"Snowplow distro\" in its entirety (unlike e.g. our Version matrix spreadsheet).\n\nConsider it from a support point of view - customer reports problem, they're using 0.1.0 of xyz - I need to check out that version in order to replicate the problem (you could download the artifact - but the source code is invaluable in figuring out the fix)\n\nIt's a bit fiddly, but at the moment you would just look up the version in the Version matrix spreadsheet and check out a Snowplow release containing 0.1.0 of xyz. Can that be replicated in a human-friendly tag?\n(A thought: in due course, I think it could be interesting to update the Version matrix as part of the CD process. This would give you a simple cross-check that all of the expected artifacts have been deployed...)\nMy gut feel is that we should move quickly to a probably-Ruby build script in snowplow/snowplow which:\n1. Has an understanding of all the potential artifacts (binaries; libraries) currently available within snowplow/snowplow\n2. Understands the dependency graph between artifacts - e.g. Common Enrich should be published before Hadoop Enrich or Stream Enrich\n3. Can automatically publish any artifacts which have an updated git history\nFor now I think we should stick to simple tags (r80, r80-rc1)...\n. Hey @ninjabear - thanks for taking the time to explain that. I think the approach on the LHS is great - it would give us a much more useful tag history, and yes it would effectively drive our version matrix, which is really neat...\nI guess the open question is whether we go:\nhuman -> tagging -> build script cross-check\nor:\nbuild script -> tagging\n. Hey @ninjabear - I agree, I much prefer option 2.\n\nSo we'd need to push a tag, like \"r80\" to get things started, then the build script can automatically establish what's changed, and what the versions now are (and push the corresponding tags back to the repo). \n\nYep.\n\nThere is one catch with that though, and that is that we'd still need a way to bake in what should be in a release.\n\nYes - we will need a bit of a dependency tree in our build script. So push Common Enrich before Hadoop Enrich, for example.\n\nmany other things make up a release and they may not all have changes - they should still be \"reissued\" under the new release. \n\nHow do you define re-issuing in this context?\n. That specific example doesn't quite flow because Hadoop Enrich would be pushed to S3 and Common Enrich would be pushed to Maven, and we should treat deployed artifacts as immutable.\nBut I know what you mean: if a release changes Stream Collector and Stream Enrich but not Elasticsearch Kinesis Sink, then yes currently our all-kinesis-apps-zip would need to include the last Elasticsearch Kinesis Sink too... \n. Thanks pushed!\n. Awesome - closing!\n. Spaces? Not a biggie as this script should be moving over to Ruby at some point...\n. Merged into release branch, closing\n. Good way of prototyping it!\n. Thanks! Assigning\n. Pushed to main, closing\n. Thanks @liningalex ! @liningalex has signed the CLA.\nAssigning to @fblundun to review...\n. Hey @BenFradet - you raise a good point, which is that it will be hard to perform the various data checks in a world where all the file move code has moved to the cluster. I'm talking here both about the \"no data to process\" no-op state and the \"data already found in {processing|enriched|shredded}, a previous run is ongoing\" no-op state.\nI think we need a new ticket in your release to brainstorm approaches here.... See https://github.com/snowplow/snowplow/issues/3112. I think for return code 3 we need the info (or warn? not error) message, as well as the return code.\nI think a companion ticket for an info message in the case of a DirNotEmptyError would be great - please add!. stdout. Hey @jramos! Thanks for this - exciting contribution.\nA feedback point - although we make an exception for the IP anonymization enrichment, using configurable enrichments to mutate existing fields is not really The Snowplow Way. Instead, if this enrichment is enabled I would expect events to get a new context which includes maybe the general language family (\"English\") and the specific language variant (\"Canadian English\") - or whatever else is easily available...\n. Thanks @jramos! We really appreciate the contribution :-)\n. Duplicate of #2943\n. Duplicate of #2937\n. Duplicate of #2941\n. Duplicate of #2939\n. @simplesteph where is the standard format for writing Confluent Schema Registry-compatible records defined? Having trouble finding it.... If it's vanilla Avro, then how is the Avro record associated with an entry in the Confluent Schema Registry?. Yech, this sounds incredibly opaque.\n\nBasically your record is split between Kafka and the schema registry\n\nSo I am attempting to write Avro[foo] to a Kafka topic. What precisely is stored in my Kafka record - and where is that documented?. Thanks, I'll feed this back into the Discourse post.... The underlying problem here is that the documentation for the Scala Tracker is not being versioned. @chuwy - please implement versioning for the documentation. Please see how e.g. the Android Tracker handles versioning. We should have versions 0.1.0, 0.2.0 and \"latest\" (0.3.0), with a warning on 0.3.0 that it is pre-release.\n. Thanks!\n. Agreed - thanks @miike!\n. Pushed, thanks, closing!\n. Whoops - thanks\n. Pushed, thanks, closing!\n. Won't do.. Great, closing\n. Re-assigning to @fblundun as he has all the info required now...\n. Let's tweak the name of this ticket, it's a bit wordy\n. Sounds good!\n. Good call. Done! Thanks so much, closing\n. Nudge @jbeemster \n. Bringing forwards\n. Done, closing\n. I wonder if it should be raw or the specific formats (clj-tomcat, cloudfront etc)...\n. > When using raw collector files as input, I assume we would just pass an empty array as the \"errors\" argument of the user-defined function\nI think that sounds good...\n. CloudFront versus Clojure is perhaps a bad example as the Clojure row is a superset of the CloudFront row - I suppose clj-tomcat versus thrift is a more challenging example...\n. Hey @bcharp - these are great questions, and we'd love to answer them! The best place for discussion questions is on our Discourse forums: http://discourse.snowplowanalytics.com/\n. Sounds like a bug! Assigning to @jbeemster...\n. The adapters are applied before the EnrichmentManager...\n. Check if other params make it through? Maybe SendGrid is stripping or overwriting the querystring...\n. Thanks for figuring this out @jramos - closing this one in favor of your #2685\n. I think that's a good idea Fred. Another suggestion is that we have some kind of intermediate representation of a failed event which is reported in place of the original raw line...\n. Yes, we could have a structure like:\n( raw payload, [ raw errors ] )\n[ ( event 1, [ event 1 errors ] ),\n  ( event 2, [ event 2 errors ] ),\n  ( event 3, [ event 3 errors ] ),\n]\nThe reason I quite like the intermediate form is that, over time, the bulk of our failures are going to relate to issues in \"well-formed\" events (such as schemas that can't be inferred), rather than issues with the raw payload itself. So going back to the raw payload each time feels a little wasteful...\n. Or maybe we are thinking about the \"bad rows\" output wrongly, and we should simply be emitting well-formed Snowplow events for each event that fails on a 1:1 basis.\n. Yes - we should be generally solving for the introduction of duplicates anywhere in the pipeline.\nBut thinking about the fact that our current bad rows output introduces a lot of duplicates needlessly when reprocessed (because its structure dates from when 1 payload :: 1 event) is a good starting point for coming up with a better bad rows format.\nThe duplicates arise because you can't tell which event in a POST payload a given failure applies to, which (regardless of the dupes) makes debugging harder. Another limitation of the current format is that it doesn't let you review the failed event in Elasticsearch. Debugging and recovery with the current bad rows format will get more challenging as we introduce a more complex enrichment DAG, and also schema inference...\nThere is a wider architectural issue too, which is that our stderr-style bad rows outputs (Kinesis or S3) don't compose. Our happy path composes well: we can route a unitary happy path all the way from collection through to storage targets. But on the failure path, we end up with all these individual silos of bad rows which need special-case treatment (like bad rows ingest into Elasticsearch).\nI suspect that the answer down the line is to unify the \"good\" and \"bad\" output, so that there is just one output stream of events from a given component, containing a spectrum of:\n1. Raw payloads which are corrupt\n2. Events which have failed schema validation\n3. Events which have failed enrichment\n4. Events which pass all validation and enrichment but contain ?-?-? schemas which cannot (yet) be inferred\n5. Events which pass all validation, enrichment and inference with warnings\n6. Events which pass all validation, enrichment and inference with no warnings\nSo it's really a spectrum of (6) shades of grey, versus \"good\" and \"bad\". The nice thing about this approach is that the whole thing composes, because it's just events. And there's no need for special casing for bad rows versus good: if you can represent an enriched event in Avro or Redshift, you can represent an event which has failed enrichment or has warnings using the exact same tech... \n. See also #351\n. Thanks @jramos ! Can you sign our CLA please: https://github.com/snowplow/snowplow/wiki/CLA\n. See also: https://github.com/snowplow/snowplow/issues/2678\n. This sounds great, thanks @ninjabear \n. Hmm, sorry I just re-read the above, spoke too soon. What's the proposal for tagging in this repository to relate to Hadoop Event Recovery?\n. I'm not convinced by using tags to drive the versions being built... I like having the versions of each application visible in source control at a given commit. Also, this starts to get very complicated fast when you have e.g. Hadoop Enrich pulling in the latest version of Common Enrich - how do you express that version dependency if it's all tag-driven?\n. Pushing back.... Hey @BenFradet - yes, this is to handle the situation where the Hadoop Enrich step has already occurred, the user is resuming from the Hadoop Shred step (--skip enrich). It's worth trying this yourself in the Dev account, you'll then see what the problem is first hand (basically: the run= folder gets copied to HDFS and Hadoop Shred dies when it encounters a folder).. I know this may go away with the move to Spark, but I think it makes sense to tackle it sooner.... Thanks @fblundun ! Can you retitle the commits for #2692 and #2694 to match the ticket names?\n. We need to get Snowplow users comfortable with the idea that there are multiple possible user identifiers for the event, just like there are multiple event timestamps:\n- The user_id in atomic.events is essentially a business-owned identifier for the user. Over time this will move out into a dedicated context (as will everything in atomic.events) and the tracker setUserId() methods can become sugar around https://github.com/snowplow/snowplow-javascript-tracker/issues/405\n- The user_id in the client session context is typically used as a synthetic (tracker or collector-created) proxy ID for the user, with this proxy user ID being held in some kind of storage (maybe cookie or SQLite or similar) so that it (mostly) survives across sessions\n. domain_userid will move out into an instance of the client_session context in due course (https://github.com/snowplow/snowplow/issues/1893). We can definitely do a breaking 2-0-0 of the client_session with the user_id property renamed - open to suggestions for a better name...\n. Sounds good. A couple more options:\n- client_userid\n- client_id\n. Discussion is here: http://discourse.snowplowanalytics.com/t/scala-stream-collector-add-support-for-cookie-bounce/306\n. Give it time!\n. Very cool! Thanks @miike. Assigning to @jbeemster to review. @miike has signed the CLA.\n. Can we rebase this when ready to a single commit \"Scala Common Enrich: add POST support to IgluAdapter (close #1184)\"\n. We need a tag syntax that supports expressing both, don't we? Because they have\nto be the same code commit, because they have an inter-dependency...\n. R81 is going to update:\n- Scala Common Enrich\n- Stream Enrich\n- Hadoop Event Recovery\n. Proposal:\n1. We define a set of aliases for each unit of deployment (e.g. Scala Common Enrich, Stream Enrich, Hadoop Event Recovery)\n2. We come up with a syntax to support Double-entry build with multiple components, like:\nsce-0.24.0;se-0.9.0;her-0.2.0\nwhere:\n1. Components are published sequentially in left-to-right order\n2. Component versions are validated against BuildSettings.scala etc\n. What about the JSON-based approach, but then allow the tag to whitelist or blacklist specific components, so e.g.\n+sce,her\nwill only push SCE and HER.\nor\n-sce\nwill push all but SCE.\n. @BenFradet @chuwy - it feels like we have consensus on how CI/CD works/should work for this repo. Can I close?. This is too complex for this release, and maybe should live in Release Manager.. Should this logic be inside the Scala Tracker? Otherwise we have to solve for this problem in every application using the tracker...\n. Hmm - why does the second option crash the tracker if the EC2 metadata is not available? That feels unnecessarily risky.\n. Got it - so maybe the problem in that Discourse ticket is slightly different?\n. Is this a verified issue?\n. Doesn't make sense to have a placeholder ticket in Snowplow for everything we do in other repos.... Hey @fblundun, @chuwy - do we think doing an in-place mutation is safe here? I wonder if we should be moving to a more robust approach where the Scala Hadoop Enrich version is provided to the bootstrap script which then decides what specific versions of things to deploy/delete...\n. When I ask if it's safe, I mean are we happy for existing clusters using older versions of SHE to have this change applied to them too...\n. Sure, I just renamed it (feel free to teak title further)\n. Assigning to @fblundun to review\n. That's fine - that's next on my list to do!\n. Thanks @chuwy ! This is looking great. Excited to get this into test with @ihortom. I have assigned some other R82 tickets to you - if you could apply them into this branch that would be great.\nI've also asked @jbeemster to merge in his work on the Iglu webhook adapter into your branch as soon as he is able...\n. Hey @chuwy, @jbeemster, this is coming together! A few pieces of feedback:\n1. By the time you read this, this version will be in test, so please bump everything to -M2, -rc2 etc\n2. For consistency we should upgrade Scala Hadoop Shred's Scala Common Enrich dependency\n3. @jbeemster please make the change to the qsBody per my comments yesterday\n4. @chuwy please rebase R82 onto R81 (which went out today)\nThanks both! Exciting...\n. Hey @jbeemster - would you mind bumping Stream Enrich to rc2 making use of Common Enrich M3? I can build CE M3 now...\n. Thanks @jbeemster !\n. Closing this PR as we are going to rebase all this onto a new branch...\n. This ticket needs to migrate to Iglu Central. Migrated to https://github.com/snowplow/iglu-central/issues/538. Descheduling. Add one to these numbers Josh...\n. No this is new functionality so 1.9.0\n. @jbeemster can you cherry-pick this into your CloudFront branch as the latter will be put into test first.... R83 is too broad - moving back...\n. R83 is too broad - moving back...\n. Awesome @ronnyml ! Assigning to @jbeemster to review. Also assigning this to a new Snowplow release milestone, R85...\n. Thanks for digging into this @chuwy . We may be able to make this fix without breaking existing Snowplow pipelines - I am going to schedule this for R83 and we'll take a look...\n. Hi @kala725 ! This is a cool idea but not something we are actively developing yet.\n. This is being worked on here: https://github.com/snowplow-incubator/snowplow-bigquery-loader. Bringing forwards into R85 - 3 webhooks per release feels about right...\n. We can achieve this with a simple (static) lookup table of Scala Hadoop Enrich versions...\n. The lookup table is about which version of commons-codec to install, not which ones to delete...\nIf SHE is >= 0.24.0, then install 0.10.0, otherwise 0.5.0\nJust be careful about doing semantically correct version number comparisons\n. Hey @guevaraf ! Great question - please raise on our forums (GitHub is for bugs and new features): http://discourse.snowplowanalytics.com/\n. Ace thanks closing\n. Please use the reaction icons if you are interested in seeing a feature being prioritised. Any progress on a ticket will be visible on the ticket with an associated git commit.. Won't fix, we are doubling down on the Scala Stream Collector per:\nhttp://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881\nThis is fixed in the SSC.. Re-opening as I think wontfixes are confusing. A bug is a bug!. Hi @jpotts18 - ah, good timing - we are working on a new Android Tracker release now. trackAdImpression is a very thin wrapper around the underlying self-describing JSON:\nhttps://github.com/snowplow/snowplow-javascript-tracker/blob/4e52a258dba248d824c360941caadd338f9e4490/core/lib/core.js#L449\nSo it should be an easy add for the Android Tracker if you want to open a PR!\n. Sounds good @jbeemster !. @jbeemster - if you put a trailing slash in the configuration for one of the MaxMind database URLs, you should get a crash...\n. The documentation page says: Must not end with a trailing slash.\nI agree - I can't figure out what this means.\n. Actually before I de-schedule, can you just try it and tell me what happens?\n. This ticket needs to migrate to Iglu Central. Migrated to https://github.com/snowplow/iglu-central/issues/539. Descheduling. Won't do, this is beyond the scope of EmrEtlRunner.... Moving into @chuwy's release. @chuwy - can we get this Iglu Scala Client release scheduled? It's been pending kick-off for a long time.... Exciting! Assigning to next open Kinesis release, assigning to @jbeemster to do a first review\n. Hi @christoph-buente - no, this was pushed back in early September... It's a complex feature and we haven't had the bandwidth to do QA on it to date.\n. This is already assigned to R92?. That makes sense. It's certainly a significant cost - but it's an explicit cost that a user will opt in to, to avoid downstream complexity/cost. And there isn't an alternative at this (front) end of the pipeline.... Pushing back\n. Assigning to R82...\n. I don't know if this is a bug, but let's investigate...\n. This is a dedicated milestone now, R83 >> \n. In that case - closing... Feel free to continue discussion on Discourse!\n. This is super-interesting @pkallos !\n\nI gather that a time-limit settings of >5mins won't yield correct results. From my memory, when a ShardIterator expires, the worker thread is interrupted and this creates a new ShardIterator from the last checkpoint, and also tosses away all the buffered data from the prior attempt\n\nI need to check with @jbeemster what our experience has been. It feels like we normally set the time-limit higher - wouldn't that just imply that we collect the output of two or three ShardIterators in sequence before flushing, rather than we get stuck with a single one timing out?\n. Hey @pkallos - I think you're right:\n1 Shard = 1 Thread = 1 Buffer = 1 File in S3\nBut I reckon we can work through multiple ShardIterators in that thread before writing a file - but I might be totally off-beam here. @jbeemster thoughts?\n. Migrated to https://github.com/snowplow/iglu-central/issues/540. Migrated to https://github.com/snowplow/iglu-central/issues/541. Migrated to https://github.com/snowplow/iglu-central/issues/543. Migrated to https://github.com/snowplow/iglu-central/issues/542. Won't fix, we are doubling down on the Scala Stream Collector per http://discourse.snowplowanalytics.com/t/replacing-the-clojure-collector/881. Migrated to https://github.com/snowplow/iglu-central/issues/537. Migrated to https://github.com/snowplow/iglu-central/issues/536. Thanks @shin-nien - I'm guessing you're not using it but would you mind adding this to the Elasticsearch Sink as well for consistency?\n. Confirmed, @shin-nien has signed the CLA. Thanks so much!\n. That's fine - we appreciate the extra effort and there's certainly no rush from our side!\n. Cool - thanks @shin-nien !\n. Not sure I understand what you need @ihortom for the second part?\n. A few sentences is fine...\n. Great, closing!\n. Superceded by more general #2461\n. Superceded by more general #2461\n. Thanks @ihortom - I can't see how it could work without the iglu: (it might \"work\" in that it gets passed to the collector okay, but it would fail validation downstream).\n. Yes, let's de-schedule too...\n. Hi @Germanaz0 - if you are having troubles using this webhook adapter with the Clojure Collector, the best place to raise this is on our forums: http://discourse.snowplowanalytics.com/\nThis GitHub ticket is about something different.\n. Update: it works. Closing this ticket in favor of #2996 . See thread here for this working: http://discourse.snowplowanalytics.com/t/iglu-webhook-response-code/837/3. Are we going to get to this before we replace EmrEtlRunner with Dataflow Runner per our RFC?. This may go away when we ditch Sluice. Keeping it in, we'll wait and see.. Shall we de-schedule?. Looks great! Super-minor feedback\n. Migrated to https://github.com/snowplow/iglu-central/issues/531. Hey @BenFradet - I don't know how you feel about this ticket. It might simplify things, plus I am not a fan of Bundler; on the other hand it might be less useful given long-term snowplowctl will probably move to Scala. It's up to you whether you want to bring this into your EmrEtlRunner robustness milestone.... Migrated to https://github.com/snowplow/iglu-central/issues/530. Migrated to https://github.com/snowplow/iglu-central/issues/529. Great news @jbeemster ! Nice work. The only remaining ticket in this release I believe is: #2492 This is the first step in the Dockerization process for Snowplow RT...\n. Closing PR, please open a fresh one when the release is ready\n. Hi @bernardosrulzon - right, yes - EmrEtlRunner doesn't support working with S3 buckets across multiple regions currently...\n. Love the ticket name!\n. I agree @chuwy, thanks for flagging\n. https://github.com/snowplow/snowplow/issues/2050\n. Great point @ninjabear !\n. Thanks for flagging this @BenFradet - another great reason to press on with this upgrade.... Thanks @philsch ! Can you sign our CLA?\n. Confirmed! Thanks\n. Thanks @philsch - your code will be fixed up into the relevant commit (it's now in the branch), closing...\n. Tweaked the title - as these tests should be revised to use real schemas.\n. Quick notes:\n- I believe that adding the debugger step is handled under the hood by the AWS API\n- Unfortunately that built-in support doesn't seem to work with eu-central-1\nI could be wrong on this... \n. Yes it works fine without the --debug flag, the ticket is correct though - the debug flag doesn't work.\n. Done in Release 90 Lascaux. Superseded by #1977 (archiving raw moving to EMR).... Hey @rgabo - yes we'd like it to move to being more idempotent - that's one of the easiest ways of making it more stable. We'll most likely get rid of the copies to HDFS as part of the move to Spark.... Thanks @rgabo! Scheduling\n. Thanks! Assigning\n. Tiny tweak, otherwise looks great!\n. Thanks Adam! Assigning for fix\n. Thanks @adamlwgriffiths - assigning for fix...\n. Hey @RajeshHegde - this is very exciting! A few comments:\n- Could you start fleshing out some documentation somewhere in the PR?\n- Am I right in thinking you are downloading all of the events to the machine running StorageLoader - like Postgres loading, that's a singleton process rather than the distributed load processes we prefer. What kinds of volumes have you tested this with?\n- It looks like you have some new options for the target in config.yml - again could you document what they are?\n- Am I right in thinking that you didn't add support for custom or derived contexts - just unstructured events?\n- It looks like the flattening of unstructured events would depend on the BigQuery table having been pre-prepared with slots for the given unstructured events, and the schema for that BigQuery table would have to have been pre-supplied in the config.yml schema field. Is that right? Does the load fail if a new event type is found in the data which hasn't been pre-added to the table and schema? What's your internal process for supporting this?\nPhew! Those are my initial questions - apologies for so many Qs, it's just very exciting to be starting a dialog with a production user of BigQuery with Snowplow!\n. Thanks for this @RajeshHegde ! I don't think there's a specific need to rewrite your Python schema script in Ruby - ultimately this capability would move into Iglu (Scala) for us anyway, so limited gain in porting over right now... Looking forward to seeing it though! \n. We've been building recently on Mac through Vagrant and it's been working fine... Can you provide steps to reproduce from a fresh Vagrant? I'll then walk through them myself.\n. Okay thanks for that @RajeshHegde , I will take a look...\n. Ah! Thanks for the update @RajeshHegde \n. Many thanks for sharing! Will take a look.\n. Hi @RajeshHegde - you are right, this PR has been around for a long time.\nIn terms of an update from our side:\n\nWe are planning a major refactor of the atomic.events table - expect an RFC on this in due course\nWe have started work on a version of Snowplow targeting Google Cloud Platform, including eventually BigQuery - expect an RFC on this in due course\nWe are sketching out some innovative ideas for how best to represent Snowplow enriched events in BigQuery\n\nGiven the above, your PR is not on a fast-track to merging. However, it does contain some really helpful code, and we'd certainly like to keep this PR open to feed back into the above action items.\nThanks again for your contribution.. Many thanks for this @RajeshHegde - it was much appreciated. I am closing this PR now because the current situation is quite different from the time at which you submitted this PR:\n\nThe Ruby StorageLoader has been retired, replaced with the Snowplow Relational Database Loader (RDB Loader), written in Scala\n@chuwy is starting work on a new Snowplow BigQuery Loader, which you can follow in this repo: https://github.com/snowplow-incubator/snowplow-bigquery-loader\n\nSo I am closing this PR - but thanks again for the original submission. It's really appreciated and I hope this closing doesn't deter you from submitting pull requests to Snowplow in the future.\n\ud83d\udc4d . Good point! Is this documented?\n. Re-opened and turned into doco ticket.. Read Ben's comment above - please document and emphasize the ability to disable the cookie with this line:\nhttps://github.com/snowplow/snowplow/blob/master/2-collectors/scala-stream-collector/examples/config.hocon.sample#L40\nRather than just having to set the duration to 0.. Won't fix, we are prioritising the Scala Stream Collector.. Great idea! We should also track which event types we support first-class, and which contexts we can auto-add\n. Assigning to Josh and Ihor - hopefully Ihor can work on the template. Added here: https://docs.google.com/spreadsheets/d/1rN2nkBet53rFlv00ByYRGpo9Ou_6ZxGqfDpsVWDtIlY/edit#gid=0\n. Moved to Trello\n. Great to see this land @ironsideshu ! We'll reach out to you once the intervening Snowplow releases have gone out and will ask you to rebase in due course, hope that's okay.\nIn the meantime - can you sign our CLA: https://github.com/snowplow/snowplow/wiki/CLA\n. Big thanks @danisola - will take a look!\n. Thanks @morgante! Unfortunately have to close as a duplicate of #2798. Appreciate the contribution though.\n. Thanks @sbonami - can you sign our CLA please: https://github.com/snowplow/snowplow/wiki/CLA\n. Confirmed! Thanks so much\n. Thanks for the contribution @sbonami - this is a won't fix now as we are prioritising our DynamoDB-based Spark deduplication.. Cherry-picked into release branch, closing\n. Hey @kingo55 - we can't unfortunately merge a PR for this right now. The reason is that in the current architecture, all Snowplow installations are using the same hosted JSON Paths file, and we can't unilaterally change that file (adding a column) without breaking people's existing Redshift loads. @chuwy is soon going to start on a full rewrite of the StorageLoader to start resolving this problem, but until this is done, we are unfortunately stuck on the 1-0-0 schemas everywhere...\nYou're of course welcome to use the newer 1-1-0 in your own installation, and I'll keep this ticket in the Blocked schemas milestone so that people are aware of the gap...\n. Migrated to https://github.com/snowplow/iglu-central/issues/519. Won't fix, we are prioritising the Scala Stream Collector going forwards.. Hey @agentgt - we definitely plan on adding full nested entity support to Postgres once we've done a full refactoring of our Redshift load code.\nOur working assumption is that our Postgres approach will very closely mirror our Redshift approach - but this isn't something we've devoted any R&D time to, so it's all still to explore.\nPostgres table inheritance sounds interesting - not something we'd really had on our radar, I'd love to see a PR and then continue the conversation  :coffee:\n. Thanks for sharing @agentgt! Super interesting. In this approach, where do the event-specific columns get created and live?\n. Ouch! That's a bad bug. It slipped through because we don't test the Postgres code paths in Snowplow (which involve downloading the data to local) as actively as we should.\n. Hey @StaymanHou - yes it's just a fatjar, you can patch it as you like!\n. Looks great! Closing. Please add to Trello as done so we can track all this...\n. Closing, has been asked on Discourse: http://discourse.snowplowanalytics.com/t/installing-emretl-runner/608\n. Many thanks for raising @adamlwgriffiths, I've updated the wiki page with your fix.\n. The Netaporter library we use is the most permissive URI parser available for the JVM - it would be worth raising an issue there and seeing if they are up for tolerating URIs with 2 fragments attached? If not, we're really back looking at #351...\n. Cool, sounds like a plan @christoph-buente !\n. Wow! Thanks so much @theon.\n. Not yet!. Given this issue causes data quality issues, we will prioritise.... Adding to R92. R93 I believe:\nhttps://github.com/snowplow/snowplow/blob/master/CHANGELOG#L126. Really! That's a pretty big change...\n. Sounds good!\n. Hi @mdespriee - I like the idea of excluding password type fields by default - that seems like a no-brainer...\n. Migrated as this is tracker-specific\n. Hey @ironsideshu - can you merge this in with your other PR please? https://github.com/snowplow/snowplow/pull/2460/files\nAlso I am not seeing the Java load code in either PR - is there a third PR I missed?\n. Hey @ironsideshu - yes please, we need a PR into this project with all relevant code, including:\n- The EmrEtlRunner changes\n- The code for the custom jar for performing the DashDB load\n- The DashDB table definition for atomic.events\nWe need to treat this as a single unit of functionality for releasing this in Snowplow.\n. Closing as this PR has diverged significantly from how storage loaders are built for Snowplow now.\nApologies that we couldn't make this happen in this form.. Migrated to https://github.com/snowplow/sql-runner/issues/74\n. Not sure which release will be next yet, but have stowed this in R8x webhooks for now...\n. Hey @bernardosrulzon - the underlying problem is that the original &u=... value is not correctly URL encoded. If it were URL encoded, then e.g. http: would be http%3A%2F%2F, and utm_term%3Db-%2Bprofessor%20%2Bespanhol would be utm_term%253Db-%252Bprofessor%2520%252Bespanhol (double encoded spaces).\nIs there an option to correctly URI encode the &u= parameter? \n. Hi @Rajadas - I think it's the same issue? The %%ASPID%% case fails because % is part of the URI encoding. If you could flag that links containing %%ASPID%% should be treated as non-escaped, that should work?\n. Yes, the idea is that we add a separate QS parameter called e.g. neu (non-escaped URI) which:\n- must be the last parameter on the QS (because otherwise it's impossible to know where the URI ends)\n- must be treated as a non-escaped URI\nDoes that make sense?\n. I have broadened the description of the ticket to include another scenario, where:\n\nAd creatives need to go through an audit process before they go live\nIn the audit process the creative is reviewed to ensure that the clickthrough works.\nA user has macros within the click url which are populated at auction time by the bidder\nDuring the audit process, since the bidder does not serve the creative, these macros will be left as is and so will be passed to the collector\nThe collector returns a 400 due to this code\nThe audit process fails due to the 400 from the collector\n\nThis is a distinct scenario from the other one above, relating to unescaped macros inside the redirect URI.. Confusingly I renamed the ticket, making it a dupe of #2915. Renaming it back.. Bringing forwards. Bringing forwards. Hey @ironsideshu ! This is a really cool idea. We noodled around with something similar before - #997. The main difference I think I'm hearing here is that you want the geofence database to be an external lookup, rather than something static that is populated at startup time. Is that because you expect the database to be very regularly updated (hourly not monthly)?\nI'm going to close #997 in favor of this ticket. If this enrichment is dependent on that external API, you would need to release that either as a commercial service or as an open-source GitHub repo so that the wider community can leverage this enrichment...\n. Makes sense - let us know how you get on!\n. Duplicate of #2986. Hi @juanstiza - your URIs should look like this:\njson\n            \"geo\": {\n                \"database\": \"GeoIPCity.dat\",\n                \"uri\": \"s3://my-private-bucket\"\n            },\n            \"isp\": {\n                \"database\": \"GeoIPISP.dat\",\n                \"uri\": \"s3://my-private-bucket\"\n            },\n            \"organization\": {\n                \"database\": \"GeoIPOrg.dat\",\n                \"uri\": \"s3://my-private-bucket\"\n            }\nCan you direct future support questions to our Discourse forums?\n. Please share a link to the regression commit?\n. Hi @StaymanHou - can you direct support questions to our Discourse forums please?\n. I am happy with option 2. Can you remind me why we allowed empty custom contexts in 1-0-1?\n. Actually we should do both - if we believe empty custom contexts is A Good Thing, we should create tickets to add support for this to KES and the Scala Analytics SDK.\n. Thanks @chuwy - can you add an equivalent of https://github.com/snowplow/snowplow-scala-analytics-sdk/issues/21 for Kinesis Elasticsearch Sink and link it back to this ticket?\n. Oh sorry! I didn't scroll up and read the name of this ticket \ud83d\udc4d Please ignore!\n. Okay - thanks for checking - let's push back!. Missed the R87 release, pushing back. Assigning to @BenFradet, adding @jbeemster for some advisory. Closing given migration to https://github.com/snowplow/snowplow/issues/2975#issuecomment-293166874. Hi @joshuacox - you are right, we really need to update the documentation to cover the Kafka support. The current functionality is per the blog post:\nhttps://snowplowanalytics.com/blog/2016/11/15/snowplow-r85-metamorphosis-released-with-beta-apache-kafka-support/. Of course! Thanks @simplesteph . Depends on rslifka/elasticity#128\n. SGTM. @jbeemster - can you take a look at this?. Ping @jbeemster . Thanks @jbeemster !. Cool idea! Any strong feelings for/against from the community?. /cc @shermozle as he was asking about this one. Pushing back. Added the creds. Permissions still need to be setup.. Hi @amolbarewar - support questions to our forum please: http://discourse.snowplowanalytics.com/. Hey @falschparker82 - many thanks for the super-detailed and thoughtful ticket.\nI like 1b) - I have nothing against 1a) but I don't know of many (any?) users who don't serve sp.js via CDN...\nThoughts from the community?. Closing - have copied @falschparker82's great post into https://github.com/snowplow/snowplow-javascript-tracker/issues/499.... Hi @sokser - can you raise support requests on our Discourse forums please:\nhttp://discourse.snowplowanalytics.com/. Thanks @sokser . Are you running in Vagrant? It looks like you might be trying to assemble this from within your IDE (PyCharm)?. Our recommended way of doing this is via Vagrant - we can provide support to developers with that setup because any issue will be reproducible on our end.. Ouch, sounds like those apps are sending illegal UAs. More background relating to Spray:\nhttps://github.com/spray/spray/issues/458. Hey @christoph-buente - I believe Spray is no longer supported, and has been superceded by Akka HTTP.\nI suggest we do a few things:\n\nUpgrade to the latest version of Akka HTTP for the collector - we should do this anyway\nLobby the Akka HTTP team to see if they would re-consider tolerating non-RFC compliant useragents\nFork Akka HTTP if they reject the idea\n\nThoughts?. Hey @BenFradet - does R92 solve this? If so, we should move this ticket in.... LGTM!. Ping @BenFradet . Pushing back into @BenFradet's milestone.... Good idea! Scheduling. Good catch!. Good plan, scheduling. Migrating over content from #2946. Given that Stream Enrich is flexible around which source you connect to which sink, we have some interesting questions around how we successfully achieve at-least once processing. Here are some examples:\n\nKinesis to Kinesis: we should only checkpoint the read when the write has been successful\nGoogle Cloud Pub/Sub to stdout: we should only ack the read messages when the print to stdout has been successful \nKafka to Google Cloud Pub/Sub: we should only checkpoint the read when the write has been successful\n\nWe could actually build a whole matrix of the source-sink options and capture the required behaviors for at-least once processing.\nI believe that currently Stream Enrich performs optimistic checkpoints on the source, and this is a result of the design decision to have very loosely coupled:\nsource -> sink\n\nI think we need to re-architect Stream Enrich and introduce some kind of first-class Checkpoint entity, which we will use to deliver pessimistic checkpointing on the source:\nsource -> sink -> checkpoint \n   ^                    /\n    \\------------------/. Sounds great, thanks @simplesteph!. Interesting question. The Kafka behavior is exactly the same as the Kinesis behavior - the app name is used *primarily* to identify the consuming group from a checkpointing perspective. We haven't had any concerns about this from the Kinesis users so I think this is a non-issue, unless there is something specific about Kafka I am missing...   . I hear you - if you're very familiar with Kafka and understand consumer groups, you're probably going to hunt for a variable with that name. If you're not \"Kafka/Kinesis native\", then the idea that the app name identifies the app from a behavioral perspective (e.g. checkpointing, though you may not know about checkpointing) is pretty intuitive.. Please add to the tracker's repo, not snowplow/snowplow. Missing #2979 @chuwy. > The command \"cd ../..\" exited with 0.\n\n\nSkipping a deployment with the script provider because the current build is a pull request.\nSkipping a deployment with the script provider because the current build is a pull request.\n\nNeed to be incredibly careful that we never overwrite a prod Hadoop Enrich or Shred with a new build of a newer version with the wrong version number... . Thanks @chuwy - that gives me a lot of confidence too.... Switching over to the release PR.... This is awesome! Thanks so much @pkallos.... Assigning to @BenFradet to review. Closing this as R89 is almost a wrap. Thanks @pkallos !. Thanks for sorting all this out @chuwy - my commits are #2963, #2985 and #2986.. Hey @chuwy - can you liaise with @dmpacheco to get his avatar on #9c53ec3?. Sure - go for it!. Hi @spy-tech - you are right, this part of the Snowplow setup is quite confusing.\nFirst we need to define our terms:\n\nKinesis S3's LZO support takes a set of Kinesis records, treats them as byte arrays, and boxes them in Twitter's Splittable LZO envelope. As well as LZO files, Kinesis S3 also outputs LZO.index files which can be optionally used for more performant processing in Hadoop\nKinesis S3's GZIP support takes a set of Kinesis records, treats them as UTF8 strings, writes them to newline-delimited files and then gzips them\n\nThe output of Kinesis S3's LZO support is designed to be compatible with Snowplow's Hadoop enrichment pipeline, in support of our Lambda architecture. So you put a LZO-configured Kinesis S3 on the raw event stream, and the payloads sunk to S3 can be read by Scala Hadoop Enrich in the Snowplow Hadoop pipeline.\nBy contrast, Snowplow's Scala Hadoop Shred process expects to run on a folder of gzipped or raw newline-delimited Snowplow enriched events, which are UTF8 Strings. This in theory makes Kinesis S3's GZIP support a good fit for loading Redshift from the enriched event stream (by using Scala Hadoop Shred plus the StorageLoader). I say theoretically because we don't do this ourselves (we run the standard Lambda architecture from the wiki page above).\nOn to your questions:\n\nSince gzip is not a splittable file format, it seems that it won't scale. I.e., that the processing capabilities of all the real-time events would be very limited in throughput.\n\nI'm not sure I follow. Once you have written your Kinesis records to S3, they are outside of the real-time pipeline. Kinesis S3 has no problem keeping up with the incoming stream and writing it out to compressed files.\n\nI'd rather not sit with a pile of LZO data, which doesn't actually work.\n\nWork for what? If you have LZO data for your collector payloads, it will work fine. If you have GZIP data for your enriched events, it will probably work with Scala Hadoop Shred. We haven't implemented any other use cases for data in S3 currently.\n\nwhy gzip support exists in the first place\n\nThe GZIP support was added by @kazjote - so probably best to ask him about his use case (remember Kinesis S3 is not a Snowplow-exclusive project).\n\nwhy there is no splittable file support\n\nDo you mean why we haven't used Twitter's Splittable LZO envelope for our enriched events? It's a good question - I haven't really thought about this before. The truth is that the enriched event format needs a lot of revision - so we should definitely consider compression formats (including row-wise versus column-wise) as part of this.\n. Thanks for sharing that @kazjote - helpful context.... Can you add a description to this ticket please @chuwy ?. Right - yes the whole config file is expected to be present whether you are using Kinesis or Kafka, this is covered in the upgrade section:\nhttp://snowplowanalytics.com/blog/2016/11/15/snowplow-r85-metamorphosis-released-with-beta-apache-kafka-support/#upgrading. Great - closing.. I removed the stale content, closing. Can we finalize the name for this ticket?. Yes but apparently we don't hyphenate deduplication (per the CHANGELOG). Ah, this ticket doesn't have a commit, sorry for confusion!. Fair point! Scheduled into the next Kinesis milestone.. Yes. This ticket exists really just to track the bump in the underlying Spark Enrich, which will bump because of the other tickets in the release. I'll update the ticket name now. Changed to 1.x.0 as not sure what the version will be on release.. The title of the ticket was updated yesterday (it's still relevant). A good start! Please:\n\n[x] Rebase off master\n[ ] Action #3009\n[x] Cherry-pick the commit from #2725 \n[x] Action #3002 \n[ ] Deploy rc1\n[ ] Let me know so I can update ${customer} to use this version\n\nIf we can get this done by Friday lunchtime that would be great!. Under what circumstances does EmrEtlRunner auto-generate a VPC?. Looks great! Please:\n\nRebase off master\nAdd #3012 \nAdd #3013\n\nThen I'll cherry-pick across to the release branch. Done and cherry-picked over, closing. Scheduling into R87.... Pushing back. Please use our Discourse for support: http://discourse.snowplowanalytics.com/ . Ah wow - great sleuthing @acgray !. Duplicate of #3121 . Changed ticket name @chuwy so CHANGELOG and release notes are internally consistent. Do you mean \"RDB Loader: remove StorageLoader Ruby files\"? Replace doesn't seem correct as I just see deletions in this commit.. Note that this release of EmrEtlRunner should still be capable of running the older versions of Hadoop Shred if desired - it should be handled by a version switch (see existing code for a couple of similar patterns).. Hmm, good question - I think we should.... We're calling the component RDB Shredder, so the config file property should be rdb_shredder - updated the ticket title.... Let's do it, please rename the tags, commits, tickets etc.... * Commit title should be Relational Database Shredder: rename from Scala Hadoop Shred (close #3031)\n* Missing an update to this: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-hadoop-shred/project/SnowplowHadoopShredBuild.scala#L27. Looks good - you can widen this branch to include some of the other tickets and add commits into this same branch; no need to silo each ticket into a separate PR.... Assigning this PR into R9x [HAD] Scala Hadoop Shred reboot. It's coming along nicely! Looks like you still have the big refactoring out of the PartitionTap to do.\nI saw a mix of hadoop and spark for the package names, maybe that update isn't finished yet.... Interesting - I think we've actually had users ask for a structure more like this (because it's more Hiveable). I don't see this as a dealbreaker - we can upgrade StorageLoader in tandem to handle the new structure.... That sounds fine to me:\n\nRemember to make sister tickets for the attendant StorageLoader changes in the same milestone\n\nI would call it shredded-types rather than shredded-events as it contains contexts/entities as well as events . Sure thing @chuwy, what do you want the repo called?. How about:\n\n\nsnowplow-scala-app-skeleton.g8\n\nsnowplow-scala-lib-skeleton.g8\n\nalthough is skeleton implied by .g8 - what else is a .g8 but a skeleton?. Done:\n\nhttps://github.com/snowplow/snowplow-scala-app.g8\nhttps://github.com/snowplow/snowplow-scala-lib.g8\n\nYou have admin access @chuwy.. Cool! Will review. In the meantime, please crack on with the EmrEtlRunner and StorageLoader changes.... Great thanks @BenFradet - ready for @chuwy to do the next review?. @chuwy - could you prioritize doing this review for @BenFradet ?. De-scheduling. It was de-scheduled because it was closed, being superceded by #3094 per @BenFradet's comment above.. Duplicate of #3102. Renamed ticket to make it a command rather than a description. Yes, sequencing is everything here!. Updated title, please update commit!. StorageLoader no space. Descheduling as per @yalisassoon and @bogaert:\n\nThe requirement to populate the DynDB table before starting isn't a strong requirement at all. Anyone upgrading to this release will need to do a one-off process to remove historical dupes and that process, if run a few weeks after upgrade, remove the handful of new dupes that might be generated post upgrade because the DynDB table starts off empty. So we can probably drop this altogether.. Welp!. I vote for the move forwards to Scala 2.11 - it ticks more boxes, even if it takes longer.... @BenFradet - to me the move to Scala 2.11 is a dependency on your milestone. So I would do it in a separate branch (so we have flexibility on releasing as 1 or 2 releases), but yes crack on with it please! . It hasn't worked in the past, correct @rgabo. Exciting that this pain finally goes away!. That's great @BenFradet !. Fixed typo/capitalization in the ticket name, please update commit.... We use a lower-case letter after the : \"bump\" not \"Bump\". Hey @colobas - please split into two commits and then ping us for review.... This also means we can de-schedule #3045. Cleared milestone, please ensure this isn't in the CHANGELOG.. Moved to R89. No, pushing back!. Can you confirm @bogaert ?. De-scheduling. Closing given @bogaert's comment. Woohoo! What's next @BenFradet - do you need code review?. Awesome - @chuwy can you do a review for @BenFradet ?. Spotted a typo, otherwise the CI/CD bash refactor looks good @BenFradet . It's for building and deploying this artifact:\n\nhttps://github.com/snowplow/snowplow/tree/master/3-enrich/hadoop-event-recovery\nThis artifact is not run as part of the regular Snowplow run, so the changes made in this PR shouldn't affect it.... Thanks for all the QA support @rgabo - it's appreciated!. Hey @rgabo! We have R88 in final test; after the release of this, we will be rebasing R89 onto R88. At that point barring any issues that come up in the R89 QA, these jars will indeed be \"semi-official\".... Makes sense!. For just using the GeoLiteCity.dat, this works fine:\n```json\n{\n    \"schema\": \"iglu:com.snowplowanalytics.snowplow/ip_lookups/jsonschema/1-0-0\",\n\"data\": {\n\n    \"name\": \"ip_lookups\",\n    \"vendor\": \"com.snowplowanalytics.snowplow\",\n    \"enabled\": true,\n    \"parameters\": {\n        \"geo\": {\n            \"database\": \"GeoLiteCity.dat\",\n            \"uri\": \"http://snowplow-hosted-assets.s3.amazonaws.com/third-party/maxmind\"\n        }\n    }\n}\n\n}\n```\nThere may be an issue with the s3:// protocol and Stream Enrich - I'll update the ticket name.... See this thread for others having a similar problem:\nhttp://discourse.snowplowanalytics.com/t/ip-lookup-arrayindexoutofboundsexception/870/20\nMy question would be how authentication currently works in Stream Enrich for the s3: protocol with the IP Lookups enrichment.. Thanks @dashirov-ga - I have modified the ticket title again.... See also: #3084. Hi @vceron - thanks for the detailed bug report. The problem must be something to do with threads or similar, given that the vanilla unthreaded code works fine:\nvagrant@snowplow:~$ irb\njruby-9.1.6.0 :003 > require 'date'\n => true\njruby-9.1.6.0 :004 > Time.at(\"1484280061\".to_i).utc.to_datetime.strftime(\"%Y-%m-%d-%H\")\n => \"2017-01-13-04\"\njruby-9.1.6.0 :005 > Time.at(\"1484276462\".to_i).utc.to_datetime.strftime(\"%Y-%m-%d-%H\")\n => \"2017-01-13-03\"\nIf the problem is indeed https://github.com/jruby/jruby/issues/3670, then hopefully it will be fixed when we release R87 very soon. Can you re-test once this has been released?. Thanks for the additional detail @vceron . Glad the problem stopped for you from R87.... Why did you remove the labels @BenFradet ?. I disagree - a bug report is immutable - it intrinsically relates to data loss, and that doesn't change with the fix being in another ticket. Removing the work assignment metadata by contrast is fine.\nIn x months from now, I want to be able to go back and review bugs which relate to data loss. The ticket that resolved the problem is uninteresting in comparison.. Hi @seanhall - thanks so much for raising this. It looks like a pretty heinous copy-paste error. We will get this fixed! . Hi @bernardosrulzon - two things to check:\n\nIs continue_on_unexpected_error set to true, and if so are there rows in the error bucket?\nAre Hadoop Shred's deduplication algorithms responsible for the drop-off? . Right - if you are not seeing any events with that signature, then it's unlikely to be dedupe behind the drop off.\n\nA good next step would be to plug one of the missing enriched events into the Hadoop Shred test suite and see if it makes it through or not. That will tell you if the problem is inside the Hadoop Shred process or outside it.... Hi @bernardosrulzon - thanks for the update. I am not surprised by that discovery - but it is still puzzling that the events are going missing outside of the code path itself. Let us know what you find out!. Hey @bernardosrulzon - this is really interesting, thanks for the detailed bug report.\nDo you know if there was any volatility in your spot instances during the run - did you witness any of them being terminated/re-started?\nThings we know:\n\nThe Hadoop Shred job reads the enriched events for processing from HDFS\nThe task instances do not get added to the HDFS cluster (source)\n\nGiven this, it feels like the problem might be something like a spot instance been rebooted and then omitting some events when it resumes (i.e. \"at-most-once processing\"). Which would be a bug for sure.\nBut this is just speculation as to the cause - interested in whether others have experienced this?. Interesting - even if the task instances take 5 minutes longer, they must be all settled by the time Hadoop Shred kicks off. Very puzzling.... What region is all this running in?. Need to try to reproduce this with Spark Shred.... De-scheduling. Hey @chuwy - where is the code for this?. /cc @jbeemster as the Dataflow Runner author. If this is blocked by any missing features in Dataflow Runner, these should go into a priority 0.2.0 for Dataflow Runner.. This functionality must be written in Java so that it can survive an EmrEtlRunner port into Scala.... Hi @BenFradet - you raise good points. I think it's okay then to write this in Ruby, same as the rest of the codebase.... Okay cool - assigned you to the 0.2.0 milestone per our convo a little earlier.... Hey @BenFradet - I think that's reasonable - Dataflow Runner will happily terminate the cluster on a step moving into WAIT, as long as it is --run-transient.... This is nice but we will push back so we can get this release out faster.. @BenFradet to schedule this back in and clarify name and add follow-up tickets. Nice ticket name. Can this be closed in favour of the newer encoding work?. I'd make the Ruby bump an explicit ticket, just for clarity.... Oh sorry!. Excellent! Requesting initial review from Anton.... Hey, a quick bit of feedback on rc2:\n\nCalling emr-etl-runner --config (i.e. forgetting the run command)\ngives \"invalid option\" not \"invalid command\"\nwhich leads me to think there's a bug in the handling code\nwhich is a pretty hard-to-read mix of exception-bubble-up versus local exit1() helper function. Got it, so emr-etl-runner config would have triggered invalid command, but emr-etl-runner --config is treated as an invalid global option.. Please take a look at this one: https://snowplow.pagerduty.com/incidents/P9WTFX2\n\nI don't know if this is a R90-era bug, or a R91 regression, but we shouldn't be trying and failing to download the RDB log if the job failed at a prior step - because there will never be a log. /cc @chuwy  . I have manually added snowplow-hadoop-fs-rmr.sh to snowplow-hosted-assets to unblock testing, but this isn't ideal, and I don't intend to manually copy it to all the other buckets.\nCan we please:\n\nVersion this file like everything else for invocation (so this filename would be deployed as snowplow-hadoop-fs-rmr-0.1.0-rc1.sh and invoked as such)\nAdd it to Release Manager so that it is auto-deployed to all buckets. Can we harmonize the step names a bit? There's a lot of inconsistency.\n\nCurrently:\n- 1. Elasticity S3DistCp Step: Staging of s3n://some-logs-snplow/\n- 2. Elasticity S3DistCp Step: Raw S3 -> HDFS\n- 3. Elasticity Spark Step: Enrich Raw Events\n- 4. Elasticity S3DistCp Step: Enriched HDFS -> S3\n- 5. Elasticity S3DistCp Step: Enriched HDFS _SUCCESS -> S3\n- 6. Elasticity Custom Jar Step: Recursively removing content from hdfs:///local/snowplow/raw-events/\n- 7. Elasticity Spark Step: Shred Enriched Events\n- 8. Elasticity S3DistCp Step: Shredded HDFS _SUCCESS -> S3\n- 9. Elasticity S3DistCp Step: Shredded HDFS -> S3\n- 10. Elasticity Custom Jar Step: Load Snowplow Redshift eu-west-1 Storage Target\n- 11. Elasticity S3DistCp Step: Shredded S3 -> S3 Shredded Archive\n- 12. Elasticity S3DistCp Step: Enriched S3 -> S3 Enriched Archive\n- 13. Elasticity S3DistCp Step: Raw S3 Staging -> S3 Archive\nLet's rename the first two steps:\n- 1. Elasticity S3DistCp Step: Raw s3n://some-logs-snplow/ -> Raw Staging S3\n- 2. Elasticity S3DistCp Step: Raw Staging S3 -> Raw HDFS\nSimplify the name of the recursive delete:\n- 6. Elasticity Custom Jar Step: Empty Raw HDFS\nFor consistency, put \"S3\" last in the final three copies, and make more consistent:\n- 11. Elasticity S3DistCp Step: Shredded S3 -> Shredded Archive S3\n- 12. Elasticity S3DistCp Step: Enriched S3 -> Enriched Archive S3\n- 13. Elasticity S3DistCp Step: Raw Staging S3 -> Raw Archive S3\nThis should make everything much less artisanal.. But EmrEtlRunner retrieves the statuses of each jobflow step - doesn't that tell us whether the RDB Loader step failed or not? /cc @chuwy . Thanks @BenFradet !. One idea is to move these checks into individual jobflow steps (run on master), and make them terminate the cluster if they don't pass. Then it's on EmrEtlRunner (and later Dataflow Runner) to realize that these steps terminating the cluster means a benevolent no-op, not a job failure per se... Thoughts @BenFradet, @chuwy ?. You are right - we are losing the ability to short-circuit the launch of the cluster; that's never coming back.\nHowever:\n\nWe have to terminate the cluster if the previous run has failed/is ongoing - the pipeline is not currently setup to successfully work through a subsequent load if the previous one failed\n\n(Less important) there may be an advantage in shortcutting a no-data run early (vs just letting the pipeline run its course with no data). It may even be that a downstream component will break if there's no data to process - needs testing.. Thanks @BenFradet -\n\n\nImplement the empty dirs checks through simple bash scripts submitted as emr steps sounds good - the challenge is we need to figure out how to capture return codes from those bash scripts\n\nNote that the LZO files check will be gone soon (#2740)\nS3DistCp for archiving raw is coming soon (#1977)\nS3Distcp for archiving enriched/shredded will be tackled by @chuwy (#1777)\nthe cleaning up of the filenames performed in s3_tasks seems a bit too involved for an s3distcp step - agree; if we consider that we have four collector formats to handle (cloudfront, clojure, lzo sink, Urban Airship), my hunch is that 2 or 3 of these can be handled by S3DistCp with a bit of imagination and lowering our expectations about the resulting filenames; 1 or 2 of these will need a dedicated script/binary which runs on the EMR cluster. > Isn't a failed/succeeded step enough?\n\nYes, I think you will end up with:\n\nSucceeding/failing the step\n\nWriting custom code in EmrEtlRunner to interpret certain jobflow steps failing as a no-op with specific return code from EmrEtlRunner, versus currently where any jobflow step failing means that EmrEtlRunner failed.. The difference is that any DirNotEmptyError or a NoDataToProcessError should not be escalated to an operator for investigation - they should not generate an incident waking up an operator in the middle of the night to verify that they are dealing with a no-op versus a failure (which does require operator intervention).. I think this sounds good @BenFradet. Maybe split this into two tickets:\n\n\nOne covering (both) DirNotEmptyError scenarios\n\nOne covering the NoDataToProcessError scenario. Closing and de-scheduling this given we have specific tickets now.... Thanks @chuwy - yes we should rename the existing, but that doesn't have to go in this release (please create a new ticket!). The table the permissions should be on should be: table/snowplow-integration-test-crossbatch-deduplication. Ticket where the variables we need to rename were added:\n\nhttps://github.com/snowplow/snowplow/issues/2963. Okay, I've added the commits. Still to do:\n\nTidy-up into one commit (or maybe add another ticket/commit for adding all the nice comments back in)\nUpdate the call sites for these renamed variables. FYI the updated secrets were added like this:\n\n```\ntravis encrypt AWS_DEPLOY_ACCESS_KEY=\"xxx\" --add\ntravis encrypt AWS_DEPLOY_SECRET_KEY=\"yyy\" --add\n```. Oh welp, I misread the ticket sorry. Will update. Done, ran:\n```\ntravis encrypt AWS_DEPLOY_ACCESS_KEY_ID=\"...\" --add\ntravis encrypt AWS_DEPLOY_SECRET_ACCESS_KEY=\"...\" --add\n```. Yes, check the branch https://github.com/snowplow/snowplow/commits/issue-3115. Hi @geetanshjindal - please direct support questions to our forum:\nhttp://discourse.snowplowanalytics.com/. Hi @gmpetrov! Please direct support questions to our forums:\nhttp://discourse.snowplowanalytics.com/. Thanks. * The global --skip emr makes no sense, agree\n I think the ability to skip individual steps doesn't make sense because the whole pipeline is linear - out of A,B,C,D,E,F, it doesn't make sense to do A,B,D,E\n However, it is still important to be able to resume from partway through, because if there is a failure partway through the job you need to have the ability to resume from partway through the job\nSo maybe this ticket is about replacing --skip with --startFrom/--resumeFrom?. We've changed our mind on this - having the option to still --skip steps gives us more flexibility, especially in terms of migrations.. Fixed the ticket name, please update the commit too. Makes sense. This brings up-to-date EmrEtlRunner's \"dead man's handle\" detection of existing data in the pipeline, where the existence of that data means that a previous run is ongoing (or has failed partway through). EmrEtlRunner uses this detection to prevent itself from running.\nIn fact, this is a flawed approach. The problem is that the existence of data in the checked folders is not a sufficient check to determine whether another pipeline run is ongoing (or failed). This is because it takes an amount of time to boot a new pipeline run in EMR, during which time no data has been written, and so this prior ongoing run cannot be detected.\nThis was a far smaller problem before #276, because previously the file moves were done by Sluice, and the file moves to processing folder were commenced within a few second of startup, so the window for this race condition was just a few seconds, not 5-12 minutes.\nThe better alternative to this ticket is to use an end-to-end locking mechanism, similar to the locking mechanism found in Dataflow Runner.. De-scheduling. This is a nice feature, because it prevents an EMR cluster from spinning up unnecessarily if there is nothing to process, but:\n\nIt adds a lot of complexity\n\nIt would be infrequently used (most pipelines have some data to process each run) . Descheduling. I don't understand this idea any more - spinning up an EMR cluster to run a jobstep to see if there is data to process defeats the purpose of the check in the first place.... Script should live in:\n3-enrich/emr-etl-runner/config/convert_targets.rb\n\n\nnot:\n3-enrich/emr-etl-runner/convert_targets.rb\n\nPlease update!. Made ticket name less cryptic. @BenFradet why does the commit message not match the ticket name? This is super confusing. We always want a 1:1 match. Correct. Thanks. Fixed capitalization, please update commit. EmrEtlRunner doesn't know that 100% of events have gone to bad rows. What's the problem we're trying to solve here?. Sorry typo, fixed. See also #3149. Thanks for all this @colobas! @chuwy - Gui has completed his internship now. Based on this, my thinking is that you do a review of this PR (there is no urgency to do this), and then we assign another engineer to apply the changes (so we basically get two sets of eyes on it). What do you think?. Hey @chuwy - can we get away with just a SCE 0.24.1, or do we have to release a SHE too?. Cool, let's schedule into R88, update CHANGELOG etc. Thanks!. > Could be not included anywhere\nCan't parse this statement - it's going into R88?. Ah right - so yes you mean \"Snowplow apps are not yet including this version\". > And actually they could never include it.\nHow come?. Of course, thanks - they \"would never include it\" - they \"could\" but it would be pointless!. I think it was Event Manifest Populator ?. Updated the ticket name, please update the commit!. Ouch, scheduling. Can you define what you mean by \"aggressively cached\" Josh?. Thanks @jbeemster. Blocked by Alex adding the underlying S3 bucket.. Thanks good catch. Thanks so much Stephan, assigning into R88.... No it's okay thanks, @chuwy will handle it!. An alternative idea here could be to write a dedicated Flink/Dataflow job which uses the API to inspect a sample of events and fire an alert if PII spotted. So it becomes more of an out-of-band detection process, rather than jumping to automatic expunging (which is difficult to get right).. Hi @liamtarpey - I'm assigning this to Anton for his thoughts.. Separate ticket is good!. Thanks, updated the documentation, closing!. Ouch! Thanks for raising @samuelgiles . Hi @jatinder85 - I've renamed the ticket to report on your first issue.\nWhy is it confusing using the same buffer object for Kinesis and Kafka? The semantics of Kinesis and Kafka are near identical.. Thanks - a PR would be great! (No special magic to contributing a PR to Snowplow). Hey @tyomo4ka - please can you direct support requests to our community forums:\nhttp://discourse.snowplowanalytics.com/. No, R88 is locked for release now. We should consider what to do with compupdate going forwards (a discretionary CLI argument doesn't make much sense once StorageLoader moves to EMR).. Thanks @miike , assigning to Ihor. Thanks @ihortom . Thanks @dilyand , assigning. Hi @hose314! We have no plans for React Native on our imminent roadmap. Would it look like bindings to our native mobile trackers, or would it be based on our JavaScript Tracker - what do you recommend? If the latter, this might be relevant:\nhttps://github.com/snowplow/snowplow-javascript-tracker/issues/432. Makes sense @hose314 - have you considered opening a PR into either of the JS Tracker projects to achieve compatibility?. Thanks @hose314 . @chuwy , @BenFradet - what do you reckon?. You make a good case @chuwy. Happy with the axios change if everybody else is - it looks like a very popular / widely used library.... Hi @halilduygulu - can you use the forum for support questions like this:\nhttp://discourse.snowplowanalytics.com/. Great discussion topic! Please migrate to our Discourse:\nhttp://discourse.snowplowanalytics.com/. Assigned to @ihortom . Ouch, thanks, @ihortom can you fix please!. Oh hey @BenFradet. This is an interesting one. There's some kind of mechanism in the GCP SDKs to customize the useragent making the API calls. Presumably the purpose of this is so that GCP can understand who are the biggest consumers of the APIs.\nI don't know what value we should use - but if you can store that as a constant somewhere, we should be able to nail down the value before launch.... https://github.com/snowplow/snowplow/blob/master/CHANGELOG#L2. Love this idea! It's kind of like \"avalanche-client\".... I like it! The only thing I am not super-hot on is the TSV format. It feels a bit artisanal / YAP (yet another protocol). Why not:\n\nUse a Thrift or Protobuf\nUse standard naming for each field based on the Snowplow Tracker Protocol. Thanks @bernardosrulzon ! Assigning to @chuwy to review.. Ah thanks for flagging, I am running the manual sync process to copy all of the final assets into each of the region-specific buckets. Should be up within the next 60 mins. Closing.... Please assign the reviewer when you are ready for review @BenFradet !. Yes to both - back to StorageLoader: and need a version bump on that. Should be 0.11.0 not 0.10.1 (minor not patch). Hey - \n\n```\n      # Versions 0.5.0 and earlier of Hadoop Shred don't copy atomic.events into the shredded bucket\n      OLD_ENRICHED_PATTERN = /0.[0-5].[0-9]/\n  # Versions 1.8.0 and earlier of the relational database shredder didn't use the field name\n  # when writing partitioned datasets\n  OLD_SHRED_PATTERN = /[0-1]\\.[0-8]\\.[0-9]/\n\n```\nThis doesn't work with the various -rcs we release and use.. Hmm good Q - let's add an issue but put it in @chuwy's release as he is working on the StorageLoader port right now.... What's the need to delete the rcs? They were rcs.. Generally we don't delete rcs unless there is some kind of security issue or similar. Better to think of artifact repos (whether Bintray or S3 or Maven Central or whatever) as append-only.. No that's great @chuwy - thanks for capturing this here!. Can we migrate this ticket into the RDB Loader repo @chuwy ?. Ah great, please split out into a dedicated commit to map onto this ticket.... Chat to @yalisassoon about this @ihortom - Yali is managing the re-housing of old docs off the Snowplow website to new homes.... Ouch - that's a nasty bug, scheduling. Cool thanks Ben. StorageLoader is one word. Sounds right to me! I'm not surprised we are finding a few remaining skeletons in our test suite :-). Assigning to @bogaert for review. No plans to extract a library from Hadoop/Spark Shred, but yes Common Enrich is coming to Maven Central soon, assigning to @BenFradet to clarify.... /cc @shermozle . This is StorageLoader, not EER no?. De-scheduling. Could also potentially include #3248. Note that we would need to consider the IP address still being in the header.. No, geo-IP enrichment is orthogonal.. We would be effectively extending the IP anonymization enrichment.. We need to push this back as it is dependent on a change to the Tracker Protocol, which will need further thought.. We don't need to update the Tracker Protocol - we can just add a self-describing context to each event flagging the IP anonymization.. Let's call it the Google Analytics Adapter, because this adapter is a strict superset of the Measurement Protocol (because of the two non-MP schemas).. Made the name a little bit clearer. Thanks for the fast review @BenFradet. @chuwy please:\n\nMake those changes\nDeploy all of the rcs\nStub the release blog post, but crucially complete the upgrading section (you can write the rest of the blog post later)\n\nThanks!. Also note that there are tickets in the milestone @chuwy that are missing from this PR. For example:\nhttps://github.com/snowplow/snowplow/issues/3242\nPlease itemise all outstanding tickets in this thread, along with your estimate for how long the ticket will take to complete. We'll then decide whether to include or push back.... Note this has been locked as R90 Lascaux, https://en.wikipedia.org/wiki/Lascaux. Hey @chuwy - unfortunately resuming from archive_enriched doesn't work. This is because the etl_tstamp generated by EmrEtlRunner does not match the one used by the previous run to store files. Any thoughts on a quick fix (!) for this?. Here's the current error when starting at archive_enriched:\n```\nD, [2017-07-07T15:16:44.166000 #24577] DEBUG -- : Initializing EMR jobflow\nD, [2017-07-07T15:16:51.411000 #24577] DEBUG -- : EMR jobflow j-XXX started, waiting for jobflow to complete...\nI, [2017-07-07T15:24:55.286000 #24577]  INFO -- : NO RDB logs\nF, [2017-07-07T15:24:55.295000 #24577] FATAL -- : \nReturnContractError (Contract violation for return value:\n        Expected: nil,\n        Actual: true\n        Value guarded in: Snowplow::EmrEtlRunner::EmrJob::output_rdb_loader_logs\n        With Contract: String, String, String => NilClass\n        At: uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:583 ):\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts.rb:45:in block in Contract'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts.rb:154:infailure_callback'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:80:in call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:inblock in redefine_method'\n    uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:554:in run'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:insend_to'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:inblock in redefine_method'\n    uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:78:in run'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:insend_to'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:inblock in redefine_method'\n    uri:classloader:/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in '\n    org/jruby/RubyKernel.java:973:inload'\n    uri:classloader:/META-INF/main.rb:1:in '\n    org/jruby/RubyKernel.java:955:inrequire'\n    uri:classloader:/META-INF/main.rb:1:in (root)'\n    uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1:in\n'\n``\n. Sounds great @chuwy - let's create a ticket for a future release to throw an error if there's more than one folder in enriched good (because otherwise you can resume from archive_enriched, it archives one folder, but the next job still can't start).. Apologies for the confusion, I'm sure everything re.archive_enriched` is well-handled in rc4, but actually the stacktrace I pasted above @chuwy was from a fresh from-the-top rc3 pipeline run:\n```\nD, [2017-07-07T15:16:44.166000 #24577] DEBUG -- : Initializing EMR jobflow\nD, [2017-07-07T15:16:51.411000 #24577] DEBUG -- : EMR jobflow j-XXX started, waiting for jobflow to complete...\nI, [2017-07-07T15:24:55.286000 #24577]  INFO -- : NO RDB logs\nF, [2017-07-07T15:24:55.295000 #24577] FATAL -- : \nReturnContractError (Contract violation for return value:\n        Expected: nil,\n        Actual: true\n        Value guarded in: Snowplow::EmrEtlRunner::EmrJob::output_rdb_loader_logs\n        With Contract: String, String, String => NilClass\n        At: uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:583 ):\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts.rb:45:in block in Contract'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts.rb:154:infailure_callback'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:80:in call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:inblock in redefine_method'\n    uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:554:in run'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:insend_to'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:inblock in redefine_method'\n    uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:78:in run'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:insend_to'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:inblock in redefine_method'\n    uri:classloader:/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in '\n    org/jruby/RubyKernel.java:973:inload'\n    uri:classloader:/META-INF/main.rb:1:in '\n    org/jruby/RubyKernel.java:955:inrequire'\n    uri:classloader:/META-INF/main.rb:1:in (root)'\n    uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1:in\n'\n```\nI suspect it's a permissions problem - though the error message is quite misleading (contract failure). Any idea how I go about investigating Anton?. Thanks @chuwy , that's good to know. There is a separate issue I am also seeing, which I will email you about now.. Makes sense, please create a ticket for this and add to the R9x RT milestone. Thanks for the detailed review @chuwy !. Tweaked title. Right - the easiest way to fix this is to forget about EmrEtlRunner, think about what properties are needed and then think what would the HOCON file look like if we writing this as a RT pipeline app today.... 1-1-0 or 2-0-0 @chuwy ?. Cool, is this the last ticket in the release?. Please raise support questions on the forum:\nhttp://discourse.snowplowanalytics.com/. > Do we need a \"release\" to take data modeling out of Snowplow core?\nYes, when we move things out we:\n\nDuplicate into a separate repo\nAnnounce the new release\nThe next snowplow/snowplow release removes the legacy content.\n\n\nDo we want a single \"data modeling\" repo or one per model? I'd start with one repo.\n\nI would go one repo per data modeling bundle (SQL, metadata, Looker bindings etc). Just because that is the unit of release.... Thanks @chuwy - I think you are thinking about the underlying problem in exactly the right way. I am thinking on this issue a lot this week - look forward to brainstorming some different approaches next week.. I think one important question here is, do we lint this as a YAML, or do we move this whole file to a self-describing JSON Schema, and lint that instead?. Ah yes - this is why we changed the file extension (for Spark support).\nWhat are our options here?. Yes agree. I saw this in the UI recently too - definitely worth exploring.... Interesting idea. The trouble is that the Scala Stream Collector would have to have knowledge of all the different payload formats (Snowplow, GA, webhooks) which can post N events... So it's a lot of business logic and JSON parsing to add into the collector.. Hi, please direct support requests to our forums:\nhttp://discourse.snowplowanalytics.com/. Hey! Please raise on our forums:\nhttp://discourse.snowplowanalytics.com/. Hey @Tiny-wlx - it's not an architecture we are looking at supporting any time soon:\n\nThe Scala Stream Collector has certain features (with more on the roadmap) which an API Gateway cannot support\nAn API Gateway-based solution will only work on AWS, and we are moving Snowplow to having first-class support for multiple clouds, plus on-prem. Also this commit: 9e950c3d58cf84dacdbb6e78eb6c68fe0c1ce932 should be broken out into the associated tickets.... 1. This is a support issue not a bug\nYou have already posted this to Discourse: https://discourse.snowplowanalytics.com/t/getting-error-in-stream-enrich-while-running/1309\n\nClosing this, and warning you @sandeshputtaraju that you are operating outside of the norms of the Snowplow open-source community.. I like the look of scalaj-http (and the scalaj org in general!). Thanks for flagging @esquire900 !. Adding data-loss label.. Hey @aldemirenes - can you break out a separate commit for #3294 please. and #3339. No, just hardlock, there's no safe softlock scenario I can think of.... Given explanation two:\n\nthrown away (in that case deduplication isn't working with full force).\n\nI'm treating this as a data quality issue.. De-scheduling.. Two ways of fixing this:\n\nWe switch from config files on CLI to some kind of Control Plane, which we secure\nWe use the EC2 Secret Store, as we do for Redshift Loader configuration. No, that's not how the EMR API works. > most people won't go through the trouble of looking at the EnrichedEvent class to find out which property they want to use\n\nNo, we would point them at the official docs for the enriched event model:\nhttps://github.com/snowplow/snowplow/wiki/canonical-event-model\nThe limiting factor here I think is more that people might want to partition on e.g. a user identity in a custom context, and this PR wouldn't let you do this. Still, I think this is a good first phase of functionality. . Let's come up with a shortlist. List looks good @BenFradet !. Thanks @BenFradet. To confirm, the Spark Enrch or Shred jobs have no knowlege of any errors-related CLI args or similar?. Cool thanks. > I just wanted to quickly evaluate the platform\nThe quickstart is for starting development work on the codebase, not for running Snowplow (we have plenty of pre-built binaries in Bintray and artifacts in S3 for that).... > Is there anything we can improve in message\nThe problem is that the old error messages used to print out the whole COMMIT that failed, which was quite helpful context around the specific command that failed.... I like it!. Many thanks for this @miike ! We are looking into ZSTD very seriously now, so this is great timing.. When adding (git cherry-pick) into the pre-release branch, make sure to update the commit message.... No, Knossos has been kicked off already. Per the off-site, we are trying not to add tickets to sprints that have already started.. I don't think this is blocked - we can emit the newer schemas, they will pass validation, we simply will be loading the older versions (1-0-0s) into Redshift.. We have to remove this otherwise when we try to resume from e.g. archive_raw, we can get a return code of 4 and message:\nThere seems to be an ongoing run of EmrEtlRunner: Cannot safely add enrichment step to jobflow, s3n://my-bucket/enriched/good/ is not empty. Updated ticket name. Fixed title. This is an urgent fix. R92 is very fragile without this.. Hmm. Is there a reason why we wouldn't follow the same behavior for the other no-ops? Here is a (admittedly tenuous) scenario:\n\nUser is running without lock, RunA\nData is in enriched\nUser upgrades to using lock\nNew job starts, RunB, creates lock but no-ops because it finds data in enriched\nRunA completes but doesn't delete lock\nRunC+ cannot start because RunB did not delete lock\n\nThoughts?. Correct - no-ops are fundamentally limited to these:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/bin/snowplow-emr-etl-runner#L97-L104\nYou should be able to work back to the original exception throw-sites.... /cc @chuwy . Good idea, adding the security label.... @chuwy we are just going through all the label:security tickets. What do you think we should be doing here exactly?. Could we simply delete all these release \"assets\" using: https://developer.github.com/v3/repos/releases/#delete-a-release-asset. /cc @stdfalse . Can you clarify the nested booleans, \"if enrich\" and \"if shred\". What conditions are you referring to here?. I think @chuwy wrote this recovery code? I'm not familiar with it.. Would you guys mind jumping on a Zoom together? Can be tomorrow morning of course, but we can't afford to introduce a regression here.... Please do this release on Monday @BenFradet !. If you think this has affected you, please read:\nhttps://discourse.snowplowanalytics.com/t/important-alert-r90-r91-bug-may-result-in-shredded-types-not-loading-into-redshift-after-recovery/1422. Thanks @rgabo !. See also https://github.com/snowplow/snowplow/issues/336. Let's add into R94 then, which is edging into a batch pipeline release.... Awesome - thanks @miike . @BenFradet can you do code review on this next week?. Yes good idea, we have had customers asking for this... Any suggestions on the metrics?. Sure - Ihor can you do this one.... Capitalized ID for consistency with existing CHANGELOG entries. This is a bug right? Please always flag bugs with a description, impact and of course the label. We don't bury bugs.. Agree - the problem comes from S3DistCp, so we should fix it in EMR.... Yep that's fine with me.... Taking a step back on this one: why was this implemented as a bootstrap action, not as a jobflow step? It's got nothing to do with preparing the EMR cluster for the job.... What will be the new approach?. Can you flatten down to just three commits please @keanerobinson, with the commit titles matching the GitHub ticket names exactly?. There is no risk of accidental duplication of data across retries here?. @chuwy can you share a link on the random string point? Maybe I can provide some context.... So I think we are generating a random fingerprint to avoid accidentally deduplicating. But I agree that shouldn't be surfaced.\nI don't know if I agree that event fingerprinting should be mandatory and not configurable, but happy to hear the arguments. But I do agree that it's very confusing to say that deduplication = event fingerprinting + DynamoDB storage target. There's nowhere where we say deduplication = ON.. > Well, I would like to put in other way round. What's the purpose of having it as an enrichment?\nI'd say the purpose of it being an enrichment is that everything should be an enrichment - in other words, there should be less hard-coded magic in the core enrichment process, not more. You could argue that Snowplow is not Snowplow without e.g. event fingerprinting or calculating the derived timestamp, but the more prescriptive we are, the more we limit the use cases / future audience of Snowplow.... > What about leaving fingerprint enrichment as it is now, allowing users to configure it, but change a shredder code substituting random UUID with \"default\" algorithm?\nWhat about throwing an exception / failing the process if an event_fingerprint is missing? Clearly the user wants to dedupe, but they can't because the fingerprint enrichment is switched off, so why proceed?. Useful for environments where a POST is not an option but you want to send in e.g. numeric values (e.g. ad-tech).\n/cc @ihortom . Reminder that we need to update this tutorial following release as the cost-sending example is broken:\nhttps://discourse.snowplowanalytics.com/t/how-to-track-ad-impressions-and-clicks-tutorial/29. I vote for 3. Agreed, the webhook adapters should respect tnuid.... Thoughts from @BenFradet on which is the \"correct\" behavior?. Hmm. The Tracker Protocol is quite confusing on this point:\nhttps://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol#15-user-related-parameters\nYou could reasonably ask, why does the Tracker Protocol specify both nuid and tnuid? By definition everything in the Tracker Protocol is t*.\nThe reason is this:\nhttps://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/java-servlet/war-resources/.ebextensions/server.xml#L138\nThe Clojure Collector \"hacks\" a nuid parameter into the querystring - so it's really a server-side property, masquerading as something tracker-sent. This is why we had to introduce a tnuid to identify the truly-tracker sent nuid. The SSC has no such issue (it adds the network ID as a separate dedicated column in the Thrift).\nSo in a way, both implementations are correct, given their starting point. tnuid is important in a Clojure Collector world; it's unnecessary with the SSC.\nGiven that we will retire the Clojure Collector, I am tempted to deprecate the tnuid option (but never retire it, as we never break old event archives) and not implement #3450. Thoughts?. > And as far as I know all cookies are present in thrift payload anyway, so we're not overriding anything in-place?\nI believe so - all the cookies are recorded in the headers, plus there's a dedicated slot in the Thrift for the actual nuid.. Thanks for sharing this @miike !. This mustn't get bumped from the release.. Yes, http://docs.aws.amazon.com/streams/latest/dev/server-side-encryption.html. Sounds good!. Thanks Ben!. Thanks for this @chuwy.. Can we fit this into R95 @BenFradet?. Closing as dupe of the better https://github.com/snowplow/snowplow/issues/3530. Good point!. @BenFradet @chuwy any feedback on this as it stands?. Hey @miike - wiring this into our schema technology is definiely planned as a phase 2, please add your ideas there to #860.\n\nIs it overkill to add a salt/shared secret option to the configuration?\n\nNo it's a good idea. We want to make the hashing as secure as possible, and eventually we will probably support encryption as well.... We have gone forwards and backwards a bit on the idea of including a salt. We've taken it out again for now. The description of it was:\n`salt` should be a secret string (at least 8 characters long) which increases the entropy of the hashed value, preventing brute-force dictionary attacks.. Hey @knservis - I am thinking of us limiting the pojo options to a whitelist which corresponds to the fields that we will bump per this comment:\nhttps://github.com/snowplow/snowplow/issues/3528#issuecomment-351835563\nCan you think of any reasons why not to restrict in this way?. Great, I have updated the spec. I'm not sure if you'll be able to enforce this at the JSON Schema level (as an enum) or if you'll have to do it in code.. @yalisassoon - can you update the whitelisted list of PII fields in the spec above to include any other fields which could include PII but weren't included in #3528 - e.g. all the se_ fields. Thanks @knservis !. The commit should be entitled:\nCommon: rename AWS deployment credentials in .travis.yml (close #3115). Best style for a ticket name is \"Actionable command to make an objective, discrete change\" hence:\n\nScala Common Enrich: make enrichments.ExtractEventTypeSpec timezone-safe\n\n(The original title \"enrichments.ExtractEventTypeSpec fails in my timezone\" was more like a bug report.). \"Default\" -> \"default\". Thanks @bernardosrulzon. We will keep this ticket open to track the requirement, but it's very unlikely to be implemented before the move to Dataflow Runner (at which point we will migrate this ticket).. I like option 1 as migratability to Dataflow Runner is a must.... I like the idea but I don't this is urgent.. I like this idea...\n\nideally we'd like to know that sampling has occurred and the sampling rate downstream\n\nWe can have a sample context, but I wonder if we also emit a singular event to express that a user has been excluded from the sample. That's a lot more robust than trying to reverse out \"real event levels\" based on the sample rates reported in the events that do make it through... It also lets you construct more accurate total metrics if you want (because you know the properties of the users who did get excluded from the sample).. Hmm, reading @christoph-buente's issue, the default placeholder here really ought to be ${SP_NUID}.. Thanks @rbolkey !. U in Use should be lowercase (use). Is this a blocker for R95, or can wait?. Updated title. Bringing forwards. Okay makes sense to me. @BenFradet please ack the plan. Ouch good catch - @BenFradet make sure to always keep that page of links up to date!. Iglu Central R82 is out. What's next here @knservis ?. I'll review the blog post as my next step. @BenFradet what needs to happen to get your green tick against this?. Great! Next step is to put the rc live on the standard canaries.. Awesome, UA is the next step - getting the rc onto our canaries. @BenFradet let me know if you need any help with that.. Just fix the tests as part of your release.. Improved title, please update git commit!. This ticket needs renaming. This ticket didn't make it into the R97 PR, so presumably it's either no longer required or should be pushed back?. Looks great! I think you're missing shred cardinality under Shred ends.... Ah good catch. Potentially we could re-write these to use Java? This would be way more performant. . @yalisassoon can you please review:\nhttps://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/atomic-def.sql\nand add to this ticket the full list of the sensitive columns which need to be widened?. Nudge @yalisassoon . Edited!. What about se_label @yalisassoon ?. Yep they are - good point!. Putting this into Knossos - but if this is \"too big\" then we can push back.... Better commit messages:\nScala Common Enrich: configure automated code formatting (closes #3496)\nScala Common Enrich: apply automated code formatting (closes #xxxx). Retitled ticket name (commit should be adjusted). This commit is going to look terrible in the CHANGELOG and release notes. Can you come up with a better name @knservis ?. Cool idea! There are probably a few webhooks we could auto-validate. Welp yes, typo, fixed. See also #3468 (let's harmonize the names or tickets). Why is there a separate whitespace commit?. Apologies, must have been butter-fingers. Is this something we should be fixing in the open-source documentation, and/or in the Snowplow Insights UI?. Closing this one then, please chat to Mike about this.... Good catch. Assigning to @ihortom . I love the name!. Looks like we are waiting on @knservis' code review now. Weird bug, what happened there?. Got it. Thanks for raising @ihortom - assigning to @BenFradet to investigate.. Don't de-schedule - keep it in until confirmed by @ihortom . Hmm good point. I think I prefer the first one because it's more templating friendly.  @chuwy what do you prefer?. Not sure I quite understand @rbolkey's suggestion?. Not sure. Your call.. De-scheduling pending further investigation of the underlying issue.. Hi @wheller - sounds like something has changed in the latest Leiningen to diverge from the assumptions in this repo.. Switched reviewer to Ben. Looks like your rebase has gone wrong. Hey @knservis - please add me as reviewer when we get to the QA/UA stage.. Hey @knservis - please add me as reviewer when we get to the QA/UA stage. This is currently in code review stage.. Thanks for the detailed feedback @BenFradet . Seems wise!. The name of this ticket needs a bit of work, or we might split it into two tickets.. Yep I think that's good for now. We'll probably want to box around EnrichedEvent in due course, as that envelope will technically contain multiple enriched events now.. Hey @knservis - what do you mean by\n\nemitting a salt\n\nSurely the salt should be very closely held?. BTW I've scheduled https://github.com/snowplow/snowplow/issues/3648. SGTM, I think it's worth adding.... Yep, you can migrate!. Removed milestone. De-milestoning pending our workshop. As in log out in the stderr?. I like the truncation as I don't want us to ever have to do log collection on the containers... . I think from a PII / GDPR perspective we can't have bad rows going to stdout - as that represents a potential PII leak.... In a separate track, can we:\n\nRebase\nTreat this as definitely R100\nCome up with an archaeological site. Intriguing.... Thanks for the PR @miike!\n\n\nthose are strong assumptions and that makes me wonder if it should live in the repository or elsewhere, e.g. a guide of some sort\nHive/Athena Loader has a right to exist\n\nI agree - for me the missing \"piece in the middle\" is the Snowplow BAPHM Loader (Blob storage, Avro, Parquet, Hive Metastore, apologies for the \ud83e\uddc0 ). This is discussed further in this Discourse thread:\nhttps://discourse.snowplowanalytics.com/t/snowflake-db-other-storage-targets/1721/5?u=alex\nThe other thing this PR has raised is that we really need to crack on with the enriched event refactor. The later we leave it, the more loaders and storage targets there are to migrate.... Yep, HQL isn't something we want to add formal support for now, especially given the upcoming atomic.events refactor.. Yes, this has been end-of-life for a while. It's worth using our other useragent enrichment. Yep, I would go with deprecating, we have to keep it around for backwards compatibility.. That's a good point. I think we would deprecate it (\"eol\") and then remove it a bit later. Key thing is that the legacy data in the enriched events in Redshift still lives somewhere (in the context).. So:\n\nMigrate the structure to a context (part of events refactor)\nProvide a warning on startup that enrichment is eol (suggesting alternatives)\n\nStop executing enrichment, and remove jar. Yep good idea, no need for the warning to be blocked by the refactor. No, there will be an RFC first.. Let's update this ticket name to make clear we are just showing a deprecation warning.... Yes, we probably should keep it at least on our radar... Re-opened it and closing this as a dupe.... Pretty much every maintenance ticket we create has prior in the CHANGELOG - check the CHANGELOG for the right naming convention.... No, gdpr-1 should be for Stream Enrich, not Spark Enrich.. Hey @knservis - please add me as reviewer only when @chuwy has green-ticked. That will be my cue to get this deployed and tested.. @chuwy is out today. Let's create a bug issue to track this edge case. Exciting!. Agree with @jbeemster - the whole point of EmrEtlRunner is to reduce the \"support surface area\" for new users and ourselves.... Thanks, scheduled those in!. Thanks @chuwy - @BenFradet please ack this plan.. This is probably a good idea. Circe seems to be what everybody is moving to.. Component is Stream Enrich, not Streaming. I think you can add this into your milestone.... This sounds great @chuwy. Let's discuss with @BenFradet straight after our weekly.. Interesting idea @colmsnowplow. Integrating client- and server-side sessions feels quite tricky but powerful if we can get it right.... Can you ASCII-art out the dataflow you are imagining @colmsnowplow? I am having trouble pinning it down.... I think there are two options:\n\n\nGenerate the session ID server-side and render it out client-side for the JS Tracker to use\n\nGenerate the session ID client-side but use cookies to make it available in the back-end (via the HTTP request headers)\n\nI haven't thought these through much further yet.... Makes sense. I don't think option 1 is particularly feasible anyway - as the session calculation logic isn't easily \"shippable\" to the server-side.\nOption 2 is quite interesting and even exciting. It's inspired by:\nhttps://github.com/simplybusiness/snowplow_ruby_duid/. Do you mean the back-end generates the page view ID and renders it into the front-end for the client-side events, or something else?. Thanks for raising @chuwy - agree we should add.... Yes, this makes sense. @chuwy could you flesh out this ticket with your suggestions for replacing this?. Yes, we should add the optional id to help identify the configuration instance.\nI also wonder if we should use this to properly vendor all the enrichment configs, as some are a bit non-standard.... Thanks @pm-fawkes ! Assigning.... Please direct support questions to our forums:\nhttps://discourse.snowplowanalytics.com/. Yes, this would be cool. I thought we had a ticket for this already, but can't find it.... Thanks for flagging - what a terrible implementation decision by Zendesk.... Good Q @razius - adding @ihortom to the ticket who built the webhook integration on our side.... I think you are probably right... It's probably safest to fix this in @stdfalse's data model.... Should be \"add\" not \"Add\". Hey @BenFradet - I think this is likely to be R102 now.. Yes - I agree with @knservis - we should be going with GooglePubSub or similar, because PubSub is a category not a brand name (unlike Kafka or Kinesis).... Np - we can just put placeholder entries in so the CHANGELOG coheres.... Release TBC. Hi @sowieso-fruehling - please open your issue in the relevant repo:\nhttps://github.com/snowplow/snowplow-java-tracker/issues. Are you talking about the variable name or its value?. What do you want the new value to be?. Why not just stream-enrich-0.14.0? That's the component name.. Cool, let's go with stream-enrich-${version}. I think this is a good plan. I can't think of any scenarios where we need e.g. NSQ to Kinesis or Kafka to stdout; if those come up, they can be handled with some kind of bridge app (e.g. NSQ->Kafka) added on the end.... Hey @knservis - huge apologies for the delay in getting to:\n\nLooks like the emitted PII event should be an actual EnrichedEvent here is how I propose to populate it (sorry about the wide format) @alexanderdean\n\nIt mostly looks good, but I suggest some changes:\n\nstream-enrich is an invalid platform value, let's use srv\nLet's use same (not new) as etl_tstamp\nLet's not set the dvce_created_tstamp\nLet's set the true_tstamp to the same as the derived_tstamp (i.e. a new timestamp)\n\nThat table also shouldn't have the pii column at the bottom - that pii column is a hack in the code, it's not a formal part of the enriched event.\nThanks!. Thanks for the thorough report @falschparker82, much appreciated.\nI've renamed this ticket and assigned it to the next GDPR milestone, which @knservis is working through. A basic enrichment-configuration-based salt should get us started, although we are interested in more sophisticated salts which incorporate a canonical user ID as well.\nInformation security is a process, not a single hero feature or release. It's great that we are getting thoughtful feedback like @falschparker82's, and that we can then quickly feed that into our further GDPR-oriented releases. Thanks again!. Note: the salt should not be directly stored in the enrichment configuration, but should be retrieved from AWS. See @chuwy's storage loader configurations for guidance.. googleCloudKms :-). I think that's fair. @chuwy do we support the same with the loader configs?. Makes sense, let's do that here too.. Thanks @colmsnowplow . Yes, we just bump for now. We are looking at Wurfl as another enrichment option (we would never swap out the underlying implementation of the existing enrichment).. I wanted to dig into:\n\naws dependency in sce, I think this is a no-go unfortunately\n\nI can probably guess, but can you set out why, @BenFradet ? If we can't do this, then we will probably have to wait on the Snowplow Control Plane before we can probably secure secrets that are otherwise embedded directly into Snowplow enrichment configs.... I wanted to dig into:\n\naws dependency in sce, I think this is a no-go unfortunately\n\nI can probably guess, but can you set out why, @BenFradet ? If we can't do this, then we will probably have to wait on the Snowplow Control Plane before we can probably secure secrets that are otherwise embedded directly into Snowplow enrichment configs.... > What we could do is decline the sce into sce and sce-aws and have aws specific artifacts depend on it.\nI think it's better if we just park the AWSisms and use this as another reason to push on with Control Plane.\n@knservis are you okay to back the AWS bits out into a parked branch?. > As an aside would it not be better to have those as environment variables anyway?\nEnv variables don't deploy particularly well (the seed value for the environment variable itself has to live somewhere).. This is actually really cool @rbolkey.\nWe were thinking of adding something very similar for a somewhat different reason: letting Snowplow operators provide information about what this collector is on the root to help data subjects understand who is collecting their data.. It looks good to me - but let me add @yalisassoon as he may have some thoughts.... > can't the content-type be specified through the headers anyway?\nOh, good point. This can be added to a later release.. This is separate from #3675 because that bump doesn't itself dictate the change of behavior (merely that the new behavior is available).\nThis may not be required though given that @tmacedo wrote in https://github.com/snowplow/scala-maxmind-iplookups/pull/35\n\nI tried to minimise breakage and change as little as possible, retaining the exact same interface for the enrichments to use.\n\nHowever, if nothing else we will want to update the test suite.. No problem @tmacedo, thanks for clarifying. Big thanks @misterpig - can we get a note from you (GitHub comment is fine) saying you are happy for us to pull that page into our Snowplow wiki in due course (i.e. you are contributing it to Snowplow).. Thanks @misterpig !. This ticket was in the right place @BenFradet - @ihortom will tackle this.. Commit doesn't correspond yet. Why is there a change to emr_job.rb in this commit?. Awesome work @misterpig !. Thanks, assigning to @BenFradet for investigation. Let's migrate this into the iOS Tracker's own issue tracker.. Let's migrate this into the iOS Tracker's own issue tracker.. Cheers @misterpig !. What does force mean in this context (I guess I'll see in the commit!). Nice idea. How is this behavior change going to look in the Beanstalk configuration UI?. Got it, thanks. Why would it involve JSON Paths?. When we do this, it would be cool to use this as a test case or tutorial:\nhttps://github.com/TheEconomist/big-mac-data. Adding @ihortom too. This is already supported by the SQL Query Enrichment:\nhttps://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow.enrichments/sql_query_enrichment_config/jsonschema/1-0-0. I don't think this is blocked - we can emit the newer schemas, they will pass validation, we simply will be loading the older versions (1-0-0s) into Redshift.. Thanks @chuwy, good catch.. Updated title. Updated title. Good idea. Good point!. > handling lzo files in the shredder\nThis probably wouldn't work so well (as the LZO format we use with raw events is a bit non-standard).... What's our chance of getting this into R103?. Makes sense - let's start sketching out the next high priority batch release. @chuwy can you compose that milestone please.. Please create something like https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/duplicate/jsonschema/1-0-0 but with a better name.. Scheduled. Updated title. We will have a third soon (referer-parser).. Pushing back makes sense.. Can you provide a description/spec for this please?. Is there a problem at @asgergb's build stage?. Hey @asgergb - would you mind pinging me an email? Would love to have a chat about your Snowplow adoption.. De-scheduled. Lol. Thanks @BenFradet - Rostyslav please action Ben's feedback.... Remember @rzats that once you get the green ticket, you need to start work on assembling the R107 PR (have you come up with an archeological site yet?). What's the hold-up here @rzats ?. /cc @yalisassoon . @jbeemster can you confirm which creds we should use here?. I don't think I understand this one. Why would we add a whole new technology stack (Akka actors) when you could just add Stream Enrich instances to handle additional load? Stream Enrich scales horizontally already.. Hey @szareiangm - thanks for this. I think I am starting to see your idea.\nIt doesn't seem to be about scalability - it seems to be about pluggability, and the ability to extend the enrichment process with out-of-band code which isn't part of the assembled jarfile.\nThis is something we've considered before with enrichments - see eg:\n\nhttps://github.com/snowplow/snowplow/issues/2054\nhttps://github.com/snowplow/snowplow/issues/2055\n\nI think your suggestion specifically relates to Snowplow adapters - i.e. you would like to support non-standard input sources via Akka? Is that right?. It's a cool idea. We don't have the concept of configurable adapters yet - that would be a fair chunk of work to do this, but would be a nice complement to our enrichment engine.\nFeel free to create a separate ticket for an Akka-powered enrichment.. Scheduled into R109. Cool - is it in the CHANGELOG?. Closing, support questions should go to our forums: https://discourse.snowplowanalytics.com/. Nice idea. Need to consider what \"to disk\" means in a container-world.... I'm not sure I understand this - if we are going to adopt encryption, why not implement it instead of hashing, rather than on top of hashing? The idea of encrypting the pre-hashed values if the strategy is hashing could be handled on the Piinguin side.. Right - but why solve this by complicating the enrichment, when instead Piinguin could just have a configuration setting to only store keys in encrypted form?. Great idea. Do we need to open a PR into the ua-parser project to de-couple the rules database?. Love the name!. Exciting! How is the blog post coming @rzats?. Are we squeezing this into R107?. Updated ticket name as there is a newer format. But @mdemri - you are using the wrong configuration setting for the CloudFront collector. Please raise a Discourse thread and we can help:\nhttps://discourse.snowplowanalytics.com/ . Do we need to specify the cookie value or domain?. > do you mean as additional checks, i.e. make sure it has this value and this domain, before triggering do not track?\nYep. This would be an opportunity to standardise this kind of selective-enrich across all enrichments (some enrichments e.g. API Lookup have a flavor of this already).. https://github.com/snowplow/snowplow/wiki/API-Request-enrichment\n\nIf a key required in the uri was not found in any of the inputs, then the lookup will not proceed, but this will not be flagged as a failure.. > The reason is that when a user of stream enrich specifies a pii setting he does that, because he intends to emit pii events.\n\nThat's right.\nCan we clarify exactly why we are suggesting decoupling this? I found the quoted comments from Josh in the description confusing.. Yep, that's the plan.. That's right @anistal - you want to configure EmrEtlRunner with \"stream enrich\" mode per:\nhttps://github.com/snowplow/snowplow/wiki/EmrEtlRunner#stream-enrich-mode. Closing - this is not a fix.. Thanks @miike . Would this be a Tracker Protocol v3?. No - that would be a great idea. TP3 feeds into the atomic.events refactor too.. Duplicate of #2260. Closing this as it should really live in the trackers.. @BenFradet I am confused by this release, specifically:\nSpark Enrich: bump scala-common-enrich to 0.35.0 (closes #TBD)\nand:\nhttps://github.com/snowplow/snowplow/issues/3869\nThere are a couple of other commits which reference Spark Enrich too, which is also a puzzle.\nAre we doing a new release of Spark Enrich, or not? The blog post doesn't mention it.. I am still not following, sorry. Why are we changing Spark Enrich through e.g. d4f7835 without changing Spark Enrich's version number?\nI read the current commitset as leaving Spark Enrich in an undefined state - its behaviour has changed since pre-R109, but that functionality is not reflected in a new version or release, nor is the change documented in the blog post.\n. Orthogonal: in this file:\nhttps://github.com/snowplow/snowplow/blob/release/r109-lambaesis/3-enrich/config/enrichments/ua_parser_config.json#L10\nCan we rename:\nregexes.yaml\nto:\nuseragents-latest.yaml\nregexes is too vague, and is inconsistent with the approach we are taking with referers.. > original dataset is actually named regexes.yaml\nFair play, let's stick with regexes-latest.yaml and keep the origins clear.... I would go with a (smaller) option 2.. I think this relates to the S3 Loader - and may already be available as a feature (just not configured like this). Please raise a ZD to L3 Eng.. Not particularly nice from a RESTful perspective, but probably a good idea in terms of utility.... > I think cache should be optional.\nAgree, I think that's as simple as changing the minimum cache size to 0, currently it's... 1?\nhttps://github.com/snowplow/snowplow/wiki/API-Request-enrichment#cache. How does this differentiate between:\n\nThe array in the POST body represents an array of events\nThe POST body represents a singleton event which contains an array at its root?\n. That's not quite what I mean - my question is, without this commit, does the above cited payload still work, it just behaves differently (it's treated as a singleton event which contains an array at its root)?\n\nI am trying to get at whether with this commit we are adding new functionality, or simply changing existing behavior?. Ah, okay. I think this behavior change is fine, given that it's highly unlikely that anybody was using the old approach, and the new approach is functionally very valuable.\nHowever, we should make clear in the ticket title and the release post that this is a behavior change.. What's the impact of the AWS/Tomcat change on existing Clojure Collector users @BenFradet ?. Advertising. Raising here is fine as we don't have a dedicated issue tracker for our AMP support. Ouch. How did this slip through?. Thanks @jankoulaga ! @christoph-buente mentioned this might be incoming.. This is somewhat similar (although this is an adapter not enrichment) to: https://github.com/snowplow/snowplow/issues/2054. I think this should be a separate repo.... This is no doubt a good idea!. Thanks @colmsnowplow! @mhadam can you take a look?. Thanks for looking into this @colmsnowplow !\n\nThis leads to unusual values in the case of events which are sent late due to connectivity issues.\n\nI am having trouble visualising this. If the event is sent late, it should be sent with a correspondingly late dvce_sent_tstamp, and then the current derived_tstamp logic should be fine.\nAre we saying there is a bug in a tracker where let's say the dvce_sent_tstamp is set to the first send attempt, rather than the final (successful) send attempt, which would certainly break the current logic?. Right, yes - the calculation is sensitive to a change in clock between creation and sending.\nWe could probably mostly fix this by tracking device TZ before sending and then adjusting as needed (i.e. manual correction to dvce_tstamp in tracker). Only mostly fix because we can't detect other changes like a manual clock adjustment. But they must be extremely rare on mobile.. Yep I reckon it must be super-rare.. Cool, in due course update the description to have a full spec.. > I'm thinking that Tracker just needs to send Thrift payloads\nThat was my first thought at the hackathon presentation, but I didn't mention it at the time.. This seems very wise.. Is there a break; missing here @mtibben ?\n. I suggest we rename payload to properties as payload is technical language, properties is business language\n. Let's shorten to ue_na which is consistent with tractStructEvent's naming approach\n. I think for consistency and futureproof safety we should rename to ue_pr (for properties).\nThis is also because we will only be supporting a subset of JSON (no nesting, we will need an opinionated date format etc).\n. ^^^ I think this is the right soln to this issue, versus the approach being explored in this PR: https://github.com/snowplow/scala-maxmind-geoip/pull/12\n/cc @potomak, @bugant\n. s typo\n. This Tuple should be ArgsConfigEnrichmentsTuple no?\n. require 'sluice' and you can use the trail_slash function instead\n. what is the significance of these \"/not-used/\"s? Shouldn't they be Nones?\n. Makes sense. Could you add a comment and rename to \"/ignored-in-test/\"\n. Actually we can! Could you add in a couple of eid= into the test suite (mutate existing tests)\n. Indentation of comment is off\n. Or how about\nscala\nif (Option(co.network_userid).isEmpty) {\n  for (uid <- ci.userId) co.network_userid = uid\n}\n. True\n. Some whitespace issues (could you fix the table you copied from too - I think these whitespace issues were inherited from an existing Redshift DDL?)\n. Whitespace\n. Whitespace\n. Whitespace\n. Whitespace\n. Whitespace\n. Whitespace\n. Hey Fred - yes please then, tabs all the way!\n. Hmm. I reckon we shouldn't use the ti_ naming convention, as that's arguably more Tracker Protocol hangover than clean JSON Schema. I would say we drop the ti_, as the fact this is an item/product is kind of obvious...\n. Actually, I think for clarity we should prefix with item, so itemSku, itemName etc.\n. I reckon we should drop pagepath. If you want to override the page uri, it feels like this should be done by adding a custom context with the updated page uri, rather than a dedicated field which will only live in the social interaction table.\n. Hmm, actually I say let's not add item. I think it's confusing. Sorry for the flipflopping\n. Looks good!\n. Can remove this comment\n. Make 0.13.2 for consistency with other projects\n. Can we make the in and out stream-name examples consistent with each other...\n. Does this need to be stringly typed? Can't we use an ADT with Good and Bad values? Then we don't have an exception possible here...\n. ADT or indeed Enum\n. You could build the ADT here\n. The pattern is fine but I would probably put executor in a Validation box. Then you can have something like\nscala\nexecutor match {\n  case Success(exec) => exec.run() // () because this is insanely side-effecting\n  case Failure(err) => throw new RuntimeException(err)\n}\n. Can you add a comment blurb onto this object please! \n. Add a // TODO: move this to shared storage/shredding utils in due course\n. And reference your ticket about this\n. Switch this into a guard: case None if validRecords.isEmpty - should make it easier to grok. This whole bit looks a bit imperative (must be all the {}s) even tho it's not...\n. Move this into a private helper function\n. These are related so wrap them in a private object:\nscala\nprivate object GeopointIndexes {\n  val latitude = 22\n  val longitude = 23\n}\n. I like the use of Set!\n. Remove blank line\n. Ah finally all those TODOs make sense. A package object would be great. Can you put that in?\nThere's an easy shuffle to do this:\ncd src/main/scala\nmkdir com.snowplowanalytics.snowplow.storage.kinesis\ngit mv com.snowplowanalytics.snowplow.storage.kinesis.elasticsearch com.snowplowanalytics.snowplow.storage.kinesis/elasticsearch\ntouch com.snowplowanalytics.snowplow.storage.kinesis/package.scala\n. Scala lang imports should be below Java ones but above Scala libs\n. Capital E\n. Add a // TODO: move out into a kinesis helpers library\n. Add a // TODO: move out into a kinesis helpers library\n. Whitespace wrong\n. can we change \"fix up\" in this file to what the function should actually be doing? Seems to be a catch-all\n. Java lang imports first\n. Capital U\n. Capital C\n. Joda-Time belongs above all Scala imports (as is a Java lib)\n. Nice approach\n. Nice\n. Iglu under Scala libs\n. Tiny point but if something is going to end up in a List, I normally imply that with \"...ToEvents\" rather than spell it out with \"...ToEventList\". A plural of Objects is normally contained in a List. \n. Move down to Scala libs\n. I reckon we should enforce that all Adapters have a VendorName in the parent trait.\n. Nice\n. Add a linebreak above here\n. \"Is a \" -> \"A \"\n. Great\n. Test sentence copy-pasted from another test\n. Test sentence copy-pasted from another test\n. Whitespace\n. Whitespace\n. What does .event contain? Is it repetition of the table name?\n. I would put .data.ts much higher as it seems to be common to all event types\n. Looks good\n. Whitespace\n. If these contain JSON because we don't support nested shredding, just add a -- comment to that effect on the end of the row..\n. That could be cool...\n. So in the table message_delayed_1, will .event always==  \"message_delayed\", or something else?\n. Yep let's remove - the instance is the event type...\n. Nice\n. UaParserUtilityConfigEnrichment -> UserAgentUtilsEnrichment\n. Looks good\n. The argument is the name of the enrichment, so this should be ...](\"user_agent_utils\")\n. UaParserUtilityConfigurationEnrichment -> UserAgentUtilsEnrichment\n. Second enrichment should be called \"UaParserEnrichment\"\n. -> \"ua_parser_config\"\n. -> \"ua_parser\"\n. -> \"user_agent_utils\"\n. -> \"UaParserEnrichment\"\n. What is this comment doing here? ^^^\n. This is Java code. There's no way this is compiling. Don't open PRs that don't compile.\n. Where is the code for this Enrichment?!\n. \"user_agent_utils\" here, not config, as this is the name of the overall enrichment...\n. correct\n. ^^ remove this commented out version please\n. Good\n. Use 2 spaces, not tabs\n. Can you rename to UserAgentUtilsEnrichmentConfig\n. Can you rename to UserAgentUtilsEnrichment\n. NICE\n. No ( ) here, it's a case object.\n. Can you rename to UserAgentUtilsEnrichmentSpec?\n. Should be \"user_agent_utils_config\"\n. Change conf => to _ => as we don't use the variable\n. Remove the ( _ ) as it's not doing anything\n. Change comment to // Parse the useragent using user-agent-utils\n. This line ^^ should be indented two more spaces\n. As in the whole block needs one more level of indenting\n. Change comment to \"* Enrichments relate to browser resolution\"\n. Comment is copy-paste error\n. Add new line above this\n. Remove last trailing line\n. No need for expected, just use beSuccessful(UserAgentUtilsEnrichment) below\n. Indentation is slightly off. cases are not aligned. line up val ca ... with Option(event.useragent) indentation\n. Remove additional blank line\n. Should we do this with a SchemaCriterion?\n. Twitter not twitter\n. Nice\n. Can we add a sentence about how we handle _framing (where one message ends and the next begins)? Explain how we use the Elephant Bird project's BinaryBlockWriter, and the Protocol Buffer array. Will save somebody a lot of pain...\n. Make description more descriptive - LZO etc\n. %  \"elephant-bird\"...\n(whitespace)\n. %  \"collector-payload-1\"\n(whitespace)\n. Should this start with Snowplow? It's not really Snowplow-opinionated...\n. Release 60 Bee Hummingbird (2015-01-xx)\n. This looks good! Can you line up the <-s and the = please.\n. Good start! A couple of things:\n1. You're going to have to compose possible errors across all four doubles in your return value\n2. .toDouble is unsafe. You'll need a helper function\n. Agree, good spot\n. Remove this line ^^\n. No underscores in variable names\n. Remove commented out code blocks\n. You can start this import at registry:\nimport registry.TransactionAmounts\n. No. CU.stringToDoublelike returns a ValidatedString. You cannot simplify stringify this. You need to work with the type.\n. The good news is that the errors from CU.stringToDoublelike are already reported in the overall success or failure of the EnrichmentManager, so you just have to worry about the happy path (you can do a pattern match to pull out the good values).\n. Add EOL\n. Change \"Error is type...\" to \"Open Exchange Rates error of type...\"\n. We use imports at the top of our file rather than fully packaging classes. So import OerResponseError and Money at the top of this file.\n. I want to see multiple tests on the failure path\n. And more happy path tests\n. Create a lambda to reduce code duplication of tryParse...\n. WHOA. We don't take errors and convert them into 0 values. This is terrible practice.\n. Do we need this import?\n. Currency is a string like GBP, it's not something that should be parsed to a Double.\n. Currency is a string like GBP, it's not something that should be parsed to a Double.\n. Currency is a string like GBP, it's not something that should be parsed to a Double.\n. Currency is a string like GBP, it's not something that should be parsed to a Double.\n. This function ^^ is downsampling again. We're losing the essential information about whether we have a success or failure. \n. How is Option((trCurrency,TransactionAmounts(trTotal, trTax, trShipping), tiPrice, tiCurrency)) ever going to be None? It's never going to be None.\n. What's this tuple for?\n. Remove, add linebreak\n. ^^ these are Joda-Money not Scala Forex\n. These are // Scala Forex\n. Your API Key should be populated from sys.env\n. Transaction Price looks more like currency to me\n. That's a Joda Time not Scala Forex import\n. Remove extra blank line\n. Remove ^^\n. Start this import from enrichments\n. :[Either.left.value on Right] doesn't mean anything to me as an error...\n. .get is unsafe. It will error on a None. Remove all .gets.\n. Remove this function. It is downsampling again. You are taking a Success or Failure boxed value and stripping out the Success or Failure. You cannot do this: you cannot take a String that is tagged with Failure and just turn it into a String. We no longer know if we have a success or a failure on our hands.\n. Why would these errors be returned inside a TransactionAmounts? They are error messages, not successful double-like Strings.\n. trCurrency is the currency we are converting from, not to. Re-read how scala-forex works.\n. This function should look much more similar to this: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/utils/ConversionUtils.scala#L328\nNone for errors is not a good response.\n. Also missing a return type in the signature...\n. /\n\n/ for function docstrings.\n. baseCurrency in Scala Forex confusingly means the opposite of what it means in this Enrichment. In Scala Forex it means \"the currency we can assume an amount to be in if it's not specified\".\n. You are dismantling a tuple you only built (pointlessly) 5 seconds before...\n. Hey @fblundun - can we come up with a more human-readable ordering/numbering/grouping of these fields please. Anyone coming to this cold won't be able to discern any pattern to this structure... A more coherent ordering would look something like:\nRequired fields which are intrinsic properties of HTTP\nRequired fields which are Snowplow-specific\nOptional fields which are intrinsic properties of HTTP\nOptional fields which are Snowplow-specific\nIt would be worth giving each of these four clusters their own block (100, 200, 300, 400 kind of thing).\nAlso - it seems odd that path is required. I think path always being set is just an artifact of the Scala Stream Collector implementation - certainly path can be null in our CloudFront and Clojure Collectors.\n. \"To\" not \"For\"\n. This looks good to me, but you don't need to assign to currencyConversion and then immediately return currencyConversion. You can just have the expression as the implicit return.\n. Why \"Final\"?\n. READ THE ABOVE NOTE\n. Move your failure tests into a def e2. No need to have successes and failures in the same test.\n. Where are the tests for invalid doubles?\n. These spec names are not helpful. What is the actual thing being tested?\n. Just add a // TODO: remove val assignment and we will fix\n. No\n. No\n. Remove space between \" ,\n. Remove spaces before and after \"ti_pr\"\n. Add // TODO: remove unnecessary val assignment\n. Rename cuConvert to currency\n. Move this ^^ comment to above the first java import above\n. Add space after //\n. Move ^^ down to under // This project below\n. \"bs\" isn't a good abbrev. in English (http://www.urbandictionary.com/define.php?term=BS). Use baseCurrency here, that's fine.\n. Remove this , baseCurrency = bsCurrency. They don't mean the same thing, so it's confusing. Let the ForexConfig use its \"USD\" default behind the scenes, we won't use it anyway. https://github.com/snowplow/scala-forex/blob/master/src/main/scala/com.snowplowanalytics/forex/ForexConfig.scala#L45\n. Either remove the unnecessary val assignment, or if that gives you a compile error (I don't see why it would), then add // TODO: remove unnecessary val assignment\n. This is both currencies missing no?\n. Move this block down, to just above the val currency = block\n. I'm puzzled how this compiles. The POJO field is called event.collector_tstamp and it is a String, not a Joda DateTime. What am I missing?\n. Change this definition to:\nstringToMaybeDouble: Validation[String, Option[Double]], and update your code accordingly...\n. These tests aren't quite right. Remember, that a given enriched event will have either the transaction (tr_) fields set, or the transaction item (ti_) fields set, or neither. Never both.\n. ^^ Unnecessary whitespace\n. Rename to stringToMaybeDouble to make clear the Option boxing in the result\n. ^^ Remove todo\n. This should be in the block two above ^^\n. Should be * (space in front of the the *)\n. Remove blank line above this def ^^\n. Is this function used? Delete if not...\n. Inconsistent capitalization in column names ^^^\n. \"Amounts\" in the variable names ^^ doesn't add anything. trTotal would be fine etc.\n. Delete this field. This is a copy of collector_tstamp\n. Replace event.co_tstamp with raw.context.timestamp\n. Remove this dependency\n. Remove this whitespace removal\n. Remove this ( \" stray whitespace\n. Update this as discussed in HipChat\n. YES!!!\n. Some(errorcoTstamp) should just be None. And delete errorcoTstamp\n. Some(errorcoTstamp) should just be None. And delete errorcoTstamp\n. Add missing:\nscala\n case None => unitSuccess\n. Capital f for FieldNames please!\n. We just need apiKey - no need for lat, long, timestamp\n. I think these should be upper-case as enums (consistent with EOD_PRIOR)\n. Is there a simpler syntax here? Something like:\nscala\nfor ((total, tax, shipping, price) <- convertedCu) {\n  ...\n?\n. I think this logic should be moved to MiscEnrichments.\n. I know it was my idea not to make this a formal enrichment, but it's quite meaty code to embed in EnrichmentManager. Suggest we move it out to one of the Enrichments modules.\n. These FOREIGN KEY references are in the wrong place (they were copied from an erroneous schema I wrote). This is the right format: https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/com.google.analytics/cookies_1.sql#L37\n. I think we should take this opp to bump both contexts fields to 15000.\n. I think the family fields at least should be 255 for sure. The mobile_context is much more generous with the field sizes: https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/com.snowplowanalytics.snowplow/mobile_context_1.sql\n. This would become DEVELOPER\n. Any reason this says Override not \"Google\"?\n. Any reason this says Override not \"Google\"?\n. We can turn it into an option then:\nscala\nfor ((a,b) <- x.toOption) { println(s\"$a,$b\") }\n. Sounds good!\n. Could you add a note here something like:\n// Note stringToMaybeDouble is applied to either-valid-or-null event POJO properties, so we don't expect a Failure\n. Can we add a newline at end of file please?\n. Could you add a comment like // Assign empty Map on missing property for backwards compatibility with 1-0-0...\n. Could we rename these case class parameters? The names are a little confusing. Rather than mktMedium, I would go with mediumParameters, clickIdParameters etc.\n. This line will be a bit easier to grok when its clickIdParameters.find rather than mktClickId.find\n. \"should\" is a bit ambiguous in a comment - people can mistake it for \"a developer really ought to implement this in the future\".\nSo better would be:\n// Should never happen (prevented by schema validation)\n. Could we report the OER error type too? https://github.com/snowplow/scala-forex/blob/master/src/main/scala/com.snowplowanalytics/forex/oerclient/OerResponseError.scala\n. Could we improve these failure messages (capitalization etc) a bit? Also inconsistent [] versus :\n. Remove spaces between function names and ( ^^^\nAlso :String and if(\n. Should be:\nAdds a period in front of a not-null version element \n. a not an\n. I think \"enrichment\" not \"enrichments\", as would never be plural for a single user.\n. Can we add a newline at end of file\n. Add \"Apache \" in front of Httpclient\n. Could you tidy up the spec names? Lots of inconsistent capitalization etc...\n. Can we rename from jsonTestNew?\n. Can we put each ClientAttribute property into a column in the data table? Should make this test much more readable... \n. Can we add a test for empty string, \"_sp\" -> \"\"\n. Can we add another good with \"incorrect\" capitalization\n. Can you add a // Reminder what -1 does\n. Can you align -- Removed with lines above, and make it \"Removed in 0.4.0 primary ...\"\n. \" ... and increased size\"\n. Hmm, good point, we should at some point but it'll be rather disruptive. Can you create an unscheduled ticket in Iglu Central to do this?\n. Ouch no good catch! The defaults are fine: https://github.com/snowplow/scala-forex/#311-forexconfig\n. Is this line a copy-paste error @fblundun ^^?\n. @fblundun - table name should be com_snowplowanalytics_snowplow_ua_parser_context_1\n. 040_pk should be 050_pk @fblundun \n. Should be char(3) I think\n. Should be char(3) I think\n. And char(3) here too!\n. Can we bump this to 15000\n. 19 lines up from this @fblundun - the MaxJsonLength should now be 15000\n. Typo @fblundun - refr_dvce_tstamp not ramp\n. Hey @fblundun ! Can we widen some more columns:\nhttps://github.com/snowplow/snowplow.github.com/blob/posts/r63/_posts/2015-03-28-snowplow-r63-red-cheeked-cordon-bleu-released.md#72-updated-fields\nPlease ping @yalisassoon if any of the updates are unclear.\nMany thanks!\n. @fblundun - can we add the derived_tstamp tickets into the CHANGELOG please?\n. This should be CU.validateUuid no @fblundun ?\n. Case class small c\n. Case class small c\n. Nice\n. Do these helpers ^^ need to be public?\n. spaces after commas\n. Unnecessary whitespace\n. Too much whitespace ^^\n. Why all the whitespace?\n. Nice\n. Too much whitespace ^^\n. Move magic constant up into top of class\n. Duplication of magic constant from previous comment above - centralize somwhere\n. TODO: yech, yech\n. Too much whitespace above\n. Put a docstring on with a tparam for each field\n. Good\n. forcible -> force for a more verby method\n. Public?\n. Too much whitespace\n. Add comment for tracker arg\n. Add comment for tracker arg\n. Add comment for tracker arg\n. Nice\n. Sounds good to me...\n. Indentation ^^\n. Comment can be removed ^^\n. https://github.com/snowplow/snowplow/issues/2028\n. case Some(t) for consistency with new function below.\n. Where is this function used?\n. This implementation is fine but please add a TODO:\nscala\n// TODO: given missing collectorTstamp is invalid, consider updating this signature to\n// `..., collectorTstamp: String): Validation[String, String]` and making the call to this\n// function in the EnrichmentManager dependent on a Success(collectorTstamp).\n. Please change this one to page_ping from ping\n. Does a JSON with { \"schema\": 23 } throw an exception here?\n. ... Looks like it should be fine. Have you got a link to the Scaladoc for <-: ?\n. This comment should say ... fields except the excludedParameters\n. Not sure about this implementation. Consider two maps:\n``` scala\nscala> val a = Map(\"a\" -> \"10\", \"b\" -> \"01\")\na: scala.collection.immutable.Map[String,String] = Map(a -> 10, b -> 01)\nscala> val b = Map(\"a\" -> \"100\", \"b\" -> \"1\")\nb: scala.collection.immutable.Map[String,String] = Map(a -> 100, b -> 1)\nscala> val algorithm = (s: String) => DigestUtils.md5Hex(s)\nalgorithm: String => String = \nscala> val excludedParameters = List()\nscala>   def getEventFingerprint(parameterMap: Map[String, String]): String = {\n     |      val builder = new StringBuilder\n     |      parameterMap.foreach {\n     |       case (key, value) => if (! excludedParameters.contains(key)) {\n     |         builder.append(value)\n     |       }\n     |     }\n     |     algorithm(builder.toString)\n     |   }\ngetEventFingerprint: (parameterMap: Map[String,String])String\nscala> getEventFingerprint(a)\nres2: String = b8c37e33defde51cf91e1e03e51657da\nscala> getEventFingerprint(b)\nres3: String = b8c37e33defde51cf91e1e03e51657da\n```\nFeels like we want all aspects of the querystring (apart from excluded params), not just concatenated param values.\n. Can we put all these into an inner-object, something like:\nscala\n  private object Schemas {\n    val PageView = SchemaKey(\"com.snowplowanalytics.snowplow\", \"page_view\", \"jsonschema\", \"1-0-0\").success\n    ...\n. Replace use of the implementation's constants for the EXPECTED SCHEMA with string values.\n. What is unrecognized about #2? It looks okay to me \n. DVCE_CREATED_TSTAMP column should be before DVCE_SENT_TSTAMP.\n. The true_tstamp overriding the other timestamps to determine the final derived timestamp is part of the derived timestamp calculation algorithm - therefore that logic switch should be inside the EE.getDerivedTimestamp function, not exposed out here in EnrichmentManager. The added advantage of this is that this behaviour is then unit-testable in the DerivedTimestampSpec.\n. Add row showing true_tstamp overriding everything else.\n. This spec needs lots more tests to validate that the implementation creates correct fingerprints (it doesn't yet).\n. When did we decide to low-fi the errors from a well structured JSON down to a string? Remember we are primarily loading these errors into Elasticsearch which has great support for JSONs.\n. See comment above - errors should go back to being JSONs.\n. Let's bump sizes for vendor and name to 1000 each.\n. I think for this JSON Paths (and associated table) file we should revert to old order with new fields on the end. Otherwise it's quite a difficult upgrade...\n. See size comments on PG file\n. I think we have to tweak order to old version with new fields on the end...\n. Done - https://github.com/snowplow/snowplow/commit/f6c188dfbdf87b876c5c9b056ba1c55e72a840da\n. Hey Fred - I think we should use ASCII Record Separator: http://www.theasciicode.com.ar/ascii-control-characters/record-separator-ascii-code-30.html - this is what this is for...\n. Sorry! I meant unit separator: http://www.theasciicode.com.ar/ascii-control-characters/unit-separator-ascii-code-31.html A record consists of multiple units.\n. https://en.wikipedia.org/wiki/Delimiter#ASCII_delimited_text\n. Looks good! Can we add a couple more tests:\n1. A test with \"overlapping\" key and value names to prove the ASCII unit separation is working\n2. Split e2 into two tests as you are implicitly testing the field exclusion in there\n. Thanks Fred, fixed and rebased\n. Can we add a docstring explaining why we are using the structural type here?\n. \" and truncating field lengths based on Postgres' column types.\"\n. We're already on 0.7.0 - this is 0.8.0?\n. Gah ignore me, this is Postgres.\n. Add a comment here:\nscala\n// TODO: move PostgresConstraints code out into Postgres-specific shredder when ready.\n// It's okay to apply Postgres constraints to events being loaded into Redshift as the PG\n// constraints are typically more permissive, but Redshift will be protected by the\n// COPY ... TRUNCATECOLUMNS.\n. I think let's leave it one-word if that's how it comes in. Hopefully one day all this will be automated and a computer wouldn't know to add a _ there...\n. I can't think of any other manipulations to do @jbeemster \n. @fblundun - this regexp doesn't successfully capture Hadoop Shred 0.2.1\n. @chuwy how could any of these three values be null? You are testing Option-boxed values... \n. Okay cool - thanks for clarifying...\n. Nice\n. Nice - you made it very easy to visually verify that the existing behaviour is safely unchanged @ninjabear!\n. Great new tests!\n. In R74, we simply updates the hadoop-x.x.x to the latest available hadoop version\n. Yep don't revert\n. Should be [:aws][:emr]... @fblundun \n. Shouldn't ami_version be bumped to 4.3.0 too?\n. Why are we exporting BUNDLE_GEMFILE for JRuby?\n. Why are we running the Ruby and then the JRuby versions?\n. We should be able to remote the export line too...\n. Do you need the altered_enriched_subdirectory = there - could just return the right-hand-side of the assignment?\n. Contract missing snowplow_tracking_enabled\n. Contract missing run_folder\n. Contract missing snowplow_tracking_enabled\n. Ah - yep I think that's an error in the Redshift file... Added: https://github.com/snowplow/snowplow/issues/2461\n. @fblundun this call to sanitize_message is missing the second argument, to credentials\n. We don't yet include shredding for PG, so I would replace with:\nyaml\n    exclude: []\n. We only support loading bad rows into ES currently, so I would replace with:\nyaml\n    exclude: []\n. It looks like we are checking the list of shredded types against the table name, rather than the type name?\n. Indentation is a little off\n. This should default to false @chuwy \n. Can reduce the whitespace in the above with losing the alignment\n. Capitalize JSON and POJO above as both are acronyms\n. Add a TODO that this can be extracted to Iglu Scala Client with 0.4.0\n. Stray newline down here\n. Whitespace alignment wrong\n. What's the ticket number?\n. Typo \"unatuhorized\" - how does the test complete okay with that typo?\n. Can we add a couple more tests in here just to validate extracting slightly more complex objects (e.g. array or inner object)?\n. Can we add a couple of beFailing as those seem to be the more interesting scenario...\n. Have re-titled this issue, https://github.com/snowplow/snowplow/issues/2534\n. These 2 ^^ should be next to the other Scala Common Enrich tickets\n. Reorder this CHANGELOG into: Documentation; Common; Config; Scala Common Enrich; Scala Hadoop Enrich; Scala Hadoop Shred. It will flow much better.\n. Ah good, was hoping that\n. I reckon we should let it crash. Otherwise the behaviour on a rare JS bug becomes very hard to reason about: exactly what events have I recovered?\n. Can you add a short paragraph description of the purpose of this script...\n. Not sure about doit as a function name\n. This is 0.2.0-rc1, not 0.1.0...\n. Update the description to use the word recovery\n. Add into Dependencies file\n. These helpers should be documented in the wiki documentation for Hadoop Event Recovery.\n. ^^ rename given new name\n. Can we test a mutation in this script please...\n. Replace these with {{}}s\n. All of this documentation should be moved into a dedicated wiki page...\n. This readme should primarily push users to the rich wiki documentation\n. I was imaging that the expected payload would always be a (single) self-describing JSON, which would become an unstructured event...\n. I can't visualize how an array would be mapped to an event...\n. Happy for that to be in a v2...\n. Snowplow style says to call this ElasticsearchSenderHttp (note the http case)\n. 2014-2016 (i.e. the copyright noticed can be thought of as global to the app's source code, not specific to the individual file)\n. Semi-colon\n. Any reason why there are no io.searchbox.... imports in the file instead of this long-form?\n. No io.searchbox.... imports in the file?\n. No io.searchbox.... imports in the file?\n. This should go first in the list of tasks done\n. I think the above two lines can be condensed into one - just iterate through the dashdb_targets, it doesn't matter if it's empty or not\n. On disk these are indeed 1 or 0, but the Redshift table definition has them as booleans. Would it be possible to load them into booleans for DashDB?\n. Can you move the SnowplowDashDBConn project into this folder:\n4-storage/dashdb-loader\nThanks!\n. Should we be adding any indexes onto this table - does DashDB support indexes?\n. Sounds like a plan @ironsideshu \n. Missing two hyphens here @chuwy . package spark or hadoop?. Let's rename \"enriched event TSVs\" to \"Snowplow enriched events\" so we don't have to update this when we change the Snowplow enriched event format.. I know they did with Cascading - is this true of Spark as well?. The layout of the imports isn't really idiomatic to how we lay them out in the Snowplow codebase:\n\nWe group similar imports onto consecutive lines, separated by blank lines\nWe order from most generic (e.g. UUID) to most specific (e.g. import from same project)\nWe tend to use multi-line package statements to reduce the amount of import com.snowplowanalytics.snowplow.... text\n\nI'm happy for some or all of our rules about the layout of imports to change, but this should be formally agreed with @chuwy , not changed ad hoc.... Can we use this opportunity to add @tparams for all of the case class properties. Can we use this opportunity to add @tparams for all of the case class properties. Can we use this opportunity to add @tparams for all of the case class properties. Can we get a scaladoc on this method. See comment above about imports. I think the use of typesafe Config is fine. Can we use this opportunity to get rid of Tap? It was a bad idea by me to introduce it to Snowplow years ago and it's easily replaceable with an idiomatic Scala block. . Is this file required by Spark for testing? Can we add a comment line to it explaining the rationale for adding?. BeforeAfterAll sounds rather cryptic. Can we at least add a scaladoc for it?. LGTM. Can we add a comment just reminding what local[1] signifies?. \"event ID\" not \"event id\". Let's capitalize JSON/JSONs throughout this file. Cool makes sense. Oh, I see what you mean. Yes, we should use command line arguments as in the Scalding job.... I think pubsub is too generic - let's call it gcpubsub. I think pubsub is too generic - let's call it gcpubsub. Can you add a one-liner comment on the project ID.... Add note: \"these topics must have been created ahead-of-time\". Auto-creating the topic is dangerous (e.g. imagine there is a typo in the config file and the SSC silently starts writing to the wrong topic). Further discussion on this here:\nhttps://github.com/snowplow/snowplow/issues/1464\nPlease remove auto-creation.... Hmm - I don't think this function is auto-creation. I think the docstring and function name are wrong?. I'd second that.... Right - my point is that createTopic is a bit misleading as a function name. You are creating a Topic POJO, but you are not creating a Pub/Sub Topic - the latter already exists.\nChange to:\nscala\n/**\n * Instantiates a new PubSub Topic instance with the...\n...\nprivate def getTopic: Topic .... This shouldn't be in your PR - you must need to rebase off master. It should be used inside the enrichment - it's a bit gnarly, but the enrichment should use the Hadoop cache paths that are populated here... . I'm not wedded to Hadoop's DistCache... One worry I have about continuing to use it is whether it's compatible with running the job on spot instances - if in the future we want to exclusively use spot / not use HDFS, then we will have to migrate off DistCache anyway?. It's just (so far) the MaxMind IP Lookups enrichment, and it just uses DistCache for storing and retrieving a 20Mb file as its IP address database. I think we can probably safely stick to your new approach.... You'd have to do what I do here, which is come up with some standard \"global-variable-style\" file paths, and then the init code knows where to write to, and the enrichment knows where to read from, without the two components being tightly coupled.... Ah! Yes that sounds problematic. A little OT but is there a reason why we have all these individual deploy_${foo}.sh scripts, when we could just have a singular deploy.sh ${foo} approach?. As above, OT but is there a reason why we have all these individual is_${foo}_release_tag.sh scripts, when we could just have a singular is_release_tag.sh ${foo} approach?. I think yes please - it's as good a release as any to rationalise the approach. Thanks @BenFradet . Will need (one?) new ticket(s). Typo, should be recovery not recory. Agree, we should use the resolver (that's what it's for). Not used anywhere. What does ls3s stand for? I can't parse it.. Have you tested this @chuwy ? It's not working for me.. I strongly dislike this magic value of 5 - this should be the length of a constant string, \"s3://\", which should also used everywhere else that \"s3://\" is hard-coded.... Replace Location.EU with \"eu-west-1\". Is args.profile optional, or not? Passing in profile_name=None to Boto is never a good idea.. In my local install, this dictionary lookup fails with a KeyError because the location is a unicode string, but the map contains non-unicode keys.... Interesting. Nothing was working for me on Ubuntu. I'll rerun later this morning when there's a new version of this script and see what happens.... u'eu-west-1'. Thinking about this more - Location.USWest2 is just a constant value for us-west-2 etc. So I think the get_valid_region function could be safely rewritten as:\nif location = 'us-east-1' or unicode(\"us-east-1\") then Location.DEFAULT else location. That's a start! Then define val BucketPrefix = \"s3://\" and use that everywhere in this file instead of the magic string.. So ls 3s? I'm still not seeing it.. Ah missing pagination, yes I buy that. Please fix this and I'll retry.. This is the error I was getting:\nFile \"./run.py\", line 153, in run_emr\n    r = get_valid_region(jar_bucket.get_location())\n  File \"./run.py\", line 110, in get_valid_region\n    return S3_LOCATIONS[location]\nKeyError: u'eu-west-1'. To be clear, there are three issues here:\n\nIssues with using unicode as a key lookup\nIncorrect value for Location.EU\nMissing regions from the dictionary\n\nThat's why my suggestion is to replace with some variant of:\nif location = 'us-east-1' or unicode(\"us-east-1\") then Location.DEFAULT else location\nIf you have an alternate fix for these three issues, let me know!. This line should not say Hadoop any more.... \"Load\" not \"Loading\" for consistency with other job step names.. Snowplow style would be Sink.Nsq, not Sink.NSQ. Snowplow style is NsqSink not NSQSink. This copyright change doesn't make sense. Who are these comments for?. Formatting. Why move sluice to the bottom?. Should be present tense. should be update not Update. Good point - let's rename to \"Iglu Central - GCP Mirror\". Makes sense. consistency_check technically should sit before analyze. consistency_check technically should sit before analyze. That should be a dedicated commit cross-codebase (not file by file). I added a couple of tickets just to be super-explicit (especially because not every component codebase will be modified in the first release of the year).. What is happening with this? I thought R100 was code complete.... I agree with @knservis - it's better to apply global formatting and then use format off / on flags around pieces which are beyond the formatter's capability. This is a non-argument in other language communities (see e.g. Golang).\nI don't agree that // format off and // format on is visual garbage - if anything, it's helpful - it says \"here be dragons\": the syntactical complexity of this piece of code is beyond a computer's ability to visualize it well for a human. That's interesting information.\nOn automated formatting - think of it as cruise control on a car - just because you have to sometimes switch it off for a complicated piece of driving, nobody seriously thinks it's better not to have it at all.. Do we need a content type?. What are our options here @knservis ?. Thanks for the detailed options @knservis ! Let's go with do nothing, but:\n\nCreate a ticket to flag that we are doing nothing, so we don't forget about this. Heavily link this new ticket back to this thread\nCreate a placeholder for exploring the fragmentation protocol idea, which is really neat. Again, heavily link it back to here\n\nThanks! @BenFradet shout if you disagree with the plan.. ",
    "shermozle": "\"rule out GET\" should be \"rule out POST\"\nBeen thinking about this somewhat. Theoretically there are no limitations on other browser URL lengths, but I imagine you'd want to keep it sane as I'm sure proxies and the like will impose their own limitations.\nSo logic would be something like this in the sp.js:\nif ( (navigator.appName === 'Microsoft Internet Explorer' && document.location.href.length > 2048) || (document.location.href.length > 4096) ) {\n    var beaconId = Math.floor(Math.random()*1000000000000000, 10);\n    \n}\nThen of course ETL needs to stitch it back when it sees a \"beaconId\".\n. You should be able to change the Content-Type header in the source S3\nbucket for the ice.png file to image/png. Under Properties > Metadata from\nmemory.\nOn Thu, Oct 4, 2012 at 10:52 PM, Alexander Dean notifications@github.comwrote:\n\nChrome giving warnings:\nResource interpreted as Image but transferred with MIME type\napplication/octet-stream:\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/54.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. Exit link click\nFile download\nForm submission\nOn Tue, Oct 30, 2012 at 12:26 AM, Yali notifications@github.com wrote:\n\nOnes to launch with:\n1. page_view\n2. ecomm_transaction\n3. social\n4. custom\n\u2014\n   Reply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/63.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. To clarify: social logged-in status detection.\nShould also detect social referrers, but that'd be in ETL layer.\nOn Mon, Nov 26, 2012 at 2:32 AM, Alexander Dean notifications@github.comwrote:\n\nhttps://github.com/stereobooster/social_detector\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/95.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. We've been toying, in a very preliminary way, with automated testing using\nSaucelabs so you can test a range of target browsers in an automated way\n(including iOS and Android).\nhttp://webanalyticsinpractice.com/automated-web-analytics-testing-with-selenium\nMy initial approach is very naive and tries to test that an analytics\nbeacon actually exists. In practice the beacon won't exist if there's a JS\nerror, so it's not a bad sanity test.\nA better approach would be to proxy the analytics request (or perhaps point\nit at a different host for testing) and parse the beacon to ensure it's got\nthe right stuff.\nI'd start with \"didn't break anything\" tests, then get more complex.\nOn Tue, Dec 11, 2012 at 4:26 AM, Alexander Dean notifications@github.comwrote:\n\nSee @robocoder https://github.com/robocoder's tests for piwik.js for\ninspiration.\nhttps://github.com/piwik/piwik/tree/master/tests/javascript\nAlso think about integration tests - potentially using SnowCannon so tests\nare in a JavaScript-only round-trip...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/106.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. On Mon, Dec 17, 2012 at 2:13 AM, Yali notifications@github.com wrote:\n\nBefore discussing how we would implement it, it'd be good if we could\nflesh out the biz requirements. I think it'd be great if:\nThe Omniture term for this is \"Merchandizing Categories\". The use case is\nexplained with a supermarket analogy: you're in a supermarket and you can\npick up batteries in the electrical goods aisle, alongside the toys in the\ntoy aisle, and arranged next to the checkouts with the chewing gum,\nchocolate bars and trashy magazines. Where did the visitor pick up the\nbatteries?  (You'll notice that the cold soft drinks from the fridge come\nin a size you can't buy warm off the shelf, which is how supermarkets work\nthis out when they really need to, and price differentiate the cold drinks.)\n\nThe syntax Omniture use is perverse, so don't dwell on that. Essentially an\n\"eVar\" (persistent variable) is associated with each product as it's added\nto the cart, then\nA description of Merchanizing in Omniture:\nhttp://adam.webanalyticsdemystified.com/2011/09/27/merchandising-evars-omniture/\nNow some non-supermarket use cases:\n- What internal search keywords were used prior to adding the product to\n  cart (or some other \"destination\" event)?\n- Which of a range of possible images/descriptions of the product were\n  visible when the user clicked \"add to cart\"?\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. An alternative approach would be to collect this data in a different way.\nThe problem you have is you want to record ALL behaviour, not just at\nspecific milestones as is the traditional way of shoehorning this into web\nanalytics data structures. You really want to be able to record and display\nviewing statistics down to the sub-second level. Then you can report back\ndropoff, scrubs etc across a video or live stream. The key here is you want\nto be able to accurately report views on advertising, and secondarily\nprogramming.\nSo you send a ping every second reporting the current status, then the\ncollector will have a timeout of some sort where if it hasn't heard from a\nparticular client, it'll write out what it has. The key here though is that\nthe collector needs to store state. That brings scaling problems to be\nsolved...\nOn 9 May 2014 05:00, Alexander Dean notifications@github.com wrote:\n\nLet's finally do this one now we have the JSON Schema stuff.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/155#issuecomment-42591157\n.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. Why can't it be cross-domain?\n. Nope that was my misunderstanding there. Now I get what you mean.\nObviously being able to calculate \"Visits\" after the fact is preferred.\nOn Thu, Apr 4, 2013 at 9:03 AM, Alexander Dean notifications@github.comwrote:\n\nHey Simon - the visit counter is stored in a first-party domain-tied\ncookie. So each domain gets a different counter.\nAre you asking why we don't add a session counter as a third-party cookie\ninto e.g. the Clojure Collector? We have thought about this - and we\nrenamed the current counter domain_sessionidx to make \"space\" for a\npotential network_sessionidx in the future.\nOr are you asking something else?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/169#issuecomment-15868280\n.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. Important thing to nail is handling timestamps. Do you trust the timestamp given by the device? Best not to.\nWe found with a set top box I was working on that some devices' RTC batteries died while switched off (some consumers seem to switch them off at the wall outlet) and the clock would start up at Unix Epoch. Worse, we were calculating \"duration watched\" on live video by startTime - currentTime, and the device would at some point sync up its clock through NTP, so we'd end up with people having watched from 1970-01-01 01:01:01 - currentTime. Fun times!\nPotential solution: record the time the device thinks it is at the start of the POST and the time the server has to get an error margin, then use that for your calculations. This assumes the clock is consistently wrong though, perhaps a problematic assumption, but not sure how much more you can do.\n. Doh, dumb me. Sorry ignore ;)\n. Also URL and Referrer by the look of it.\n. Wow great stuff guys.\nI think we should nail #20 along with this. The aim here is to hand developers an instruction \"pass EVERYTHING in\" without worrying about things like URL length, storage size etc. That's gonna really quickly hit the 2000 bytes.\nOf course, #20 is gonna be a bit of a pain in the butt to implement, but I think it's worth it.\nFailing that, you have to ensure the tracker puts these things at the very end of the beacon.\n. Yeah I think Base64 is fine. Can easily think up a grep wrapper to search\nplaintext and Base64 when you wanna grep. Equally debug bookmarklet for in\nbrowser.\nOn 28 Mar 2013 22:47, \"Gabor Ratky\" notifications@github.com wrote:\n\nGuys, do we all agree on the Base64 encoding for ue_pr? We'd like to go\nahead and implement this so that we can start tracking away for testing\npurposes and start work related to this in the Scalding ETL.\n20 https://github.com/snowplow/snowplow/issues/20 is unrelated and\nquite frankly, orthogonal to this feature so I would not mix them. It's\nalso a fairly large amount of work and I think we can live with the 2000\ncharacter limitation for now.\nWith regards to type suffixes, as long as the trackers handle the\n\"encoding\", it does make the ETL's life easier and at this point, I think\nit's really just Date that we need to handle internally as geo will need\nto be explicit from the user anyways (no native type to support). I also\nlike the idea of using Unix Epoch, it's what the Mixpanel HTTP REST API\nuses and their reporting supports.\nWhat would a { \"time\": new Date() } serialize into by default? Unix Epoch\nwith second precision ({\"time:dt\": 1364471251}) or ISO8601 with\nmillisecond precision (`{\"time:ts\": \"2013-03-28T11:46:46.343Z\"})?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/pull/198#issuecomment-15584498\n.\n. It certainly does feel like it's going to be a whack-a-mole process for\nbrowser makers to stop tracking definitively.\n\nBut be careful. Companies doing this kind of thing have been smacked down\nin the past:\nhttp://www.wired.com/threatlevel/2012/10/kissmetrics-tracking/\nhttp://online.wsj.com/article/SB10001424052970204880404577225380456599176.html\nhttps://www.eff.org/deeplinks/2012/02/time-make-amends-google-circumvents-privacy-settings-safari-users\nAs an industry we desperately need to find a middle ground. The browser\nmakers unilaterally doing things is just encouraging workarounds, while Do\nNot Track was a bit toothless even before IE broke it.\nI actually have a bookmark folder for these kinds of things, labelled \"Evil\nTracking\".\nOn 19 August 2013 04:47, Gabor Ratky notifications@github.com wrote:\n\nHow about evercookie?\nGabor\nOn Sun, Aug 18, 2013 at 7:37 PM, Alexander Dean notifications@github.com\nwrote:\n\nSee also: http://lucb1e.com/rp/cookielesscookies/\nReply to this email directly or view it on GitHub:\nhttps://github.com/snowplow/snowplow/issues/210#issuecomment-22835116\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/210#issuecomment-22836479\n.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. I have now!\nOn 29 Aug 2013 03:16, \"Alexander Dean\" notifications@github.com wrote:\n\nMany thanks Simon. Have you signed our CLA yet -\nhttps://github.com/snowplow/snowplow/wiki/CLA\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/pull/331#issuecomment-23431650\n.\n. Here's an example row:\nE3EMFQTE1RZ8WL.2013-10-21-21.4i68OHIv.gz:2013-10-21     21:50:01\n SYD1    362     60.229.23.241   GET     dsdfh21guhhqy.cloudfront.net    /i\n     304     -\nMozilla/5.0%2520(Linux;%2520U;%2520Android%25204.3;%2520en-gb;%2520Nexus%25207%2520Build/JWR66Y)%2520AppleWebKit/534.30%2520(KHTML,%2520like%2520Gecko)%2520Version/4.0%2520Safari/534.30\n\nnuid=88b2b836-d09f-45a2-a510-3cfeb05d435c&e=se&se_ca=identify&se_ac=open%2520email&uid=\nsimon@simonrumble.com    -       Hit\nVuu7X4kYO3lca3spx52Nz43SfjvG1HyY5F4DXcXGjRdcEkK60ydAtQ==\ndsdfh21guhhqy.cloudfront.net    https   585\nThat was generated by this beacon:\nhttp://cs.datarepublic.com.au/i?e=se&se_ca=identify&se_ac=open%20email&uid=simon@simonrumble.com\nwhich redirects to:\nhttps://dsdfh21guhhqy.cloudfront.net/i?nuid=2f53d8b1-e627-4846-a801-138b089ccfe5&e=se&se_ca=identify&se_ac=open%20email&uid=simon@simonrumble.com\nOn 24 October 2013 05:14, Alexander Dean notifications@github.com wrote:\n\nMakes sense - please @shermozle https://github.com/shermozle or\n@jrobgood https://github.com/jrobgood add a row to this thread...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/399#issuecomment-26929440\n.\n\n\nSimon Rumble simon@simonrumble.com\nwww.simonrumble.com\n. ## General description of SiteCatalyst campaigns\nSiteCatalyst campaign codes are quite different from Google Analytics. More complex, more configurable and more of them.\nAt heart, a SiteCatalyst campaign code is just an eVar. There's one special eVar, s.campaign, but it's just an eVar that happens to default to show up in the Campaigns menu. You can have multiple campaign codes, with different configurations (allocation, expiry etc) for different purposes. I've commonly seen separate ones used for internal and external campaigns, so you can track on-site banners separately to paid external activity.\nThe name of the campaign parameter on the querystring is configured in the s_code, so it can be anything. cmpid and cid are common, but I've also seen pid and even complex approaches using server-side code for non-JS feature phones. It's completely possible to write a single campaign code to multiple eVars, so you can have different allocations to get a short-term view alongside a long-term view. It's also quite common to do \"Campaign Stacking\" where you keep adding campaign codes with a delimiter and record that into an eVar, a primitive \"path to conversion\" approach.\nThe value of the querystring name=value pair is generally just stuffed straight into the eVar, but you can conceivably do anything you like with it before recording. It's not uncommon to have a cookie to make it only be recorded on the first instance of a particular value.\nOn the SiteCat side eVars can be classified (their term is SAINT) so that a particular value can be grouped along different dimensions. This allows integrations with things like campaign management systems and reporting at different levels of a hierarchy. For example, you might have a campaign code fb:1234567. Your campaign system knows this campaign and uploads a CSV to SiteCatalyst classifying it as Channel = Facebook, Campaign name = December share the fun, Creative version = Small banner. On the reporting side you're now able to report on performance across all campaigns where Channel = Facebook, or drill down to just this campaign, or look at a single creative execution's performance.\nHow you could make this work in Snowplow\nYou could conceivably do some JavaScript-side or ETL-level munging of campaign codes (as described in the initial issue) into the GA-style UTM parameters. This has some downsides: there's a limited number of UTM parameters, it all hinges on that data being embedded in the URL (in plaintext generall) and you can't update them after it's been recorded.\nI would suggest you just record the SiteCat campaign code into a single one of Snowplow's UTM parameters. Then have a separate DB table holding your classifications. Then at reporting time, join with that table to get the same behaviour as you get in SiteCat.\n. Sounds good, and very flexible.\nOn 21 Dec 2013 22:26, \"Alexander Dean\" notifications@github.com wrote:\n\nHey Simon - thanks for this, super-helpful explanation and\nrecommendations. Factoring in what you've said and Dimitris, how about a\nnew enrichment in the config.yml like this - standard Google-style settings:\n:enrichments:\n  :campaigns:\n    :enabled: true\n    :mapping: static\n    :fields:\n      :mkt_medium: utm_medium\n      :mkt_source: utm_source\n      :mkt_term: utm_term\n      :mkt_content: utm_content\n      :mkt_campaign: utm_campaign\nTo use it in the way you suggest:\n:enrichments:\n  :campaigns:\n    :enabled: true\n    :mapping: static\n    :fields:\n      :mkt_medium:\n      :mkt_source:\n      :mkt_term:\n      :mkt_content:\n      :mkt_campaign: cid\nAnd eventually, we could add support for Dimitris' version, like so:\n:enrichments:\n  :campaigns:\n    :enabled: true\n    :mapping: script # Note 'script', not 'static'\n    :fields:\n      :mkt_medium: cmpid.split('|')[0]\n      :mkt_source: cmpid.split('|')[1]\n      :mkt_term: cmpid.split('|')[2]\n      :mkt_content: cmpid.split('|')[3]\n      :mkt_campaign: cmpid.split('|')[4]\n@yalisassoon https://github.com/yalisassoon , @shermozlehttps://github.com/shermozle,\n@dtheodor https://github.com/dtheodor - what do you guys all think?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/435#issuecomment-31061511\n.\n. That's not what I get :(\n\n```\n[simon:~/snowplow] master \u00b1 vagrant ssh\nWelcome to Ubuntu 14.04.4 LTS (GNU/Linux 3.13.0-79-generic x86_64)\n\nDocumentation:  https://help.ubuntu.com/\n\nSystem information as of Tue Mar  8 21:57:57 UTC 2016\nSystem load:  0.91              Processes:           82\n  Usage of /:   3.5% of 39.34GB   Users logged in:     0\n  Memory usage: 3%                IP address for eth0: 10.0.2.15\n  Swap usage:   0%\nGraph this data and manage this system at:\n    https://landscape.canonical.com/\nGet cloud support with Ubuntu Advantage Cloud Guest:\n    http://www.ubuntu.com/business/services/cloud\n0 packages can be updated.\n0 updates are security updates.\nSetting up Ansible to run out of checkout...\nPATH=/vagrant/vagrant/ansible/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\nPYTHONPATH=/vagrant/vagrant/ansible/lib:\nANSIBLE_LIBRARY=/vagrant/vagrant/ansible/library:/usr/share/ansible/\nMANPATH=/vagrant/vagrant/ansible/docs/man:\nRemember, you may wish to specify your host file with -i\nDone!\nvagrant@snowplow:~$ cd /vagrant/3-enrich/scala-common-enrich\nvagrant@snowplow:/vagrant/3-enrich/scala-common-enrich$ sbt test\nNo command 'sbt' found, did you mean:\n Command 'skt' from package 'latex-sanskrit' (universe)\n Command 'sb' from package 'lrzsz' (universe)\n Command 'sb2' from package 'scratchbox2' (universe)\n Command 'sbmt' from package 'atfs' (universe)\n Command 'lbt' from package 'lbt' (universe)\n Command 'sat' from package 'sat-xmpp-core' (universe)\n Command 'mbt' from package 'mbt' (universe)\nsbt: command not found\n```\n. ",
    "yalisassoon": "I was imagining the following alternative, which I was hoping would work with Cascading / Scalding (so not strictly speaking stream-based, rather batch based), but is in essence a streaming based approach:\n1. Maintaining a list of the last X hours worth of processed lines of data in a lookup table. (The value of X would depend on how regularly the ETL batch process works, but I think we need to move from a daily mode to an hourly mode at the very least.)\n2. Cross referencing every new line of data with the lines stored in the lookup table and apply some de-duping logic\n3. At an X hours delay, the processed data would be written from the lookup table to S3\nWe will need to have a number of lookup tables referenced as part of a Scalding ETL, especially as we build out an OLAP friendly version of the events table. I imagined it would make sense to use Amazon SimpleDB or RDS for the tables for simplicity, but probably worth discussing on a dedicated thread...\n. See below:\n\n. Yes!\n. Documentatin has been updated!\n. Updated!\n. Hi @mtibben  - apologies for only catching up with this thread now - I've been without access to computer for the last couple of days... Just to re-iterate @alexanderdean - this contribution is brilliant!\nOn the boundary issue, there are two things to bear in mind:\n1. As @alexanderdean pointed out, a file generated in the early hours of e.g. September 26th may contain records from events that occurred towards the end of September 25th\n2. When we load data into a partition in a table in Hive (using INSERT OVERWRITE), that complete partition is overwritten. \nTo work around the boundary issue, we need to process files for the specific dates we're looking at, and the following day. So if we want to update records for September 26th (at 4am on September 27th), we'd move all the files for September 26th and September 27th to the processing bucket, run the job, and keep the WHERE condition as before, so that we only upload files for the 26th to Hive. (The records we pass over for the 27th will then be processed when the script is re-run on September 28th. There's no point processing the few of them that exist on the morning of the 27th as they'll all be overwritten.)\nDoes that make sense? To clarify:\nImplications for processing\n1. When running the daily-etl.q, we should be processing files for yesterday and today\n2. When running the datespan.q, we should be processing files over the specified time period and the following day\nImplications for archiving\n1. When running the daily-etl.q, even through we process files for yesterday and today, we only archive files from yesterday. That is because we need to reprocess today's files tomorrow, when we update the data for today.\n2. When running the datespan.q we only archive files for the specified time period, even though we process files over the specified time period and the following day\nAny questions / clarifications - give me a shout!\n. I think this is the library we should use: https://github.com/tobie/ua-parser\nMaster YAML is here: https://github.com/tobie/ua-parser/blob/master/regexes.yaml\n. Looks like this is done via a bootstrap option, when provisioning the\ninstances:\nhttp://vangjee.wordpress.com/2012/03/24/an-approach-to-controlling-logging-on-amazon-web-services-aws-elastic-mapreduce-emr/\nOn Tue, Oct 30, 2012 at 3:42 PM, Alexander Dean notifications@github.comwrote:\n\nCurrently seeing the message (within the task logs):\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.mapred.Task).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/65.\n. @ryanrozich the best place for this sort of question is our discourse. We already have the user fingerprinting explored on it here - if any of that isn't clear if you reply on the thread we can continue the discussion there!\n\nThanks\nYali\n. Crikey the Omniture approach is just baffling...\n@alexanderdean and I have batted around a few different potential implementations - what do you guys reckon about this spec: https://gist.github.com/4327909.\nPing back comments in the thread, and feel free to forks as appropriate...\n. This is a good question...\nMy view is that none of the fields are required. GA has to make some of the fields required, because otherwise the out-of-the-box event charts and graphs don't work. Because we don't have out-of-the-box charts and graphs to support, we don't need to insist on the corresponding fields in the database being populated. This also gives users maximum flexibility to use the event tracking in a way that makes sense to them.\nHaving said that, the flip side is you may get people doing strange implementations and then complaining that e.g. the example queries in the analytics cookbook don't work. (For example - I can't imagine that someone would use the function but keep the ev_action field NULL. All my example queries use that field.) Given the wide range of potential uses for event tracking though, I'm not confident that there isn't a sensible example where you wouldn't set the field, I suspect I'm just not being imaginative enough in coming up with one...\n. I really like you suggested approach and think it's perfect for SnowPlow. Just to check we're all on the same page, we'd capture all the following data with each pageping event:\n- Document height\n- Viewport height\n- Max scroll top\nIt would then be trivial for someone at analysis phase to calculate the % of each page read at analysis time for each pageping.\nIn addition, it would also be straightforward at analysis time to plot the scroll height for a particular user on a particular page over time. (By comparing max scroll top between different page pings.) This might give a nice picture of how users digest different types of content e.g. are they fast forwarding to a particularly interesting part of the content, or is there a bit of the content people reach and then give up on the rest?)\nWhilst we're batting ideas around - what about doing the same measurements but for width as well as height? I know we typically think of people consuming content by working down web pages, but there may be some sites where you have to scroll right -left. (The only service I can think of that's like this is Lifecake - where you view your baby photos presented in chronological order left right, starting on the furthest right (now) and working back in time. But there's no reason other web services might not elect to do something similar in future.) In that case we'd also capture\n- Document width\n- Viewport width\n- Max scroll left (if you had to scroll left)\n- Max scroll right\nSomething to think about - we could do this on a v2 if there was demand for it.\nGiven that - maybe we should also be capturing\n- Max scroll up\nFor the odd occasion when someone opens a webpage some way down. (E.g. by clicking on a link with a page anchor.) @alexanderdean what do you think?\n. For the initial implementation our thinking is that nothing will be overwritten...\nWe'll create a new field utm_share and a new corresponding column in the events table mkt_share. We can then push it into the wild and test:\n- What proportion of visits where mkt_share=1 have a first event where page_referrer is set? (E.g. to a bookmarking site, webmail URL, t.co..., blog post urls etc.)\n- What URLs do we see where `mkt_share=1? (I'd expect URLs for some of the services identified by @alexanderdean here: https://github.com/snowplow/referer-parser/issues/9)\nOnce we've done that, we may decide to use utm_share as an input into setting mkt_source, mkt_medium fields. For the moment, though, I think we're safer treating them separately, and leaving it up to individual users which visits they classify as dark social vs referred based on the contents of those fields.\nOn rel=\"noreferrer\" I'm not quite sure what options we have available to us... If this becomes supported by all the major browsers, then whenever someone uses the link to a SnowPlow site tagged site with a link tagged rel=\"noreferrer\", the page_referrer field on the SnowPlow events table will be blank. If the link was generated with a copy+paste, it would still contain the utm_share=1 parameter on the querystring of the landing page (i.e. the page_url field in the SnowPlow events table). \nMy guess is that anyone creating a link to a website who doesn't want referrer information passed enough that they add a rel=\"noreferrer\" will be careful enough to spot the \"utm_share\" parameter on the query string and strip it off. That doesn't seem to be a huge problem to me though - am I missing something?\n. Fixed!\n. @gsmith85 - Alex is abroad at the moment. He'll be back next week - we'll catch up on the development roadmap, and post back here to keep you updated on the release ETA.\n. I think this is a simple fix - will action ASAP:\nhttp://stackoverflow.com/questions/6842245/converting-date-time-to-24-hour-format\nOn Sat, Jan 26, 2013 at 9:26 AM, Alexander Dean notifications@github.comwrote:\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/146.\n. The fix is ready to be merged - the branch is\nclj-collector/fix-24-hour-clock\n\nOn Sat, Jan 26, 2013 at 9:54 AM, Alexander Dean notifications@github.comwrote:\n\nCool, this becomes the new 0.7.2. (Fix in separate branch then ping this\nthread and I'll merge & release.)\nOn Jan 26, 2013 9:52 AM, \"Yali\" notifications@github.com wrote:\n\nI think this is a simple fix - will action ASAP:\nhttp://stackoverflow.com/questions/6842245/converting-date-time-to-24-hour-format\nOn Sat, Jan 26, 2013 at 9:26 AM, Alexander Dean \nnotifications@github.comwrote:\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/146>.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/146#issuecomment-12733314>.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/146#issuecomment-12733329.\n. Thanks @GuiSim - have updated the doc now\n. I thought it was weird I couldn't find it!\n. That makes a lot of sense @michaelwexler - so it seems like we should enable a handful of \"perm profiles\" that are set at a user level, maybe another handful that are explicitly session level, and then set no scope for any other dimensions / metrics. It would then be up to the analyst to set a scope for them, as part of the analysis stage. (So we're deferring the problem rather than solving it - but at least we're not forcing any false assumptions...)\n. I think we should load the raw ping events into Redshift. It's then super\neasy to calculate things like 'total time spent watching' by counting page\nping events for e.g. a particular video across a particular cohort. You can\ncalculate at what minute a user drops off using a window function, the same\nway you can do sessionization in Redshift.\n\nBeing able to do that in Kinesis is nice - ultimately, we need to express\nthese types of things in a layer of abstraction above either Kinesis or\nRedshift, and then have an automated way of translating that to both\nenvironments (and Hadoop). But that's a long way down the line :-).\nOn Fri, May 9, 2014 at 8:25 AM, Alexander Dean notifications@github.comwrote:\n\nI like the idea of a play ping running every X seconds. Stateful\ncollectors are not a good idea - there lies the path to server affinity\nwhich is a world of a pain. I think there are three options:\n1. Send all the 1 second play pings into Redshift and do the\n   calculations there\n2. Create a Kinesis app which groups up the play pings, detects a\n   lapse and generates its own \"viewer stops watching stream\" event, which\n   then gets sent on for enrichment and storage\n3. Process the play pings into viewing summary stats in Hadoop, then\n   load summary stats into Redshift (a shame because then Redshift isn't\n   seeing the raw video event flow)\nThoughts @yalisassoon https://github.com/yalisassoon @shermozlehttps://github.com/shermozle\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/155#issuecomment-42640048\n.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoonhttps://twitter.com/yalisassoon\n. Doesn't ring a bell...\nI know a lot of the social networks (including Facebook) have given up on \"sessions\", and are much more interested in the number of days per week or per month that you log in. Makes more sense for a service that people open in a tab at the beginning of the day, and occasionally dip into. (Rather than giving it your full attention, to a particular end, completing that task, and then doing something else.)\nMore generally, I think the idea of a session makes less and less sense in a multi-screen, multi-tab world... But even if we still think there's value in it, you really want to keep the definition client-specific, so that it makes sense given the patterns of engagement for that company's customers...\nApologies - a bit of a ramble - I think it just means we need to make sure any server-side sessionization is super configurable...\n. My bad - thanks for the spot @shermozle - fixed now\n. Hi @frutik - I've updated the documentation again - hope it's clearer now. (Do let me know if you think we can improve it further.)\nMany thanks!\nYali\n. Thanks for the spot Simon! Have updated the link...\n. On dates - agree we should insist on wrapping dates as suggested by @alexanderdean  \nOn updating the wiki - @rgabo happy for you to clone the wiki, update and submit a pull request. If you'd prefer though, create an with the requested updates and assign it to me - I'll process them ASAP, and that way updates shouldn't slip between the cracks...\nGreat to see this progressing so quickly :-)\n. @rgabo many thanks for updating the wiki page! I've copied the updates through to the main wiki page. (Not sure if there was a cleverer way to use Git to merge the changes...)\n. This is what we're doing in Avro\n. I think we can close this ticket and create new ones for any specific third parties we want to integrate?. IE Version 8.0.7601.17514\nError message:\n'pageXOffset' is undefined\nsp.js\nCode: 0\nURI: https://d2zn5qkst6mwpf.cloudfront.net/sp.js\n. http://www.modern.ie/en-us/virtualization-tools\nhttp://www.modern.ie/en-us/virtualization-tools#downloads\n. Issue appears on at least two different websites with enableActivityTracking (so not a site specific issue)\n. Hive on EMR supports gzipped files!\n. Hi @mfu0 - a quick question (apologies for jumping in on this issue - but pathing is a really interesting usecase for Snowplow that I haven't had a chance to dig my teeth into yet)...\nI have a question - what are you using to do the pathing analysis? Are you doing it on the raw SQL, are you doing it in a scripting language (Python / Ruby / R)  on top of Snowplow data in Redshift, or are you doing it in EMR using Cascalog / Scalding / Pig or something else altogether? If there's a particular approach that you'd recommend, or any ideas to help me get started biting this off, they'd be much appreciated.\nMany thanks,\nYali\n. Thanks @mfu0 - really helpful input. We ultimately want to develop an approach that works for many more levels - a number of different tools we could potentially use - but we'll share the results via the blog as we explore it...\n. Done! \nBranch is called: update/hive-table-def\nNew table definition: https://github.com/snowplow/snowplow/blob/update/hive-table-def/4-storage/hive-storage/hive-table-def.q\n. Will do!\n. Works a treat!\n. @rgabo - really sorry - @alexanderdean and I are snowed under right now with a load of Professional Services contracts. The next 3 weeks are a real crunch - @alexanderdean is not getting a chance to come up for air - and in these situations we have to prioritise the paying clients: those clients keep the Snowplow project alive and kicking, for themselves and also for the rest of the community.\nI'm sure @alexanderdean will look at it as soon as he gets an opportunity - but realistically - that might not be for 2-3 weeks...\n. Hi @energyfirefox - it looks like you're trying to run the old (deprecated) Hive-based ETL.\nIt is possible to reprocess your old raw collector logs into the new enrichment process and load the data into PostgreSQL. Please make sure that your config files for EmrEtlRunner and StorageLoader are as described today's blog post: note that the format has changed somewhat since the last versions. \nIf you can't spot the mistake in the configs - paste them into the issue (remove your AWS creds) and I'll look through and see where the mistake is.\n. Try updating the line: assets: s3://snowplow-emr-assets to assets: s3://snowplow-hosted-assets in your EmrEtlRunner config file and see if that works?\n. Weird - we use Adwords ValueTrack parameter {keyword} to dynamically\npopulate utm_term parameter in the landing page query string e.g:\nwww.psychicbazaar.com/2-tarot-cards/genre/celtic/type/all?n=24&utm_source=Google{ifsearch:Search}{ifcontent:Content}&utm_medium=cpc&utm_campaign=uk-tarot--celtic-tarot&utm_term={keyword}&utm_content={creative}\nOn Sun, Sep 29, 2013 at 12:41 PM, Alexander Dean\nnotifications@github.comwrote:\n\n@yalisassoon https://github.com/yalisassoon the problem on this\nspecific row is that utm_term=bohemian gothic tarot - note the spaces.\nFrom doing a lot of testing, it looks like this breaks the URI parsing in a\nway which is unrelated to the Amazon CloudFront format changes or our fix.\nFrom yesterday's PBZ bad rows, here's another one:\nProvided URI string [http://www.psychicbazaar.com/2-tarot-cards/genre/celtic/type/all?n=24&utm_source=GoogleSearch&utm_medium=cpc&utm_campaign=uk-tarot--celtic-tarot&utm_term=pagan tarot cards&utm_content=29272174408&gclid=COGGut_J7bkCFYyWtAod6QoA3g] violates RFC 2396\nDoes PBZ have AdWords campaigns with spaces in the utm_term? From using\nthe GA URL Builderhttps://support.google.com/analytics/answer/1033867?hl=en-GB,\nit looks like words in utm_term should be \"+\" separated.\nThis does not solve the wider problem, which is that a lot of services\ngenerate URLs which use spaces rather than %20 or +. Even if a Snowplow\nuser can fix the problems on her own site and utm campaigns, she does not\nhave control over badly-formatted referers. Here is another example:\nProvided URI string [http://www.amazon.co.uk/s/ref=nb_sb_noss_2?url=search-alias=aps&field-keywords=tarrot cards] violates RFC 2396\nTherefore I am going to update this ticket, to defensively fix raw spaces\nin incoming URLs.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/346#issuecomment-25318781\n.\n. Would be nice to get this one scheduled if possible :-)\n. I've picked out the fields in the JSONs generated when I queried the job status (./elastic-mapreduce --describe --jobflow j-18Z91SJL8WNYH) that indicate something went wrong. On the basis of the below, it appears that:\n1. The overall JobFlow  fails with {\"ExecutionStatusDetail\": {\"State\": \"FAILED\", \"LastStateChangeReason\": \"On the master instance..., bootstrap action 1 timed out\"}}\n2. Each instance group has \"ReadyDateTime\": null\n3. Each step in the job has \"ExecutionStatusDetail\": { \"State\": \"CANCELLED\", \"StartDateTime\": null }\n\nI reckon 1 and 3 look like the safest indicators to use - I can email you the full JSONs for reference...\njs\n{\n    \"JobFlows\": [\n        {\n            \"JobFlowId\": \"j-18Z91SJL8WNYH\",\n            \"ExecutionStatusDetail\": {\n                \"State\": \"FAILED\",\n                \"LastStateChangeReason\": \"On the master instance (i-4e1a0802), bootstrap action 1 timed out executing\",\n                \"ReadyDateTime\": null,\n                \"StartDateTime\": null\n            },\n            \"Instances\": {\n                \"InstanceGroups\": [\n                    {\n                        \"InstanceRole\": \"MASTER\",\n                        \"State\": \"ENDED\",\n                        \"LastStateChangeReason\": \"Job flow terminated\",\n                        \"InstanceRunningCount\": 0,\n                        \"ReadyDateTime\": null,\n                        \"Name\": null,\n                    },\n                    {\n                        \"InstanceRole\": \"CORE\",\n                        \"State\": \"ENDED\",\n                        \"LastStateChangeReason\": \"Job flow terminated\",\n                        \"ReadyDateTime\": null,\n                        \"Name\": null,\n                        \"StartDateTime\": null,\n                    }\n                ],\n                \"NormalizedInstanceHours\": 0,\n            },\n            \"Steps\": [\n                {\n                    \"StepConfig\": {\n                        \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n                        \"Name\": \"Elasticity Custom Jar Step\",\n                    },\n                    \"ExecutionStatusDetail\": {\n                        \"State\": \"CANCELLED\",\n                        \"LastStateChangeReason\": \"Job terminated\",\n                        \"EndDateTime\": null,\n                        \"CreationDateTime\": 1379907057,\n                        \"StartDateTime\": null\n                    }\n                },\n                {\n                    \"StepConfig\": {\n                        \"ActionOnFailure\": \"TERMINATE_JOB_FLOW\",\n                        \"Name\": \"Elasticity Custom Jar Step\",\n                    },\n                    \"ExecutionStatusDetail\": {\n                        \"State\": \"CANCELLED\",\n                        \"LastStateChangeReason\": \"Job terminated\",\n                        \"EndDateTime\": null,\n                        \"CreationDateTime\": 1379907057,\n                        \"StartDateTime\": null\n                    }\n                }\n            ]\n        }\n    ]\n}\n. Let's try it! Maybe one day in the future we'll discover that there's\nsomething else that can go wrong that also causes sets all statuses to\ncancelled, but for the moment it's an unknown unknown...\nOn Thu, Oct 3, 2013 at 11:59 AM, Alexander Dean notifications@github.comwrote:\n\nThanks @yalisassoon https://github.com/yalisassoon that's v helpful.\nI'm thinking we can get away with just checking if all steps have status\nCANCELLED. Do you think that's adequate or should we also regexp\n\"LastStateChangeReason\"?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/354#issuecomment-25611593\n.\n. It'll be a SQL view, that you'll be able to access directly in a pivoting / OLAP tool (E.g. Powerpivot / Tableau etc.) You'll be able to drag different cohort definitions (categorise users based on when they first touched your website / signed up / made a first transaction / which channel they were acquried on) against different metrics over time (retention, lifetime value, engagement levels) so you can quickly generate lots of different cohort analyses.\n. Delivered as part of the 0.8.10 release (should have closed this much earlier...)\n. Great stuff - is certainly sane!\n\nOn Wed, Oct 16, 2013 at 9:33 PM, Alexander Dean notifications@github.comwrote:\n\nScheduled for 0.8.14\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/381#issuecomment-26454917\n.\n. It looks like the status of different availability zones changes over time, with some being unavailable for days on end.\n\nIt also seems unlikely that someone would have a preference for one zone over another.\nFor that reason, it might be nicest if we don't ask people to specify a preference in their config file. Instead, EmrEtlRunner checks which is available at run time and then selects one at of the available ones at random.\nNote that it is straigthforward to perform this check using the AWS CLI i.e. \n$ aws ec2 describe-availability-zones --region us-east-1\n{\n    \"AvailabilityZones\": [\n        {\n            \"State\": \"available\", \n            \"RegionName\": \"us-east-1\", \n            \"Messages\": [], \n            \"ZoneName\": \"us-east-1a\"\n        }, \n        {\n            \"State\": \"available\", \n            \"RegionName\": \"us-east-1\", \n            \"Messages\": [], \n            \"ZoneName\": \"us-east-1b\"\n        }, \n        {\n            \"State\": \"available\", \n            \"RegionName\": \"us-east-1\", \n            \"Messages\": [], \n            \"ZoneName\": \"us-east-1d\"\n        }\n    ]\n}\n. I think timestamp would be fastest for querying and richest (because you\ncould analyze e.g. how regularly queries occur etc.)\nAlso agree etl_ts is a better name for this in the database\nOn Sat, Oct 26, 2013 at 2:44 PM, Alexander Dean notifications@github.comwrote:\n\nOn disk, run ids look like this: run=2013-08-22-11-20-46\nI'm wondering if we should store the run id in Redshift as:\n1. string, run=2013-08-22-11-20-46\n2. string, 2013-08-22-11-20-46\n3. timestamp, 2013-08-22 11:20:46\n3 https://github.com/snowplow/snowplow/issues/3 Might be most flexible\nfor querying... Maybe we call it etl_ts rather than run_id.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/396#issuecomment-27146532\n.\n. Thanks @kingo55! Will action now...\n\nOn Sat, Nov 30, 2013 at 4:18 AM, Alexander Dean notifications@github.comwrote:\n\nMany thanks @kingo55 https://github.com/kingo55 ! @yalisassoonhttps://github.com/yalisassooncan you action in the website then close?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/415#issuecomment-29545491\n.\n. That will be really useful\n\nOn Fri, Dec 6, 2013 at 7:29 PM, Alexander Dean notifications@github.comwrote:\n\n/cc @yalisassoon https://github.com/yalisassoon\nhttp://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-plan-tags-add-new.html\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/425\n.\n. Thanks @bamos!\n\nOn Sun, Jan 5, 2014 at 2:06 PM, Alexander Dean notifications@github.comwrote:\n\nMany thanks for this - @yalisassoon https://github.com/yalisassoon can\nyou please fix these?\nBTW Brandon I have tweeted about your new tool!\nhttps://twitter.com/SnowPlowData/status/419820663537684480\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/455#issuecomment-31604928\n.\n. Awesome!\nOn 11 Jan 2014 03:19, \"Brandon Amos\" notifications@github.com wrote:\nAlso #490 https://github.com/snowplow/snowplow/issues/490\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/460#issuecomment-32085163\n.\n. Note that GA's approach to the above is to over-report the number of visits / sessions.\n. We can use amazon-kinesis-connectors. I've tested this and can use it to output the contents of the stream to S3.\n\nTo see it in action, clone the repo:\ngit clone https://github.com/awslabs/amazon-kinesis-connectors.git\nThere is an s3 example app that generates a stream and then writes it to S3 as an example. It can be run using ant, by executing ant run from the samples/s3 directory in the repo. Note that in order to successfully run, you need to:\n1. Download the required libraries (The AWS Java SDK and the Amazon Kinesis Client.)\n2. Update the build.xml file to point at the location on your VM / EC2 instance where the two compiled dependencies are saved. (Note that the build.xml file assumes that the AWS Java SDK is saved in amazon-kinesis-connectors/src/aws-java-sdk-1.6.12 and the Kinesis Client Library is saved in amazon-kinesis-connectors/src/KinesisClientLibrary/lib/amazon-kinesis-1.0.0.jar... You will need to compile the Kinesis Client Library prior to making the JAR available.)\n3. Make the AWS credentials available to the library. This is easiest done by setting environment variables i.e. EXPORT AWS_ACCESS_KEY_ID=xxx, EXPORT AWS_SECRET_KEY=yyy. See this issue and this class for more details\n4. optional update the S3Sample.properties\nMy guess is that we should:\n1. Actually compile the project and upload it to a maven repo so it can be included as a dependency in our own library. (Someone appears to have created a pom.xml file for the project in this fork.)\n2. Implement a Scala wrapper so we don't have to repeatedly mess with the Java API\nWe can then use:\n1. The S3Emitter to write from the Snowplow Raw and Enriched stream to S3. This writes a file to S3 for every X records in the stream.\n2. The RedshiftManifestEmitter to efficiently write from a stream to Redshift. This makes use of the S3ManifestEmitter, which writes out streams to S3 (like the pure S3Emitter) but in parallel writes the S3 filenames of the outputted file to a different Kinesis stream. The RedshiftManifestEmitter pulls the filenames off that stream and then uses the Redshift COPY command to upload several of those files to Redshift in parallel.\n. I think passing new EMR jobs into existing clusters is pretty standard, but it's only something I've done myself as part of Hive interactive sessions. It is something you'd like me to prototype?\n. I can't ever remember seeing that error...\nI have double checked and can't find it in any of our records for either\nSnowplow or Psychic Bazaar. Does make me wonder if it's a Scala Stream\nCollector specific issue (maybe to do with the encoding of the user agent\nstring?)\nOn Thu, Mar 20, 2014 at 9:17 AM, Alexander Dean notifications@github.comwrote:\n\nTwo things I can think:\nWe bumped the version of our useragent library recently\nMaybe the Scala Stream Collector is having some kind of issue recording user agents\n\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/576#issuecomment-38147190\n.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoonhttps://twitter.com/yalisassoon\n. This is an example of the error in the syslog:\n2014-03-18 18:52:51,255 INFO org.apache.hadoop.fs.s3native.NativeS3FileSystem (main): Creating new file 's3n://snowplow-enrichment-output-l2foplpcaft7/bad-rows/run=2014-03-18-18-39-33/part-00000' in S3\n2014-03-18 18:52:51,521 INFO cascading.flow.hadoop.FlowMapper (main): cascading version: Concurrent, Inc - Cascading 2.1.6\n2014-03-18 18:52:51,522 INFO cascading.flow.hadoop.FlowMapper (main): child jvm opts: -Xmx288m\n2014-03-18 18:52:58,238 INFO cascading.flow.hadoop.FlowMapper (main): sourcing from: Hfs[\"TextLine[['offset', 'line']->[ALL]]\"][\"hdfs:/local/snowplow-logs\"]\n2014-03-18 18:52:58,239 INFO cascading.flow.hadoop.FlowMapper (main): sinking to: Hfs[\"TextLine[['offset', 'line']->[ALL]]\"][\"s3n://snowplow-enrichment-output-l2foplpcaft7/bad-rows/run=2014-03-18-18-39-33\"]\n2014-03-18 18:52:58,239 INFO cascading.flow.hadoop.FlowMapper (main): trapping to: Hfs[\"TextDelimited[[UNKNOWN]->[ALL]]\"][\"s3://snowplow-enrichment-output-l2foplpcaft7/error-rows/run=2014-03-18-18-39-33\"]\n2014-03-18 18:57:33,038 INFO org.apache.hadoop.fs.s3native.MultipartUploadOutputStream (main): 10000 write 1 bytes total:6048261\n2014-03-18 18:59:46,115 INFO org.apache.hadoop.fs.s3native.MultipartUploadOutputStream (main): 20000 write 1 bytes total:12156629\n2014-03-18 19:02:20,924 INFO org.apache.hadoop.fs.s3native.MultipartUploadOutputStream (main): 30000 write 1 bytes total:18281772\n2014-03-18 19:03:11,983 ERROR cascading.flow.stream.SourceStage (main): caught throwable\nStatus Code: 403, AWS Service: Amazon S3, AWS Request ID: 310DE465CF5389B9, AWS Error Code: null, AWS Error Message: Forbidden, S3 Extended Request ID: U1KdH1PS/E7oG37OxGWPtkc9xxc5RjdWrPl8WbSHzBsitSkGC7VzskY5WdvPOuOq\n    at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:619)\n    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:317)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:170)\n    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:2943)\n    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:767)\n    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:747)\n    at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:166)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:83)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n    at org.apache.hadoop.fs.s3native.$Proxy3.retrieveMetadata(Unknown Source)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:755)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdir(NativeS3FileSystem.java:1039)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdirs(NativeS3FileSystem.java:1032)\n    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1128)\n    at cascading.tap.hadoop.util.Hadoop18TapUtil.makeTempPath(Hadoop18TapUtil.java:278)\n    at cascading.tap.hadoop.util.Hadoop18TapUtil.setupJob(Hadoop18TapUtil.java:70)\n    at cascading.tap.hadoop.io.TapOutputCollector.initialize(TapOutputCollector.java:92)\n    at cascading.tap.hadoop.io.TapOutputCollector.<init>(TapOutputCollector.java:79)\n    at cascading.tap.hadoop.io.TapOutputCollector.<init>(TapOutputCollector.java:68)\n    at cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector.makeCollector(HadoopTupleEntrySchemeCollector.java:57)\n    at cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector.<init>(HadoopTupleEntrySchemeCollector.java:49)\n    at cascading.tap.hadoop.Hfs.openForWrite(Hfs.java:413)\n    at cascading.tap.hadoop.Hfs.openForWrite(Hfs.java:78)\n    at cascading.flow.hadoop.HadoopFlowProcess.openTrapForWrite(HadoopFlowProcess.java:289)\n    at cascading.flow.stream.TrapHandler.getTrapCollector(TrapHandler.java:58)\n    at cascading.flow.stream.TrapHandler.handleException(TrapHandler.java:131)\n    at cascading.flow.stream.TrapHandler.handleException(TrapHandler.java:115)\n    at cascading.flow.stream.ElementStage.handleException(ElementStage.java:134)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:107)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\n    at cascading.flow.stream.SourceStage.map(SourceStage.java:102)\n    at cascading.flow.stream.SourceStage.run(SourceStage.java:58)\n    at cascading.flow.hadoop.FlowMapper.run(FlowMapper.java:127)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:441)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:377)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\n2014-03-18 19:03:17,969 INFO org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore (main): Complete s3.putObject s3://snowplow-enrichment-output-l2foplpcaft7/bad-rows/run=2014-03-18-18-39-33/part-00000 len:20308585 sec:5\n2014-03-18 19:03:17,987 INFO org.apache.hadoop.mapred.TaskLogsTruncater (main): Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1\n2014-03-18 19:03:18,109 INFO org.apache.hadoop.io.nativeio.NativeIO (main): Initialized cache for UID to User mapping with a cache timeout of 14400 seconds.\n2014-03-18 19:03:18,134 INFO org.apache.hadoop.io.nativeio.NativeIO (main): Got UserName hadoop for UID 105 from the native implementation\n2014-03-18 19:03:18,137 WARN org.apache.hadoop.mapred.Child (main): Error running child\ncascading.flow.FlowException: internal error during mapper execution\n    at cascading.flow.hadoop.FlowMapper.run(FlowMapper.java:138)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:441)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:377)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: Status Code: 403, AWS Service: Amazon S3, AWS Request ID: 310DE465CF5389B9, AWS Error Code: null, AWS Error Message: Forbidden, S3 Extended Request ID: U1KdH1PS/E7oG37OxGWPtkc9xxc5RjdWrPl8WbSHzBsitSkGC7VzskY5WdvPOuOq\n    at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:619)\n    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:317)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:170)\n    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:2943)\n    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:767)\n    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:747)\n    at org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:166)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:83)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n    at org.apache.hadoop.fs.s3native.$Proxy3.retrieveMetadata(Unknown Source)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:755)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdir(NativeS3FileSystem.java:1039)\n    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.mkdirs(NativeS3FileSystem.java:1032)\n    at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1128)\n    at cascading.tap.hadoop.util.Hadoop18TapUtil.makeTempPath(Hadoop18TapUtil.java:278)\n    at cascading.tap.hadoop.util.Hadoop18TapUtil.setupJob(Hadoop18TapUtil.java:70)\n    at cascading.tap.hadoop.io.TapOutputCollector.initialize(TapOutputCollector.java:92)\n    at cascading.tap.hadoop.io.TapOutputCollector.<init>(TapOutputCollector.java:79)\n    at cascading.tap.hadoop.io.TapOutputCollector.<init>(TapOutputCollector.java:68)\n    at cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector.makeCollector(HadoopTupleEntrySchemeCollector.java:57)\n    at cascading.tap.hadoop.io.HadoopTupleEntrySchemeCollector.<init>(HadoopTupleEntrySchemeCollector.java:49)\n    at cascading.tap.hadoop.Hfs.openForWrite(Hfs.java:413)\n    at cascading.tap.hadoop.Hfs.openForWrite(Hfs.java:78)\n    at cascading.flow.hadoop.HadoopFlowProcess.openTrapForWrite(HadoopFlowProcess.java:289)\n    at cascading.flow.stream.TrapHandler.getTrapCollector(TrapHandler.java:58)\n    at cascading.flow.stream.TrapHandler.handleException(TrapHandler.java:131)\n    at cascading.flow.stream.TrapHandler.handleException(TrapHandler.java:115)\n    at cascading.flow.stream.ElementStage.handleException(ElementStage.java:134)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:107)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\n    at cascading.flow.stream.SourceStage.map(SourceStage.java:102)\n    at cascading.flow.stream.SourceStage.run(SourceStage.java:58)\n    at cascading.flow.hadoop.FlowMapper.run(FlowMapper.java:127)\n    ... 7 more\n2014-03-18 19:03:18,145 INFO org.apache.hadoop.mapred.Task (main): Runnning cleanup for the task\n2014-03-18 19:03:18,146 INFO org.apache.hadoop.mapred.DirectFileOutputCommitter (main): Nothing to clean up on abort since there are no temporary files written\n. Weird that the error I posted above does follow an attempted multipart upload. (And there doens't seem to be anything in it to suggest a temp file was created.)\n. What does it mean?\n. I've checked some more errors and they're all of the same style as the one I pasted above i.e. related to multipartuploads into the error-rows table.\nThis is very strange because the job had very generous permission on the error-rows location i.e.\njs\n    {\n      \"Sid\": \"PermissionsOnErrorRowsLocation\",\n      \"Action\": [\n        \"s3:*\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::snowplow-enrichment-output-l2foplpcaft7/error-rows/*\"\n      ],\n      \"Effect\": \"Allow\"\n    },\n...\n. ...but, I can see a weird temporary file in s3:\n$ aws s3 ls s3://snowplow-enrichment-output-l2foplpcaft7/\n                           PRE bad-rows/\n                           PRE error-rows/\n2014-03-14 05:24:43          0 error-rows_$folder$\n$\n. Ooooh - could it be that the temp file is being written to ...error-rows_* and the permissions are on ...error-rows/*?\n. Would delivering this fix it? https://github.com/snowplow/snowplow/issues/278\n. Or another potential fix might be to remove the trailling backslash in the error location ARN in the IAM permissions file e.g. \njs\n      \"Resource\": [\n        \"arn:aws:s3:::snowplow-enrichment-output-l2foplpcaft7/error-rows*\"\n      ],\nInstead of \njs\n      \"Resource\": [\n        \"arn:aws:s3:::snowplow-enrichment-output-l2foplpcaft7/error-rows/*\"\n      ],\nI can't see any examples online of people doing this, though...\n. Three files:\n- https://github.com/snowplow/snowplow/blob/feature/0.9.4-rebase/5-analytics/looker-analytics/lookml/events_ad_clicks.lookml\n- https://github.com/snowplow/snowplow/blob/feature/0.9.4-rebase/5-analytics/looker-analytics/lookml/events_ad_impressions.lookml\n- https://github.com/snowplow/snowplow/blob/feature/0.9.4-rebase/5-analytics/looker-analytics/lookml/events_ad_conversions.lookml\n. Looks great!\n. https://github.com/snowplow/snowplow/blob/feature/0.9.4-rebase/5-analytics/looker-analytics/lookml/events_link_click.lookml\n. https://github.com/snowplow/snowplow/blob/feature/0.9.4-rebase/5-analytics/looker-analytics/lookml/events_screen_view.lookml\n. Thanks!\nOn Thu, Apr 24, 2014 at 4:15 PM, Fred Blundun notifications@github.comwrote:\n\nsetLinkTrackingTimer has been replaced by pageUnloadTimer. This has been\ndocumented:\nhttps://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker#page-unload-timer\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/680#issuecomment-41292146\n.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoonhttps://twitter.com/yalisassoon\n. See http://docs.looker.com/documentation/reference/reference-derived-tables.html\n. I've been working on this the last couple of days - hoping to have\nsomething ready this week.\nHowever - may be worth making it a release of it's own, rather than\nbundling it into something bigger...\nAll development has been in\nhttps://github.com/llooker/snowplow/commits/master because that's the\naccount that's integrated with our Looker account.\nOn Sun, May 25, 2014 at 4:42 PM, Alexander Dean notifications@github.comwrote:\n\nMoving back\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/687#issuecomment-44137855\n.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoonhttps://twitter.com/yalisassoon\n. Closing - this encompasses https://github.com/snowplow/snowplow/issues/696 and https://github.com/snowplow/snowplow/issues/688\n. Totally agree with this approach - generate the unique identifier as early\nin the data pipeline as possible. Hope we can do this ultimately for the JS\ntracker as well\nOn Sun, May 4, 2014 at 8:53 PM, Alexander Dean notifications@github.comwrote:\n\nFor trackers which are capable of generating their own UUIDs.\n/cc @yalisassoon https://github.com/yalisassoon for feedback\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/722\n.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoonhttps://twitter.com/yalisassoon\n. Done in https://github.com/snowplow/snowplow/commit/749c393905b9e333f07f040fad300e079febcca4\n. Note that this would also make it possible for any user querying the SQL to data by country names rather than country codes\n. Great idea\nOn Thu, May 29, 2014 at 10:30 PM, Alexander Dean notifications@github.com\nwrote:\n\nhttp://www.product-open-data.com\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/780.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Assigning to @fblundun as he's likely to get round to this before me... @fblundun - hope that's OK!\n. Note that this will be written in Scalding\n. Thanks for sharing @akhill\nOn Mon, Jun 16, 2014 at 4:02 AM, Akhill notifications@github.com wrote:\n\nPer Keith @ looker, it seems the issue of persistent derived tables\nauto-generating may be related to quotation mark syntax:\n\"There was a small typo in the transactions and transaction items PDTs.\nThe SQL for the trigger had the wrong quotation marks around the\ninside/outside strings and it was throwing an error behind the scenes when\nthe PDT tried to update.\nOld code:\nSELECT\nCASE\nWHEN FLOOR(EXTRACT(hour FROM (current_time))) BETWEEN 6 AND 23\nTHEN \u2018inside\u2019\nELSE \u2018outside\u2019\nEND\nRevised code:\nSELECT\nCASE\nWHEN FLOOR(EXTRACT(hour FROM (current_time))) BETWEEN 6 AND 23\nTHEN 'inside'\nELSE 'outside'\nEND\n\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/830.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Closing as we no longer use this SQL for triggering table generation\n. This doesn't need to be done as we no longer trigger table generation using the above logic\n. This is an awesome idea.\nWould be fantatsic if we could push this functionality upstream into the\nindividual trackers...\nOn Thu, Jun 26, 2014 at 9:38 AM, Alexander Dean notifications@github.com\nwrote:\n\nPII = Personally Identifiable Information\nThe basic idea:\n- Any JSON Schema (ue or context) can be annotated with \"pii\": true on\n  a per-property basis\n- If this PII Scrubber is turned on, then we encrypt any given PII\n  field using AES - so you end up with a unique but non-PII value, e.g. \"Fred\n  Blundun\" always -> \"1de6e53cb23\"\nThis would be of potential interest to users in healthcare or finance,\nwhere the ability for analysts to drill down to individual users could be a\nprivacy concern\n/cc @yalisassoon https://github.com/yalisassoon @fblundun\nhttps://github.com/fblundun\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/860.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Instead of 'skip shred', what about a 'shred only' option that takes a\ncommand line argument pointing at the enriched output in s3, which would be\nthe input for this step?\nOn 28 Aug 2014 22:46, \"Alexander Dean\" notifications@github.com wrote:\n\nLet's do this in 0.9.7\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/927#issuecomment-53805806.\n. Thanks @fblundun!\n\nI'm going to have a go reworking the Overview section, as I wrote the\noriginal a long time ago now, when our thinking about the canonical event\nmodel was well behind where it's got to today.\nOn Wed, Aug 27, 2014 at 9:23 AM, Fred Blundun notifications@github.com\nwrote:\n\nI've added in the missing fields and updated the overview to mention\nshredding - could you check to see if I've described it correctly?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/966#issuecomment-53539908.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. I've made those updates - @fblundun would you mind updating the section on\nlink clicks and ad impressions, then adding corresponding sections on ad\nclicks, ad conversions, and screen views - these are now supported!\nThanks,\nYali\nOn Wed, Aug 27, 2014 at 9:35 AM, Yali Sassoon yali@snowplowanalytics.com\nwrote:\n\nThanks @fblundun!\nI'm going to have a go reworking the Overview section, as I wrote the\noriginal a long time ago now, when our thinking about the canonical event\nmodel was well behind where it's got to today.\nOn Wed, Aug 27, 2014 at 9:23 AM, Fred Blundun notifications@github.com\nwrote:\n\nI've added in the missing fields and updated the overview to mention\nshredding - could you check to see if I've described it correctly?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/966#issuecomment-53539908.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Test line:\n2014-10-11  14:01:05    -   37  172.31.38.31    GET 24.209.95.109   /i  200 http://www.myvideowebsite.com/embed/ab123456789?auto_start=e9&rf=cb Mozilla%2F5.0+%28Macintosh%3B+Intel+Mac+OS+X+10.6%3B+rv%3A32.0%29+Gecko%2F20100101+Firefox%2F32.0   e=se&se_ca=video-player%3Anewformat&se_ac=play-time&se_la=efba3ef384&se_va=\n. Nice!\nOn Thu, Oct 16, 2014 at 3:30 PM, Alexander Dean notifications@github.com\nwrote:\n\nUnscheduled\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1073.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Done - closing (see https://github.com/snowplow/snowplow/wiki/Configuring-the-Clojure-collector for additional setup steps)\n. Especially important as more and more users want to run their data pipeline every hour or two...\n. Sweet!\nOn Mon, Oct 27, 2014 at 2:37 PM, Alexander Dean notifications@github.com\nwrote:\n\nThis is in production now in 0.9.9 and seems to be working a treat:\n, [2014-10-27T14:07:53.781000 #12183] DEBUG -- : Initializing EMR jobflow\nF, [2014-10-27T14:07:58.505000 #12183] FATAL -- :\nSnowplow::EmrEtlRunner::DirectoryNotEmptyError (Cannot safely add shredding step to jobflow, s3n://xxx/shredded/good/ is not empty):\n    /opt/snowplow-0.9.9/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:250:in initialize'\n    org/jruby/RubyMethod.java:140:incall'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/contracts.rb:230:in call_with'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/decorators.rb:157:ininitialize'\n    /opt/snowplow-0.9.9/emr-etl-runner!/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:59:in run'\n    org/jruby/RubyMethod.java:124:incall'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/contracts.rb:230:in call_with'\n    /opt/snowplow-0.9.9/emr-etl-runner!/gems/contracts-0.4/lib/decorators.rb:157:inrun'\n    file:/opt/snowplow-0.9.9/emr-etl-runner!/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in (root)'\n    org/jruby/RubyKernel.java:1081:inload'\n    file:/opt/snowplow-0.9.9/emr-etl-runner!/META-INF/main.rb:1:in (root)'\n    org/jruby/RubyKernel.java:1065:inrequire'\n    file:/opt/snowplow-0.9.9/emr-etl-runner!/META-INF/main.rb:1:in (root)'\n    /tmp/jruby8025908767391820260extract/jruby-stdlib-complete-1.7.16.jar!/META-INF/jruby.home/lib/ruby/shared/rubygems/core_ext/kernel_require.rb:1:in(root)'\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1124#issuecomment-60602555.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Example JSON outputted by shredder:\njson\n{\n    \"schema\": {\n        \"vendor\": \"com.snowplowanalytics.snowplow\",\n        \"name\": \"change_form\",\n        \"format\": \"jsonschema\",\n        \"version\": \"1-0-0\"\n    },\n    \"data\": {\n        \"formId\": \"new_spree_user\",\n        \"elementId\": \"spree_user[invitation_code]\",\n        \"nodeName\": \"INPUT\",\n        \"type\": \"text\",\n        \"elementClasses\": [\n            \"title\"\n        ],\n        \"value\": \"\"\n    },\n    \"hierarchy\": {\n        \"rootId\": \"697ad3dd-66b8-4e60-a2af-d59fe7d8309b\",\n        \"rootTstamp\": \"2014-11-07 13:58:11.000\",\n        \"refRoot\": \"events\",\n        \"refTree\": [\n            \"events\",\n            \"change_form\"\n        ],\n        \"refParent\": \"events\"\n    }\n}\nOn Fri, Nov 7, 2014 at 3:40 PM, Alexander Dean notifications@github.com\nwrote:\n\n@yalisassoon https://github.com/yalisassoon is checking to see if the\nJSON output of the shredder contains \"\" or null...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1134#issuecomment-62162818.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Maybe we should move to putting commas at the start of table row definitions, rather than at the end?\n. Note that the models in this will be equivalent to those in https://github.com/snowplow/snowplow/issues/1272\n. Confirming!\nOn Tue, Jan 20, 2015 at 2:48 PM, Alexander Dean notifications@github.com\nwrote:\n\n@yalisassoon https://github.com/yalisassoon - can you confirm your 6\ncommits in this branch:\nhttps://github.com/snowplow/snowplow/commits/feature/sql_data_models\nrelate to this (and only this) ticket?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1273#issuecomment-70664169.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Update the permissions so you can perform vacuums? Or do you have something\nelse in mind?\nOn Mon, Jan 19, 2015 at 6:24 PM, Alexander Dean notifications@github.com\nwrote:\n\nAssigned #1328 https://github.com/snowplow/snowplow/issues/1328 to\n@yalisassoon https://github.com/yalisassoon.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1328#event-221150315.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Need to specify the string of API calls necessary, and how long after teh click ID we need to leave it before the data can be fetched from the Adwords API\n. Depends on https://github.com/snowplow/snowplow/issues/1073\n. Can we also update the way that the files are renamed as part of EmrEtlRunner so that the instance ID is added to the filename? Currently the instance ID is prepended to the filename - this means that if you list the files in any s3 editor or the cli, they don't list in chronological order\n. Ideally via the schema field\n. Love it - great idea\nOn Wed, Mar 4, 2015 at 9:20 PM, Alexander Dean notifications@github.com\nwrote:\n\n/cc @yalisassoon https://github.com/yalisassoon\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1470#issuecomment-77251355.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. We want to end up with two versions of the LookML\n1. For people who are not using the SQL data modeling\nFor these users Looker will still be responsible for generating the derived tables. We need to take the current models and update the SQL in them so that they are consistent with the SQL in the latest versions of the data models - note especially the much cleaner approach to frame-first and frame-last in the sessions processing.\n2. For people who are running SQL Runner\nFor these users we don't need Looker to manage the process for generating the derived tables. We simply need to update the first version so that it includes the same views (events / sessions / visitors) but that they are read directly from Redshift. This should mean updating the first part of the LookML views definitions for those views and removing any views that create intermediary tables.\n. Can this be closed now?\nOn Thu, Apr 16, 2015 at 1:22 PM, Alexander Dean notifications@github.com\nwrote:\n\nReopening till merge to master\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1522#issuecomment-93720637.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. No it hasn't...\n. Just done another analysis on a decent sized data set and found that 0.11% of events were recorded against IP's beginning with 192.168\nsql\nSELECT\nCASE WHEN user_ipaddress LIKE '192.168.%' THEN 'IP starts 192.168' ELSE 'Normal IP' END,\nCOUNT(*)\nFROM atomic.events\nGROUP BY 1\nORDER BY 1\nReturns:\nIP starts 192.168   5745723\nNormal IP   5093314942\n. This is a great idea Alessandro - we're discussing this internally - thanks\nfor raising!\nOn Mon, May 18, 2015 at 11:42 PM, Alessandro Andrioni \nnotifications@github.com wrote:\n\nThe manual decommissioning process described here\nhttps://github.com/snowplow/snowplow/wiki/Troubleshooting-Clojure-Collector-instances-to-prevent-data-loss\ncould probably be automated.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1696#issuecomment-103238423.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Love it\nOn 10 Jun 2015 3:07 pm, \"Alexander Dean\" notifications@github.com wrote:\n\nIf the SQL for an event is named canonically, why don't the pipelines\nexecute the create statement? It seems like a needless manual step.\nIf Redshift DDL is available under the jsonpaths assets path (say), then\nwe could perform this algorithm:\n1. Check if table exists by executing SELECT or inspecting Redshift\n   metadata\n2. If doesn't exist, fetch DDL from jsonpaths assets path and execute\n   DDL against database\nThis would require create table permissions.\nThis is more code to have to factor out when we move table definitions to\nIglu (along with JSON Paths files), but given that that will take some\ntime, this could be a good way of getting started with automated entity\ncreation in dbs.\nThoughts @yalisassoon https://github.com/yalisassoon ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1765.\n. Looks like for users with lots of custom context, the 'contexts' field accounts for almost 50% of the space taken, so fingers crossed switching the encoding to lzo will result in big space savings...\n. The team at Snowflake-Analytics has experimented with this and found large atomic.events table only take up 1/3 of the space as with the current set of defaults: http://snowflake-analytics.com/blog/infrastructure/snowplow-redshift-zstd/.\n\nAlso worth looking into if we need to be careful compressing sort keys: https://twitter.com/snowflake_data/status/885020933131554816. It would be nice to fix that page - @bogaert can you please create a ticket\nso we don't forget? Feel free to assign it to me...\nWhen the event studio is launched we can take it off the public site and\npop it in there...\nOn Wed, Sep 30, 2015 at 2:05 PM, Alexander Dean notifications@github.com\nwrote:\n\nHey @bogaert https://github.com/bogaert - my inclination would be to\nremove that page until as/when we have a Snowplow Event Studio that it can\nbe a part of - but see what @yalisassoon https://github.com/yalisassoon\nthinks...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2059#issuecomment-144389620.\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. This is genius\n. I don't know that this is the desired behaviour. We're working with a user now where we want to do some tidying up of the data with the JS enrichment (consolidating data from a number of fields in a single field) before using it that field as a key with the API enrichment. So we'd want to make sure that the JS enrichment runs before the API enrichment.\nI think the ideal scenario would be for each user to specify the order in which they want enrichments run?. cc @bogaert . This should make it easy to fire Snowplow events in response to actions in all the different third party webhooks that Zapier supports: https://zapier.com/app/explore\n. I think it should just read \"enter the endpoint collector URL\" (i.e. that's the only thing you can enter).\n. I've investigated this and it appears that any transactions in the base currency fail enrichment (and hence don't make it into the good rows) because the enrichment fails with the above error message.\nThis is really bad bug, because it means a large % of transactions (which are very high value events) fail to be processed successfully. \n. This impacts the table SORTKEY as well\n. Looks good - closing!\n. Looks great @ihortom! I've added a screenshot here: setup-guide/images/gtm-debug.png @alexanderdean please merge.\n. Shouldn't we be separately solving deduplication? Then we can happily\nreprocess away without worrying about accidentally reprocessing good data?\nOn Friday, May 20, 2016, Alexander Dean notifications@github.com wrote:\n\nOr maybe we are thinking about the \"bad rows\" output wrongly, and we\nshould simply be emitting well-formed Snowplow events for each event that\nfails on a 1:1 basis.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2684#issuecomment-220559956\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. That's awesome - I hadn't thought about it in that way before...\nOn Fri, May 20, 2016 at 11:34 PM, Alexander Dean notifications@github.com\nwrote:\n\nYes - we should be generally solving for the introduction of duplicates\nanywhere in the pipeline.\nBut thinking about the fact that our current bad rows output introduces a\nlot of duplicates needlessly when reprocessed (because its structure dates\nfrom when 1 payload :: 1 event) is a good starting point for coming up with\na better bad rows format.\nThe duplicates arise because you can't tell which event in a POST payload\na given failure applies to, which (regardless of the dupes) makes debugging\nharder. Another limitation of the current format is that it doesn't let you\nreview the failed event in Elasticsearch. Debugging and recovery with the\ncurrent bad rows format will get more challenging as we introduce a more\ncomplex enrichment DAG, and also schema inference...\nThere is a wider architectural issue too, which is that our stderr-style\nbad rows outputs (Kinesis or S3) don't compose. Our happy path composes\nwell: we can route a unitary happy path all the way from collection through\nto storage targets. But on the failure path, we end up with all these\nindividual silos of bad rows which need special-case treatment (like bad\nrows ingest into Elasticsearch).\nI suspect that the answer down the line is to unify the \"good\" and \"bad\"\noutput, so that there is just one output stream of events from a given\ncomponent, containing a spectrum of:\n1. Raw payloads which are corrupt\n2. Events which have failed schema validation\n3. Events which have failed enrichment\n4. Events which pass all validation and enrichment but contain ?-?-?\n   schemas which cannot (yet) be inferred\n5. Events which pass all validation, enrichment and inference with\n   warnings\n6. Events which pass all validation, enrichment and inference with no\n   warnings\nSo it's really a spectrum of (6) shades of grey, versus \"good\" and \"bad\".\nThe nice thing about this approach is that the whole thing composes,\nbecause it's just events. And there's no need for special casing for bad\nrows versus good: if you can represent an enriched event in Avro or\nRedshift, you can represent an event which has failed enrichment or has\nwarnings using the exact same tech...\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2684#issuecomment-220734389\n\n\nCo-founder\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0)203 589 6116\n+44 7841 954 117\n@yalisassoon https://twitter.com/yalisassoon\nhttps://twitter.com/yalisassoon\n. Thanks Ihor!\n. done!. Sorry for the delay - adding below:\n\nuser_id\nuser_ipaddress\nuser_fingerprint\ndomain_userid\nnetwork_userid\nip_organization\nip_domain\ntr_orderid\nti_orderid\nmkt_clickid\nrefr_domain_userid\ndomain_sessionid. Any of the se_ fields could require anonymization but I think they're all long enough already?. Hi @falschparker82 - thanks so much for raising this issue.\n\n@knservis is absolutely right. The purpose of the enrichment is not to prevent a malicious actor who wants to identify individual data subjects in the data. It is to prevent well intentioned data consumers accidentally using the data for a reason that is incompatible with the lawful basis for which the data has been collected. So as a concrete example, a marketer might be using the data to optimize the effectiveness of an ad campaign (by comparing the performance of different variations of the campaign). If the marketer then spotted as part of that analysis that one of the users that had engaged with the campaign had done something interesting that meant they might be susceptible to remarketing, the marketer would not be able to remarket to that user, because e.g. the 3rd party cookie ID necessary to setup that campaign would be obfuscated.\nHaving said that, the harder we can make it to un-anonymize the data, the better. For that reason I think your suggestion that we enable adding a salt is a great one and we should action it.. I love this! From a GDPR perspective it'd be an awesome place to make clear:\n(i) who is collecting the data\n(ii) on what basis data is collected\n(iii) how data subjects can exercise their rights\non the page in question.\nMassive thanks @rbolkey !. From my perspective this makes a lot of sense - it doesn't sound like very much functionality to add to EmrEtlRunner, but would drive a big benefit for our users in terms of much more frequent loading of Redshift. ",
    "ramn": "This is for the pixel hosting documentation. The javascript hosting docs seems ok\n. Looking at this\n. Began here https://github.com/ramn/snowplow/commit/7df0e300c95a726c0acd1364f190214163ba56b8\n. Sure! Also, see my latest email :)\n. Thanks @larsyencken I've added your test cases to my branch aswell.\n. This is configurable in Amazon. In the CloudFront behaviour, you have probably choosen Forward Query String = yes. This means you will get the query string both in the last column and also as you note in path column. You can safely disable query string forwarding, and end up with the query string in only one column.\n. Sure, lets merge!\nThis was just a first step, more to do.\nFor example, I think the aliases for window, document and such were introduced so they could be shortened during minification. This doesn't work anymore with the namespace, so the resulting file is bigger than before.\n. Related: https://github.com/snowplow/snowplow/pull/110\n. ",
    "larsyencken": "Hey, good stuff @ramn. I was looking at this independently as well. Feel free to grab a basic test from my branch https://github.com/99designs/snowplow/commit/068b53a5a262b0ee46ee3a62706824df6bd268bb\n. Some more details.\nWhat it does It scans an S3 bucket for new CloudFront logs. It keeps a manifest of files already included for each hour, and only fetches logs from S3 which aren't in the manifest for that hour. We then only need to upload hours which have changed.\nHow it's deployed Ideally we could just use S3, but because of the consistency issues we've found, we're instead aggregating on an EBS volume attached to an EC2 instance. This is the \"master\" copy of the aggregated data. We just keep an S3 bucket for hourly logs in sync with the master.\n. Here's a gist for that script:\nhttps://gist.github.com/4076413\nIt's actually pretty self-contained. Since our Ruby ETL script was archiving the logs into a second bucket, we set it up to read from both the original and archive bucket. Once configured, you run it like:\nfetch_and_combine.py /path/to/local/mirror\nInside /path/to/local/mirror, you get folders by date, and files by hour. For example, you'd get a file 2012-11-03/01.gz containing the aggregate logs and 2012-11-03/01.manifest listing the files it's already included for that hour.\n. We're still working out some of these issues.\nFor (1), the short answer is that we're planning to add a delay of 24h to our Hive workflow. Last we measured, 95% of logs arrived in S3 within 3h, the remaining 5% within 14h (a single spike in log delays). So 24h feels like a reasonable and conservative length to wait, in order to capture all of a day's data.\nThe longer answer is that we've had some trouble with Hive, so we're running two workflows. The second workflow is like a dependency graph. If the data for an hour has changed, it reparses just that hour and regenerates all derivative data. So, this second way of doing things is a bit more tolerant to log delays, but it's only efficient at incremental updates. For big batches, it's far less scalable than a working Hive setup.\nFor (2), you're absolutely right, we'll still need to archive logs to another bucket, and also to limit the scan for updated data. So far this has been done by the ruby ETL runner, but we may end up adding it to the Python workflow instead.\n. ",
    "mtibben": "@ramn Ahh thanks for that. I think we'll turn that setting off to cut down on log file size. \n@alexanderdean Great, I'll test the new version today\n. OK great.\nRegarding the boundary issue, I agree that we can take out the WHERE dt='${DATA_DATE}';.  That should solve the issue.\n. Yes that's right\n. Or perhaps have:\n--skip-staging  (Runs EMR on the :in bucket directly instead of staging)\n--skip-archive  (Doesn't archive after EMR)\nWould that achieve the same thing ?\n. > Where does the Commons Logging actually log to in a Hive/EMR world?\nThat's a good question. I've been logging the hive jobs to S3 using\n:emr:\n  :jobflow:\n    log_uri: 's3n://my-joblog-bucket/'\nand I'm assuming that the commons logging is going to the same place. \nI haven't been able to verify this yet however\n. Test cases have been fixed.\nI noticed that the useragent lib is not parsing some iPhone user agent strings correctly, were you aware of that ?\n. I've reported an issue: http://java.net/jira/browse/USER_AGENT_UTILS-30\n. awesome, great stuff\n. This may be of use for timezone detection https://bitbucket.org/pellepim/jstimezonedetect\n. The big advantage of this lib is that it gives you back a Olson tz key (e.g. Europe/Berlin) rather than a UTC time offset, which can be skewed due to daylight savings etc\n. Hmmm I haven't had any luck using subfolders on the processing bucket - I was getting hive errors any time I tried. Is that actually working ?\n. I've run into this myself, and I believe this could be an issue with S3 consistency. The files do disappear in time... but for some reason some files get stuck and take up to 24 hours to disappear\n. Yeah, this is a problem with big runs.. I have been bumping up the S3 concurrency constant when I need to do a big run, which does seem to help\nSomething else that @larsyencken has just started doing is aggregating the cloudfront logs hourly. That way there are only 24 files per day.. This is a quick and dirty python script that's running at the moment, but perhaps we could add an aggregation step to EmrEtlRunner?\n. Yes exactly\n. A massive advantage to doing this step is that there may be no need for the processing and archiving steps. Once the data is aggregated like that, the ETL can be run on any particular day quite easily\n. Sorry this has a bug...\n. No probs. By the way, the browser fingerprinting feature looks really interesting. This may be quite useful to us\n. This might help. Minified with --nomunge, the stack trace looks like this\n<anonymous function: d.addEventListener>([arguments not available])@ \naddClickListener([arguments not available])@sp-0.8.1-dbg.js:34 \naddClickListeners([arguments not available])@sp-0.8.1-dbg.js:35 \n<anonymous function: enableLinkTracking>([arguments not available])@sp-0.8.1-dbg.js:38 \nloadHandler([arguments not available])@sp-0.8.1-dbg.js:41 \nready([arguments not available])@sp-0.8.1-dbg.js:41 \n<anonymous function: e>([arguments not available])@\n. Awesome  :)\n. With the prepending I get Uncaught ReferenceError: Snowplow is not defined. \nI'm going to roll back to the previous stable version..\n. No I'm using snowpak.sh. hmm.. perhaps I did something wrong \nHere's my minified version with Snowplow. prepended to addEventListener \nhttps://dl.dropbox.com/u/123173/sp-0.8.1-dbg2.js\nDo you have a version I could compare?\n. Yup my mistake, I was using Snowplow instead of SnowPlow  :/\nI've re-minified and I'll see how this version goes.\nAlso the prefixing is required on the domainFixup function also\nhttps://github.com/99designs/snowplow/compare/bugs;js-prefix-snowplow\n. Prefixing also required for urldecode\n. Another issue: ReferenceError: getHostName is not defined\nI've now added the getHostName function into tracker.js.\n. Perhaps it might be a good idea to start putting together a testsuite for the tracker to test all codepaths and protect against regressions like this?\n. I haven't tried to reproduce these errors, it seems like it might be a bit tricky to set up the specific circumstances these fail in. I'm relying on Muscula to give me the stack traces..\n. Cool, I'm putting 0.8.2 into production now\n. ah yes, fixed in 53dda2504e98d82ddc7ed74682ac72fcd060099f\n. ",
    "rslifka": "Great discussion guys.  Now I'm wishing I brought my laptop with me to London!  Turns out I can't get much Elasticity done from this iPad ;)\nWill be back on 10/1 and will jump in then.\n. I believe it's inferred through the presence of the log_uri setting.  I.e. EMR infers that debugging is enabled by the presence of this path.\nhttps://github.com/rslifka/elasticity/blob/31b7bba38da776b0399afca2c67ae11a93bc9a8f/lib/elasticity/job_flow.rb#L14\n. Hmm...  That's from 2010 and my understanding was that SimpleDB is no longer supported.  I wonder if the method for enabling debugging has changed.  @alexanderdean, would you mind sending me an XML dump of a jobflow with debug successfully enabled?\nFor example:\nrequire 'elasticity'\ne = Elasticity::EMR.new(key, secret)\ne.describe_jobflow('jobflow_id'){|x| puts x}\n. Awesome thanks!  It's how I figure out how the UI settings map to their\nAPI.  I can implement it quickly if I have the XML from a jobflow.\nOn Sunday, August 11, 2013, Alexander Dean wrote:\n\nThanks @rslifka https://github.com/rslifka - good idea, will do...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/279#issuecomment-22462814\n.\n. Opened an Elasticity issue https://github.com/rslifka/elasticity/issues/47.\n. Released Elasticity 2.6 with debugging support, enjoy!\n. @fblundun Yep, AWS has changed the return and submission protocols, hence our conundrum :smile:   \n\nFortunately they're backwards compatible on the submission side which I've already updated alongside the switch to v4 signing.\nIn the process of implementing the new APIs now.  Fortunately no new plumbing is required - just deleting a whole bunch of XML code and implementing some new commands!  Follow along for 6.0 - https://github.com/rslifka/elasticity/milestones/6.0%20-%20New%20Amazon%20API\n. 6.0.2 released, will help with some of the error messages you're getting.  Let me know if there's anything else!\n. Elasticity release 6.0 in July 2015 removed all calls that were deprecated and updated to the latest APIs.  I'm not sure why you'd be getting this message :)\n. ",
    "rgabo": "@alexanderdean this can be closed, IMHO.\n. yeah, we would probably invoke this through resque-scheduler at some point\n. We have seen companies have a second part of their visit / session definition: after a 24 hour continuous session (no more than 30 minutes of inactivity), a new session is started. I wonder if anyone came across this definition, but I guess if sessionization happens as part of the ETL, this is easier to implement (even after the fact).\n. @alexanderdean, does this mean that you would standardise unstructured event fields with the usc_ prefix? IMHO ue_ might be a better choice as it is shorter and follows the same naming as se_.\nLet's discuss and decide here and we can contribute the documentation change to https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol#310-custom-unstructured-event-tracking.\n. @alexanderdean yes, we were thinking along the same lines, the Base64 encoded string would only show up in the (CloudFront) Collector logs and downstream subsystems would have it in a better representation (decoded or already serialized with Atro).\nI suggest we also update the tracker protocol reflect this. I can write it up, just let me know if I should go at it.\n(not sure why the Wiki wouldn't fork with the project on GitHub, would be awesome to include documentation changes in the same pull request)\n. Nesting timestamps just to explicitly call out the type might be adding unnecessary complexity but I can see how it is useful and probably even necessary with coordinates (as an array of numbers could be ambiguous: array of numbers vs coordinates).\nGiven that JS has a Date type, I looked at how it would be handled today:\n``` javascript\n\nJSON2.stringify({timestamp: new Date()})\n'{\"timestamp\":\"2013-03-27T12:12:39Z\"}'\n```\n\nThe JS tracker would need to take care of the nesting to allow passing Date objects (users shouldn't be concerned with this implementation detail) which has some added complexity.\n@alexanderdean 's arguments are valid for the nested types so as long as the trackers implement the functionality and the user can just pass a native timestamp object, it sounds good to me.\n. Guys, do we all agree on the Base64 encoding for ue_pr? We'd like to go ahead and implement this so that we can start tracking away for testing purposes and start work related to this in the Scalding ETL.\n20 is unrelated and quite frankly, orthogonal to this feature so I would not mix them. It's also a fairly large amount of work and I think we can live with the 2000 character limitation for now.\nWith regards to type suffixes, as long as the trackers handle the \"encoding\", it does make the ETL's life easier and at this point, I think it's really just Date that we need to handle internally as geo will need to be explicit from the user anyways (no native type to support). I also like the idea of using Unix Epoch, it's what the Mixpanel HTTP REST API uses and their reporting supports.\nWhat would a { \"time\": new Date() } serialize into by default? Unix Epoch with second precision ({\"time:dt\": 1364471251}) or ISO8601 with millisecond precision ({\"time:ts\": \"2013-03-28T11:46:46.343Z\"})?\n. @alexanderdean I like the idea of using two fields depending on the encoding to make it straightforward. Not sure about the:dtprefix, I associate it withdate` and we're also including time information down to the second. Is this a naming convention other JS libraries use? I just might not be familiar with them.\n/cc @tarsolya\n. Not bad!\n:ts => :timestamp\n:tms => :timestamp_milliseconds\nMakes sense to me. Is there any prior art we can look at? If not, I'm happy to go with this.\nI would also allow Date to be encoded as just :ts using the following syntax:\n{ \"timestamp:ts\": new Date()} would end up { \"timestamp:ts\": 1364471251}\nand\n{ \"timestamp\": new Date()} would end up {\"timestamp:tms\": \"2013-03-28T11:46:46.343Z\"}\ninternally.\n. @alexanderdean my bad, makes total sense. Let's just make sure that it fits downstream.\n. @alexanderdean @yalisassoon I have updated the SnowPlow Tracker Protocol page in the wiki in our fork with information and examples about the unstructured event tracking, feel free to change wording around. I also made some minor changes to outdated portions, but you guys should verify before pulling it into the main repo.\n. @yalisassoon no worries, I bet there is a better way but now it shows them as your edits so I can't be held responsible ;)\n. @alexanderdean, @yalisassoon what should the tracker do with an arbitrary suffix? Something that is not ts, tms or geo. According to your previous comment, I would assume that it needs to be validated, but this has not be spec'ed out. Any guidance here? We'd like to bowtie the pull request.\n/cc @tarsolya \n. @alexanderdean, let's do validation separately, in this case, we will leave the conversion of Date to unix epoch as it is for arbitrary suffixes.\nI'll add examples for the timestamp flavors and extract the SnowPlow.base64encode function to src/lib/base64.js.\nI find : to be a natural separator between property names and suffixes and I would only allow a single : in the property name to denote the type. Not quoting an object key and using a suffix will surely result in a syntax error:\njavascript\nJSON.stringify({ created_at:ts: new Date() })\nSyntaxError: Unexpected token :\nBear also in mind that proper JSON requires all keys to be quoted and I do not see a problem with having to quote type-suffixed keys. I also can't think of a better character that could be used unquoted.\n. @alexanderdean we discussed it with @tarsolya and we like the $ sign, it feels a bit weird but your reasoning is sound.\nWe'll go ahead and make these changes.\n. @alexanderdean we'll update the examples today and I think that's pretty much it. We'll build from our own fork until everyone is happy with it and it is merged back.\n/cc @tarsolya \n. Updated the examples with an explicit type suffix ($ts) and realized that in some cases a single date (without any time or even timezone information) is what you have and want to record. The $dt prefix would be perfect for this, but I'm not sure how it could be represented. Unix Epoch inherently carries time information (down to the second or millisecond) and that implies a timezone, even if its not part of the payload so there's some second-guessing when analyzing the data, unless we stick with UTC everywhere. Device time is currently recorded in local time with timezone information and I would guess collector_dt and collector_tm are UTC.\nWould love to hear how others deal with the chaos that is timezones and DST, even if we don't add a $dt suffix just yet.\n. @alexanderdean the Avro definitions make sense and look great, but we should have a discussion about the performance characteristics of how they are represented in Redshift for instance, because we had an internal discussion and we have some concerns with the typed property tables that you proposed. What would be the best medium for this so that everyone can be involved?\n. @tarsolya, thoughts on @alexanderdean's comment?\n. :+1:, we'll make these changes tomorrow.\n. @alexanderdean base64url encoding sounds good to me. Do we have a preferred implementation of this URL safe encoding in JavaScript? I'm looking around, but I wonder if someone has a choice that was battle tested. Some implementations rely on stuff that is not available in IE.\nPending this change, we'd like to have this verified by you guys and merged back as I think everything we wanted to implement as part of this pull request is done.\n. @alexanderdean anything left for this feature? We're starting on the ETL work for unstructured events, would love to see this merged and closed.\n. How about evercookie?\nGabor\nOn Sun, Aug 18, 2013 at 7:37 PM, Alexander Dean notifications@github.com\nwrote:\n\nSee also: http://lucb1e.com/rp/cookielesscookies/\nReply to this email directly or view it on GitHub:\nhttps://github.com/snowplow/snowplow/issues/210#issuecomment-22835116\n. There are a couple issues that this pull request relates to: #142, #211, #212, #213\n\nThe work is not based on the Avro work so far, we should discuss how we can sync these things up.\n/cc @yalisassoon @alexanderdean \n. @alexanderdean I was going to work on the additional step of processing ue_json from the canonical output on this branch and subsequently not keep the raw JSON in the Redshift schema :) Anyways, thanks for merging, it's still on the feature branch so if there's new stuff to look at either this pull request will show or we'll just look at it separately.\nLet me know what you think of the issues referenced in my comment and how they fit into the big picture (roadmap, etc.)\n. :+1: \n. I'm seeing potentially the same thing happening in the ue_json field (unstructured events payload). Simple letters such as \u00e9 are treated as unknown characters. This is in Navicat Premium 9.1.2 on OS X 10.8.\n\nCan they be related?\n. @alexanderdean will try to backtrack it to the request itself but it's pretty low priority for us to find the root cause right now. It's definitely happening often so it will be worth the investigation down the road.\n. This has been implemented in f17058a and b619626 simplifies continuing on unexpected errors\n. We have used this feature at least once in the past month ;) Is there any downside to not removing it?\n. @alexanderdean will get back to you re: Our users page. Thanks for keeping it in :dancers: \n. Hi @alexanderdean,\nI thought of the final jar as fully self-contained. We did a clean assembly of the jar with all dependencies retrieved again. Not sure what in the environment could affect this on the EC2 instances. We are bumping to 0.3.2 only now, so far we have been running on 0.3.1. Scala 2.10.0 is new in that regard as well.\nAny tips how to troubleshoot this?\n. I did @alexanderdean but I can go the extra mile to do it on an even cleaner one :) See if that solves it..\n. @alexanderdean I assembled the jar on a vanilla box that has not seen SBT / Scala or anything before. I'll try doing it on an Ubuntu box, but so far no luck. If you have any ideas to try, let me know.\n. @alexanderdean no luck, please let me know when you can look at this and how I can be of assistance. thanks so much--\n. @alexanderdean did you have a chance to look at this? We can't migrate to the latest version / schema and its causing us some headaches. If you let me know what I should try, we can try to figure this out what is causing this.\n. @yalisassoon makes total sense and thanks for letting me know. We'll try to figure out or stick with 0.3.1 for the time being. Still, if there is any idea what to check without spending any time on it, let us know. thanks!\n. @yalisassoon @alexanderdean the issue seems to be gone with 0.3.3. We're still only running 0.3.3 in our dev environment, if checks out in production as well this issue can be closed.\n. Related to and keeping an eye on #276, #271, #270 and the discussion on uncompressing, concatenating and recompressing for each hour.\nAll this would help tremendously like introducing S3DistCp did for the actual ETL and Redshift load.\n. @alexanderdean this is what we ended up doing: https://github.com/snowplow/snowplow/pull/319\nBasically create a filecrush step for each datestamped directory under the archive bucket. Although not the most optimal (steps are run in series and as such, S3DistCp doesn't really shine), it saves us over a million S3 requests to move the logfiles around.\nSuggestions more than welcome and let's make it more robust before it is worthy of merging (if at all).\n. @alexanderdean that would be awesome and would be the right way to do.\n. seems like others jumped on the feature request as well! I hope AWS can implement it.\n. rebased to 0.9.0 to use with latest Snowplow enrichment process\n. @alexanderdean yes, this is pretty cool and all files will have the right relative path (./log.gz).\n. @alexanderdean Do you have an ETA for this? We'd like to schedule the rerun of the affected period and would love to know when do you think we can do that.\nThanks!\n. The ability to filter by event type would be useful too. i.e. we record page_view (along with unstruct) events but might not want process them as part of our daily ETL process, only if we go back to archived logs.\n. @alexanderdean Redshift now provides basic JSON parsing functions that work with regular varchar columns. See http://docs.aws.amazon.com/redshift/latest/dg/json-functions.html for more information.\nWe have been keeping the raw JSON as part of our custom ETL under a ue_json column in Redshift for na\u00efve debugging (eyeballing, ue_json LIKE '%key:value%' and the likes) but we're moving to ue_properties so that we're consistent once the different unstructured event branches meet.\n. @alexanderdean I should say it would be highly unoptimized to use it for querying because none of the benefits of Redshift would apply. Not consisting of homogeneous values, there are no encoding and compression benefits so it is only useful for ELT style processing (UPDATE specific columns based on the JSON) and at low row numbers.\nThat being said, I'm certain you could create a custom dimension in Tableau today that uses this function and that's pretty rad. Haven't tried it though.\nWith this piece of information, how do you imagine the unstructured events would be laid out in the events schema?\nMaybe we should also measure performance and see how viable it is to just keep it in ue_properties.\n. @alexanderdean I meant the events table in the Snowplow schema.\n. Hey @jbeemster, yes, we are using the latest Java Tracker with BatchEmitter.\n. Ouch, forgot that trackers now have their own repos. Thanks Josh!\n. Great stuff as always @alexanderdean, looking forward to it. Do you think the process will move towards being more idempotent in the future? We've found that when orchestrating EmrEtlRunner as a single task in our workflow (using Luigi), if only shred fails we have to manually intervene with --process-shred and rerunning the task will not suffice, unless we add more complex logic to detect what output is already available and what isn't. Any thoughts?. If I'm not mistaken, spark-enrich will correctly pick up logfiles in nested hierarchies, i.e. if s3-dist-cp copies and groups logfiles into HDFS, but keeps the datestamp directories from an archive folder, the Spark job will still pick up all logfiles. This has not worked in the past with hadoop-enrich, right @alexanderdean? Would be amazing if it did now.. @BenFradet @chuwy @alex this is amazing work, thank you.\nWe are in the process of reprocessing a lot of archived Snowplow logs (both cloudfront and clj-tomcat) and I was wondering if you had an ETA for Scala 2.11 support?\nWe have in the past written custom pyspark code to parse the logfiles as we wanted to avoid rerunning hadoop-enrich and did not need a lot of what enrich produces. We'd much rather rely on spark-enrich for the reprocessing, but have a few time sensitive projects in progress.\nThe moment spark-enrich compiles on Scala 2.11, we will start testing it locally and on EMR 5.3.0 and let you know about any issues we face. Data we're planning to process contains everything from JS Tracker 0.9 to 2.x as well as the Java Tracker collected using both CloudFront and Clojure Collectors :)\nThanks for the continuous amazing work that you're doing!. @BenFradet jumping on the Scala 2.11 support and testing with Spark 2.1. I am going to submit the app without EmrEtlRunner for now, but is the current version of EmrEtlRunner compatible as far as the job submission goes?. Looks like #3048 is also coming along nicely. I will lego together what I need and let you know if I bump into any issues. Thanks a ton!. snowplow-common-enrich 0.25.0 does not seem to be published for 2.11 just yet:\nerror] (*:update) sbt.ResolveException: unresolved dependency: com.snowplowanalytics#snowplow-common-enrich_2.11;0.25.0: not found. @BenFradet I got to a point where I successfully ran spark-enrich using spark-submit locally and bumped into the first issue:\nEnrichJob tries to cache GeoLiteCity.dat locally and it is hardcoded to download to HDFS which is not available for local runs. For local Spark master, file:/// would work just fine.\nI can work around the issue by configuring MaxMind from S3 so that no downloading happens, but I still wanted to raise that Spark execution model now allows us to run enrichment locally and that means that HDFS/DistributedCache might have to be avoided.. @BenFradet yep, seemed to remember the discussion and I've reread it now. As long as FileUtils just passes s3 and similar paths through, it is easy to work it around. I'll see if I can think about something more idiomatic in Spark that works nicely with the GeoIP enrichment. Btw the file is larger (~100MB) if you actually purchase it :wink:. Well, IpLookupsEnrichment errors out with java.io.FileNotFoundException: ./ip_geo (No such file or directory), bubbled up from IpLookups because symlinking to HDFS/S3 still requires an underlying HDFS for the DistributedCache.createSymlink and DistributedCache.addCacheFile calls.\nHack-ish workaround is to copy/symlink the GeoIP database to ./ip_geo for local runs.\nIdeally there will be a better approach to distributing the file, but it's definitely out-of-scope for this PR, just documenting my findings.. @BenFradet it seems that Kryo serialization is configured in EnrichJob, but Array[com.snowplowanalytics.snowplow.enrich.common.outputs.EnrichedEvent] is not registered.\njava.lang.IllegalArgumentException: Class is not registered: com.snowplowanalytics.snowplow.enrich.common.outputs.EnrichedEvent[]\nNote: To register this class use: kryo.register(com.snowplowanalytics.snowplow.enrich.common.outputs.EnrichedEvent[].class);\n        at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:488)\n        at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97)\n        at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517)\n        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:622)\n        at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:312)\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:324)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nAdding the missing class to EnrichJob.classesToRegister fixes the issue. I have successfully ran spark-enrich producing both good and bad output in one pass on local mode. YAY!. @BenFradet no problem. I totally forgot to pull before I checked what's going on. Unfortunately if you force push, notifications don't work as you'd expect them as there are no \"new\" commits. I'd avoid force pushes to better document what's going on and to be outside collaborator friendly :wink:. Hi @BenFradet, I'm facing an interesting issue when running spark-enrich on EMR with a private MaxMind GeoIP database configured. It seems that the configured database is not properly download on the executors and even though it resides on S3, the Spark executors fail the enrichment with Could not extract geo-location from IP address [x.x.x.x]: [java.io.FileNotFoundException: ./ip_geo (No such file or directory).\nAs we have a paid database, we would prefer to use that instead of the publicly available GeoLiteCity.dat baked into the fat jar.\nAny pointers appreciated!\nEnrichment configuration:\njson\n{\n    \"name\":\"ip_lookups\",\n    \"vendor\":\"com.snowplowanalytics.snowplow\",\n    \"enabled\":true,\n    \"parameters\":{\n        \"geo\":{\n            \"database\":\"GeoIPCity.dat\",\n            \"uri\":\"s3://top-secret-bucket/lib/snowplow\"\n        }\n    }\n}. Thanks @BenFradet, temporarily removing the ip_lookups enrichment altogether results in 100% good rows. Let me know if you find something.. @BenFradet I found an inefficiency during my testing: The isEmpty checks for bad and good RDDs result in a single task assigned to a single executor that will check all partitions for the existence of at least 1 item using take(1) == 0. The complexity is O(n) where n is the number of partitions, but the problem is that with gz files this means that a single executor has to download all filles, uncompress all files, process all files only to realize there are no bad rows. That is obviously inefficient :smile:\nA simple paralellization of the isEmpty check does the trick: https://github.com/sspinc/snowplow/commit/d5509966f03be5603443d2fdae7637c254c1ee04\nThere's a lot more unrelated work on that branch that we need to do to maintain backward-compatibility, but feel free to cherry pick the commit itself.. @BenFradet that was my thought as well, you'll have to adjust the tests as they are looking for the existence or absence of those directories.. @BenFradet thanks for the heads-up, good stuff.. @BenFradet GREAT news, I will test later today or tomorrow.. @BenFradet I'm running the latest spark-enrich and see our GeoIPCity.dat cached:\n17/03/17 14:49:11 INFO Utils: Fetching s3://secret-bucket/lib/snowplow/GeoIPCity.dat to /mnt/tmp/spark-4633d8c1-414f-405b-b036-00faadb62ece/userFiles-6a5bfc2c-609c-43eb-bd4b-f53a25e29f41/fetchFileTemp7566080315525470476.tmp\nUnfortunately soon after the tasks fail with:\n17/03/17 14:49:20 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4, ip-10-64-33-234.ec2.internal, executor 1): java.nio.file.FileAlreadyExistsException: ./ip_geo\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\n        at sun.nio.fs.UnixFileSystemProvider.createSymbolicLink(UnixFileSystemProvider.java:457)\n        at java.nio.file.Files.createSymbolicLink(Files.java:1043)\n        at com.snowplowanalytics.snowplow.enrich.spark.EnrichJob$$anonfun$4$$anonfun$apply$3.apply(EnrichJob.scala:172)\n        at com.snowplowanalytics.snowplow.enrich.spark.EnrichJob$$anonfun$4$$anonfun$apply$3.apply(EnrichJob.scala:169)\nCommand used to invoke Spark Enrich:\nspark-submit lib/snowplow/snowplow-spark-enrich-1.9.0.jar --input-folder \"s3://secret-bucket/raw/logs/snowplow/$pipeline/$datestamp/\" --input-format $format \\\n                 --output-folder \"s3://secret-bucket/etl/ingest/enrich_snowplow_events/$pipeline/$datestamp/good/\" --bad-folder \"s3://secret-bucket/etl/ingest/enrich_snowplow_events/$pipeline/$datestamp/bad/\" \\\n                 --etl-timestamp $(date \"+%s\") \\\n                 --iglu-config ew0KICAic2NoZW1hIjogImlnbHU6Y29tLnNub3dwbG93YW5hbHl0aWNzLmlnbHUvcmVzb2x2ZXItY29uZmlnL2pzb25zY2hlbWEvMS0wLTAiLA0KICAiZGF0YSI6IHsNCiAgICAiY2FjaGVTaXplIjogNTAwLA0KICAgICJyZXBvc2l0b3JpZXMiOiBbDQogICAgICB7DQogICAgICAgICJuYW1lIjogIlNlY3JldCBTYXVjZSBTdGF0aWMgSWdsdSBTZXJ2ZXIiLA0KICAgICAgICAicHJpb3JpdHkiOiAxMCwNCiAgICAgICAgInZlbmRvclByZWZpeGVzIjogWw0KICAgICAgICAgICJjb20uc25vd3Bsb3dhbmFseXRpY3Muc25vd3Bsb3ciLA0KICAgICAgICAgICJpby5zc3BpbmMiDQogICAgICAgIF0sDQogICAgICAgICJjb25uZWN0aW9uIjogew0KICAgICAgICAgICJodHRwIjogew0KICAgICAgICAgICAgInVyaSI6ICJodHRwOi8vaWdsdS5zc3BpbmMuaW8iDQogICAgICAgICAgfQ0KICAgICAgICB9DQogICAgICB9LA0KICAgICAgew0KICAgICAgICAibmFtZSI6ICJJZ2x1IENlbnRyYWwiLA0KICAgICAgICAicHJpb3JpdHkiOiAyMCwNCiAgICAgICAgInZlbmRvclByZWZpeGVzIjogWw0KICAgICAgICAgICJjb20uc25vd3Bsb3dhbmFseXRpY3MiDQogICAgICAgIF0sDQogICAgICAgICJjb25uZWN0aW9uIjogew0KICAgICAgICAgICJodHRwIjogew0KICAgICAgICAgICAgInVyaSI6ICJodHRwOi8vaWdsdWNlbnRyYWwuY29tIg0KICAgICAgICAgIH0NCiAgICAgICAgfQ0KICAgICAgfQ0KICAgIF0NCiAgfQ0KfQ0K \\\n                  --enrichments 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\nI am still troubleshooting if it somehow I am causing the issues, but I wanted to report the issue with the hope that you have some ideas.\nThanks!. @BenFradet I've tested the change, all Spark executors fail a single task once but work on second retry and complete all tasks successfully. Definitely an improvement, now why would the first task for each executor fail.... Turns out it's not all executors, but in both test runs I've seen 8 tasks fail:\n\n. @BenFradet what are next steps for Spark Enrich? I see that the features are scheduled for R89, is there anything missing? Is there any way there could be a \"semi-official\" release of the features that we could use and thoroughly test in anticipation of R89? Thanks!\n/cc @alexanderdean . @alexanderdean awesome, looking forward to both.. That's an interesting thought, but makes sense and it's also shorter. We ought to specify the format and encoding in the tracker protocol specification. Would you already limit the type of content to be sent through the wire there or deal with that only when it's ETL time? The single properties argument looks and acts like it can take any JS object now and there's no validation that someone doesn't start sending up nested object.\n(The original names were coming from the canonical event specification which was already available at the time. These shorter names of course make a lot more sense so I'm making these changes as we speak.)\nBtw, this is also the reason I updated the example for trackUnstructEvent so that it is limited to certain field types.\n. Temporary, known limitation, the Redshift events table will not have the raw JSON as it has no support for it, but for the time being it comes handy for end-to-end testing.\n. There's a typo in \"dataase\" that I just came across.. \ud83d\ude0a . sbt assembly produces target/scala-2.11/snowplow-spark-enrich-x.x.x.jar (rightly so). I think the name of the jar should be consistent with the project name (spark-enrich).. ",
    "rcs": "+1. Just wanted to pull a quick \"Mobile vs non-mobile\" device thing, and this was pretty sad with the exclusion of iOS.\nWhat can be done to help move this forward?\n. +1\n. +1 just ran into this for the refr_term column, with people putting full (1000+ character) urls into google search boxes.\n. ",
    "kingo55": "For the interim, should we just move to the latest version of Useragent Utils (this is the one we're using right?)? They're up to 1.11 now and look to have fixed a number of things. \nhttps://github.com/HaraldWalker/user-agent-utils\n. The fact that it's broken could be quite the opportunity though. We could make clicktracking a bit more useful (similar to how GA uses its new link attribution). What do you think about this, @alexanderdean :\nhttp://analytics.blogspot.com.au/2012/11/announcing-enhanced-link-attribution.html\n. I never found the connection speed dimension all that useful when GA used to have it. I do however find the timings in the site speed reports much more handy - you know, the ones that use performance.timing.\nhttp://www.igvita.com/2012/04/04/measuring-site-speed-with-navigation-timing/\nI find it's particularly useful to demonstrate the worth of clients optimising site speed and investing in better servers / CDNs. More of a nice to have though.\n. Glad you like the idea, guys. I've been wanting to do this for several\nmonths now but haven't been able to do it.\nCustom Vars make sense for this. How flexible are they going to be in\nSnowPlow? I wanted to do this in GA but obviously you're limited to just so\nmany bytes (127?) and slots.\ntrackOffer and trackContent were the only names I could come up with that\nmade any sense at the time. I'm sure we'll find something more appropriate\ndown the track. I think we need a name that works for products, articles,\nlistings and what not.\nI thought about tracking only visible products too and figured it would\ncomplicate things like the setup and bump up the costs of Amazon. I figured\nranking on the page/within each area would give us some good basic figures\n- like those available in Google AdWords.\n  On 17/12/2012 2:13 AM, \"Yali\" notifications@github.com wrote:\n\nIt's an awesome idea - I'm convinced there's a huge amount of business\nvalue in that product specific analytics. We've done a bunch of demoing of\na Tableau cube on top of SnowPlow data, and the visualisation that gets the\nmost interest is the one that compares product views with add to baskets\nwith number purchased for an online retailer. (It's the third video on this\nblog post:\nhttp://snowplowanalytics.com/blog/2012/10/24/web-analytics-with-tableau-and-snowplow/.\nThis was achieved by joining the SnowPlow data set with a table from the\nCMS that mapped SKUs to product URLs, so we were able to infer product\nviews from page views.)\nBefore discussing how we would implement it, it'd be good if we could\nflesh out the biz requirements. I think it'd be great if:\n1. We could track not only what products (for retailers) or articles /\n   media (for a media company) were viewed but meta data associated with those\n   products / articles e.g. SKU, author and where the product listing\n   is. It would also be good to be able to distinguish the type of\n   presentation e.g. a product page vs a shortened listings / feature on the\n   homepage or a listing on a catalogue page. We might also want to capture\n   whether a user had hovered over a product listing.\n2. We could decouple product views from page views so that if e.g. a\n   user opened a catalogue page with a long list of products on it, the\n   \"trackProduct\" function would only fire for products below the fold as the\n   user scrolled down the page.\nTwo general implementation options I can think of:\n1. A dedicated Javascript function call and corresponding dedicated\n   fields in the data structure, as suggested by @kingo55https://github.com/kingo55.\n   The more I think about it, the more I think this is the best approach\n2. If decoupling product views from page views wasn't possible (or was\n   technically flakey for some reason) - I'd be tempted pass the product array\n   as a page-level custom variable. We haven't implemented custom variables\n   yet - but the general idea is that there are 4 types of custom variable\n   distinguished by scope: user-level variables, session-level variables,\n   page-level variables and event-level variables. We could therefore set one\n   of the page-level variables to be a product array. This would be directly\n   queryable in Hive. In Infobright, we'd need to deserialize it as part of\n   the ETL process into a line of data per product in the array\nIf everyone agrees we go with the first option, the key things to finalise\nwould be the fields in the schema and the name / structure of the\nJavascript function...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/113#issuecomment-11418651.\n. @shermozle - That was certainly confusing. From what I understand, it's just for cart additions though. Is that right?\n\n@yalisassoon - Amazing! So cool to see this on Christmas Day too. Will check this out over the next few days in detail. A couple of thoughts, so far:\n- item ranking - If this is missing, will this be inferred based on the order of items passed through the trackItemView() and will the order of item locations also play a part in determining ranking?\n- Will each product create a new trackItemView() and request for the pixel?\n- One general comment I think is important is that I have a few ideas about things we can track through pageviews. Should we include some mechanism to switch certain functionality off and on? e.g. What if someone wants to use the slots we're using for this feature for something else?\n. Cheers, Alex.\nRanking\nGot you... that makes sense.\nTracking Pixels\nWhat about category pages with 100+ products per page.\nSurely 100 extra requests will add a little bit of weight to the page\nalongside some extra CPU resource (surely on mobile devices). Is there any\nreason we couldn't send an array of values through the one request and\nstrip it apart in the ETL? e.g. the reverse of what's been done for the\nevents tracking.\nOn 26/12/2012 6:23 AM, \"Alexander Dean\" notifications@github.com wrote:\n\nHey @kingo55 https://github.com/kingo55 - glad you're excited about\nthis feature! We are too.\nOn your specific questions:\nitem ranking - If this is missing, will this be inferred based on the\norder of items passed through the trackItemView() > and will the order of\nitem locations also play a part in determining ranking?\nWe cannot guarantee any ordering once the item views get to the\ncollector/ETL, so any ordering information needs to be set in the tracker.\nWe might be able to come up with an implicit ordering based on the order in\nwhich the trackItemView() calls are added to the async tracking queue (\nsnaq), but this feels a bit 'magical'/hairy to me. Probably best just to\nmake this explicit with the item ranking field.\nWill each product create a new trackItemView() and request for the pixel?\nYep.\nOne general comment I think is important is that I have a few ideas about\nthings we can track through pageviews.\nShould we include some mechanism to switch certain functionality off and\non? e.g. What if someone wants to use\nthe slots we're using for this feature for something else?\nI would say a couple of things here:\n1. Do please share your ideas of what we should be tracking as first\n   party data through pageviews!\n2. We are working on our custom variable approach, and current\n   thinking is that (among other types of custom variable), the user will get\n   10 \"slots\" for custom page-level variables. The idea is that you set these\n   once per page and then they are transmitted with every event sent on that\n   page. So our current thinking is that, if someone wants to track something\n   else outside the \"first class\" pageview fields, then they can always use\n   those page-level variables. But again, interested to hear your thoughts -\n   this is all still up-in-the-air!\n\u2014\n   Reply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/113#issuecomment-11675537.\n. That makes sense... I'm not all that familiar with the ETL process unfortunately. \n\nAnd yes, I saw that workaround for the IE URL length... good idea! I'm sure that'll come very much in handy for big events like this.\nGiven the situation, are we best off waiting a couple of months for the Hadoop ETL? Can imagine this will have some repercussions for the implementation.\n. Agreed.\nAs for using it, I'm trying it out on my blogs at the moment. I'd like to get more comfortable with it myself before recommending it to clients (I have a few retail clients that would benefit massively from this). I'm not really familiar with Hadoop and the like though, so even though I have the desire to use it, it's going to take me time to really get my head around it.\n. Cheers, Alex.\nYou might prefer the Github to get straight to the readable commented\nscript: https://github.com/kingo55/ga-scroll-reach\nIt's fairly rudimentary and based off jQuery for browser compatibility. I\nfigured it would be perfect for the heartbeat feature you guys are looking\ninto.\nI wrote it with responsive sites in mind.\nOn 08/01/2013 8:43 PM, \"Alexander Dean\" notifications@github.com wrote:\n\nSee @kingo55 https://github.com/kingo55's blog post for implementation\nideas:\nhttp://www.optimisationbeacon.com/analytics/track-how-far-your-users-scroll-in-google-analytics\nhttp://twitter.com/robkingston/status/288555947675242497\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/127.\n. Well it's a tricky thing to measure because different browsers return different figures for different measurements. I found IE 5-8 completely unreliable and have excluded them from the tracking - even using jQuery. Whereas other browsers appear to be more consistent.\n\nFor a good read on viewports and document height, I recommend the following resource: http://responsejs.com/labs/dimensions/\nDo we really want to track percent scroll reach?\nThere are a number of ways around the whole different viewport dimensions but I decided to opt for a percentage scrolled approach for a couple of reasons:\n- Responsive designs \n- Users changing their window sizes\n- The sheer number of viewport sizes you'll encounter\n- Updated content/designs\nThere were quite a few things which I didn't track in the GA script (purely because I wanted to keep it simple for passing the cookie value to GA) but these are also potentially useful measurements.\n- Viewport/Document height\n- Viewport width\n- Max scroll top\nFor this reason, it could be a better idea to track a couple of variables and calculate scroll reach in the reporting. You would simply add the viewport height and scrolltop together and divide it by the document height to arrive at scroll reach %.\nDoes that sound like a better option or am I overcomplicating it?\n. Fascinating... will have to check this out.\nCouple of thoughts:\n- Bookmark data could be included in here - particularly for return visits.\n- What about the dark social that includes a referrer? E.g. non-secure web mail.\nOn 09/01/2013 4:57 AM, \"Alexander Dean\" notifications@github.com wrote:\n\n/cc @ppcanalytica https://github.com/ppcanalytica\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/130#issuecomment-12008722.\n. Oops - didn't even notice the JS code the other day (very nifty). Was too excited about measuring dark social ;).\n\nI take it that if a referrer exists, it will override utm_share? Just thinking about blog posts or tweets where people copy and paste your URL.\nAlso what about rel=\"noreferrer\"? Not too sure how much this is been used though.\n. One of my readers shared another method that I find particularly fascinating. Using cached redirects for cookieless tracking (by its nature it would be cross-domain too).\nhttp://danielmiessler.com/blog/tracking-web-visitors-using-cached-http-redirects-scatmania\nhttps://gist.github.com/avapoet/5318224\nThoughts, @alexanderdean, @yalisassoon and @shermozle?\n. Hey Alex,\nI managed to work around this issue by using the following query to load events:\nruby\n\"COPY #{target[:table]} FROM '#{f}' WITH CSV ESCAPE E'\\\\\\\\' DELIMITER '#{EVENT_FIELD_SEPARATOR}' NULL '#{NULL_STRING}';\"\nIt works on the data in my table without error. Mind you my SQL is pretty bad though...\n. Hi Alex,\nYou're welcome. It's the least I can do.\nWouldn't this effectively escape \\t anyway?\nOn 29/08/2013 7:44 PM, \"Alexander Dean\" notifications@github.com wrote:\n\nHi Rob,\nThanks for this and for your pull request! I think your approach is the\nright one - basically changing the escape character so that e.g. \\t can't\nbreak the COPY statement.\nHowever, we can't guarantee that \\t won't naturally occur in a field\neither... So I think we need to replace the escape character with\nsomething\nguaranteed not to appear in a field - e.g. a control character explicitly\nremoved by:\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/utils/ConversionUtils.scala#L89\nI will take a look when I'm back next week!\nA\nOn 24 August 2013 03:09, Robert Kingston notifications@github.com\nwrote:\n\nHey Alex,\nI managed to work around this issue by using the following query to load\nevents:\n\"COPY #{target[:table]} FROM '#{f}' WITH CSV ESCAPE E'\\' DELIMITER\n'#{EVENT_FIELD_SEPARATOR}' NULL '#{NULL_STRING}';\"\nIt works on the data in my table without error. Mind you my SQL is\npretty\nbad though...\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23198658>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23477933\n.\n. Cool. Was worried my fix was \"too easy\"\nOn 29/08/2013 7:56 PM, \"Alexander Dean\" notifications@github.com wrote:\nHmm - it might do, I need to check! I'll add in some unit tests for\nfixTabsNewlines in the next release so we can decide exactly what it a\nsafe\nescape character for Postgres...\nhttps://github.com/snowplow/snowplow/issues/332\nA\nOn 29 August 2013 12:48, Robert Kingston notifications@github.com\nwrote:\n\nHi Alex,\nYou're welcome. It's the least I can do.\nWouldn't this effectively escape \\t anyway?\nOn 29/08/2013 7:44 PM, \"Alexander Dean\" notifications@github.com\nwrote:\n\nHi Rob,\nThanks for this and for your pull request! I think your approach is\nthe\nright one - basically changing the escape character so that e.g. \\t\ncan't\nbreak the COPY statement.\nHowever, we can't guarantee that \\t won't naturally occur in a field\neither... So I think we need to replace the escape character with\nsomething\nguaranteed not to appear in a field - e.g. a control character\nexplicitly\nremoved by:\n\nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/utils/ConversionUtils.scala#L89\n\nI will take a look when I'm back next week!\nA\nOn 24 August 2013 03:09, Robert Kingston notifications@github.com\nwrote:\n\nHey Alex,\nI managed to work around this issue by using the following query to\nload\nevents:\n\"COPY #{target[:table]} FROM '#{f}' WITH CSV ESCAPE E'\\' DELIMITER\n'#{EVENT_FIELD_SEPARATOR}' NULL '#{NULL_STRING}';\"\nIt works on the data in my table without error. Mind you my SQL is\npretty\nbad though...\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23198658>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23477933>\n.\n\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23478131>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/329#issuecomment-23478524\n.\n. Oops - it's signed now.\nOn 27/08/2013 3:49 AM, \"Alexander Dean\" notifications@github.com wrote:\nMany thanks @kingo55 https://github.com/kingo55 ! Have you signed our\nCLA - https://github.com/snowplow/snowplow/wiki/CLA\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/pull/330#issuecomment-23280028\n.\n. Ahuh. I was going to file a bug for this. You're on top of it it seems,\nAlex.\n\nGoing to bump that AMZN thread as this is just annoying.\n@alexanderdean https://github.com/alexanderdean Do you have an ETA for\nthis? We'd like to schedule the rerun of the affected period and would love\nto know when do you think we can do that.\nThanks!\n\u2014\nReply to this email directly or view it on\nGitHubhttps://github.com/snowplow/snowplow/issues/333#issuecomment-23778935\n.\n. Thanks for the quick resolution on this, @alexanderdean.\n. Interesting. My Postgres warehouse is really degrading at the moment. I'm\nrunning out from an M3.xlarge instance and as soon as I hit 18M queries it\nstarted taking exponentially longer. Queries that would take 15 seconds @\n10M rows began taking 5-10 minutes.\nI ended up having to delete old client data from my atomic.events table and\nrunning Vacuum Full (I know... The horror, right?).\nBefore my M3.xlarge instance, I was able to run it off an old dual core\nlaptop with 3gb of ram. Once it hit 5M rows it ground to a halt. Based on\nthis, I suspect upgrading to an M3.2xlarge will stem the bleeding... But\nfor how long? Some of these instances aren't cheap.\nJust curious if you could share your volumes and specifications.\nOn 26 Jun 2014 23:40, \"Matt\" notifications@github.com wrote:\n\nWe migrated from postgres to Hive after running the warehouse there for a\nfew months. I was impressed by how gracefully postgres degraded, but it\nonly wound up handling data from about 2 of 10-ish clients. With Hive, the\nonly real bottleneck has been getting data out, not anything internal that\nsome simple partitioning couldn't solve.\nThat being said, we did a little tuning in postgres before we gave up on\nit. To diagnose issues, we used a tool http://explain.depesz.com/s/hWDs\nfrom a colleague we worked with at Etsy that visualizes explains. One trick\nwas simply to bump the work_mem from the default 1MB to something much more\ngenerous like 512MB or 1GB, which let sorts be done in-memory.\nWe stopped short of actual indexing, but if we had, event and page_path\nwould have been our first two targets because most of our queries are done\nalong those dimensions (we partition by event now, and have several large\nper-page analytics jobs). For ad-hoc analytics, users and visits are more\nimportant, so indexing domain_userid and domain_sessionidx makes sense.\nIt's unfortunate that we tore down our postgres warehouse or we could have\nused it as a testbed for you optimizations.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/356#issuecomment-47226452.\n. What sort of costs are you seeing @mrwalker ? \n\n@alexanderdean  - if you need a test bed for some Postgres optimisations, I might have my old laptop with Snowplow on it. \n. Hi guys,\nPerformance is degrading again so I tested a bunch of tweaks to postgresql.conf but so far nothing seems to have an effect on query times. Followed your advice @mrwalker and some recommendations on the wiki: https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server\n- Changed work_mem 1MB, 64MB, 1024MB => no change\n- Changed shared_buffers: 32MB, 64MB, 512MB, 4096MB => no change\n- Changed the random_page_cost: 5.0, 4.0, 3.0 => no change\n@mrwalker - did performance degrade linearly for you? I.e. DB size +2% = Query time +2% \nHow long were your queries taking before you moved to Hive?\n. +1\nDo you mean like an SQL statement? Another program/script? \n. Have also found that it doesn't always attribute the correct referrers. \nRunning the following query produced values in my mkt_* fields but when I look at the same sessions under the cube, I don't see anything in the mkt_* fields:\nSELECT\nmkt_medium,\nmkt_source,\nmkt_campaign,\ndomain_userid,\ndomain_sessionidx,\nuser_fingerprint || '-' || user_ipaddress AS browser_fingerprint,\nuseragent,\nmin(collector_tstamp) as visit_start_ts\nFROM\natomic.events\nWHERE\nmkt_medium = 'video' OR mkt_medium = 'display'\nGROUP BY\n1,2,3,4,5,6,7\nORDER BY\n6, 4;\nEven the refr_* fields are null, so would that mean we'd need to consider updating this part of the referrer_basic view to look for NOT NULL mkt_medium?:\nWHERE\n    (\n        (\n            (events.refr_medium) :: TEXT <> 'internal' :: TEXT\n        )\n        AND (\n            events.refr_medium IS NOT NULL\n        )\n    )\nThe only way I could see the referrer was to change the logic like so:\nWHERE\n    (\n        (\n            (\n                (events.refr_medium) :: TEXT <> 'internal' :: TEXT\n            )\n            OR (events.mkt_medium IS NOT NULL)\n        )\n        AND (\n            (\n                events.refr_medium IS NOT NULL\n            )\n            OR (events.mkt_medium IS NOT NULL)\n        )\n    )\nSeems like a messy fix and not sure what unintended consequences this might have.\n. Try this handy link for a list of releases without digging through the\nblog.\nhttp://snowplowanalytics.com/categories.html#releases\nOn Fri, 26 Jun 2015 09:20 Eric Zimmerman notifications@github.com wrote:\n\nI am about to start the daunting task of migrating from 0.9.1 to the\nlastest so the AWS emr service will keep running :-( Something like this\nwould be super helpful. Even a page with all of the release blogs posts\nwould give me a starting point.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1136#issuecomment-115428160.\n. Nice! Take a look at Stack Lead - we use them on our site. \n\nThey're probably a better candidate for web hooks but they have a nice and easy API, and they're considerably cheaper for low volumes: https://stacklead.com/dev\n. Hah - that would explain the silence!\n. Adding to this thread, migrating our table from the OOB encoding to Redshift's documented encoding, we saw a ~61% saving in space.\nHappy to submit a pull request for the migration script. Not sure if you guys have done some work internally which serves the community as a whole better though.\n@danielzohar - is your encoding similar to mine?:\nhttps://groups.google.com/forum/#!topic/snowplow-user/0pGdjV-vjYQ\n. Ah of course. No worries @alexanderdean ...\n. There you go Ben. Hope that helps.... Oops, yes of course.. ",
    "moncaubeig": "See also http://trac.webkit.org/wiki/Fingerprinting\nand proposed implementation \nhttps://github.com/carlo/jquery-browser-fingerprint/blob/master/src/jquery.browser-fingerprint-1.1.js\n. ",
    "ryanrozich": "I have a question about this feature, not sure if this is the right place to ask it - feel free to point me to another venue.\nWe have just started looking at switching to Snowplow for our web analytics. So far I've been very impressed. We have just loaded our first batch of about a days worth of data into redshift. We are running snowplow on a single website (domain) at the moment.\nIn comparing the number of distinct user_fingerprint values (in redshift) to the number of distinct domain_userid values, we noticed much higher counts of domain ids than user fingerprints. For example: we are seeing about 102K distinct domain ids and 35.6K distinct user fingerprint values (or about 2.87 domain ids per user fingerprint on average). So a couple questions:\n(1) Is it safe to say that for a days worth of event data measured from events coming from a single domain that the difference in fingerprints and domain IDs represent \"collisions\", where multiple users (browser-installs) hash to the same fingerprint, or is there another explanation for the difference?\n(2) Is this ratio of domain ids to fingerprints about what you would expect?\n. thanks!\n. My first question would be whether the Clojure collector should just work out of the box with HTTP2 and whether there are any code changes required at all. \nSee point #1 in Mark Nottinghams blog post below\nhttps://www.mnot.net/blog/2014/01/30/http2_expectations\nQuote:\n\nThis means HTTP/2 isn\u2019t introducing new methods, changing headers or switching around status codes. In fact, the library that you use for HTTP/1 can be updated to support HTTP/2 without changing any application code.. \n",
    "testower": "Thank you that solved the problem!\n. It's wierd because all the log files are actually processed correctly after my fix.\nMy ruby version is ruby 1.8.7 (2011-06-30 patchlevel 352) [x86_64-linux]\nHere's my bucket list:\n```\nUpdate assets if you want to host the serde and HiveQL yourself\n:assets: s3://snowplow-emr-assets\n:log: s3n://xxx-snowplow-in/log\n:in: s3n://xxx-snowplow-logs\n:processing: s3n://xxx-snowplow-in/processing\n:out: s3n://xxx-snowplow-out/events\n:archive: s3n://xxx-snowplow-archive\n```\n. Glad to help :)\n. Just to be clear, I don't have any subfolders under the processing folder. But I do have one other folder (the log folder) in the same bucket. Like this:\ns3n://some-bucket/processing\ns3n://some-bucket/logs\n. Well that's embarassing. I used the older CSV library instead of the recommended FasterCSV. Nothing wrong with the data structure, it parses just fine.\nThank you for your patience :)\n. ",
    "richo": "I added some documentation for Bundler in the wiki, can you pull from my fork as per #22 ?\n. In fairness, snowplow should use .ruby-version anyway since I don't think it's using any RVM specific features. I'll open a PR in a moment.\n. ",
    "robocoder": "You can probably adapt the qunit javascript tests from https://github.com/piwik/piwik/tree/master/tests/javascript ... it's independent of the Piwik server.  It just requires sqlite and php.\n. ",
    "stoja": "Hi Dean,\nUnfortunately, I haven't given the project a lot of attention since I moved\nthe code to Github.\nI wrote a paper on it soon after but that's about it.\nThat said, I spent a bit of time trying to understand the various\napproaches while I was working on the project.\nUltimately, I used the \"host based\" approach because we wanted to achieve\nour goal of validating our analytics implementation as a bi-product of\nrunning our automated testing.\nAny other approach would have required changes to our automation framework\n/ code so we didn't choose go down this path.\nAs far as alternatives go, as per Simon's post, you could validate the\nactual analytics code on each page. This is certainly a valid approach.\nAs you've suggested, another possible option is that you setup selenium in\na proxy mode and write code behind the scenes to validate any requests\nbeing sent to your omniture analytics beacon. This is essentially the same\nunderlying concept as the \"host based\" approach I use, just that its more\nof a synchronous model as validation is done inline within your automation\ncode (vs offline).\nI hope this helps. Feel free to get in touch if there's anything I can\nassist with.\nRegards,\nSteve\nOn Tue, Dec 11, 2012 at 10:50 AM, Alexander Dean\nnotifications@github.comwrote:\n\nThanks Simon - there's a ton of interesting stuff there. I've left a\ncomment/question on @stoja https://github.com/stoja's blog too and I\nfound his project here:\nhttps://github.com/stoja/WebAnalyticsBeacon\nI wonder if we could we wire a basic custom collector into Selenium, so\nthat we can use Selenium's asserts directly to check that the beacon is\ngetting the right stuff.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/106#issuecomment-11224572.\n. \n",
    "frutik": "wow! that's definitely great idea\nOn 18 August 2013 16:58, Alexander Dean notifications@github.com wrote:\n\n@kingo55 https://github.com/kingo55 what are your thoughts on #308https://github.com/snowplow/snowplow/issues/308\n?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/109#issuecomment-22830902\n.\n\n\nLinkedIn: http://www.linkedin.com/in/frutik\n. my thought was in regard of idea to have rum embedded into snow plow\n. Hi, Alex! Any estimations on release date for  0.8.13?\n. Hi Alex. Thank you for pointing out  on those branches. I've looked on diff between branches and found too much (for me) scala and other internal stuff to manage merge by myself. I should relax and wait for December - no other way :-) \n. I am also curious how it can determine uri for collecting pixel\n. Hi Alexander! Thanks for quick answer.\n. you can close this issue. but one small note regarding documentation - it's quite not obviously, that value of setCollectorCf should be not a full domain on cloudfront.\n. Thx, Yali. Much more clear now :-)\n. Thanks a lot, Alexander. I think it would be useful to have this limitation described in a docs :-)\n. I have upgraded instance to m1.small but still bad luck\nmy config is:\n:aws:\n  :access_key_id: ***\n  :secret_access_key: *****\n:s3:\n  :region: us-east-1\n  :buckets:\n    # Update assets if you want to host the serde and HiveQL yourself\n    :assets: s3://snowplow-emr-assets\n    :log: s3n://bk-processing-logs/\n    :in: s3n://bk-in/ \n    :processing: s3n://bk-processing/\n    :out: s3n://bk-out/events/\n    :archive: s3n://bk-archive/\n:emr:\n  # Can bump the below as EMR upgrades Hadoop\n  :hadoop_version: 1.0.3\n  :placement: us-east-1b\n  :ec2_key_name: sp-aws\n  # Adjust your Hive cluster below\n  :jobflow:\n    :instance_count: 2\n    :master_instance_type: m1.small\n    :slave_instance_type: m1.small\n:etl:\n  :collector_format: cloudfront\n  :continue_on_unexpected_error: false\n  :storage_format: non-hive # Or switch to 'hive' if you're only using Hive for analysis\nCan bump the below as SnowPlow releases new versions\n:snowplow:\n  :serde_version: 0.5.5\n  :hive_hiveql_version: 0.5.6\n  :non_hive_hiveql_version: 0.0.7\nubuntu@ip:~/snowplow/3-etl/emr-etl-runner$ bundle exec bin/snowplow-emr-etl-runner --config config/config.yml\nStaging CloudFront logs...\n  moving files from s3n://bk-in/ to s3n://bk-processing/\n    MOVE bk-in/2013-02-25-20-36-30-B978B765381DB916 -> bk-processing/2013-02-25-20-36-30-B978B765381DB916\n    MOVE bk-in/2013-02-25-20-34-25-BB301E4B4EB6864E -> bk-processing/2013-02-25-20-34-25-BB301E4B4EB6864E\n      +-> bk-processing/2013-02-25-20-36-30-B978B765381DB916\n      x bk-in/2013-02-25-20-36-30-B978B765381DB916\n      +-> bk-processing/2013-02-25-20-34-25-BB301E4B4EB6864E\n      x bk-in/2013-02-25-20-34-25-BB301E4B4EB6864E\nWaiting a minute to allow S3 to settle (eventual consistency)\nInitializing EMR jobflow\n[lot of time - about 6 min]\nEMR jobflow j-3KGAB1C3MNNTD failed, check Amazon logs for details. Data files not archived.\nIn aws emr console job all the time in state STARTING and then in state FAILED\nElasticity - Install Hive   COMPLETED   2013-02-25 22:45 GMT+0200   2013-02-25 22:46 GMT+0200   s3://elasticmapreduce/libs/script-runner/script-runner.jar\nElasticity Hive Step (s3://snowplow-emr-assets/hive/hiveql/non-hive-rolling-etl-0.0.7.q)    FAILED  2013-02-25 22:46 GMT+0200   2013-02-25 22:47 GMT+0200   s3://elasticmapreduce/libs/script-runner/script-runner.jar\nAny suggestions? I have no clue how to debug that.\n. Logging initialized using configuration in file:/home/hadoop/.versions/hive-0.8.1/conf/hive-log4j.properties\nHive history file=/mnt/var/lib/hive_081/tmp/history/hive_job_log_hadoop_201302261032_307299352.txt\nconverting to local s3://snowplow-emr-assets/hive/serdes/snowplow-log-deserializers-0.5.5.jar\nAdded /mnt/var/lib/hive_081/downloaded_resources/snowplow-log-deserializers-0.5.5.jar to class path\nAdded resource: /mnt/var/lib/hive_081/downloaded_resources/snowplow-log-deserializers-0.5.5.jar\nOK\nTime taken: 11.215 seconds\nOK\nTime taken: 1.082 seconds\nOK\nTime taken: 0.988 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks is set to 0 since there's no reduce operator\njava.io.IOException: cannot find dir = s3n://news-processing/2013-02-25-20-42-00-D32F5F45CE4096CB in pathToPartitionInfo: [s3n://news-processing/]\n    at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:291)\n    at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getPartitionDescFromPathRecursively(HiveFileFormatUtils.java:258)\n    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat$CombineHiveInputSplit.(CombineHiveInputFormat.java:108)\n    at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:423)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1044)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1036)\n    at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:174)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:952)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:905)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:905)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:879)\n    at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:482)\n    at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:136)\n    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:133)\n    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)\n    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)\n    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)\n    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)\n    at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:261)\n    at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:218)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:409)\n    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)\n    at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:442)\n    at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)\n    at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:655)\n    at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:567)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:187)\nJob Submission failed with exception 'java.io.IOException(cannot find dir = s3n://news-processing/2013-02-25-20-42-00-D32F5F45CE4096CB in pathToPartitionInfo: [s3n://news-processing/])'\nFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask\nCommand exiting with ret '255'\n. btw, not empty file exists at location s3n://news-processing/2013-02-25-20-42-00-D32F5F45CE4096CB and this is file with first timestamp in a bucket - so, etl script was failed on the first log\n. accidentally closed %)\n. well, in my example was used not real names :-)\nreal ones are:\n:s3:\n  :region: us-east-1\n  :buckets:\n    # Update assets if you want to host the serde and HiveQL yourself\n    :assets: s3://snowplow-emr-assets\n    :log: s3n://news-processing-logs/\n    :in: s3n://news-in/\n    :processing: s3n://news-processing/\n    :out: s3n://news-out/events/\n    :archive: s3n://news-archive/\n. this particular issue was solved. but have next one\nLogging initialized using configuration in file:/home/hadoop/.versions/hive-0.8.1/conf/hive-log4j.properties\nHive history file=/mnt/var/lib/hive_081/tmp/history/hive_job_log_hadoop_201302261142_37460179.txt\nconverting to local s3://snowplow-emr-assets/hive/serdes/snowplow-log-deserializers-0.5.5.jar\nAdded /mnt/var/lib/hive_081/downloaded_resources/snowplow-log-deserializers-0.5.5.jar to class path\nAdded resource: /mnt/var/lib/hive_081/downloaded_resources/snowplow-log-deserializers-0.5.5.jar\nOK\nTime taken: 12.473 seconds\nOK\nTime taken: 0.993 seconds\nOK\nTime taken: 1.141 seconds\nTotal MapReduce jobs = 1\nLaunching Job 1 out of 1\nNumber of reduce tasks is set to 0 since there's no reduce operator\nStarting Job = job_201302261138_0001, Tracking URL = http://ip-10-82-202-23.ec2.internal:9100/jobdetails.jsp?jobid=job_201302261138_0001\nKill Command = /home/hadoop/bin/hadoop job  -Dmapred.job.tracker=10.82.202.23:9001 -kill job_201302261138_0001\nHadoop job information for Stage-1: number of mappers: 1; number of reducers: 0\n2013-02-26 11:43:16,975 Stage-1 map = 0%,  reduce = 0%\n2013-02-26 11:44:11,827 Stage-1 map = 100%,  reduce = 100%\nEnded Job = job_201302261138_0001 with errors\nError during job, obtaining debugging information...\nExamining task ID: task_201302261138_0001_m_000002 (and more) from job job_201302261138_0001\nTask with the most failures(4):\nTask ID:\n  task_201302261138_0001_m_000000\nURL:\nhttp://ip-10-82-202-23.ec2.internal:9100/taskdetails.jsp?jobid=job_201302261138_0001&tipid=task_201302261138_0001_m_000000\nCounters:\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask\nMapReduce Jobs Launched: \nJob 0: Map: 1   HDFS Read: 0 HDFS Write: 0 FAIL\nTotal MapReduce CPU Time Spent: 0 msec\nCommand exiting with ret '255'\n. btw, logfile contains non latin character in utf-8 (just notice)\n. removed\n. definately something related to parsing\n. f_k_ng non latin based languages :-)\n. wow, let me check\n\nHave you gone through our CloudFront collector setup guide?\n\nsure. at least I hope so\n. hmm wrong bucket for logs... but was changing it yesterday to correct one. sorry. will try with correct settings\n. works now. it was mix of logs from cf and s3\n. Hi guys. Do you have any ideas/estimates when that great feature could be implemented?\n. (party)\n. would be good to have not normalized version preserved too\n. also would be nice to add message 'EMR is running' after message 'Starting EMR' because current situation is a bit confusing\n. btw everything was working perfectly before upgrade but now I can't make necessary downgrade :(\n. well, looks like i have some stale stuff in s3 generated by upgraded version of snowplow. will try to make full cleanup...\n. it was definitely my fault. I have made few stupid things :-). first of all i have made checkout of some unkown (head of master at the installtion moment) version of snowlow. then I upgraded it by making pull of some new (and again uknown) head of master.  I think it's kinda good idea to update install manual with different approach of installations process -  with use of stable tags instead of master or via tarballs of stable tags\n. yes. it woks after full cleanup of all previously created artefacts\n. https://github.com/snowplow/snowplow/pull/371\n. I have the same issue\n. ",
    "sdepablos": "@alexanderdean what is the status of the Hadoop ETL. I could a link on the technical documentation https://github.com/snowplow/snowplow/wiki/SnowPlow-technical-documentation but points to a wrong page.\n. Thanks for your answer @alexanderdean Any technical limitation to make it work with Infobright instead of Redshift? I was thinking that for most small sized ecommerces Redshift may be overkill but at the same having information on how the product listings work should be a must for all of them. \n. Good to know you're taking into account all approaches. Regarding the 65k limit I thought that as Redshift is based on Paraccel and AFAIK a true columnar DB, they wouldn't have this kind of limit.\n. I see: \"You might be able to create a very wide table but be unable to insert data into it or select from it.\" :(\n. ",
    "jasonlynes": "This fixed the error I was getting from gem build: \nuninitialized constant SnowPlow::EmrEtlRunner::Config::Set\nthanks!\n. Excuse the wrong place for this question, but do you have any docs on incorporating the ETL runner into an existing Rails app?\n. +1 would love this\nPurpose is to manage the cron in the same application I'm tracking and displaying the stats.\n. +1 thanks\n. ",
    "gsmith85": "@alexanderdean is the 0.8.13 release still on track to release in 10 days?  Can we expect unstructured events along with it?\n. ",
    "ngsmrk": "Hi Alex\nI looked into the cookie secure setting as well and it doesn't seem to make any difference which is odd\nThen again I'm not a Javascript guru and for all I know it may vary based on browser implementation of the W3C standard ...\nMy feeling right now is that the problem is to do with the value of the site we specify when configuring snowplow on different applications and how that is used when the cookies are set - I've got some examples I'l show you on Tuesday.\nP.S. I had a stab at some Jasmine tests and I'll keep you posted.\nP.P.S We did some work using Selenium network traffic capture to test Snowplow integration in our apps (Ruby and Java) which I am going to write a blog post about.\nAngus.\n. The issue is in the getCookieName method in tracker.js - it uses the configTrackerSiteId to build the cookie name.\n. ",
    "GuiSim": "I am not exactly sure what the impacts are but I think that this documentation\nhttps://github.com/snowplow/snowplow/wiki/canonical-event-model#212-date--time-fields\nshould be updated with information regarding these new fields.\n. ",
    "michaelwexler": "This is a classic data warehouse problem, also referred to as slowly changing dimensions.  On one end are things like Gender, which for most people, will not change and at the other end are things that change multiple times a second (rotated images shown, for example, if we are tracking that, or experiences during a game or video).   The traditional design requires that you set a date range of \"applicability\" to a dimension variable, and join by date to your events. Events in that range get that profile variable, out of range either get a null, a default, or whatever else covers that range.  If values both cover Feb, you either a) don't allow it in data model or b) set a hierarchy, so \"high value customer\" and \"customer\" can both be set, but HVC rules over customer in this example.   \nSo, if I was a \"prospect\" in January but bought the product and am now a \"customer\" in the data, by using date ranges, we can query : current customers, what happened in Jan to prospects, what events triggered conversions from people who were not yet customers as of Jan, what prospects in Jan are still prospects in Feb, etc.\nThis can be a lot of work... and slow.  But you'll need to be careful; projecting anything back in infinite time beyond gender and birthdate means you may not be using the data \"that was in effect at that time\".\nSo, allow a few in the \"perm profile\" category which apply to all events for that user, but think through what are good ranges for others: other tools punt and just use session, page, and cookie (user). \n. ",
    "SyedWasiHaider": "Hi @alexanderdean. Would passing a negative value count as a \"refund\" event for transaction/item events or is refund just not supported? (This is in the context of using something like the .NET library for instance). . ",
    "featureless": "To resolve this I added this function :\nvar storageCheck = function(){\ntry {\nwindow.sessionStorage.setItem('History.test',true);\n} catch (e) {\nreturn false;\n}\nwindow.sessionStorage.removeItem('History.test');\nreturn true;\n};\nAnd replaced:\nsessionStorage = window.sessionStorage||false\nby\nsessionStorage = (storageCheck())?(window.sessionStorage||false):false\n. ",
    "chuwy": "Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/7. Related: https://github.com/snowplow/snowplow/issues/3485. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/8. Migrated to https://github.com/snowplow/snowplow/issues/389. Can be closed due https://github.com/snowplow/snowplow/issues/2992. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/11. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/10. Is this still an issue? Having that we're using native Redshift driver now.. Duplicate of https://github.com/snowplow/snowplow/issues/3204. Hey @ganeshsp! Please direct support questions to our forums.. Duplicate of https://github.com/snowplow/snowplow/issues/3204. Hunting for random obsolete issues. \nDuplicate: #881\n. I think this can be closed as we're removing scala-utils and Json Utils don't imply something dependency-free.. This is done.. No more incomplete tests #919 removed and #1960 restored.. Came to this conclusion (and googled same links) while thinking about Sauna extension mechanisms.\n. Test was removed entirely in #2271 . Hey @BenFradet, do you think we can do something about it in near future? Maybe along with https://github.com/snowplow/iglu-central/issues/778\nIt will be quite useful downstream of enrich stage to have more assumptions about the format of an event we'd like to consume. We already had issues with RDB Shredder not properly truncating columns also we have similar problems in Snowflake. We can go around all of them, but I'm wondering if this is a right path and why not to restrict the schema upstream.. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/17. Migrated to https://github.com/snowplow/iglu-central/issues/632. This was not a behavior of StorageLoader in its last version.. Done in https://github.com/snowplow/snowplow/issues/1344\n. I guess, this can be closed.\n. It seems StorageLoader was able to load from enriched good at some point. Closing as this is not the case with RDB Loader.. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/12. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/15. Outdated ticket, I think?. Test now fails only on Travis CI with following exception:\n- [error] ! split output by the schema path\n   - [error]     FlowException: unhandled exception (BaseFlow.java:918)\n   - [error] cascading.flow.BaseFlow.complete(BaseFlow.java:918)\n   - [error] com.twitter.scalding.Job.run(Job.scala:296)\n   - [error] com.twitter.scalding.JobTest.runJob(JobTest.scala:201)\n   - [error] com.twitter.scalding.JobTest.runHadoop(JobTest.scala:155)\n   - [error] com.snowplowanalytics.snowplow.enrich.hadoop.outputs.CustomPartitionSourceTest$$anonfun$1$$anonfun$apply$1.apply(CustomPartitionSourceTest.scala:69)\n   - [error] com.snowplowanalytics.snowplow.enrich.hadoop.outputs.CustomPartitionSourceTest$$anonfun$1$$anonfun$apply$1.apply(CustomPartitionSourceTest.scala:53)\n   - [error] org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:350)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n   - [error] cascading.flow.hadoop.planner.HadoopFlowStepJob.internalNonBlockingStart(HadoopFlowStepJob.java:108)\n   - [error] cascading.flow.planner.FlowStepJob.blockOnJob(FlowStepJob.java:236)\n   - [error] cascading.flow.planner.FlowStepJob.start(FlowStepJob.java:162)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:124)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:43)\n   - [error] org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:350)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n   - [error] cascading.flow.hadoop.planner.HadoopFlowStepJob.internalNonBlockingStart(HadoopFlowStepJob.java:108)\n   - [error] cascading.flow.planner.FlowStepJob.blockOnJob(FlowStepJob.java:236)\n   - [error] cascading.flow.planner.FlowStepJob.start(FlowStepJob.java:162)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:124)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:43)\n   - [error] short-hostname: short-hostname\n   - [error] org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:350)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n   - [error] cascading.flow.hadoop.planner.HadoopFlowStepJob.internalNonBlockingStart(HadoopFlowStepJob.java:108)\n   - [error] cascading.flow.planner.FlowStepJob.blockOnJob(FlowStepJob.java:236)\n   - [error] cascading.flow.planner.FlowStepJob.start(FlowStepJob.java:162)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:124)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:43)\n   - [error] short-hostname\n   - [error] org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:350)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n   - [error] org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:562)\n   - [error] org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:557)\n   - [error] javax.security.auth.Subject.doAs(Subject.java:415)\n   - [error] org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:557)\n   - [error] org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:548)\n   - [error] cascading.flow.hadoop.planner.HadoopFlowStepJob.internalNonBlockingStart(HadoopFlowStepJob.java:108)\n   - [error] cascading.flow.planner.FlowStepJob.blockOnJob(FlowStepJob.java:236)\n   - [error] cascading.flow.planner.FlowStepJob.start(FlowStepJob.java:162)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:124)\n   - [error] cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:43)\nI'm bit suspicious about these lines:\nshort-hostname: short-hostname\nThis is host name on Travis CI/CD set to fix #2447, which is not relevant any more. I'll remove this line and if this is gonna work - I'll move https://github.com/snowplow/snowplow/issues/2674 to current milestone.. Closed in favor of individual tickets. For me it seems pretty nice. The only thing I can notice is that every particular API probably need specific cache, for it. For example we can't always use just a uri as key, but need more sophisticated hash function.\n. If this data source would be our weather enrichment, we have to round some parameters (latitude, longitude, timestamp to begin of the day) to find a right bucket.\n. Ok. Anyway it would be great if we could describe simple hash functions in some declarative way.\n. Thought about future of API Lookup feature: our next API could be GraphQL.\n. Just update. I was thinking about what strategy we need to choose for URL encoding. We have two main options:\n- use java.net.URLEncode everywhere (per-input basis). Disadvantage is that it encodes too much, ':' as example\n- just replace spaces with '+'. I'm sure spaces will be most often invalid characters, everything else could be passed as is (other invalid characters will fail event enrichments)\nI decided to pick first one, because it's probably API maintainer responsibility to desanitize URL input properly. In akka-http client 1.0 default behavior was to desanitize URL in request even if it was percent-encoded and leave encoded only invalid chars like spaces. Don't know whether it was intentional or not, but now we need to decide how to encode it properly.\n. Yes, input keys.\n. I think it often will be the case. Do we want implement it now?\n. For the second instance I think proper name would be \"instance_identity_document\". It refers to URL where it can be found (/dynamic/instance-identity/document).\n. There three more endpoints: signature, dsa2048 and pkcs7. All with meaningless keys (or some other base64).\n. Can we see any akka output? If it can't connect to the endpoint it logging errors.\n. Can we see any akka output? If it can't connect to the endpoint it logging errors.\n. Leaving here everything I collected so far.\nThis tutorial assumes it's your first installation and you probably want just to checkout platform,\nthus many steps describe low-performance and unsecure installation. On real-world you may want to fix that.\nPrepare your system\nBefore get started you need to have:\n- Account on Amazon Web Services\n- Installed AWS CLI\n- IAM user, first one need to be created in AWS Console\n- IAM user need to have attached AdministratorAccess (permissions tab)\n- Configured credentials on your local machine. you can use aws configure for it\n- For some steps you may want to install jq. It's optional, but handy.\nEverything else can be done from CLI.\nSetting up EC2 instance for EmrEtlRunner/StorageLoader\nIn the end of this step you'll have an AWS EC2 instance, SSH access to it and key on local-machine. Nothing snowplow-specific though.\n1. Find your Default VPC ID\nWe will refer to it as {{ VPC_ID }}\nbash\n$ aws ec2 describe-vpcs | jq -r \".Vpcs[0].VpcId\"\n2. Create Security Group for SSH access\nTODO: For demo purpose probably would be better to create a single SG for all full-access (SSH, Redshift, etc). Also, investigate something more strict..\nOn output you'll get GroupId, we will refer to it as {{ SSH_SG }}.\nbash\n$ aws ec2 create-security-group \\\n    --group-name \"EC2 SSH full access\" \\\n    --description \"Unsafe. Use for demonstration only\" \\\n    --vpc-id {{ VPC_ID }} \\\n    | jq -r '.GroupId'\n3. Add rule allowing SSH access from anywhere\nbash\n$ aws ec2 authorize-security-group-ingress \\\n    --group-id {{ SSH_SG }} \\\n    --protocol tcp \\\n    --port 22 \\\n    --cidr 0.0.0.0/0\n4. Create SSH key-pair named snowplow-ec2 on local machine:\nbash\n$ aws ec2 create-key-pair --key-name snowplow-ec2 \\\n    | jq -r \".KeyMaterial\" > ~/.ssh/snowplow-ec2.pem\n$ chmod go-rwx ~/.ssh/snowplow-ec2.pem\n5. Run t2.small instance with Amazon Linux AMI with previously created SSH-key:\nOn output you will get your instance id, we will refer to it as {{ INSTANCE_ID }}\nbash\n$ aws ec2 run-instances \\\n    --image-id ami-60b6c60a \\\n    --count 1 \\\n    --instance-type t2.small \\\n    --key-name snowplow-ec2 \\\n    | jq -r '.Instances[0].InstanceId'\n6. Attach security group to Instance\nbash\n$ aws ec2 modify-instance-attribute \\\n    --instance-id {{ INSTANCE_ID }} \\\n    --groups {{ SSH_SG }}\n7. Check public IP-address of newly created Instance:\nFurther we will refer to it as {{ PUBLIC_IP }}\nbash\n$ aws ec2 describe-instances \\\n    --instance-ids {{ INSTANCE_ID }} \\\n    | jq '.Reservations[0].Instances[0].PublicDnsName'\n8. Log-in\nFill-in {{ PUBLIC_IP }} from previous step.\nbash\n$ ssh -i ~/.ssh/aws-ec2.pem ec2-user@{{ PUBLIC_IP }}\n. #### Create Redshift cluster\nIn the end of this step you'll have running Redshift cluster. Your next step will be to create tables for your data and run StorageLoader.\n1. Create Redshift cluster\nbash\n$ aws redshift create-cluster \\\n    --node-type dc1.large \\\n    --cluster-type single-node \\\n    --cluster-identifier snowplow \\\n    --db-name pbz \\\n    --master-username admin \\\n    --master-user-password TopSecret1\nThis command may take some time, if you'll get some errors on further steps, that probably because cluster isn't running yet.\n2. Create security group\nOn output you'll get GroupId, we'll refer to it as {{ REDSHIFT_SG }}.\nbash\n$ aws ec2 create-security-group \\\n    --group-name \"Redshift unimited access\" \\\n    --description \"Unsafe. Use for demonstration only\" \\\n    --vpc-id {{ VPC_ID }} \\\n    | jq -r '.GroupId'\n3. Add access rules to new security group\nbash\n$ aws ec2 authorize-security-group-ingress \\\n    --group-id {{ REDSHIFT_SG }} \\\n    --protocol tcp \\\n    --port 5439 \\\n    --cidr 0.0.0.0/0\n4. Tie previously created security to our cluster\nOn output you'll get cluster address, we'll refer to it as {{ REDSHIFT_ADDRESS }}\nbash\n$ aws redshift modify-cluster \\\n    --cluster-id snowplow \\\n    --vpc-security-group-ids {{ REDSHIFT_SG }} \\\n    | jq -r '.Cluster.Endpoint.Address'\n5. Connect\nbash\n$ psql -h {{ REDSHIFT_ADDRESS }} -U admin -d pbz -p 5439\n. Hey @ninjabear! I glad it helped someone and extremely appreciate your addition (since I haven't chance to test it meticulously). Hope someday it will evolve to full setup tutorial.\nAs for stack name, I'm pretty surprised they do not allow it. I'll need to update that most recent stack can be fetched with following:\nbash\n$ aws elasticbeanstalk list-available-solution-stacks \\ \n    | jq -r '.SolutionStacks[]' \\\n    | grep \"Tomcat 8 Java 8\" \\ \n    | head -n 1\n. Can we rephrase the title so it could be in correspondence with commit message and CHANGELOG?\n. Thanks! I couldn't come up with how to describe a commit in correspondence with previous title :)\n. Thanks for details, Fred. I thought about additional dimensions a lot. One more idea about it (probably overengineering though) is that in some places on Earth weather is extremely stable, so for example we probably don't need to do requests for weather on Middle East even for 50km/3hrs difference.\n. Very good point. Does it have significant drawbacks for us?\n. I actually noticed many times while working with OWM that it could give you slightly different results for same request, so context for one event could be different anyway.\nOn the other hand there's a chance it affects only current weather requests (I haven't fetched history yet).\n. @alexanderdean nope.. Yes, it uses plain strings.. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/18. One of quick possible solutions is to use arrays in JS and API lookups configuration JSONs.\nOr it could be objects with arbitrary keys:\njs\n{\n\"my_company_api_lookup\": {}, // usual API lookup configuration\n\"acme_api_lookup\": {}        // another usual API lookup configuration\n}\nThat could help us to express dependencies between enrichments.\n. Should be fixed in RDB Loader. Closing.. I also remember at least one case on discourse where preceding JS enrichment could be desired and one opposite (cannot find a link). \nSo, this is really should be up to user, we cannot decide for him upfront. Ideally user should have an ability to build a dependency graph of enrichments. See https://github.com/snowplow/snowplow/issues/2260. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/14. Got it.\n. I think we need to add to notes in query section that it is on user's behalf to surround placeholders with quotes. We cannot determine type of input in runtime.\nTaking it in account, I don't think it really makes a lot of sense to use question marks as placeholders rather than template-keys (as in API Request enrichment). AFAIK it's common practice because of tools like Java's PreparedStatement, but it's useless in our case and template-keys much easier to implement correctly and safer to use.\n. Hey @alexanderdean,\nHere's a short list of proposals for configuration. Actually, only first one is really desirable for me, last two are just something we can think about.\n- I think expectedRows should really belong to output. I need to pass Query#expectedRows to methods in Output to get it work. Instead it could be just inner property of Output class. I think it could be easier to understand from user perspective as well.\n- If we remove expectedRows from Query it will be just case class Query(sql: String), no more methods, no more properties, just a sql which I passing between functions. Not a problem, but looks not very elegant.  May be we can look at db object as at abstract Source of data with access-configuration and query (as we do for ApiRequestEnrichment's api object).  Or it could be a separate root-level string key.\n- I also think we can get rid of propertyName. SQL has AS keyword which allows us to set any column name we want (although user will need to embrace in double-quotes to preserve case)\n. Everything makes sense for me.\n. Hi @RyanCodes. No, unfortunately it hasn't been implemented. But we just didn't have time for it, if anyone is willing to make a PR - it will be welcomed.. @RyanCodes you can look at our contributing guide as a general recommendation. If you're looking something more technical then you can have a look at SQL Query enrichment and API Request enrichment implementations, which should be very similar to DynamoDB (although I believe DynamoDB should be more straightforward).. @alexanderdean I also wrote a mock HTTP server  in Python for integration tests. Where can I put it?\n. Or https://github.com/sangria-graphql/\n. Closing in favor of: https://github.com/snowplow/snowplow/issues/2327\n. Closing in favor of: https://github.com/snowplow/snowplow/issues/2326\n. Done.\n. @khalidjaz just remove please unused import (BasicHeaderValueParser), format config file in according with other enrichments (cookie extractor config has different formatting) and squash commits into one. Everything else looks good. Thank you!\n. Closing in favor of: https://github.com/snowplow/snowplow/issues/1373\n. Sure, I'll have a look. \n. This is not the case anymore in joda-money 0.11: https://github.com/JodaOrg/joda-money/issues/63\nMay be if for some reason OER give us wrong rate, exception here can be desirable?\n. This can be pushed back and left as a reminder if we're not going to bump Iglu client in this version.\n. @alexanderdean I guess this should be pushed back as Iglu Client in Stream Enrich is still 0.3.1 therefore doesn't support resolver 1-0-1.\n. @alexanderdean I just realized that config/iglu_resolver.json can be updated to 1-0-1 too. On the other hand, may be we should wait until Iglu Client in Stream Enrich will be updated to not confuse users with different SchemaVers.\n. Oh. And also resolver sample in SE will be removed https://github.com/snowplow/snowplow/issues/1932\n. @alexanderdean I afraid I cannot, since it has been deleted (#1932). Or do you want me insert it before commit with delete?\n. We need to warn users to check if they really have 4 CPUs. If we're setting up more CPUs than computer actually has performance degrades so much I nearly can feel CPU is switching contexts.\n. @alexanderdean done. Sorry it took so long. I also haven't added a CHANGELOG. Should I create a separate commit for it?\n. Btw, I think we need to pay some attention to this one: https://github.com/snowplow/snowplow/pull/2483/commits/be72f3f7e170df0b5d0c2bdfcdc0a2c8bd729825\nI don't have much background about what happening there.\n. No. It actually stores error in cache, but it will always return this failure, not trying make request again.\n. Good idea.\njson\n{\n    \"api\": {\n      \"http\": {\n        \"method\": \"GET\",\n        \"retries\": {\n          \"5xx\": 5,\n          \"4xx\": 0\n        },\n        \"uri\": \"http://api.acme.com/users/{{client}}/{{user}}?format=json\",\n        \"timeout\": 5000,\n        \"authentication\": {}\n      }\n    }\n}\n@alexanderdean what do you think?\n. @alexanderdean I think I'm done with it, but I see com.snowplowanalytics.snowplow.enrich.hadoop.good.RunHadoopSpec is failing with cryptic exception on my vagrant box. I remember I've seen this test failing before, but cannot remember what was wrong that time. It starts to fail only when SCE is set to 23 in dependencies.\n. Is it something known?\n. Here's stacktrace https://gist.github.com/chuwy/92c0abd2e76a8670fd8da9dea90c4f3f\n. @alexanderdean should be fixed now.\n. I think yes https://github.com/snowplow/snowplow/commit/862ecc58ff743774c01a4bb4f79eed150920e07c. This shouldn't belong here anymore. Also assets look correct:\n\nJSONPaths\nSQL\nJSON Schema. It is very likely provision step was interrupted as it takes too long (> 30min for me) and process may seem unresponsive.\n. I just agree this functionality must be implemented in Shredder, we already ported a ticket for that: https://github.com/snowplow/snowplow-rdb-loader/issues/59. I also agree on a way it should be configured: iglu URIs in config.yml, einding up in a common Shredder/Loader config.\n\nI think we can close this.. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/6. Done.\n. Hey @ninjabear! Could you please reprocess com.amazon.aws.lambda/s3_notification_event/jsonschema/1-0-0 with latest Schema Guru? There was a critical bug in schema-ddl 0.3.0 which I guess affected these DDLs.\n. Hey @ninjabear! Thanks, looks good to me.\nCherry-picking it into r82.\n. Fixed in RDB Loader. . @alexanderdean ok, I'll try.\n. Shouldn't we update copyright years? At least in renamed config.hocon.sample?\n. I also saw return keyword in KinesisEnrichApp. It actually quite safe in that place, but there's a very dangerous misconception about it in Scala. I'd remove it everywhere I see it.\n. @alexanderdean @fblundun LGTM! Nothing except minor style remarks.\n. Also, are we going to use infinitive verb form for commit messages?\n. Good to hear. I also saw this article and think this could very good decision. I also remember we talked about it few times.\n. I tend to think it caused by force-push as well. When I added this commit, I mistakenly referenced wrong issue #2604. But last time I ovverrode it with right one (we can see reference above). But I cannot see why in commit log we have wrong reference now. \n. But why my yesterday force-push didnt corrected it back?\n. Do you mean .nonEmpty? It is in Scala stdlib, not scalaz.\n. Missing milestone.. Fully agree. I was tempted to validate contexts in API Request enrichment (this is actually really easy), but decided that doing so per-enrichment basis is wrong.\n. Thank you @0xABAB. This will be fixed in https://github.com/snowplow/snowplow-scala-tracker/issues/34.\n. This will be fixed in https://github.com/snowplow/snowplow-scala-tracker/issues/34.\n. Done!\n. It fixed test. Moving to \"Dedupe\" milestone.. Seems like this bug. But from what I can see in their history - it is fixed in 2.0.2 which is used in API Enrichment. I'll try to reproduce.\n. Looks like it has been really fixed in 2.0.3 http://akka.io/news/2016/01/26/akka-streams-2.0.3-released.html. Should we create separate ticket to bump?\n. Thanks. But I guess, it will be better to bump straight to 2.0.4.\n. Fine by me.. To get instance identity we need to either explicitly call getInstanceContextBlocking which waits for context and if it is unavailable just return None or explicitly call getInstanceContextFuture which switches on automatic context attaching and if Future is faling it crashes tracker.\nSo, in both cases user of tracker needs to explicitly fetch context.\n. So, what I suggest is to switch to first option with Option[Context] which will not return a context if it is not an EC2.\n. Hm, sorry, I was a bit wrong (it was a long time since I worked on that context). In Stream Enrich we're using enableEc2Context which should switch on automatic context attaching and it shouldn't crash app.\n. The only thing I can suggest is that what was posted there is just a warning and not a critical error.\n. We need clarify it and if it's just a warning - this ticket appears to be invalid.\n. No, I think it is not. Scala Tracker didn't crash my application (so it is safe to try enableEc2Context on non-EC2) and Spycomb's problem also lays not in the tracker. \n. Probably typo. I see commons-codec-1.5.\n. Hey @alexanderdean! Just heads up that we need to release Iglu Central to make this test work.\n. Branch has been force-pushed with test in SHE enabled and fixed com.amazon.aws.lambda/s3_notification_event. Please make sure you have last changes.\n. Hey @alexanderdean! @ihortom discovered a bug related to MySQL. Could you please publish rc2?\n. @jbeemster https://github.com/snowplow/snowplow/issues/2716 https://github.com/snowplow/snowplow/issues/2717 these are scheduled for 82.\n. Migrated to https://github.com/snowplow/iglu-central/issues/631.. @alexanderdean can I rephrase #2713 about deletion of file? Or do I need to create new ticket and close that one?\n. @alexanderdean could you please elaborate on lookup table? As I can see we need to delete codec that already installed, not codec shipped with Scala Hadoop Enrich. How can we find version information from SHE?\n. Oh, that should be same version. Sorry, nevermind.\n. Closing in favor of https://github.com/snowplow/snowplow/issues/3092. Will do!. Closing in favor of https://github.com/snowplow/snowplow/issues/3090. Missing milestone.. On a side note, our commonly used approach to SBT configuration needs to be refactored a bit. SBT Build trait has been deprecated in 0.13.12 and likely will be removed in 1.0.\nIt doesn't mean we need to remove all project/Dependencies.scala and project/BuildSettings.scala. It just mean we need to refactor project/ProjectBuild.scala to build.sbt. I think it's good idea also to start to add this build.sbt-tickets to projects.\n. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/5. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/16. Hello @simplesteph,\nCould you please readdress your question to our Discourse forum so other users could find an answer? (Github is only for bugs and features)\nThanks\n. Hello @YuxinChou ,\nCould you please readdress your question to our Discourse forum so other users could find an answer? (Github is only for bugs and features)\nThanks\n. Duplicate https://github.com/snowplow/snowplow/issues/3263. > Can you remind me why we allowed empty custom contexts in 1-0-1\nNo, I'm wondering about it as well. It was long before I joined Snowplow.\nReal question is whether we really think empty contexts is good thing. For me empty contexts make not much sense.\n. Sorry @alexanderdean, I'm not following - what do I need to add?\n. Thanks @shermozle. I rolled back that page until 2.7.0 released (which should be very soon).. Duplicate https://github.com/snowplow/snowplow/issues/2562. Sorry @BenFradet, I need to bring it forward to R88 Milestone, since I need sbt-assembly 0.14.4 to shade dependencies.. Closing in favor #2981 . Whops. Checked wrong branch.. @alexanderdean about overriding existing jars: to overwrite existing asset all following conditions should be truth:\n\nWrong (previous) version in Build.scala (very likely to happen)\nSame wrong version in TRAVIS_TAG (unlikely, but still possible)\noverride: true in release.yml\n\nAll three these bullets make me pretty confident about safety of published instances.. @alexanderdean it seems you pushed branch feature/dedupe which was removed github. Actual ready-for-release branch is release/dedupe.. I cherry-picked data-modeling commits. All except #2929, because there's no commit yet.\nUPD: all data-modeling commits cherry-picked.\n/cc @bogaert . @alexanderdean I need to force-push this branch now with ready release/dedupe. Did you change/add anything except #2963 and #2985?. Ok, cool. Force-pushing!. @alexanderdean done!. Travis failed because OpenWeatherMap didn't respond at time. Hope this is just short outage.. Rebased with missed data-modelling commits.\n/cc @bogaert . Hey @alexanderdean! Can I rebase it one more time? I need to delete event-recovery-ci.bash as part of #2983.. Done.. @alexanderdean do you agree with new ticket name?. Ok, fixed.. Renamed ticket, as code-wise it's just remove of 4-storage/storage-loader.. Yes, \"remove\" sounds more acceptable.. Hey @BenFradet. This is indeed a good idea. \nWe implied it as a part of https://github.com/snowplow/snowplow/issues/2824#issuecomment-235549134. I have a sbt template (for sbt new) which I wanted to propose as a basis for all newly created and existing snowplow projects. It includes a build.sbt and project files based on current our current projects. The only project with build.sbt is sauna, I think you may take it as an example.. @alexanderdean can you create a project in snowplow organization for sbt template I mentioned above (it is incoplete though)?. @alexanderdean currently it called snowplow-scala-skeleton.g8 (g8 is by convention). We can come up with better name taking in account that we probably may want to have two these templates:\n\nfor libs, like SCE, iglu-client etc\nfor apps, like schema-guru, igluctl\n\nThese two types of apps usually have very different configuration, especially for deployment: release manager, sbt assembly, maven publishing etc.. Agree. It is always skeleton/template. Let's call it snowplow-scala-app.g8 and snowplow-scala-lib.g8.. Whoops. Missed it. Will review ASAP.. I think this should be for \"StorageLoader\"?. @BenFradet is it a render signature? I remember there was lot of problems with binary-compatibility in these versions because of render function.\nAlso what dependency uses 3.2.10?. https://github.com/json4s/json4s/issues/212?. Currently most of our libraries use json4s 3.2.11, including Scala Common Enrich (which uses fromJsonNode as well). So, that can be a problem.. @BenFradet short list of our core libs using Json4s 3.2.11:\n- [x] iglu client (directly)\n- [x] scala common enrich (direcly and through iglu client)\n- [x] scala hadoop enrich (through scala common enrich and iglu client)\n- [x] scala tracker (directly)\n- [ ] stream enrich (through scala tracker and scala common enrich)\n- [x] scala hadoop shred (directly, through scala common enrich and iglu client) (not so important though, as we're rewritting it). @BenFradet's another option was to force transition to 2.11. Comparing how many libs we need to downgrade - this is sane alternative.. @BenFradet I have some ongoing work on it. I created a milestone to track progress https://github.com/snowplow/snowplow/milestone/139. @BenFradet please note that this bump breaks tests output (each suite outputs in one line), so fixing  this should be in this ticket as well.. Hey @BenFradet,\n\nI just finished review. This looks absolutely perfect, I have no notes.\nMain important moment for CI/CD is that we need to build a DAG of dependencies in case when both lib (SCE) and app (for example, spark enrich) are included in release. Our algorithm looks like following: for each DAG leafs (spark enrich, spark shred) make publishLocal for all its dependencies. Iff app (spark enrich) tests are fine we need to publish lib (SCE) to central and then publish app (spark enrich). This is approach taken for snowplow/iglu (another multi-repo). I think above algorithm should work with our release manager (build_commands).\n\n/cc @ninjabear @alexanderdean . This is a single missing ticket in R88 CHANGELOG.. @alexanderdean this isn't in CHANGELOG for sure. Do we want to make it R89?. Yep!. @BenFradet just a heads-up. This should be done in same release with https://github.com/snowplow/snowplow/issues/3098 and https://github.com/snowplow/snowplow/issues/3090, as Iglu Client 0.5.0 contains changes that are probably not binary compatible (but source-compatible) with previous versions.. Fixed title (commit message should be fixed as well).. Still need a fix :) \nRelational Database Shredded. This also will have to include #3170 and #3102 added in R88.. Pushed now ^^. Thanks. Cherry-picked.. Closing in favor of https://github.com/snowplow/snowplow/pull/3134. Closing in favor of https://github.com/snowplow/snowplow/pull/3267. Done.. For what I understand from this comment, it should output error-specific message and exit with 0?. Hello @LBirk,\nSorry, but it's not entirely clear what expectations do you have about structured events. It looks like you confuse structured and self-describing events. Structured events are modeled after a Google Analytics event, therefore they have com.google.analytics vendor. These should be not confused with Unstructured (self-describing) events.\nIf my suggestion is right and you need more information about these - you can ask questions on discourse forums and this issue can be closed (Github is for issues only). Please, reopen if I'm wrong.. Done.. @alexanderdean I like it! I'll do a review by the end of this week.. Luckily, just SCE.. @alexanderdean I meant apps such as Scala Hadoop Enrich and Scala Hadoop Shred.. Right. And actually they could never include it.. It's useless for them as all code is identical to 0.24.0. Only one test is changed.. Sorry for confusion.. Thanks @miike. Fixed.. Changed README to plural since it affects all READMEs.. Hey @vceron, we did analysis during QA on few our pipelines which showed us that most of natural duplicates are grouped within 3-6 months - increasing span wouldn't give us much better de-duplication results, at the same time it looked like reducing it doesn't save us much at AWS costs (if your item count doesn't imply manifest populator's count then we were probably wrong). Anyway - we cannot confgiure it right now and I'm 100% agree this is a good idea to make it configurable, please feel free to create an issue in RDB Loader repo (which is a new home for Redshift-specific part of pipeline).. I afraid that if you're really concerned about manifest's size then only one possible solution is to tweak the value in source code and use your own asset.. No, I meant DynamoDB table. We call it \"Event Manifest\". Sorry for confusion.. @vceron yes, you should be able to use vagrant as per our short quickstart guide. \nshell\n host$ git clone https://github.com/snowplow/snowplow.git\n host$ cd snowplow\n host$ git checkout r88-angkor-wat\n host$ vagrant up && vagrant ssh\nguest$ cd /vagrant/3-enrich/scala-hadoop-shred\nguest$ sbt assmebly\nLast command will provide you an asset in scala-hadoop-shred/target/scala-2.10/. You'll have to upload this asset to your own s3.bucket.assets. This bucket should also contain scala-hadoop-enrich and mimic our own hosted assets structure (aws s3 ls s3://snowplow-hosted-assets/).. Cherry-picked into https://github.com/snowplow/snowplow/pull/3133. Thanks @zcei!. Hello @liamtarpey. This indeed an interesting approach and I don't foresee any issues with splitting init script. In the end it repeats all same actions as far as I can see. You can even shorten loadSnowplow further. If you have more questions about what is in init tag - here's prettified version.\nI'm closing this as it's not an issue. If you have any questions - please post on https://discourse.snowplowanalytics.com or file an issue on dedicated JS tracker repository: https://github.com/snowplow/snowplow-javascript-tracker/issues . @alexanderdean do we want to put it into R88?. Renamed ticket as we didn't ever use compupdate.. I think snowplow-nodejs-tracker's problem is that it is actually \"JavaScript (CommonJS) tracker\", and our snowplow-javascript-tracker is in fact \"webbrowser-tracker\". From this perspective it makes some sense to me to advise Node.js Tracker. \nFew more advantages:\n\nIt doesn't use all this extremely weird and hard-to-understand IO logic we have to use in snowplow-javascript-tracker\nIt can be statically typed in more natural manner (snowplow_name_here('somefunc', 'argument') is not most pleasant interface)\nSnowplow JavaScript Tracker contains too much logic that makes sense only in web browsers (such as Optimizely). Maybe other things such as cross-domain linker make sense in React Native?\n\nSo if request/axios is the only problem I'd go in that direction. Especially because it should be extremely simple to fix it. \nBut out of curiosity, @hose314, what exactly the problem with snowplow-javascript-tracker and React Native?. @hose314 snowplow-tracker-core is a separate component, you can use it without snowplow-javascript-tracker if that is helpful. Regarding nodejs tracker - yes, emitters are pretty independent components.. @hose314, yep that's because core and trackers itself reside in same repository, but core is a separate package, while tracker is assembled JS asset. \nI'm ok with moving emitter to axios (@alexanderdean?) if this will make it possible to run it on React Native.. Hi @simplesteph,\nYes we used old so called Build.scala SBT syntax. You can find definitions in project directories. And Intellij IDEA should handle these without problems (at least it does for me).\nAnyway, Build.scala has been deprecated some time ago and we're already switching to build.sbt: https://github.com/snowplow/snowplow/issues/3056, https://github.com/snowplow/snowplow/issues/2972. @simplesteph by top level do you snowplow repository itself? In that case, yes it doesn't work and I suppose it wouldn't even with build.sbt. And that's for purpose. snowplow/snowplow is a monorepo with different projects, many of them are not even Scala, so it wouldn't make much sense to provide single build definition.. I can say so far is that Python tracker's async emitter performance is extremely low. With 4 threads it loads collector with 2 req/sec.. Hey QA-workgroup (@alexanderdean @BenFradet @jbeemster @ihortom)!\nTL;DR I propose a TSV file format that trackers can read to get events and then we can compare Redshift content and these files. \nI implemented small PoC of \"QuickCheck for Snowplow\" in Scala Tracker. It basically consist of:\n\nRandom event generator\nLogic to save  objects to TSV files with format similar to following:pv  chuwy.me/page.html  Main page   http://google.com       ttm:12312313253 where first pv refers to page view and subsequent columns are just tracking functions argument. Having that most of our trackers use same positional argument, I believe this should be quite ubiquitous\nLogic to create emitter and tracker, read these TSV files and flood collector with these events from TSV.\n\nWhat do I propose here? As I said we can re-use this format to see if there's any data-loss on Tracker -> Collector part of pipeline, compare tracker's performance and discover problems.. Sure, why not. If this can be implemented on all our platforms.. Great @BenFradet! I'll start to review it tomorrow. Meanwhile could you please delete scala-hadoop-shred from 3-enrich: https://github.com/snowplow/snowplow/tree/r89/3-enrich. Hi @BenFradet! Code looks really great! But I don't see 1:1 correspondence with issues in miliestone and number of commits in branch (50 vs 41), also this ticket is missing. \nI think when ticket correspondence is done you can add:\n \"Prepared for release\" commit with CHANGELOG and version bumps\n Temporary bump commit where all libs (including dependencies, e.g. SCE in Spark Enrich) bumped to M1 and all jobs bumped to rc1. I worked on this piece few days ago. To match rcs I created Semver class with defined ordering as I found regex approach not-reliable enough and unintuitive, but this is for OLD_SHRED_PATTERN introduced in R89.\nRegarding OLD_ENRICHED_PATTERN - I decided to omit it entirely because I don't think it is possible to use current stack with ancient 0.5.0 version of Hadoop Shred. . Was raised before in https://github.com/snowplow/snowplow/issues/657 and https://github.com/snowplow/snowplow/issues/658. \nClosing old ones, leaving this one as umbrella-ticket.. Moved to https://github.com/snowplow/snowplow-rdb-loader/issues/47, closing this one.. Follow-up for https://github.com/snowplow/snowplow/issues/3154\nHey @BenFradet, can we maybe make date as close to today as possible? Like 2017-05-01T23:56:01.003+00:00, as current expectation will start to fail within 4 next months.. This is invalid. EMR 5.5.0 still provides 1.10.75. Descheduling.. This is quite interesting. I don't know if @BenFradet already realized this, but here's how I see it.\nInputs:\n1. Test expectation doesn't meet result because TSV lengths mismatch. Expectation has 124 fields. Also expectation has ecommerce transaction within JSON, which is not the case anymore - it should be separate TSV columns.\n2. Test isn't running if OEM_KEY isn't set.\n3. We set OEM_KEY on Travis, but it tests only SCE, not Enrich job\n4. Previously @alexanderdean was publishing Enrich job manually\n5. We added CI/CD to Enrich job at R86 Petra, but we didn't publish Enrich since R83, so test still was not running.\nSo, my conclusion is that this test is not valid for a long time, but we didn't notice it because it was never running.. You're right.. Closing, was fixed in RDB Loader.. Thanks @miike. Fixed.. LGTM! I included these tickets in R90 PR. @alexanderdean #3242 and notes from @BenFradet will be implemented in few hours.. @alexanderdean I've fixed run_id at archive_enriched step in rc4. \nIf user runs EER with enrich step - archive_enriched will take predefined run_id and archive this known folder. Otherwise (without enrich step), EER will list folders inside enriched good, take one latest and archive it (same run id for shredded good).\nThis was tested with -x staging,s3distcp,enrich,elasticsearch,archive_raw,rdb_load,shred\n. @alexanderdean I think I figured out what happened with contracts. \nruby\n[4] pry(main)> logger = Logger.new(STDOUT)\n[5] pry(main)> logger.info(\"Program started\")\nI, [2017-07-09T05:15:16.675000 #17425]  INFO -- : Program started\n=> true\nFor some reasons info returns boolean instead of nil. Disservice from ruby contracts.\nI added nil to the end of output_rdb_loader_logs in rc4, so this also must be fixed.. Everything is ready here.. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/19. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/20. Although whole configuration (except AWS creds since rc3) file is passed to RDB Loader, it does not require whole configuration to be present/valid. If config file misses whole aws.emr.jobflow section - RDB Loader doesn't care, it will accept it as valid.\nBut RDB Loader requires config properties from very different parts of config file: aws.s3.buckets.jsonpath_assets, aws.s3.buckets.shredded.good, aws.emr.region, storage.versions, monitoring.snowplow etc. So we could generate separate config file for RDB Loader on-fly.. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/21. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/22. 2-0-0!. Yes, but I want to fix https://github.com/snowplow/snowplow/issues/3322 as part of https://github.com/snowplow/snowplow/issues/3242. I'm double-checking everything right now. . Oh, right. I had vague feeling it already exists. Then I'll rephrase ticket to reflect non-existing features.. Don't know yet, I just discovered this problem.\nI'll try to search for workarounds inside Spark. Otherwise I see just batch-rename for dedupe.. I think this can be used as solution inside Spark https://medium.com/@samuelboyd/spark-reading-gzipped-files-from-s3-without-the-gz-extension-58cc78e6a09d\nWill test it now.. Ok. Fix worked. \n@alexanderdean do you agree this should go into upcoming R90 release as 0.1.1?. Bugfix release https://github.com/snowplow/snowplow/issues/3293. Hello @sandeshputtaraju,\n\nYour collector address should not include http:// protocol. Just x.x.x.x:8080\nIf you're getting 502 error - this is likely collector, not tracker issue\nLatest JS Tracker version is 2.8.0\ncookeDomain should be valid domain\n\nAlso, please send support questions to our forums, github is for bugs and feature requests only.. Fixed in https://github.com/snowplow/snowplow/issues/3242. Closing. Thanks @Eliada! Fixed.. Hi @nishanth6,\nPlease put support requests to our forums. Github is for issues and bugs only.. Actually, let's close this. I performed a test that didn't reveal any suspicious results back then.. Just to clarify what the fix should include:\n\nRDB Logs output go to stderr\nRDB Logs prefixed with correct log level\n\nIs there anything we can improve in message Invalid operation: Cannot COPY into nonexistent table com_snowplowanalytics_snowplow_desktop_context_1?. Migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/23. Re-opening, this is mostly EmrEtlRunner.. Hey @acgray,\nRDB Loader attempts to load JSONPaths assets from s3://snowplow-hosted-assets[-region] only if:\n\nNo assets were found at custom jsonpaths bucket OR\nNo custom assets were specified at all\n\nIs this issue something you encountered in production?\nEdit: getHostedAssetsBucket surrounding context: https://github.com/snowplow/snowplow/blob/8e5e1901af4c9073d0dceda2e2450ea6e6b486bb/4-storage/rdb-loader/src/main/scala/com/snowplowanalytics/snowplow/rdbloader/ShreddedType.scala#L125-L130. Sorry, @acgray, I'm still not following. You still can use aws.s3.buckets.jsonpath_assets setting in config.yml to point RDB Loader to your mirror. If your mirror is identical to snowplow-hosted-assets then Loader won't ever reach our bucket, because custom assets bucket (jsonpath_assets) is always prioritized. This is how StorageLoader worked and I see it's quite consistent with other parts of pipeline. Please, correct me if I missing something. If what I described is not possible now - then it's quite serious issue.\nYou're totally right at this point. We're working on providing more consistent (and format-independent) approach to work with Iglu registries. But so far, there's no clear direction for this problem. I cannot give you any clues or ETA so far to not also give false promises, but you can be sure - we're agree and working on that.. Hey @acgray. I'm moving this into new repository, let's continue discussion there: https://github.com/snowplow/snowplow-rdb-loader/issues/4. Thanks for report @acgray! We will look into it.\nBtw, we now have a dedicated repository for RDB Loader: https://github.com/snowplow/snowplow-rdb-loader, all code and issues will migrate there soon.\nMoving ticket into new repository, closing this one.. Tweaked a title.. Especially useful for AWS-published assets, which can be easily overwritten by anyone including us.. It seems @BenFradet already started doing this.. Ah, so it happens automatically! Sorry, didn't realize. Then leaving this ticket open.. @alexanderdean sorry, I don't remember if I had exact plan on this, but at some point I just realized how unreliable S3 is for storing assets. I'd like solve two problems with  signatures:\n\nVerifying that asset is published by us\nVerifying that asset is built from same git tag it claims it is\n\nI guess possible workflow can be similar to following:\n\nProject owner (@BenFradet) publishes a signed tag (like we're already doing, but using special private GPG key)\nProject owner sings an asset's hash with same private key (snowplow-spark-enrich-1.12.0.jar.crc32.sig)\nPublic GPG key is available somewhere in README or on website or on some reliable medium\nWhenever someone wants to check if asset is real - he/she is decrypting sig file with public key\nSame for git tag\n\nThis however doesn't answer how we can confirm that asset is built from particular tag. Maybe someone who has more experience with GPG can advice here.. Sorry for delay here.\nRegarding code: if we're in recover mode - we don't know exact run id to archive (because timestamp is lost) therefore S3DistCp archives latest found directory; pipeline mode means we're in usual pipeline run and run id is known.\nEmrEtlRunner can decides we're in recovery mode iff archive_enriched is present and enrich is absent.\nDoes it answer a question?. Also, in shred job, we're generating random strings (instead of leaving blank or mark somehow) for event_fingerprint which confuses me even more.. @alexanderdean here in RDB Shredder: https://github.com/snowplow/snowplow-rdb-loader/blob/master/shredder/src/main/scala/com.snowplowanalytics.snowplow.storage/spark/ShredJob.scala#L107-L114\n. Here's a commit adding it: https://github.com/snowplow/snowplow-rdb-loader/commit/788144d1c099c4a3f65165a6b8d4c3cde229f58b#diff-f2fb0e331ce18870d9066074e3a52733. Well, I would like to put in other way round. What's the purpose of having it as an enrichment?\nI mean, do we know any cases where users tried to actually configure it? We're not saving disk space - it's not that big and always present anyway. Having it as mandatory field means it's easier to port deduplication to stream environment. For me it's a field of same important as true_tstamp.. That makes sense. Though fingerprinting logic still exists independently of enrichment (and it is not always satisfactory). \nWhat about leaving fingerprint enrichment as it is now, allowing users to configure it, but change a shredder code substituting random UUID with \"default\" algorithm?. I like it as well! Moving ticket into RDB Loader repo.. Done https://github.com/snowplow/snowplow-rdb-loader/issues/45. /cc @jbeemster . Ok. Closing then. \nThird options also means SSH tunnels can be used with any version of EmrEtlRunner as long as RDB Loader is >= 0.14.. And as far as I know all cookies are present in thrift payload anyway, so we're not overriding anything in-place? If so - I agree.\nWe'll need to make sure there's unit tests for tnuid and add a comment with link to this conversation to tnuid-logic as it'll be matter of archeology and in some way it is already.. > I think this entails adding more logic to eer which is not the direction we want to take right now.\nI agree. Just wanted to raise it, so eventually similar logic (with versions that will be old on that moment) could be implemented in Snowplow CLI.\n\nWhat's wrong with rdb loader failing because it was given wrong targets?\n\nRDB Loader requires roleArn and fails to read configuration when anything is invalid. When it fails to read configuration it fails to instantiate interpreter. When it fails to instantiate interpreter it cannot publish logs in right place and users see just cryptic EER exception in stdout:\nuri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:586:in `run'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:in `send_to'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in `call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:in `block in redefine_method'\n    uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:103:in `run'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:in `send_to'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in `call_with'\n    uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:in `block in redefine_method'\n    uri:classloader:/emr-etl-runner/bin/snowplow-emr-etl-runner:41:in `<main>'\n    org/jruby/RubyKernel.java:979:in `load'\n    uri:classloader:/META-INF/main.rb:1:in `<main>'\n    org/jruby/RubyKernel.java:961:in `require'\n    uri:classloader:/META-INF/main.rb:1:in `(root)'\n    uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_r\nWhich means they have to dig into EMR step logs.. This issue will potentially hit any storage config with schema bump anywhere except addition. For example in 0.14.0 we allowed users to have password in form of:\njson\n\"password\": {\n    \"ec2ParameterStore\": {\n        \"parameterName\": \"snowplow.rdbloader.redshift.password\"\n    }\n}\nIf RDB Loader 0.12/0.13 will encounter this configuration - it will successfully validate it by Iglu, but fail to parse configuration. Though same 2-1-0 with old string-password will be successfully parsed.\nNot trying convince anyone, just describing how broad the problem is.\nSo, it should have follwing logic:\nVersion X of Snowplow CLI has matrix of schema versions supported to all known RDB Loaders up to X release date:\n| Consumer    | Supported Schemas     |\n|-------------|-----------------------|\n| Loader 0.13 | [1-0-?, 1-1-?]        |\n| Loader 0.14 | [1-0-?, 1-1-?, 2-0-?] |\nIf user is trying to pass:\n\n1-0-0 or 1-0-3 to Loader 0.13 - fine.\n1-2-0 to Loader 0.13 - warn that it might cause problems (as with above example)\n2-0-0 to Loader 0.13 - it will cause problems. 2-0-0 cannot be parsed by definition.\n2-1-0 to Loader 0.15 - it might cause problems. We don't know yet if 0.15 support it or too old\n\nThis actually not a EER/CLI logic. Consumer/producer compatiblity should be a part of some core Iglu library and CLI should just use it.. Hi @SarathyIyer, that's a wrong JDK version. Please direct support questions to our forums. Closing this.. I would actually also change \"SnowPlow\" to \"Snowplow\". As we re-discovered recently - cross-batch deduplication is idempotent. When you're shredding event that is already in manifest and has same etl timestamp it will not be marked as duplicate. It means that we can safely re-deduplicate events on spot instances without loosing them.. This can implemented as:\n\nEMR Custom Jar, blocking on UpdateTable. Adds some idle to EMR cluster, ~5-8 minutes, but very safe in terms on unexpected peaks. Can be migrated to Dataflow Runner easily.\n\nAPI call from EmrEtlRunner. Here we cannot predict good timings for update. It will always have a lot of idle, or risk of another throughput exception.. Some real-world examples to calculate throughput.\n\n\nm3.xlarge master and 2x m3.xlarge - up to 1.100 writes\n\nm4.xlarge master and 6x c4.8xlarge - up to 20.000 writes\nr4.xlarge master and 3x r4.xlarge - up to 1.400 writes\n\nMaintaining capacity for second cluster will cost about 10.000 USD per month.. Awesome. Closing.. Wouldn't it be a problem if people start to apply sbt compile only on HEAD and thus HEAD will contain changes (only styling) that should be part of previous commits? To solve this we will have to run sbt compile each time before git commit and on each rebase, which also can be quite problematic.. alias gitc=\"scalafmt && git commit?. Definitely not a blocker. It's a clean-up.. It turns out, modern AMIs still do have commons-codec-1.4.jar. The message appears because commons-codec-1.4.jar appears not immediately after launch and bootstrap script misses it few times, then whenever it is deleted bootstrap script stops to print this warning.\nMy job did not fail with non-deleted commons-codec-1.4.jar, but I assume nothing in enrich/shred used it, so it is safer to leave this script.\nDescheduling this one, though it is partly implemented in https://github.com/snowplow/snowplow/issues/3609. If no credentials nor role is specified we can use default credentials provider chain which will pick up EMR-role.. Regarding style - I'm also all for embracing Scala Style Guide. Not that I like it, but it's better even to have a bad one rather than not having it at all. Also looking forward for your scalafmt commit.. Just a heads-up: we usually use exact version: ...bump to 1.11.0. If version is unknown it should be a placeholder.. Would be good if we can be sure this is reverse of https://github.com/snowplow/iglu/blob/master/0-common/schema-ddl/src/main/scala/com.snowplowanalytics/iglu.schemaddl/StringUtils.scala#L62-L66. Similar thing happened for Iglu recently - build was interrupting silently because of exit(1) in script and error message wasn't in logs. Travis disappoints me these days.. Hello @pawanpatil08. As I mentioned before - we use Github issues for bug-reports and feature-requests only. Please, direct support questions to our forums - this is the right place for asking for help.\nSetup guide is available on our wiki with examples for each particular tracker.\nClosing this.. Actually it seems like a #3436 duplicate, which was fixed in R95. De-scheduling now, but @ihortom please let us know if you remember any details.. Hey @ihortom, this issue is still missing implementation because I still believe it was #3436. Could you please confirm?. Great! I think it was fixed in R95. Closing.. At least in JSON configurations we prefer first approach (even have linter for that). I also think that first approach is more explicit.. Hey @knservis, I'm off today (on road). Will take a look as soon as will get to computer.. Hey @knservis, it just a matter of absence of explicit \"greenlight\". \nNo greenlight == not everything was addressed == peers are continuing to review.. We should be using Iglu Scala Core for this https://github.com/snowplow/iglu-scala-client/issues/20. Thank you @jethron! We've fixed these oversights now.. There's mocks inside scala-weather. API access layer under this enrichment. \nOWM as many other 3rd-party can change format of output data and this spec tests exactly that - we want to be sure that we're on the same page with OWM.\nHowever, I agree that it's not a best approach and weather enrichment is not a special citizen - 3rd party data providers can break adapters as well. I can imagine a separate module with scheduled tests testing all 3rd-party data provider's output in one place. \nSo, there's definitely something we can explore regarding that, but mocks cannot solve the problem, unfortunately.. > In which case the test should not fail when there is a connection timeout. \nIf their service suddenly becomes retired - we will want to know about it as well. This is another goal of this test. \n\nIs the OWM API not versioned\n\nNo, it is not. They claim they're, but also they prove the opposite all the time - I have not much trust for them.. Hello @jethron,\nThank you very much for detailed report. However, I'm not sure I understand how the key with passphrase worked in the end, having that RDB Loader does not allow to use it and \"passpharsed\" key is basically useless without passpharse being applied.\nI think most likely issue was in incorrectly uploaded key. Probably, you tried to copy key in buffer and paste it through AWS Console. This approach corrupts a key by adding special characters in the end. I recommend to upload key via following command:\nbash\n$ aws ssm put-parameter --name \"storage.redshift.tunnelkey\" --type SecureString --value $(cat secret.key)\nWhere $(cat secret.key) ensures that key was uploaded correctly.\nPlease, let us know if this assumption is true - and thanks for reminder, I'm adding it to docs.. I'm closing this, but @jethron, feel free to re-open if I misunderstood something.. I think Hive/Athena Loader has a right to exist in a form similar to our Snowflake Loader.. Isn't it same as https://github.com/snowplow/snowplow/issues/2322?. Whoops! Missed the PR. Pushing back to R103.. To continue discussion...\n\nI don't know if this should go alongside snowplow-ami4-bootstrap-0.2.0.sh, or if we should create a snowplow-ami5-bootstrap-0.3.0.sh and include this in it...\n\nI think this should be a separate script that is not added by EER automatically as it happens with snowplow-ami4-bootstrap-0.2.0.sh now, but user can enable it explicitly in config.yml. In the end this is unnecessary script helpful to only small portion of users with:\n\nRDB Loader\nNAT Gateway\n\nLong enough loading times. Ok, fixing title.. Some more context on this. \n\n\nI've added this as new snowplow-ami5-bootstrap.sh with only this NAT Gateway fix.\n\nAll previous bootstrap fixes are for AMI 3 and 4 and need to be deleted (https://github.com/snowplow/snowplow/issues/3497)\n\nRight now we're adding this manually, but this should be done by EER, created ticket here https://github.com/snowplow/snowplow/issues/3609. Changing plan here a little bit.\n\n\nRenamed this ticket to add AMI 5.x shell script itself. Script is mostly the same as one for AMI 4.x, but a) does not remove Scala 2.11 (as it is unnecessary for modern pipeline); b) adds NAT Gateway fix for RDB Loader (RDB Loader is anyway compatible only with AMI 5.x)\n\nRenamed https://github.com/snowplow/snowplow/issues/3609 to stop launching AMI 4.x script on AMI 5.x and switch to AMI 5.x-specific script in EmrEtlRunner\nDescheduled https://github.com/snowplow/snowplow/issues/3497 for extra-safety (wrong version commons-codec is still present on AMI 5.x). /cc @BenFradet . Hey @alexanderdean, @BenFradet, here's what I came up with regarding this feature. Would like to brain-storm it and change approach if you have objections as it looks like many EER UX decisions were made.\n\nBasically, this commit:\n\nAdds new buckets.enriched.stream path in config.yml \nMakes buckets.raw optional as with this \"stream mode\" buckets.enriched.stream is source of data\nIf buckets.enriched.stream is defined: enrich step doesn't happen at all; instead pipeline starts from special stage-enriched-stream step. staging and archive_raw also never happen.\nSo instead staging,enrich,shred... we now have stage-enriched-stream,shred..., where stage-enriched-stream just copies data to shredder's HDFS directory\n\nThis also affects new RDB Loader in many ways mostly because we cannot use etl_tstamp anymore.. I think we can use this as a master-ticket with EnrichmentRegistry refactoring. When parsing for specific enrichment's configuration it should rely only on schema. \n\nname should be human-readable\nvendor should be dropped altogether\noptional id should be added to identify configuration instance (?)\n\nIt also requires to create tickets in Iglu Central for each configuration.\nIt can be paired with this enrichments refactoring, looks like a good fit.\n. @knservis, nope sorry, will have a look right now.. @knservis nope, greenlight from me!. This can be just DynamoDB table name to avoid exposing credentials on EMR console, but I think it would be better to standardize on same approach as taken in Snowflake Loader and always use secure credentials whenever manifest is used.. Clarified title - we don't pass whole configuration.. > can't deploy rcs \ud83c\udf86 , logged travis-ci/dpl#778\nSame for me, QA is blocked.. That's a broad question, but generally no, AWS Parameter Store is designed for use cases like this, with careful role-management I don't see any shortcomings.\nAs of other platforms - most of them provide similar services, e.g. Google Cloud KMS. In configuration it'll be just:\n{\n  \"salt\": {\n    \"cloudKms\": {\n      \"parameterName\": \"foo\"\n    }\n  }\n}\ninstead of \n{\n  \"salt\": {\n    \"ec2ParameterStore\": {\n      \"parameterName\": \"foo\"\n    }\n  }\n}. @alexanderdean, @knservis I thinks so. It is defined as \"type\": [\"string\", \"object\"], so it is either \"plain value\" or \"reach some service and decrypt this key to get actual secret\".. I think we should make it very explicit in CI/CD process that we want to upload/update those scripts, it looks like a bit scary for me to re-upload them on each EmrEtlRunner release.. Failing tests don't cancel deploy currently.. Got it!. Renamed ticket to reflect the fact that config.yml for stream mode has a lot of differences.. Wondering about it too. Will fix.. Are we also going to avoid rvm implode as advised in that ticket?. Assigning to @BenFradet to merge. Tags we need to push for release:\n- ami5_bootstrap/0.1.0\n- emr/r102-afontova-gora\n- r102-afontova-gora. Looks green enough!. I just hope this is not going to involve JSONPaths.... This is already implemented, starting to test it. So question is for @BenFradet.. Ok, will do.. Turns out fix should (can) be different from one described in title. When we're resuming from shred in batch-enrich mode, S3DistCp knows nothing about run-folder and also uses enriched/good, but with --srcPattern .*part-.*, which seems to handle files from subfolders.  With stream-enrich mode we use  --srcPattern .+, which somehow doesn't handle subfolders in the same way.. Don't understand why, but --srcPattern .*\\.gz did the trick.. Do you mean because we can support other formats in future? AFAIK gz right now is only format for enriched data produced by S3 Loader.. I'll try to test it with --srcPattern .* (which I guess can be different from initial .+), but also not sure if this is more bullet-proof option.. Ok, .* also worked. @BenFradet can you confirm .* seems like a better option for you than .*\\.gz?. Ok, no unnecessarily files were moved. Pushing rc then.. Sorry, I was wrong this regex handles $folder$ files and tries to move them from enriched/good to HDFS for shredding. I cannot find any evidences that S3DistCp ignores empty files, so I think it still would be better to stick with .gz.. Rebased on top of master.. Thanks @BenFradet! I will address feedback re outputCompression and do some more tests.. archive_raw is also invalid in Stream Enrich mode. Significantly less disruptive, but still invalid.. Cherry-picked into https://github.com/snowplow/snowplow/pull/3725. Closing. So, we have to bump it every 1st of May. Nice.. Hi @nicokruger,\nPlease direct support questions to our forum.\nThanks!. > If this is only the sample config\nYes, it is only one line in sample config. You can check other similar commits.\n\nDoes it involve doc changes in the blog and the wiki?\n\nI think it would be nice to briefly mention my post in a section where you describe a JSON Path bug.. Hey @jwhansome, thanks for your report! \nJust to clarify, does the following model describe this bug in a way you observed it?\n\nWall clock: 1st June 0:55AM, enrichment processes an event\nEvent time: 1st June 0:10AM\nRequest: give all historical weather data starting from 1st June 0:00 to 1st June 23:59\nResponse: failure (because data is not historical yet?)\n\nAlso does OWM just return New city string as a response? No actual JSON data?. - [ ] Check same for SQL Query enrichment . Few todo items are missing:\n\n[ ] Rebase\n[ ] Canaries QA (will discuss on 1:1)\n[ ] Prepared for release commit. Also, @rzats, please pay attention - few commits missing their corresponding tickets.. Hello @dnedev,\n\nThis is an interesting issue. I'm sure that Stream Enrich supports HTTPS. All our internal pipelines use https, but not with Iglu Server (Static S3 Registry, instead).\nCould you please show your resolver.json? Very likely that you've missed /api part in the endpoint. If not - could you please try to curl your schema.\nEdit: in fact we use Iglu Server with HTTPS as well.. Probably. Do we have a dedicated milestone for these changes?. Hi @arunma! Not yet, unfortunately - it will be out with R112.. By not containing valid schema, do you mean that it misses iglu: prefix?. Changed title as it seems only Beam Enrich now is affected?. So, basically what we need to do is to copy our entrypoint approach from snowplow-docker https://github.com/snowplow/snowplow-docker/blob/develop/iglu-server/0.4.0/docker-entrypoint.sh. - [ ] Change commit message to reference #3870 . We want it to behave like: SQL Query and API Request enrichments, just adding new derived contexts. Otherwise we need to validated whole event twice: before and after enrichments. With added contexts we're always sure event is not corrupted.. The whole point of raising this issue was that we could be able to guarantee that once enrich step has finished - event is valid and anything downstream can work with it without re-validating. Ability to mutate an event is what makes it much harder to give this sort of guarantee.\nI believe that main use-case of mutation in JS enrichment is fixing bad rows on-fly. And if that is the case then I'm even more sure that JS enrichment shouldn't be able to mutate events and this process should be factored out into bad data recovery (which should be much easier if we have mentioned guarantee). If that is not the case - I'm happy to discuss other ways.. Similar examples:\n- DynamoDB (not implemented)\n- SQL (implemented)\n- HTTP API (implemented). I think \"Elasticity\" (and \"Step\" is just useless) was always most confusing part, which doesn't bear any useful information. Is it possiblt to remove it?. Right now I'm thinking that Tracker just needs to send Thrift payloads, mimicking Stream Collector, but leaving this PR as just reminder of work we've done on hackathon.\n/cc @mhadam . Same here: https://github.com/snowplow-incubator/snowplow-bigquery-loader/issues/10. Closing in favor of https://github.com/snowplow-incubator/snowplow-bigquery-loader/issues/11. We have separate repositories for loaders these days.. I believe this is the reason to use S3DistCp for RDB Shredder. Problem that without HDFS, due weak S3 consistency guarantees, Spark would have to first write data to _temporary S3 directory, then start renaming (which for S3 is copying) all files one by one, which would take way longer.. Almost did the same mistake on BQ Loader.. /cc @colmsnowplow . The ticket is blocked by Apache Common, it does not support IPv6 at the moment https://issues.apache.org/jira/browse/NET-405\nException is thrown here https://github.com/snowplow/iab-spiders-and-robots-java-client/blob/master/src/main/java/com/snowplowanalytics/iab/spidersandrobotsclient/lib/internal/IpRanges.java#L78. Related https://github.com/snowplow/snowplow/issues/3953. Triggered it for you @arunma . I added this check because ran into NullPointerException on Float => JFloat conversion several times.\nscala\nimport java.lang.{ Float => JFloat }\nclass A {\n  @BeanProperty var latitude: JFloat = _\n}\nval a = new A\ndef get(foo: Option[Float]) = 1  // It will raise NPE\nget(Option(a.latitude))\nI don't think there's a big chance above values could be nulls because we control calling code, but overall this check adding safety for me.\n. Well, I'm more concerned not about quickness, but about a case where developer can just forget to set CI=true and miss some important failure, because it was skipped silently. So, yes I think it is a good idea. I'll create one.\n. Yes! I think we can even short-circuit enrichment process if invalid JSON Path was given, instead of getting full enriched/bad bucket.\n. Same typo was in python server.\n. @alexanderdean #2563. We also had a quick talk about it with @ninjabear \n. render is redundant here.\n. Unused import. As well as import java.net.URL, import java.util.Date, ConfigRenderOptions, com.snowplowanalytics.snowplow.scalatracker.Tracker.\n. Redundant parenthesis  around IO(Http).ask(bind)(5.seconds).\n. Maybe Suceess(_) => ()?\n. Redundant braces around errorCode. Or missing around sampleMessage?\n. failurePairs.nonEmpty would look nicer?\n. What is your java -version output @BenFradet? I noticed differences in output when compiled with java 8.. I'd recommend to use our vagrant setup to make sure that our environments are identical.. I like it! Hard-coded length is confusing.. Did we check that all excludings are unnecessary now?. What libraries require these repos? I think we need to move all our dependencies to Maven (and include CI/CD) prior to 2.11 crosspublishing?. Can we refactor contents of this tuple into separate values like this:\nscala\n      .map { line =>\n        val registry = RegistrySingleton.get(igluConfig, enrichConfig.enrichments, enrichConfig.local)\n        val loader = LoaderSingleton.get(inFormat).asInstanceOf[Loader[Any]].toCollectorPayload(line)\n        val event = EtlPipeline.processEvents(registry, etlVersion, enrichConfig.etlTstamp, loader)(ResolverSingleton.get(igluConfig))\n        (line, event)\n      }\nI think it would be more readable even to factor it out as separate function. Basically this small lambda is the heart of Snowplow.\nAlso, I see that registry and Loader[Any] are singletons created once per worker, but is it really necessary to instantiate them straight inside lambda?. line is not used, can be substituted with _. newlines before if \n. Why not to import?. Why MultiInputFormat is imported here?. It seems nothing from implicits is used here.. I think we can omit Scalaz._ import.. It would be safer to enclose procedural ifs into curly braces.. Iglu resolver configuration.. I like this private approach!. Milliseconds?. I'd rephrase it to reflect this is path to local dir.. Separate project.. This can be inferred.. It's okay to put this import to top (and fromJsonNode below). Just out of curiosity - is synchronized really necessary? I mean, are these objects really accessed in multi-threading env?. It seems toDF is part of DataSet class. And it also compiles without this import.. Well, maybe this is matter of taste, but I'd still import things that are used only once at the top of file.. Where all these fog dependencies come from?. Iglu::IgluError usually means it \"parseable\", but invalid.. Can we print this list ({enrich,shred,elasticsearch,archive_raw}) dynamically generated from RESUMABLES? Otherwise it's easy to forget to update all help messages if something being added/changed.. I think if it's not called --schemaver it worth to at least mention SchemaVer in help message.. Same about all other occurrences.. This should be iglucentral.com.. Actually, it's questionable whether it should be hardcoded at all or parsed from resolver config.\nAt one side, Iglu meant to be fully decentralized and I tend to avoid using single point of failure whenever possible. On the other hand, all EmrEtlRunner logic is fully Snowplow-related, so it may be acceptable here.. One more advantage of using resolver here is that we don't need all get_schema_name_from_version/download_as_string logic here, as it is a part of Iglu Ruby Client (Resolver::lookup_schema). I think it's worth to stick to single style of hash-encoding. It either should symbolic keys or string keys everywhere. I see symbols in most other parts.. Valid options can be dymamicaly generated.. Got it.. Does it mean Avro::Schema.validate requires keys to be strings?. Welp. That's ok then.. It's optional. None is what it gets by default replacing it with default profile later. Do we need to make it required?. ls stands for unix ls command.. Ok, we can replace it with bucket.stripPrefix(\"s3://\"). Yes. I just tested it again and see it works. There's also unit tests for filter.. @alexanderdean I think I found a bug! How many runs were in the bucket you tested it in? S3 truncates keys to 1000 in list requests. If there were >1000 then it probably didn't reach specified date at all.. There's no difference between u'location' and 'location' in python. What was location value?. In Python 2:\n```python\n\n\n\n'us-east-1' == unicode('us-east-1')\nTrue\n```\n\n\n\nIn Python 3: NameError: name 'unicode' is not defined\nIt should be fine regardless unicode.. Ah! It's 3s!. Is there a reason why relational database shredder has its own is_release_tag?. Is there a reason why relational database shredder has its own deploy.sh?. I don't think comment is necessary here.\nAlso EMR 5.5.0 is released. Is there a reason to not bump it to 5.5.0 as we're jumping through all these versions anyway.. Just don't forget to bump this to M1 before QA.. I think \"Shred Job\" would fit better than \"Hadoop shred\".. \"Enrich\" instead of \"shred\". Also it's not Hadoop shred anymore. It's just a \"Shred Job\".. 2012-2017. I'm just wondering if it is possible to bump target to Java 8? We have a ticket for this: https://github.com/snowplow/snowplow/issues/2381\nAnd as we're bumping EMR to 5.x.x where Java 8 is default version this seems reasonable for me.. 2.11.11?. Here, same question about Java 8 and Scala 2.11.11.. Why not def loadConfigFrom(args: Array[String]): ValidatedNelMessage[ParsedEnrichJobConfig] = { (single-line) ?. Merge artifact.. Same Java 8/Scala 2.11.11 question.. It seems we're not using ScalazArgs anymore. So test suite and ScalazArgs itself can be deleted.. I'd move these class and trait into EnrichJobConfig as this is generatlly good practice to always left on top-level only entities with same name as file.. Also, my approach with scopt is something like following:\nscala\ncase class Config(option: ParsedString)\nprivate case class RawConfig(option: String)\ndef transform(rawConfig: RawConfig): Validated[Config] = ???\nprivate val rawConfig = RawConfig(\"non-sense value\")\nval parser = new scopt.OptionParser[RawConfig]\nparser.parse(args, rawConfig).map(transform) match {\n  case Some(Success(config)) => ???\n}\nI don't say it's better, but less verbose and less boilerplate.. Here's an example https://github.com/snowplow/snowplow/blob/54c88b8ba0a75747889daaddb79b5a555cc48a44/5-data-modeling/event-manifest-populator/src/main/scala/com/snowplowanalytics/snowplow/eventpopulator/Main.scala#L43-L61. Isn't snake_case always true?. /*with DataTables with ValidationMatchers*/ can be removed.. We can also happily replace hadoop here with spark.. Please fix links according to CommonMark spec. There should be no spaces between brackets.. Replace mention of Scalding below with Spark and update copyright years.. com.snowplowanalytics.snowplow.enrich.hadoop.inputs.EnrichedEventLoader doesn't exist anymore.. Also few notes regarding READMEs:\n\nScala Hadoop Enrich should be replaced with Spark Enrich in 3-enrich/README.md\nScala Hadoop Shred should be removed from 3-enrich/README.md\nRDB Shredder should be added to 4-storage/README.md\nIn SCE README we need to mention that it's used by Spark job and legacy Hadoop.. I think we also need to have cd 3-enrich/scala-common-enrich && sbt publishLocal. Otherwise it'll fail to fetch right dependencies. Same for rdb shredder and all other applications/jobs.. It's only about SCE, Enrich and Shred. But that's totally fine to push them back.. Here spark and common also need to be bumped to rc1 and M1.. This file breaks build:\n\n4-storage/relational-database-shredder:r89 \u276f sbt                                                                                                                        \u2731 \u25fc 17:04:09\n[info] Loading global plugins from /Users/chuwy/.sbt/0.13/plugins\n[info] Loading project definition from /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project\n[info] Updating {file:/Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/}relational-database-shredder-build...\n[warn] Multiple dependencies with the same organization/name but different versions. To avoid conflict, pick one version:\n[warn]  * com.eed3si9n:sbt-assembly:(0.14.3, 0.14.4)\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n[warn] There may be incompatibilities among your library dependencies.\n[warn] Here are some of the libraries that were evicted:\n[warn]  * com.eed3si9n:sbt-assembly:0.14.3 -> 0.14.4\n[warn] Run 'evicted' to see detailed eviction warnings\n[info] Compiling 2 Scala sources to /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/target/scala-2.10/sbt-0.13/classes...\n[error] /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/BuildSettings.scala:49: not found: value sbtassembly\n[error]   import sbtassembly.AssemblyPlugin.autoImport._\n[error]          ^\n[error] /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/BuildSettings.scala:52: not found: value assemblyJarName\n[error]     assemblyJarName in assembly := { name.value + \"-\" + version.value + \".jar\" },\n[error]     ^\n[error] /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/BuildSettings.scala:55: not found: value assemblyExcludedJars\n[error]     assemblyExcludedJars in assembly := {\n[error]     ^\n[error] /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/BuildSettings.scala:71: not found: value assembly\n[error]     test in assembly := {},\n[error]             ^\n[error] /Users/chuwy/workspace/snowplow/4-storage/relational-database-shredder/project/BuildSettings.scala:72: not found: value assemblyMergeStrategy\n[error]     assemblyMergeStrategy in assembly := {\n[error]     ^\n[error] 5 errors found\n[error] (compile:compileIncremental) Compilation failed\nProject loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? q. It didn't - because this was never running. On branch pushes we test only Scala Common Enrich.\nUPD: this was response to thread from previous review.. Could you please group all these issues by project as they grouped in previous releases?. Hey @BenFradet, I just realized we're not running tests in builds. test in assembly := {} should not be in work tree. In .travis.yml we test only SCE. We used to test enrich in shred when publishing them as part of sbt assembly but this line makes them buliding without tests. \nSame for Shred job.. Interesting. But I guess we don't need this duplicate anyway? Newer SBT assembly listed in plugins.sbt.. I think we don't need stream collector?\nHey @jbeemster! @BenFradet is doing CI/CD refactoring and we want to add local publishing for SCE on Travis CI. What do you reckon about above list?. @BenFradet I just double-checked list and, seems you're right, we can add publishLocal to all these repos. Even if we missed something - that's totally not a disaster, this won't break anything.. We never want to catch all exceptions. This should be something similar to \nscala\n  case _ @ (_: NoSuchFieldException | _: IllegalAccessException) => UUID.randomUUID.toString\nBut it is also very important to distinguish between cases where user tries to use non-existing property (must fail config validation long before runtime reaches this code) and when he tries to use null property.. I'm not entirely sure it is going to work. As all JVM fields in Scala are private. Access to them is made only through getters.. This whole if-else expression can be extracted as separate function with tests.. Why not Option[String]?. Would also like to see some tests if this is going to be extracted into separate function.. Sorry, @BenFradet, missed these points.\nNo, this plugin is to avoid type-lambdas, it's not supported by any versions of Scala.. This is private method. And return type is quite long.. Maybe it's worth, but\n\nEventually we will try to fetch all logs from single location\n\nAkka inside Scala tracker likes to write non-sense messages to stderr. Order seems correct to me. Here's ordered steps from real EMR job (minus es, though):\n\n\nstaging\n\nenrich\nElasticity Spark Step: Enrich Raw Events\nElasticity S3DistCp Step: Enriched HDFS -> S3\nElasticity S3DistCp Step: Enriched HDFS _SUCCESS -> S3\nshred\nElasticity Spark Step: Shred Enriched Events\nElasticity S3DistCp Step: Shredded HDFS -> S3\narchive_raw\nElasticity S3DistCp Step: Raw S3 Staging -> S3 Archive\nrdb_load (including analyze)\nElasticity Custom Jar Step: Load AWS Redshift enriched events storage Storage Target\narchive_enrich\nElasticity S3DistCp Step: Enriched S3 -> S3 Enriched Archive\nElasticity S3DistCp Step: Shredded S3 -> S3 Shredded Archive\n. I think we used to refer to this file as \"cluster config\". We can at least mention \"cluster\" in description.. I'm not sure if \"formatted\" is right word here (and everywhere below). Quite hard to parse, to be honest. Is there anything in OptionParse or similar that can help here?. It matches 10.0.0.. Don't we need Spark here?. Can we handle a case where user wants to use environment variables?. Wanted to say that it's safer to use SchemaVer class from Iglu client, but https://github.com/snowplow/iglu-ruby-client/issues/7. Still I don't see anything preventing user from passing --schemaver foobar.. Links in 4-storage/README.md still point to projects in snowplow/snowplow.. Not sure get_enriched_step exists.. I think it is hypothetically possible to have different run ids in shredded and enriched. Therefore latest_run_id must be evaluated for both steps separately (and only if we're in recovery). I successfully compiled and tested collector without even these repos (maybe deps were cached though?).. I find reloading main method slightly confusing.. I think this can be replaced with simple partial function:\n\n``scala\nflatMap {\n  case querystringExtractor(qs) => Some(qs)\n  case _ => None\n}. Should we remove empty params?. Would it be a valid config with both Kinesis and Kafka? Shouldn't it be something coproduct-like?. Is there any particular reason it returnsList, having that all implementations beside of test one returnNil`?. I think vendor, version would be more descriptive. Also, does it handle Iglu adapter, which have more that two segments? (I'm not an expert in adapters at all). \nComment about adapter could be useful here as well.. Can we comment what's the Service's responsibility?. 2013. As far as I know all routes can be val.. Orphan timeout param in above ScalaDoc.. I would move this isDefault || isIam logic to AWSConfig class and add getProvider: AWSCredentialsProvider method to it. My point here is that this is configuration concert and we always would like to fail as soon as possible for invalid configuration before actor system even started.. val in private constructor looks little bit ugly for me. Maybe we should make it usual param and add:\ndef shutdown(): Unit = {\n  executorService.shutdown()\n  executorService.awaitTermination(10000, MILLISECONDS)\n}\nAs this is the only reason executorService is publicly available.. This message duplicates one in require. If stream does not exists it'll immediately fail.. Can be a val.. Here as well - I don't think overloading main is a good idea.. Can we give these values more descriptive names?. Let's import scala.io.Source at the top of file.. File is imported.. Ok. Seems these were cached for me (though quite unexpected since I've never compiled SCE on this laptop). For me they look something like getAppConfig and run.. Sorry for confusion - somehow I thought Iglu Adapter URL looks like http://collector.com/com.vendor/event/jsonschema/1-0-0?user=6353af9b-e288-4cf3-9f1c-b377a9c84dac&name=download&publisher_name=Organic&source=&tracking_id=&ad_unit=\n. Very good point!. I think having GCP somewhere in name would make much more sense.. Hm. I thought this should be matter of snowplow-emr-etl-runner --skip consistency_check. Cannot we right now skip/include vacuum and analyze this way?. Someday this should be even part of storage target JSON configuration. But right now I believe CLI-options are the way to go.. This looks quite wrong for me.. Doesn't seem useful.. Very confusing.. I don't really understand this horizontal alignment.. Maybe this is matter of personal preference, but:\nscala\nfor {\n  schemaUri <- lookupSchema(eventType.some, VendorName, EventSchemaMap)\n  event <- payloadBodyToEvent(bodyMap)\n  mEvent <- mutateMailgunEvent(event)\n} yield NonEmptyList(...)\nLooks much more readable.\nThough, I'd also recommend to use flatMap on validation - it's not supported out of box in scalaz 7.1 nor cats.\n. failNel is deprecated. failureNel should be used instead.. I think pattern-matching would be more readable here.. I thought we don't use override unless we're overriding behavior? In Service this method is empty.. I'd rewrite it as:\nscala\n/** \n  * Transformation of measurement protocol fields into Snowplow-specific\n  * @param keyName new key name to replace one from MP\n  * @param f translation function over value\n  */\ncase class Translation(keyName: String, f: String => Validation[String, FieldType])\nThis is mostly because it took quite some time for me to realize that it transforms both key and value. If I understood its purpose right. \n  . Missed newline.. Also, shouldn't this be something like KvTranslation, while real Translation should be type Translation = String => KvTranslation?. I tend to leave SchemaKey unstringified as long as possible.. Constants should start with capital letter.. Why not String \\/  List[(String, Map[String, FieldType])] or Either?. I think it would be more readable to split this into dedicated values:\n```scala\nval hitData = unstructEventData.get(hitType).map(.translationTable).toSuccess(s\"No matching $vendorName hit type for hit type $hitType\".wrapNel)\nval schema = lookupSchema(hitType.some, vendorName, unstructEventData.mapValues(.schemaUri))\nval contexts = buildContexts(params, contextData, fieldToSchemaMap)\n(hitData |@| schema |@| contexts ...)\n```\nJust do avoid too long lines.\n. Isn't it identical to filter here?. I think it would be more readable to split this into dedicated values:\n```scala\nval hitData = unstructEventData.get(hitType).map(.translationTable).toSuccess(s\"No matching $vendorName hit type for hit type $hitType\".wrapNel)\nval schema = lookupSchema(hitType.some, vendorName, unstructEventData.mapValues(.schemaUri))\nval contexts = buildContexts(params, contextData, fieldToSchemaMap)\n(hitData |@| schema |@| contexts ...)\n```\nJust do avoid too long lines.\n. .flatten?. Ah, forgot about that.. I disagree, but that's not a strong preference, so ok.. 2018!. Does it mean we have to put this commit for each first release in new year?. Yep, I believe it makes sense if property is optional.. Is there any specific order or grouping in these imports?. Either \nscala\nimport scalaz._\nimport Scalaz._\nor one of those.. This will fail to match {\"data\": {}, \"schema\": \"iglu:uri\"}. We don't (and can't) have the requirement of ordered keys in JSON.. traits should never use vals unless there's a strong reason to do so. vals lead to very hard to debug initialization issues:\n```scala\ntrait PII {\n  val fieldName: String\n  val isBrowser = fieldName.startsWith(\"br_\")\n}\nobject BrowserPII extends PII {\n  val fieldName = \"br_useragent\"\n}\nBrowserPII.isBrowser\njava.lang.NullPointerException!!!\n``. Can we have a minimal description of members?.valagain. Why is thisflatMapANDx.success? This is justv.map(applyStrategy)`. This is just:\nscala\ncase (inputField, (f: ((String, String) => ValidatedString), outputField: String)) if outputField == fieldName =>\n  val updatedTransform: TransformFunc = (k, v) => f(k, v).map(applyStrategy)\n   (inputField, (updatedTransform, outputField))\nNo need to use tupled, andThen, strategyApplication (with flatMap) etc. At the same time it's crystal clear that this is nothing more than applying applyStrategy to the result of TransformFunc otherwise purpose of function is buried under implementation details.. Return type TransformMap and description would be useful to understand its purpose.. Also, I don't see why this function is necessary. And even it is then it should be protected, because it is just implementation detail, nothing crucial for application's logic.. Also, if you think that tf has type (String, String) => Validation[String, String], then you're wrong. It has type (String, String) => Validation[String, ?], where ? can be at least (String, String) based on what I know about TransformMap. \nThis is because of type-erasure. Your pattern-matching in fact checks that it is only Function2. Output of this transformation will be very surprising:\nscala\nval f1 = (a: String) => a.toUpperCase()\nval f2 = (a: String) => (a, a)\nval fs: List[Function[String, _]] = List(f1, f2)\nfs.collect {\n  case f: Function[String, (String, String)] => f(\"foo\")\n}\nres0: List[(String, String)] = List(\"FOO\", (\"foo\", \"foo\"))   // compiler is tricked here. .fail is deprecated. .failure should be used.. Can we extract all these 3-4-liners into separate private functions? It will reduce parse size to 6-8 lines.. Again, it will fail to match {\"schemaCriterion\": \"\", \"field\": \"\", \"jsonPath\": \"\"} - object with keys sorted in different order. This is critical.\nscala\nparse(\"\"\"{\"key1\": 1, \"key2\": null}\"\"\") match {\n  case JObject(List((\"key2\", n), (\"key1\", x))) => println(\"should match here\")\n  case JObject(List((\"key1\", n), (\"key2\", x))) => println(\"but matches here\")\n}. Unnecessary parenthesis.. supportedSchema.matches(schemaKey). Why these parameters don't include description. They should either have description or be omitted.. Use json4s' serialization instead.. .leftMap(_.toProcessingMessageNel).. Duplicates com.snowplowanalytics.iglu.client.validation.ProcMsgString#toProcessingMessageNel\n. Missing description.. Why flatMap AND x.success. This is just map.. I thought that it's quite obvious to put under Snowplow imports only Snowplow libraries or all Java-libraries under Java imports. It is a matter of consistency, not something specific to our codebase.. Pii = \"Personally identifiable information\". This should be written at least one somewhere.. Not Snowplow.. Iglu. This is a very unusual way to describe members. Why this is not a ScalaDoc?. Still don't see a return type.. This is much better and cleaner looking for-comprehension.. .failure. Same remark as @BenFradet's - this should be clear on what went wrong.. Missed space.. Why is this not def scramble(input: String): String. Isn't it ScalazJson4sUtils.extract[String](jValue, field)? Especially having that you're transforming its output into Validation later?. Unnecessary parenthesis.. ja == contexts. This is not \"This project\", this is Iglu client.. Missing space.. Constant starts with uppercase letter.. I'm not sure I understand your usage pattern for override modifier. People either use it only when overriding some default behavior (like me) or when overriding anything defined in parent trait/class like @BenFradet, but here I see it randomly.. Type-erasure one is super-important. But readability one is also untouched - it takes several minutes to understand its simple purpose.. supportedSchema in PiiPseudonymizerEnrichment also implements a trait, but doesn't have a modifier. I just thought that parse is method of ParseableEnrichment as well, but turns out modifier is missing only in supportedSchema.. .fail is deprecated in next version of Scalaz.. I believe it should be something like:\nscala\ncase json => s\"PII Configuration: pii field does not include 'pojo' nor 'json' fields. Got: [${compact(json)}]\".failure\nWhen user makes a typo in one of many fields - he doesn't want to revisit whole his configuration and make guesses what exactly went wrong, thus error messages if we have them should always be as explicit as possible.. The good way to start would be to:\n\nUnderstand how it impacts TransformMap's fields like vp, res and others if they return something beside of plain String\nMake it very-very clear in pattern-matching that it does not imply types that are written there now\nWrite unit-tests with those fields\nSubmit an issue, reflecting that we need to refactor TransformMap. This is probably going to be part of much bigger atomic events refactoring, but still worth to note that type-safety problem affects PII Enrichment\nDepending on impact of problem - we'll probably have to define overloaded applyStrategy accepting (String, String) and pattern-match it inside tf or leave it full of warnings:\n\nscala\nf(k, v).map {\n  case s: String => applyStrategyString(s)\n  case p: (String, String) => applyStrategyPair(p)\n}\n6. Make it more readable to so that peers wouldn't have to spend time to parse and re-write it into more readable format\nWhat is more important that this question should have been raised before claiming that all issues were addressed.\n. Feel free to ignore, but I'd like to not make a precedent of pattern-matching JSONs this way. It's quite dangerous if for example junior developer who doesn't know that keys in JObject(keys) is order-sensetive.. s/teh/the. Ideally, we shouldn't deal with all these {\"schema\": \"iglu:...\", \"data\": ...}. These primitives are part of Iglu Core with maximum type-safety, associated parsers and transformations.\nEdit: this is just FYI, not a request for action. Feel free to create issue to replace this in future.. I'm worry that this function will be applied to all fields in JSON.\nscala\nparseJson(\"\"\"{\"foo\": 1, \"nested\": {\"foo\": 2}}\"\"\").transformField {\n  case JField(\"foo\", JInt(x)) => JField(\"bar\", JInt(x + 1))\n}\nJObject(List((\"bar\", JInt(2)), (\"nested\", JObject(List((\"bar\", JInt(3)))))))\nSo, having a context with schema and data keys seems a bit dangerous for me now. As far as I can understand it won't transform anything if structure is not exactly as configured. Still - maybe this is wasteful to apply this recursive function to all fields in JSON?. Some == pure;\nflatMap \u2218\u2009pure == map;. Having recursive nature of transformField it turns out that if we'll get a context like:\n{\n  \"schema\": \"iglu:com.acme/event/jsonschema/1-0-0\",\n  \"data\": {\"schema\": 1, \"data\": 2, \"foo\": 3}\n}\nthen s.extract[String] will fail at runtime? If that is the case (I'd like to have a test for that assumption) then above request about of changing transformWith is even stronger.. scala\ndef transformObject(json: JValue, f: PartialFunction[JField, JField]): JValue = json match {\n  case JObject(fields) =>\n    JObject(fields.map { field => if (f.isDefinedAt(field)) f(field) else field })\n  case other => other\n}. _ == \"data\". s/psudonymizantion/pseudonymization. Unnecessary curly braces.. Weird spacing.. if parsedSchemaMatches. Can we move this to separate set of settings, like BuildSettings.formatting.. Is coursier a part of scalafmt commit? Is it required?. I'm still very hazy on these kind of indentations.. So, as we deleted its single member, is there any other justification for its existence left?. What implications has first option?. Yeah, it seems to enable some kind of reflection-based machinery, but I don't see why it cannot be implemented in less boilerplate and reflection-less way: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/enrichments/EnrichmentRegistry.scala#L330. Do you think it is worth to create an issue to explore?. Typo ApplySt_ar_tegyFn. Also, no need for vals in case classes.. format is unused. Should be _.. format is unused. Should be _.. I think it is generally not a best way to structure your code. Specifically, we have PiiPseudonymizerEnrichment.scala file with class PiiPseudonymizerEnrichment, object PiiPseudonymizerEnrichment and multiple related objects and traits, whereas usually X.scala contains only class X and object X.\nYou can move all types and classes inside PiiPseudonymizerEnrichment or (even better to enrichments.pii package and dedicated files).\nAs a common convention in Scala, if you want to place multiple classes/objects into same file, you should use snake-cased file name as in collectorPayload.scala, but we don't use that often.. Shouldn't it be fieldName instead of compact(jsonField) as in extractPiiScalarField?. Few issues in this function (missed in last review).\nAll errors will be simply concatenated as Strings so we'll get messy \"Something went wrongNot supportedAlso error\" messages. This was missed mostly due greatly over-used flatMap on Validation. You should factor out each part of expression into separate values and avoid using faltMap.\nLike:\n```scala\n  private def extractPiiJsonField(jsonField: JValue): ValidationNel[String, PiiJson] = {\n    val schemaCriterion = extractString(jsonField, \"schemaCriterion\")\n      .flatMap(sc => SchemaCriterion.parse(sc).leftMap(_.getMessage))\n      .toValidationNel\n    val jsonPath = extractString(jsonField, \"jsonPath\") .toValidationNel\n    val mutator = extractString(jsonField, \"field\").flatMap(getJsonMutator).toValidationNel\nval validatedNel = (mutator |@| schemaCriterion |@| jsonPath)(PiiJson.apply)\nvalidatedNel.leftMap(x => s\"Unable to extract PII JSON: ${x.list.mkString(\",\")}\")\n\n}\n``. Can we make these definitions less terse (separate definitions with newlines). It's huge diversity of information for just 10 lines of code.. It' s quite strange too see that scalafmt doesn't align first two lines, then aligns last two. Looks very arbitrary.. I feel extremely bad about putting mutable data structures intocase classes, especially non-private ones. I think this shouldprivate classat least.. Shouldn't it be2012-2018?. I thought we decided to move these into dedicated object?. Yes, it looks good to me. Though I don't think I'm aware of all implications it has.. Sorry, too many PII PRs these days.. Right, I think we're not quite consistent there. I think that copyright years should describe project's years, but happy to stick to any approach.. Just one more alignment that scalafmt handles incorrectly.. I think this can be replaced with justcase \"MD2\" => DigestUtils.md2Hex, also it would probably be better to store it as aMap[String, DigestFunction]. Let's removecasefromcase classas it contains mutable collection.. Can you please clarify howenabled` works here?\nAs far as I can tell, EnrichmentRegistry.buildEnrichmentConfig checks this value before and if it is false then PiiPseudonymizerEnrichment.parse will never be invoked. Am I missing something? Also it looks strange to me that on enabled = false below, we still have an enrichment object, which is just no-op.. Unnecessary curly braces.. Quite strange that we use Scala interpolation everywhere and Java-ish .format in this single place.. I think @BenFradet already raised that it's very hard to read this lisp-ish notation. Can we split this into separate values (and preferably smaller functions).. Yep, seems .success is the problem. You can use case \"MD2\" => { DigestUtils.md2Hex _ }.success or leave as is.. First two lines of for-comprehension. They are not aligned, while everything after = aligned.. Ah, ok. So final case \"MD2\"     => { DigestUtils.md2Hex(_: String) }.success won't be much better than original solution. Up to you.. Yep. That is exactly an argument to not force automated formatters - we're swapping one visual garbage with another.. > In my opinion you probably get better overall results using a formatter\nI never said anything against using a formatter. I said only about forcing formatting.. Could you please remove commented code (or better just uncomment)?. Copyright should be 2012-2018. Redundant curly braces around VendorName. failNel should be failureNel as former is deprecated.. failNel is deprecated. Should be failureNel.. We can wrap only this invocation into try/catch block. And IIRC there was a ready wrapper Common Enrich codebase.. parsed.transformField. \nCan we leave a comment here with link to schema that we're trying to enforce?. I know it wasn't added in this commit, but can we change format to string-interpolation.. newline. I think we can use only Try here and then in the end convert it to \\/.. Also, to be honest, I don't see much value in using \\/ at all. If we already defined disjunction-specific function in package object - we can in the same way define implicit map and flatMap extensions for Either and use it like we're on 2.12 (actually this approach has taken by many project including cats).. Megabyte - MB (all uppercase). newline. Didn't know about this plugin. Do you think we should standardize on it for all code bases? (instead of custom scalify settings). I'd advise against App trait in any application https://www.reddit.com/r/scala/comments/4zvelw/app_trait_vs_main_method/. Option(null) == None. Hey @BenFradet, do we have a function in SCE codebase that parses string into Validation[Something, JValue], just to show @misterpig how ideally it would look like.. Type annotations should be unnecessary.. Also, type annotations should be unnecessary.. Linter inside my IDE usually proposes to change these to a.contains(true) || b.contains(true). Just a nit, not requirement.. So, it wasn't changed in the end?. Yeah, good idea, but I think full region-support is more work than that. We at least first have to ask @jbeemster to create corresponding snowplow-hosted-assets-eu-west-3 bucket.. I actually think about leaving these. They're really first versions with processing manifest and probably will be used for QA and somewhere else for some time. What do you think?. Ok, cool. I'm just glad Gem::Version handles pre-releases properly.. So, article advises to avoid set -e because some apps intentionally return non-zero as success. However, I'm totally sure this isn't the case of any below commands and what is more important - absence of set -e can bite much harder - asset with failed tests will be silently published. But thanks for article, I never actually realized set -e is so dumb.. Thanks, that's lack of Ruby experience, however having that ids is usually array of single element - I don't think we'll save enough CPU cycles. . Just heads-up that we also have stream_config.yml.sample now, but on the other hand spark_enrich there is useless and subject for removal in future https://github.com/snowplow/snowplow/issues/3711 (up to you to bump it or not). Should we change it to NonFatal?. We're not going to delete for production?. Sorry, I just checked if temporary commit exists.. For me it looks like \\/[String, Unit] rather than Boolean - it'll never return false.. This flatMap can be changed to streamExists(client, streamName).leftMap(_.getMessage).ensure(\"Kinesis stream $streamName doesn't exist\")(_ == true). Don't we want to aggregate these? If validate fails on config.out.enriched we'll never know whether config.out.bad is also invalid.. Is this $ redundant?. .fail is deprecated, try to stick with .failure (few more places). throwable.getMessage.failure (quotes and curlies are redundant). I'm kind of surprised we don't have this method defined somewhere.. Shouldn't it be bugfix 1.13.1 release?. If we're using stubs in configuration (s3://non-existent) then we don't want to have it enabled by default because 3-enrich/config/enrichments supposed to contain ready-to-go configs (some are enabled, some are not).. Remove please.. I think we have somewhere parse(json: String): Validation function? Ignore me if we don't.. I'm bit hazy on this exception-throwing .extract. Can we wrap it into Validation?. This can be rewritten as \nscala\nregistry.getIabEnrichment.map { iab =>\n  iab.getIabContext(???)\n}\nTo avoid .map(_.some).\nThis is a nit however. Feel free to ignore.. Missing license header.. We usually add trailing () only to site-effecting Unit methods.. Can we factor out it into a separate function. I feel bad about having such big expression in yield.. We prefer string interpolation over String#format.\nscala\ns\"Exception parsing useragent [$useragent]: [${e.getMessage}]\".failure. Cannot it also be fromTryCatch?. scala\n.flatMap {\n  case Some(u) => ???\n  case None => ???\n}. Good point.. I think this should be uncommented (having that we have a bump-ticket)?. Unbelievable!. Shouldn't we add this also to stream_config.yml.sample?. Does scio repl really give us something?\nAlso don't we need an usual Find out more section?. 1.2.1. I think Redshift never had anything in PATCH except 0, even when only ALTER TABLE was enough. I still think that these versions are preferable. At least if we're going to change something - let's change Postgres.. Just noticed. Why we're adding varchar whereas in DDL it is char?. I actually don't think transaction works for ALTER COLUMN. I'm in doubt as I also don't know what are semantics of these versions (it isn't SchemaVer for sure). But I think yes - let's change postgres.. I just mean that it probably won't be a transaction, e.g. if COMMENT will fail (let's imagine) ALTER COLUMN won't be rolled back.. Okay, I'm actually happy to be wrong here.. We're avoiding to add environment-specific entries to gitignore.. What was the reason for second private function?. Elasticity also has the necessary function https://github.com/rslifka/elasticity/blob/master/lib/elasticity/emr.rb#L124-L135.\nI generally agree that we should prefer the official SDK rather than wrapper, but in sake of consistency I'd not introduce new dependencies.. I think nested if can be nicely replaced with elsif of case statements.. Can we add a short note here that marker is used for pagination.. 2019?. Not sure why do we have this separate _impl function.. Since SCE is a library, I think it makes sense to make it private.. Wrong import.. Strange import again.. ",
    "hzarka": "2 suggestions to throw into the mix:\n- Type suffixes in object keys (delimited by ':' or another character not valid in identifiers) would be more concise than envelope:\njavascript\ndata = {\n    \"tweet_location:geo\": [-88.21337, 40.11041],\n    \"when_clicked:ts\":  \"2012-12-14T20:24:01.123000+00:00\",\n    \"another_date\": (new Date())   // library could add \"ts\" suffix directly if a Date is passed\n}\n- Use UTC unix timestamp instead of ISO8601. It is ~half the size (removing millisecond granularity since it's mostly not useful, but could have distinct datetime and timestamp types to capture the difference):\n``` javascript\n\ntest = function(v) { console.log(v, v.length); }\ntest(JSON.stringify([new Date()]));\n[\"2013-03-27T19:41:46.343Z\"] 28\ntest(JSON.stringify([Math.floor((Date.now()/1000))]));\n[1364413306] 12\n```\n\nWith base64 everything will need to be ETLed to be useful, so might as well be as concise as possible in light of the 2000 character limit. I'd also find it much more straightforward as an end-user to use type suffixes rather than envelope. \n. ",
    "tarsolya": "While I'm all in for using Base64 encoding for ue_pr, it might be a good idea to make it optional. I think we are a bit biased towards heavy payloads in ue_pr, but we will make life unnecessarily harder for those who would fit the URL limit with the current URL-encode method.\n. @alexanderdean @rgabo This should be all what was discussed above, including dates, encoding, etc. Feel free to take it apart, if you find anything you don't like.\n. @alexanderdean Great, testing it already in our stack.\nOne offtopic question, though: do you have some kind of source code formatting guideline? I would like to append vim modelines to the js source files at least, becasue it's pretty messy sorting through commits with all these whitespace changes lying around :)\nI can open a separate issue for this, if you agree.\n. ",
    "mac-r": "s3://snowplow-etl-assets-ajaila is empty... should I place assets there?\n. ",
    "sriv": "+1\n. Is there a reason for ETL not working with Infobright? Can it be patched somehow?\n. ",
    "butlermh": "This can be solved by adding this line to 2-collectos/clojure-collector/project.clj\n: war-resources-path \"war-resources/.ebextensions\"\n. Sorry Alex, I was running on Tomcat on my machine, I missed the crucial detail that they needed to be in .ebextensions, running on local it still needed server.xml on the classpath to define the log format, that's the detail I queried about.\n. After some discussion with weavejester, it looks like modifying lein ring is not a viable solution.\nI have been exploring adding a task to lein to override lein ring's skip-file? behavior to include the resources needed by Elastic beanstalk. This seems possible but I don't have working code yet.\nSee \n- Stackoverflow question about how to extend Leiningen tasks from project.clj\n- short blog post on how to add a project task in Leiningen\n- Robert Hooke plugin used to override leiningen behavior\n- Leiningen docs on writing plugins\n- The lein ring war extension overrides ring behavior and demonstrates how this could work\n. ",
    "mfu0": "We're using a custom IIS/aspnet module to capture snowplow events. The \"i.png\" is a real png file which handing is intercepted by a module to set (or renew) the snowplow cookie. The module then writes cookies and http-headers to the IIS log and we use another module to prepare the IIS W3C log files for sp.\nWe plan to contribute this module (if there is any interest). Originally we planned to use the clojure collector, but it was easier to get a simple aspnet module in production (corportate policy).\n. With some web.config tweaking it was possible to serve an extensionless pixel, so this issue can be closed. \nThanks a lot for quick feedback!\n. Thanks! Can confirm that the fix works with snowplow.js from feature/js-improve. \n. Agree on solution 1. \n. @alexanderdean I understand -- I'll see if it's possible to do something with the struct fields that are currently empty with page_view events. \nFor the Our users page -- I'll have to check and come back to you.\n@yalisassoon \nWe do it in SQL:\nSELECT\npage_from_url,\npage_to_url,\nCOUNT(*) as hits,\nCOUNT(DISTINCT CONCAT(domain_sessionidx, domain_userid)) as visits\n-- date, other dimension etc here\nFROM\nevents\nGROUP BY\npage_from_url, page_to_url\nBuilding a graph with more levels will be more difficult (and much more computationally expensive), but 3-4 levels it should be possible to do in SQL with recursion and/or subqueries.\n. ",
    "ehsmeng": "My sincere apologies, I reported this to the wrong bugtracker. It was meant to go to google tag manager.\nBest regards,\nMarcus\n. ",
    "joshspivey": "Found a proxy detection method https://github.com/feross/detect-proxy\n. ",
    "BenFradet": "done, closing. consolidation is done when moving data from hdfs to s3, closing. consolidation is now done when moving data from hdfs to s3 closing. Here's the brute force way of doing things I came up with:\n\nkeep the original folder structures (no flattening): as a result there wouldn't be any overwrite and we would have to glob the input path of the enrich step\nhandle the --end and --start flags on a per collector format basis\n\nAdvantages:\n- everything is s3distcp\n- fairly generic (if we wish to add another format, we don't have to write and support another script or binary)\nDrawbacks:\n- no renaming, correct me if I'm wrong but since our folder structure is not flattened that wouldn't be an issue\nWould love feedback as I don't know if the file renaming serve other purposes.. The sub-folder structure will be fairly short-lived as it'll only be persisted up to enrich which will have a flat output just like right now.\nI'll create a ticket for removing --end and --start then :+1: .. True, that's something I haven't investigated yet but since it's S3DistCp-based that shouldn't be an issue.. It has become true as well for the ip lookup enrichment 2-0-0 for which the api (scala-maxmind-iplookups) doesn't suppress errors anymore.\nMaybe handling could be set at the enrichment level: suppress / warn / fail. not the same website, closing. There is now, closing. We're moving to docs.snowplowanalytics.com, closing. As we've discussed with @jbeemster and @ihortom this is a huge job in itself.\nOne needs to start from scratch (no permissions at all) and build one's way up from there which is very much time-consuming.\nDe-scheduling.. done in #3945 , closing. done in the wiki, closing. everything moved to 1.8, closing.. doc for the cc is marked as deprecated and refer to the aws doc instead, closing. I'm wondering if it'd make sense to have a SparkStep directly in Elasticity. no the same website, closing. ^^. doesn't apply anymore, closing. df-runner config generations are well-tested and since we're moving in this direction closing.. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/schemas. timestamps will be part of the new bad rows. done. Use of scala-util has been removed as well as ScalazArgs, closing. overhauled in iglu client, closing. overhauled in iglu client, closing. It has been fixed in the doc, closing. closing. Sure, if I can validate the final event :+1: . dupe of snowplow/snowplow-rdb-loader#59, closing. the ssc is already vendor and version aware, closing. done in #3945 , closing. done in https://github.com/snowplow/snowplow/blob/d8f028f533c80196a68f2a61af9d635079ab6852/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/loaders/IpAddressExtractor.scala. dupe of snowplow/iglu-central#57, closing. migrated to snowplow/snowplow-rdb-loader#120. json paths are getting binned. json paths are getting binned. done in loader, closing. migrated to: snowplow/snowplow-elasticsearch-loader#30. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/sql/com.mailchimp. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/jsonpaths/com.mailchimp. moving to docs website, closing. We're planning on removing the sluice dependency and using fog-aws directly which supports iam, I think there was a ticket dedicated to the move but I can't seem to find it.\nHowever, AFAIK elasticity, the ruby wrapper around the emr api we're using inside emr etl runner doesn't provide a way to use iam yet. So there might still be a bit of work there in the medium term.\nHowever, in the longer term, we're planning on moving to dataflow-runner which supports IAM directly.\nThis is for the batch pipeline, the real-time pipeline already supports iam roles.. good idea, however I don't think we'll ever get to it, closing. vagrant removed #3851 . won't do, closing. migrated to snowplow/snowplow-elasticsearch-loader#29. vagrant removed #3851 . closing. done. no cross cloud, see https://github.com/snowplow-incubator/snowplow-bigquery-loader. @duncan \nWe have moved forward with all the changes you proposed in this PR in #3274.\nSorry for the lack of activity on our end and thanks a lot for your contribution anyway!. no cross-cloud. done, closing. moved to docs, closing. done in ssc and se. this links back to optional enrichments, closing. closing. moved to docs, closing. done in https://github.com/snowplow/snowplow/milestone/145. superseded by #3811 . @ferrlin \nUnfortunately, we've made some progress parallel to this in #3274.\nSorry for the lack of activity on our side and thanks a lot for your contribution anyway.. migrated to snowplow/snowplow-rdb-loader#121, closing. done in the wiki, closing. migrated to snowplow/snowplow-elasticsearch-loader#27. migrated to snowplow/snowplow-javascript-tracker#590. duplicate of #2037, closing. closing. this commit is part of #1658, closing. mmh not using vagrant + assemblies are generated on travis, closing. https://www.slf4j.org/api/org/slf4j/impl/SimpleLogger.html. This is done as part of #3137, closing. vagrant is removed #3851 . done, closing. closing. Index files are not necessary, closing. closing. closing. there is cd, closing. migrated to snowplow/snowplow-elasticsearch-loader#25. migrated to: snowplow/snowplow-elasticsearch-loader#24. there is a retrying logic now, closing. hey @andrebrov , we're going to way of metrics you can read more about it in our RFC. We have included a set of metrics in beam-enrich (section \"Implementation\" in the RFC) and we're planning on doing the same for stream-enrich but there is no clear timeline at this time.. we're way past this, closing. Should this be broken down into two commands:\n\nlint resolver\nlint enrichments\n\n?. We could, would we need generate all as well for #3105 ?. follow up #3364 . isn't this just a matter of raising the number of open files on the machine?. yup, maybe. On second thought, I think inspecting the exception and providing a better message is quite clumsy and I don't think it's that useful, wdyt?. they're incoming connections. Descheduling and labelling as won't fix, this error is widely known and it's not worth the trouble.. done, closing. done, closing. done through spark settings, closing. this is done, closing. We are thinking of adopting a new format for the enriched data but we don't have a timeline yet.. migrated to: snowplow/snowplow-elasticsearch-loader#23. done, closing. We're now moving away from emr etl runner to Dataflow Runner so this PR doesn't make a lot of sense anymore, sorry for the lack of follow-up, closing.. done as part of #3067 (to 4.14). Not applicable anymore since the move to Spark, closing. done in https://github.com/snowplow/snowplow-google-analytics-plugin , closing. StorageLoader has been ported to RDB Loader so unfortunately the pull request itself doesn't make sense anymore.\n@chuwy do you think the requirement (skipping loading of some shredded event types through whitelist / blacklist or some other mechanism) still makes sense? If so please log an issue.\nSorry for the lack of follow up and thank you for your contribution, closing.. closing. migrated to: snowplow/snowplow-elasticsearch-loader#10. closing. superseded by #3811 . this is the analytics sdk, closing. running with df-runner, closing. this has been done in #3032. no more vagrant, closing. done in #3136, closing. I think adding names to enrichments as a separate ticket and leaving this one for a real dependency-driven dag is fine :+1: . this has been done in #3033. this has been done, closing. updated the wiki with Josh's review, closing. Looked into the log valve and there is no way to ensure proper url encoding of extracted headers (e.g. referer), renamed and descheduled. closing. done in the wiki, closing. There is no error trap since the move to Spark, closing. it's been kept for backwards-compatibility purposes in eer. >even if 90% of this will go \"for free\" when we move to Dataflow Runner...\nIt depends on how long we want to keep supporting the scalding-based pipeline for.. moving to docs, closing. closing. superseded by #2501, closing. closing. not in the repo, closing. not in the repo, closing. closing. closing. closing. Most of those are irrelevant now, closing. closing. closing. closing. done, closing. Done in #3252. migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/122. moved to docs, closing. migrated to: snowplow/snowplow-elasticsearch-loader#22. done as doNotTrackCookie, closing. woops, the changes don't seem to apply anymore, closing, thanks anyway. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/jsonpaths/com.callrail. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/jsonpaths/com.callrail. moved to docs, closing. done in r92, closing. dupe of https://github.com/snowplow/snowplow-rdb-loader/issues/59. closing. EMR steps have been renamed in #3945 , closing. migrated to: snowplow/snowplow-elasticsearch-loader#21. can always rely on travis or do the manual setup, closing. migrated to: snowplow/snowplow-elasticsearch-loader#20. done, closing. ~ duplicate of snowplow/snowplow-elasticsearch-loader#10. superseded by #3827 . removed in #3851 . migrated to: snowplow/snowplow-elasticsearch-loader#19. can be done through snowplow-event-recovery through partitioning. migrated to snowplow-incubator/snowplow-event-recovery#21. migrated to snowplow-incubator/snowplow-event-recovery#20. @alexanderdean Is this regarding the staging Sluice operations?\nIf so, they should be converted to S3DistCps as per #276 and we won't have control over whether or not the directory was empty.\nWhat do you think?. Since this is back on the menu per #3112, do you think putting the error message is enough?\nAlso, shouldn't we do the same in case of a DirNotEmptyError?. By the way did you mean a log message or a tracker message?. closing as stale, feel free to reopen :+1: . done. migrated to snowplow/snowplow-elasticsearch-loader#17. migrated to snowplow/snowplow-elasticsearch-loader#18. done in iglu-central: https://github.com/snowplow/iglu-central/blob/master/jsonpaths/com.adjust/install_1.json. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/sql/com.adjust. migrated to snowplow-incubator/snowplow-event-recovery#19. @jramos \nUnforunately, this has been taken care of as part of the refactor in #3299, sorry.\nThanks anyway for the contribution!. we're now running consolidation to avoid the issue, closing. moved away from akka-streams, closing. @alexanderdean Is this assuming an enrich run already occurred?. ahah I was about to post this lol. done in #3216. closing. closing. closing. superseded by #3533, closing. will be overhauled in the new bad row format, closing. Speaking of which, could you rebase against master or even better against feature/mv-kinesis-es-sink which is the R92 branch?. 30% additional requests seems like a pretty steep price to pay.\nI'm curious to hear what other people think, @alexanderdean @jbeemster ?. Reimplemented it in https://github.com/snowplow/snowplow/pull/3274/commits/611242a6e939e1e33c5869e26d9567330c1decfe if you want to have a look, closing. https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/jsonpaths/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/jsonpaths/com.mailgun. https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/jsonpaths/com.mailgun. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/sql/com.mailgun. dupe of #1318, closing. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/sql/com.olark. done in iglu-central: https://github.com/snowplow/iglu-central/tree/master/sql/com.olark. If you're talking about stream-enrich it's part of R93 https://github.com/snowplow/snowplow/pull/3274/files#diff-bdb9b4d6b465094cbe068174f6badebbR113. dupe of #2803, closing. Didn't have time to fully investigate, but we're now using the official aws sdk so it shouldn't leak credentials.\nThe example above is definitely taken care of now.. This now pertains to configuration failing validation.. Ok, I'll give it some thoughts as I'm discovering ruby/jruby.. will be fixed by #276. Somewhat related, the 5.x amis support Spark 2.0+ but only built against Scala 2.11.\nAs a result, we're currently stuck with Spark 1.6.1.. created rslifka/elasticity#135. done in elasticity, closing. we have cd now, closing. there is a ticket in release-manager, closing. migrated to snowplow/snowplow-elasticsearch-loader#15. collector.cookie.enabled = true, closing. integrated into #3824 sorry for the huge delay and thank you for your contribution :+1: . closing. there are tickets in the concerned trackers, closing. duplicate of #2928 and #2982 . migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/123, closing. this is done, closing. this is done, closing. links to #3880. The priority has been given to the Scala Stream Collector in #2915 (for which the change is almost out the door) since the Clojure Collector is on its way out as per our RFC.\nI believe we'll get around to making this change to the Clojure Collector too, however the timeline is not clear at this time.. migrated to: snowplow/snowplow-elasticsearch-loader#13. migrated to: snowplow/snowplow-elasticsearch-loader#14. Thanks for the update, PRs are most welcome :+1: . superseded by #3811. moved to docs, closing. done, closing. done, closing. closing. it's already in r92, the move to akka-http should solve it by itself since akka-http is more relaxed than spray as far as ua are concerned from looking at the code. I had an integration test planned for this.. A bit of an update on this: due to the parsing rules having been relaxed in akka-http less user-agents produce warnings.\nHowever, despite the log messages, they are all successfully processed by the collector and further downstream.\nClosing, thanks :+1: .. Can I take care of this since I'm in the middle of a pretty big refactor?. Delaying until we get stream enrich onto beam (and an appropriate runner) where we'll get checkpointing for free. this is done for beam enrich, descheduling. closing. migrated to: snowplow/snowplow-elasticsearch-loader#12. Superseded by #3038. superseded by #3438, closing. duplicate of #3073. migrated to snowplow/snowplow-elasticsearch-loader#11\nwill integrate #3020. Should we move the config to the storage part as well, instead of enrich?. Should we rename the tags as well: https://github.com/snowplow/snowplow/pull/3203/files#diff-354f30a63fb0907d4ad57269548329e3R75 ?. same goes for https://github.com/snowplow/snowplow/blob/78676a1e2497f0169260986f606b89535a08dff5/.travis/release_relational_database_shredder.yml. Updated :+1:. Oh, ok, welp I'll pick commits between branches.. for the spark job:\n- [x] finish the job\n- [x] test\n- [x] doc\n- [x] rm the scalding job\n- [x] rm useless deps\n- [x] move to build.sbt\n- [x] rm typesafe config\n- [x] ci/cd\n- [x] 2.11\n- [x] update deps ~~+ rm BeforeAfterAll~~\n. As we discussed with @chuwy , so far I have an output dir which looks something like:\nshredded/\n\u2002\u2002\u2002\u2002good/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002run=2016-11-26-21-48-42/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002atomic-events/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00000\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002...\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002vendor=com.amce/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002name=event/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002format=jsonschema/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002version=1-0-0/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001-00010\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002name=context/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002format=jsonschema/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002version=1-0-0/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001-00010\ninstead of\nshredded/\n\u2002\u2002\u2002\u2002good/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002run=2016-11-26-21-48-42/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002atomic-events/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00000\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002...\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002com.amce/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002event/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002jsonschema/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u20021-0-0/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001-00010\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002context/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002jsonschema/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u20021-0-0/\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001-00010\n@alexanderdean would that be a problem?. Found another issue compared to the existing when writing the tests, in Spark you can't write to a directory that has already been written to. This is the case when writing shredded events to the parent of the atomic events directory.\nOne way to circumvent this issue would to have the following directory structure:\nshredded/\n    good/\n        run=2016-11-26-21-48-42/\n            atomic-events/\n                part-00000\n                part-00001\n                ...\n            shredded-events/\n                vendor=com.amce/\n                    name=event/\n                    \u2002\u2002\u2002\u2002format=jsonschema/\n                    \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002version=1-0-0/\n                    \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001-00010\n                    name=context/\n                    \u2002\u2002\u2002\u2002format=jsonschema/\n                    \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002version=1-0-0/\n                    \u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002part-00001-00010\nNotice the added shredded_events layer.. created #3044. Since we're upgrading sbt, we might as well convert to a .sbt-based build instead of the now deprecated .scala-based build, what do you guys think?. @chuwy ok thanks, I'll follow the guidelines used in sauna. @chuwy ok thanks, I'll follow the guidelines used in sauna. Done with the port, reviews welcome!. yup. I'm wondering if I shouldn't ditch the typesafe config for simple application arguments given that there are only 4 of them (input, good output, bad output, iglu resolver), what do you think?. Thanks for your review!. updated :+1: . yup. Downgrading to 1.6.1. done :+1: . This isn't a real priority since I'm gonna need to revert back to Spark 2.0.. @chuwy rebumped to spark 2.0.2 :+1: . The move to 2.11 depends on #3062. Need to remove ValidatedNel in favor of ValidatedNelMessage.\nSame goes for Validated -> ValidatedMessage\nAlso remove JsonUtils in favor of sce's. Closing in favor of #3094. It depends on the timeline of the releases: if RDB Loader is released before the Spark port, I'll need to port the commit over to the RDB Loader, otherwise it belongs to StorageLoader yes.. I'll treat #3039, #3035, #3033 and #641 in this one. redacted\n. Welp, I found a nasty compatibility problem between the version of jackson we use compared to Spark's. Hoping both are binary compatible.. Apparently, there is code in the iglu-scala-client depending on features of json4s 3.2.11 which are not part of 3.2.10.. Spark 1.6.1 (the version we have to use in scala 2.10) uses 3.2.10 and since it provides its dependencies itself when launching the job, we have to go back to 3.2.10.. Currently, the problem comes from fromJsonNode/asJsonNode added in 3.2.11.. All that for those two one-liners :sob: . @alexanderdean \n@chuwy and I came up with the following strategy:\n\ndowngrade to json4s 3.2.10 all the libraries mentioned above\nmove everything back to 3.2.11 when we move to scala 2.11 as later versions of Spark use 3.2.11 but are only built against scala 2.11 in EMR\n\nWe'd like your input. Yup, I'm personally leaning towards moving everything to scala 2.11 as it'd simplify things a fair bit.. Ok, should we integrate the move to my milestone? \n. Ok, I'll get on it then.. Should we kill two birds with one stone and move to sbt 0.13.13 also?. Created both rslifka/elasticity#133 and rslifka/elasticity#134. Made a few tests:\n\nSpark enrich + rdbs :heavy_check_mark: \nHadoop enrich + rdbs :heavy_check_mark:\nSpark enrich + Hadoop shred :heavy_check_mark: \nHadoop enrich + Hadoop shred :heavy_check_mark: for good measure\n\nSpark enrich + Hadoop shred doesn't fail but it seems the shred job just copies the output of Spark enrich and that's it, still investigating.. @rgabo Spark will indeed recursively pick up everything if the path ends with /*. Spark enrich + Hadoop shred works!\nI didn't see any shredded types the first time around that's why I reported it as not working but it must have been because they were too slow to appear, it just happened again.. closing in favor of #3203. This might be 6.0.12 after all, cf rslifka/elasticity#135. @chuwy true, I've just pushed it. I still have to split a couple of commits for #3007 and #3008 as I just saw them.. Depends on snowplow/scala-forex#110. @chuwy how would we go about publishing this to central?. closed in favor of #3203 . closing in favor of #3153 and #3624 . Depends on #3062. is this still part of r89?\n. @iaingray nobody is working on this particular issue inside Snowplow at the moment so feel free to have a crack at it and open a PR :+1: . redacted. I'm done with the port, except for a few integration tests dealing with thrift, still looking into that.. All tests are working :+1: . I'll add ci/cd and update to 2.11 with a local sce tomorrow but the code shouldn't change much so a review would be much appreciated :+1: . @chuwy thanks for your review, I think I've taken care of everything. Hey @rgabo, thanks for the kind words.\nI'm in the process of migrating to 2.11, it should land in this PR today or on Monday :+1: .. @alexanderdean reworked the travis scripts if you want to have a look :+1: . Nope, we're also modifying EmrEltRunner so it can run Spark jobs in #3048. No problem, I'll also be doing approximately the same tests once I'm done with EmrEtlRunner :+1: . Yup, I'm currently publishing it locally for the tests.. @alexanderdean Can you share what the hadoop event recovery entry is for?. @rgabo we had a discussion regarding this specific topic with @alexanderdean here: https://github.com/snowplow/snowplow/pull/3077#pullrequestreview-20015413 and it seems that there is no way around DistributedCache for now.. @rgabo I added a few classes yesterday among them Array[EnrichedEvent] cf https://github.com/snowplow/snowplow/pull/3077/files#diff-9e5742dff28d1d166dcb990bcdaa5a79R56\nShould have let you know, sorry.. @rgabo Thanks a lot for reporting it, I'll investigate why the symlink doesn't seem to be created.. @rgabo I was actually thinking about removing the empty checks entirely and write empty files similar to what is done currently in hadoop-{enrich,shred}. @rgabo I pushed a fix for the enrichments, I'll test it today and keep you updated.. doesn't seem to have fixed the issue, I'll try a few other things.. From what I've gathered so far, the .dat file is copied in HDFS but the symlink might not be created.. @rgabo (finally) found a fix that is working on our side, added bonus: it doesn't rely on DistributedCache/HDFS anymore.\nIt'd be cool if you could also try it on your side :+1: . @rgabo I added https://github.com/snowplow/snowplow/pull/3077/files#diff-9e5742dff28d1d166dcb990bcdaa5a79R171 because of this but it's possible that another synlink is created between the deletion and creation.\nI think I'm better off catching this exception and ignoring it or checking if the symlink already exists before creating it.. @rgabo pushed a fix, which I'll try in the afternoon. Weird I haven't encountered those in my tests, I'd look into the corresponding logs to see why they failed.. closing in favor of #3203. this has been done, closing. @acgray \nUnfortunately, we''re moving away from Scalazon since it doesn't seem to be maintained anymore (cf #3300 and #3341). Moreover, the sdk versions will be updated as part of #3274.\nThanks a lot for your contribution anyway!. @vceron Have you had a chance to update to emr etl runner 0.23 (or 0.24) as we updated jruby to 9.1.6 in both those releases?. will be fixed by #3136. @vceron we have experienced this bug post r87 too, that's why we're going forward with the fix in #276 and not 3136. my bad. The changes discussed here will move the staging step away from Sluice to S3DistCp.. Since it's been treated in another issue, it prevents people from going back to the e.g. data-loss label x months from now and look at what happened in this ticket.. @vceron \nUnfortunately, we've moved to S3DistCp for staging and we're not doing any renaming anymore.\nThanks a lot for your contribution anyway!. no indeed, the filenames and the folder structure will be kept as is. You can work with the unix timestamp in the filename e.g. _var_log_tomcat8_rotated_localhost_access_log.txt1502161261.gz.\nAlso there should be one folder per collector instance now.. @chuwy Ok thanks, I think the plan was to combine the bumps with the spark port milestone anyway.. Thanks for catching it.. woops, ty. closed in favor of #3203 . superseded by #3358. @alexanderdean Are we sure, we want to write this part in Java?\nIt seems that, to be able to interact with your own java code, you need to deal with unmanaged dependencies (require 'lib/mylib.jar').\nPlus, if it's destined to be rewritten in Scala we might as well rewrite this part as well because I don't think it'll be portable as it might be written anyway (the Java code will be peppered with JRuby stuff from what I understand).\nPlease, correct me if I'm wrong as it's my first time even remotely approaching some JRuby (and Ruby for that matter).. > If this is blocked by any missing features in Dataflow Runner, these should go into a priority 0.2.0 for Dataflow Runner.\nIt's not a blocker since I don't really know the order of things but the Spark stuff relies on snowplow/dataflow-runner#6. @alexanderdean \nBy the way, dataflow-runner only allows CANCEL_AND_WAIT and CONTINUE as actionOnFailure and not TERMINATE_JOB_FLOW, I defaulted to CANCEL_AND_WAIT as a result in the making of the playbook.\nDo you think it's reasonable?. closing, there is no storage loader anymore. It is already: https://github.com/snowplow/snowplow/pull/3029/commits/70c9bc1dba4a9be01dcb086a9881a768deb527eb\nIt'll disappear with the rebase.. I think I'm done, reviews very welcome.. I think I've taken care of everything you mentioned @chuwy :+1: . @alexanderdean \nThe handling isn't at fault here, you've effectively specified an invalid option, the only global option being --version. Moreover, this handling is done by OptionParser so I have no control over it.. exactly :+1: . @chuwy what do you want to do regarding rdb loader logs? Should we just retrieve them only if the emr job was successful?. @alexanderdean the problem seems to be that we want the logs especially if rdb loader failed and we have, at the moment, no way to know which step failed.. pushed an rc4 taking care of hopefully everything and an rc1 of s3://snowplow-hosted-assets/common/emr/snowplow-hadoop-fs-rmr-0.1.0-rc1.sh. migrated to: snowplow/snowplow-elasticsearch-loader#10. The added value from the current checks I see is the short-circuiting of the launch of the EMR cluster.\nOnce the EMR cluster is up, I don't think those checks matter, it has virtually no impact whether to run the no-ops steps or not, unless I'm mistaken?\nAnd since we want to move everything to EMR, the cluster would be up no matter what.. From what I gathered/experimented with today, we could:\n\nimplement the different checks (empty dirs and even number of lzo files) through simple bash scripts submitted as emr steps as you said earlier\nuse s3distcp for staging and archiving. However, the cleaning up of the filenames performed in s3_tasks seems a bit too involved for an s3distcp step. As a result do you think we can do without or that we should rely on something else entirely to do the staging step?. > the challenge is we need to figure out how to capture return codes from those bash scripts\n\nIsn't a failed/succeeded step enough?\nI'll keep on investigating the last point. I guess I don't really see the difference between:\n\nnow:\na no-op which results in the termination of both the cluster (or avoid starting it but this goes away no matter what as we said earlier) and eer (through a DirNotEmptyError or a NoDataToProcessError), for me this does seem like a kind of failure\n\nthe failure of a step like enrich which results in the same thing (through EmrExecutionError)\n\n\nand what would be (as I imagine it):\n\nno-ops and failures both result in the termination of both the cluster and eer (through EmrExecutionError)\n\nSince EmrExecutionError invites the user of eer to check the console, she/he will see why things failed (no-op or \"real\" failure).\nI think this simplify things quite a bit actually regarding the transition to dataflow runner compared to having to inspect the status of a particular set of steps.\nWould love to hear your thoughts.. So here is how I've managed to mimic the current behavior of eer without relying on Sluice based on two scripts that check whether a directory is empty:\n\nscript step checking that the processing is empty, if not failure of the step which is then picked up by eer and results in a DirNotEmptyError\nstaging s3distcp step (still has to be coded)\nscript step checking that the processing has been filled, if not -> failure of the step -> NoDataToProcessError\nscript steps checking that shred and enrich are empty, if not -> failure of the steps -> DirNotEmptyError. @alexanderdean Ok, I'll try to split the commit in two.. created and fixed #3130 and #3131 :+1: . migrated to: snowplow/snowplow-elasticsearch-loader#9. Closing because of #3376. I must have missed your renaming. migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/124 closing. migrated to: snowplow/snowplow-elasticsearch-loader#8. migrated to: snowplow/snowplow-elasticsearch-loader#7. ah so we can support datastore for gcp\n\ncc @alexanderdean from our convo this morning. incorporated into #3624, thanks :+1: . Logged #3407 as a spin-off since enforcing a time to live on a file being used in an always on application is not trivial.. This is a byproduct of library upgrades, notably #3377, closing. This is a byproduct of the library upgrades notably #3310. closing. migrated to: snowplow/snowplow-elasticsearch-loader#6. migrated to snowplow/snowplow-elasticsearch-loader#5. Integrated into #3274, thanks!. I think it might be weird to point Snowplow users using React to the node tracker so I'd lean towards adapting the js tracker.. will do :+1: . @simplesteph works for me :+1: , closing.. dupe of #2998, closing. @alexanderdean what's the story behind this?. ok :+1: . migrated to snowplow/snowplow-elasticsearch-loader#4. What does this refer to?. sorry for the huge delay, merged into the R109 branch. yup trying to deal with the nightmare that this rebase is. This is (finally) ready for review @chuwy . @chuwy done\n. done :+1: . @alexanderdean @chuwy \nDoes 1b8ba44aec6e9d4bedff2632deb446d5c2224acf warrant a minor release for storage loader?\nMaybe the ticket should be renamed as well.. CrossBatchDeduplicationSpec is the only one left standing!. Should we create an issue for OLD_ENRICHED_PATTERN since it wasn't introduced in this release?. ok, I'll fix both then. Fixed the rdb shredder naming (rc5 available at s3://snowplow-hosted-assets/4-storage/rdb-shredder/snowplow-rdb-shredder-0.12.0-rc5.jar) and the storage loader regexes (eer and sr rc5).\nCan we remove the rcs in s3://snowplow-hosted-assets/4-storage/relational-database-shredder/ ?. just to clean up the bucket since the relational-database-shredder folder won't be used anymore. true, will do. I'll take care of it :+1: . moved to snowplow/snowplow-rdb-loader#54, closing. Do you mean in the wiki? because it's already the case in eer.. moved to docs, closing. This is apparently fixed in travis-ci/travis-build#1057, need to try it out. see it live here: https://travis-ci.org/snowplow/snowplow/builds/233635955 !!. Has indeed been fixed, closing. @miike\nThe tests seem a bit convoluted I agree.\nAs is, I don't think there is way to test the created payload since only the http response is sent back in the route.. Closing this one since the commit will close #2685 :+1: . I went ahead with a patch, I reckoned it didn't warrant a minor. Happy to change if it does.. superseded by #3220. ~~No CI/CD for the scala stream collector, closing.~~ everything is in release_kinesis.yml.. superseded by #3220. No CI/CD, closing. Added in https://github.com/snowplow/snowplow-elasticsearch-loader/pull/31/commits/95efcefb90767f44f38f45d5bedb70b67d1ab0b1 , thanks a lot :+1: !. covered by #4010, closing. yup, I think it never worked too, just as #3224 . woops didn't mean to close this. @feynmanliang thanks \ud83d\udc4d \nHowever, this particular bump is already taken care of in our upcoming release (the pr is #3203 and the specific issue is #3007). As Alex said, the next version of scala-common-enrich (0.25.0) will be on maven central.\nHowever, I don't think it makes sense to have the final applications (spark enrich or scala-hadoop-shred for example) in maven central.\nHowever, if you don't want to have to compile those locally you can download them from s3:\nbash\naws s3 cp \\\n  s3://snowplow-hosted-assets/3-enrich/scala-hadoop-shred/snowplow-hadoop-shred-0.10.0.jar \\\n  .. Thanks for reporting this, however this has been fixed in the next release (#3203). migrated to snowplow/snowplow-elasticsearch-loader#3. done, closing. Done, closing. closing for now. will be part of #3274 :+1: . @chuwy thanks for the review, implemented your suggestions \ud83d\udc4d . moved to docs, closing. in time, yes :+1: . hey @arunma it's dependent on #3270. I would think so too, either kinesis or pubsub maybe. closing this one since the commit in the PR is part of #3274 :+1: . Started load testing the collector and we've cut the 99th percentile response time in half (18ms to 8ms) :+1: . @chuwy I think I've taken care of everything, thanks for the review :+1: . The web model has moved to https://github.com/snowplow/web-data-model, please open this PR there.. there is already a lint command which validates resolver and enrichments in #3110 as per #1946. migrated to https://github.com/snowplow/snowplow-rdb-loader/issues/125, closing. kinda related not at the events level but at the requests level #3637. Closing since this is a dupe of #3362 \n@ashwinGokhale please try to keep only one PR open for the same issue.. @devesh-shetty ping me when you want me to review :+1: . this is parked in a branch no need for a pr. duplicate of #2974. @aldemirenes please don't open a new pull request every time, just push your changes to the branch against which the pr is open. @alexanderdean do we need a softlock too?. This is already the case?. superseded by #3771, closing. I still don't like this approach of using reflection and finding some property based on a name, the main reason being it's really expensive: we're reflecting and we're going over all fields of EnrichedEvent for every single event we enrich.\nSurely, most of the fields wouldn't make good candidates for a partition key and a limited number would. My current thinking is still around suggesting a limited set of potential fields that would make good partition keys.. Here's my shortlist, tell me what you think @alexanderdean :+1: \n\nevent_id\nevent_fingerprint\ndomain_userid\nnetwork_userid\nuser_ipaddress we should remove useIpAddressAsPartitionKey as part of this PR btw\ndomain_sessionid\nuser_fingerprint\n\nOne thing worth investigating would be the proportion of events where those fields are missing. @ashwinGokhale Could we refactor this PR to leverage this list and not reflection?. Integrated into #3274 , thanks!. no, just the Scalding ones. Did you check out rvm/rvm#3544 ?. no more vagrant, closing. Given that, R91 outputs the logs on failure (#3361), is it just a matter of:\n\nRDB Logs output go to stderr\nRDB Logs prefixed with correct log level\n\n?\nI'm not sure how we would go about the second point given that we don't know what's in the logs.\nA first step would be to error if the step failed, warn if cancelled and info otherwise, what do people think?. This has been incorporated into #3438 by @keanerobinson , thanks a lot!. Could we add this and snowplow/iglu-central#628 to Knossos ? @alexanderdean @knservis . Yup, that's my bad I wasn't aggressive enough in scheduling, scheduling now.. mmmh I thought those would in 102.... Hey @acgray ,\nApparently we have customers running the Mandrill integration without running into issues with this new subaccount property.\nBecause updating the jsonpaths associated with the 1-0-1 schemas would fail their data loading into redshift if we don't update their tables, we're postponing this feature.\nThe commit now lives in #3716, closing.. true...reopening. Closing. @acgray the 2nd gist 404s. Initially suspected tagged types but they seem to behave fine at as far as the cache is concerned.. @acgray yup, got it :+1: . moved to docs, closing. You'll have to look into the EMR logs, and please direct any support questions to our forums: https://discourse.snowplowanalytics.com/. @alexanderdean there is only NoDataToProcessError right?. I guess the problem was which cases are no-ops and which aren't was not clear to me.\nAm I right that no-ops are limited to the following scenarii:\n\nin buckets without data\nenrich good not empty\nshred good not empty\n\n?. I haven't changed anything in my workflow :fearful: . The current behavior is:\nif we skip archive_enriched, we don't do any archiving of either enrich or shred\notherwise:\n\u00a0\u00a0if enrichwe archive enrich and shred for the current run id\n\u00a0\u00a0otherwise we archive enrich and shred for the last run id\nDo we want to convert that to:\nif we skip archive_enriched, we don't archive enrich\notherwise:\n\u00a0\u00a0if enrich we archive enrich for the current run id\n\u00a0\u00a0otherwise we archive enrich for the last run id\nand\nif we skip archive_shredded, we don't archive shred\notherwise:\n\u00a0\u00a0if shred we archive shred for the current run id\n\u00a0\u00a0otherwise we archive shred for the last run id\n?. I'm referring to https://github.com/snowplow/snowplow/blob/master/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb#L70-L76\nping @chuwy . yup. @alexanderdean I put down tomorrow as the release date, if a release doesn't happen tomorrow that'll need to be changed.\nWhat's left to be done:\n\nrm the rc commits (I left them in case there is need for another one)\nmerge\ntags. This has been taken care of as part of #3274 : https://github.com/snowplow/snowplow/blob/3dd91edfadf1b3bd6145bbc95a1052dcc4ed6ac1/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/CollectorRoute.scala. Hey @abioneperhobby , did you manage to get to the bottom of this?\n\nIf not, feel free to open a thread on our Discourse.. mmh it seems this wasn't integrated into r92 :fearful: , descheduling for now. Incorporated into #3624 , thanks a lot \ud83d\udc4d . Hello, could you create the issue in the repository dedicated to the javscript tracker: https://github.com/snowplow/snowplow-javascript-tracker? Thanks.. superseded by #3489, closing. R93, which is the next release, will migrate the collector to akka-http (cf #3299).\nIn akka-http cookie parsing is just a matter of configuration, see cookie-parsing-mode in the akka-http default configuration. Closing since R93 has been released :+1: . Please direct your support request to our forum: https://discourse.snowplowanalytics.com. @chuwy @knservis please help review the emr etl runner and scala common enrich changes :+1: . Will investigate whether s3distcp is even capable of removing those files.. Apparently there is no way to do this with s3distcp, are we okay introducing a new bootstrap action removing the existing $folder$ files?\nIt would go through the configured buckets and remove the $folder$ files.. I don't believe those are included in the manifest since they are created as a byproduct of the s3distcp job, i.e. they weren't in hdfs to begin with.. Descheduling since this non-trivially impacts job times. hey @frankcash we have an internal process that removes those files which is launched on demand, it's unlikely it will make its way into the off-the-shelf job given its impact on job time.. same as spark enrich: a release_ssc and a release_se with separate tags. That'd keep us from having to maintain the version in release_kinesis too.. superseded by #3447 . no, this is idempotent. @neekipatel please redirect your support requests to our Discourse forum. cherry-picked into #3438, closing. I personally think the ssc's current approach gives more flexibility, and so should be preferred? Happy to hear counterarguments.. Thanks a lot for the investigation @miike :+1: \nIt seems there are 3 work streams:\n\nencryption at rest on S3 which @miike describes thoroughly above (I tend to lean towards SSE-S3 which seems to have the less hassle)\nencryption at rest on HDFS in EMR which seems to be a matter of configuration:\nhttps://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SecureMode.html#Data_Encryption_on_RPC\nhttps://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/SecureMode.html#Data_Encryption_on_Block_data_transfer.\nencryption in transit on Spark in EMR:\nRPC (e.g. for shuffle) is encrypted by default with EMR > 5.9.0\nHTTP (e.g. to the web interace) is another matter of configuration https://spark.apache.org/docs/latest/security.html#ssl-configuration\n\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html. @alexanderdean was this regarding server-side encryption?. Ok this will be more a matter of QA than code since it's advertised as being transparent for the consumers and producers.. Ok this will be more a matter of QA than code since it's advertised as being transparent for the consumers and producers.. works as expected and doesn't need anything from our side, closing. I think this entails adding more logic to eer which is not the direction we want to take right now.\nWhat's wrong with rdb loader failing because it was given wrong targets?. @chuwy on it :+1: . done, closing. we could, the changes should be lightweight. Would supporting different strategies at a more granular level (pii level) make sense?\ne.g. names in a custom context which don't appear often so we can afford to use sha-512 but there are email addresses in every event so we limit ourselves to sha-1\nPlus we could encrypt certain fields and hash others.\nAdditionally that would help with constraint validation, i.e. I can use the hash function that is the most appropriate wrt the field's max length\nFinally that would give more flexibility regarding \"collision prevention\".. cherry-picked into release/r103-paestum, thanks!. Please direct your support requests to our discourse forum https://discourse.snowplowanalytics.com.. Unfortunately cross-batch deduplication through DynamoDB is not idempotent. As a result, we'd need to make the two exclusive.\nThis kind of makes sense since the two are going in different directions, cross-batch deduplication is a sizable added cost whereas when using spot instances you're trying to save money.. I'd be for leaving it to the default scalafmt task, and run this as part of the ci so if anyone forgot to run it the ci will just fail because I don't want to further delay compile.. really? I've always heard that scalafmt is terribly slow for medium to large projects.. This isn't a blocker, the bootstrap action just doesn't perform anything.. sgtm :+1: \n. superseded by #3553 \ud83d\ude1e , closing. this has been fixed in the upcoming r95. I'll have a last look but I think we're good to go. @knservis Could you rebase so that the the \"Prepare for release\" is last and remove rc commits?. @knservis the date of the \"Prepared for release\" commit needs to be updated otherwise it's not chronological. Merged, closing. If I understand correctly you would like the location header to contain the https url of the original load balancer?. yup I was thinking about leveraging this header as well \ud83d\udc4d . No timeframe yet, feel free to open a PR \ud83d\udc4d . Closing since #3512 will be closed when the release happens.. -jar needs to be the last argument e.g. https://github.com/snowplow/snowplow-docker/blob/master/s3-loader/0.6.0/docker-entrypoint.sh#L10-L12\nAlso you should update your s3 loader version 0.4.1 is quite old.\nIn the future, please raise your support requests in our discourse.. It's true that cookie bounce was meant to support get requests only.\nYour approach looks good to me. However, I have trouble understanding what the problem would be with \n\nthe next thing that needs to be done is to allow that content type to be pushed to the sink\n\n? It seems to me like it's just another route in the collector?. A PR would be interesting indeed.\nTo come back to your earlier post, do you have references for:\n\nSome browsers (like Firefox) will not follow a 307 POST redirect unless the xhr content type is set to application/x-www-form-urlencoded\n\n?. @miike what do you think about:\n\nTo use stdout, comment or remove everything in the \"collector.streams.sink\" section except \"enabled\" which should be set to \"stdout\"\n\n?. @knservis pls rename according to our naming convention: it should state what is being fixed and how and not what the problem is e.g. Spark Enrich: enable dropwizard metrics sink and not Spark Enrich: dropwizard metrics sink is missing. This will be taken care of in the upcoming R97 release, the original issue is #3497.\nPlease direct your future support requests to our discourse forum \ud83d\udc4d . Ideally we'd just wouldn't do empty checks in eer, this leads to unnecessary memory problems. You can always cherry-pick them.\nAnyway, I'll add #3529 first, and then change the base branch.. @chuwy updated \ud83d\udc4d . @knservis you can cherry-pick:\n\nEmrEtlRunner: uncompress gzipped raw files when copying to HDFS (closes #3525)\nEmrEtlRunner: bump to 0.30.0 (closes #3526)\nEmrEtlRunner: add ability to skip RDB Loader consistency check (closes #3529)\n\nand close \ud83d\udc4d . changed the title: no caps after :. superseded by #3516, closing. I don't know wtf is going on with travis but publishing errors out for stream enrich without error messages...\nhttps://travis-ci.org/snowplow/snowplow/jobs/322933438#L3324\n\n/home/travis/.travis/job_stages: line 156:  9797 Terminated. ~~mmh same thing happened for spark enrich: https://travis-ci.org/snowplow/snowplow/jobs/315961847#L3998~~\n\nnvm all builds end the same way. the configuration value is based on the case object name which happens to be StdoutConfig.. from @rbolkey :\n\nI looked at the thread there. I didn't see anyone propose using both Option at the top level along with an enabled field? You can kind of have both pieces of cake there. Missing block implies disabled (less pain for user's configuring). Present block has enabled defaulting to true, but you can template it to false if desired? Thoughts?\n\nMy personal view is that it might be kinda confusing, wdyt @chuwy @alexanderdean ?. both\nconf\na {\n  x {\n    enabled = true/false\n    restOfTheConfig = ...\n  }\n}\nconf\na {\n}\nare valid. Quoting the FAQ:\n\nQ: I got Tried to use insecure HTTP repository without TLS, what is that about?\nA: This means your project was configured to download dependencies from a repository that does not use TLS encryption. This is very insecure and exposes you to trivially-executed man-in-the-middle attacks. In the rare event that you don't care about the security of the machines running your project, you can re-enable support for unprotected repositories by putting this in your project.clj file:\n;; never do this\n(require 'cemerick.pomegranate.aether)\n(cemerick.pomegranate.aether/register-wagon-factory!\n \"http\" #(org.apache.maven.wagon.providers.http.HttpWagon.))\nIt's also possible you have a dependency which includes a reference to an insecure repository for retrieving its own dependencies. If this happens it is strongly recommended to add an :exclusion and report a bug with the dependency which does this.\n\nSo we might have to add:\nclj\n(require 'cemerick.pomegranate.aether)\n(cemerick.pomegranate.aether/register-wagon-factory!\n \"http\" #(org.apache.maven.wagon.providers.http.HttpWagon.))\nor find out which dependency transitively relies on an HTTP repo.. Yes definitely, you might have some luck trying to update the dependencies as well, it might be worth a shot.. Interesting stuff, thanks :+1: . Hello @simondowdles, please redirect your support requests to our Discourse.. problem is that there is a lot of overlap between the two prs and that slows down review.. descheduling as snowplow/snowplow-rdb-loader#21 needs to happen first. I think @chuwy hasn't had time to look at it yet.. @knservis can you not merge until we've greenlighted the changes?\nNow we'll have to re-review it in #3585 which is much more noisy. This is for the reviewers to assess, not the person who opened the PR.\ne.g. I waited on your greenlight (and Anton's) on #3250 before putting it into QA.. which configuration parameters will you need apart from the output stream name?. no, we've been trying to limit the size of the matrix of supported configurations :+1: . done. I've been thinking about this lately and, given that the migration to beam is not a long way off (where, ideally, we'll be able to run some end to end tests independently of the targeted storage), I wonder if it's wise to spend time on testing kinesis-only things.\nBasically, it boils down to how big of a time investment this is going to be.\nwdyt?. @knservis kafka would indeed be easier as you can do everything programmatically without resorting to another project.. @knservis it will depend on the qa phase for both this and ga. either that or truncate it into the BadRow. yeah I reread and remembered the original intent of this issue and it is indeed to log the row to stdout as it is already turned into bad rows.. Actually we could do both as oversized bad rows just take the size and not the content as input.. in this case I'll just truncate the initial event into the bad row as we originally discussed (going in circles on this one :fearful: ). yes, I'll let @chuwy do another pass \ud83d\udc4d . @knservis when you merge those \"mini\" prs into the main one, can you delete the associated branch, thanks. I think this should live as a guide outside of the repo, thanks @miike , feel free to share it on discourse. Thanks for pointing it out \ud83d\udc4d . For me there are two things that need to happen:\n\nreformat atomic events and so remove those fields (among others), the problem with this is that it breaks a lot of stuff and so will take a lot of time\neither deprecate (and dump the output in a context) or completely remove this enrichment. The atomic events refactor might be a good opportunity to get rid of it though.\n\nOtherwise, it'd mean a jar that is used for an enrichment can never be kicked out which could constitute a security risk since there'd be no way to patch it if it's eol.. Regarding the first step, we've:\n\npublished a message on discourse: https://discourse.snowplowanalytics.com/t/user-agent-utils-enrichment-deprecation/2119\nedited the wiki: https://github.com/snowplow/snowplow/wiki/user-agent-utils-enrichment. exactly yes, should have looked into the closed tickets too :(.\n\n@alexanderdean should we fix it anyway?. >however the artefact seems to be there.\nI thought there was a redirect? so it's not actually there, changing the issue title to reflect that. The way I read the above desc is that we don't know why the build is failing. And by chance, we tried https and it worked.\nA better desc would be that the old repo is issuing redirects to the same repo in https so we're updating the URL.. same issue here snowplow/scala-maxmind-iplookups#37 :(\nI'm afraid these will be popping up now. yup feel free to add it do the \"SCE modernization\" milestone (maybe not a great name), that's the milestone I created to do a massive cleanup of those kind of issues (moving away from scalaz, updating libraries, scala 2.12 and such).. yeah feel free to rename (pretty much every project will be affected). welp I'm gonna have to steal this for #3250. superseded by #3843. fomatting looks nice \ud83d\udc4d . @knservis afaict #3598 is missing from the commits and #3604 is missing from the changelog entries. @knservis can you add an example pii enrichment in https://github.com/snowplow/snowplow/tree/master/3-enrich/config/enrichments as part of #3472 ?. and also reorder the commits so that Prepare for release is last both in order and chronologically.. it's because gh orders commits according to their timestamps. Also how do we deal with:\n\nFor a specific invalid configuration scenario the code may create a new key pointing to an empty object. See: json-path/JsonPath#438. created #3636 . sgtm. closing as merged in #3599. can't deploy rcs :fireworks: , logged https://github.com/travis-ci/dpl/issues/778. This is just for symmetry, we don't do AWSKinesis or AmazonKinesis so GooglePubSub, or even  GoogleCloudPubSub, seemed clumsy to me, but I don't have strong preferences.. yes that's my bad I did the diff between the commits and the entries instead of between the issues and the entries.\n\nProblem is there are no commits for those entries. should be taken care of by #3524 , leaving open for now. taken care of by #3524 closing. Given how long it takes (12 mins for the ssc), it should only be triggered when deploying artifacts. descheduling as we're evaluating other options. then lgtm\nwhen asking for review and the pr is noisy, please pinpoint what you want reviewed or make it less noisy, thanks. nope, just forgot, done :+1: . Incorporated into a1051070e6bee23e225a000473808d4fc4178c65, closing. @knservis heads up that I renamed the ticket :+1: . Please, log your support requests to our Discourse.. value. either enrich-0.14.0 or {{stream-processing-tech}}-enrich-0.14.0. yup, don't have strong opinions, just don't want to leave kinesis. @knservis do you mean https://github.com/snowplow/snowplow/pull/3646/commits/d27455ecb1683c0913485748fe12e9ea1861db68 ?. @knservis, yeah don't worry about this, this will be overhauled in #3624. will look at it tomorrow morning :+1: . - platform is user-defined, I don't know if we'd want to change that\n- should we change the timestamps ? (elt_tstamp and derived_tstamp)\n- shouldn't the pii transformations be split (e.g. one event per pii transformation, I remember reading List[EnrichedEvent])?\nI'll let Alex as TPM handle those qs. can you rebase first? I don't want to review twice. @knservis can you look into the test failures and fixup my trailing commit. thanks. @knservis ping me when the tests and commits are sorted. I meant my \"fix me up\" commit. Hey, please redirect your support requests to our Discourse, GitHub is only for code-related issues.. Btw, as a late input into this process, I've been working with a codebase using https://github.com/47deg/github4s/blob/master/.scalafmt.conf and it looks great.. The whole recent effort on enrich and ssc was purposely not to embed aws dependencies if you're not on aws. If these get into the sce we're back to square one.\nWhat we could do is decline the sce into sce and sce-aws and have aws specific artifacts depend on it.\nOf course this doesn't make this specific part of the enrichment available to people on prem or on gcp but that was already the case.... Thanks @rbolkey , I'll have a look on Monday :+1: . doesn't make sense to include this in R106 since it doesn't touch the collector, rescheduling. incorporated into #3824 , closing. Thanks a lot :+1: . Integrated into #3799, thanks so much, closing :+1: . Thanks for flagging :+1: . logged snowplow/snowplow-python-tracker#197. >Hey @BenFradet, am I right that we can use existing credentials for that?\nyup. @chuwy heads up I renamed the ticket. Integrated into #3799, thanks so much, closing :+1: . We can't as specified in the ticket:\n\nWe've had to reinstall rvm manually because of travis-ci/travis-ci#8717, maybe this is linked?\n\nUnless we want to update bundler, I think it's been fixed in the latest version. I'll log an issue detailing that. merging when green :+1: . descheduling, bundler on travis is 1.16.0 which doesn't contain the necessary fix.. Could you mention your issue to our Discourse forum, people will be able to help you there?\nWe try to keep github issues for code changes in the appropriate code base.. Thanks a lot, everything should be fixed now.. we have to add the dependency ourself despite the fact that we don't use it. We can't use 2.9.3 because there was no release for jackson-module-scala.. - shading resulted in conflicts with json4s (java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.fromJsonNode(Lshadedjackson/databind/JsonNode;)Lorg/json4s/JsonAST$JValue;\n- no shading resulted in conflicts with maxmind (java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.node.ArrayNode.<init>(Lcom/fasterxml/jackson/databind/node/JsonNodeFactory;Ljava/util/List;)V)\n- downgrading geoip2 to 2.5.0 which depends on jackson 2.6.7 worked. Hey @weipe007 , how are you building Spark Enrich because it depends on geoip2 0.5.0 which in turns depends on jackson-databind 2.6.4. Spark, depends on jackson-databind 2.6.7 which is binary compatible with 2.6.4.. It transitively gets 2.9.3 but can be overriden, that's what we do in spark-enrich and it works fine.\nI would advise comparing what gets into your assembly and from where by running sbt-dependency-graph.. Please, log your support requests to our Discourse forum.. Bumping to 2.0.0 instead since there is a ghost 1.2.0 in s3. SP_XX configurations will work while right now on recent AMIs they don't, nothing wrt to the ui will change.. Good idea :+1: , @chuwy can you have a look?. descheduling. @omenocal can you sign the CLA. Otherwise we unfortunately can't merge your PR :cry: . closing feel free to reopen once you've signed the CLA. 103 has been in code freeze for over a week, so I'm :-1: . plus it'd make sense to integrate at least #3719 with this. this is locking us down to a particular file format (gz) which I don't really like :(. As long as there are no other files being unnecessarily moved, yes. @chuwy, the original problem was that only one \"currency code composite context\" was built but if there were multiple composite contexts needing the currency code (e.g. multiple product impression) only one would receive the currency code.\nWhat this PR does is attach the currency code to all the composite contexts which need it after the fact so that every context which needs it gets it.\nAdditionally, there is a small improvement into how the raw params are transformed before being translated to our schemas so that the data is rectangular which facilitates transposition.. In #3799 closing. renamed since we now have other enrichments relying on local files, c.f. #3750. pushing back:\n\nscio has a stripped-down datastore dependency which will conflict with the \"usual\" sdk\nthis is not aligned with the approach we want to take with the control plane. hey @asgergb , thanks for the report.\n\nCould you help us reproduce as I haven't able to using the published artifacts in https://bintray.com/snowplow/snowplow-generic/snowplow-stream-enrich/0.16.0#files?. Reopening til we get to the bottom of this.\nAlso you might be interested in https://github.com/snowplow/snowplow-docker \ud83d\udc4d . I was able to find the root cause:\n\nour travis uses publishLocal for SCE and that includes jackson-databind-2.9.3\nwhen releasing we use publish and that includes jackson-databind-2.2.3 (c.f. http://search.maven.org/#artifactdetails%7Ccom.snowplowanalytics%7Csnowplow-common-enrich_2.11%7C0.32.0%7Cjar)\n\nApparently publishLocal relies on ivy2 whereas publish relies on maven and I guess they have different dependency resolution mechanisms.. hey @asgergb , could you expose your issue in our discourse and provide as much info as possible (number of shards of each stream, number of instances for stream enrich, etc) so we can try to reproduce?. The problem was that the same sink was reused across kcl's IRecordProcessors and so the same sink would flush as many times as there were shards.. closing as duplicate of snowplow/snowplow-s3-loader#53. didn't read the desc, reopening. This doesn't result in any code changes, closing. @chuwy thanks, updated!. @knservis the summary in https://discourse.snowplowanalytics.com/t/important-alert-r101-bug-may-result-in-duplicated-data-in-the-real-time-pipeline/1987 details the issue.. Also since this enrichment relies on local files, they'll need to be cached, e.g. for spark enrich: https://github.com/snowplow/snowplow/blob/master/3-enrich/spark-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.spark/EnrichJob.scala#L179-L183. @rzats feel free to ping me once you've resolved the points I raised during the last review. superseded by #3799, closing. maps nicely to https://doc.akka.io/api/akka-http/current/akka/http/scaladsl/model/headers/HttpCookie.html. My investigations revealed that we'd need admin-level creds. pushing back. Moved away from blackduck, closing.. @wheller can you sign the CLA and action the feedback? Otherwise we won't be able to merge your PR unfortunately.. closing feel free to reopen once you've signed the CLA. hey @tclass you can look at https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-5x.html for the correspondance between spark and emr versions. yes I saw your comment on your pr, will comment there. Fix -> fix please rename the commit accordingly. last commit looks good :+1: . :+1: from me. it's already in 107: #3799. I'm about to have a final review on the pr so I'll soon find out. Good idea, feel free to open a pull request :+1: . Moving forwards, since this is critical for Kafka. integrated into #3799 thanks a lot!. Thanks a lot, merged :+1: . yes please, open the corresponding issue in https://github.com/snowplow/snowplow-s3-loader and the associated PR :+1: \nclosing this one.. it's in the issue-3636 branch? :thinking: . >Sorry @BenFradet.. I meant in the release/gdpr-2 branch (after fixing the commit messages). We can change the PR to that branch instead of master\nah ok, yes feel free to do it :+1: . Hey @vokurka , please share your community support requests on our Discourse. This way more people will be able to answer and refer back to your question.. This is actually what we are moving towards with the referer parser enrichment and it does also make sense for ua-parser :+1: . @jwhansome thanks for the bug report :+1: \nWe've discussed this internally and we think that the easiest short-term solution would be to give a \"free pass\" to those \"New city\" events meaning that we don't attach a weather context to the event but we don't fail the whole event's enrichment phase at the same time.\nAs for the longer term, I think the strategy needs to be two-fold:\n\nWe need to evaluate and integrate other weather providers (e.g. darksky) as this is a serious shortcoming for OWM\nWe need to evolve our enrichment engine in order to make it possible to have \"optional\" enrichments meaning that a failure of a specific enrichment doesn't necessarily mean a failure of the whole enrichment phase. We actually have a planned RFC on this subject.\n\nPlease tell us what you think.. Please ask general questions on our Discourse forum, this way more people from the community will be able to help you.\nWe use GitHub issues for things that will result in code changes.\nWe're working on better communicating that :+1: . @rzats you'll need to add this to the changelog for R107. logged snowplow/iglu-central#823, closing. Thanks a lot, merged into the release branch for R109 :+1: . Merged, thanks a lot! :+1: . yes the issue has been scheduled for 107. integrated into #3799, cloisng. Thanks for the report @mdemri :+1: , feel free to open a PR :+1: . Hey @Arsalan9002, please log your support requests on our Discourse, more people will be able to help you there.. Merged, thanks a lot!. Excellent find, I logged snowplow/snowplow-rdb-loader#110 :+1: \nFeel free to open a PR for the atomic.events model. renamed to Lambaesis in #3853 . do you mean as additional checks, i.e. make sure it has this value and this domain, before triggering do not track?\nFor the domain, I'll need to check if it's accessible.. can't specify the cookie domain with the Cookie header https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cookie. I was wrong closing. blocked by snowplow-referer-parser/jvm-referer-parser#22. Hey, @anistal thanks :+1:\nWe don't use kinesis firehose ourselves yet, that's why it might not be supported out of the box.\nCan you open the PR so we know what the change entails?. We don't directly load the enriched events into redshift.\nWe have a shredding process, which, amongst other things, extracts the different contexts into their own entities.\nThose entities are then loaded into separate Redshift tables along with the atomic.events table you just linked by a dedicated loader.. logged snowplow/snowplow-objc-tracker#356, closing. superseded by #3890 . closed by 80523d5f67f23597a1065e4ae83a833874373c75. >Are we doing a new release of Spark Enrich, or not?\nNo, but a couple of important tickets are applicable to both:\n\n\n3789\n\n\n3793\n\n\nAnd it made a lot of sense to add them now instead of, for example, in 4 releases where batch will get all the features we've accumulated, that's why there is a sce bump.\nTo sum up, it will greatly ease the next batch release.. @darrenhaken unfortunately no, it's named after the Algerian archaeological site :smile: . @alexanderdean \n\nCan we rename ...\n\nWe've renamed it to regexes-latest.yaml to be consistent with the monthly thing we want to have, i.e. a latest and a yyyy-MM. However, the original dataset is actually named regexes.yaml so I don't know if we'd want to deviate from that too much.. I found \nand\n\nwhich one did you have in mind?. @aparra yup that would be the way to go about it :+1: . > The array in the POST body represents an array of events\nYou have to specify the schema multiple times and people don't want to have to modify their payloads to add the schema at all\n\nThe POST body represents a singleton event which contains an array at its root?\n\nNo columns can be derived from arrays at the moment when shredding/loading plus people don't want to modify their payloads as above\n. aah, then yes previously it was considered to be a singleton event with an array at its root. This wasn't done indeed, feel free to open a PR :+1: . you need to open the PR against this repo not your fork :+1: . nope you're right about the duplication, we're still missing a couple of components in order to migrate though.. They can't use 8.0.53 or above.. hey @jbeemster, are you happy with 65536?. @jbeemster would you consider this done with the latest version (i.e. the one not using sluice)?. integrated into #3965 , thanks a lot! :+1: . Thanks @kingo55 , it seems I missed a tag.\nIt's now been published :+1: . hey @christoph-buente , we're short on bandwidth at the moment but this will be going into the next full-fledged snowplow release which should be the one after next (the next one being a very small release).. integrated into #3965 , thanks a lot! :+1: . the sbt version confused me it seems. Incorporated into #3965 , thanks a lot! :+1: . Integrated into #3965 , thanks a lot! :+1: . cherry-picked into #3945 , thanks :+1: . hey @miike , could you update the PR?. Incorporated into #3965 , thanks a lot! :+1: . Agreed, I think HTTP is fine and the most non-controversial for the moment. cool thanks I'll have a look. I can create a repo for you :+1: . yes, you can track our progress here: https://github.com/snowplow-incubator/snowplow-event-recovery. postponing until snowplow-event-recovery works with events coming from the batch pipeline.. hey @misterpig , could you add a spec in spark enrich, as per my comment?. Incorporated into #3965 , thanks a lot! :+1: . No worries, I ended up doing it :+1: . Integrated into #3965 , thanks a lot! :+1: . As part of hacktoberfest, we have a raffle to win Snowplow t-shirts.\nTo enter the raffle you can fill out this form: https://docs.google.com/forms/d/e/1FAIpQLSeBbwQMWD9w9CEvjKt1KblDVpMy9mHOvQXS6YMRYH3geM8pCw/viewform. thanks :+1: , cherry-picked into #3945 . Thanks for the report :+1: . migrated to https://github.com/snowplow/snowplow-android-tracker/issues/297. migrated to:\n\nhttps://github.com/snowplow/snowplow-objc-tracker/issues/398\nhttps://github.com/snowplow/snowplow-android-tracker/issues/290. Hello, please redirect your support requests to our Discourse.\n\nIn your case, emr version 5.17 is not supported.\nThe last supported version is 5.9.0:\nhttps://github.com/snowplow/snowplow/blob/08d2f51b44148169427e5f423f7ea4dd489af9e5/3-enrich/emr-etl-runner/config/config.yml.sample#L29\nYou can check our compatibility matrix here: https://docs.google.com/spreadsheets/d/1oI3n7nUzNfCwuNJk-LNdMdkAwUt6kI1irPg1FCFyxKc/pubhtml?gid=1937493131&single=true. https://github.com/ua-parser/uap-scala. yup. Here's what I currently have, feedback welcome:\nbash\n[cleanup] Emtpy Snowplow HDFS\n[cleanup] Empty HDFS trash\n[staging] s3-dist-cp: Raw {raw location} -> Raw Staging S3\n[enrich] s3-dist-cp: Raw S3 -> HDFS\n[enrich] spark: Enrich Raw Events\n[enrich] s3-dist-cp: Enriched HDFS -> S3\n[enrich] s3-dist-cp: Enriched HDFS _SUCCESS -> S3\n[staging_stream_enrich] s3-dist-cp: Stream Enriched {enriched stream location} -> Enriched Staging S3\n[shred] s3-dist-cp: Enriched S3 -> HDFS\n[shred] spark: Shred Enriched Events\n[shred] s3-dist-cp: Shredded HDFS -> S3\n[shred] s3-dist-cp: Shredded HDFS _SUCCESS -> S3\n[archive_raw] s3-dist-cp: Raw Staging S3 -> Raw Archive S3\n[rdb_load] Load {storage target} Storage Target\n[archive_enriched] s3-dist-cp: Enriched S3 -> Enriched Archive S3\n[archive_shredded] s3-dist-cp: Enriched S3 -> Shredded Archive S3\ncc @ihortom @stdfalse . Thanks! edited my previous message. Thanks but this is already covered by #2260 :+1: . Incorporated into #3965 , thanks a lot! :+1: . This has already been done in #3912, sorry. hey @stdfalse , the connection isn't maintained for the duration of the steps, just when sending steps to be executed, can you link the full incident?. incorporated into #3945. Incorporated into #3965 , thanks a lot! :+1: . please move it to https://github.com/snowplow/snowplow-android-tracker. The problem with compaction in s3-dist-cp occurs if we have alternatives in the groupBy regex, e.g. for the step moving the results of the shred step from hdfs to s3: .*\\/atomic-events\\/(part-)\\d+-(.*)|.*\\/shredded-types\\/vendor=(.+)\\/name=(.+)\\/format=(.+)\\/version=(.+)\\/(part-)\\d+-(.*).\nGroups are filled with null even if they don't match, resulting in filenames like:\n\nnullnullcom.amazon.aws.cloudfrontwd_access_logjsonschema1-0-6part-b417c9a0-e69f-4b20-b958-ffb939d675c1-c000.txt.gz for shredded types\npart-b417c9a0-e69f-4b20-b958-ffb939d675c1-c000.txtnullnullnullnullnullnull.gz for atomic events\n\nAnd if we choose to split this step in two, the step moving shredded types can fail if they are absent.. We've decided to split this step in two.\nTo not cause issues to people not having shredded types, it will be disabled by default in the configuration.. one minor thing though, could you split your commit in two:\n\none for the consumer\none for the producer. Incorporated into #3965 , thanks a lot! :+1: . Good idea, we've done something similar for the s3 loader: https://github.com/snowplow/snowplow-s3-loader/commit/48364eea128b19a4dfa085afb731eb12885ca671. Thanks!. >That's why i thought it would be better to have a working base version of the config file in the actual code repo available to have it shipped with the docker images.\n\nright, agreed :+1: , regarding the tests failing could you leave the quoting as it was? there are now strings that should be numbers. Incorporated into #3965 , thanks a lot! :+1: . Incorporated into #3965 , thanks a lot! :+1: . ah the infamous jackson...\nI think we should try aligning to Spark, which uses 2.6.7.1: https://mvnrepository.com/artifact/org.apache.spark/spark-core_2.12/2.4.0.\nAnd list/check which dependencies are incompatible.\nI realize this is more involved than the original ticket, so feel free to leave that to me, we'll have to do it in the near future anyway :+1: . hey @rolandjohann , we already have docker images here: https://github.com/snowplow/snowplow-docker (which we might move to sbt-native-packager at some point).\nHowever, we're not planning on supporting RPMs in the immediate future.. dupe of #1829, closing. Thanks, fixed! And no unfortunately there is no easy way to contribute to the wiki.. apparently we can configure retries https://docs.aws.amazon.com/AWSRubySDK/latest/AWS/Core/Configuration.html thanks @jbeemster . Should be doable through a shutdown hook. btw, which platforms are you running on? I'm asking because Kinesis has something like this in place: https://github.com/snowplow/snowplow/blob/aaed36e24b3facd6048d5a71b3b44be27062fa2c/2-collectors/scala-stream-collector/kinesis/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/sinks/KinesisSink.scala#L64-L70. Incorporated into #3965 , thanks a lot! :+1: . https://maven.twttr.com/ is down for now so we'll have to delay publishing the version of spark enrich the rest is good to go however.. hey @mirkoprescha , sorry I force pushed your commit, could you open a new PR?. Also, I think it'd be useful for stream enrich too, wdyt?. Thanks a lot!. hey @mirkoprescha , when do you think you will have time to open the pr for stream enrich? I'm asking because we're hoping to do a release next Wednesday.. ok thanks I'll have a look :+1: . should be accompanied by a corresponding specific schema. hey @tonicebrian , it's on our list to review :+1: . It seems like wiremock is a test dependency :(. thanks, closing. relevant info https://doc.akka.io/docs/akka-http/current/routing-dsl/directives/coding-directives/decodeRequest.html. Thanks a lot, really appreciated! :+1: . same for https://github.com/snowplow/snowplow/blob/dd8eec21d75da7ba56f2a2ee37971c981d1b9ac9/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/adapters/registry/MailchimpAdapter.scala#L169-L173. https://snowplow.zendesk.com/agent/tickets/13808. yes that's already the case. Hello yes, we have old artifacts that are only available in our repo: https://github.com/snowplow/snowplow/blob/dd8eec21d75da7ba56f2a2ee37971c981d1b9ac9/3-enrich/scala-common-enrich/project/Dependencies.scala#L28-L29\nThese repos are also in stream enrich: https://github.com/snowplow/snowplow/blob/dd8eec21d75da7ba56f2a2ee37971c981d1b9ac9/3-enrich/stream-enrich/project/Dependencies.scala#L19-L20\nCould you expand on what you want us to do?. We are moving as much as possible to central (through bintray) but old artifacts are still in http://maven.snplow.com/releases/. Maybe remove this, since this is not project-dependent.. Maybe remove Scalding and replace 2.4 by 2.0.. might as well remove it. Do we still need those repos?. Maybe remove the unnecessary parens.. Shouldn't this be snowplow-spark-enrich?. Shouldn't the SparkSession be stopped at the end?. I left the old code in place, this is an artifact from the change of package #3036. Yup, I've had issues with this in the past as in spark-kafka-writer for example.\nThough, tbh, I haven't tried with those particular tests.. I don't have a strong opinion for either.\nThis approach just seems more compact and gets rid of the import clutter faster imho.. It isn't required, without it everything is logged to stdout and clutters the test output however.. I was afraid it might be a bit too much since we only need 4 config values.\nI was thinking we might pass them as command line arguments as was the case with the scalding job, what do you think?. Haven't found out why this is happening, yet.. Same here, given that the json4s version didn't change I don't why the default date formats would have changed.. Might be that as I'm on java 8. Couldn't this be a val?. Moved the job name here since it's the same for both enrich and shred.. Moved hadoop es here as well since it's in storage. @chuwy curious as to what you think about that, that would remove the length of the beginning of the tag being hardcoded?. Do you think it's worth it to modify the other scripts in the same fashion?. They're not pulled in so yes. AFAIK, sce, collector-payload-1 and snowplow-thrift-raw-event are not published on central.. It brings toDF below. The only way to have them once per worker is to call the singleton inside the lambda.. Because it's specific to this particular format. Because it's used only once and here. All executors are multithreaded so yes. I'm wondering what was the intent of this in the original job since it wasn't used after caching the files. Welp, I guess I'll need to revert back to what was done before since this works a bit differently: it distributes those files on each node and then those files can be accessed inside Spark but I'm assuming the enrichments use HDFS' DistributedCache to access them?. We might yes, however I'm not familiar with how those particular enrichments leverage Hadoop's DistCache and what migrating away from it would entail.. My concern is how would the ip lookup enrichment find where the files are stored?. yup, you're right. I'm too used to always having it around. Good idea to factor that out inside its own function, will do.. Welp the problem with my approach is that I don't have any control over where the files will be stored on each node (/tmp most likely), Spark handles it for me.. I don't know I've just adapted what was already there.. Should we incorporate those changes to this PR?. Ok, I'll work on that on Monday.. Thanks for pointing it out :+1: . Don't really know what this is used for, would love some guidance.. @alexanderdean Thanks, so this should have been deploy_hadoop_event_recovery.sh that's what I thought.. nicely spotted, thanks :+1: . @chuwy This lines gives me:\nNoMethodError:\n       undefined method `map' for nil:NilClass\n       Did you mean?  tap\n     # /home/vagrant/.rvm/gems/jruby-9.1.7.0/gems/iglu-ruby-client-0.1.0/lib/iglu-client/resolver.rb:71:in `parse'\n     # ./spec/snowplow-emr-etl-runner/linter_spec.rb:35:in `block in subject'\n     # ./spec/snowplow-emr-etl-runner/linter_spec.rb:40:in `block in (root)'. From what I understand between those two versions of fog the project got splitted in multiple gems and requiring fog requires everything.. good idea. And Elasticity requires fog.. Yup I'll do that, I went this route not to have to supply a resolver file to generate an emr-config file but it doesn't really seem worth it.. The problem is that using symbolic keys, the json fails Avro validation.. It does seem to be the case, yes. Building on your remark, I think I'll open a PR against elasticity specifying that we only depend on fog-aws that'll avoid all those unnecessary deps.. opened rslifka/elasticity/pull/140. forgot to remove those apparently. never tested it with 5.5.0 since it wasn't out when I wrote all this but I see no reason why it wouldn't. woops https://github.com/snowplow/snowplow/pull/3203/files#diff-d7de3e27e5cf9941de4e37b1373b3bf3R403. is it just regarding sce, enrich and shred? if not I think it's better to push back and do everything at once. I don't like having the return type being the first thing one the line, just a matter of personal preference.. ~~funny how sbt doesnt complain~~ :fearful: \n. it was you! :laughing: https://github.com/snowplow/snowplow/blame/937bd51ad2f756462f46584afee4e369ae82b7a8/3-enrich/scala-common-enrich/src/test/scala/com.snowplowanalytics.snowplow.enrich.common/utils/shredder/ShredderSpec.scala. How did it work before?. Because for the other jobs I'm gonna need to create issues. @chuwy could you help review I just made the tests pass but I'm not really aware of the implications?. same, it seems toString changed for DateTime between java7 and java8 regarding the tz :fearful: . @chuwy nvm the differences came from the env (vagrant vs my machine) and not java8. Mmh I can't reproduce your issue.. Other than rdb and spark enrich, here's the list of projects that need cd 3-enrich/scala-common-enrich && sbt +publishLocal && cd ../..:\n\nstream-enrich\nscala-stream-collector\nkinesis-elasticsearch-sink\nhadoop-elasticsearch-sink\n\nDo you see anything else?. I'm wondering why this why needed / not here before.. wouldn't it be better to have a ssl: boolean = false?. I haven't found a better way to maintain this mapping yet if someone has any ideas they're welcome.. those links won't work. ?. link won't work either. do we need that since it's not cross-built with 2.10?. missing return type. might be worth to System.err.println those, no?. these two lines should say 0.11.0, right?. can't this be simplified?. can we move this next to getCaseObject?. should be Sink.NSQ. shouldn't localhost be replaced by some configuration parameter?. please refactor this into a function, no need to duplicate the code twice. what about closing the producer?. please remove the unused imports. those changes will be taken care of by separate tickets: e.g. #3311. please try to keep the dependency changes to a minimum, they will be taken care of somewhere else. same shouldn't the host be part of the configuration?. and what about closing?. I don't think this is used. the scaladoc doesn't have the same indent than the method.. we can refactor this in a method too. you can refactor this as:\nscala\nval nsqCallback  = new NSQMessageCallback {\n  // ...\n}. should we specify the charset in getBytes(). is there a release incoming? I'm uneasy at the idea of having an RC as dependency.. let's leave this as is since people might not be familiar with hocon's duration format. ? https://github.com/snowplow/snowplow/blob/master/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors/scalastream/sinks/KinesisSink.scala#L206-L220. empty lines. let's leave the copyright. being sent to a sink. ?. ?. ?. lookup. ?. please don't change library versions if you don't need to. this will be taken care of separately. why name those properties nsqd- why not just nsq-?. formatting. @chuwy flagging this as I'm not entirely sure of the order of the steps. I had removed it and had to add it back. nothing regarding subcommand unfortunately. There wasn't Spark enrich/shred at the time. Anyway the generate command needs to support r89 and r90.. this is handled by df-runner: https://github.com/snowplow/dataflow-runner/blob/master/src/utils.go#L25-L41. logged #3359. could we refactor this as:\nscala\ndef getProprertyValue(ee: EnrichedEvent, property: String): String =\n  Introspector\n    .getBeanInfo(ee.getClass)\n    .getPropertyDescriptors\n    .find(_.getName == property)\n    .map(p => String.valueOf(p.getReadMethod.invoke(ee)))\n    .getOrElse(UUID.randomUUID().toString). empty line. I'm keen on treating an empty partition key as None. I don't think this has any benefit over a random UUID since specifying no partition will result in a round robin and hashing UUIDs will result in hopefully the same thing.. mmmh, the record-limit configuration represents a number of records, batch.size is a number of bytes, I don't think it really applies here.. could you leave the config as is?. this will be handled in a separate ticket. this will be overhauled in a separate ticket. this doesn't translate, same as in the other PR. please keep your changes to a minimum and modify the configuration where it originally was, ie here. this was done in #3317, sorry I wasn't aware of this PR. same. good idea :+1: . how is this different from https://github.com/snowplow/snowplow/blob/master/3-enrich/stream-enrich/examples/config.hocon.sample ?. this will be handled separately. same. please modify the configuration in place. I don't think this is necesarry either. let's keep only one empty line. I think nsq-port makes more sense given that the fact that the host config is called nsq-host. 2 instead of 1 empty lines. Let's move this line up, just below\n\n # Incoming events are stored in a buffer before being sent to Kinesis/Kafka.. missing scaladoc and let's remove the space between `NsqSink` and `(config...`. please add `override`. this will be taken care of separately. same. please don't change the dependencies other than the ones you need, this will be taken care of separately. let's rename this property to `nsq-lookup-port`. let's rename to `nsq-port`. please modify the scaladoc according to your code. empty line. why not use [`produceMulti`](https://github.com/brainlag/JavaNSQClient/blob/master/src/main/java/com/github/brainlag/nsq/NSQProducer.java#L72) ?. why not use the consumer using an error callback: https://github.com/brainlag/JavaNSQClient/blob/master/src/main/java/com/github/brainlag/nsq/NSQConsumer.java#L57?. Agreed but it is just confusing as is, let's have nsq-port and nsq-lookup-port. it's only one message sent to nsq instead of n: https://github.com/brainlag/JavaNSQClient/blob/master/src/main/java/com/github/brainlag/nsq/NSQCommand.java#L97-L109. Now that I think about it, it's pointless to repeat nsq, since the globbing config is already called NSQ, let's refactor it as:\n\n\nconfig\nNSQ {\n  host = ...\n  port = ...\n  lookup-port = ...\n}\n. What I mean is that there is a ticket dedicated to the upgrade and it's separate from the tickets you're trying to solve (e.g. #3325).\nAdding the security feature is a separate concern from updating the library.. I don't think any current buffer setting applies to batch.size, and it doesn't make sense to have a batch.size equals to buffer.memory.. Not sure what you mean but this is the convention that has been adopted throughout all projects in Snowplow so we might as well keep things coherent.. nit: could you add a space after # here and below?. could you indent this section properly?. for the 3 properties above, why aren't you using the configuration in streams, i.e.:\n\nin.raw\nout.enriched\nout.bad\n\n?. enrichAndStoreEvents gives back a boolean saying whether or not you should checkpoint which, in the case of NSQ, means acking the message.. Can't we use byte-limit as output buffer size and time-limit as output buffer timeout?. but this introduces two sources of truth and people won't know what is used so I suggest reusing everything and keeping the channel config here. what do you mean?\n\nhttp://nsq.io/clients/tcp_protocol_spec.html#identify\nhttps://github.com/brainlag/JavaNSQClient/blob/master/src/main/java/com/github/brainlag/nsq/NSQCommand.java#L44-L54\nhttps://github.com/brainlag/JavaNSQClient/blob/9f5d767eb5e3495ebc032eae711fba7929fed132/src/main/java/com/github/brainlag/nsq/Connection.java#L72. I don't think it really applies to anything since the buffer settings are producer-related and not consumer-related so they shouldn't be in stream-enrich either.. What happens if we have two location headers because of https://github.com/snowplow/snowplow/pull/2755/files#diff-326d86609fbd1260182e8d32c3e165a0R231 ?. could you we modify the documentation accordingly?. could we remove this option as a result of your changes?. yup this is done locally. fixed locally too. will fix. can we have camel case here like below?. ?. grpc-netty above isn't used here. this import doesn't seem used. same. why a def here?. same. let's just have:\n\nscala\n} catch {\n  case e: com.google.api.gax.grpc.GrpcApiException => false\n}. let's do:\nscala\nsubscriptions.size > 0. let's do\nscala\nval topic = TopicName.create(config.googleProjectId, topicName). let's rename this to topicExists. I would move this check to the class level:\nscala\nclass PubSubSink(config: CollectorConfig, inputType: InputType.InputType) extends AbstractSink {\n  // ...\n  private val topicName = ...\n  private val topic = TopicName.create(config.googleProjectId, topicName)\n  require(topicExists(topic), s\"Topic $topicName doesn't exist\")\n  // .... should this be uncommented?. the curlies are unnecessary. missing return type. can't we extract something from publish to know whether or not the write succeeded?. what's blocking the release of 1.1.0?. this can be on one line. can't we use the recordLimit like in the loaders?. true that in the case of NSQ, we're only buffering incoming events, let's leave it as is. then let's go ahead and release it. could you reformat the imports like in CollectorService\n\njava imports\nscala imports\nother libraries imports\nsnowplow-related imports. This gives the possibility to have kafka as input and kinesis as output or vice-versa, I think that's why it's there in the first place, no?. I've tried to only make the necessary changes in this release, but yeah they need a fresh coat of paint.. yup tried not to refactor the sinks/sources too much, but I'll make that change. I think the first one can be removed, however I think the ua parser is needed if you haven't compiled sce before? I don't think resolvers are transitive.. what do you think would be a good name?. not sure which adapter you're referring to? https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/adapters/AdapterRegistry.scala#L80. yup good idea :+1: . Apparently I had check that since both are needed:\n\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: org.clojars.timewarrior#ua-parser;1.3.0: not found\n[warn]  :: com.snowplowanalytics#snowplow-thrift-raw-event;0.1.0: not found\n[warn]  :: com.snowplowanalytics#collector-payload-1;0.0.0: not found\n[warn]  :: com.snowplowanalytics#schema-sniffer-1;0.0.0: not found\n[warn]  :: com.snowplowanalytics#referer-parser_2.11;0.3.0: not found\n[warn]  :: com.snowplowanalytics#scala-maxmind-iplookups_2.11;0.3.0: not found\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::. this commit isn't going to be in #3438, I'll do it myself.. should we make ${SP_UUID} configurable?. could you move this test to https://github.com/snowplow/snowplow/pull/3491/files#diff-88f19e201a23cbae2294497905a6b8a1R201 , you can make a local copy of the event if there are issues with setting the networkUserId. let's remove those unnecessary curlies. same here, let's remove those curlies. why not just map? what am I missing?. why do we discard a failed parsing?. can't we refactor out some of that logic? that'll make things clearer I feel. let's remove those unnecessary curlies here too. I feel like the eventWithData manipulation could at least be extracted, no?. same getMessage here. let's use https://github.com/snowplow/snowplow/blob/master/2-collectors/scala-stream-collector/src/test/scala/com.snowplowanalytics.snowplow.collectors.scalastream/utils/SplitBatchSpec.scala#L36. iirc the runner tells you why it didn't run, needs to be verified though. can we leave this as it was?. nit: missing space [String, String]. the indent seems off. let's refactor this as:\n```scala\nif (bounce) {\n  val forwardedScheme = for {\n    headerName <- bounceConfig.forwardedProtocolHeader\n    headerValue  <- request.headers.find(_.lowercaseName == headerName.toLowerCase)\n    scheme         <-\n      if (Set(\"http\", \"https\").contains(headerValue)) {\n        Some(headerValue)\n      else {\n        logger.warn(s\"Header $headerName contains invalid protocol value $headerValue.\")\n        None\n     }\n  } yield scheme\nval redirectUri = request.uri\n    .withQuery(Uri.Query(queryParams + (bounceConfig.name -> \"true\")))\n    .withScheme(forwardedScheme.getOrElse(request.uri.scheme))\n  Some(Location(redirectUri))\n} else {\n  None\n}\n``. can you use=` as above?. can't we refactor those 2 nested failure/success into a for comprehension?. unnecessary parens. if we want to be consistent with other parts of the codebase this will look like:\nscala\nval bodyMap = getBoundary(ct)\n  .map(parseMultipartForm(body, _))\n  .getOrElse(toMap(URLEncodedUtils.parse(URI.create(\"http://localhost/?\" + body), \"UTF-8\").toList)). no need for _ match {. empty lines. no need for brackets, they just clutter the code. let's only go with NonFatal here. white spaces to rm. let's do\nscala\ncase (Some(body), _) if body.isEmpty => ...\ncase (Some(body), _) => ...\nthis will be more readable. for comp will be more readable here too. useless parens for the 3 lines above. useless parens. pls try to limit brackets here and in the other files too. NonFatal. here and for the others too, can't we move the try/catch closer to what can actually throw an exception, I'm guessing the parse?\nAlso can't this logic be factored out? I feel like I'm seeing the same logic in all the adapters. do we need a map here?. empty spaces. @alexanderdean in the end it's only clj-tomcat since cf and ndjson are lzo-consolidated above and thrift is already lzo.. ah my bad I thought it was some weird comment formatting. mmh true, I'll modify. let's go with true. let's go with some, that'll remove those weird alignments @chuwy was talking about. also can we have style = default at the top?. could we add AsciiSortImports?. can't we refactor those nested matches as:\nscala\n            // ...\n            case Some(eventType) =>\n              for {\n                event <- payloadBodyToEvent(bodyMap).leftMap(_.wrapNel)\n                schemaUri <- lookupSchema(eventType.some, VendorName, EventSchemaMap)\n              } yield NonEmptyList(RawEvent(\n                api         = payload.api,\n                parameters  = toUnstructEventParams(TrackerVersion, params, schemaUri, cleanupJsonEventValues(\n                  mutateMailgunEvent(event),\n                  (\"event\", eventType).some, \"timestamp\"), \"srv\"),\n                contentType = payload.contentType,\n                source      = payload.source,\n                context     = payload.context\n              ))\nthis will be way more readable imo. can't we take the try/catch closer to what can actually throw?. same as before:\n\nfor comp\ntry/catch closer to what can throw. pls handle those at the lowest level possible, make this an option or a Validation\n\nIt shouldn't be the caller's responsibility to blindly handle a function's shortcomings. . conflict. nit: indent. empty line. Could you order the lines according to the project similar to what's done below? Thanks.. I tend to always write override, this way you know this method was first defined elsewhere (here in the trait).\nHere it's not the case but if this was still abstract you have the added benefit of signature checking between this and the overriden method instead of defining a new method.. good point \ud83d\udc4d . Those are extremely minor syntax changes which do not warrant a specific commit. I just made them when browsing through the other adapters looking for inspiration.. yes this is used for currency-related fields. mmh interesting point, I'll look into it. went with a regex approach. can we find a way around those 4 nested flatMaps?. why do you need pattern matching if there aren't any alternatives / type checking?. flatMap {, no need for parens when pattern matching. only flatMap is defined on ValidationNel \ud83d\ude28 . not Either because I don't want to deal with the not right biased version in 2.11 and I don't like the infix notation of scalaz \\/ but I guess that's just preferences. no because there is buildJson, I don't want to filter and then map. good point. Plus, it might be easier to convert \\/ to Either with scalafix if it's not infix once we move off scalaz :D. good point I'll make it clearer and reformat. Or this can go into the prep for rel commit since it's just housekeeping. changed to half even \ud83d\udc4d . nit: empty line. can we have defs instead of vals, that will leave the most freedom to the implementer. does this need to be public?. can we have the SchemaCriterion as a parameter to make this more generic?. can we make this more readable, e.g.:\nscala\ncase JObject(List(\n  (\"json\", JObject(List(\n    (\"field\", JString(fieldName)),\n    (\"schemaCriterion\", JString(schemaCriterion)),\n    (\"jsonPath\", JString(jsonPath))))))) =>. what happens if the object structure doesn't fall into those two patterns?. I'm not sure I understand \"and there being only one implemented strategy\" in the context of the sentence. can we specify the return type?\n. no need for parens. wouldn't this be more readable as:\nscala\nval strategyApplication =\n  (v: Validation[String, String]) => v.flatMap(applyStrategy(_).success)\n(inputField,\n  ((a1: String, a2: String) => (tf.tupled andThen strategyApplication)((a1, a2)),\n    outputField)). typo. same thing below for json. It might be worth it to explore extending TransformMap to act directly on JValues for json fields, that would avoid this parse/render stuff but that might be outside of the scope of this PR. let's rename to jsonPathReplace or something else, the first letter being in caps is confusing. typo. can we have more descriptive variable names?. should the charset be an arg?. why is this a def?. I think we were trying to keep those aligned, at least except the last one. comments are generally //{{space}}Content as you wrote below. weird spacing. I would flesh this out saying you're matching according to the schema criterion too or create a different test. what's the reasoning for not failing enrichment here? is it because of additional properties?. can we factor this out in a variable, it seems to be repeated a lot. weird spacing. nonfatal. I agree, it's just this way it's just one extract away from making its way into an utils package if someone has the same need as you in the future. then yes make it sealed and its descendants final.. please always specify the return type even for private functions.. Couldn't we have been more granular into what went wrong with the different extractString?\nEven if we don't accumulate errors, I think the original message might be more useful than Configuration file has unexpected structure.. couldn't fieldsDisj be a map which you can reference to with fieldsDisj.get(\"schemaCriterion\")\nI don't feel good about directly indexing a list.. isn't it just map? same remark as above about directly indexing the list. could this be moved to the top of the object?. couldn't we extract this SchemaKey.parse... into a function, that will make things more readable.. currentValue is Any and the fallback is on AnyRef, what happens on AnyVal?. I don't think you need to specify scala in scala.Any. shouldn't this be fromTryCatchNonFatal?. did we solve this type erasure issue?. yup, turning it into a map would be good as I said above.. why isn't currentValue AnyRef then?. ? the quesiton is why are you discarding the MappingException from extractString and optionally why are you not accumulating those errors?. This is for pojo and json now, no?. I have trouble understanding why you have to go through all this, isn't the above equivalent to:\nscala\ncontext.toMap.filterKeys(List(\"schema\", \"data\").contains). return type. curious if scalafmt is going to catch this, this is just a marker for me for later :D. no it's about indentation {}, not being on the same level as (). it would be even better to extract out the List or do s => s == \"schema\" || s == \"data\", that would avoid having to create n lists.. I kinda prefer scalar mutator but I don't mind either way.. can we use 1.0.1? It's been released since this commit.. can we fix this in the end?. can we reduce this so that it at least fits github number of columns (I believe it's 120) or even lower? that will ease reviews. can we exclude this type of alignment? doesn't make a lot of sense to me, if we can't that's fine.... I don't understand why sometime => are aligned and sometimes members of the patmat are aligned, can we prioritize one over the others?. could we move Modif_i_edField's declarations up, that will make things easier to understand. Once we move to 2.12 and make EnrichedEvent a case class, it might be interesting to abstract all this away by treating an EnrichedEvent as a partial function maybe through shapeless.\nAnyway just food for thought.. can we break this down? this will make it easier to understand. can we break this down, e.g.:\nscala\n(modifiedJson, modifiedFields) = jsonPathReplace()\n// ...\nyield {\n  val updatedFields = fieldsObj\n    .updated(\"schema\", schema)\n    .updated(\"data\", modifiedJson)\n    .toList\n  (JObject(updatedFields), modifiedFields))\n    .getOrElse((JObject(context), List.empty[JsonModifiedField]))\n}\nyou might find better variable names than above. can you explain your choice in a comment. this comment might need refreshing given the function seems to have changed significantly. can we make all those final? I don't feel good about publicly-exposed non final case class. can we restrict those serializers' scope?. why do we need another formats inside the serializer?. same thing below. can't we extract this data wrangling from the json construction? this will make thing easier to understand. also can we turn this into a fold, that'll be more performant. I meant moving https://github.com/snowplow/snowplow/pull/3618/files#diff-326e31f4255bc69c2c5c60cd68ec4104R510 up in the file.. Have you read:\n\nI reliably get those results https://gist.github.com/BenFradet/13c4a33ea6c0a850de8728f4cca3d82c\nanyway doing one pass instead of two over the data is preferable imo\nplus you avoid creating useless stuff. Anyways ... you can even save another pass by doing:\n\nscala\npmf.modifiedFields.foldLeft(Map.empty[String, List[ModifiedField]]) { case (m, mf) =>\n  mf match {\n    case s: ScalarModifiedField => m + (\"pojo\" -> (s :: m.getOrElse(\"pojo\", List.empty[MF])))\n    case j: JsonModifiedField => m + (\"json\" -> (j :: m.getOrElse(\"json\", List.empty[MF])))\n  }\n}. you can do:\nscala\nval (r, _) = collectorService.cookie(qs, Some(body),\n  path, cookie, userAgent, refererURI, host, ip, request, false, Some(ct))\nincrementRequests(r.status)\ncomplete(r)\nsame thing below. can we move this next to incrementRequests so we know what it relates to. you can import monitoring.BeanRegistry\nalso we try to separate internal from external imports by a newline, e.g. https://github.com/jspc/snowplow/blob/30d01e8909ffb76239759bdedb70ee473cecd4d0/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/Collector.scala#L27-L29. this was fixed afaict. doesn't scalafmt handle reordering imports alphabetically?. can be a scalar. is this scalfmt's doing? looks weird to me. can we comment why this configuration was chosen, same below with SUPPRESS_EXCEPTIONS. no I meant the \"wither\" typo. ah ok then we might look into something that does it for us too. nit: weird indent / spacing. not the right lines. I feel like this is checking the list, not the content of each element?\nAlso, why would it contain \"pii\"?. sure :+1: . we've standardized against enabled-style in #3556, it'd be cool if this new feature could follow this guildeline too :+1: . I think we can have 404 as default for statusCode and Map.empty[String, String] for headers, wdyt?. you should be able to do:\nscala\nheadersMap.map { case (k, v) => RawHeader(k, v) }.toSeq. can't the content-type be specified through the headers anyway?. this is just inherited from the scalding code, I don't have any reason. yes there are a few tickets already open: https://github.com/snowplow/snowplow/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+buildinfo. what I wanted was to write the minimal amount of code that we'd have to throw away once we migrate to 2.12 where we can just replace every \\/ by Either but I do agree.... that's just how fold works. does it just ignore max columns if it's aligning? if so that's hilarious. can't we extract all this kafka setup out? That'll make it easier to reuse. can't we use Either?. isn't this just Option(event.pii).filter(_.nonEmpty) ?. can we add additional isDefined checks so that if it fails we'll have an idea why?. can't we just poll once for timeoutSec, also why do we need a future, we can just have this be synchronous it's tests?. why is to[immutable.Seq] needed?. no need for the if (c.headers.nonEmpty) btw. on second thought, I don't think we need default values since config parsing will make sure everything is in order. I mean producer, getConsumer, getRecords, etc. >I think using tests like that more could be great as they will improve coverage.\nThat's exactly what I meant. So what would be easiest would be to extract all this into a trait so that a spec can just extend it and have kafka integration.. mmh weird, I'll have a look. yes, so there are two levels to this issue:\n\nwe've opted out of maintaining backwards compat for the config in #3556 so RootResponseConfig can't be an option itself\ninside RootResponseConfig, we can have options but in this case I don't think having options / default values for everything makes sense? maybe for body and headers but not for statusCode, wdyt?\n\nI'm generally against options inside individual configs as this might result in hidden behaviours.. > As it stands, people do need to provide full configuration of the cookieBounce, cookie, redirectMacro, etc even if they don't want to use this feature, which is kind of a pain point.\nI agree but it makes it harder to template as discussed in #3556. I think it's a bit too ambiguous to have both.. Fixed it with:\nscala\nval rawHeaders = c.headers.map { case (k, v) => RawHeader(k, v) }.toList\nSeq are truly evil :(. we use camel case. this can be:\nscala\n      val eventType = (parsed \\ \"type\").extractOpt[String].getOrElse(\"user_updated\")\n      val formattedEvent =\n        if (eventType == \"user_updated\") reformatParameters(parsed)\n        else cleanupJsonEventValues(parsed, (\"type\", eventType).some, s\"$eventType_at\"). isn't reformatParameters unnecessarily called twice when the event type is user_updated?. I don't think we need to wrap the whole thing in a try/catch but just parse? or am I wrong (cc @chuwy). can this fail? if so the result should ideally be a Validation[E, JString]. no need for package private here. could we limit the vertical alignments to circumvent this issue?. unecessary parens here and above. weird formatting :'(. empty line. shouldn't this be called produce?. you're missing the return type too, that'd be cool to make it a rule btw. shouldn't this be called getRecords, missing return type. nice :+1: . no need for the import anymore. scala\nval parsed = Try(parse(bodyJson)\nval eventType = ...\ninstead of\nscala\ntry {\n  val parsed = parse(bodyJson)\n  val eventType = ...\n} catch { ... }. can't we do away with the Try / Await.result as it defeats the purpose of having futures in the first place?\nwe can maybe let specs2 do the matching directly on futures?. You're building a future to just await its results synchronously so you lose the purpose of the future (async computations) and you wrap the result in a try, losing Failure / Success resulting of the future.\nYou might as well do without the future if you're doing synchronous computations wrapped in a try / catch.\nDo you see what I mean?. yes since specs2 doesn't do async testing apparently.... that should disappear with the rebase. should we add eu-west-3?. nice :+1: . reminder to change that. nooooo! not sluice :smile: . mmh why not. I just checked that too :). the result of the matchmaking is discarded, it should be:\nscala\nval parsed = Try(parse(json)) match {\n  case Success(parsed) => parsed\n  case Failure(e) => s\"$VendorName event failed to parse into JSON: [${e.getMessage}]\".failureNel\n}. you should be able to do:\nscala\nval eventType = parsed.toOption.flatMap(p => (p \\ \"type\").extractOpt[String]).getOrElse(\"user_updated\"). you should be able to do:\nscala\nval formattedEvent = parsed.map { p =>\n  if (eventType == \"user_updated\") p\n  else cleanupJsonEventValues(p, (\"type\", eventType).some, s\"$eventType_at\")\n}. you can chain things:\nscala\njson.transformField {\n  case (\"triggered_at\", JInt(value)) => (\"triggered_at\", toStringField(value.toLong * 1000))\n}.removeField {\n  case (\"action\", _) => true\n  case _ => false\n}. In general, it's good practice to avoid .get. I think we're good. you don't need to return early you just need to carry on with your failure, this is why there is a parsed.map in my comment below because parsed will have type ValidationNel[String, JValue].\nby the way, the comment above should say case Success(parsed) => parsed.successNel, that's my bad.. it's in the checklist of the pr above to rm it once an iglu central release has gone out. cant hurt :+1: . yup, you can also do eventType.some too\nHere you don't need it to be an option and you can have it be a simple string so it's better to go towards the simpler type. you can do a for comprehension:\nscala\nfor {\n  parsedEvent <- Try(parse(json)) match {\n    case Success(p) => p.successNel\n    case Failure(e) => s\"$VendorName event failed to parse into JSON: [${e.getMessage}]\".failureNel\n  }\n  eventType = parsed.toOption.flatMap(p => (p \\ \"type\").extractOpt[String]).getOrElse(\"user_updated\")\n  formattedEvent =\n    if (eventType == \"user_updated\") parsedEvent\n    else cleanupJsonEventValues(parsedEvent, (\"type\", eventType).some, s\"$eventType_at\")\n  reformattedEvent = reformatParameters(formattedEvent)\n  params = toUnstructEventParams(TrackerVersion, toMap(payload.queryString), schema, reformattedEvent, \"srv\")\n  schema <- lookupSchema(eventType.some, VendorName, EventSchemaMap).toValidationNel\n  rawEvent = RawEvent(api = payload.api, parameters = params, contentType = payload.contentType, source = payload.source, context = payload.context)\n} yield rawEvent. nit: could you add override here?. mmmh, sce can't really depend on aws unfortunately.. can't we just have those lines as: DigestUtils.sha512Hex(_: Array[Byte]).success?. can't this fail?. can't this function return listOfModifiedValues instead of passing and mutating modifiedFields?. why can't we move everything into the pii object? package object are ideally to avoid since they'll be removed. i think we need \"--outputCodec\" << \"none\" in both modes.. you're missing a couple of tests for archive_raw and skip enrich also nit: to raise_exception(Config.... I thought this had been parked into its own branch?. can we split this function into smaller chunks?. isn't it named spiders_and_robots? https://github.com/snowplow/iglu-central/pull/767/files#diff-5355420c0aced6f5c3e3134aa3c1f278R6. same. coordinates seem wrong here too. the imports are:\nscala\nimport scalaz._\nimport Scalaz._. unnecessary parens. no need for a for comp if there is only one line. this is flatten, no?. you can directly do:\nscala\n.flatMap {\n  case ...\n}. seems to be worth it to introduce a case class for this triple. seems to be the wrong coordinates here too. it's worth introducing a test which references not existing files. I think this is unsafe with the scalaz version we're using as it will also catch exceptions that are not NonFatal. getMessage. this is too light testing. integration tests are usually done in stream enrich or spark enrich. woops my bad. I would rename it to integration-tests. weird indent. is this needed since it depends on the kafka module?. this will need to be refactored after 105. unnecessary parens. one thing I missed during the 1st review is, should we check for the sizes of the pii events separately from the usual enriched events?. why is this needed? it should either be all env vars or no env vars?. please modify the comment above \"Configuration shown...\". can't we use the default one. why is future used?. can't those properties be extracted, no need to recreate one every time we call produce. why are you using a future if you're discarding the result of the only async code here?. empty line. yes let's leave it as is ... as discussed it can be useful when testing and using a specific registry without having to modify the code itself. no need for curlies. scala\n(uri.toValidationNel |@| db.toValidationNel) { (uri, db) => getDatabaseUri(uri, db).toValidationNel.map(u => (name, u, db)) }\nmodulo type inference. ah yes, you can use identity instead of x => x. I would shy away from doing path handling using string concatenation, what if uri has a / at the end? does ConversionUtils handle it?. mmmh even if this is only for tests maybe we should make it safer wrt to npes. should this be occuredAt?. what if it's not an ip address? I'd rather we correctly type our failure states instead of blanket wrapping in try/catch. can we keep the types further? i.e. no toString here. same here, this is our code, we shouldn't need try catch at this level. This type alias doesn't provide any benefits as we can't infer what's inside from the name. occurred even. this example doesn't have two trailing slashes afaict. then I think it's a typo in the client since accurateAt being a DateTime doesn't make sense to me?. yes. I'd mention that this example is dedicated to the clj collector. space. weird indent. could we have a test that is a bot?. normalize doesn't deal with the case I meant, doing string concatenation on paths can result in:\n```scala\nval uri = \"http://google.com/\"\nval databse = \"database\"\nuri + \"/\" + database\n// http://google.com//database\nConversionUtils.stringToUri(uri + \"/\" + database)\n// Success(Some(http://google.com//database))\nnew URI(\"http://google.com/\").resolve(\"/\" + database)\n// http://google.com/database\n```\nDo you see what I mean now?. let's add a test incorporating my remark above. please remove before release. can we have the two iab specs turned into one spec? No need to have two different files for the same enrichment.. Cbor is only needed by Kinesis iirc, no?. you should be able to do:\nscala\ncustomEndpoint.getOrElse(region match {\n  // ...\n})\nsame thing below. I'd say Optional endpoint url..., same thing below. nit: I'd remove basic auth or change the uris to be http to make the example coherent. One thing I missed before, at this point event.user_ipaddress may have been modified by anon ip, no?. I think this should be defined in the enrichment registry, I logged #3789. Of course, it's out of scope for your release.. don't forget to rm. why is this a val?. this is just map, no?. ?. that was meant to add a / at the end of uri to check uri resolving works properly. I don't see this https://github.com/snowplow/snowplow/blob/master/3-enrich/stream-enrich/core/src/main/scala/com.snowplowanalytics.snowplow.enrich.stream/Enrich.scala#L216-L243 being modified.. you don't need that since it's already been validated?. same here?. nvm I thought https://github.com/snowplow/snowplow/pull/3766/files/81d0b8248a4fcfee0d5438262b63c61b91d169fe..230cb00266b95fbac7a7eaa511b8c35d9af60d36#diff-b9d72942ccb9985afcce223303adf230R35 was used. I wouldn't call when creating it either. the word start was missing from the above sorry, what I meant was:\n```scala\nobject NsqSink {\n  def validateAndCreateProducer(nsqConfig: Nsq): \\/[Throwable, NSQProducer] =\n    new NSQProducer().addAddress(nsqConfig.host, nsqConfig.port).right\n}\nclass NsqSink(...) {\n  producer.start()\n}\n```. Excellent :+1: . this will need to be version 2. This should be a validation based on the result of:\nscala\nelse {\n  for {\n    input <- Validation.fromTryCatch(new FileInputStream(filesToCache.head._2))\n   parser <- try {\n     new Parser(input).success\n    } catch {\n      case NonFatal(e) => e.failure\n    } finally {\n      input.close()\n    }\n  } yield parser\n}. actually I think we shouldn't use fromTryCatch at all since it also catches fatal errors in the version of scalaz we use. Can we not use fromTryCatch here too?. this makes me nervous, can't we better validate those cases?. I'll log a separate ticket. I've actually renamed it to enriched to fit https://github.com/snowplow/snowplow/blob/master/3-enrich/stream-enrich/examples/config.hocon.sample. I thought so but apparently not. good point, I added a comment to the constant saying: \"We want to take one-tenth of the payload characters and one character can take up to 4 bytes\". there are two different methods because for enriched events that are too big we already know their size whereas for bad rows we don't. woops, wrong comment. good point, I added a comment to the constant saying: \"We want to take one-tenth of the payload characters and one character can take up to 4 bytes\". I think it's pretty cool to have a repl to try out SCollection operations. the problem here is that you are recompiling the regex for every request which might non-trivially impact performance, the solution would be to extract the computation. feel free to reach out if you need further help. You can modify the configuration to compile the regex directly.\nThen doNotTrackCookie won't be an Option[HttpCookie] but a case class you will have defined, e.g. Cookie(name: String, value: Regex).\nDoes that make sense?. should the content type be an additional parameter?. no need for brackets :+1: . should this be 0.10.1, to mirror postgres?. please modify the existing config: https://github.com/snowplow/snowplow/blob/master/2-collectors/scala-stream-collector/examples/config.hocon.sample. should this be optional?. @chuwy so should we change postgres?. no need to change the version, this will be handled separately :+1: . can you add the apache header to the different files you've added? :+1: . no need for final in objects :+1: . why not just toSeconds and getting rid of the division?. also if you could add Scaladoc to the different public members that'd be much appreciated :+1: . Isn't there a better metric type than gauge for those metrics?. no need for all this reflection, you can leverage a sbt plugin we're already using to expose this build info: https://github.com/sbt/sbt-buildinfo. this can just be https://github.com/snowplow/snowplow/blob/08d2f51b44148169427e5f423f7ea4dd489af9e5/2-collectors/scala-stream-collector/core/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/CollectorService.scala#L73-L74 no need for the jar name. should this section be named prometheusMetrics?. metics. this needs to be uncommented otherwise configuration parsing won't work :+1: . let's use camel case :+1: . The problem is that I don't think neither nanos nor seconds are useful as a request duration unit of measurement. In my mind milliseconds would definitely be better suited, wdyt?. ah right, I disregarded it was an option. can we avoid this get?. No need for the reflection for both the java and scala versions. :art: can you leave the imports as they were?. I think this should be should include. same. problem is that elasticity is way out of date. this makes me really nervous and should be treated on its own c.f. #3691. what happens if there is a cluster satisfying the conditions in let's say both the first and second pages?. do we want a specific error code like https://github.com/snowplow/snowplow/blob/9a866b459ef993ec750d0fe588470e5c7a461f9c/3-enrich/emr-etl-runner/bin/snowplow-emr-etl-runner#L93-L103. can't we do @run_tstamp = Time.new?. I meant combining the two lines into one. is the double indexing safe to do?. should we specify a 0 exit code in this case?. does this work with command-runner? it's not listed in https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-commandrunner.html. could we combine those as one step?. or just rm /local/snowplow. :art: no need for quotes compression.type = snappy. you should be able to leverage putAll no?. maybe we should mention the format. should we check no steps are running?. also can we combine the ifs. Could you replace the nulls with nones?. compression.type is not a consumer configuration. ah yes, too bad :/. we might want to specify the configuration that are already used so that there is no override, wdyt?. I'm not sure I fully understand what you mean, could you give an example? either here or as an additional commit :+1: . that's excellent I think :+1: . could you leave all this as is? thanks.. this had quotes originally, that's why the tests are failing :+1: \nIf you're interested:\n\ntemplating goes on here: https://github.com/snowplow/snowplow/blob/master/3-enrich/stream-enrich/integration-tests/src/test/scala/com.snowplowanalytics.snowplow.enrich.stream/PiiEmitSpec.scala#L77\nthe example config is turned into a resource here: https://github.com/snowplow/snowplow/blob/master/3-enrich/stream-enrich/project/BuildSettings.scala#L56-L58. I think we'd want the consumers of scala-common-enrich, e.g. spark-enrich, to provide this configuration like for the enrichments. I'm not too sure about the input/output of the adapter, could we find a format better-suited for http?. In my mind the adapter could take care of transforming the input before forwarding the collector payload and then turn the result into a RawEvent when receiving it. :art: these two classes seems misaligned. :art: this seems misaligned. do you need the whole config as well?. I'm assuming there is also an async version of the client, what are the trade-offs between the two?. In your opinion, how hard would it be to build some kind of retrying logic in this function?. :art: \"azure-eventhubs-eph\" is padded one character too many to the right I think. :art: could you put eventhub before stdout so that they're all sorted by order of importance :+1: . could you give more information regarding each and everyone of those fields in a comment?. also, could you comment out the event hub config, the kinesis config is already in place, uncommented. it's true for other places too, some quotes have been unnecessarily remove. done, thanks :+1: . there is https://github.com/lemonlabsuk/scala-uri/blob/07756b8e2eb7de1318767edbc730cee6b768b736/shared/src/main/scala/io/lemonlabs/uri/Uri.scala#L556-L560. this catches everything and should be avoided. same. should we ditch relaxed and use scala-uri directly?. can we check the result?. would we be able to verify their result as well?. yes but this catches OOM, InterruptedException, etc which isn't what we want. makes sense, in this case we could split the two, one that uses java and is strict and one that uses scala-uri for non-strict usages. I'd rather do:\n\nscala\nfor {\n  scalaUri <- Uri.parseTry(uri).toEither\n  uri <- Either.catchNonFatal(scalaUri.toJavaURI)\n} yield uri\nwithout the try/catch.\nAlso I'd remove the usual URI parsing above. yes, there are equivalent with scalaz Validation for the second and you can do the first manually easily. looks good on posting the collector payload, however why receive base64 and not directly json?. yup I'd rather we go json out json in, created the repo: https://github.com/snowplow-incubator/remote-adapter-example/. ",
    "petervanwesep": "here's the problematic line viewed in sublime text: http://cl.ly/image/1f2z0Z1k3e3S\n. trying to copy and paste the string just leads to it being truncated at the NUL segment\n. Yup, good call. Didn't see that column there. I'm all set on the CLA now too, so we should be good!\n. ",
    "rbolkey": "Hi. Was asked to leave a comment here about file moves in s3. We're currently using the clojure collector, and have run into an issue that the file naming scheme for files placed in the processing folder do not provide enough precision to uniquely identify a published file. As a result, the log files overwrite each other in the processing folder, and we lose data.\nThe root of the problem is that we're needing to generate logs more frequently than 1 per hour per instance, but the file name only keeps hour precision on the timestamp. For us, if the file name could retain both minute and second precision that would prevent our need for a custom staging script.. Also running into this on AMI version 4.5.0, and getting the error that @bhattdeepak90 is seeing.. No need to merge this.. @BenFradet I made the suggested changes, and updated the documentation.. No issue changing the default. I wasn't distinguishing between the different types of identities that snowplow recognizes. The original value was simply what we've been using in production.. I updated the default token value, and rebased off master.. Yes, but to be more specific, the protocol of the URL used to connect to\nthe original load balancer is the crucial bit.\nDigging into the ELB docs, they do provide a custom header that contains\nthis information: X-Forwarded-Proto.\nUsing this header to help construct the URL would be sufficient. Would this\nbe a satisfying solution on your end?\nhttp://docs.aws.amazon.com/elasticloadbalancing/latest/classic/x-forwarded-headers.html#x-forwarded-proto\nOn Nov 17, 2017 4:15 AM, \"Ben Fradet\" notifications@github.com wrote:\nIf I understand correctly you would like the location header to contain the\nhttps url of the original load balancer?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/3505#issuecomment-345202243,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AADT6JGE9C0fGyAnpJN5upIVNNsuw9V9ks5s3VyHgaJpZM4QfeVk\n.\n. Fantastic! Is there a time frame for this? Would it speed things up if I tried to put together a pull request?. Hi Ben, I created a Pull Request. Can someone review for the next release?. Thanks @BenFradet. I've applied the recommended changes. Let me know if I missed anything.. @BenFradet fixed the example config, and rebased.. Great. I had that idea in the back of my mind as well. I've been wading through GDPR (as have you all) over the past month.\nHow close is functionality to what you all were envisioning? What's missing or should be different?. @BenFradet applied changes based on your suggestions.. @knservis changed base branch and rebased.. Not up to speed on the travis checks. Tests pass locally :-).. @BenFradet Applied those updates.. Also, not completely sure why I have so many changed files in this commit. Does it look like I botched a rebase or need to sync with the upstream?. Rebased again.. woops ... didn't realize I had changed that. I was curious if the entire HttpEntity could be stored in HOCON, but didn't push it that far.. Thanks for the pointer. I think I can switch to the case statement. The toSeq seems to have troubles due to its return type. It returns scala.collection.Seq, but HttpResponse is expecting a more specific type, scala.collection.immutable.Seq.. Will do. Ironically, that's what my first pass at this code did :-).. No issues with that.. @BenFradet I accidentally kept the toSeq in there, but the to[immutable.Seq] is necessary unless there is an implicit that I should add instead? Without the to[immutable.Seq] I get the following compilation error:\n[error] /Users/rbolkey/src/onespot/snowplow/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/CollectorService.scala:170: type mismatch;\n[error]  found   : Seq[akka.http.scaladsl.model.headers.RawHeader]\n[error]  required: scala.collection.immutable.Seq[akka.http.scaladsl.model.HttpHeader]\n[error]       HttpResponse(c.statusCode, rawHeaders, HttpEntity(c.body)). I can drop the if check. The default for the headers parameter to HttpResponse is Nil, so I wasn't sure if it was wasteful to pass an empty map.. I may need some more context on desired behavior here. If functionality it truly optional, should a user need to put any configuration at all in the config file?\nAs it stands in this commit, the rootResponse key will need to be defined in the configuration file (when the value was an Option) it did not need to be added, and the config was backward compatible. \nIf I start removing the default values, a user will need to define all of them in their configuration file even if they don't want to use this feature?\nCurrent behavior if rootResponse block left out of hocon file:\njava -jar ./target/scala-2.11/snowplow-stream-collector-0.12.0.jar --config ./examples/config.hocon\nException in thread \"main\" pureconfig.error.ConfigReaderException: Cannot convert configuration to a com.snowplowanalytics.snowplow.collectors.scalastream.model.CollectorConfig. Failures are:\n  at 'rootResponse':\n    - (file:/Users/rbolkey/src/onespot/snowplow/2-collectors/scala-stream-collector/./examples/config.hocon:20) Key not found.\nBehavior if I have an empty rootResponse block but remove the headers default value from model.scala and I don't define it in hocon file:\njava -jar ./target/scala-2.11/snowplow-stream-collector-0.12.0.jar --config ./examples/config.hocon\nException in thread \"main\" pureconfig.error.ConfigReaderException: Cannot convert configuration to a com.snowplowanalytics.snowplow.collectors.scalastream.model.CollectorConfig. Failures are:\n  at 'rootResponse.headers':\n    - (file:/Users/rbolkey/src/onespot/snowplow/2-collectors/scala-stream-collector/./examples/config.hocon:81) Key not found.. As it stands, people do need to provide full configuration of the cookieBounce, cookie, redirectMacro, etc even if they don't want to use this feature, which is kind of a pain point.. > inside RootResponseConfig, we can have options but in this case I don't think having options / default values for everything makes sense? maybe for body and headers but not for statusCode, wdyt?\nThat makes sense to me. It's not surprising for free-form Strings and Collections to default to the empty value. The status code is enum like and doesn't have a sensible default. Closest was following the previous behavior.. > I agree but it makes it harder to template as discussed in #3556\nI looked at the thread there. I didn't see anyone propose using both Option at the top level along with an enabled field? You can kind of have both pieces of cake there. Missing block implies disabled (less pain for user's configuring). Present block has enabled defaulting to true, but you can template it to false if desired? Thoughts?. That's fine. I will attest that it was an annoying nuisance to have to deal with when configuring and upgrading the collector, but I'm just one data point :-).. ",
    "jrobgood": "Same here. Error log starting to pile up!\nevents  71103   3540    refr_term   String length exceeds DDL length\nevents  70195   3540    refr_term   String length exceeds DDL length\nevents  71535   3540    refr_term   String length exceeds DDL length\nevents  70417   3540    refr_term   String length exceeds DDL length\nevents  71322   3540    refr_term   String length exceeds DDL length\nevents  70879   3540    refr_term   String length exceeds DDL length\nevents  71748   3540    refr_term   String length exceeds DDL length\nevents  70657   3540    refr_term   String length exceeds DDL length\nevents  69983   3540    refr_term   String length exceeds DDL length\nevents  69754   3540    refr_term   String length exceeds DDL length\n. Another use case for this: we wrote a system tray app a few years back and wanted to log end-user behavior without the app communicating directly to WebTrends (our client's preferred platform.) The desire was to replay the log events from the app's calls to our servers and send the end user IP with the events.\nTherefore, relaying/proxying events via the pixel request even down to the UA IP might be a nice piece of functionality to have here.\n. Sorry, I did not see that documented in https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol so I assumed it was not permitted. So to be clear: I can push the keyword 'ip' into the querystring for the pixel request, and the ETL job will overwrite the reported IP from the GET request in the Cloudfront log with what I pass in?\n. +1. Right now I'm experimenting with creating phantom CF logs by transforming proxied events into the CF format so the timestamp and UA is correct. Allowing this via QS would be nice.\n. Two cents here:\n- Snowplow should STRONGLY discourage storing email in the clear. Hash it or use a key from the sending system\n- In my email system, I use a UUID per-mailing in addition to a hashed email address as identifiers\n- Again, for what it's worth, I use the following syntax in my email platform, even though it doesn't work yet in Snowplow's ETL. I use EC2 for my infrastructure, so changing some of my event logic to fit with Snowplow was easy. \nSEND EVENTS:\nse_ca = email-send\nse_ac = [ sent, suppressed, invalid ]  -- actually, I have about 30 granular event types, e.g.: Global Blacklist Suppress, Client Blacklist Suppress, Cross-Campaign Dedupe, Velocity Limit, etc.\nse_la = campaign ID -- a compound key of my Client and Campaign IDs\nse_pr = segment ID -- a compound key of my Segment and Group IDs\nTRACKING PIXEL:\nse_ca = email-open\nse_ac = [ viewed, forwarded, printed ] -- these events are derived on my edge servers through my own methods, but I'm working on proxying them to Snowplow this way.\nse_la = campaign ID -- a compound key of my Client and Campaign IDs\nse_pr = segment ID -- a compound key of my Segment and Group IDs\nCLICK EVENTS:\nse_ca = email-click\nse_ac = [ mobile, desktop, web, staticview ] -- again, derived by my edge servers\nse_la = campaign ID -- a compound key of my Client and Campaign IDs\nse_pr = segment ID -- a compound key of my Segment and Group IDs\nurl = destination URL\npage = destination name\nBOUNCE EVENTS:\nse_ca = email-critical\nse_ac = [ bounce, unsubscribe, reply, complaint, noc ] -- I use BoogiePOP to parse at my MX servers for this.\nse_la = campaign ID -- a compound key of my Client and Campaign IDs\nse_pr = segment ID -- a compound key of my Segment and Group IDs\n. Campaign parameters from my sending system. The OP mentioned he was using Mailchimp, which like pretty much any system out there would have the same array of selection criteria and syntax when creating a campaign. In my case (btw, I designed this email platform myself) the hierarchy cascades logically like this:\nCLIENT -- \"Foo\"\nCAMPAIGN -- \"US 2013-09 Back to School\" \nSEGMENT -- [ \"Non-owner handraisers\", \"Q1 2013 leadgen\", \"Product 123 owners\" ]\nGROUP -- [ \"Free Shipping ver A\", \"Free Shipping ver B\", \"Extended Warranty ver A\", \"Control group\" ]\nIn this example, my SEGMENTs describe (loosely) the source of targeted recipients within the campaign audience. The GROUPs indicate which specific content they received. Between these two variables we can measure response, and I map them to GA tags on the destination site.\n. I use a SHA256 but I think it's incumbent on users to decide what satisfies their level of data protection.\n. I'm having the same problem, both with firing events directly via the pixel or by attempting to create CloudFront log rows manually. I'll dig up some evidence.\n. I'm also dead in the water without support for APP as a value. Should I edit the scala or is this on the roadmap soon?\n. ",
    "dominiclovell": "Turns out I was running:\nbundle exec bin/snowplow-emr-etl-runner --skip staging,archive --config ./config/config.yml\nThe '--skip staging' flag doesnt move files from your :in to your :processing folders, so when the EMR job runs there was no files there for it to process.\n. ",
    "jakewilliamson88": "@alexanderdean I'm experiencing the same error (stderr) with a couple of differences. My processing bucket does contain log files (staging was successful) and the S3DistCp step reports a different error:\n2014-08-27T14:22:27.879Z INFO Fetching jar file.\n2014-08-27T14:22:34.910Z INFO Working dir /mnt/var/lib/hadoop/steps/1\n2014-08-27T14:22:34.910Z INFO Executing /usr/java/latest/bin/java -cp /home/hadoop/conf:/usr/java/latest/lib/tools.jar:/home/hadoop:/home/hadoop/hadoop-tools.jar:/home/hadoop/hadoop-tools-1.0.3.jar:/home/hadoop/hadoop-core-1.0.3.jar:/home/hadoop/hadoop-core.jar:/home/hadoop/lib/*:/home/hadoop/lib/jetty-ext/* -Xmx1000m -Dhadoop.log.dir=/mnt/var/log/hadoop/steps/1 -Dhadoop.log.file=syslog -Dhadoop.home.dir=/home/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,DRFA -Djava.io.tmpdir=/mnt/var/lib/hadoop/steps/1/tmp -Djava.library.path=/home/hadoop/native/Linux-amd64-64 org.apache.hadoop.util.RunJar /home/hadoop/lib/emr-s3distcp-1.0.jar --src s3://snowplow-pg-processing/processing/ --dest hdfs:///local/snowplow/raw-events/ --groupBy .*\\.([0-9]+-[0-9]+-[0-9]+)-[0-9]+\\..* --targetSize 128 --outputCodec lzo --s3Endpoint s3.amazonaws.com\n2014-08-27T14:22:46.250Z INFO Execution ended with ret val 1\n2014-08-27T14:22:46.251Z WARN Step failed with bad retval\n2014-08-27T14:22:52.585Z INFO Step created jobs:\nGoing over your suggested checks:\n1. s3n://<< bucket >>/processing/ definitely exists, is readable by your AWS creds and is definitely in us-east-1 ?\n   - My processing bucket exists and contains logs\n   - My processing bucket is definitely readable by my AWS credentials as it was able to be written to during staging.\n   - The bucket claims a region name of \"US Standard\", which, according to the documentation is essentially an alias for us-east-1\n2. Your << keypair >> keypair was created in the correct region - us-east-1?\n   - From the EC2 Management Console, I see two Key Pairs in the us-east-1 region.\n   - One for unrelated EC2 projects, and one created specifically for use with Snowplow.\nI've tried two different availability zones by setting :placement: first to us-east-1a, then us-east-1b as my other EC2 instances use this zone and function properly. I didn't really expect this to have any effect, so I wasn't terribly disappointed when it didn't.\nI'm unsure how to debug this. What do you recommend?\nShould I open a new issue or will this one suffice?\n. ",
    "energyfirefox": "I am trying to process old logs (collected with snowplow 0.0.7) using new version of snowplow - but no luck too\n. Exectly! I am trying to process my logs and load them into infobright. I am going to upgrade to postgresql version in the nearest time (and was waiting for this version since infobright deprecated). But, at the first, my system is broken now (there were no changes in it for few months - I am collecting data with old tracker and trying to process them using appropriate old version of snowplow) and I need to do something to fix it as soon as possible :( \n. Upgrading to postgresl is my next goal. But unfortunately I can't process old logs with new etl\nerrors are\n```\n2013-08-05T19:14:19.302Z INFO Fetching jar file.\n2013-08-05T19:14:26.007Z INFO Working dir /mnt/var/lib/hadoop/steps/1\n2013-08-05T19:14:26.007Z INFO Executing /usr/java/latest/bin/java -cp /home/hadoop/conf:/usr/java/latest/lib/tools.jar:/home/hadoop:/home/hadoop/hadoop-core.jar:/home/hadoop/hadoop-core-1.0.3.jar:/home/hadoop/lib/:/home/hadoop/lib/jetty-ext/ -Xmx1000m -Dhadoop.log.dir=/mnt/var/log/hadoop/steps/1 -Dhadoop.log.file=syslog -Dhadoop.home.dir=/home/hadoop -Dhadoop.id.str=hadoop -Dhadoop.root.logger=INFO,DRFA -Djava.io.tmpdir=/mnt/var/lib/hadoop/steps/1/tmp -Djava.library.path=/home/hadoop/native/Linux-i386-32 org.apache.hadoop.util.RunJar /home/hadoop/lib/emr-s3distcp-1.0.jar --src s3n://spn-processing/events/ --dest hdfs:///local/snowplow-logs --groupBy ..([0-9]+-[0-9]+-[0-9]+)-[0-9]+.. --targetSize 128 --outputCodec lzo --s3Endpoint s3-eu-west-1.amazonaws.com\n2013-08-05T19:14:32.744Z INFO Execution ended with ret val 1\n2013-08-05T19:14:32.744Z WARN Step failed with bad retval\n2013-08-05T19:14:39.015Z INFO Step created jobs: \n```\n```\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/hadoop/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/hadoop/lib/slf4j-jcl-1.7.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nException in thread \"main\" java.lang.RuntimeException: Error running job\n    at com.amazon.elasticmapreduce.s3distcp.S3DistCp.run(S3DistCp.java:724)\n    at com.amazon.elasticmapreduce.s3distcp.S3DistCp.run(S3DistCp.java:549)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n    at com.amazon.elasticmapreduce.s3distcp.Main.main(Main.java:13)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:606)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:187)\nCaused by: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs:/tmp/daa0447e-c63e-414d-8e38-07e0c538d6d1/files\n    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n    at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:40)\n    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n    at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:1044)\n    at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1036)\n    at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:174)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:952)\n    at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:905)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)\n    at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:905)\n    at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:879)\n    at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1316)\n    at com.amazon.elasticmapreduce.s3distcp.S3DistCp.run(S3DistCp.java:706)\n    ... 9 more\n```\n```\n2013-08-05 19:14:27,167 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Running with args: [Ljava.lang.String;@588116\n2013-08-05 19:14:30,756 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Using output path 'hdfs:/tmp/daa0447e-c63e-414d-8e38-07e0c538d6d1/output'\n2013-08-05 19:14:31,072 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Created AmazonS3Client with conf KeyId 2013-08-05 19:14:31,072 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): AmazonS3Client setEndpoint s3-eu-west-1.amazonaws.com\n2013-08-05 19:14:31,437 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Skipping key 'events/' because it ends with '/'\n2013-08-05 19:14:31,437 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Created 0 files to copy 0 files \n2013-08-05 19:14:31,604 INFO org.apache.hadoop.mapred.JobClient (main): Default number of map tasks: null\n2013-08-05 19:14:31,604 INFO org.apache.hadoop.mapred.JobClient (main): Setting default number of map tasks based on cluster size to : 8\n2013-08-05 19:14:31,604 INFO org.apache.hadoop.mapred.JobClient (main): Default number of reduce tasks: 3\n2013-08-05 19:14:32,356 INFO org.apache.hadoop.security.ShellBasedUnixGroupsMapping (main): add hadoop to shell userGroupsCache\n2013-08-05 19:14:32,358 INFO org.apache.hadoop.mapred.JobClient (main): Setting group to hadoop\n2013-08-05 19:14:32,370 INFO org.apache.hadoop.mapred.JobClient (main): Cleaning up the staging area hdfs://10.234.189.35:9000/mnt/var/lib/hadoop/tmp/mapred/staging/hadoop/.staging/job_201308051913_0001\n2013-08-05 19:14:32,370 ERROR org.apache.hadoop.security.UserGroupInformation (main): PriviledgedActionException as:hadoop cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs:/tmp/daa0447e-c63e-414d-8e38-07e0c538d6d1/files\n2013-08-05 19:14:32,370 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Try to recursively delete hdfs:/tmp/daa0447e-c63e-414d-8e38-07e0c538d6d1/tempspace\n```\n```\n:aws:\n  :access_key_id: ***\n  :secret_access_key: *\n:s3:\n  :region: eu-west-1\n  :buckets:<--->\n    :assets: s3://snowplow-hosted-assets # DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket\n    :log: s3n://-spn-processing-logs/..................................................................................................\n    :in: s3n://-spn-in/................................................................................................................\n    :processing: s3n://cahootsy-spn-processing/events..........................................................................................\n    :out: s3n://-spn-out/events........................................................................................................\n    :out_bad_rows: s3n://*-spn-bad-rows/        # e.g. s3://my-out-bucket/bad-rows\n    :out_errors: # Leave blank unless :continue_on_unexpected_error: set to true below\n    :archive: s3n://-spn-archive/.............\n:emr:\n  # Can bump the below as EMR upgrades Hadoop\n  :hadoop_version: 1.0.3\n  :placement: eu-west-1b\n  :ec2_key_name: ****-emr\n  # Adjust your Hadoop cluster below\n  :jobflow:\n    :master_instance_type: m1.small\n    :core_instance_count: 2\n    :core_instance_type: m1.small\n    :task_instance_count: 0 # Increase to use spot instances\n    :task_instance_type: m1.small\n    :task_instance_bid: # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances\n:etl:\n  :job_name: SnowPlow ETL # Give your job a name\n  :hadoop_etl_version: 0.3.3 # Version of the Hadoop ETL\n  :collector_format: cloudfront # Or 'clj-tomcat' for the Clojure Collector\n  :continue_on_unexpected_error: false # You can switch to 'true' (and set :out_errors: above) if you really don't want the ETL throwing except\n```\n. Do you mean new version of EmrEtlRunner? it already have that value - I've created it from scratch (using example file from 0.8.8 tarball) :( \nThat were configuration and logs of new version's run in my comment. But in start message is error of old version's run \n. It works. Thanks. Will try to upgrade now\n. ",
    "EZWrighter": "I know this is closed, but I ran into the same issue.  I won't have the bandwidth to upgrade very soon, can you make these asset files available for self hosting?  Also, then I don't have to worry about things disappearing from my process in midstream :-)  Thanks.\n. I know this is pretty old, but I will add a note here.  We have about 7 million rows and ran into a huge delay in very simple queries to count events (1 -2 minutes).  We segment based on app_id, so putting an index on app_id brought queries under a second.  Not sure if there are going to be any long term issues with having that index, but so far it has fixed the slow query issues we were having.  Waiting to see if there are implications when adding new rows.\n. I am about to start the daunting task of migrating from 0.9.1 to the lastest so the AWS emr service will keep running :-(  Something like this would be super helpful.  Even a page with all of the release blogs posts would give me a starting point.\n. Also, the translation of old config properties to new is super helpful here: https://groups.google.com/d/msg/snowplow-user/1k6DIpRHKwA/DIoXhMH7ZmMJ  If there are other places to help navigate the config file upgrade aka: history file that would also be very helpful!\n. Ok...upgraded and running normally again.\nCouple notes on Migrating from 0.9.2 --> r64\n1.  Update to r64 codebase\n2.  Migrate Postgres from 2-->3-->4-->5 with scripts in storageloader sql folder (Don't use 3-->4 Original, it was missing user_id, use latest migrate script which is in r64)\n3.  Update cloudfront collector pixel file\n4.  Update config file based on the link I added above to Alex's translations of old config\n5.  Add --skip shred since I am using postgres and it doesn't seem to do anything with the emr processed shred files.\n6.  Change out all buckets to remove any periods in their names.\n7.  Success :-)\n. It processed normally in 16 minutes on reprocessing.  But still concerning that the EMR gets stuck for so long.\n. Thanks Alex...I assumed it might be.  Just wanted to make sure you guys were aware.\n. ",
    "gregakespret": "@alexanderdean Thanks, I've signed the CLA.\n. ",
    "chandru9279": "Is this fix/build going to be deployed as 0.3.4 jar or 0.3.5 jar on snowplow's public buckets? \nI am planning to reprocess all the bad rows, once I have these changes. \n. ",
    "fblundun": "Closing in favour of https://github.com/snowplow/kinesis-tee\n. There is something else that sets all statuses to CANCELLED: manually terminating the cluster before any step has completed. (I found this out by accident - I terminated a job without killing the EmrEtlRunner and the retry logic got called.)\nSo I guess we should implement the regex for LastStateChangeReason, maybe just searching for the string \"bootstrap action\"?\n. In that case I'll try checking LastStateChangeReason for both \"bootstrap action\" or \"Master instance startup failed\" - that probably won't give any false positives.\n. We support ingest of email events from MailChimp and Mandrill already, with SendGrid (#1161) on the way, so this should be covered now, closing...\n. The update has been made.\n. A first step here would be to reboot https://github.com/snowplow/scala-weather to be a Scala client of Open Weather Map with an LRU cache (obviously not necessary if there's an excellent Java client already, but we will need some LRU caching).\n. For now, bad rows in JSON is good as much easier to ingest into ElasticSearch\n. This is done, closing...\n. Moving back...\n. Moving back...\n. Closing since this was done in r65\n. If the MaxMind database isn't found in the working directory, it will be downloaded from the URL specified in the ip_lookups JSON. If this fails the result is a more transparent error like\njava.io.IOException: Server returned HTTP response code: 403 for URL: http://my-nonexistent-bucket.s3.amazonaws.com/third-party/maxmind/GeoIPISP.dat\nHow does that sound?\n. Closing  #522 since it is covered by #695\n. This is done, closing...\n. This is done, closing...\n. This is done, closing...\n. Hi @pkallos , we're planning to merge this PR soon. I've successfully used it to read some events from the Scala Stream Collector stream and write them to S3. What's the simplest way for me to deserialize the resulting .lzo and .lzo.index files back into SnowplowRawEvents? I'd like to be able to do it without a Hadoop cluster if possible, to make testing easier...\n. Thanks a lot Phil! I'll give it a try.\n. Closing since this has been merged into the next release.\n. What's the desired redirection behaviour for a malformed URL with path \"/r\" and no \"u\" parameter on the querystring?\n. Should we make this configurable so that it can be switched off? This would slightly improve performance for people not using click redirects, since the collector wouldn't need to do any querystring parsing\n. Shouldn't be too hard!\n. Actually the configuration option is unnecessary - querystring parsing only needs to happen for requests to the /r path.\n. This is done, closing...\n. Closing this since we decided to pass the binary Thrift records to the loader directly as byte arrays.\n. Closing #573 since it has already been resolved. The EnrichmentManager now does this: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/enrichments/EnrichmentManager.scala#L82\nAnd I've done some local testing showing that the networkUserId in ThriftLoader.scala is the same as the one in EnrichmentManager.scala.\n. I guess this comes down to which is more permissive out of NetAPorter and java.net.URL, which I'm not sure about - needs more investigation...\n. Here is the default configuration for netaporter % encoding: https://github.com/NET-A-PORTER/scala-uri/blob/91873b6eaa55759ecc8f6b3ed4798f257184233a/src/main/scala/com/netaporter/uri/encoding/PercentEncoder.scala\nThe PATH_CHARS_TO_ENCODE variable is a list of characters which netaporter encodes when it finds them in the path. It looks comprehensive. The only case I can find where java.net.URL is more permissive than netaporter is when the domain itself is non-compliant (e.g. www.ex{ample.com). This is pretty rare.\n. Rescheduling for 0.9.16...\n. Can I close this? I think it's covered by #62 and #792.\n. This is done, closing...\n. This is done, closing...\n. This is done, closing...\n. This was closed because we deleted the develop branch, but it's still scheduled to be incorporated into master in release 69.\n. Closing as this was done a while ago.\n. Closing, as this was already done a long time ago.\n. I think this is currently in the branch for adding post support. The code is here: https://github.com/snowplow/snowplow/blob/feature/ssc-post-support/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/CollectorService.scala\nIs that what you had in mind @alexanderdean ?\n. Understood! I'll replicate this behaviour with Spray.\n. Closing since this has been incorporated into the next release: https://github.com/snowplow/snowplow/commit/60adc9b7192a032b97a2f65706b12f7890046787\n. Yes, closing since this is a duplicate of #1265\n. What events should we send?\n. Rather than extracting the whole jobflow-describing XML, we decided to go with a flat JSON containing these fields:\nname\njobflow_id\nstate\ncreated_at\nended_at\nlast_state_change_reason\nOf these, the first 4 can go in emr_job_started, emr_job_succeeded, and emr_job_failed events. The last 2 are always nil at job start time, so there is no reason to include them in job_started events.\n. That would make sense! In which case I don't think that the unstructured events would actually have anything in their data field.\n. Should I also implement jobflow_step_status events as mentioned here?\n. A sample contexts array:\n[\n{:schema=>\"iglu:com.snowplowanalytics.monitoring.batch/application_context/jsonschema/1-0-0\", :data=>{:name=>\"snowplow-emr-etl-runner\", :version=>\"0.15.0\", :tags=>{}, :logLevel=>\"DEBUG\"}},\n{:schema=>\"iglu:com.snowplowanalytics.monitoring.batch/emr_job_status/jsonschema/1-0-0\", :data=>{:name=>\"Snowplow ETL\", :jobflow_id=>\"j-2GOCQLBHV03XK\", :state=>\"STARTING\", :created_at=>\"2015-07-07 15:20:20\", :ended_at=>nil, :last_state_change_reason=>\"\"}},\n{:schema=>\"iglu:com.snowplowanalytics.monitoring.batch/jobflow_step_status/jsonschema/1-0-0\", :data=>{:name=>\"Elasticity S3DistCp Step: Raw S3 -> HDFS\", :state=>\"PENDING\", :created_at=>\"2015-07-07 15:20:20\", :started_at=>nil, :ended_at=>nil}},\n{:schema=>\"iglu:com.snowplowanalytics.monitoring.batch/jobflow_step_status/jsonschema/1-0-0\", :data=>{:name=>\"Elasticity Scalding Step: Enrich Raw Events\", :state=>\"PENDING\", :created_at=>\"2015-07-07 15:20:20\", :started_at=>nil, :ended_at=>nil}},\n{:schema=>\"iglu:com.snowplowanalytics.monitoring.batch/jobflow_step_status/jsonschema/1-0-0\", :data=>{:name=>\"Elasticity S3DistCp Step: Enriched HDFS -> S3\", :state=>\"PENDING\", :created_at=>\"2015-07-07 15:20:20\", :started_at=>nil, :ended_at=>nil}},\n{:schema=>\"iglu:com.snowplowanalytics.monitoring.batch/jobflow_step_status/jsonschema/1-0-0\", :data=>{:name=>\"Elasticity S3DistCp Step: Enriched HDFS _SUCCESS -> S3\", :state=>\"PENDING\", :created_at=>\"2015-07-07 15:20:20\", :started_at=>nil, :ended_at=>nil}},\n]\n. The load_failure can have an error field. I don't know if there are any fields we want in the load_success event. We can add add an application_context to both events as well...\n. At the moment the error field contains the text of the exception. But this contains the AWS credentials, as in this anonymized example:\nJava::OrgPostgresqlUtil::PSQLException error executing COPY and ANALYZE statements: BEGIN;\nCOPY atomic.events FROM 's3://fred-test-out/enriched/good/' CREDENTIALS 'aws_access_key_id=xxxxxxxxxxxxx;aws_secret_access_key=xxxxxxxxxxxxx' REGION AS 'eu-west-1' DELIMITER '\\t' MAXERROR  EMPTYASNULL FILLRECORD TRUNCATECOLUMNS  TIMEFORMAT 'auto' ACCEPTINVCHARS GZIP;\nANALYZE atomic.events;\nCOMMIT;: ERROR: syntax error at or near \"EMPTYASNULL\"\n  Position: 225\nIs there a useful error message that doesn't risk including credentials? We could try to anonymize the credentials in the query before sending the event, but I don't know if it is possible to make that watertight.\n. That should work for the actual COPY statement but I'm also thinking about the \"syntax error\" part of the message containing private information, something like this:\nERROR:  syntax error at or near \"BADTOKEN\"\nLINE 1: ...ess_key=xxxxxxxxxxxx' BADTOKEN REGIO...\nWe could work around this by doing find and replace on the whole error message for every substring of length at least 7 which is identical to any substring of either credential string.\n. Suppose we decide to use this method of choosing some n and erasing all substrings of the error message which match a substring of a credential of length at least n. Then n should be large enough that it's unlikely that any credential substring of length n will appear in the error message outside the credentials. (For example, if n = 3 and the credentials contain the substring \"ERR\", then all instances of \"ERR\" in the error message will be replaced.) But if n is too large, then we risk substantial chunks of the credentials appearing in the error message. 7 is my guess at an intermediate value of n to avoid both these problems...\n. setLinkTrackingTimer has been replaced by pageUnloadTimer. This has been documented: https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker#page-unload-timer\n. Done in 0.9.12, closing...\n. This is done, closing...\n. Closing #715 since it is a duplicate of #695\n. Closing as duplicate of #1161\n. Closing since we decided that it's best to keep additionalProperties set to false. This means that adding new optional properties to a schema makes it more permissive rather than less permissive.\n. Done, closing...\n. Changed from us_contexts.json to contexts.json following discussion with @alexanderdean -\nSnowplow contexts can be distinguished from userspace contexts by the vendor field.\n. I have tried plugging the user agent string Mozilla/5.0 (iPhone; CPU iPhone OS 7_0_2 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Mobile/11A501 into the Hadoop Enrich integration test suite, and I can't reproduce the problem. The user-agent-utils library has gone through several releases since this ticket was opened so perhaps it was a problem with the library that has been since been resolved.\nI'm going to close this ticket since I can't reproduce the bug. We can reopen if it turns out to still exist.\n. Done, closing...\n. This is done, closing...\n. This is done, closing...\n. This is done, closing...\n. That's fine with me!\n. This is done, closing...\n. I've made these changes. Closing...\n. Here's the page history: https://github.com/snowplow/snowplow/wiki/3-Scheduling-the-StorageLoader/_history#\nAnd here's the diff: https://github.com/snowplow/snowplow/wiki/3-Scheduling-the-StorageLoader/_compare/5a211d1d04018985d3996e55df0b22756d02ae3c...e384313a7c5a13242976d7dcdbec5a46661483b9\n. This is done, closing...\n. Closing as the change has been made to Iglu Central.\n. It seems like the container holding all enrichments should be an object rather than an array, since each enrichment has its own name. Unless it's something like this:\njson\n{\n   ...\n    \"enrichments\": [\n    {\n        \"schema\": \"iglu:com.snowplowanalytics.snowplow/anon_ip/1-0-0\",\n        \"data\": {\n            \"enabled\": true,\n            \"parameters\": {\n                ...\n             }\n        }\n    }\n    ]\n}\n. Your second version is probably the right one - I think that's what it should look like.\n. I guess this would save some duplication - the individual enrichment schemas would then be changed to only validate the inner \"parameters\" object. So the enrichments field of the config.json file would look something like this:\n``` json\n{\n    \"schema\": \"iglu:enrichments\",\n\"data\": {\n\n    \"anon_ip\": {\n        \"enabled\": false,\n        \"parameters\": {\n            \"schema\": \"iglu:com.snowplowanalytics.snowplow/anon_ip_parameters/jsonschema/1-0-0\",\n            \"data\": {\n                \"anon_octets\": 1\n            }\n        }\n    },\n\n    \"geo_ip\": {\n        \"enabled\": true,\n        \"parameters\": {\n            \"schema\": \"iglu:com.snowplowanalytics.snowplow/geo_ip_parameters/jsonschema/1-0-0\",\n            \"data\": {\n                \"maxmind_file\": \"...\"\n            }\n        }\n    }\n}\n\n}\n``\n. - https://www.elastic.co/guide/en/elasticsearch/hadoop/current/cascading.html\n- https://github.com/elastic/elasticsearch-hadoop\n- https://github.com/Cascading/cascading.elasticsearch\n- https://github.com/scalding-io/scalding-taps\n- https://github.com/scalding-io/ProgrammingWithScalding\n. Should this job have an optional exceptions folder like the others?\n. I have been gettingorg.elasticsearch.index.mapper.MapperParsingException: failed to parse` errors from Elasticsearch. The reason for this is that I was using testing with old-style enriched bad rows with string errors instead of processing messages. The job would first send the enriched bad rows to ES, causing ES to infer this mapping:\njson\n      \"badrows\" : {\n        \"properties\" : {\n          \"errors\" : {\n            \"type\" : \"string\"\n          },\n          \"line\" : {\n            \"type\" : \"string\"\n          }\n        }\n      }\nIf I send in the shred bad rows (which use processing messages) first, this is the inferred mapping:\njson\n      \"badrows\" : {\n        \"properties\" : {\n          \"errors\" : {\n            \"properties\" : {\n              \"level\" : {\n                \"type\" : \"string\"\n              },\n              \"message\" : {\n                \"type\" : \"string\"\n              },\n              \"repositories\" : {\n                \"type\" : \"string\"\n              }\n            }\n          },\n          \"line\" : {\n            \"type\" : \"string\"\n          }\n        }\n      }\nSending one before the other causes a mapper failure as neither fits the other's mapping.\nShould we only support new-style enrich bad rows with processing messages rather than strings?\nIf not, we need to either put the two different types of failure in different types or indexes or create a predefined mapping which allows the errors to be either strings or hashes.\n. In ES 2.0.0, the two types of bad row (old-style and new-style) would actually have to go in different indexes: https://www.elastic.co/blog/great-mapping-refactoring\n. One more thing: if the \"copy to Elasticsearch\" step fails halfway through, then rerunning the step will end up creating some duplicates. Is this acceptable?\nIf not there are some alternatives to explore:\n- Have the job actually parse the bad row JSON strings and add an extra field which is a hash of the whole JSON. This hash would be used as the ES document ID of the bad row, ensuring uniqueness. It wouldn't actually need to go into the document (see https://github.com/elastic/elasticsearch-hadoop/issues/230).\n- Add a UUID to all our bad rows. Use this as the ES document ID.\n- Have some sort of Elasticsearch rotation system. For instance, write bad rows to a temporary index and when the EMR succeeds copy them to the main index.\nEdit: the 1st and 2nd alternatives assume that you are just rerunning the \"copy to ES\" step, not the actual enrichment / shred which generated the bad rows (since that would change the timestamps (and possibly UUIDs) of the bad rows).\n. It probably wouldn't be table: - would probably be type: (assuming that doesn't break Contracts!)\n. The dual meaning of \"type\" here is pretty inconvenient. Perhaps we should start using \"type\" to mean the Elasticsearch type and replace \"type: redshift\" with something else, e.g. \"target_kind: redshift\"?\n. How about something like\nyml\n  - name: myelasticsearch\n    type: elasticsearch\n    host: myhost\n    database: mydatabase\n    port: 9200\n    table: badrows\n    sources: [\"s3://path/run=xx\", \"s3://path/run=yy\"]\nWhere \"sources\" is an array of all the buckets containing bad rows which you want to send to Elasticsearch.\nIf the \"sources\" field isn't given, it would default to the two bad rows buckets for the current run (enrich failures and shred failures).\n. We should probably add an option to toggle connecting only to declared es.nodes (this would be true for AES and false otherwise):\nyml\n  - name: myelasticsearch\n    type: elasticsearch\n    host: myhost\n    database: mydatabase\n    port: 9200\n    table: badrows\n    sources: [\"s3://path/run=xx\", \"s3://path/run=yy\"]\n    \"es.nodes.wan.only\": true # allows writing to AES\n. We should probably add an option to toggle connecting only to declared es.nodes (this would be true for AES and false otherwise):\nyml\n  - name: myelasticsearch\n    type: elasticsearch\n    host: myhost\n    database: mydatabase\n    port: 9200\n    table: badrows\n    sources: [\"s3://path/run=xx\", \"s3://path/run=yy\"]\n    \"es.nodes.wan.only\": true # allows writing to AES\n. I've added this now.\n. Here is the page: https://github.com/snowplow/snowplow/wiki/Using%20registry%20aware%20enrichments\n. If I understand correctly, the problem is that the \"common\" pipe gets run twice - once to find good events and once to find bad events.\nBut when I run the any test from the Hadoop Enrich test suite, these three lines get run once per event each (based on extra logging statements I inserted):\nCommon: https://github.com/snowplow/snowplow/blob/60adc9b7192a032b97a2f65706b12f7890046787/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala#L172\nBad: https://github.com/snowplow/snowplow/blob/60adc9b7192a032b97a2f65706b12f7890046787/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala#L178\nGood: https://github.com/snowplow/snowplow/blob/60adc9b7192a032b97a2f65706b12f7890046787/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala#L191\nSo the \"common\" pipe seems to be run once rather than twice. Am I misunderstanding something?\n. I just tried the same thing with the r60 tag, and got the same result. So it probably wasn't due to the Cascading update.\n. Looks like this does indeed speed things up!\nI added a line to log a message to stdout for every enriched event. Then I ran Hadoop Enrich with and without forceToDisk, each time with just one event as input.\nFor each job, I downloaded the logs for the cluster and grepped for the log message. With forceToDisk, the message appeared 3 times. Without forceToDisk, it appeared 6 times.\n. Is the idea that the schema would look something like:\njson\n{\n ...\n  \"type\": \"object\",\n  \"properties\": {\n    \"publicProperty\": {\n      \"type\": \"string\"\n    },\n    \"privateProperty\": {\n      \"type\": \"string\",\n      \"pii\": true\n    }\n  }\n}\n. Just tested it - it's allowed and doesn't change the results of any tests. I remember that the meta-schema for JSON Schema didn't forbid extra properties.\n. An alternative format would be something like:\njson\n{\n ...\n  \"properties\": {\n    \"public\": {\n      \"type\": \"string\"\n    },\n    \"private\": {\n      \"type\": \"string\"\n    }\n  },\n  \"pii\": [\"private\"]\n}\nSimilar to how the required keyword works. I'm not sure which is better.\n. This is done, closing...\n. This is done, closing...\n. Pushing back to next Kinesis release\n. This is done, closing...\n. Closing since this is now handled in scala-maxmind-iplookups.\n. This is done, closing...\n. This is done, closing...\n. This is done, closing...\n. @alexanderdean I've added this to the CHANGELOG.\n. I've made an edit but it hasn't appeared on Github yet. When it does, could you see if you think it's more readable @alexanderdean ?\n. This is done, closing...\n. This is done,  closing...\n. Is the second case for POSTs whose body is an array of individual event JSONs, e.g.\njson\n[{\n  \"e\": \"se\",\n  \"p\": \"srv\",\n  \"tv\": \"py-0.9.0\"\n}, ...]\n?\n. I've added in the missing fields and updated the overview to mention shredding - could you check to see if I've described it correctly?\n. Thanks @yalisassoon , I'll add those sections.\n. I did this a while ago, closing...\n. Be aware this workaround may cause problems (see snowplow-user thread) https://groups.google.com/forum/#!msg/snowplow-user/NhiUPDdoJOI/sgliSSXp3CgJ\n. When running in local mode I'm able to send GET requests with about 90 000 characters and write the corresponding enriched events to stdout.\n. Yes I forgot to include the # - will fix now...\n. This is done, closing...\n. The only changes I've made to Scala Hadoop Enrich are to the test suite. Should I still bump its version too?\n. I think the cookie_extractor config schema looks good.\nFor the cookie_extractor_context - should have one context per cookie rather than all cookies in a single context? I think this would be more elegant in Redshift since we would avoid having a field containing a raw literal JSON array string.\nThoughts @yalisassoon ?\n. @Deano according to http://docs.aws.amazon.com/redshift/latest/dg/r_names.html Redshift table names are case insensitive. Will this cause any problems with the table name org_w3_PerformanceTiming_1?\n. Yes it does! I'll change the table name to  org_w3_performance_timing_1.\n. Thanks for pointing this out @michaelpryor , it's fixed now.\n. @michaelpryor this method is fine! According to this SO question GitHub doesn't support pull requests for wiki pages anyway: http://stackoverflow.com/questions/10642928/how-to-pull-request-a-wiki-page-on-github\n. Thanks for spotting this @michaelpryor - I forgot to remove that link when setLinkTracking was renamed to  pageUnloadTimer, whose documentation you can see here: https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker-v2.0#page-unload-timer\n. Fixed now!\n. Closing #1072 since this is a duplicate of #978 (which will be in the next release).\n. The links are fixed now, closing #1094\n. See #1095\n. This is done, closing.\n. Also note that I haven't yet bumped any versions.\n. I'll go with Kinesis 1 since then it can go alongside #534.\nHave also made the corresponding ticket for Kinesis Enrich: #1117 \n. Done in 0.9.12, closing...\n. Hello @dominickendrick, we have merged #978 into Kinesis 0.2.0. It looks like its changes to the Scala Stream Collector are functionally the same as your PR, it just uses the old version of scalazon so is slightly more verbose. It also makes specifying the endpoint mandatory.\nYou can see the merged KinesisSink file here: https://github.com/snowplow/snowplow/blob/kinesis-1/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/sinks/KinesisSink.scala#L149\n. Yes, I'll switch to ip_organization in the Elasticsearch Sink.\n. Closing as this is a subset of #1345 .\n. Sorry about this! It should be working now.\n. Only using one pass will slightly change the function's behaviour. For example, the string\n\"http://www.google.com/search?q=gateway+oracle+cards+denise+linn&hl=en&client=safari\"\nwould be converted to the URI\n\"http://www.google.com/search?q=gateway%2Boracle%2Bcards%2Bdenise%2Blinn&hl=en&client=safari\"\nrather than \n\"http://www.google.com/search?q=gateway+oracle+cards+denise+linn&hl=en&client=safari\"\nThe difference is that NetAPorter changes + to  %2B. Will this change cause any problems?\n. There's some background here:\nhttps://github.com/NET-A-PORTER/scala-uri/issues/2\nhttps://github.com/NET-A-PORTER/scala-uri/pull/3\nI haven't been able to switch this off. I tried using their custom encoding to keep pluses as pluses but didn't succeed.\n. This is done:\nhttps://github.com/snowplow/snowplow/wiki/kinesis-elasticsearch-sink\nhttps://github.com/snowplow/snowplow/wiki/kinesis-elasticsearch-sink-setup\nClosing...\n. Done in 0.9.12, closing...\n. Done in 0.9.12, closing...\n. Thank you!\n. That's right.\n. Rescheduling...\n. We don't fully get this for free with the SCE version bump because of custom-made errors which don't fit the format: https://github.com/snowplow/snowplow/blob/8d04a5ee48c3b5a24827f7de66c1cf5d3d1e97e8/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/kinesis/sources/AbstractSource.scala#L75-L111\nWe need to update the SCE bad row class to permit a \"size\" field instead of a \"line\" field, and use that instead.\n. Bad rows now look like this:\njson\n{\n  \"failure_tstamp\": \"2016-02-16T09:14:23.574Z\",\n  \"errors\": [\n    {\n      \"message\": \"Error deserializing raw event: Cannot read. Remote side has closed. Tried to read 2 bytes, but only got 1 bytes. (This is often indicative of an internal error on the server side. Please check your server logs.)\",\n      \"level\": \"error\"\n    }\n  ],\n  \"line\": \"cGFjayBteSBib3ggd2l0aCBmaXZlIGRvemVuIGxpcXVvciBqdWdzCg==\"\n}\n. Rescheduling...\n. Sample bad row in the new format:\njson\n{\n  \"failure_tstamp\": \"2016-02-16T09:18:05.260Z\",\n  \"errors\": [\n    {\n      \"message\": \"Event contained only 1 tab-separated fields\",\n      \"level\": \"error\"\n    }\n  ],\n  \"line\": \"---\"\n}\n. Rescheduling...\n. Rescheduling...\n. Thanks for offering @fail-fast! There's no need - we will soon put out a release incorporating this PR which fixes the conflict: https://github.com/snowplow/snowplow/pull/1214\n. Thanks for this fix @knservis ! have you signed our CLA yet? https://github.com/snowplow/snowplow/wiki/CLA\n. I couldn't find a library which specifically extracted the querystring without doing any decoding, so I used this regex to extract everything between \"?\" and \"#\":\n\"^[^?]\\?([^#])(?:#.*)?$\"\n. Should this ticket move to iglu-scala-client? It seems like the best place to make the change is in the Resolver class itself.\n. I've edited the title to specify that I'm bumping the version to 1.14. The https://github.com/HaraldWalker/user-agent-utils repo's README suggests that there is a 1.15 available, but adding it as a dependency caused an error:\n```\n[info] Resolving bitwalker#UserAgentUtils;1.15 ...\n[warn]  module not found: bitwalker#UserAgentUtils;1.15\n[warn] ==== local: tried\n[warn]   /home/vagrant/.ivy2/local/bitwalker/UserAgentUtils/1.15/ivys/ivy.xml\n[warn] ==== public: tried\n[warn]   http://repo1.maven.org/maven2/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[warn] ==== Sonatype Snapshots: tried\n[warn]   https://oss.sonatype.org/content/repositories/snapshots/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[warn] ==== Concurrent Maven Repo: tried\n[warn]   http://conjars.org/repo/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[warn] ==== Twitter Maven Repo: tried\n[warn]   http://maven.twttr.com/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[warn] ==== Snowplow Analytics Maven repo: tried\n[warn]   http://maven.snplow.com/releases/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[warn] ==== Snowplow Analytics Maven snapshot repo: tried\n[warn]   http://maven.snplow.com/snapshots/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[warn] ==== user-agent-utils repo: tried\n[warn]   https://raw.github.com/HaraldWalker/user-agent-utils/mvn-repo/bitwalker/UserAgentUtils/1.15/UserAgentUtils-1.15.pom\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: bitwalker#UserAgentUtils;1.15: not found\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n```\n. Renaming to \"add collector-payload-1 dependency\" because the CollectorPayload class is now being published separately\n. Currently the SSC doesn't actually know which shard each record ends up in - rather than specifying a shard directly, it specifies a partition key which Kinesis then hashes to a 128-bit integer in the keyspace. Each shard gets all the records assigned to a particular range in the keyspace.\nWe could override this hashing by explicitly providing a value using the ExplicitHashKey parameter (http://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html)\n. No, I think we know that this works now.\n. Rescheduling...\n. Moving forwards...\n. Will etl_tags be the same for all events processed in a single job? If so I should probably change the encoding to runlength.\n. Actually we will now have a test dependency on SnowplowRawEvent and CollectorPayload due to #669. I'm renaming the ticket accordingly...\n. Closing since this has been incorporated into the next release: https://github.com/snowplow/snowplow/commit/ef0c88f1be44432cbd2783bbaace4cb2ff857f5a\n. Hey @danisola ,\nWe're incorporating this into the next EmrEtlRunner/StorageLoader release. The gzip option works great, but when I try the lzo option, the Redshift load fails with the error:\n```\nCOPY atomic.events FROM 's3://path-to/bucket/' CREDENTIALS 'aws_access_key_id=xxx;aws_secret_access_key=xxx' REGION AS 'eu-west-1' DELIMITER '\\t' MAXERROR 1 EMPTYASNULL FILLRECORD TRUNCATECOLUMNS TIMEFORMAT 'auto' ACCEPTINVCHARS LZOP;\nERROR:  failed to inflate with lzop: unexpected end of file.\nDETAIL:  \n\nerror:  failed to inflate with lzop: unexpected end of file.\n  code:      9001\n  context:   S3 key being read : s3://fred-test-out/enriched/good/\n  query:     244\n  location:  table_s3_scanner.cpp:348\n  process:   query0_60 [pid=5615]\n\n```\nThis is in spite of the fact that if I download the enriched lzo-compressed events, I can successfully inflate them from the command line:\nlzop -d part-00000.lzo\nHave you come across anything like this?\n. No problem @danisola , we will just add the gzip compression for now.\n. Similarly here https://github.com/snowplow/snowplow/blob/master/4-storage/kinesis-elasticsearch-sink/src/main/scala/com.snowplowanalytics.snowplow.storage.kinesis/elasticsearch/Shredder.scala#L103-L104, \"context\" should be \"contexts\"\n. Moving forwards\n. Did this, closing...\n. Rescheduling...\n. Moving forwards since release 63 already has some Kinesis Elasticsearch Sink changes...\n. I think this is covered by these four tickets:\n1702\n1703\n1704\n1706\nThe difference is that in those tickets, the initialization, shutdown, heartbeat, and failed write events are all separate, but in this ticket, the heartbeat would be the only event and would contain information about which state the app is in.\nI think that separate events is the better option. Using the heartbeat-only approach, the app might shutdown between heartbeats so that no shutdown heartbeat event is ever sent.\n. According to this documentation, there is no way to query every items whose hash key begins with a particular string, so instead I am using a scan to get all items from the table and filter them locally. This involves examining every item in the table so it's best if the table doesn't contain too many extraneous items. What do you think of this approach @alexanderdean ?\n. I mean all items.\n. Closing as duplicate of #1226\n. I've edited the CHANGELOG to remove this.\n. I do now...\n. I haven't yet squashed all the Kinesis LZO S3 Sink commits together - I will do once we have decided we are happy with them.\n. I got around this by including the AWS region in the bucket name. So this doesn't work:\ns3 {\n    endpoint: \"http://s3.amazonaws.com\"\n    bucket: \"outer-bucket/inner-bucket\"\n  }\nbut this does:\ns3 {\n    endpoint: \"http://s3-eu-west-1.amazonaws.com\"\n    bucket: \"outer-bucket/inner-bucket\"\n  }\nNot sure why this only matters for nested buckets...\n. I've added the crossdomain.xml path. Is there a simple way to test whether it works?\n. This is the output of curl for the SSC:\n``` bash\n$ curl --verbose http://mysubdomain.ngrok.com/crossdomain.xml\n\nGET /crossdomain.xml HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: 14e0b321.ngrok.com\nAccept: /\n< HTTP/1.1 200 OK\n Server nginx/1.6.2 is not blacklisted\n< Server: nginx/1.6.2\n< Date: Mon, 09 Mar 2015 11:31:04 GMT\n< Content-Type: text/xml; charset=ISO-8859-1\n< Content-Length: 116\n< Connection: keep-alive\n< \n<?xml version=\"1.0\"?>\n\n\n Connection #0 to host 14e0b321.ngrok.com left intact\n\n```\n\nAnd for the Clojure Collector:\n``` bash\n~$ curl --verbose http://collector.snplow.com/crossdomain.xml\n\nGET /crossdomain.xml HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: collector.snplow.com\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: text/xml;charset=ISO-8859-1\n< Date: Mon, 09 Mar 2015 11:21:40 GMT\n Server Apache-Coyote/1.1 is not blacklisted\n< Server: Apache-Coyote/1.1\n< Content-Length: 116\n< Connection: keep-alive\n< \n<?xml version=\"1.0\"?>\n\n\n Connection #0 to host collector.snplow.com left intact\n\n```\n\nThe only relevant difference is that the Scala Stream Collector's Content-Type has a space between the semicolon and the charset. I doubt that will cause any problems.\n. I think we should log a warning if the number of fields in the incoming enriched event TSV isn't as expected.\n. We decided against this since we can't widen the domain_userid column in place to accommodate UUIDs.\n. Should we also change the encoding for nested JSON fields in tables like submit_form?\n. Moving forward...\n. Isn't this already done here https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/atomic-def.sql#L156?\n. This was incorporated here: https://github.com/snowplow/snowplow/commit/993c6ffd4a22251957c34163f959aeb8cc8f44a1\n. This is the relevant section: https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker#multiple-trackers\nI'll edit it to use the term \"namespace\".\n. My addition of the zip -j option above completely flattens the structure so that unzipping the zipfile simply adds all the kinesis apps to the current working directory. Is this the desired behaviour?\n. Moving forward...\n. I've updated the links and mentioned the new \"thrift\" collector_format in the wiki's example config yaml. Thanks @r-com !\n. Closing because there are a couple of fields I need to convert from string to number...\n. Reopening now that the generated JSON matches the schema.\n. Just assigned myself, I'll write it now...\n. Whoops, shouldn't have PR'ed into master...\n. Good point! I'll implement this in the loader rather than the adapter (since adapters have to return NonEmptyLists of raw events)\n. I moved the commits onto the latest version of release/r61-pygmy-parrot and now the CHANGELOG is unchanged.\n. This PR now also contains the Travis commits.\n. Closing as subset of #678 \n. Something like:\nyaml\n:monitoring:\n  :tags:                      # Array of tags to describe this job\n    - prod\n  :logging:\n    :level: DEBUG             # You can optionally switch to INFO for production\n  :snowplow:\n    :method: POST             # Or GET if you prefer\n    :collector: ADD HERE      # e.g. d3rkrsqld9gmqf.cloudfront.net\n    :app_id: ADD HERE         # e.g. snowplow\n. This is my suggestion for a configuration file which means tracking is disabled:\nyaml\n:monitoring:\n  :tags:                      # Array of tags to describe this job\n    - prod\n  :logging:\n    :level: DEBUG             # You can optionally switch to INFO for production\n  :snowplow:                  # nil\n. Duplicate of #2045\n. Should this be a configurable enrichment or something that just happens automatically?\n. Closing as duplicate of #1626\n. The Amazon Kinesis Connector Library configures the Kinesis client to call processRecords even if the list of records to process is empty (i.e. no records have appeared in the stream since the last batch were processed). This enables it to flush the buffer at regular intervals without needing to create a new thread to schedule the flush.\nOur planned approach has the flaw that if the input stream dries up, processRecords stops getting called and the buffered enriched events never get flushed (since the maximum time between flushes only gets checked whenever processRecords is called). callProcessRecordsEvenForEmptyList provides a solution to this problem: once the incoming stream dries up, we can still keep checking whether the maximum time between flushes has been exceeded.\n. It's a setting for the KinesisClientLibConfiguration class from the Kinesis Client Library.\n. This comment suggests checkpointing about every 5 minutes, which is a lot less frequently than Scala Kinesis Enrich does. But I haven't encountered any ThrottlingExceptions and the Amazon Kinesis Connector Library makes no attempt to prevent them by checkpointing infrequently, so it may not be an issue.\n. Closing since this was done in Red-Cheeked Cordon Bleu: #1345\n. OK - I have made all the changes suggested so far!\n. No problem!\n. Ok, I've made the changes.\n. Closing in favour of #1731\n. My current approach is to have a configurable TimeThreshold, RecordThreshold, and ByteThreshold. Every TimeThreshold milliseconds, all stored events are sent in a single PutRecords API call. Additionally, all records are sent whenever the total number of stored records reaches RecordThreshold or the total stored bytes reaches ByteThreshold.\n. That's right - they will be configurable in case the limits change in the future.\n. I've changed how the timer works - it resets every time the buffer is flushed for any reason. Otherwise the collector will occasionally send a large PutRecords request when RecordThreshold is reached, and immediately afterwards send a small one because TimeThreshold has been reached.\n. Closing as duplicate of #1227\n. Closing since it is actually used here: https://github.com/snowplow/snowplow/blob/master/4-storage/kinesis-elasticsearch-sink/src/main/scala/com.snowplowanalytics.snowplow.storage.kinesis/elasticsearch/SnowplowElasticsearchEmitter.scala#L130\n. @alexanderdean - am I right in thinking that this means completely removing support for reading creds from a file on the classpath as described here?\n. That makes sense... I don't think there's any reason to keep supporting the classpath configuration option.\n. This is a mistake in the comment in the sample config file - the app doesn't currently support cpf.\n. Attempts to set the log level on startup by a command line argument fail due to our argument parsing:\n```\n./snowplow-stream-collector-0.3.0 --config active.hocon.sample -Dorg.slf4j.simpleLogger.defaultLogLevel=error\nException in thread \"main\" org.clapper.argot.ArgotUsageException: Unknown option: -Dorg.slf4j.simpleLogger.defaultLogLevel=error\nsnowplow-stream-collector: Version 0.3.0. Copyright (c) 2013,\ncom.snowplowanalytics.\nUsage: snowplow-stream-collector [OPTIONS]\nOPTIONS\n--config filename  Configuration file.\nat org.clapper.argot.ArgotParser.usage(Argot.scala:1294)\nat org.clapper.argot.ArgotParser$$anonfun$15.apply(Argot.scala:1372)\nat org.clapper.argot.ArgotParser$$anonfun$15.apply(Argot.scala:1372)\nat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\nat scala.collection.AbstractMap.getOrElse(Map.scala:58)\nat org.clapper.argot.ArgotParser.parseCompressedShortOpt(Argot.scala:1371)\nat org.clapper.argot.ArgotParser.parseShortOpt(Argot.scala:1434)\nat org.clapper.argot.ArgotParser.parseArgList(Argot.scala:1479)\nat org.clapper.argot.ArgotParser.parse(Argot.scala:1132)\nat org.clapper.argot.ArgotParser.parse(Argot.scala:1118)\nat com.snowplowanalytics.snowplow.collectors.scalastream.ScalaCollector$delayedInit$body.apply(ScalaCollectorApp.scala:65)\nat scala.Function0$class.apply$mcV$sp(Function0.scala:40)\nat scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)\nat scala.App$$anonfun$main$1.apply(App.scala:71)\nat scala.App$$anonfun$main$1.apply(App.scala:71)\nat scala.collection.immutable.List.foreach(List.scala:318)\nat scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)\nat scala.App$class.main(App.scala:71)\nat com.snowplowanalytics.snowplow.collectors.scalastream.ScalaCollector$.main(ScalaCollectorApp.scala:38)\nat com.snowplowanalytics.snowplow.collectors.scalastream.ScalaCollector.main(ScalaCollectorApp.scala)\n\n``\n. I think that's a good idea - the other possibilities would be setting it in the configuration hocon or getting it from an environment variable.\n. At the moment we make our jars executable by prepending thedefaultShellScript` here which is\n``` bash\n!/usr/bin/env sh\nexec java -jar \"$0\" \"$@\"\n```\nShould we instead copy the prepended script from the EER here?\nbash\nexec \"$java\" $java_args -jar $MYSELF \"$@\"\nThis would allow you to specify JVM arguments, e.g.\nbash\njava_args=\"-XX:+TieredCompilation -XX:TieredStopAtLevel=1 -Xcompile.invokedynamic=false\" ./myexecutable\nI think this would also let you pass the -Dorg.slf4j.simpleLogger.defaultLogLevel=error argument.\n. Using Scala's ParVector seems to speed up the conversion of a collection of raw events to a collection of enriched events by a factor of about 2 on my machine with 4 cores.\n. The original attempt to make this change:\n```\ncommit aeb39c2a27bb995e16f04b34c55e4be32ab813f9\nAuthor: Fred Blundun fred@snowplowanalytics.com\nDate:   Fri Mar 20 09:56:03 2015 +0000\nScala Kinesis Enrich: parallelized processing of raw events (closes #1537)\n\ndiff --git a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/ISink.scala b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/ISink.scala\nindex dec3d96..91fa307 100644\n--- a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/ISink.scala\n+++ b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/ISink.scala\n@@ -19,6 +19,9 @@\n package com.snowplowanalytics.snowplow.enrich\n package kinesis.sinks\n+// Scala\n+import scala.collection.parallel.immutable.ParSeq\n+\n /\n  * An interface for all sinks to use to store events.\n  /\n@@ -32,7 +35,7 @@ trait ISink {\n    * String until such time as https://github.com/snowplow/snowplow/issues/211\n    * is implemented.\n    /\n-  def storeEnrichedEvents(events: List[(String, String)]): Boolean\n+  def storeEnrichedEvents(events: ParSeq[(String, String)]): Boolean\ndef flush()\n }\ndiff --git a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/KinesisSink.scala b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/KinesisSink.scala\nindex a98f79c..a94d0c9 100644\n--- a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/KinesisSink.scala\n+++ b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/KinesisSink.scala\n@@ -32,6 +32,7 @@ import com.amazonaws.services.kinesis.AmazonKinesis\n import com.amazonaws.regions._\n// Scala\n+import scala.collection.parallel.immutable.ParSeq\n import scala.util.control.NonFatal\n import scala.collection.JavaConverters._\n@@ -206,9 +207,11 @@ class KinesisSink(provider: AWSCredentialsProvider,\n    * @param events List of events together with their partition keys\n    * @return whether to send the stored events to Kinesis\n    */\n-  def storeEnrichedEvents(events: List[(String, String)]): Boolean = {\n+  def storeEnrichedEvents(events: ParSeq[(String, String)]): Boolean = {\n     val wrappedEvents = events.map(e => ByteBuffer.wrap(e.1.getBytes) -> e._2)\n-    wrappedEvents.foreach(EventStorage.addEvent())\n+\n+    // Convert back to seq so that events get stored in correct order\n+    wrappedEvents.seq.foreach(EventStorage.addEvent(_))\n if (System.currentTimeMillis() > nextRequestTime) {\n   nextRequestTime = System.currentTimeMillis() + TimeThreshold\n\ndiff --git a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/StdouterrSink.scala b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/StdouterrSink.scala\nindex 4def307..2efb4b8 100644\n--- a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/StdouterrSink.scala\n+++ b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sinks/StdouterrSink.scala\n@@ -19,6 +19,9 @@\n package com.snowplowanalytics.snowplow.enrich.kinesis\n package sinks\n+// Scala\n+import scala.collection.parallel.immutable.ParSeq\n+\n // Snowplow\n import com.snowplowanalytics.snowplow.collectors.thrift._\n@@ -35,10 +38,11 @@ class StdouterrSink(inputType: InputType.InputType) extends ISink {\n    * String until such time as https://github.com/snowplow/snowplow/issues/211\n    * is implemented.\n    /\n-  def storeEnrichedEvents(events: List[(String, String)]): Boolean = {\n+  def storeEnrichedEvents(events: ParSeq[(String, String)]): Boolean = {\n     inputType match {\n-      case InputType.Good => events.foreach(e => println(e._1)) // To stdout\n-      case InputType.Bad => events.foreach(e => Console.err.println(e._1)) // To stderr\n+      // Convert events to sequence so that they get printed in the correct order\n+      case InputType.Good => events.seq.foreach(e => println(e._1)) // To stdout\n+      case InputType.Bad => events.seq.foreach(e => Console.err.println(e._1)) // To stderr\n     }\n     true\n   }\ndiff --git a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala\nindex caf73ba..1fc2b82 100644\n--- a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala\n+++ b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala\n@@ -127,8 +127,9 @@ abstract class AbstractSource(config: KinesisEnrichConfig, igluResolver: Resolve\n    * @param binaryData Thrift raw event\n    * @return Whether to checkpoint\n    /\n-  def enrichAndStoreEvents(binaryData: List[Array[Byte]]): Boolean = {\n-    val enrichedEvents = binaryData.flatMap(enrichEvents())\n+  def enrichAndStoreEvents(binaryData: Vector[Array[Byte]]): Boolean = {\n+    // Parallelization\n+    val enrichedEvents = binaryData.par.flatMap(enrichEvents())\n     val successes = enrichedEvents collect { case Success(s) => s }\n     val failures = enrichedEvents collect { case Failure(s) => s }\n     val successesTriggeredFlush = sink.map(_.storeEnrichedEvents(successes))\ndiff --git a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/KinesisSource.scala b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/KinesisSource.scala\nindex 9e5b60c..ffb6454 100644\n--- a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/KinesisSource.scala\n+++ b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/KinesisSource.scala\n@@ -147,7 +147,7 @@ class KinesisSource(config: KinesisEnrichConfig, igluResolver: Resolver, enrichm\n private def processRecordsWithRetries(records: List[Record]): Boolean = {\n   try {\n\n\nenrichAndStoreEvents(records.map(_.getData.array).toList)\nenrichAndStoreEvents(records.map(_.getData.array).toVector)\n       } catch {\n         case NonFatal(e) =>\n           // TODO: send an event when something goes wrong here\ndiff --git a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/StdinSource.scala b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/StdinSource.scala\nindex 65acd37..0034ea4 100644\n--- a/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/StdinSource.scala\n+++ b/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/StdinSource.scala\n@@ -48,7 +48,7 @@ class StdinSource(config: KinesisEnrichConfig, igluResolver: Resolver, enrichmen\n   def run = {\n     for (ln <- io.Source.stdin.getLines) {\n       val bytes = Base64.decodeBase64(ln)\nenrichAndStoreEvents(List(bytes))\nenrichAndStoreEvents(Vector(bytes))\n     }\n   }\n }\n```\n. Pushing back as this requires all enrichments to be thread-safe.\n\nSpecifically, the currency conversion enrichment and IP lookups enrichment use LruMap rather than SynchronizedLruMap.\nIf we do put this back in, we will need to ensure that we never add any non-thread-safe enrichments... We will need to decide whether the speedup is worth it.\n. Blocked by https://github.com/snowplow/scala-forex/issues/89 and https://github.com/snowplow/scala-maxmind-iplookups/issues/28\n. Rather than making the enrichments thread-safe, we could make them ThreadLocal so that a new copy is created for each thread. We would need to check the performance of this approach.\n. If we use ThreadLocal, I think that the important thing is to ensure that old threads are reused with each batch of events. If new threads are created, new copies of the thread-unsafe enrichments will be generated with empty LRU caches.\nSee http://docs.scala-lang.org/overviews/parallel-collections/configuration.html\n. Parallel collections do reuse threads rather than creating new ones, so this code prints \"new thread\" once per core rather than 100 times:\n``` scala\nobject ThreadLocalTest extends App {\n  val i = new ThreadLocal[Int] {\n    override def initialValue = {\n      println(\"new thread\")\n      100\n    }\n  }\n(1 to 100).toVector.par.map {\n    n => i.get\n  }\n}\n```\n. An implementation of the ThreadLocal approach can be found in the threadlocal-enrichments branch.\nDescheduling so that we have more time to work out the best approach.\n. Another possible approach to parallelism: make separate expensive enrichments run in parallel. The IP lookups, currency conversion lookups, and weather lookups could happen simultaneously for a given event.\n. Hello @drewgillson ,\nDid you explicitly use Elasticsearch's geo_point mapper type? If you don't then Elasticsearch will interpret the geo_location field as a plain string. See here for the recommended way to create the mapping for the Snowplow enriched event type.\n@alexanderdean - that's an omission from the schema, I'll update it now.\n. Will dvce_sent_tstamp and dvce_created_tstamp both be fields in EnrichedEvent rather than derived contexts?\n. They are - I was forgetting that dvce_created_tstamp is actuallly just called dvce_tstamp.\n. Closing since this has been split into other tickets: #1557 #1558 #1559\n. Thanks for pointing this out @kavehfa , it will be fixed in the next release of Scala Kinesis Enrich.\n. Here in the Scala Stream Collector we are using a formatted timestamp rather than a simple Unix timestamp for bad rows. Is this so that Elasticsearch recognizes them as timestamps? And if so should we be doing the same thing in Common Enrich?\n. This breaks some of the existing tests in Hadoop Enrich that don't expect a timestamp field in bad row JSONs. Are we going to have a new Hadoop Enrich version as part of this release?\n. Done!\n. This is done, closing...\n. Hi @kavehfa ,\nThis will be fixed in the next release of the sink (which will be going into test soon).\nClosing as duplicate of #1582\n. Closing since this was effectively done in r68 here\n. Thanks Alex, I've changed the embedded hocons and lowercased \"json\".\n. I can't find anything on msclkid and dclid sizes - should we change them too?\n. Never mind - they end up in the same field anyway\n. I've split this into #1603, #1604, #1605, and #1606\n. You can get the leading hyphen by using a date that is BC (source). Although I don't know how we could have ended up with such a timestamp.\nMy plan is to return a Failure if the collector_tstamp is not set at all or if the result of toTimestamp starts with a hyphen, and a Success otherwise.\n. Presumably we should validate that all timestamp fields aren't negative? (e.g. it would be possible for somebody using the Python Tracker to set a negative dvce_tstamp.)\n. The point of this one is that if you are getting enrichments from DynamoDB and make a typo in the prefix you are looking for (e.g. if all your enrichments start with \"enrichment_\" but you configure the enricher to get all enrichments which start with \"enrichment-\") then no enrichments will be found but the app will still run. In this situation there should be some sort of indication that a mistake may have been made.\n. For unstructured events and custom contexts, we might not know all the field names inside those entities beforehand. So we need to actually change Elasticsearch's default rather than individually changing how each text field is indexed.\nThis thread suggests that using the keyword analyzer is equivalent to using no analyzer.\nThis thread has an example:\njson\ncurl -XPUT localhost:9200/test -d '{\n    \"index\" : {\n        \"analysis\" : {\n            \"analyzer\" : {\n                \"default\" : {\n                    \"type\" : \"keyword\"\n                }\n            }\n        }\n    }\n}'\n. I think this is what we want:\ncurl -XPUT 'http://localhost:9200/snowplow' -d '{\n    \"settings\": {\n        \"analysis\": {\n            \"analyzer\": {\n                \"default\": {\n                    \"type\": \"keyword\"\n                }\n            }\n        }\n    },\n    \"mappings\": {\n        \"enriched\": {\n            \"_timestamp\" : {\n                \"enabled\" : \"yes\",\n                \"path\" : \"collector_tstamp\"\n            },\n            \"_ttl\": {\n              \"enabled\":true,\n              \"default\": \"604800000\"\n            },\n            \"properties\": {\n                \"geo_location\": {\n                    \"type\": \"geo_point\"\n                }\n            }\n        }\n    }\n}'\n. We can then tokenize whichever specific fields we want:\ncurl -XPUT 'http://localhost:9200/snowplow' -d '{\n    \"settings\": {\n        \"analysis\": {\n            \"analyzer\": {\n                \"default\": {\n                    \"type\": \"keyword\"\n                }\n            }\n        }\n    },\n    \"mappings\": {\n        \"enriched\": {\n            \"_timestamp\" : {\n                \"enabled\" : \"yes\",\n                \"path\" : \"collector_tstamp\"\n            },\n            \"_ttl\": {\n              \"enabled\":true,\n              \"default\": \"604800000\"\n            },\n            \"properties\": {\n                \"geo_location\": {\n                    \"type\": \"geo_point\"\n                },\n                \"some_tokenized_field\": {\n                    \"type\": \"string\",\n                    \"analyzer\": \"english\"\n                }\n            }\n        }\n    }\n}'\n. I've updated the documentation on how to do this here.\n. Closing since this was done in r65 (but only involved changing documentation)\n. Here it is: https://github.com/snowplow/referer-parser/issues/101\n. The batch pipeline has special logic for the case where the URI starts \"s3\", but the Kinesis pipeline just tries to treat it as a normal URL.\n. Yes, this is already done: https://github.com/snowplow/snowplow/blob/feature/kinesis-5/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/sinks/KinesisSink.scala#L227-L229\n. This was done a long time ago and since superceded, closing and de-scheduling\n. Weirdly the amazon-kinesis-connectors library doesn't specify an encoding (https://github.com/awslabs/amazon-kinesis-connectors/search?utf8=%E2%9C%93&q=getdata). Is that just an oversight by them?\n. In R73, the column truncation code (which is only really needed for PG anyway) has moved from Hadoop Enrich (and by implication Kinesis Enrich) to Hadoop Shred. This means that the enriched event stream coming out of Kinesis Enrich will no longer have its JSON fields truncated (as of R74 - the Kinesis follow-up to R73).\n. Moving back since this isn't very pressing...\n. Closing as duplicate of #2009\n. See http://stackoverflow.com/questions/19543530/sbt-assembly-task-runs-slowly-after-adding-some-dependencies\n. So far I haven't got this to work - I am following these instructions and have added this:\nassemblyOption in assembly ~= { _.copy(cacheOutput = false) },\nbut without noticeable speedup.\nI notice that sbt assembly runs much faster in the host than in the guest. Maybe increasing the number of cores we give the snowplow/snowplow VM would help?\n. Here are some interesting links on giving a VM multiple cores:\nhttp://superuser.com/questions/297697/are-there-any-real-advantages-to-assigning-more-cores-to-virtualbox\nhttps://stefanwrobel.com/how-to-make-vagrant-performance-not-suck\n. Unfortunately increasing to 4 cores doesn't seem to reduce the assembly time either.\nIn this thread, joeroot is experiencing the same problem of performance on guest being much worse than performance on host: https://github.com/sbt/sbt-assembly/issues/68\n. Descheduling since we haven't made any progress.\n. @alexanderdean @jbeemster I think I've finally made progress on this one. My idea is to do the assembly outside the shared directory - i.e. copy the project to be assembled out of /vagrant and into e.g. /home/vagrant before assembling.\nI am testing this with the current version of scala-kinesis-enrich on a VM with 1 core without NFS enabled. Assembling outside the shared directory took 63 seconds. Assembling inside the shared directory is currently at 20 minutes and still going.\n. Replacing with #1665 as this doesn't seem to work.\n. Replacing with #1665 as this doesn't seem to work.\n. Good point! I had forgotten about that move...\n. I think that's a good idea - handle the null in the ResponseHandler. The key argument to storeRawEvent should probably be randomly generated if IP is null.\n. As far as I know, empty IP address strings shouldn't come up...\n. We could use the toOption method like this example\n. Hi @christiangda ,\nIf you want to turn off logging for the Scala Stream Collector, would your problem be solved by redirecting its output to /dev/null?\nbash\n./snowplow-stream-collector-0.3.0 --config myconfig.hocon >> /dev/null\n. ERROR is the highest level the Scala Stream Collector logs at, so you could try using grep:\nbash\n./snowplow-stream-collector-0.3.0 --config myconfig.hocon | grep \"ERROR\"\nDoes that meet your needs?\n. Our ticket about exploring logger configuration: #1526 \n. The documentation is about how to turn off Akka logging when running the collector in local mode (i.e. writing events to stdout rather than Kinesis) to prevent the event stream from being polluted by akka log messages. We will add the ability to configure the collector app's own logging level when writing events to Kinesis in a later version.\n(Note that you should already be able to set the Akka log level by setting loglevel = ERROR in the configuration HOCON. This will only affect log messages generated by Akka.)\n. Would this be covered by the domain_sessionid field? (It isn't currently populated by any tracker.)\n. I see - thanks for explaining!\n. I think this ends up being 0.15.0 since r66 seems to bump common enrich to 0.14.0.\n. This will also provide synchronized LRU caches for IP lookups and Scala Forex\n. Pushing back...\n. This should be in a Kinesis release, not a Snowplow CLI release\n. It would be interesting to have a field for why the app shut down (for example: exited normally or an uncaught exception or terminated using ctrl-C).\nBut I don't know whether it's possible to get that information from the JVM.\n. This seems to confirm that no information about why the VM is shutting down is provided.\n. We could add an optional section in the hocon like this:\nhocon\ntracking {\n  endpoint = \"mycollector.cloudfront.net\"\n  appId = \"productionSink\"\n}\nAlternatively we could use the DynamoDB app-name as the appId.\nI don't know whether there is any reason to make the tracker namespace (tna) configurable.\nI can't think of anything else that should be configurable.\n. I've implemented this on a separate thread. This means heartbeats will be sent regardless of whether the sink is stuck failing to write to Elasticsearch.\n. If the download process fails because the MaxMind url is bad, the result will be an empty ip_geo file. This means that if the app is restarted it will see the ip_geo file and not need to download it again. Then the empty ip_geo file will be used with every event, causing every event to fail enrichment.\nSo the app should check not only the existence of the file but also whether it is empty.\n. Doing it there mostly works but it doesn't see bad rows caused by oversized events (which are only introduced here). So I think that the logging should either be done in the enrichAndStoreEvents method or the KinesisSink's storeEnrichedEvents method.\nThis should only happen if the app is writing bad rows to Kinesis rather than stderr (since if bad events are being written to stderr logging adds nothing and pollutes stderr.)\n. The reason for this is that Kinesis gives each failed record its own error message, so different records in a batch could theoretically be rejected for different reasons.\nWe could log individual failures at a lower level so you aren't forced to see them.\n. The offending line is here.\nPossibilities:\n- Change that particular log level from error to debug\n- Log at the error level just a list of unique error codes (each together with an error message) in the responses to events in the batch, rather than all the error codes and messages.\n. The offending line is here.\nPossibilities:\n- Change that particular log level from error to debug\n- Log at the error level just a list of unique error codes (each together with an error message) in the responses to events in the batch, rather than all the error codes and messages.\n. Sounds good! I have implemented it that way with messages like this:\nscala\nerror(s\"$count records failed with error code ${errorCode}. Example error message: $sampleMessage\")\n. Using Sluice and performed after copy to processing, before starting the MR job\n. This would probably involve moving FixedPathLzoRaw to Common Enrich, since there's no need to have it in both Hadoop Enrich and Hadoop Shred.\n. Following discussion, let's close this one. We can test out kinesis-s3 -> s3.gz -> hadoop shred -> redshift.\n. @jbeemster - I think we came to the conclusion that this doesn't help?\n. Ok, closing.\n. The thing that we made configurable was the maximum total size of a PutRecords rather than the maximum size of an individual record.\nI'm glad that the individual record size limit is increasing - I'll incorporate it into kinesis-6 milestone.\nShould we actually try to pre-empt Amazon and make the maximum record size configurable too? It seems unlikely to me that they will change it again any time soon...\n. https://blogs.aws.amazon.com/bigdata/post/Tx3ET30EGDKUUI2/Implementing-Efficient-and-Reliable-Producers-with-the-Amazon-Kinesis-Producer-L\n. Does this actually require a change in the algorithm rather than just making the minBackoff and maxBackoff smaller?\n. If an \"in\" buckets is empty or nonexistent, EmrEtlRunner doesn't give a warning (or an error) - is this something we should change in Snowplow Sluice?\n. Of course! It's in the r67 branch now.\n. Updated title to reflect #1936 - the expected JSONs are now formatted as processing messages\n. I think I understand the problem. In writing the app I assumed that each thread would have its own instance of the KinesisSink. Actually it seems that if there are enough shards, an instance of the KinesisSink class can get shared between multiple threads. This leads to race conditions.\nWe can get around this either with synchronization or giving each thread its own copy of the KinesisSink using ThreadLocal. I think the second option will give better performance.\n. Until the above fix is released, a partial workaround is to change the record-limit field of the configuration hocon to something less than 500 so that if the race condition causes too many records to be set at once, it won't trigger an AWS exception.\n. Blocked by @alexanderdean \n. Blocked by @alexanderdean \n. Would this be just the access key and secret key?\nSomething like\nyml\n  :access_key_id: stdin\n  :secret_access_key: stdin\n?\n. Example of how this script operates:\nstorage_loader_config.yml:\n:aws:\n  :access_key_id: ADD HERE\n  :secret_access_key: ADD HERE\n:s3:\n  :region: ADD HERE # S3 bucket region\n  :buckets:\n    :jsonpath_assets: # If you have defined your own JSON Schemas, add the s3:// path to your own JSON Path files in your own bucket here\n    :enriched:\n      :good: ADD HERE # Must be s3:// not s3n:// for Redshift. This is the same as the :enriched:good: bucket specified for EmrEtlRunner\n      :archive: ADD HERE # Where to archive enriched events to\n    :shredded:\n      :good: ADD HERE # Must be s3:// not s3n:// for Redshift. This is the same as the :shredded:good: bucket specified for EmrEtlRunner\n      :archive: ADD HERE # Where to archive shredded types to\n:download:\n  :folder: # Not required for Redshift\n:targets:\n  - :name: \"My Redshift database\"\n    :type: redshift\n    :host: ADD HERE # The endpoint as shown in the Redshift console\n    :database: ADD HERE # Name of database \n    :port: 5439 # Default Redshift port\n    :table: atomic.events\n    :username: ADD HERE \n    :password: ADD HERE \n    :maxerror: 1 # Stop loading on first error, or increase to permit more load errors\n    :comprows: 200000 # Default for a 1 XL node cluster. Not used unless --include compupdate specified\neer_config.yml:\n:logging:\n  :level: DEBUG # You can optionally switch to INFO for production\n:aws:\n  :access_key_id: ADD HERE\n  :secret_access_key: ADD HERE\n:s3:\n  :region: ADD HERE\n  :buckets:\n    :assets: s3://snowplow-hosted-assets # DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket\n    :log: ADD HERE\n    :raw:\n      :in: ADD HERE\n      :processing: ADD HERE\n      :archive: ADD HERE    # e.g. s3://my-archive-bucket/raw\n    :enriched:\n      :good: ADD HERE       # e.g. s3://my-out-bucket/enriched/good\n      :bad: ADD HERE        # e.g. s3://my-out-bucket/enriched/bad\n      :errors: ADD HERE     # Leave blank unless :continue_on_unexpected_error: set to true below\n    :shredded:\n      :good: ADD HERE       # e.g. s3://my-out-bucket/shredded/good\n      :bad: ADD HERE        # e.g. s3://my-out-bucket/shredded/bad\n      :errors: ADD HERE     # Leave blank unless :continue_on_unexpected_error: set to true below\n:emr:\n  :ami_version: 3.6.0      # Don't change this\n  :region: ADD HERE        # Always set this\n  :jobflow_role: EMR_EC2_DefaultRole # Created using $ aws emr create-default-roles\n  :service_role: EMR_DefaultRole     # Created using $ aws emr create-default-roles\n  :placement: ADD HERE     # Set this if not running in VPC. Leave blank otherwise\n  :ec2_subnet_id: ADD HERE # Set this if running in VPC. Leave blank otherwise\n  :ec2_key_name: ADD HERE\n  :bootstrap: []           # Set this to specify custom boostrap actions. Leave empty otherwise\n  :software:\n    :hbase:                # To launch on cluster, provide version, \"0.92.0\", keep quotes\n    :lingual:              # To launch on cluster, provide version, \"1.1\", keep quotes\n  # Adjust your Hadoop cluster below\n  :jobflow:\n    :master_instance_type: m1.small\n    :core_instance_count: 2\n    :core_instance_type: m1.small\n    :task_instance_count: 0 # Increase to use spot instances\n    :task_instance_type: m1.small\n    :task_instance_bid: 0.015 # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances\n:etl:\n  :job_name: Snowplow ETL # Give your job a name\n  :versions:\n    :hadoop_enrich: 1.0.0 # Version of the Hadoop Enrichment process\n    :hadoop_shred: 0.4.0 # Version of the Hadoop Shredding process\n  :collector_format: cloudfront # Or 'clj-tomcat' for the Clojure Collector, or 'thrift' for Thrift records, or 'tsv/com.amazon.aws.cloudfront/wd_access_log' for Cloudfront access logs\n  :continue_on_unexpected_error: false # Set to 'true' (and set :out_errors: above) if you don't want any exceptions thrown from ETL\n:iglu:\n  :schema: iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-0\n  :data:\n    :cache_size: 500\n    :repositories:\n      - :name: \"Iglu Central\"\n        :priority: 0\n        :vendor_prefixes:\n          - com.snowplowanalytics\n        :connection:\n          :http:\n            :uri: http://iglucentral.com\nRun the command ruby 3-enrich/emr-etl-runner/config/combine_configurations eer_config.yml storage_loader_config.yml resolver.json combined.yml and get the following 2 files:\nresolver.json:\n{\n  \"schema\": \"iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-0\",\n  \"data\": {\n    \"cache_size\": 500,\n    \"repositories\": [\n      {\n        \"name\": \"Iglu Central\",\n        \"priority\": 0,\n        \"vendor_prefixes\": [\n          \"com.snowplowanalytics\"\n        ],\n        \"connection\": {\n          \"http\": {\n            \"uri\": \"http://iglucentral.com\"\n          }\n        }\n      }\n    ]\n  }\n}\ncombined.yml:\n```\naws:\n  access_key_id: ADD HERE\n  secret_access_key: ADD HERE\n  s3:\n    region: ADD HERE\n    buckets:\n      assets: s3://snowplow-hosted-assets\n      log: ADD HERE\n      raw:\n        in:\n        - ADD HERE\n        processing: ADD HERE\n        archive: ADD HERE\n      enriched:\n        good: ADD HERE\n        bad: ADD HERE\n        errors: ADD HERE\n        archive: ADD HERE\n      shredded:\n        good: ADD HERE\n        bad: ADD HERE\n        errors: ADD HERE\n        archive: ADD HERE\n      jsonpath_assets:\n  emr:\n    ami_version: 3.6.0\n    region: ADD HERE\n    jobflow_role: EMR_EC2_DefaultRole\n    service_role: EMR_DefaultRole\n    placement: ADD HERE\n    ec2_subnet_id: ADD HERE\n    ec2_key_name: ADD HERE\n    bootstrap: []\n    software:\n      hbase:\n      lingual:\n    jobflow:\n      master_instance_type: m1.small\n      core_instance_count: 2\n      core_instance_type: m1.small\n      task_instance_count: 0\n      task_instance_type: m1.small\n      task_instance_bid: 0.015\n    bootstrap_failure_tries: 3\ncollectors:\n  format: cloudfront\nenrich:\n  job_name: Snowplow ETL\n  versions:\n    hadoop_enrich: 1.0.0\n    hadoop_shred: 0.4.0\n  continue_on_unexpected_error: false\n  output_compression: NONE\nstorage:\n  download:\n    folder:\n  targets:\n  - name: My Redshift database\n    type: redshift\n    host: ADD HERE\n    database: ADD HERE\n    port: 5439\n    table: atomic.events\n    username: ADD HERE\n    password: ADD HERE\n    maxerror: 1\n    comprows: 200000\nmonitoring:\n  tags: {}\n  logging:\n    level: DEBUG\n  snowplow:\n```\n. Is it:\n\"#{assets_bucket}3-enrich/hadoop-etl/snowplow-hadoop-etl-#{hadoop_enrich_version}.jar\"\nif the first digit of the version is 0, and\n\"#{assets_bucket}3-enrich/scala-hadoop-enrich/snowplow-hadoop-enrich-#{hadoop_enrich_version}.jar\"\notherwise?\n. Would it be possible to incorporate this into the existing JS enrichment, maybe by letting users access an LRU cache and the DDB lookup method?\n. Good point! At the moment it looks like the credentials from the config file are used for getting records from Kinesis but not for writing to the bad stream.\n. Yes, this was my mistake...\n. Duplicate of #1935, closing...\n. To keep the number of files in the jar under 2^16, preventing this problem: https://github.com/sbt/sbt/issues/850\n. This will also involve bumping Scalazon to 0.11 to avoid dependency problems.\n. I don't understand why this stack trace doesn't seem to involve any of the EmrEtlRunner's functions...\nAnyway, this isn't covered by the current r68 list of rescued errors yet:\nhttps://github.com/snowplow/snowplow/blob/snowplow-cli-configuration-unification/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb#L442-L453\nSo I'll add it as a new recovery option.\n. I see!\n. Note that if you do try that build, the YAML key is now \"ssl_mode\" rather than \"sslmode\".\n. May get fixed by #1720 \n. I've just successfully reproduced this using a Hadoop Shred version which uses Hadoop 2.4.1. I get the same errors that were reported in the thread:\n{\"line\":\"snowplowweb\\tweb\\t2015-08-13 11:43:40.352\\t2015-06-09 10:36:45.000\\t2015-06-09 10:36:48.975\\tpage_view\\t29deb5cc-b6c5-4126-bb7a-89223c1a2354\\t\\tcloudfront\\tjs-2.4.3\\tcloudfront\\thadoop-1.0.0-common-0.14.0\\t1893875.322272248\\t37.157.33.178\\t2622883426\\t3865bf4f087cbad3\\t156\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\thttp://snowplowanalytics.com/test\\tPage not found <80><93> Snowplow\\t\\thttp\\tsnowplowanalytics.com\\t80\\t/test\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t{\\\"schema\\\":\\\"iglu:com.snowplowanalytics.snowplow/contexts/jsonschema/1-0-0\\\",\\\"data\\\":[{\\\"schema\\\":\\\"iglu:org.schema/WebPage/jsonschema/1-0-0\\\",\\\"data\\\":{\\\"genre\\\":\\\"test\\\",\\\"inLanguage\\\":\\\"en-US\\\"}},{\\\"schema\\\":\\\"iglu:org.w3/PerformanceTiming/jsonschema/1-0-0\\\",\\\"data\\\":{\\\"navigationStart\\\":1433846208625,\\\"unloadEventStart\\\":1433846208714,\\\"unloadEventEnd\\\":1433846208714,\\\"redirectStart\\\":0,\\\"redirectEnd\\\":0,\\\"fetchStart\\\":1433846208626,\\\"domainLookupStart\\\":1433846208626,\\\"domainLookupEnd\\\":1433846208626,\\\"connectStart\\\":1433846208626,\\\"connectEnd\\\":1433846208626,\\\"secureConnectionStart\\\":0,\\\"requestStart\\\":1433846208627,\\\"responseStart\\\":1433846208711,\\\"responseEnd\\\":1433846208713,\\\"domLoading\\\":1433846208721,\\\"domInteractive\\\":1433846208894,\\\"domContentLoadedEventStart\\\":1433846208899,\\\"domContentLoadedEventEnd\\\":1433846208907,\\\"domComplete\\\":0,\\\"loadEventStart\\\":0,\\\"loadEventEnd\\\":0,\\\"chromeFirstPaint\\\":1433846208929}}]}\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tMozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.81 Safari/537.36\\t\\t\\t\\t\\t\\ten-US\\t1\\t1\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t1\\t24\\t1680\\t1341\\t\\t\\t\\tEurope/London\\t\\t\\t2560\\t1440\\tUTF-8\\t1680\\t1341\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\",\"errors\":[{\"level\":\"error\",\"message\":\"Could not find schema with key iglu:com.snowplowanalytics.snowplow/contexts/jsonschema/1-0-0 in any repository, tried:\",\"repositories\":[\"Iglu Client Embedded [embedded]\",\"Iglu Central [HTTP]\"]}]}\n. It seems to work fine in AMI 3.7.0.\n. There's some information on changes between AMI versions here but none of the changes for 3.8.0 seem very suspicious\n. I agree - closing since probably won't support 3.8.0.\n. This is fixed by https://github.com/snowplow/iglu-scala-client/issues/42, just like #1926 \n. I haven't added the 2 relevant tickets to the CHANGELOG.\n. Blocked by @alexanderdean \n. Thanks!\n. I think it would - if the app stops getting records then it probably also stops checkpointing.\n. The nice thing about keeping this a String rather than a DateTime in the EnrichmentManager is that it means we don't need to convert the DateTime to a Redshift-compatible string with every event we process.\nInstead we could make EtlPipeline a class rather than an object and give it a constructor with a DateTime etlTstamp parameter.\n. The nice thing about keeping this a String rather than a DateTime in the EnrichmentManager is that it means we don't need to convert the DateTime to a Redshift-compatible string with every event we process.\nInstead we could make EtlPipeline a class rather than an object and give it a constructor with a DateTime etlTstamp parameter.\n. With Kinesis Enrich you would create a new EtlPipeline with a new timestamp (or update the timestamp of the existing EtlPipeline) for every call to GetRecords.\nBut proportionally, the effect on performance should be pretty small, so maybe we just make it a DateTime and do the conversion in each call to enrichEvent.\n. @alexanderdean do you think it would be better to just catch all nonfatal errors rather than having this growing list of specific errors?\n(This assumes that all the exceptions we want to catch really do descend from StandardError...)\n. Reminder to self: this shouldn't be merged until the cli-monitoring-tables branch (with the Redshift DDLs and JSON path files) has been included.\n. All the exceptions we currently handle are indeed subclasses of StandardError.\n. StackOverflow question here: http://stackoverflow.com/questions/31343646/cannot-load-lzop-compressed-files-from-s3-into-redshift\n. Current status on this:\nUsing Ruby 2.2.1 and an existing AWS account, EmrEtlRunner:\nworks with Elasticity 5.0.1\ndoesn't work with Elasticity 5.0.2 (apparently because of https://github.com/rslifka/elasticity/issues/86, which was fixed in 5.0.3)\ndoesn't work with Elasticity 5.0.3 - for some reason, the JobFlow.status method is returning nil.\n. I just edited my comment above to say that it is JobFlow.status, not JobFlow.steps, which is returning nil.\n. Good idea!\n. More information on what's going wrong:\nHere in 5.0.1, the AwsRequest returns an XML string which is correctly parsed by Nokogiri::XML.\nHere in 5.0.3, the AwsSession instead returns a JSON string which Nokogiri basically ignores. This is what causes the nil value for JobFlow.status.\n. Thanks for the update @rslifka ! Looks like it's going well.\n. I've updated to use the new API. Here's a sample error message from when I manually terminate a job:\nSnowplow::EmrEtlRunner::EmrExecutionError (EMR jobflow j-3J8LOJN8AWOOU failed, check Amazon EMR console and Hadoop logs for details (help: https://github.com/snowplow/snowplow/wiki/Troubleshooting-jobs-on-Elastic-MapReduce). Data files not archived.\nr67 Job: TERMINATING [USER_REQUEST] ~ elapsed time n/a [2015-07-20 10:25:24 +0000 - ]\n - 1. Elasticity S3DistCp Step: Raw S3 -> HDFS: COMPLETED ~ 00:01:53 [2015-07-20 10:25:24 +0000 - 2015-07-20 10:27:17 +0000]\n - 2. Elasticity Scalding Step: Enrich Raw Events: CANCELLED ~ elapsed time n/a [2015-07-20 10:27:17 +0000 - ]\n - 3. Elasticity S3DistCp Step: Enriched HDFS -> S3: CANCELLED ~ elapsed time n/a [ - ]):\nI've tried to make it identical to the error messages from the last version.\n. Looks like user-agent-utils has moved to Maven Central: http://www.bitwalker.eu/blog/user-agent-utils-now-at-maven-central\nSo we can also remove the GitHub-hosted repo as a dependency.\n. Hey @bogaert , could you give me some of the useragent strings which were resulting in surprising Opera versions so that I can see if version 1.16 of the library makes a difference?\n. Looks like 1.16 is more effective - it returns 26.0.1656.60 as the browser version for all four of the recent useragent strings you listed.\n. Good point!\n. Might as well keep it in r68...\n. I think we can close this since it's covered by #1914\n. At the moment I think there are 2 versions of this class: one in common enrich which uses Strings and one in hadoop shred which uses ProcessingMessages. Should we standardize on ProcessingMessages or Strings for the shared class?\n. Sounds good!\n. See #679 for discussion\n. Moving this from the r71 branch into its own branch since r71 doesn't contain any changes to Kinesis apps.\n. Moving these files will cause links to the example configuration will break. But I think it's still worth it - if we leave them in src/main/resources it looks as though they are meant to go in the final jar. I think we should give each subproject a directory named \"examples\" and move the corresponding example hocon there.\nThe alternative is to configure sbt not to include them, using something like unmanagedResourceDirectories in Compile := Nil.\n. I have split this into #2566, #2567, and #2568.\n. Is it worth having a configuration option to disable JSON validation in Scala Common Enrich for the benefit of people who have historical raw data containing schema-less contexts and unstructured events?\n. I've made comments for all the feedback I have. It's pretty superficial.\n. Thanks @danisola - I'll add your commits into the feature/r71 branch!\n. This is going into r71.\n. Edit: actually https://raw.github.com/HaraldWalker/user-agent-utils/mvn-repo/ is still functioning and is necessary to get UserAgentUtils. So we need both resolvers.\n. See also #1751 \n. I'm adding the whole stack trace to the message to make it possible to work out the source of the exception. Sample message for an artificially thrown NumberFormatException:\n{\"line\":\"...\",\"errors\":[\"error: Unexpected error processing events: java.lang.NumberFormatException\\n\\tat com.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$.enrichEvent(EnrichmentManager.scala:69)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:94)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:93)\\n\\tat scalaz.NonEmptyList$class.map(NonEmptyList.scala:23)\\n\\tat scalaz.NonEmptyListFunctions$$anon$4.map(NonEmptyList.scala:196)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:93)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:91)\\n\\tat scalaz.Validation$class.map(Validation.scala:114)\\n\\tat scalaz.Success.map(Validation.scala:347)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:91)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:89)\\n\\tat scala.Option.map(Option.scala:145)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:89)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:87)\\n\\tat scalaz.Validation$class.map(Validation.scala:114)\\n\\tat scalaz.Success.map(Validation.scala:347)\\n\\tat com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$.processEvents(EtlPipeline.scala:87)\\n\\tat com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:172)\\n\\tat com.snowplowanalytics.snowplow.enrich.hadoop.EtlJob$$anonfun$7.apply(EtlJob.scala:171)\\n\\tat com.twitter.scalding.MapFunction.operate(Operations.scala:58)\\n\\tat cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\\n\\tat cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\\n\\tat cascading.flow.stream.FunctionEachStage$1.collect(FunctionEachStage.java:80)\\n\\tat cascading.tuple.TupleEntryCollector.safeCollect(TupleEntryCollector.java:145)\\n\\tat cascading.tuple.TupleEntryCollector.add(TupleEntryCollector.java:133)\\n\\tat com.twitter.scalding.FlatMapFunction$$anonfun$operate$2.apply(Operations.scala:48)\\n\\tat com.twitter.scalding.FlatMapFunction$$anonfun$operate$2.apply(Operations.scala:46)\\n\\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\\n\\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\\n\\tat com.twitter.scalding.FlatMapFunction.operate(Operations.scala:46)\\n\\tat cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\\n\\tat cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\\n\\tat cascading.flow.stream.SourceStage.map(SourceStage.java:102)\\n\\tat cascading.flow.stream.SourceStage.call(SourceStage.java:53)\\n\\tat cascading.flow.stream.SourceStage.call(SourceStage.java:38)\\n\\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\\n\\tat java.lang.Thread.run(Thread.java:744)\\n\\n    level: \\\"error\\\"\\n\"],\"failure_tstamp\":\"2015-08-12T15:57:39.244Z\"}\nThe downside is that this can lead to very long messages - the one above is 3454 characters. Do we think this could cause problems later? If so I could try deleting all but the first few lines of the stack trace.\n. Here it is: https://groups.google.com/forum/#!topic/cascading-user/GwlSn7DFYaU\n. This is split into #2147, #2148, and #2149\n. This can indeed go into r71.\nThe fields set by the client in the querystring are listed here.\nWe can calculate the hash based on all of those fields, possibly minus some subset. user_ipaddress can be set either in the querystring or based on the actual origin of the event. Similarly, the page_url field set from the querystring can be overriden based on the Cloudfront log's referer field. So maybe these fields should be excluded.\n. OK - sounds like we are going to just calculate a hash based on the original querystring / post body hash, minus stm (which corresponds to dvce_sent_tstamp)\n. Would this be a new field in atomic.events, or would the true timestamp just go directly into the derived_tstamp field?\n. Closing as duplicate of #1980\n. Now that we no longer save the old table as atomic.events_0x0, should we add some sort of metadata to the table to indicate what version it is on?\n. Sample string output:\nCollectorPayload(schema:iglu:com.snowplowanalytics.snowplow/CollectorPayload//1-0-0, ipAddress:255.255.255.255, timestamp:1381175274000, encoding:UTF-8, collector:collector, userAgent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.8 Safari/537.36, path:/i, querystring:e=pp&page=Loading%20JSON%20data%20into%20Redshift%20-%20the%20challenges%20of%20quering%20JSON%20data%2C%20and%20how%20Snowplow%20can%20be%20used%20to%20meet%20those%20challenges&pp_mix=0&pp_max=1&pp_miy=64&pp_may=935&cx=eyJkYXRhIjpbeyJzY2hlbWEiOiJpZ2x1OmNvbS5zbm93cGxvd2FuYWx5dGljcy5zbm93cGxvdy91cmlfcmVkaXJlY3QvanNvbnNjaGVtYS8xLTAtMCIsImRhdGEiOnsidXJpIjoiaHR0cDovL3Nub3dwbG93YW5hbHl0aWNzLmNvbS8ifX1dLCJzY2hlbWEiOiJpZ2x1OmNvbS5zbm93cGxvd2FuYWx5dGljcy5zbm93cGxvdy9jb250ZXh0cy9qc29uc2NoZW1hLzEtMC0wIn0=&dtm=1398762054889&tid=612876&vp=1279x610&ds=1279x5614&vid=2&duid=44082d3af0e30126&p=web&tv=js-2.0.0&fp=2071613637&aid=snowplowweb&lang=fr&cs=UTF-8&tz=Europe%2FBerlin&tna=cloudfront&evn=com.snowplowanalytics&refr=http%3A%2F%2Fsnowplowanalytics.com%2Fservices%2Fpipelines.html&f_pdf=1&f_qt=1&f_realp=0&f_wma=0&f_dir=0&f_fla=1&f_java=1&f_gears=0&f_ag=0&res=1280x800&cd=24&cookie=1&url=http%3A%2F%2Fsnowplowanalytics.com%2Fblog%2F2013%2F11%2F20%2Floading-json-data-into-redshift%2F%23weaknesses, headers:[X-Forwarded-For: 123.123.123.123, 345.345.345.345], hostname:localhost, networkUserId:8712a379-4bcb-46ee-815d-85f26540577f)\n. This was done, closing...\n. Note that r71 doesn't yet have a changelog entry\n. @alexanderdean - I have made a new branch called final-stork-billed-kingfisher-rebase which is the same as feature/r71 with these changes:\n- Rebased to make history cleaner\n- Removed test version suffixes (\"0.16.0-M1\" -> \"0.16.0\")\n- Added missing tickets to the CHANGELOG\n- Fixed a comment typo\n- Wrapped the migrate_wd_access_log_1_r3_to_r4.sql script in a transaction\n- Updated copyright notice dates for files changed in this release\nSo when you get around to releasing, use final-stork-billed-kingfisher-rebase rather than this PR.\n. Update: I also removed the 2 augur-related files as discussed.\n. We can't use that exact regex since the regex needs to be able to extract groups. So instead we should use ^iglu:([a-zA-Z0-9-_.]+)/([a-zA-Z0-9-_]+)/[a-zA-Z0-9-_]+/([0-9]+-[0-9]+-[0-9]+)$\n. Necessary because the current regex, .+:([a-zA-Z0-9_\\.]+)/([a-zA-Z0-9_]+)/[^/]+/(.*), doesn't allow vendors with hyphens.\n. My guess: we originally didn't allow hyphens because we wanted our field names to be consistent with SQL.\nWe are already messing with field names to make them snake case: https://github.com/snowplow/snowplow/blob/master/4-storage/kinesis-elasticsearch-sink/src/test/scala/com.snowplowanalytics.snowplow.storage.kinesis.elasticsearch/ShredderSpec.scala#L33-L48\nSo to be consistent we should convert hyphens to underscores.\n. Sister of #1975 \n. Does a commit exist for this?\n. I have done this.\n. It shouldn't be too hard - we just need to publishLocal CommonEnrich before we do the tests for projects which have it as a dependency. I can't think of any other obstacles at the moment...\n. Moving back since I think there's enough in r73 already.\n. Thanks @christoph-buente! Closing since this is now in the wiki's master branch.\n. Should the MaxMind database configuration live in the ip_lookups configuration JSON?\n. @alexanderdean - looks like only one copy of the file has code for decoding \"%3D\". Do you know what the point of that is and whether it is necessary?\n. Thanks Alex!\nWe can avoid having Scalding as a dependency of Common Enrich by using a structural type in place of com.twitter.scalding.Args: https://github.com/snowplow/snowplow/blob/128591c621d73b3c0b26e29ee4e3dd2df640fbf7/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/utils/ScalazArgs.scala#L35-L37\n. Yes, closing...\n. I have added an r73 branch to the wiki repo containing the new CEM page.\n. Thanks! Can I just cherry-pick this commit? https://github.com/snowplow/snowplow/commit/e95c262cede627edaec1b04ef17cfaf70041dce8\n. This is done now (https://github.com/snowplow/snowplow/wiki/snowplow-tracker-protocol#12-date--time-parameter), closing...\n. Pushing back since I didn't discover what was causing this\n. Note that the SSC and SKE both have a \"size\" field in their bad row JSONs. This probably means we should change the BadRow class in SCE to include this field.\n. https://github.com/snowplow/snowplow/blob/2001aef473215d3422b5a9ae61537b1c4b94465e/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors/scalastream/utils/SplitBatch.scala#L130-L131\nhttps://github.com/snowplow/snowplow/blob/2001aef473215d3422b5a9ae61537b1c4b94465e/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/kinesis/sources/AbstractSource.scala#L108-L109\nWhen discussing how to handle events which are too large for Kinesis, we realised that we couldn't put the whole bad event in the error JSON, so we added a size field saying how large it is instead.\n. This was split into #1207 #2006 #2419\n. For reference, some code which can be used to serialize Avro to a string or byte array (e.g. prior to sending it to Kinesis): https://gist.github.com/fblundun/98e1892a9d8068e765c7\n. r72 will move some shredder logic into common enrich to get the event name, vendor, etc from unstructured event JSONs. As a side effect this will check whether unstructured events are valid JSONs. Should we continue to check whether contexts are valid JSONs in common enrich, or should that be the responsibility of later parts of the pipeline (currently hadoop shred and kinesis es sink)?\n. One benefit of doing this check is that we could make common enrich's output slightly smaller by removing unnecessary whitespace from JSONs.\n. For posterity, this is how I generated the full list of maximum field lengths:\nbash\ncat atomic-def.sql | grep  '^\\s*\"' | awk '{print $2}' | awk -F' |\\(|\\)' '{if ($0 ~ /char/) print $2\",\"; else print \"null,\"}'\nHaving access to an AST for the table would make this easier to do programmatically...\n. This release will involve removing the unstruct_event, contexts, and derived_contexts columns. So if the migration script upgrades tables in place, we should warn that the data in those columns will be gone for good.\n. Apparently Manifest itself is now deprecated and TypeTag should be used instead: http://stackoverflow.com/questions/10513336/how-do-the-new-scala-typetags-improve-the-deprecated-manifests\n. That all sounds good... So IUserEnrichment would look like this:\n``` scala\ntrait IUserEnrichment {\n  @throws[Exception]\n  def configure(config: JsonNode): Unit\n@throws[Exception]\n  def createDerivedContexts(event: EnrichedEvent): Array[JsonNode]\n}\n. java\ninterface IUserEnrichment {\npublic void configure(JsonNode config) throws Exception;\npublic ArrayList createDerivedContexts(\n    EnrichedEvent event,\n    JsonNode unstructEvent,\n    ArrayList existingContexts) throws Exception;\n}\n```\nWe could also have a top-level boolean configuration parameter \"requiresDerivedContexts\". If set to false then null would be passed as the value of existingContexts to every custom enrichment object. This would avoid the cost of converting existing contexts to JsonNodes for no reason.\nAn alternative: don't pass the existingContexts argument at all, and let the user deal with deserialization.\nThe same would apply to unstructured events.\n. @christoph-buente - we know you're using the JavaScript enrichment. Do you have any thoughts on our discussion here about a general JVM language enrichment?\n. TODO: check whether users will need to recompile against Scala Common Enrich when we add new fields to the EnrichedEvent class.\nEdit: the answer is no. Source.\n. We could potentially load custom enrichments into the cluster as a bootstrap action.\n. An example of source code for a custom enrichment:\n``` java\npackage com.acme;\nimport java.util.ArrayList;\nimport com.fasterxml.jackson.databind.;\nimport com.fasterxml.jackson.databind.node.;\nimport com.snowplowanalytics.snowplow.enrich.common.outputs.EnrichedEvent;\nimport com.snowplowanalytics.snowplow.enrich.common.enrichments.IUserEnrichment;\npublic class MyCustomEnrichment implements IUserEnrichment {\npublic void configure(JsonNode config) throws Exception {}\n\npublic ArrayList<JsonNode> createDerivedContexts(EnrichedEvent event, JsonNode unstructEvent, ArrayList<JsonNode> existingContexts) {\n    ArrayList<JsonNode> output = new ArrayList<>();\n    if (event.getV_tracker() != null) {\n        JsonNodeFactory factory = JsonNodeFactory.instance;\n        ObjectNode root = factory.objectNode();\n        root.put(\"schema\", \"iglu:com.snowplowanalytics.snowplow/my_schema/jsonschema/1-0-0\");\n        ObjectNode data = factory.objectNode();\n        data.put(\"trackerType\", event.getV_tracker().split(\"-\")[0]);\n        root.put(\"data\", data);\n        output.add(root);\n    }\n    return output;\n}\n\n}\n```\nThis is compiled using the Scala Common Enrich and Jackson-Databind libraries.\nEdit: in order to use the JsonNode.get method you also need the Jackson-Core library on the classpath because it contains the TreeNode interface which JsonNode implements (see here for more information).\n. There is a subtlety here which is worth making a note of:\nI am using a URLClassLoader to load the user-implemented class called MyCustomEnrichment from a given file location. MyCustomEnrichment implements IUserInterface, which is found in the Scala Common Enrich source.\nWhen I attempt to run Stream Enrich using sbt, I get this error:\njava.lang.NoClassDefFoundError: com/snowplowanalytics/snowplow/enrich/common/enrichments/IUserEnrichment\n    at java.lang.ClassLoader.defineClass1(Native Method)\n    at java.lang.ClassLoader.defineClass(ClassLoader.java:800)\n    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n    at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)\n    at java.net.URLClassLoader.access$100(URLClassLoader.java:71)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.CustomEnrichment$$anonfun$liftedTree1$1$2.apply(CustomEnrichment.scala:104)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.CustomEnrichment$$anonfun$liftedTree1$1$2.apply(CustomEnrichment.scala:100)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n    at scala.collection.immutable.List.foreach(List.scala:318)\n    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:105)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.CustomEnrichment.liftedTree1$1(CustomEnrichment.scala:100)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.CustomEnrichment.instances$lzycompute(CustomEnrichment.scala:100)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.CustomEnrichment.instances(CustomEnrichment.scala:100)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.registry.CustomEnrichment.getDerivedContexts(CustomEnrichment.scala:124)\n    at com.snowplowanalytics.snowplow.enrich.common.enrichments.EnrichmentManager$.enrichEvent(EnrichmentManager.scala:451)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:94)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2$$anonfun$apply$3.apply(EtlPipeline.scala:93)\n    at scalaz.NonEmptyList$class.map(NonEmptyList.scala:23)\n    at scalaz.NonEmptyListFunctions$$anon$4.map(NonEmptyList.scala:196)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:93)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2.apply(EtlPipeline.scala:91)\n    at scalaz.Validation$class.map(Validation.scala:114)\n    at scalaz.Success.map(Validation.scala:347)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:91)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1$$anonfun$apply$1.apply(EtlPipeline.scala:89)\n    at scala.Option.map(Option.scala:145)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:89)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$$anonfun$1.apply(EtlPipeline.scala:87)\n    at scalaz.Validation$class.map(Validation.scala:114)\n    at scalaz.Success.map(Validation.scala:347)\n    at com.snowplowanalytics.snowplow.enrich.common.EtlPipeline$.processEvents(EtlPipeline.scala:87)\n    at com.snowplowanalytics.snowplow.enrich.kinesis.sources.AbstractSource.enrichEvents(AbstractSource.scala:184)\n    at com.snowplowanalytics.snowplow.enrich.kinesis.sources.AbstractSource$$anonfun$5.apply(AbstractSource.scala:214)\n    at com.snowplowanalytics.snowplow.enrich.kinesis.sources.AbstractSource$$anonfun$5.apply(AbstractSource.scala:214)\n    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)\n    at scala.collection.immutable.List.foreach(List.scala:318)\n    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)\n    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)\n    at com.snowplowanalytics.snowplow.enrich.kinesis.sources.AbstractSource.enrichAndStoreEvents(AbstractSource.scala:214)\n    at com.snowplowanalytics.snowplow.enrich.kinesis.sources.KinesisSource$RawEventProcessor.processRecordsWithRetries(KinesisSource.scala:157)\n    at com.snowplowanalytics.snowplow.enrich.kinesis.sources.KinesisSource$RawEventProcessor.processRecords(KinesisSource.scala:148)\n    at com.amazonaws.services.kinesis.clientlibrary.lib.worker.V1ToV2RecordProcessorAdapter.processRecords(V1ToV2RecordProcessorAdapter.java:42)\n    at com.amazonaws.services.kinesis.clientlibrary.lib.worker.ProcessTask.call(ProcessTask.java:169)\n    at com.amazonaws.services.kinesis.clientlibrary.lib.worker.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:49)\n    at com.amazonaws.services.kinesis.clientlibrary.lib.worker.MetricsCollectingTaskDecorator.call(MetricsCollectingTaskDecorator.java:24)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: com.snowplowanalytics.snowplow.enrich.common.enrichments.IUserEnrichment\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n    ... 58 more\nThe problem appears to be that the URLClassLoader can't find the IUserEnrichment interface. However, this only happens when using sbt run - if I assemble the jar and then run it directly, there is no problem.\nBut sbt run is useful for rapid development so I think it's probably worth looking into a solution to this.\n. I have put an example enrichment in the snowplow-java-example-enrichment project and added a couple of TODOs to the kinesis-custom-jar branch.\n. Looks like this can't be done without creating a new table because 0.6.0 changed mkt_clickid from varchar(64) to varchar(128). \nhttp://stackoverflow.com/questions/17101918/alter-column-datatype-in-amazon-redshift\n. ```\n[debug] Running TaskDef(com.snowplowanalytics.snowplow.enrich.hadoop.jobs.good.LinkClickEventSpec, sbt.SubclassFingerprintWrapper@1a222f0c, false, [SuiteSelector])\njava.lang.Throwable: If you know what exactly caused this error, please consider contributing to GitHub via following link.\nhttps://github.com/twitter/scalding/wiki/Common-Exceptions-and-possible-reasons#javaneturisyntaxexception\n    at com.twitter.scalding.Tool$.main(Tool.scala:137)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.jobs.JobSpecHelpers$.runJobInTool(JobSpecHelpers.scala:171)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.jobs.good.LinkClickEventSpec$$anonfun$1.apply$mcV$sp(LinkClickEventSpec.scala:96)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.jobs.good.LinkClickEventSpec$$anonfun$1.apply(LinkClickEventSpec.scala:93)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.jobs.good.LinkClickEventSpec$$anonfun$1.apply(LinkClickEventSpec.scala:93)\n    at org.specs2.mutable.SideEffectingCreationPaths$$anonfun$executeBlock$1.apply$mcV$sp(FragmentsBuilder.scala:282)\n    at org.specs2.mutable.SideEffectingCreationPaths$class.replay(FragmentsBuilder.scala:254)\n    at org.specs2.mutable.Specification.replay(Specification.scala:12)\n    at org.specs2.mutable.FragmentsBuilder$class.fragments(FragmentsBuilder.scala:22)\n    at org.specs2.mutable.Specification.fragments(Specification.scala:12)\n    at org.specs2.mutable.SpecificationLike$class.is(Specification.scala:14)\n    at org.specs2.mutable.Specification.is(Specification.scala:12)\n    at org.specs2.specification.SpecificationStructure$$anonfun$content$1.apply(BaseSpecification.scala:55)\n    at org.specs2.specification.SpecificationStructure$$anonfun$content$1.apply(BaseSpecification.scala:55)\n    at org.specs2.specification.SpecificationStructure$class.map(BaseSpecification.scala:43)\n    at org.specs2.mutable.Specification.map(Specification.scala:12)\n    at org.specs2.specification.SpecificationStructure$class.content(BaseSpecification.scala:55)\n    at org.specs2.mutable.Specification.content$lzycompute(Specification.scala:12)\n    at org.specs2.mutable.Specification.content(Specification.scala:12)\n    at org.specs2.runner.TestInterfaceRunner.runSpecification(TestInterfaceRunner.scala:61)\n    at org.specs2.runner.TestInterfaceRunner.run(TestInterfaceRunner.scala:54)\n    at sbt.RunnerWrapper$1.runRunner(FrameworkWrapper.java:203)\n    at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:239)\n    at sbt.TestRunner.runTest$1(TestFramework.scala:84)\n    at sbt.TestRunner.run(TestFramework.scala:94)\n    at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:219)\n    at sbt.TestFramework$$anon$2$$anonfun$$init$$1$$anonfun$apply$8.apply(TestFramework.scala:219)\n    at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:207)\n    at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:219)\n    at sbt.TestFramework$$anon$2$$anonfun$$init$$1.apply(TestFramework.scala:219)\n    at sbt.TestFunction.apply(TestFramework.scala:224)\n    at sbt.Tests$.sbt$Tests$$processRunnable$1(Tests.scala:211)\n    at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:217)\n    at sbt.Tests$$anonfun$makeSerial$1.apply(Tests.scala:217)\n    at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)\n    at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)\n    at sbt.std.Transform$$anon$4.work(System.scala:64)\n    at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)\n    at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)\n    at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)\n    at sbt.Execute.work(Execute.scala:244)\n    at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)\n    at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)\n    at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)\n    at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: cascading.flow.FlowException: local step failed\n    at cascading.flow.planner.FlowStepJob.blockOnJob(FlowStepJob.java:219)\n    at cascading.flow.planner.FlowStepJob.start(FlowStepJob.java:149)\n    at cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:124)\n    at cascading.flow.planner.FlowStepJob.call(FlowStepJob.java:43)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: cascading.pipe.OperatorException: [com.twitter.scalding.M...][com.twitter.scalding.RichPipe.each(RichPipe.scala:487)] operator Each failed executing operation\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:107)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\n    at cascading.flow.stream.SourceStage.map(SourceStage.java:102)\n    at cascading.flow.stream.SourceStage.call(SourceStage.java:53)\n    at cascading.flow.stream.SourceStage.call(SourceStage.java:38)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalStateException: How did I get there??\n    at com.github.fge.jsonschema.core.util.URIUtils$2.apply(URIUtils.java:113)\n    at com.github.fge.jsonschema.core.util.URIUtils$2.apply(URIUtils.java:87)\n    at com.github.fge.jsonschema.core.util.URIUtils.normalizeURI(URIUtils.java:238)\n    at com.github.fge.jsonschema.core.ref.JsonRef.fromURI(JsonRef.java:183)\n    at com.github.fge.jsonschema.core.ref.JsonRef.fromString(JsonRef.java:207)\n    at com.github.fge.jsonschema.core.tree.BaseSchemaTree.extractDollarSchema(BaseSchemaTree.java:273)\n    at com.github.fge.jsonschema.core.tree.BaseSchemaTree.(BaseSchemaTree.java:97)\n    at com.github.fge.jsonschema.core.tree.CanonicalSchemaTree.(CanonicalSchemaTree.java:50)\n    at com.github.fge.jsonschema.core.load.Dereferencing$1.newTree(Dereferencing.java:50)\n    at com.github.fge.jsonschema.core.load.Dereferencing.newTree(Dereferencing.java:91)\n    at com.github.fge.jsonschema.core.load.SchemaLoader.load(SchemaLoader.java:136)\n    at com.github.fge.jsonschema.main.JsonValidator.buildData(JsonValidator.java:233)\n    at com.github.fge.jsonschema.main.JsonValidator.validateUnchecked(JsonValidator.java:149)\n    at com.github.fge.jsonschema.main.JsonValidator.validateUnchecked(JsonValidator.java:171)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$.validateAgainstSchema(validatableJson.scala:72)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3$$anonfun$apply$4.apply(validatableJson.scala:134)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3$$anonfun$apply$4.apply(validatableJson.scala:133)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3.apply(validatableJson.scala:133)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3.apply(validatableJson.scala:132)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2.apply(validatableJson.scala:132)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2.apply(validatableJson.scala:129)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$.validateAndIdentifySchema(validatableJson.scala:129)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonNode.validateAndIdentifySchema(validatableJson.scala:233)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1$$anonfun$apply$5.apply(Shredder.scala:128)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1$$anonfun$apply$5.apply(Shredder.scala:126)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n    at scala.collection.immutable.List.foreach(List.scala:318)\n    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:105)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1.apply(Shredder.scala:126)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1.apply(Shredder.scala:124)\n    at scalaz.Validation$class.map(Validation.scala:114)\n    at scalaz.Success.map(Validation.scala:329)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$.shred(Shredder.scala:124)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$loadAndShred$1.apply(ShredJob.scala:74)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$loadAndShred$1.apply(ShredJob.scala:73)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$.loadAndShred(ShredJob.scala:73)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$5.apply(ShredJob.scala:138)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$5.apply(ShredJob.scala:137)\n    at com.twitter.scalding.MapFunction.operate(Operations.scala:58)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\n    at cascading.flow.stream.SourceStage.map(SourceStage.java:102)\n    at cascading.flow.stream.SourceStage.call(SourceStage.java:53)\n    at cascading.flow.stream.SourceStage.call(SourceStage.java:38)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.net.URISyntaxException: Expected scheme-specific part at index 5: iglu:\n    at java.net.URI$Parser.fail(URI.java:2829)\n    at java.net.URI$Parser.failExpecting(URI.java:2835)\n    at java.net.URI$Parser.parse(URI.java:3038)\n    at java.net.URI.(URI.java:680)\n    at com.github.fge.jsonschema.core.util.URIUtils$2.apply(URIUtils.java:110)\n    at com.github.fge.jsonschema.core.util.URIUtils$2.apply(URIUtils.java:87)\n    at com.github.fge.jsonschema.core.util.URIUtils.normalizeURI(URIUtils.java:238)\n    at com.github.fge.jsonschema.core.ref.JsonRef.fromURI(JsonRef.java:183)\n    at com.github.fge.jsonschema.core.ref.JsonRef.fromString(JsonRef.java:207)\n    at com.github.fge.jsonschema.core.tree.BaseSchemaTree.extractDollarSchema(BaseSchemaTree.java:273)\n    at com.github.fge.jsonschema.core.tree.BaseSchemaTree.(BaseSchemaTree.java:97)\n    at com.github.fge.jsonschema.core.tree.CanonicalSchemaTree.(CanonicalSchemaTree.java:50)\n    at com.github.fge.jsonschema.core.load.Dereferencing$1.newTree(Dereferencing.java:50)\n    at com.github.fge.jsonschema.core.load.Dereferencing.newTree(Dereferencing.java:91)\n    at com.github.fge.jsonschema.core.load.SchemaLoader.load(SchemaLoader.java:136)\n    at com.github.fge.jsonschema.main.JsonValidator.buildData(JsonValidator.java:233)\n    at com.github.fge.jsonschema.main.JsonValidator.validateUnchecked(JsonValidator.java:149)\n    at com.github.fge.jsonschema.main.JsonValidator.validateUnchecked(JsonValidator.java:171)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$.validateAgainstSchema(validatableJson.scala:72)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3$$anonfun$apply$4.apply(validatableJson.scala:134)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3$$anonfun$apply$4.apply(validatableJson.scala:133)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3.apply(validatableJson.scala:133)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2$$anonfun$apply$3.apply(validatableJson.scala:132)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2.apply(validatableJson.scala:132)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$$anonfun$validateAndIdentifySchema$2.apply(validatableJson.scala:129)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonMethods$.validateAndIdentifySchema(validatableJson.scala:129)\n    at com.snowplowanalytics.iglu.client.validation.ValidatableJsonNode.validateAndIdentifySchema(validatableJson.scala:233)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1$$anonfun$apply$5.apply(Shredder.scala:128)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1$$anonfun$apply$5.apply(Shredder.scala:126)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n    at scala.collection.immutable.List.foreach(List.scala:318)\n    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n    at scala.collection.AbstractTraversable.map(Traversable.scala:105)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1.apply(Shredder.scala:126)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$$anonfun$shred$1.apply(Shredder.scala:124)\n    at scalaz.Validation$class.map(Validation.scala:114)\n    at scalaz.Success.map(Validation.scala:329)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.shredder.Shredder$.shred(Shredder.scala:124)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$loadAndShred$1.apply(ShredJob.scala:74)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$loadAndShred$1.apply(ShredJob.scala:73)\n    at scalaz.Validation$class.flatMap(Validation.scala:141)\n    at scalaz.Success.flatMap(Validation.scala:329)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$.loadAndShred(ShredJob.scala:73)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$5.apply(ShredJob.scala:138)\n    at com.snowplowanalytics.snowplow.enrich.hadoop.ShredJob$$anonfun$5.apply(ShredJob.scala:137)\n    at com.twitter.scalding.MapFunction.operate(Operations.scala:58)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:99)\n    at cascading.flow.stream.FunctionEachStage.receive(FunctionEachStage.java:39)\n    at cascading.flow.stream.SourceStage.map(SourceStage.java:102)\n    at cascading.flow.stream.SourceStage.call(SourceStage.java:53)\n    at cascading.flow.stream.SourceStage.call(SourceStage.java:38)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\n[error] Could not run test com.snowplowanalytics.snowplow.enrich.hadoop.jobs.good.LinkClickEventSpec: java.lang.Throwable: If you know what exactly caused this error, please consider contributing to GitHub via following link.\n```\n. The guilty $schema: \"$schema\": \"iglu:com.redacted/event/redacted/jsonschema/1-0-0\"\n. http://json-schema.org/latest/json-schema-core.html#anchor23\nI think the problem is that the JSON schema library can throw an exception if the $schema field is not a well-formed URI. In the above case, removing the colon from the field prevents the exception.\n. I think Hadoop Shred should have an equivalent of https://github.com/snowplow/snowplow/issues/1954 to make this sort of thing easier to debug in future.\n. This is the origin of the failure: https://github.com/fge/json-schema-validator/blob/fd781ef26318ce4b7bde294307dd32d6c2e8da03/src/main/java/com/github/fge/jsonschema/processors/validation/ValidationStack.java#L208.\nThere's room for improvement in the error message.\n. Moving this ticket to Iglu Scala Client: https://github.com/snowplow/iglu-scala-client/issues/33\n. This can be copied from Scala Hadoop Enrich\n. This looks great to me @kazjote - thanks very much!\n. Warbler has a config option for this: https://github.com/jruby/warbler/blob/master/lib/warbler/config.rb#L45\n. Ok - I've added those comments.\n. I really don't understand why Travis failed the DecodeBase64UrlSpec spec - it works for locally.\nhttps://travis-ci.org/snowplow/snowplow/jobs/86191378\n. That sounds plausible...\n. Note that I haven't yet rebased this to 1 commit per ticket\n. We could go further: let people specify whether each string they provide is an exact cookie name or a regular expression.\n. This is done, closing...\n. This is done, closing...\n. Hi @xkidro , what version of Scala Kinesis Enrich are you using?\n. The problem is your se_va field - a structured event's value should be a number, not a string. See the documentation here: https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#trackStructEvent\n. It still prints out the failing hash and contract - it's just that that information isn't very helpful for pinpointing the error since they are both quite large.\n. I think we should first explore creating a contract with a custom error message without changing the Contracts source, and make a PR if that approach is unworkable.\n. Well spotted - I forgot that the collector adds some parameters.\nhttps://github.com/snowplow/snowplow/blob/master/2-collectors/clojure-collector/java-servlet/war-resources/.ebextensions/server.xml#L138\nShould we also disable cv to prevent the collector version affecting the fingerprint?\n. Done!\n. My benchmark was flawed because it didn't take JIT optimization into account - the actual performance increase is a lot smaller. Still, I think this design change is an improvement.\n. Thanks @mattmueller , this is fixed now...\n. Hi @xkidro ,\n9300 is the default value used in the amazon-kinesis-connectors library. We override several of these default values here, but the port is not (yet) one of them.\n. Unfortunately not - we only support the transport client. Compatibility with Amazon Elasticsearch Service would be nice but as you say they don't support the transport client (source). A pull request adding the option to use the HTTP client would be welcome!\n. Note that we don't actually use the amazon-kinesis-connectors Elasticsearch emitter - we implement that ourselves: https://github.com/snowplow/snowplow/blob/9ee272a429dfa0da1aa1df6af8978948c2fd64ee/4-storage/kinesis-elasticsearch-sink/src/main/scala/com.snowplowanalytics.snowplow.storage.kinesis/elasticsearch/SnowplowElasticsearchEmitter.scala#L165\n. We have done this...\n. I edited the version 0.6.0 of SKE by adding the line t.enableEc2Context before tracking any events. I then ran the resulting jar on an EC2 instance. The monitoring events generated by the tracker (app_initialized and app_heartbeat) were correctly generated. But I haven't seen any EC2 context.\n. Using curl, I verified that the instance identity document is indeed at the expected endpoint.\n. Using curl, I verified that the instance identity document is indeed at the expected endpoint.\n. There isn't any akka output.\n. On my machine, there is a 3-second pause between starting the CLI and Ruby code in the main snowplow-emr-etl-runner.rb file executing.\nThe biggest delay is the 5 seconds taken by the statement require 'elasticity'. This suggests that we should load Elasticity lazily, since some invocations of the CLI will not need to use it.\n. For the storage-loader:\nThere is a 3-second pause before Ruby code in the main file begins executing.\nLoading the net/http module in snowplow-ruby-tracker takes 2 seconds.\nLoading fog takes 2 seconds.\n. We decided that EER and SL aren't going to be combined in a single app after all, meaning that it won't be necessary to wait for all the SL code to load before EER does anything. So the startup time is not going to be as problematic as we thought. I think we can postpone this ticket.\n. We could also use git blame to return the id of the last commit to touch that line.\n. Hello @jramos ,\nIt isn't currently possible to change the log level - there are some thoughts about this in ticket #1526.\nYou can go through some slightly weird steps to edit the Kinesis Enrich executable and change the logging level:\ntail -c +42 snowplow-kinesis-enrich-0.6.0 > snowplow-kinesis-enrich-0.6.0.jar\njava -Dorg.slf4j.simpleLogger.defaultLogLevel=error -jar snowplow-kinesis-enrich-0.6.0.jar {{config options}}\nThe tail command strips off the initial shell script bytes of the file, leaving you with a normal jar file which is runnable using java -jar.\n. Note that this won't actually tell you what went wrong with your events - for this you have to actually inspect the bad stream.\n. Looks like this is caused by the StorageLoader expecting a non-nil value in the configuration for aws.s3.buckets.shredded.archive, even though Postgres won't need that bucket.\nA workaround is to use the --skip shred option to prevent this line from executing.\n. Following the release of r73 in December, an archive bucket for shred is necessary even when not using Redshift. So failing if the bucket isn't provided is no longer a bug.\n. Once we are ready to merge these I will rebase to 1 commit per file.\n. Based on the output of aws emr describe-clusters, jobs which fail during bootstrapping signal it with \"Code\": \"BOOTSTRAP_FAILURE\". This must have changed since #354.\n. A downside to this: if the same event is processed twice, it won't necessarily end up with the same weather context both times. The context can depend on whether another event with a nearby location was processed first.\n. We should only be catching ConfigErrors here\n. See #2163\n. Thanks for spotting this @timelf123 !\n. Confusingly, the additional_info that you configure doesn't seem to show up anywhere in the AWS UI or the output of describe-cluster. But setting additional_info to a malformed JSON string causes the job creation to fail, so I assume it's working.\n. Sorry about that! I'll standardize on hadoop-elasticsearch-sink-0.1.0.jar?\n. Hi @mitulshah44 ,\nIf you want to load Postgres locally you need to set the storage.download.folder field in your . At the moment you have left this blank, causing the above error.\nIf you want to load Redshift rather than another Postgres database, you need to remove all targets with type \"postgres\" from the targets field in your configuration YAML.\n. yaml\nstorage:\n  download:\n    folder: mydownloadfolder\nYou will have to ensure that the mydownloadfolder directory exists. You don't have to put anything in it yourself.\n. Try using an absolute path to the download folder in the config file\n. If you want to run the shred step, you should uncomment this line:\n``` yaml\narchive: s3://metafunding-snowplow-emretlrunner/shredded/archive    # Not required for Postgres currently\n```\nOtherwise, you should run the StorageLoader with the --skip shred option.\n. This is technically already possible using ERB templating, e.g.\naccess_key_id: <%= require 'json'; JSON.parse(`curl http://169.254.169.254/latest/meta-data/iam/security-credentials/EMR_EC2_DefaultRole`)['AccessKeyId'] %>\nIn the Kinesis apps we mark that the credentials should come from the role by setting those fields to \"iam\". Should we do the same thing here?\nOf course, if we use the same configuration file for both EER and StorageLoader then this assumes that EER is also running on EC2 with the necessary role.\n. It is currently possible to do this outside StorageLoader like this:\nbash\n./snowplow-storage-loader --config <(base64 --decode encodedconfig)\nor\nbash\nbase64 --decode encodedconfig | ./snowplow-storage-loader --config -\nThis is a bit indirect since it requires wrapping SL in a script.\nHow about adding a new on/off --base64-decode-config switch?\n. Good point. We can have a --base64-config-string argument instead.\n. What is the maximum length for jobflow step arguments? The example config file is 7728 bytes when base 64 encoded.\nThis functionality can be tested easily with this command:\n./snowplow-storage-loader --base64-config-string $(base64 -w0 path/to/config.yml)\n. Great!\n. Yes, this should work. setCustomUrl will make the tracker use a string of your choice rather than window.location.href, so you just need to include the campaign parameters in that string's querystring.\n. This is fixed now. Thanks for spotting it @aragnon !\n. My understanding is that the groupBy completely filters out files which do not match the regex, then partitions files according to the captured group.\ne.g. for the files\n0a,\n0b,\n0c,\n1a,\n1b,\n1c\nand the regex [0-9]([a-z]), the files would end up in 3 groups like this:\n0a 1a,\n0b 1b,\n0c 1c\nbut with the regex ([0-9])[a-z] they would be grouped like this:\n0a 0b 0c,\n1a 1b 1c\nSo the regex (.*) would only concatenate files with exactly the same name.\nI haven't actually tried it - this is just based on reading http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_s3distcp.html\n. Closing as I have updated both the migration guide and the wiki page on shredding.\n. Closing as I have updated both the migration guide and the wiki page on shredding.\n. It's AMI 3.8.0 and higher.\n. Done!\n. Good point!\n. In the end we decided to make this a configurable setting, defaulting to random partitioning.\n. I'll add an optional boolean configuration setting called collector.sink.kinesis.useIpAddressAsPartitionKey defaulting to false.\n. The latest version is found at \nhttp://maven.snplow.com/releases/com/snowplowanalytics/snowplow-thrift-raw-event/0.1.0/snowplow-thrift-raw-event-0.1.0.jar\nThe original author probably thought that truncating that link to just\nhttp://maven.snplow.com/releases/com/snowplowanalytics/snowplow-thrift-raw-event/\nwould point to a listing of all the available versions, but apparently maven.snowplow.com doesn't work like that.\nI think the simplest answer is to just remove point 2 and change 0.2.0 to 0.0.0 in point 3, since 0.0.0 is the version we actually use here.\n. I just ran github-wiki-link-validator and got the following output:\n+ https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualize-Snowplow-data\n  + https://github.com/snowplow/snowplow/wiki/-setup-guide-images-chartio-cr3.PNG\nSo apparently the only fully broken link is the image link here. It looks like github-wiki-link-validator doesn't validate whether the fragment of a link refers to an actual HTML element.\n. I changed github-wiki-link-validator to look for broken anchors too. You can see the changed code and the list of broken links on the Snowplow wiki here.\n. By the way - that list is just for internal links within https://github.com/snowplow/snowplow/wiki. There may be unreported invalid external links.\nIf anybody finds any false positives or false negatives in my list of broken links please let me know so I can improve the script.\n. @ihortom sorry for the delay! I re-ran the script. Here are the results:\n```\nBad links on page https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker-v2.3\n\nBad links on page https://github.com/snowplow/snowplow/wiki/Canonical-event-model-v70\n\n\n\n\n\n\n\nPage pings\nBad links on page https://github.com/snowplow/snowplow/wiki/Javascript-Tracker-Core\nImporting the module\nCreating a tracker\nBad links on page https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker-v2.4\nSetting the application ID\nConfiguring cookie timeouts using setSessionCookieTimeout\nBad links on page https://github.com/snowplow/snowplow/wiki/.NET-tracker-setup\n\n\n\n\nBad links on page https://github.com/snowplow/snowplow/wiki/ActionScript3-Tracker\nsetViewport\nBad links on page https://github.com/snowplow/snowplow/wiki/Node.js-Tracker-v1\n\nBad links on page https://github.com/snowplow/snowplow/wiki/Configure-Scala-Kinesis-Enrich-v0.3\nconfiguring enrichments\nBad links on page https://github.com/snowplow/snowplow/wiki/canonical-event-model\n\n\n\n\n\n\n\nBad links on page https://github.com/snowplow/snowplow/wiki/Getting-started-with-Looker\n3.3\nBad links on page https://github.com/snowplow/snowplow/wiki/Setting-up-Qubole-to-analyze-Snowplow-data-using-Apache-Hive\nSign up to Qubole\nBad links on page https://github.com/snowplow/snowplow/wiki/1-Installing-EmrEtlRunner\nSetting up EMR command line tools\nBad links on page https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker\nDisabling cookies\nBad links on page https://github.com/snowplow/snowplow/wiki/Ruby-Tracker-v0.2\nCreating multiple trackers\nBad links on page https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker-v2.0\n\nBad links on page https://github.com/snowplow/snowplow/wiki/Setup-IAM-permissions-for-operating-Snowplow\nAdd the new user to your new group\nBad links on page https://github.com/snowplow/snowplow/wiki/Troubleshooting-jobs-on-Elastic-MapReduce\nDiagnosing an ETL job failure\nBad links on page https://github.com/snowplow/snowplow/wiki/Node.js-Tracker\ntrackEcommerceTransactionItem()\nBad links on page https://github.com/snowplow/snowplow/wiki/Setting-up-ChartIO-to-visualize-Snowplow-data\n/setup-guide/images/chartio/cr3.PNG\nBad links on page https://github.com/snowplow/snowplow/wiki/Canonical-event-model\n\n\n\n\n\n\n\nBad links on page https://github.com/snowplow/snowplow/wiki/Iglu-webhook-setup\nYour webhook\n```\nThere are a lot fewer bad links this time.\n. It looks like GitHub has changed how it handles links to nonexistent pages. I seem to remember that it used to send you to a \"this page does not exist\" page, but now it just redirects to the wiki homepage.\nThis doesn't actually effect the validator since it checks links for the \"internal absent\" class which GitHub apparently still attaches to invalid links.\n. I just re-ran the script and it didn't find any broken links!\n. To clarify: we already escape the whole tag once so that when the wizard appends it to the document, it is visible as a plain string. We need to escape the ampersands again so that when the wizard user copies and pastes the tag into their HTML, it is still correctly escaped.\n. Done!\n. Done!\n. There were some tickets created after the CHANGELOG was written. So we will need to do one more rebase just before merging.\n. I think this was intentional. There are actually 3 additional null entries in that PostgresConstraints list. They correspond to unstruct_event, contexts, and derived_contexts. They are there because the truncation happens before the deletion of those fields from the TSV (here).\n. Thanks for spotting this @rbkn. It's fixed now.\n. This will require some changes to the config file. At the moment it's like this:\nsource: stdin | kinesis\nsink: stdouterr | elasticsearch-kinesis\nHow about changing that to:\nsource: stdin | kinesis\nsink: {\n  good: elasticsearch | stdout\n  bad: kinesis | stderr | none\n}\n. Hi @jramos ,\nIt's an interesting point. This is the intended behaviour. The Clojure Collector behaves the same way. The problem is that the collector has no reliable way of knowing whether the browser will accept or block the cookie. So we might as well always set the cookie in the hope that it is accepted.\nI hope that makes sense!\n. While we are at it, do you think we should drop the \"Scala\" too? Since we usually just refer to it as \"Kinesis Enrich\" anyway.\n. Are we going to reset the version to 0.1.0?\n. Note: when we rebase this PR later, we only want the commits after c23de2e.\n. I agree that it would be better to do the fallback globally - I can't imagine why one collector instance would need to fallback but others wouldn't.\nAlso - if we have an external process restarting the collector, we will need to make sure that any records buffered by the collector are flushed before the restart.\n. I think communication rather than hard killing is a good idea. If Kinesis ceases to exist, the collectors need to do something with their in-memory data before they are killed and restarted. So we might as well just build in a way to switch between Kinesis and S3 (or another \"safe\" target) without the collector restarting.\n. At the moment, the tracker only knows whether to hold onto the event based on the status code returned by the collector. Is the idea that the collector would receive the event, quickly try to fully process and send it to Kinesis, and only then respond to the original request with a status code depending on whether the event was sent on successfully?\n. I see what you mean!\nWe could go even further and send a certain status code when the collector is under heavy load. Then any client-side caching tracker receiving that status code would stop sending events for a certain period.\n. Masking sensitive output shouldn't be too hard - we already do that when error messages might contain a user's AWS credentials.\n. This looks good to me. Thanks @kazjote !\n. It would be nice to actually calculate the number of processors in the Vagrantfile, but this seems to be quite awkward to do in Ruby.\n. @alexanderdean sure!\n. I actually did look over this but couldn't find any problems. I'll look again today.\n. I can see why that's the right approach for some APIs - e.g. if the API always returns a bad status code when the data you are looking for cannot be found.\nWe can explore extra configuration options for this - like how many times to retry, and whether to retry on certain status codes. e.g. maybe you want to retry on a 5xx status code but not a 404.\nAnyway we can postpone thinking about that until a later release.\n. @chuwy I have one more comment here. Other than that it all looks good to me!\n. \"at-least-once-processing\" is usually hyphenated. Is it possible to have hyphens in the title of a wiki page?\nRather than starting at TRIM_HORIZON or LATEST the apps can also start from a specific sequence number. We have never documented this in the config file comments. But we should probably start.\nOther than that it looks good!\n. You're right - it is for individual GetRecords calls, not configuring the whole KCL. My mistake!\n. Presumably this should happen even if the collector has previously set a different value for the cookie?\n. \"//://{{SUBDOMAIN}}.cloudfront.net/sp.js.cloudfront.net/2.6.0/sp.js\"\nshould be\n\"//{{SUBDOMAIN}}.cloudfront.net/sp.js.cloudfront.net/2.6.0/sp.js\"\nOther than that it looks good to me...\n. @alexanderdean I have updated the release badge.\n. See https://groups.google.com/forum/#!topic/snowplow-user/L8ZteHGwtWg\n. Sorry, didn't realize @ihortom was assigned rather than me! Closing since I have just fixed this.\n. I have just made this change everywhere in the wiki except the upgrade guide and the version matrix.\nI have left pages for earlier versions of SKE (like this one) as they were, since the project was still called Scala Kinesis Enrich at the time of those versions.\nI have tried to avoid creating outdated or broken links but I won't be surprised if I missed some - please let me know if you find any.\n. I just noticed the large comment in the markdown for the version matrix page here. Should I change it there too? Does that comment need to continue to exist?\n. I have removed it, and this ticket is finished.\n. For instance, Kinesis-S3 has lots of unnecessary EMR classes in.\n. Thanks for this @kala725 ! It looks good - I have just made a comment on the code about avoiding unnecessary ANALYZE statements.\n. @kala725 your latest commit looks good to me. Could you sign our CLA please?\n. @kala725 - ShreddedType's initialization logic involves deriving the vendor and name parts of the schema from the s3_objectpath here. So you could extract the vendor and name from each blacklisted schema, and ignore each ShreddedType returned by discover_shredded_types which matches both.\nI don't think we currently have a Ruby function to extract the name and vendor from a schema string, But a regular expression along the lines of /([^\\/]*)\\/([^\\/]*)/ should do it. e.g.\nruby\ninput = 'com.acme/shopping_cart/...'\nmatch = /([^\\/]*)\\/([^\\/]*)/.match(input)\nmatch[1] == 'com.acme'\nmatch[2] == 'shopping_cart'\nDoes that make sense?\n. It looks good to me. Only one question. At the moment it looks like you have to explicitly provide the shredded_types.exclude array even if you don't want to exclude any types. Should it default to an empty array if the setting is not included, for backwards compatibility? Or is that overcomplicating the config?\n. This can be reproduced by running two SSC processes attempting to use the same port.\nThis looks promising.\n. Re adding a longer timeout: it looks like this is done using the bind-timeout setting documented here, which apparently defaults to 1s. What do we think the timeout should be?\nAlso, if the target port is already in use the bind fails immediately regardless of the timeout. I think this is fine - if the port is already in use there is no reason to think it will become free in the near future.\n. Done!\n. This was discovered by running find . -name '*.scala' | xargs grep -ri '[^s]\".*$.*\"' in the project root.\n. I assume these 3 tickets are all going to have the same format for the two ways to get the config, i.e. dynamodb:region/table/key or file:path/to/file. Should we put the logic in Common Enrich to avoid code duplication?\n. Actually this should probably be in a new library, separate from Common Enrich. Any suggestions on what to call it?\n. I have only found one way to resolve this: using Runtime.getRuntime.halt().\n. I was able to reproduce this by setting akka.logger-startup-timeout = 1ms.\nBumping Akka to 2.3.9 and Spray to 1.3.3 solves this - the app crashes immediately instead of hanging. So I am closing this issue because it is covered by #2522.\n. Thanks @chuwy ! I agree with all your points and will make the necessary changes.\nI have been doing imperative commit messages for a while, since it's recommended here. I think we are all supposed to be moving to using them @alexanderdean ?\n. @alexanderdean could you publish rc2 please?\n. @alexanderdean this should be fixed by #2614 - could you try again please?\n. @alexanderdean could you try again please?\nAlso, is the test suite running particularly slowly for you? Based on your last comment, it looks like the PostSpec test took over 6 seconds to run.\n. Oops, you are right!\n. I prefer spaces - they are easier to look at on GitHub.\n. Moving back - for now we are letting users manually define which input buckets to reprocess using s3distcp regular expressions.\n. It would be possible for users to do this in the custom JavaScript.\n. Hi @liningalex , we actually fix this in the enrichment step instead of in the collector. Scala Common Enrich has access to the headers for the event and will use the X-Forwarded-For header (if it exists) as the IP address. The relevant ticket is here. Closing since I believe that makes this ticket unnecessary.\n. Closing - see #2642.\n. Closing as duplicate of #2688\n. How about \"Common: add encrypted S3 credentials to .travis.yml\"?\n. We could do this with a mandatory --input-format argument which would initially support two values, raw and bad.\n. When using raw collector files as input, I assume we would just pass an empty array as the \"errors\" argument of the user-defined function. That way, users could write a single function which would work in both cases.\n. I think there are two configurable things here:\n- What created the raw lines (cloudfront vs clj-tomcat)\n- Whether the raw lines are wrapped in a bad row JSONs\nAt the moment, only the second of these would change the logic that the job actually runs - I don't know how we would treat cloudfront vs clojure raw lines differently?\n. It does seem unfair to force users to decode Thrift using JS.\nIn a discussion we came to the conclusion that for the first Hadoop Event Recovery release we will make it possible to configure the input type as either raw lines or bad rows, and won't support Thrift. A future release will orthogonally make it possible to configure whether to expect Thrift, Cloudfront, or clj-tomcat log lines.\n. It's an interesting trade-off - the intermediate representation makes it easier to do this kind of programmatic analysis of the bad rows, but also means we lose the ability to see exactly what went into the enrichment process. I guess we could expand the bad rows to contain both the raw line and the intermediate representation.\n. I have renamed this ticket!\n. Good point.\n. Looks like this is the failing test\n. Oddly, when I call the function on \"\ub3e2\uacbb\" in the sbt console it doesn't fail:\nscala> ConversionUtils.decodeBase64Url(\"e\",\"\ub3e2\uacbb\")\nres5: scalaz.Validation[String,String] = Success()\nmaybe copy-and-pasting the characters involved changing their encoding or something.\n. An example with a different failing input\n. The Base64 class in commons-codec 1.5 is not thread-safe. This can cause corruption in the test suite (when tests are run in parallel) and in Stream Enrich (when multiple KCL threads simultaneously access an instance of the class).\nThe problematic instance\n. The bug in the base 64 library: https://issues.apache.org/jira/browse/CODEC-96\n. cc @ninjabear @alexanderdean @jbeemster @chuwy \n. sbt lets you set the version for the current session. So we could extract the version from the tag, then do\nsbt <<EOF\nset version := \"0.5.0\"\nassembly\nEOF\n. Done!\n. A big advantage of tag-driven builds: you can be selective about what you build. A given subproject may be changed in the development branch, but that doesn't necessarily mean that you want to spend time building and deploying it with every commit in that branch.\n. Before doing it in place, we should at least check whether it will actually works without giving us some sort of binary incompatibility error.\nWe could also just put the bootstrap script's S3 location in the configuration YAML.\n. @chuwy you are right, it was a typo. Should have said 1.5 instead of 0.5.0.\n. Thanks for raising @esquire900, but I am closing as this is a duplicate of https://github.com/snowplow/snowplow-javascript-tracker/issues/493 (which will be fixed in the next release of the JS Tracker and is currently in test).\n. Those are called \"/not-used/\" because when in testing, localMode is on, so the URI is ignored and the database is loaded from somewhere in test/resources/ instead.\n. @alexanderdean looks like the problem is mixed tabs and spaces. atomic-def.sql uses tabs, so I'm guessing tabs are the correct way to go?\n. So just have unprefixed fields - sku, category, etc?\n. Makes sense - I'll take this field out.\n. I think this should be \"user_agent_utils_config\" to match the name of the schema - otherwise how can the EnrichmentManager look up the enrichment in from the EnrichmentMap?\n. Similiarly I think this should be \"ua_parser_config\"\n. I see what you mean. I've made a new branch with a hopefully more readable version here.\nIt looks like the AdapterRegistry rejects anything without an approved path here - wouldn't this reject all events from the Clojure and Cloudfront Collectors with a null path?\n. @AALEKH - should this variable (check) be used somewhere or can I take it out?\n. This is testing the ability to override the built in mappings (like \"gclid\" -> \"Google\") with custom ones (like \"gclid -> \"Override\").\n. scala\nfor ((a,b) <- Some((5, 10)) {}\nworks but\nscala\nval x: Validation[NonEmptyList[String], (String,String)] = Success(\"a\",\"b\")\nfor ((a,b) <- x) {}\nfails with the error\ncould not find implicit value for parameter M: scalaz.Monoid[scalaz.NonEmptyList[String]]\nfor reasons which I do not understand...\n. I agree, it's too complicated for the EnrichmentManager. I will move it into enrichments/web/PageEnrichments.scala.\n. Sounds good!\n. Should we also update the field names in the configuration JSON? https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/campaign_attribution/jsonschema/1-0-1\n. Ticket created: https://github.com/snowplow/iglu-central/issues/155\n. @alexanderdean Should these cache sizes be 0?\n. Yes, that shouldn't be there...\n. I've added them now!\n. I think it's cleaner to keep this without the new.\n. This comment has become detached from the DateTime import\n. Fair enough!\n. Just tried this and it doesn't throw - it just returns Failure(\"Unstructured event couldn't be extracted\").\n. Validation is a Bifunctor. Looks like Dani is using <-: as an equivalent of leftMap.\nBlog post explaining Bifunctor: http://tonymorris.github.io/blog/posts/funky-scala-bifunctor/\nScaladoc for Bifunctor: http://docs.typelevel.org/api/scalaz/nightly/index.html#scalaz.Bifunctor (although this doesn't actually mention <-:).\n. Fair enough. How about also adding the querystring separators and parameter names like this?\nscala\n  def getEventFingerprint(parameterMap: Map[String, String]): String = {\n     val builder = new StringBuilder\n     parameterMap.foreach {\n      case (key, value) => if (! excludedParameters.contains(key)) {\n        builder.append(key)\n        builder.append(\"=\")\n        builder.append(value)\n        builder.append(\"&\")\n      }\n    }\n    algorithm(builder.toString)\n. @alexanderdean - are the vendor and name for the structured event schema key correct?\n. @alexanderdean - in the previous version of this table, this field was mistakenly named \"x_edge_request_type\" rather than \"x_edge_request_id\". Does this mean some sort of migration script is needed?\n. When combining the bad row classes for hadoop shred (which uses ProcessingMessages) and common enrich (which uses Strings) I originally converted all the ProcessingMessages to Strings. But I agree that it would be better to do it the other way round. I could instead change Common Enrich's Strings to ProcessingMessages like this:\nscala\nnew ProcessingMessage().setMessage(errorString).setLogLevel(LogLevel.ERROR)\nDoes that sound good @alexanderdean ?\n. Should SnowplowAdapter become a package rather than an object? Then the various Snowplow Adapters can go into their own files, rather than all in one big file.\n. I've added these extra tests!\n. I'm guessing this XXX isn't meant to be here.\n. For a malformed header like \"Cookie: first=a;second\", I believe this will give us the JSON\njson\n{\n  \"first\": \"a\",\n  \"second\": null\n}\nwhich would not pass the schema (since null values are not allowed).\nI don't know what the best approach is here - we could fail the whole event, or silently remove the bad headers, or alter the schema to allow nulls. @alexanderdean might have an opinion when he gets back from leave.\nIt shouldn't matter much since malformed cookie headers ought to be pretty rare.\n. Can these comment layouts be changed to\n/**\n *\n *\n */\nto match the rest of the repo?\n. scala\n          lookupSchema(eventType, VendorName, index, EventSchemaMap) match {\n            case Success(schema) => {\n              Success(\n                RawEvent(\n                  api = payload.api,\n                  parameters = toUnstructEventParams(TrackerVersion,\n                    queryString,\n                    schema,\n                    itm,\n                    \"srv\"),\n                  contentType = payload.contentType,\n                  source = payload.source,\n                  context = payload.context\n                )\n              )\n            }\n            case Failure(err) => Failure(err)\n          }\nis equivalent to\nscala\n          lookupSchema(eventType, VendorName, index, EventSchemaMap) map {\n            schema => {\n                RawEvent(\n                  api = payload.api,\n                  parameters = toUnstructEventParams(TrackerVersion,\n                    queryString,\n                    schema,\n                    itm,\n                    \"srv\"),\n                  contentType = payload.contentType,\n                  source = payload.source,\n                  context = payload.context\n                )\n            }\n          }\n. Ah, fair enough!\n. No problem! Wanting to do something to a Validation if it's a Success but leave it unchanged if it's a Failure is pretty common case.\n. Catching all Throwables can also catch Java system errors like OutOfMemoryError. Instead you can use case scala.util.control.NonFatal(e) => ... to only catch non-fatal errors. (We are switch to using NonFatal in the rest of the project in a coming release - see this ticket.)\n. Yes I think it is actually the same class.\n. Sorry @alexanderdean - this is fixed now with /0\\.[0-5]\\.[0-9]/.\n. The indentation is slightly off here\n. Good point! I will change this now.\n. Good spot! I'll remove the bundle exec line.\n. I'll remove it.\n. A couple of things here:\n- We don't automatically assume that the table is called \"atomic.events\" - the user can set the name of the table in the configuration file\n- It looks like this will run ANALYZE atomic.events once per skipped shredded type. We only want to run it once.\nI don't think we actually need to create any ANALYZE statements here, because they are already created here. So for skipped types we don't need to add anything to the array returned by get_shredded_statements. One way to do this would be to return nil to the array of SqlStatements for each skipped type, then call .compact on the resulting array to remove the nils.\n. The integration tests will only run if the CI environment variable is set to \"true\". Do you think we should do this differently, for example by having an sbt command to run integration tests? This would be faster than manually setting CI=true and starting the Python server process whenever you want to run integration tests locally.\nWhat you have at the moment works fine, so maybe my suggestion should be a ticket for a future release.\n. Can we move this compilation outside the method call so it only gets called once (instead of once per event)?\n. Yep good point!\n. The dangers of mutable state: I forgot that I was meant to modify the unsentRecords var and instead created a new unsentRecords val shadowing it.\n. @alexanderdean here I am catching and printing exceptions which occur when running the custom JS. Do you think we should just let it crash instead?\n. Yep, I think you're right...\n. Fair enough - I originally went with 0.1.0 thinking of this as a new project rather than a continuation of Scala Hadoop Bad Rows. Will update to 0.2.0...\n. ",
    "dideler": "The wiki says that contributors must sign a CLA. I've never signed one before so excuse me if this is a silly question. Is that still needed for a small change like this?\n. Thanks for the info. I just signed the Individual Contributor License Agreement. :+1:\n. ",
    "mrwalker": "We migrated from postgres to Hive after running the warehouse there for a few months.  I was impressed by how gracefully postgres degraded, but it only wound up handling data from about 2 of 10-ish clients.  With Hive, the only real bottleneck has been getting data out, not anything internal that some simple partitioning couldn't solve.\nThat being said, we did a little tuning in postgres before we gave up on it.  To diagnose issues, we used a tool from a colleague we worked with at Etsy that visualizes explains.  One trick was simply to bump the work_mem from the default 1MB to something much more generous like 512MB or 1GB, which let sorts be done in-memory.\nWe stopped short of actual indexing, but if we had, event and page_path would have been our first two targets because most of our queries are done along those dimensions (we partition by event now, and have several large per-page analytics jobs).  For ad-hoc analytics, users and visits are more important, so indexing domain_userid and domain_sessionidx makes sense.\nIt's unfortunate that we tore down our postgres warehouse or we could have used it as a testbed for you optimizations.\n. I honestly don't remember the specs at this point (we're many months into using Hive at this point), but I do know that we bumped the instance size at least twice and were in a similar range to you.\nThe great thing about Hive is that you don't have to worry about it eventual drastic failure, but it comes at the cost of some pretty stiff fixed overheads.  So you'll be expecting 1-5 minute queries, not 1-15s queries.  On the other hand, it's really easy to get started if you use a services like Qubole, and you avoid this bi-monthly performance crunch that usually requires you to migrate to the next instance size (you can seamlessly add/remove instances from your cluster, and we even keep multiple cluster specs laying around dormant for different workloads).\n. Do you mean monetary costs?  Depends on and scales with your workload, but Qubole shuts the instances down when they're not in use, which was a big win for us.\n. Sorry for missing this @kingo55 -- we didn't carefully measure performance, so I'm not quite sure.  However, the performance became unacceptably slow very quickly after some threshold, so it seemed like it degraded super-linearly.  Simple ad-hoc queries were taking between 2 and 5 minutes to return at around the time our nightly reporting began to become noticeably delayed.\nThe result was a mad scramble to Hive that only lasted about a day and a half thanks to stumbling on Qubole through Alex and Yali's blog posts.\n. Sorry, opened against wrong branch.\n. Hey @alexanderdean, we have not, but I'll do that shortly.\n. @alexanderdean CLA signed\n. ",
    "smugryan": "It's probably also worth investigating Redshift performance tuning as Redshift is postgresql-like and not 100% postgresql (no such thing as a real index in Redshift). I say this because Snowplow has Redshift as a recommended storage platform and anything they know can help many customers.\nCurrently we're working on improving performance for a few clusters running 2 to 16 machines with > 300 million rows (growing 20 million rows/day). Amazon sent us some information on tuning that could be linked to or the Snowplow docs could contain some best practices for tuning Redshift databases.\nhttp://docs.aws.amazon.com/redshift/latest/dg/tutorial-tuning-tables.html\n. Will do next time it occurs (which should be in a day or two). Thanks!\n. Oh and to answer your question about the next run overlapping with the current one, I don't believe this is occurring. Each run takes about 15 minutes and we run it hourly.\n. I did not find the log file left behind in the processing directory in the archive directory.\nAccording to our logs it was 'copied':\nMOVE smug-snowplow/processing/prod/processing/var_log_tomcat7_localhost_access_log.txt-1396461661.gz -> smug-snowplow/archive/prod/var_log_tomcat7_localhost_access_log.txt-1396461661.gz\nI'll look into turning on logs for our S3 buckets to get more info.\n. Ok, found the x in our logs:\n<13>Apr  2 11:24:10 ip-10-100-8-112 snowplow-live:       x smug-snowplow/processing/prod/processing/var_log_tomcat7_localhost_access_log.txt-1396461661.gz\n. Hi @alexanderdean , I'll look into our files to see if #700 affects us.\n. At first glance after reading the google group topic, this sounds like the exact same issue. It will take me a few hours to comb through our logs as we generate hundreds of log files per hour.\n. Sounds great, thanks!\n. Thanks! Sorry, I searched for 'emr retry' and didn't find those tickets, my fault.\n. That is awesome to hear! \nAnother (crazy) idea would be to allow for multiple snowplow pipelines to be managed via one box. We are setting up a 2nd pipeline here because there's other tools that will be pushing over 10x more volume of events. \n. :+1: \n. Why a separate and blank ticket? (Just curious if I could be filing tickets in a better manner)\n. :+1: \n. ",
    "pkallos": ":+1: nice was seeing a few of these myself\n. very cool idea :+1: \n. Can you also label this issue with \"2. Collectors\", or maybe create a separate ticket, because if I understand correctly an S3Sink would be useful for the raw Thirft data?\n@alexanderdean can I take a stab at making an S3Sink for the Kinesis collector? I will follow the above suggestions by @yalisassoon .\nI would also propose a design that allows for several sinks to be used simultaneously, so the raw data can be directed to Kinesis for realtime processing and to S3 for archiving and ETL. Maybe change the configuration schema to allow for an array of sinks?\nAs a warm up exercise I will try modifying the existing code to allow for concurrent usage of both the \"kinesis\" sink and the \"stdout\" sink :).\n. @alexanderdean here's my first attempt with a lengthy blurb in the pull request description: pkallos/snowplow#1 . Feedback from you and your team will be very appreciated.\n. That is a really good point! Good practice writing that patch anyways :) closing.\n. Oh wait, I just reread this:\n\nI'm not sure I understand this one. Why would you write the raw events to multiple Kinesis streams, when you could just have multiple apps reading from the same Kinesis stream?\n\nThe intention wasn't to write to multiple Kinesis streams, it was to be able to write into a Kinesis stream and an s3sink at once. You can have the events in Kinesis for realtime processing and store the raw events in s3 for ETL.\nDoes that make some sense? Or should the raw events be read from Kinesis and stored in S3 in another module?\n. @alexanderdean Ok yes that makes sense! Where is the right place to add the app? As a subcomponent in the snowplow/snowplow repository somewhere or as a standalone application?\nAnd also is there any documentation about the specific format and directory structure of the objects in S3? My inclination is to store them as s3://s3-bucket/some/s3/path/year=YYYY/month=MM/day=DD/hour=HH/raw-thrift-serialized-events.gz in 100MB chunks.\nIf there's an existing convention on how to store raw events in a snowplow-friendly way I will do it that way though!\n. fantastic! thanks\n. I'd like to address this issue next, I think it will close the gap for my particular use-case of snowplow. What's the best way to get started?\n. @alexanderdean the output to the Kinesis S3 Sink looks like binary on disk, it's the binary serialized Thrift events.\n. hmm when trying to run sbt test for feature/thrift-emr inside 3-enrich/scala-hadoop-enrich\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] downloading http://maven.snplow.com/releases/com/snowplowanalytics/snowplow-common-enrich/0.1.0/snowplow-common-enrich-0.1.0.jar ...\n[info]  [SUCCESSFUL ] com.snowplowanalytics#snowplow-common-enrich;0.1.0!snowplow-common-enrich.jar (483ms)\n[info] downloading http://repo1.maven.org/maven2/commons-lang/commons-lang/2.4/commons-lang-2.4.jar ...\n[info]  [SUCCESSFUL ] commons-lang#commons-lang;2.4!commons-lang.jar (205ms)\n[info] downloading http://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.1.1/httpclient-4.1.1.jar ...\n[info]  [SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.1.1!httpclient.jar (176ms)\n[info] downloading http://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.1/httpcore-4.1.jar ...\n[info]  [SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.1!httpcore.jar (117ms)\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  ::          UNRESOLVED DEPENDENCIES         ::\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\n[warn]  :: bitwalker#UserAgentUtils;1.11: not found\n[warn]  ::::::::::::::::::::::::::::::::::::::::::::::\nsbt.ResolveException: unresolved dependency: bitwalker#UserAgentUtils;1.11: not found\nAny ideas?\n. Hm ok that resolved the dependency issue but sbt test still fails for me when trying to run sbt test for feature/thrift-emr inside 3-enrich/scala-hadoop-enrich.\nIs this a problem with my local config? Or are these compilation errors legit? \n$ sbt test\n[info] Loading project definition from /home/pkallos/dev/snowplow/3-enrich/scala-hadoop-enrich/project\n[info] Set current project to snowplow-hadoop-etl (in build file:/home/pkallos/dev/snowplow/3-enrich/scala-hadoop-enrich/)\n[info] Compiling 6 Scala sources to /home/pkallos/dev/snowplow/3-enrich/scala-hadoop-enrich/target/scala-2.10/classes...\n[error] /home/pkallos/dev/snowplow/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala:156: type mismatch;\n[error]  found   : l.type (with underlying type String)\n[error]  required: _$1\n[error]       EtlJob.toCanonicalOutput(ipGeo, etlConfig.anonOctets, loader.toCanonicalInput(l))\n[error]                                                                                     ^\n[error] /home/pkallos/dev/snowplow/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala:155: ambiguous implicit values:\n[error]  both object CascadingTupleSetter in trait TupleConversions of type EtlJob.this.CascadingTupleSetter.type\n[error]  and object UnitSetter in trait TupleConversions of type EtlJob.this.UnitSetter.type\n[error]  match expected type com.twitter.scalding.TupleSetter[Nothing]\n[error]     .map('line -> 'output) { l: String =>\n[error]                            ^\n[error] two errors found\n[error] (compile:compile) Compilation failed\n[error] Total time: 41 s, completed Mar 11, 2014 1:37:39 PM\n. For the moment here's the patch I'm working with, doesn't look right but it's the best I could do given my weak scala-fu:\n``` diff\ndiff --git a/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala b/3-enrich/scala\nindex 802522b..0c67a3a 100644\n--- a/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala\n+++ b/3-enrich/scala-hadoop-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.hadoop/EtlJob.scala\n@@ -152,8 +152,8 @@ class EtlJob(args: Args) extends Job(args) {\n// Scalding data pipeline\n   val common = trappableInput\n-    .map('line -> 'output) { l: String =>\n-      EtlJob.toCanonicalOutput(ipGeo, etlConfig.anonOctets, loader.toCanonicalInput(l))\n+    .map('line -> 'output) { l: Any =>\n+      EtlJob.toCanonicalOutput(ipGeo, etlConfig.anonOctets, loader.asInstanceOf[CollectorLoader[Any]].toCanonicalInput(l))\n     }\n// Handle bad rows\n```\nSeems like a side effect of 944be1e05b14a0fa91a3a0bf1ae3992ee793e885\n. ^ yup totally stuck on that now :)\n. Hm okay so my findings after today:\n- scala-hadoop-enrich currently using scalding version 0.8.11\n- version 0.9.0rc4 brings in easy LzoThrift support into com.twitter.scalding.commons\n- if it's okay we should upgrade to 0.9.0rc4.\n- I'm working on some code to support that but upgrading to 0.9.0rc4 causes some compilation breakage.\nFrom there I need to alter the code in #546 to write LzoThrift files to S3 (right now it just writes plain old binary serialized but apparently that's not cool enough :smile:).\nThen as far as ETLJob is concerned it should just be a matter of intelligently deciding to use either MultipleTextLineFiles or a new LzoThriftSource source defined as \nscala\ncase class LzoThriftSource(p: String*) extends FixedPathSource(p: _*) with LzoThrift[SnowplowRawEvent]\nIf all goes well I will have a publishable branch soon.\n. Ok here's my current progress: https://github.com/pkallos/snowplow/compare/feature;thrift-emr\nTook shockingly long and didn't yield a whole lot but it's a start.\n- bumped to scalding 0.9.0rc4\n- added a failing unit test for lzothrift -- it doesn't even run, it chokes on java.util.NoSuchElementException: None.get somewhere\n- added lzothriftsource class\nSome guidance would be appreciated!\n. Alright some more progress here : https://github.com/pkallos/snowplow/compare/feature;thrift-emr\n- Bumped to rc15 because may as well\n- Resolved issues with JsonLine in tests, had to do something a bit goofy to get unit tests to pass, see the change that caused and this test mock for details\n- Unit tests now pass with the exception of the LzoThriftTest I added... Not clear at the moment why it's failing so I will start to throw myself at it now!\n. Holy jumping jackrabbits!! I got it:\n[info] Passed: Total 53, Failed 0, Errors 0, Passed 53\n[success] Total time: 24 s, completed Mar 14, 2014 4:34:25 PM\nThe trick to getting the LzoThriftSource to play nice with the rest of the code was to call\nscala\nLzoThriftSource(path).toPipe('line)\nAnd then you can use it like the other pipes and everything is happy, with minimal code-splitting and hackery!\nHere's the updated comparison , I will squash it down and submit a pull request soon. First I want to test and make sure it actually works in EMR.\n. :+1:\n. Thanks @alexanderdean! Looking forward to cleaning this code up.\nIn the meantime I am pondering issue 538 . \n. Updated this to use elephantbird ThriftBlockWriter, turns out this was the key to getting it to properly recognize the format when decoding in #538 . So exciting!!\n. @fblundun ahh good question, I hacked up something like this a while ago for testing:\n``` scala\nimport java.io.{FileInputStream, BufferedInputStream}\nimport com.snowplowanalytics.snowplow.collectors.thrift.SnowplowRawEvent\nimport com.twitter.elephantbird.mapreduce.io.{\n    ThriftBlockReader\n}\nimport com.twitter.elephantbird.util._;\nobject ThriftReader extends App {\n  val filename = args(0)\nval input = new BufferedInputStream(new FileInputStream(filename))\n  val typeRef = new TypeRefSnowplowRawEvent{}\n  val reader = new ThriftBlockReaderSnowplowRawEvent\n  for (i <- 1 to 10) {\n    println(reader.readNext())\n  }\n}\n```\n. Sure, it's been a while but let me know if there's any cleanup to do in this code\n. :+1:\n. > Can we find a way of making both approaches play nicely together? Maybe something like this:\n\nhttps://gist.github.com/alexanderdean/9588012\n\ntotally :+1: that would work\n. Here is my proposed fix, it appears to work. Feedback appreciated!\n. I have some code for this now, but it depends on #669 so I can cherry pick it over into a fresh branch and open the PR after 669 is ready/merged.\nCode is here.\n. Hm I am actually having some issues with this feature in Amazon EMR at the moment, so not fully confirmed working yet.\n. I actually didn't spend any more time on it, I spent a few hours on it but the documentation was above my head so I redirected my focus :. No new insight, sorry!\n. OK so it turns out this was browser useragent utils failing to recognize IE 11's useragent. \nI first took a stab at #62 but the deltas between ua_parser and the existing library are pretty significant.\nua_parser doesn't report as many fields so the clientattributes would have to be paired down to \ndiff\n   case class ClientAttributes(\n       // Browser\n-      browserName: String,\n       browserFamily: String,\n-      browserVersion: Option[String],\n-      browserType: String,\n-      browserRenderEngine: String,\n+      browserVersion: String,\n       // OS the browser is running on\n-      osName: String,\n       osFamily: String,\n-      osManufacturer: String,\n       // Hardware the OS is running on\n       deviceType: String,\n       deviceIsMobile: Boolean)\nnot sure if worth doing but in my case losing the convenience of \"browserType\" -> Computer, Tablet, Mobile, ... is kind of a non-starter.\nWill open a PR that addresses this ticket.\n. :+1:\n. This is great! Can I suggest not checking in a copy of the kinesis-connector java libs? pkallos/amazon-kinesis-connectors has a pom.xml and the jars are temporarily hosted on a maven repo against github. You can include by adding https://raw.githubusercontent.com/pkallos/amazon-kinesis-connectors/mvn-repo/ as a resolver and then.\nxml\n<dependency>\n   <groupId>com.amazonaws</groupId>\n   <artifactId>amazon-kinesis-connector</artifactId>\n   <version>1.0.0</version>\n</dependency>\n@alexanderdean Does snowplow have a maven repo that could host the amazon-kinesis-connector libs?\n. I disagree, it's totally a library! Granted it has a few samples in the codebase, but the code is modular and stands alone. Plus it's listed here under \"Libraries\" :).\nIt's generally bad practice to version control source code that is already being versioned elsewhere. IMO should be an external dep . Also the package is namespaced under com.amazon.\nThat way if you find a bug and need to back-contribute changes you can open a pull request to the main repo, or maintain your own fork :smile: . And if @awslabs releases a new version it will be easier to upgrade without having to commit another large blob of code to this repo.\n. @potomak you can add it to your pom.xml like this\n. @bugant @potomak are you using this with any success in production? I am having the following issues using com.snowplowanalytics.snowplow.kinesis.redshiftbasic.RedshiftBasicExecutor:\n- The output format is incorrect, \\n is not used as a separator for rows, because of how the default S3Emitter works. Couple of ways to fix this, either shim the scala-kinesis-enrich record output to append a newline, or override the emit method in S3Emitter and join records with a \\n character. @alexanderdean any thoughts on this?\n- I am getting the following error before REDSHIFT COPY command gets run, which causes the load to fail.\n```\nJun 22, 2014 1:17:27 PM com.amazonaws.services.kinesis.connectors.redshift.RedshiftBasicEmitter emit\nSEVERE: java.io.IOException: org.postgresql.util.PSQLException: ERROR: S3Service\nException:The AWS Access Key Id you provided does not exist in our records.,Status 403,Error InvalidAccessKeyId,Rid B7FDA6608B34DDE7,ExtRid XXXXXX/XXXXXX/XXXXXXXXXXXXXXXXXXXXXXXXX,CanRetry 1\n  Detail:\n\nerror:  S3ServiceException:The AWS Access Key Id you provided does not exist in our records.,Status 403,Error InvalidAccessKeyId,Rid B7FDA6608B34DDE7,ExtRid XXXXXX/XXXXXX/XXXXXXXXXXXXXXXXXXXXXXXXX,CanRetry 1\n  code:      8001\n  context:   Listing bucket=xxbucketxx/49539002103272760693888231783979533373498568791159734273-49539002103272760693890055962535999495967614988355371009\n  query:     471109\n  location:  s3_utility.cpp:525\n  process:   padbmaster [pid=14949]\n```\nNot clear what is causing this but maybe I have my IAM roles misconfigured (as far as I know I have allowed s3:* permissions to the relevant machine).\n. I see your point but the kinesis enrich output is tab separated isn't it?\n. nice :+1:\n. :+1:\n. > Hey @pkallos - how come you didn't need this already? I thought you were running behind ELB...\nI just used /i as healthcheck :).\n. true, healthcheck is the way to go I was just lazy/in a hurry :+1:\n. What type of de-duplication window are you exploring? 2 mins? 2 hours?\nDepending on the window size, this may be a good opportunity to explore Spark Streaming to process batches and do the event de-duplication within a trailing window of time, might avoid the complication of persisting to DynamoDB.\n. > The trouble with using Spark Streaming for de-duplication (vs using it for e.g. identity stitching or sessionization) is that we have to write the de-duplicated output somewhere, and if we write it to Kinesis then we are immediately introducing dupes again.\nAh I see, good point :)\n. For what it's worth, on our internal applications that sink data to .thrift.lzo data to S3, we actually decided to disable writing of the lzo.index files because we found that there was not much benefit.\nAnd there was also pretty significant overhead in trying to fetch and read all of the .index files (which is more of an issue when reprocessing archived data in hadoop)\n. Basically the equivalent of changing createIndexedOutputStream to createOutputStream here\nAnd not emitting the index file here\n. I believe it will happily process the lzo files with missing index files, although it will still attempt to get file statuses for the index files (not a big deal).\nAlso worthwhile upgrading elephant-bird from 4.5 to 4.10, there are a handful of performance improvements (however you will need to take care to peg your thrift version down to <= 0.9.0)\nFull disclosure that I haven't tested this with Snowplow Hadoop Enrich itself, but another project we have that uses elephant-bird. YMMV :)\n. I believe the buffer and/or time limits from the KCL are indeed tied to a shard+worker thread.\nI have however run into situations where ShardIterators expire faster than a buffer can be filled+flushed because the buffer size is set too large.\nAccording to the AWS documentation:\n\nA shard iterator expires five minutes after it is returned to the requester.\n\nfrom which I gather that a time-limit settings of >5mins won't yield correct results. From my memory, when a ShardIterator expires, the worker thread is interrupted and this creates a new ShardIterator from the last checkpoint, and also tosses away all the buffered data from the prior attempt.\nI've had the most success by tuning all 3 of the buffering settings to correspond closely to the data volume I expect, i.e. if I want 50MB chunk files I try and ballpark the record-size limit and file-size limits to match, and then adjust the timeout to be <5mins to ensure a health regular flush.\nMight be possible to just issue a warning when the time-limit exceeds 5mins? \nHope this helps!\n. I think the rule is:\n1 Shard = 1 Thread = 1 Buffer = 1 File in S3\nAnd all of the buffering settings/rules apply to the individual thread, which is not super intuitive but when you look at the implementation it makes sense. \nSince sequence numbers are specific to a Shard, and a Buffer object maintains the starting and ending sequence number, and the file in s3 contains the start+end seq numbers... But yeah not clearly documented anywhere and not super intuitive when looking at the available settings for the KCL.\n. :+1:. This should be bufferByteSizeLimit\n. what about defaulting this to \\t to match the output from scala-kinesis-enrich ?\n. maybe more idiomatic to scala:\nscala\nif (co.network_userid == null) {\n  ci.userId.foreach(co.network_userid = _)\n}\nOr even replace the if/null\nscala\nOption(co.network_userid).foreach(_ => ci.userId.foreach(co.network_userid = _))\n. off by 1 space here\n. How about instead\nscala\nprivate val thriftSerializer = new ThreadLocal[TSerializer] {\n override def initialValue = new TSerializer()\n}\nSo the initialization is handled outside of the serializeEvent code?\n. ",
    "bamos": "Hi Alex, can you glance over the tests and let me know if there's anything else they should check?\nBy the way, do you want me to add more comments to the tests better explaining how the collector should act? I'm about to make a pass and comment/clean the collector code before writing the main documentation.\n. Additional config options: https://github.com/snowplow/snowplow/wiki/additional%20configuration%20options\n. @alexanderdean, should we test that we're storing Kinesis events, or just the HTTP responses? The difficulty with testing Kinesis is that we have to retrieve the events, which is difficult to get right using direct access, but would require some tweaking to do right using the IRecordProcessor.\n. Hi @alexanderdean  - again, just taking a stab here. Let me know if you want me to change anything on the stream collector tests, and if I should go ahead and do something similar for the Kinesis-based enrichment.\n. Hi @alexanderdean  - currently, empty payloads are represented with an empty String rather than null:\nScala\n        storedEvent.payload.protocol must beEqualTo(PayloadProtocol.Http)\n        storedEvent.payload.format must beEqualTo(PayloadFormat.HttpGet)\n        storedEvent.payload.data must beEqualTo(\"\")\nBefore creating the event in the collector, should we change empty strings to null with:\nScala\nOption(s).filter(_.trim.nonEmpty).getOrElse(null)\n. Okay, re-open if there's anything else on this.\n. Should 'contributing' point here?\nAlso, in the Clojure collector, there aren't any links to 'roadmap' or 'contributing'.\nhttps://github.com/snowplow/snowplow/tree/feature/scala-rt-coll/2-collectors/clojure-collector\nDo you want me to replace these with 'coming soon' as in the cloudfront collector?\nhttps://github.com/snowplow/snowplow/tree/feature/scala-rt-coll/2-collectors/cloudfront-collector\n. Okay, will do.\nAlso, how does this outline for the Scala collector setup guide look?\n1. Create a Scala/sbt environment.\n   - Such as using our Vagrant VM [link]\n2. Setup configuration options\n   - web service interface and port\n   - AWS credentials\n   - Kinesis stream information\n3. Build the assembly and run\n   - Using java -jar\n4. Additional configuration options\n   - P3P\n   - Cookie duration\n. Also, should the key used in Kinesis be a config option?\n. In about-snowplow/Developer-FAQ:\nHow should this portion read? Should we mention the Scala stream collector and Kinesis's real-time abilities, or just remove 1?\n```\nIs Snowplow real-time?\nNo, currently Snowplow is not a real-time analytics solution. This is for two main reasons:\n\n\nBoth of the supported collectors (the CloudFront collector and the Clojure-based collector) feature a lag (typically 20-60 minutes) before events are written to Amazon S3\n\n\nOur ETL process (which takes raw Snowplow events and enriches them) is based on Hadoop/Hive, which are batch-based processing tools. They are not designed for real-time (or near-real-time) data processing\n\n\nWe have real-time support for Snowplow on our radar, but this is not a priority currently.\n```\n. Oh, also, on the Developer-FAQ, should something be added here?\n```\nDoes Snowplow scale?\nYes! In fact we designed Snowplow primarily with extreme scalability in mind. In particular:\nxxx\nyyy\nzzz\n```\n. Hi @alexanderdean, when you get a chance, can you make a pass on\nhttps://github.com/snowplow/snowplow/wiki/setting-up-the-Scala-Stream-collector\nand\nhttps://github.com/snowplow/snowplow/wiki/Scala-stream-collector\n. Oops, just noticed this says 'lib' folder. Is pulling from maven fine?\n. Hi Alex, sure I'll do it shortly.\n. Hi Alex, I cloned the latest copy and building and testing the collector worked great. If you were having any issues, one of my commits earlier today may have fixed it. Let me know if there's anything else on this, though.\n. or maybe ctrl-A as the delimiter as impossible for it to appear in a payload (probably)\n. Hi @alexanderdean, Thrift doesn't provide an automatic way to delimit fields, and hardcoding them into the stdout backend would be difficult to maintain because every time the Thrift schema is updated, the StdoutBackend would have to be updated to print out the new/modified field.\nWhat if instead we just dump the serialized SnowplowEvent object?\nThen whatever's reading the serialized object from stdout can use Thrift to deserialize rather than writing something custom to the stdout backend and having to maintain that as the Thrift schema grows.\n. @alexanderdean , how's this?\n~/scala-stream-collector [feature/scala-rt-coll*] \u00bb java -jar target/scala-2.10/snowplow-scala-collector-0.0.1.jar | grep '^\\[Snowplow\\]'\n[Snowplow] CgABAAAAAFLBa1EMAAoIAAEAAAABCAACAAAAAQsAAwAAAAAACwAUAAAAHnNub3dwbG93LXNjYWxhLWNvbGxlY3Rvci0wLjAuMQsAHgAAAAVVVEYtOAsAKAAAAAkxMjcuMC4wLjELACkAAAAJMTI3LjAuMC4xCwAyAAAAaE1vemlsbGEvNS4wIChYMTE7IExpbnV4IHg4Nl82NCkgQXBwbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzMxLjAuMTY1MC42MyBTYWZhcmkvNTM3LjM2CwBQAAAAJDVmYTExZWJmLTA3YWQtNDI0OC1iMzQ0LWJjY2MyNzBmNTMwMAA=\n[Snowplow] CgABAAAAAFLBa1kMAAoIAAEAAAABCAACAAAAAQsAAwAAABFudGFoZXU9YW5vdGVodWFvYwALABQAAAAec25vd3Bsb3ctc2NhbGEtY29sbGVjdG9yLTAuMC4xCwAeAAAABVVURi04CwAoAAAACTEyNy4wLjAuMQsAKQAAAAkxMjcuMC4wLjELADIAAABoTW96aWxsYS81LjAgKFgxMTsgTGludXggeDg2XzY0KSBBcHBsZVdlYktpdC81MzcuMzYgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvMzEuMC4xNjUwLjYzIFNhZmFyaS81MzcuMzYLAFAAAAAkYzhiODQ0NWYtNjU3NC00NDUyLThiMWMtZWQxZjc3ZDk2YmYyAA==\n. Okay, here's how it works now if logging is disabled:\n~/scala-stream-collector [feature/scala-rt-coll*] \u00bb java -jar target/scala-2.10/snowplow-scala-collector-0.0.1.jar\nCgABAAAAAFLBdBwMAAoIAAEAAAABCAACAAAAAQsAAwAAAAthbnRob2V1PWFuYQALABQAAAAec25vd3Bsb3ctc2NhbGEtY29sbGVjdG9yLTAuMC4xCwAeAAAABVVURi04CwAoAAAACTEyNy4wLjAuMQsAKQAAAAkxMjcuMC4wLjELADIAAABoTW96aWxsYS81LjAgKFgxMTsgTGludXggeDg2XzY0KSBBcHBsZVdlYktpdC81MzcuMzYgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvMzEuMC4xNjUwLjYzIFNhZmFyaS81MzcuMzYLAFAAAAAkOTg3M2JiYWItN2RjYi00ZWMwLTkxOTItNTRhNDk3Nzg1NmQ1AA==\nCgABAAAAAFLBdCEMAAoIAAEAAAABCAACAAAAAQsAAwAAAC5hbnRob2V1PWFuYWFlYW50aGFvZXVhbnRob3JjLmh1bnRhaGV1dGFvaGFudGhpAAsAFAAAAB5zbm93cGxvdy1zY2FsYS1jb2xsZWN0b3ItMC4wLjELAB4AAAAFVVRGLTgLACgAAAAJMTI3LjAuMC4xCwApAAAACTEyNy4wLjAuMQsAMgAAAGhNb3ppbGxhLzUuMCAoWDExOyBMaW51eCB4ODZfNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8zMS4wLjE2NTAuNjMgU2FmYXJpLzUzNy4zNgsAUAAAACRkZTU0OTdkZS1iZmUwLTQzZDYtYjA0My04NTJkZWE5MTA1ZjUA\nIs there anything else you want on this?\n. Oops, sorry for the spam. I thought I had already set this and it wasn't working.\n. Hi @alexanderdean, I don't know anything about publishing to maven, btw.\nShould I follow https://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/project/BuildSettings.scala and strip all libraries and META-INF out of the jar produced by sbt-assembly?\n. Ah, okay. Let me know if there's anything else on this. I don't really like putting the thrift schema in src/main/thrift rather than the root, but I still can't figure out how to set the sbt-thrift option to change the default directory...\n. :)\nfind . -name '*.scala' -exec sed -i 's/com\\.snowplowanalytics\\.collectors/com\\.snowplowanalytics\\.snowplow\\.collectors/g' {} \\;\n. java -jar target/scala-2.10/snowplow-scala-collector-0.0.1.jar --config my.conf will load my.conf into the Scala Collector and will also be passed to the ActorSystem so configuration options will also be correctly reflected in 'spray'.\nLet me know if there's anything else on this!\n. Hm, passing the config to the ActorSystem to reflect in Spray doesn't make much sense, but I've verified changing spray.can.server.remote-address-header (the only Spray configuration we're using) in the configuration passed on the command line changes as expected.\n. Maybe spray does read from the ActorSystem?\nhttps://github.com/spray/spray/blob/master/spray-client/src/test/scala/spray/client/RedirectionIntegrationSpec.scala shows one of their tests passing a configuration with spray.can values into the ActorSystem.\n. Hi Alex,\nI haven't used ScalaCheck before, but the one in the enrichment process looks cool!\nFor this, should I go through each element in each struct and send nothing (for optional elements), null, empty, and normal values and make sure the Thrift objects hold them?\n. Hi @alexanderdean, just taking a stab here. Is this what you wanted? Also, if so, can you send a reference for checking Java enums? I couldn't find anythinhg Googling scalacheck enum.\n. Anything else on this?\n. Sure, changed. Can you publish?\n. Hi @alexanderdean, can you make a pass?\nhttps://github.com/snowplow/snowplow/tree/feature/scala-rt-coll/2-collectors/thrift-raw-event\n. Ah, I accidentally mistook userId as being the collectorUserId you describe here:\nhttps://github.com/snowplow/snowplow/blob/feature/scala-rt-coll/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/ResponseHandler.scala#L74\n. Hi @alexanderdean, can you publish?  I've added and bumped the version number.\n. Oops, one was on the Scala collector page I made.\nNot sure about these 2:\n- https://github.com/snowplow/snowplow/wiki/Technical-architecture\n  - https://github.com/snowplow/snowplow/wiki/Features-and-benefits\n  - https://github.com/snowplow/snowplow/wiki/Technical-FAQ\n. Wonder if there's a better way to avoid all of these nests.\n. From Alex on hipchat:\n1. we port the consumer back into a new branch in snowplow/snowplow\n2. we replace the sample thrift with the snowplow thrift\n3. Use https://github.com/snowplow/snowplow/tree/feature/enrich-improv/3-enrich/scala-common-enrich to enrich\n4. Output to new Kinesis stream\n. Also, use the common enrich from maven and 3-enrich/scala-kinesis-enrich\n. Also, update common enrich to support Thrift input\n. Hi @alexanderdean - a CanonicalOutput object needs to be serialized to a byte array to be stored in a Kinesis stream. Instead of serializing/deserializing the existing CanonicalOutput, would creating a Thrift object be better?\n. Hi @alexanderdean  - really cool seeing this through!\n1. I started the Scala collector streaming to SnowplowRaw\n2. I started the Kinesis enricher reading from SnowplowRaw and streaming to SnowplowEnriched\n3. I started the kinesis consumer reading from SnowplowEnriched\nMaking a request to 127.0.0.1:8080/i?foo=bar, the event is able to propagate all the way to the Kinesis consumer to show the enriched events!\nSequence number: 49535611099413857891988072088731267192743433407515590657\ndata: app_id    null    platform        null    collector_tstamp        2014-01-11 02:52:13.004     dvce_tstamp     null    event   null    event_vendor    com.snowplowanalytics       event_id        37ac7f58-e510-426d-8f06-fac62604509b    txn_id  nullv_tracker       null    v_collector     ssc-0.0.1-kinesis       v_etl   kinesis-0.0.1-common-0.2.0-SNAPSHOT user_id null    user_ipaddress  127.0.0.x       user_fingerprint    null    domain_userid   null    domain_sessionidx       null    network_userid      null    geo_country     null    geo_region      null    geo_city        nullgeo_zipcode     null    geo_latitude    null    geo_longitude   null    page_url   null     page_title      null    page_referrer   null    page_urlscheme  null    page_urlhost        null    page_urlport    null    page_urlpath    null    page_urlquery       null    page_urlfragment        null    refr_urlscheme  null    refr_urlhostnull    refr_urlport    null    refr_urlpath    null    refr_urlquery   null    refr_urlfragment    null    refr_medium     null    refr_source     null    refr_term  null     mkt_medium      null    mkt_source      null    mkt_term        null    mkt_content null    mkt_campaign    null    se_category     null    se_action       nullse_label        null    se_property     null    se_value        null    tr_orderid null     tr_affiliation  null    tr_total        null    tr_tax  null    tr_shippingnull     tr_city null    tr_state        null    tr_country      null    ti_orderid null     ti_sku  null    ti_name null    ti_category     null    ti_price        nullti_quantity     null    pp_xoffset_min  null    pp_xoffset_max  null    pp_yoffset_min      null    pp_yoffset_max  null    useragent       Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36    br_name     Chrome 31       br_family       Chrome  br_version      31.0.1650.63    br_type     Browser br_renderengine WEBKIT  br_lang null    br_features_pdf null    br_features_flash   null    br_features_java        null    br_features_director    nullbr_features_quicktime   null    br_features_realplayer  null    br_features_windowsmedia    null    br_features_gears       null    br_features_silverlight null    br_cookies  null    br_colordepth   null    br_viewwidth    null    br_viewheight   nullos_name Linux   os_family       Linux   os_manufacturer Other   os_timezone     nulldvce_type       Computer        dvce_ismobile   0       dvce_screenwidth        nulldvce_screenheight       null    doc_charset     null    doc_width       null    doc_height  null\nPartition key: 127.0.0.x\nI'll follow up on this with https://github.com/snowplow/snowplow/issues/488 and https://github.com/snowplow/snowplow/issues/489, and let me know if there's anything else I can do to help pass off the projects from this winter, or if there's anything else I can work on as we go into next week.\n. Also #490\n. Hi @alexanderdean - should we rename one of com.snowplowanalytics.snowplow.collectors.thrift.TrackerPayload and com.snowplowanalytics.snowplow.enrich.common.inputs.TrackerPayload?\n. Okay, I'll do that\n. Hm, with renaming, I get importedTrackerPayload' is permanently hidden by definition of object TrackerPayload in package inputs, so I'm doing thrift.TrackerPayload => ThriftTrackerPayload.\n. Hi @alexanderdean - just taking a shot here. Can you make a pass when you get a chance?\n. Awesome, thanks @alexanderdean!\n. ~/scala-stream-collector [feature/scala-rt-coll] \u00bb ack -i backend\n~/scala-stream-collector [feature/scala-rt-coll] \u00bb find . -name '*backend*'\n~/scala-stream-collector [feature/scala-rt-coll] \u00bb find . -name '*Backend*'\n. Hi @alexanderdean, bumped Thrift version to 0.4.0 and made the ip address the key in the collector. Can you publish?\n. Before (List of Strings):\n[Remote-Address: 127.0.0.1, Raw-Request-URI: /i, Cookie: org.cups.sid=b3ec9bccd2056c0b419a0362f59872ec, Accept-Language: en-US, en, et, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Connection: keep-alive, Host: 127.0.0.1:8080]\nAfter:\n[Cookie: org.cups.sid=b3ec9bccd2056c0b419a0362f59872ec, Accept-Language: en-US, en, et, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Cache-Control: max-age=0, Connection: keep-alive, Host: 127.0.0.1:8080]\nI've checked these against the headers my browser is sending and there isn't anything extra.\nAlso, not sure why the cookie looks like that. Should I further look into this @alexanderdean?\n. Looks like Content-Type has a charset portion that's optional. The trackers on Snowplow's website currently aren't sending them. encoding is currently required in the Thrift object, so I'm not sure on this. Also not sure on 1) or 2) you mentioned (sorry).\n. I use CUPS on 127.0.0.1 and it set this as a cookie:\nhttp://mybookworld.wikidot.com/forum/t-370552\nStill looking into why the sp cookie isn't being sent.\n. I'm pretty sure I'm sending the correct Set-Cookie header:\n\nI wonder if this is related to https://code.google.com/p/chromium/issues/detail?id=56211, but Firefox also isn't setting a cookie.\nAlso, running on another server, Set-Cookie isn't actually setting a cookie.\n\n@alexanderdean, do you see anything weird here? I'm not sure why my browser isn't sending a cookie back.\n. Same from running on port 80 on dijkstra.\n. Sorry, this isn't an issue. In the config, I set the expiration to 365d, assuming d was for days, but this was interpreted as 365 milliseconds. Changing to 365 days fixes the expiration date and the browser sends the cookie.\n\n. Sure, let me know if there's anything else for now. I'll close this once I'm using the new config.\n. Closing, let me know if there's anything else on the scala enrich config.\n. @alexanderdean  - Should the anon_octets field be anon_quartets to match the AnonQuartets class?\n. Scala\n  private def tabSeparateCanonicalOutput(output: CanonicalOutput): String = {\n    output.getClass.getDeclaredFields.map{ field =>\n      field.setAccessible(true)\n      field.getName + \"\\t\" + field.get(output)\n    }.mkString(\"\\t\")\n  }\n. Okay, changed. Is there anything else on this?\n. https://github.com/snowplow/snowplow/wiki/_compare/b1b2f388f4304477057aca1c6dabb80e50430c11...f7716efd5ff5353a54b08259784be8aef0c86d4a\n. https://github.com/snowplow/snowplow/wiki/_compare/f7716efd5ff5353a54b08259784be8aef0c86d4a...33ce3a602274cd631537a1395625c0082fbe6cd4\n. https://github.com/snowplow/snowplow/wiki/_compare/33ce3a602274cd631537a1395625c0082fbe6cd4...02d114621d22b2766e2f00408438752c0e7433da\n. Hi @alexanderdean - I've slightly restructured the Wiki to add what I've seen from Scala enrichment, but reassigning to you to make a pass and add more.\n. Also, not sure if this belongs here, but there are still a lot of todo/to write comments in the wiki. \n```\nrepos/snowplow.wiki [master] \u00bb ag -i 'todo'\nPython-Tracker.md\n454:Tracking a Null value for a given field is currently untested in the Python Tra\nrepos/snowplow.wiki [master] \u00bb ag -i 'to write'\nabout-snowplow/Developer-FAQ.md\n83:To write.\ntechnical-documentation/1-trackers/javascript-tracker/_Sidebar.md\n40:D. Snowplow storage formats (to write)\ntechnical-documentation/Snowplow-technical-documentation.md\n44:### D. Snowplow storage formats (to write)\ntechnical-documentation/2-collectors/Collector-logging-formats.md\n23:TO WRITE\n28:TO WRITE\n33:TO WRITE\ntechnical-documentation/2-collectors/SnowCannon.md\n3:TO WRITE\ntechnical-documentation/2-collectors/Cloudfront-collector.md\n3:TO WRITE\ntechnical-documentation/_Sidebar.md\n36:D. Snowplow storage formats (to write)\ntechnical-documentation/3-enrich/Instrumentation.md\n3:TO WRITE\ntechnical-documentation/3-enrich/The-Enrichment-Process.md\n3:TO WRITE\nsetup-guide/common/iam-setup/Setup-IAM-permissions-for-operating-Snowplow.md\n246:Note that we only need to be able to write to these buckets. Finally, we can add the permissions for the :archive: location:\n273:Note that the :in: location for the StorageLoader is the :out: location for EmrEtlRunner - we have already set the relevant permissions on this location. Therefore, we only need to add permission to write to the :archive: location identified above (i.e. s3://snowplow-test-archive-eu-west-1/hadoop-redshift/events in our example):\nsetup-guide/4-setup-alternative-data-stores/redshift/Setting-up-Redshift.md\n53:TO WRITE\n127:TO WRITE\nsetup-guide/5-analyse-your-data/Setting-up-R-to-perform-more-sophisticated-analysis-on-your-Snowplow-data.md\n167:TO WRITE\nsetup-guide/5-analyse-your-data/Getting-started-with-Looker.md\n174:TO WRITE\nsetup-guide/5-analyse-your-data/hive/Running-Hive-using-the-command-line-tools.md\n50:TO WRITE: Add instructions to launch a session from the PC command-line, incl. setting up Putty and Pageant to SSH successfully\n55:TO WRITE: Add instructions on creating jobs via the Amazon web UI. \n112:TO WRITE\n324:TO WRITE\nsetup-guide/5-analyse-your-data/Setting-up-Excel-to-analyze-Snowplow-data.md\n186:TO WRITE\n334:TO WRITE\n```\n. I'm having too much fun reading/annotating my code...\nhttp://www.docdroid.net/882m/listings-annotated.pdf.html\nhttps://github.com/bamos/latex-templates/tree/master/listings\n. Closing - there aren't any extraneous 'TODO's in the code, and everything I've worked on is compiling and tests are passing. @alexanderdean - let me know if there's anything else!\nBash\nSnowplowDirs=( kinesis-example-scala-consumer \\\n  kinesis-example-scala-producer \\\n  scala-kinesis-enrich \\\n  scala-stream-collector )\nfor Dir in \"${SnowplowDirs[@]}\"; do\n  echo $Dir\n  pushd $Dir\n  echo \"sbt test\"\n  sbt clean test\n  echo -e 'n\\nag -i todo'\n  ag -i todo src\n  popd\n  echo -e \"\\n\\n\"\ndone &> /tmp/out\ncat /tmp/out\n```\nkinesis-example-scala-consumer\n~/kinesis-example-scala-consumer ~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nsbt test\nLoading /usr/share/java/sbt/bin/sbt-launch-lib.bash\n[info] Loading project definition from /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/project\n[info] Set current project to kinesis-example-scala-consumer (in build file:/home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/)\n[success] Total time: 0 s, completed Jan 16, 2014 10:41:21 PM\n[info] Updating {file:/home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/}kinesis-example-scala-consumer...\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n[info] Compiling schema with command: thrift -gen java:hashcode -o /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/target/scala-2.10/src_managed/main /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/src/main/thrift/thrift_data.thrift\n[info] Compiling 4 Scala sources and 1 Java source to /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/target/scala-2.10/classes...\n[warn] Note: /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-consumer/target/scala-2.10/src_managed/main/gen-java/com/snowplowanalytics/kinesis/consumer/generated/StreamData.java uses unchecked or unsafe operations.\n[warn] Note: Recompile with -Xlint:unchecked for details.\n[info] Passed: Total 0, Failed 0, Errors 0, Passed 0\n[info] No tests to run for test:test\n[success] Total time: 9 s, completed Jan 16, 2014 10:41:30 PM\nn\nag -i todo\n~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nkinesis-example-scala-producer\n~/kinesis-example-scala-producer ~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nsbt test\nLoading /usr/share/java/sbt/bin/sbt-launch-lib.bash\n[info] Loading project definition from /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/project\n[info] Set current project to kinesis-example-scala-producer (in build file:/home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/)\n[success] Total time: 0 s, completed Jan 16, 2014 10:41:34 PM\n[info] Updating {file:/home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/}kinesis-example-scala-producer...\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n[info] Compiling schema with command: thrift -gen java:hashcode -o /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/target/scala-2.10/src_managed/main /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/src/main/thrift/thrift_data.thrift\n[info] Compiling 3 Scala sources and 1 Java source to /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/target/scala-2.10/classes...\n[warn] Note: /home/brandon/docs/work/2013-2014/snowplow/repos/kinesis-example-scala-producer/target/scala-2.10/src_managed/main/gen-java/com/snowplowanalytics/kinesis/producer/generated/StreamData.java uses unchecked or unsafe operations.\n[warn] Note: Recompile with -Xlint:unchecked for details.\n[info] Passed: Total 0, Failed 0, Errors 0, Passed 0\n[info] No tests to run for test:test\n[success] Total time: 11 s, completed Jan 16, 2014 10:41:45 PM\nn\nag -i todo\nsrc/main/scala/com/snowplowanalytics/kinesis/producer/StreamProducer.scala:163:      case (true,  None)    => throw new RuntimeException(\"Ordered stream support not yet implemented\") // TODO\nsrc/main/scala/com/snowplowanalytics/kinesis/producer/StreamProducer.scala:165:      case (true,  Some(c)) => throw new RuntimeException(\"Ordered stream support not yet implemented\") // TODO\n~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nscala-kinesis-enrich\n~/scala-kinesis-enrich ~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nsbt test\nLoading /usr/share/java/sbt/bin/sbt-launch-lib.bash\n[info] Loading project definition from /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/project\n[info] Set current project to snowplow-kinesis-enrich (in build file:/home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/)\n[success] Total time: 0 s, completed Jan 16, 2014 10:41:50 PM\n[info] Updating {file:/home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/}snowplow-kinesis-enrich...\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n[info] Compiling 9 Scala sources to /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/target/scala-2.10/classes...\n[info] Compiling 1 Scala source to /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/target/scala-2.10/test-classes...\n[info] KinesisEnrichSpec\n[info] \n[info] Snowplow's Kinesis enricher should\n[info] + enrich a valid SnowplowRawEvent.\n[info] \n[info] Total for specification KinesisEnrichSpec\n[info] Finished in 20 ms\n[info] 1 example, 0 failure, 0 error\n[info] Passed: Total 1, Failed 0, Errors 0, Passed 1\n[success] Total time: 23 s, completed Jan 16, 2014 10:42:12 PM\nn\nag -i todo\nsrc/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala:101:          // TODO: Store bad event if canonical output not validated.\nsrc/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala:106:      // TODO: Store bad event if canonical input not validated.\n~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nscala-stream-collector\n~/scala-stream-collector ~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\nsbt test\nLoading /usr/share/java/sbt/bin/sbt-launch-lib.bash\n[info] Loading project definition from /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow-collector/2-collectors/scala-stream-collector/project\n[info] Set current project to snowplow-stream-collector (in build file:/home/brandon/docs/work/2013-2014/snowplow/repos/snowplow-collector/2-collectors/scala-stream-collector/)\n[success] Total time: 0 s, completed Jan 16, 2014 10:42:16 PM\n[info] Updating {file:/home/brandon/docs/work/2013-2014/snowplow/repos/snowplow-collector/2-collectors/scala-stream-collector/}snowplow-stream-collector...\n[info] Resolving org.fusesource.jansi#jansi;1.4 ...\n[info] Done updating.\n[info] Compiling 8 Scala sources to /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow-collector/2-collectors/scala-stream-collector/target/scala-2.10/classes...\n[info] Compiling 1 Scala source to /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow-collector/2-collectors/scala-stream-collector/target/scala-2.10/test-classes...\n22:42:39.296 [com-snowplowanalytics-snowplow-collectors-scalastream-CollectorServiceSpec-akka.actor.default-dispatcher-2] INFO  akka.event.slf4j.Slf4jLogger - Slf4jLogger started\n22:42:39.306 [com-snowplowanalytics-snowplow-collectors-scalastream-CollectorServiceSpec-akka.actor.default-dispatcher-2] DEBUG akka.event.EventStream - logger log1-Slf4jLogger started\n22:42:39.306 [com-snowplowanalytics-snowplow-collectors-scalastream-CollectorServiceSpec-akka.actor.default-dispatcher-2] DEBUG akka.event.EventStream - Default Loggers started\n22:42:40.079 [com-snowplowanalytics-snowplow-collectors-scalastream-CollectorServiceSpec-akka.actor.default-dispatcher-3] DEBUG akka.event.EventStream - shutting down: StandardOutLogger started\n[info] CollectorServiceSpec\n[info] \n[info] Snowplow's Scala collector should\n[info] + return an invisible pixel.\n[info] + return a cookie expiring at the correct time.\n[info] + return the same cookie as passed in.\n[info] + return a P3P header.\n[info] + store the expected event as a serialized Thrift object in the enabled sink\n[info] \n[info] \n[info] Total for specification CollectorServiceSpec\n[info] Finished in 333 ms\n[info] 5 examples, 0 failure, 0 error\n[info] Passed: Total 5, Failed 0, Errors 0, Passed 5\n[success] Total time: 24 s, completed Jan 16, 2014 10:42:40 PM\nn\nag -i todo\n~ ~/scala-stream-collector/collector-listing ~/scala-stream-collector/src ~/scala-stream-collector/src/main ~/scala-stream-collector/src/main/scala ~/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream\n``\n. Sure, sounds good!\n. Hi @alexanderdean - anything else on testing this?\n. Hi @alexanderdean - assigned to you, but I'm happy to help pass off.\n. Hi @alexanderdean  - I can look into this. Assign to me if you're done.\n. Usingsbt console(after addingslf4j` to dependencies -- won't commit)\nScala\nimport com.snowplowanalytics.snowplow.collectors.thrift._; import org.joda.time.DateTime; import org.apache.thrift.TSerializer; import com.snowplowanalytics.snowplow.enrich.common.inputs.ThriftLoader;\nval s = new TSerializer()\nval dt = DateTime.parse(\"2013-08-29T00:18:48.000+00:00\")\nval thriftEvent = new SnowplowRawEvent(dt.getMillis, new TrackerPayload(PayloadProtocol.Http, PayloadFormat.HttpGet, \"foo=bar\"), \"test-collector\", \"UTF-8\", \"127.0.0.1\")\n// thriftEvent: com.snowplowanalytics.snowplow.collectors.thrift.SnowplowRawEvent = SnowplowRawEvent(timestamp:1377735528000, payload:TrackerPayload(protocol:Http, format:HttpGet, data:foo=bar), collector:test-collector, encoding:UTF-8, ipAddress:127.0.0.1)\nval thriftSerialized = s.serialize(thriftEvent)\nval thriftCanonical = ThriftLoader.toCanonicalInput(new String(thriftSerialized.map(_.toChar)))\n// thriftCanonical: com.snowplowanalytics.snowplow.enrich.common.ValidatedMaybeCanonicalInput = Success(Some(CanonicalInput(2013-08-28T20:18:48.000-04:00,NvGetPayload(NonEmptyList(foo=bar)),InputSource(test-collector,None),UTF-8,Some(127.0.0.1),None,None,List(),None)))\nThe millisecond value of  the DateTime object is 1377735528000, which translates to Wed Aug 28 2013 20:18:48 GMT-0400 (EDT) .\nFor consistency, should we change\nScala\n            new DateTime(snowplowRawEvent.timestamp),\non line 83 of ThriftLoader.scala to\nScala\n            new DateTime(snowplowRawEvent.timestamp, DateTimeZone.UTC),\nto yield the time in UTC?\n``` Scala\nscala> new DateTime(1377735528000L)\nres0: org.joda.time.DateTime = 2013-08-28T20:18:48.000-04:00\nscala> new DateTime(1377735528000L, DateTimeZone.UTC)\nres2: org.joda.time.DateTime = 2013-08-29T00:18:48.000Z\n``\n. Okay!\n. Also, do you expect theThriftLoader` tests to fail?\n[error] x toCanonicalInput should return a CanonicalInput for a valid Thrift SnowplowRawEvent\n[error]   | SPEC NAME | RAW                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | EXP. TIMESTAMP                | EXP. PAYLOAD                                          | EXP. IP ADDRESS | EXP. USER AGENT                                                                                                | EXP. REFERER URI | EXP. HEADERS                                                                                                                                                                                                                                                                                                                                                                            | EXP. USER ID                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n[error] x | Thrift #1 | CgABAAABQ3KVZkgMAAoIAAEAAAABCAACAAAAAQsAAwAAABh0ZXN0UGFyYW09MyZ0ZXN0UGFyYW0yPTQACwAUAAAAEHNzYy0wLjAuMS1zdGRvdXQLAB4AAAAFVVRGLTgLACgAAAAJMTI3LjAuMC4xCwApAAAACTEyNy4wLjAuMQsAMgAAAGhNb3ppbGxhLzUuMCAoWDExOyBMaW51eCB4ODZfNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8zMS4wLjE2NTAuNjMgU2FmYXJpLzUzNy4zNg8ARgsAAAAHAAAAL0Nvb2tpZTogc3A9YzVmM2EwOWYtNzVmOC00MzA5LWJlYzUtZmVhNTYwZjc4NDU1AAAAHkFjY2VwdC1MYW5ndWFnZTogZW4tVVMsIGVuLCBldAAAACRBY2NlcHQtRW5jb2Rpbmc6IGd6aXAsIGRlZmxhdGUsIHNkY2gAAAB0VXNlci1BZ2VudDogTW96aWxsYS81LjAgKFgxMTsgTGludXggeDg2XzY0KSBBcHBsZVdlYktpdC81MzcuMzYgKEtIVE1MLCBsaWtlIEdlY2tvKSBDaHJvbWUvMzEuMC4xNjUwLjYzIFNhZmFyaS81MzcuMzYAAABWQWNjZXB0OiB0ZXh0L2h0bWwsIGFwcGxpY2F0aW9uL3hodG1sK3htbCwgYXBwbGljYXRpb24veG1sO3E9MC45LCBpbWFnZS93ZWJwLCAqLyo7cT0wLjgAAAAWQ29ubmVjdGlvbjoga2VlcC1hbGl2ZQAAABRIb3N0OiAxMjcuMC4wLjE6ODA4MAsAUAAAACRjNWYzYTA5Zi03NWY4LTQzMDktYmVjNS1mZWE1NjBmNzg0NTUA | 2014-01-08T11:00:30.280-05:00 | NvGetPayload(NonEmptyList(testParam=3, testParam2=4)) | Some(127.0.0.1) | Some(Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36) | None             | List(Cookie: sp=c5f3a09f-75f8-4309-bec5-fea560f78455, Accept-Language: en-US, en, et, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Connection: keep-alive, Host: 127.0.0.1:8080) | Some(c5f3a09f-75f8-4309-bec5-fea560f78455) | 'Failure(NonEmptyList(Record does not match Thrift SnowplowRawEvent schema))' is not Success with value'Some(CanonicalInput(2014-01-08T11:00:30.280-05:00,NvGetPayload(NonEmptyList(testParam=3, testParam2=4)),InputSource(ssc-0.0.1-stdout,Some(127.0.0.1)),UTF-8,Some(127.0.0.1),Some(Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36),None,List(Cookie: sp=c5f3a09f-75f8-4309-bec5-fea560f78455, Accept-Language: en-US, en, et, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Connection: keep-alive, Host: 127.0.0.1:8080),Some(c5f3a09f-75f8-4309-bec5-fea560f78455)))' (ThriftLoaderSpec.scala:94)\nEdit: copy/paste left out some characters.\n. Ah, didn't notice the dependency was updated\n. Sure, I can make changes for the collector and enrichment process to handle:\nC\nstruct SnowplowRawEvent {\n  01: i64 timestamp // Milliseconds since epoch.\n  20: string collector // Collector name/version.\n  30: string encoding\n  40: string ipAddress\n  41: optional string hostname\n  50: optional string userAgent\n  60: optional string refererUri\n  70: optional list<string> headers\n  80: optional string networkUserId\n  90: optional TrackerPayload payload\n}\nHowever, does CanonicalInput in the enrichment process expect the payload to be non-null? I've gotten validation errors from canonicalInput, line 119 with an empty querystring.\ncase Nil => \"No name-value pairs extractable from querystring [%s] with encoding [%s]\".format(qs, encoding).fail\nShould this portion of the enrichment process also be changed to validate null data?\n. @alexanderdean  - can you publish the updated thrift schema? I'll update the collector after.\n. Sorry! Fixing now\n. Sure\n. How's this?\nC\nstruct SnowplowRawEvent {\n  01: i64 timestamp // Milliseconds since epoch.\n  20: string collector // Collector name/version.\n  30: string encoding\n  40: string ipAddress\n  41: optional TrackerPayload payload\n  45: optional string hostname\n  50: optional string userAgent\n  60: optional string refererUri\n  70: optional list<string> headers\n  80: optional string networkUserId\n}\n. Changed from 40->41 after posting, and changed the version.\n. Okay, try now\n[info] + SnowplowRawEvent.timestamp: OK, passed 100 tests.\n[info] + SnowplowRawEvent.protocolVal: OK, passed 100 tests.\n[info] + SnowplowRawEvent.protocolFormat: OK, passed 100 tests.\n[info] + SnowplowRawEvent.payloadData: OK, passed 100 tests.\n[info] + SnowplowRawEvent.collector: OK, passed 100 tests.\n[info] + SnowplowRawEvent.encoding: OK, passed 100 tests.\n[info] + SnowplowRawEvent.ip: OK, passed 100 tests.\n[info] + SnowplowRawEvent.hostname: OK, passed 100 tests.\n[info] + SnowplowRawEvent.userAgent: OK, passed 100 tests.\n[info] + SnowplowRawEvent.refererUri: OK, passed 100 tests.\n[info] + SnowplowRawEvent.networkUserId: OK, passed 100 tests.\n[info] + SnowplowRawEvent.headers: OK, passed 100 tests.\n[info] Passed: Total 12, Failed 0, Errors 0, Passed 12\n. Can we change this to serializing a Thrift object rather than decoding a Base64 String?\n. Okay - thanks!\n. I like this idea @alexanderdean! Adding configurable inputs to the current code will be easy and just add a small layer abstraction. I'll start on this shortly, after Kinesis testing.\nAlso, I'll add enrich.source and enrich.sink make the config look like:\n```\nenrich {\n  # Sources currently supported are:\n  # 'kinesis' for reading Thrift-serialized records from a Kinesis stream\n  # 'stdin' for writing Base64-encoded Thrift-serialized records to stdout\n  source = \"kinesis\"\n# Sinks currently supported are:\n  # 'kinesis' for writing Thrift-serialized records to a Kinesis stream\n  # 'stdout' for writing Base64-encoded Thrift-serialized enriched\n  #    events to stdout.\n  #    Using sbt assembly and java -jar is recommended to disable sbt\n  #    logging.\n  sink= \"kinesis\"\naws {\n    access-key: \"cpf\"\n    secret-key: \"cpf\"\n  }\n  streams {\n    in: {\n      raw: \"SnowplowRaw\"\n    }\n    out: {\n      enriched: \"SnowplowEnriched\"\n      enriched_shards: 1 # Number of shards to use if created.\n      bad: \"SnowplowBad\" # Not used until #463\n      bad_shards: 1 # Number of shards to use if created.\n    }\n# `app-name` is used for a DynamoDB table to maintain stream state.\napp-name: SnowplowKinesisEnrich-${enrich.streams.in.raw}\n\n# LATEST: most recent data.\n# TRIM_HORIZON: oldest available data.\n# Note: This only effects the first run of this application\n# on a stream.\ninitial-position = \"TRIM_HORIZON\"\n\nendpoint: \"https://kinesis.us-east-1.amazonaws.com\"\n\n}\n  enrichments {\n    geo_ip: {\n      enabled: true # false not yet suported\n      maxmind_file: \"/tmp/GeoLiteCity.dat\"\n    }\n    anon_ip: {\n      enabled: true\n      anon_octets: 1 # Or 2, 3 or 4. 0 is same as enabled: false\n    }\n  }\n}\n``\n. Also @alexanderdean, forstdout` in the enrichment process, how should invalid events be handled?\n. Sure, sounds clean!\n. Hi @alexanderdean  - excited for this!\nUsing stdin and stdouterr, copying and pasting in a string works:\n~/scala-kinesis-enrich [feature/kinesis-enrich] \u00bb sbt \"run --config my.conf\"\nLoading /usr/share/java/sbt/bin/sbt-launch-lib.bash\n[info] Loading project definition from /home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/project\n[info] Set current project to snowplow-kinesis-enrich (in build file:/home/brandon/docs/work/2013-2014/snowplow/repos/snowplow/3-enrich/scala-kinesis-enrich/)\n[info] Running com.snowplowanalytics.snowplow.enrich.kinesis.KinesisEnrichApp --config my.conf\nCgABAAABQ5iGqAYLABQAAAAQc3NjLTAuMC4xLVN0ZG91dAsAHgAAAAVVVEYtOAsAKAAAAAkxMjcuMC4wLjEMACkIAAEAAAABCAACAAAAAQsAAwAAABh0ZXN0UGFyYW09MyZ0ZXN0UGFyYW0yPTQACwAtAAAACTEyNy4wLjAuMQsAMgAAAGhNb3ppbGxhLzUuMCAoWDExOyBMaW51eCB4ODZfNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8zMS4wLjE2NTAuNjMgU2FmYXJpLzUzNy4zNg8ARgsAAAAIAAAAL0Nvb2tpZTogc3A9YzVmM2EwOWYtNzVmOC00MzA5LWJlYzUtZmVhNTYwZjc4NDU1AAAAGkFjY2VwdC1MYW5ndWFnZTogZW4tVVMsIGVuAAAAJEFjY2VwdC1FbmNvZGluZzogZ3ppcCwgZGVmbGF0ZSwgc2RjaAAAAHRVc2VyLUFnZW50OiBNb3ppbGxhLzUuMCAoWDExOyBMaW51eCB4ODZfNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8zMS4wLjE2NTAuNjMgU2FmYXJpLzUzNy4zNgAAAFZBY2NlcHQ6IHRleHQvaHRtbCwgYXBwbGljYXRpb24veGh0bWwreG1sLCBhcHBsaWNhdGlvbi94bWw7cT0wLjksIGltYWdlL3dlYnAsICovKjtxPTAuOAAAABhDYWNoZS1Db250cm9sOiBtYXgtYWdlPTAAAAAWQ29ubmVjdGlvbjoga2VlcC1hbGl2ZQAAABRIb3N0OiAxMjcuMC4wLjE6ODA4MAsAUAAAACRjNWYzYTA5Zi03NWY4LTQzMDktYmVjNS1mZWE1NjBmNzg0NTUA\n        2014-01-16 00:49:58.278         com.snowplowanalytics   ef69eb53-59f7-41d2-beb4-3addf2bc44a3            ssc-0.0.1-Stdout    kinesis-0.0.1-common-0.2.0-SNAPSHOT     127.0.0.x               Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36    Chrome 31   Chrome  31.0.1650.63    Browser WEBKIT      Linux   Linux   Other       Computer    0\nEven cooler, piping directly from the collector works!\n(Note: I have these dirs symlinked to different branches.)\n~ \u00bb java -jar scala-stream-collector/target/scala-2.10/snowplow-stream-collector-0.0.1.jar --config scala-stream-collector/src/main/resources/application.conf | java -jar scala-kinesis-enrich/target/scala-2.10/snowplow-kinesis-enrich-0.0.1.jar --config scala-kinesis-enrich/my.conf\n        2014-01-16 11:39:04.884         com.snowplowanalytics   d31e6e84-0fcd-4544-9761-34039a4f6bde            ssc-0.0.1-Stdout    kinesis-0.0.1-common-0.2.0-SNAPSHOT     127.0.0.x               Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36    Chrome  Chrome  32.0.1700.77    Browser WEBKIT          Linux   Linux   Other       Computer    0\nIs there anything else on this issue? Or any other functionality/testing?\nThe only issues I have left (assuming #490 is good) are #470, #488, and #489.\n. Good catch - thanks!\n. Something weird's going on with the Thrift tests. Sometimes this one case fails as above, but other times, it succeeds but doesn't match the test object:\n[error] x toCanonicalInput should return a CanonicalInput for a valid Thrift SnowplowRawEvent\n[error]   | SPEC NAME | RAW                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | EXP. TIMESTAMP                | EXP. PAYLOAD                                          | EXP. IP ADDRESS | EXP. USER AGENT                                                                                                | EXP. REFERER URI | EXP. HEADERS                                                                                                                                                                                                                                                                                                                                                                            | EXP. USER ID                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n[error] x | Thrift #1 | CgABAAABQ5iGqAYLABQAAAAQc3NjLTAuMC4xLVN0ZG91dAsAHgAAAAVVVEYtOAsAKAAAAAkxMjcuMC4wLjEMACkIAAEAAAABCAACAAAAAQsAAwAAABh0ZXN0UGFyYW09MyZ0ZXN0UGFyYW0yPTQACwAtAAAACTEyNy4wLjAuMQsAMgAAAGhNb3ppbGxhLzUuMCAoWDExOyBMaW51eCB4ODZfNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8zMS4wLjE2NTAuNjMgU2FmYXJpLzUzNy4zNg8ARgsAAAAIAAAAL0Nvb2tpZTogc3A9YzVmM2EwOWYtNzVmOC00MzA5LWJlYzUtZmVhNTYwZjc4NDU1AAAAGkFjY2VwdC1MYW5ndWFnZTogZW4tVVMsIGVuAAAAJEFjY2VwdC1FbmNvZGluZzogZ3ppcCwgZGVmbGF0ZSwgc2RjaAAAAHRVc2VyLUFnZW50OiBNb3ppbGxhLzUuMCAoWDExOyBMaW51eCB4ODZfNjQpIEFwcGxlV2ViS2l0LzUzNy4zNiAoS0hUTUwsIGxpa2UgR2Vja28pIENocm9tZS8zMS4wLjE2NTAuNjMgU2FmYXJpLzUzNy4zNgAAAFZBY2NlcHQ6IHRleHQvaHRtbCwgYXBwbGljYXRpb24veGh0bWwreG1sLCBhcHBsaWNhdGlvbi94bWw7cT0wLjksIGltYWdlL3dlYnAsICovKjtxPTAuOAAAABhDYWNoZS1Db250cm9sOiBtYXgtYWdlPTAAAAAWQ29ubmVjdGlvbjoga2VlcC1hbGl2ZQAAABRIb3N0OiAxMjcuMC4wLjE6ODA4MAsAUAAAACRjNWYzYTA5Zi03NWY4LTQzMDktYmVjNS1mZWE1NjBmNzg0NTUA | 2014-01-08T11:00:30.280-05:00 | NvGetPayload(NonEmptyList(testParam=3, testParam2=4)) | Some(127.0.0.1) | Some(Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36) | None             | List(Cookie: sp=c5f3a09f-75f8-4309-bec5-fea560f78455, Accept-Language: en-US, en, et, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Connection: keep-alive, Host: 127.0.0.1:8080) | Some(c5f3a09f-75f8-4309-bec5-fea560f78455) | 'Success(Some(CanonicalInput(2014-01-16T00:49:58.278Z,NvGetPayload(NonEmptyList(testParam=3, testParam2=4)),InputSource(ssc-0.0.1-Stdout,Some(127.0.0.1)),UTF-8,Some(127.0.0.1),Some(Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36),None,List(Cookie: sp=c5f3a09f-75f8-4309-bec5-fea560f78455, Accept-Language: en-US, en, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Cache-Control: max-age=0, Connection: keep-alive, Host: 127.0.0.1:8080),Some(c5f3a09f-75f8-4309-bec5-fea560f78455))))' is not Success with value'Some(CanonicalInput(2014-01-08T11:00:30.280-05:00,NvGetPayload(NonEmptyList(testParam=3, testParam2=4)),InputSource(ssc-0.0.1-stdout,Some(127.0.0.1)),UTF-8,Some(127.0.0.1),Some(Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36),None,List(Cookie: sp=c5f3a09f-75f8-4309-bec5-fea560f78455, Accept-Language: en-US, en, et, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36, Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/webp, */*;q=0.8, Connection: keep-alive, Host: 127.0.0.1:8080),Some(c5f3a09f-75f8-4309-bec5-fea560f78455)))' (ThriftLoaderSpec.scala:94)\n. Not critical (or needed) for anything I'm doing, but I remember @alexanderdean talking about adding a control for this.\n. ",
    "dtheodor": "Thanks for the responses. The single sitecatalyst variable is configurable, I've been told that cmpid is the most common. The pipe delimited strings are our client's impementation, so these are pretty custom...\nI think the proposal of @alexanderdean is very workable, pushes this whole analytics configuration mess to snowplow configuration.\nI am also looking to modify the enrichment job myself. If I am correct, modifying this method https://github.com/snowplow/snowplow/blob/master/3-enrich/hadoop-etl/src/main/scala/enrichments/web/AttributionEnrichments.scala#L103-L125 would allow me to implement any custom business rules. Also the javascript parsing you refer to would replace hard-coded strings (like \"utm_source\") in the transformMap?\n. ",
    "christoph-buente": "Or maybe a background worker could fetch the bulk data hourly and all lookups would be local then. http://openweathermap.org/current#bulk\n. Oh, i just saw bulk downloading comes with a price of at least $470/month\n. I generally like the idea of getting rid of PII, because it might be a legal requirement. However, if it does not happen in the tracker (specifically the JS tracker), users could still see that PII is transferered to the collector. Using the clojure collector ensures the PII ends up in disk in the log files.\nSo i created https://github.com/snowplow/snowplow-javascript-tracker/issues/465 to allow to encrypt data before it is being send off. I think a key is not nessecarily needed to encrypt data like names or email addresses. MD5, SHA1, SHA256 are the industry standards and are widely used. Wouldn't it be sufficient if we could apply a scrubber/anonymizer as a callback function to the formTracking?\n. :+1: \n. Wouldn't it be more desirable to provide an sbt task that dockerizes the kinesis apps? That would make it much easier to deploy with AWS beanstalk or opsworks for example.\n. As the peeps from elastic.io highly recommend to use data-based index names, this would come in very handy. Kibana is made to handle with those types on indexes out of the box. And it also makes it much easier to delete old data, by just killing the full index.\n. @alexanderdean It is usually not a problem with several sink instances talking to a single index. It's actually how it is done today. But as the hour or day turns, a new index would need to be created automatically. Which leaves the old index read-only. And it would also be better performance when re-sharding happens. We have 86GB of data from 72h of event tracking. Re-sharding is really painfull and slow. A big \ud83d\udc4d  for this issue :)\n. What about getting the feature implemented first, learn from it, and then add stuff on top like auto-shard discovery and such things? The number of shards are in the config file already. But the immediate benefit from this feature is a much more performant and easily reshardable ES cluster. Resharding a 30GB or even couple of hundred GB in one single index is a real pain.\nMaybe ot could be done separately from the sink proces itself with a scheduled job, that creates a new index with current date/time in the name, and then switch the alias to point to the new index. So what the sink essentially has to do is: instead of creating an index, it should create an index and an alias. And it needs to write to the alias instead of the index directly.\n. Thx @jbeemster for the explanation. Makes total sense! I know about curator, but i wasnt aware it had out of the box index swapping capabilities.\n. Hey @jbeemster,\nwe implemented an alias and index rotation scheme using curator. But there is an issue with it, that I'm not sure how to resolve.\nWe use hourly indices due to our high write volume. And we have an alias pointing to the current hour's index. Now let's say the kinesis-es-sink falls behind, because of too much traffic. Then it would write last hours events into this hour's index. If you look in Kibana on the full timeline, you don't see a difference, but if i zoom in last hours time frame, i see no data, cause it actually lives in this hour's index.\nIs there a way around it?\n. That is a possible workaround. But coming back to the original cause of this ticket: the kinesis-es sink has the notion of the timestamp already. If it had the notion of the index pattern, it could decide event by event, in which index to writen. And what is more: on re-running raw logs from the past, that would work too. \nI'm not saying anymore, that we should handle the indexes from snowplow. But making it aware of an index pattern could be helpful still.. We are using an hourly index with 2 partitions each and a replication factor of 1. That means for every hour there are 4 shards (2x shard 0 and shard 1)\nThe naming pattern is something like: useful-name-YYYY.MM.DD.HH\nWe are using a bash script which runs every hour minute 0, which does the following:\n create the index for the current hour including mapping and so on\n add the alias name (from the kinesis-es.conf index name) to this index\n remove the alias from last hour's index\n delete indexes older than our retention threshold\n* create next hour's index, just to make sure it there, on the brink of next hour :)\n. Sure, done.\n. @alexanderdean How would i start collecting JSON instances in the first place in the simplest way? Is there a piece of software to take POST requests and dump the JSON body payload to an S3 bucket? Or is this supported by AWS out of the box? Can you point me to a starting point to read on this?\n. Ok, i set it up and registered several webhooks. Mainly add-to-cart, update-cart, create-customer, update-customer.\nNow i get logs. I think we are interested in those types of logs: _var_log_tomcat8_rotated_localhost_access_log.txt1449763262\nIs it wise to have a separete url for every web hook action like mentioned above? Like this http://<<endpoint>>/com.shopify/cart-create-v1 and http://<<endpoint>>/com.shopify/order-create-v1\nAnd then i strip out the column with base64 encoded JSON payload and pass it to the schema guru? So i end up with a separate schema per webhook pub/sub topic?\n. Ok, i'll keep collecting data now. But in the end i will have a schema for every singe webhook topic?\nschemas/com.shopify/create_cart/jsonschema/1-0-0\nschemas/com.shopify/update_cart/jsonschema/1-0-0\nschemas/com.shopify/create_order/jsonschema/1-0-0\nschemas/com.shopify/update_order/jsonschema/1-0-0\nAnd so on? Then what? How can i turn those schemas into a valid webhook implementation?\n. Awesome, thx a bunch\n. Hi @alexanderdean i'm still collecting, not sure if i have enough data from my own shop. Is there a rule of thumb how many events i need to see for a given endpoint?\n. I collected over 27MB if gzipped event data now. The problem i see now: How can i see from the request, to which webhook endpoint this request belongs? A typical event looks like this:\n\n2016-02-21    21:23:00    -   -   23.227.xxx.xxx  POST    23.227.xxx.xxx  /com.shopify/v1 200 -   Ruby    &cv=clj-1.1.0-tom-0.2.0&nuid=7ccf5558-4b87-45a5-b3e5-91bd42c4ba2d   -   -   -   application%2Fjson  eyJjcmVhdGVkX2F0IjoiMjAxNi0wMi0yMVQyMToyMzowMC4wNzFaIiwidXBkYXRlZF9hdCI6IjIwMTYtMDItMjFUMjE6MjM6MDAuMDcxWiIsImlkIjoiYTI4NDI5NGU3NjM3ZTMxYTlkYjI3Mzg1MDg4MTdkNDIiLCJ0b2tlbiI6ImEyODQyOTRlNzYzN2UzMWE5ZGIyNzM4NTA4ODE3ZDQyIiwibGluZV9pdGVtcyI6W3siaWQiOjE0MjM1NzkyMzI3LCJwcm9wZXJ0aWVzIjpudWxsLCJxdWFudGl0eSI6MSwidmFyaWFudF9pZCI6MTQyMzU3OTIzMjcsInRpdGxlIjoiQmlvIEVkZWxzY2hpbW1lbC1TYWxhbWkgdm9tIFNhdHRlbHNjaHdlaW4gLSBTY2h3ZWluIDIxMyBcLyAxODAgZyIsInByaWNlIjoiOS45MCIsImxpbmVfcHJpY2UiOiI5LjkwIiwidG90YWxfZGlzY291bnQiOiIwLjAwIiwiZGlzY291bnRzIjpbXSwic2t1IjoiRWRlbFNhLUhTVC1SUy1TTjIwMy0xODAiLCJncmFtcyI6MTkwLCJ2ZW5kb3IiOiJCYXVlciBIZW5yaWsgU3RhYXIiLCJwcm9kdWN0X2lkIjozNTk3MTY4MDcxLCJnaWZ0X2NhcmQiOmZhbHNlfV19\n\nAs i setup the webhooks to use the very same collector endpoint path, i cannot see what type of event i'm looking at. Is there a way of figuring out?\nThx, Christoph\n. The payload is very different, depending on the type of event. So an actual transaction event is much longer and contains address, lineitems and transaction information.\nSo i guess my initial approach would have saved us a bit time. Ok, I'll setup the webhooks accordingly and delete old data.\n. Ok, i setup a little processing tool for all the log files. But i have an issue with some JSON files. When i try to base64 decode the log file, sometime parts of the JSON object is missing. Can i configure the clojure collector to not base64 encode in the log files?\nThis is how i create all the schemas:\n./schema-guru-0.5.0 schema \\\n--enum 20 \\\n--vendor com.shopify \\\n--name $name \\\n--output schemas/$name \\\n$directory\nHere is an example of the output. Would that be something useful?\ncreate-cart.txt\n. Thx for the feedback. Any idea about the non decodable base64 encoded text?\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_dj0xNDU2MjQ2NDc4IiwidmFyaWFudF9pZHMiOltdfX0\ntranslates into this:\n{\"id\":4102207111,\"title\":\"Bio-Braten vom Sattelschwein (Keule)\",\"body_html\":\"\\u003cp\\u003eBraten aus der Keule (ohne Knochen) vom Sattelschwein, geschossen auf der Weide von Gut Hirschaue: Ein Traum f\u00fcr jeden beWursten Genie\u00dfer!\\u003c\\/p\\u003e\\n\\u003cp\\u003e\\u003cstrong\\u003eACHTUNG:\\u003c\\/strong\\u003e\u00a0Versand im Isolierkarton mit K\u00fchlgel-Pads am Donnerstag, 25.02.2016! Du musst sicherstellen, dass du das Paket am 26.02.2016 entgegen nehmen kannst (95% aller DHL-Pakete kommen am n\u00e4chsten Tag an, die K\u00fchlung im Karton h\u00e4lt ca. 48 Stunden)\\u003c\\/p\\u003e\\n\\u003cp\\u003eGek\u00fchlt mindestens haltbar bis 05.03.2016.\\u003c\\/p\\u003e\\n\\u003cp\\u003e\\u003cimg alt=\\\"\\\" src=\\\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/files\\/bio_small.jpg?3104375712574746664\\\"\\u003e\\u003cimg alt=\\\"\\\" src=\\\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/files\\/EU_BioLogo_small.jpg?3104375712574746664\\\"\\u003e\\u003c\\/p\\u003e\\n\\u003cp\\u003eDE-\u00d6KO-039\\u003c\\/p\\u003e\\n\\u003cp\\u003ePreis\\/Kg: 29,00 \u20ac (inkl. MwSt.)\\u003c\\/p\\u003e\\n\\u003cp\\u003eVersand: 7,90 \u20ac\\u003c\\/p\\u003e\",\"vendor\":\"Bauer Henrik Staar\",\"product_type\":\"Frischfleisch\",\"created_at\":\"2016-01-21T13:46:54+01:00\",\"handle\":\"bio-keule-vom-sattelschwein-ohne-knochen\",\"updated_at\":\"2016-02-23T17:54:38+01:00\",\"published_at\":null,\"template_suffix\":null,\"published_scope\":\"web\",\"tags\":\"Schwein\",\"variants\":[{\"id\":14070988999,\"product_id\":4102207111,\"title\":\"920 g\",\"price\":\"26.68\",\"sku\":\"BioKeu-HST-FF-SNX-920\",\"position\":1,\"grams\":1000,\"inventory_policy\":\"deny\",\"compare_at_price\":null,\"fulfillment_service\":\"manual\",\"inventory_management\":\"shopify\",\"option1\":\"920 g\",\"option2\":null,\"option3\":null,\"created_at\":\"2016-01-21T13:49:21+01:00\",\"updated_at\":\"2016-02-23T17:53:30+01:00\",\"requires_shipping\":true,\"taxable\":true,\"barcode\":\"\",\"inventory_quantity\":1,\"old_inventory_quantity\":1,\"image_id\":null,\"weight\":1.0,\"weight_unit\":\"kg\"},{\"id\":16495010183,\"product_id\":4102207111,\"title\":\"680 g\",\"price\":\"19.72\",\"sku\":\"BioKeu-HST-FF-SNX-680\",\"position\":2,\"grams\":1000,\"inventory_policy\":\"deny\",\"compare_at_price\":null,\"fulfillment_service\":\"manual\",\"inventory_management\":\"shopify\",\"option1\":\"680 g\",\"option2\":null,\"option3\":null,\"created_at\":\"2016-02-23T17:52:09+01:00\",\"updated_at\":\"2016-02-23T17:53:30+01:00\",\"requires_shipping\":true,\"taxable\":true,\"barcode\":\"\",\"inventory_quantity\":1,\"old_inventory_quantity\":1,\"image_id\":null,\"weight\":1.0,\"weight_unit\":\"kg\"}],\"options\":[{\"id\":5034811207,\"product_id\":4102207111,\"name\":\"Gewicht\",\"position\":1,\"values\":[\"920 g\",\"680 g\"]}],\"images\":[{\"id\":9752445511,\"product_id\":4102207111,\"position\":1,\"created_at\":\"2016-02-23T17:54:38+01:00\",\"updated_at\":\"2016-02-23T17:54:38+01:00\",\"src\":\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/products\\/SchweineBraten_Hirschaue.JPG?v=1456246478\",\"variant_ids\":[]}],\"image\":{\"id\":9752445511,\"product_id\":4102207111,\"position\":1,\"created_at\":\"2016-02-23T17:54:38+01:00\",\"updated_at\":\"2016-02-23T17:54:38+01:00\",\"src\":\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/products\\/SchweineBraten_Hirschaue.JPG?v=1456246478\",\"variant_ids\":[]\nNote the missing closing bracket at the end of the JSON string. I tried to decode it using ruby, bash, openssl. Even some online tools like this: http://base64decode.com/ \nFunny enough the online decoder seemed to work pretty good. \n. Well, if i use this url http://base64decode.com/ and paste in the aforementioned base64 decoded text, it turns out to be fine. So I'm wondering if there was a charset issue. As you can see the online service handles utf-8 charaters wrong like: \u00c3\u00bc\n{\"id\":4102207111,\"title\":\"Bio-Braten vom Sattelschwein (Keule)\",\"body_html\":\"\\u003cp\\u003eBraten aus der Keule (ohne Knochen) vom Sattelschwein, geschossen auf der Weide von Gut Hirschaue: Ein Traum f\u00c3\u00bcr jeden beWursten Genie\u00c3\u009fer!\\u003c\\/p\\u003e\\n\\u003cp\\u003e\\u003cstrong\\u003eACHTUNG:\\u003c\\/strong\\u003e\u00c2\u00a0Versand im Isolierkarton mit K\u00c3\u00bchlgel-Pads am Donnerstag, 25.02.2016! Du musst sicherstellen, dass du das Paket am 26.02.2016 entgegen nehmen kannst (95% aller DHL-Pakete kommen am n\u00c3\u00a4chsten Tag an, die K\u00c3\u00bchlung im Karton h\u00c3\u00a4lt ca. 48 Stunden)\\u003c\\/p\\u003e\\n\\u003cp\\u003eGek\u00c3\u00bchlt mindestens haltbar bis 05.03.2016.\\u003c\\/p\\u003e\\n\\u003cp\\u003e\\u003cimg alt=\\\"\\\" src=\\\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/files\\/bio_small.jpg?3104375712574746664\\\"\\u003e\\u003cimg alt=\\\"\\\" src=\\\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/files\\/EU_BioLogo_small.jpg?3104375712574746664\\\"\\u003e\\u003c\\/p\\u003e\\n\\u003cp\\u003eDE-\u00c3\u0096KO-039\\u003c\\/p\\u003e\\n\\u003cp\\u003ePreis\\/Kg: 29,00 \u00e2\u0082\u00ac (inkl. MwSt.)\\u003c\\/p\\u003e\\n\\u003cp\\u003eVersand: 7,90 \u00e2\u0082\u00ac\\u003c\\/p\\u003e\",\"vendor\":\"Bauer Henrik Staar\",\"product_type\":\"Frischfleisch\",\"created_at\":\"2016-01-21T13:46:54+01:00\",\"handle\":\"bio-keule-vom-sattelschwein-ohne-knochen\",\"updated_at\":\"2016-02-23T17:54:38+01:00\",\"published_at\":null,\"template_suffix\":null,\"published_scope\":\"web\",\"tags\":\"Schwein\",\"variants\":[{\"id\":14070988999,\"product_id\":4102207111,\"title\":\"920 g\",\"price\":\"26.68\",\"sku\":\"BioKeu-HST-FF-SNX-920\",\"position\":1,\"grams\":1000,\"inventory_policy\":\"deny\",\"compare_at_price\":null,\"fulfillment_service\":\"manual\",\"inventory_management\":\"shopify\",\"option1\":\"920 g\",\"option2\":null,\"option3\":null,\"created_at\":\"2016-01-21T13:49:21+01:00\",\"updated_at\":\"2016-02-23T17:53:30+01:00\",\"requires_shipping\":true,\"taxable\":true,\"barcode\":\"\",\"inventory_quantity\":1,\"old_inventory_quantity\":1,\"image_id\":null,\"weight\":1.0,\"weight_unit\":\"kg\"},{\"id\":16495010183,\"product_id\":4102207111,\"title\":\"680 g\",\"price\":\"19.72\",\"sku\":\"BioKeu-HST-FF-SNX-680\",\"position\":2,\"grams\":1000,\"inventory_policy\":\"deny\",\"compare_at_price\":null,\"fulfillment_service\":\"manual\",\"inventory_management\":\"shopify\",\"option1\":\"680 g\",\"option2\":null,\"option3\":null,\"created_at\":\"2016-02-23T17:52:09+01:00\",\"updated_at\":\"2016-02-23T17:53:30+01:00\",\"requires_shipping\":true,\"taxable\":true,\"barcode\":\"\",\"inventory_quantity\":1,\"old_inventory_quantity\":1,\"image_id\":null,\"weight\":1.0,\"weight_unit\":\"kg\"}],\"options\":[{\"id\":5034811207,\"product_id\":4102207111,\"name\":\"Gewicht\",\"position\":1,\"values\":[\"920 g\",\"680 g\"]}],\"images\":[{\"id\":9752445511,\"product_id\":4102207111,\"position\":1,\"created_at\":\"2016-02-23T17:54:38+01:00\",\"updated_at\":\"2016-02-23T17:54:38+01:00\",\"src\":\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/products\\/SchweineBraten_Hirschaue.JPG?v=1456246478\",\"variant_ids\":[]}],\"image\":{\"id\":9752445511,\"product_id\":4102207111,\"position\":1,\"created_at\":\"2016-02-23T17:54:38+01:00\",\"updated_at\":\"2016-02-23T17:54:38+01:00\",\"src\":\"https:\\/\\/cdn.shopify.com\\/s\\/files\\/1\\/1076\\/4310\\/products\\/SchweineBraten_Hirschaue.J\u00ff\u00ff\u00ffv=1456246478\",\"variant_ids\":[]}}\n. Not sure. Neither Linux+Mac commandline, openssl nor ruby clients could decode it successfully. But this online decoder can. But ... it is screwing up the german umlauts with wrong charset. Either way it is less than optimal ;( Can you successfully decode the text above with any tool at hand?\n. I may found the answer to the faulty files. I have setup the webhook for each possible type of events in shopify like this: /com.shopify/v1/update-customer\nAnd i saw in the log files, that the collector answered with a 404 response code. Yesterday i changed the path to this: /com.shopify.v1/update-customer Which leads to a 200 status code. Significantly less events with status 200 are borked, but still enough to be worried.\n. I also tried the url_safe version from the ruby base64 library. But in case the string could not be parsed correctly it raises an exception. See here: http://ruby-doc.org/stdlib-2.3.0/libdoc/base64/rdoc/Base64.html#urlsafe_decode64-method\nAt least this way i can assure to have proper JSON object as result.\n. Alright, i sent a PR to see what you think. https://github.com/snowplow/iglu-central/pull/365\n. I have collected the data until today. I have ~6000 valid webhook calls. Some events, like cancel order just happened twice, and some events happened 600+ times. It's not evenly distributed. But i still have the issue with the crippled events from the closure collector. This means at least 30% of the raw data can not be base64 decoded and is garbage.\n. @alexanderdean have a look at my PR: https://github.com/snowplow/iglu-central/pull/365. Oh brilliant, we were actually discussing the ways how to implement enrichments by calling some of our own API endpoints. Nice to see this feature being on the way. :+1: \n. Now I'm curious. What is Snowplow Mini?\n. Hey @fblundun \nwe actually do not use the custom javascript enrichment, as we do not have access to the raw event data.\nI think your interface is really spot on, given the fact that JsonNode unstructEventis actually representing the raw event object.\nIt would be cool to have custom enrichments, which do not require to be compiled in the SP core. I think people can be really creative when it comes to custom enrichments.\n. The problem with AWS ES service is, that they support the HTTP interface only not the binary TCP interface. See here, at the bottom of the page. http://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/aes-limits.html#d0e9673\nThis renders the AWS service useless, unless the kinesis-es sink can be configured to use the HTTP interface for bulk inserts.\n. Any news on that, or will this be obsolete with Snowplow Mini?\n. Did this ticket become obsolete? Or is this still on the radar?\n. Ah, that was the ticket i remembered, thx.\n. I can also provide an bunch of files then seemed to be truncated when trying to catch shopify webhooks. Maybe that helps with the analysis.\n. Can i send then directly to you? These files probably contain personal identifyable information about our clients and i don't want to expose them here ... You've got mail.\n. I think so, yes. Any other behaviour doesn't make sense. But maybe this could be a config option. \n. @fblundun is there any news on that?\n. How was i able to unasign people from the issue? @alexanderdean ??\n. That's a bummer. Now what?\n. :+1: I know it is a pain to keep this library up to date. But i think it is quite essential for proper event processing.\n. Did this PR ever make it into upstream? I cannot see it in the current master.\n. @alexanderdean this one is also still assigned to @fblundun. \n. Actually i didn't, because it is referring code, that i did not touch at all. So i thought, this should not be part of this PR.\n. I applied some comments now, that fix specs and provide real value.\n. Thx @alexanderdean, but it does not seem to resound with the needs of the rest of the community :)\n. \u231a waiting\n. @alexanderdean Can you check our internal pull request?\nhttps://github.com/liveintent-berlin/snowplow/pull/1/files and provide some feedback on it? \n. @jbeemster did you have a chance to look at the code?\n. @alexanderdean is it still scheduled for R86?\n. Hi @alexanderdean, i send some \ud83d\ude18  from \ud83c\udde9\ud83c\uddea. Would that make it easier to accept our PR and have it released?. Hey @alexanderdean or @BenFradet,\nR8x is way over. Is there a new milestone, we could attach this PR to?. Well, with traffic i mean additional requests. However, the collector checks for cookie and special parameter presence first. So it does not emit extra message to kinesis. From our perspective with ~= 100MM requests a day, it works perfectly fine with a 2-4 collectors, depending on the traffic pattern.. Ok, maybe I described it wrong. We see roughly about 30% of the traffic, we cannot put a third party cookie. This leads to one additional request-response cycle for these browsers for every call to the collector. However, the first request coming, that leads to a redirect to the collector is really cheap to handle in terms of computation. And only the second call coming from the redirect leads to a proper analysis of the request and thus a kinesis message.\nAnd this computation is much cheaper and more reliable than the batch computation described in discourse: https://discourse.snowplowanalytics.com/t/measuring-the-number-of-users-who-have-blocked-3rd-party-cookies/671. @alexanderdean You're right, it's a feature that has an \"enable\" flag, which is off by default. So people need to actively opt in for it. That should not have much impact on anyone, not willing to use it.. Thx, would be cool. Love to upgrade the es sink too.\n. Can't wait to see this feature being released. Good work @shin-nien \n. \ud83d\udc4d . Are we the only ones seeing this behaviour? @alexanderdean?\n. Thx @alexanderdean,\nand you know what, there is an issue already, regarding double fragment separators:\nhttps://github.com/NET-A-PORTER/scala-uri/issues/114\nI left a comment and asked for the changes.\n. @alexanderdean: I've never seen a faster fix!\nhttps://github.com/NET-A-PORTER/scala-uri/commit/22dd7672c9c3af68facbf949069fbfa4e67df468\n. @alexanderdean Has this dependency bump released ever since?. Thanks @alexanderdean, would be wonderful to see this being released. We're still losing out on 200k events every day.. @alexanderdean and @BenFradet from what collector version was this Scala Common Enrich version being used? Is it R92?. @yalisassoon is that still in planning?. Hi @alexanderdean,\ni commented on the spray github, as we see an increasing amount of requests being discarded by the collector, as the useragent does not seem to be valid. From my point if view, this is hurting snowplow, as it makes us lose a significant amount of requests. See here: https://github.com/spray/spray/issues/458#issuecomment-306763534. Haha, got a nice answer :) You are right, spray is not supported anymore for several years. https://github.com/spray/spray/issues/458#issuecomment-306798110\nGoing forward with akka http sounds good to me. Is Akka HTTP used right now at all? . Maybe it's a good idea to make a config option available, that would invalidate the cached files based on time. For example a TTL in seconds. We are subscribed to the professional Maxmind service, which allow us to download a fresh file every other day. Right now we are forced to overwrite the cached files and restart the service. Would love to have something more convenient.. For the collector, i think the usual suspects for any http endpoint:\n http_requests_total{path, code, method}\n http_request_duration_seconds_count\n http_request_duration_seconds_sum\n http_request_duration_seconds_bucket(histogram percentiles)\n http_request_size_bytes\n http_request_size_bytes_count\n* http_request_size_bytes_sum\nAlso for all parts reading from and writing to kinesis/kafka\n kinesis_events_write_total{type,format,retry,failure/success}\n kinesis_events_read_total{type,format,retry,failure/success}\nFor all parts in common:\n* build_info{scalaversion,javaversion,version,gitsha}\nAdd anything that the parts send as data metrics to the collector right now.\n. That fit's really nicely to my issue https://github.com/snowplow/snowplow/issues/2491\nThanks!. @alexanderdean yes, whatever it might be, i care more about the possibility to do it at all. As @rbolkey pointed out, it is needed to implement some sort of cookie sync.. @alexanderdean and @BenFradet \nwhat are the plans to merge the PR?. Thanks @BenFradet, sounds good. @BenFradet  and thoughts?. @alexanderdean and @BenFradet \ni submitted another corporate CLA for our new engineers including @melgenek . @BenFradet Were we able to address all your concerns?. Thank you @melgenek and @BenFradet for moving forward \ud83e\udd47 . Looks like the changes break the test setup. I apologize, but i cannot run the tests locally. Maybe @BenFradet can take a look and say, wether this PR is considered useful and how to fix the tests.. Technically, it is not limited to docker. You could also run the jar on bare metal and use environment variable for configuration rather than fixed file. This falls into the domain of 12 factor apps: https://12factor.net/\nThat being said, i looked into the snowplow-docker repo. It looks like for each release version and each stream provider you need a different config file, which is not part of the repo yet. That's why i thought it would be better to have a working base version of the config file in the actual code repo available to have it shipped with the docker images.\nWhat do you think @BenFradet?. I can certainly revert the double quote changes. However, the hocon parser fails to interpret the {{placeholders}} in the file. What mechanism is being used to replace those placeholders with real values? It seems to me there is another replacement process, similar to the environment variables.\nIs it possible to have real values in the file with sensible defaults?. I removed the quoting, but i checked the hocon format documentation. And it looks like unquoted { and } characters are actually not interpreted as values. @BenFradet How do you make the hocon parser work with it? I did not succeed.\nhttps://github.com/lightbend/config/blob/master/HOCON.md#unquoted-strings. Regarding to the best practices guidelines from prometheus, they encourage to use base metrics like seconds as a name for the metric. The value of the metric in the milliseconds range would then be a value below 1. See here: https://prometheus.io/docs/practices/naming/#base-units. We followed advice from Brian Brazil, core maintainer of prometheus regarding the software version. Have a look here: https://www.robustperception.io/exposing-the-software-version-to-prometheus. Ideally each newly introduced variable should have the ${?ENV_VARIABLE} counterpart coming with it, do you agree?. ",
    "naomiwells": "Sorry, found it :)\n. Okay, thought we found it. Sorry! So back to my original question :)\nDo you have any documentation on how you did this or have a reference where we can look? I'd really appreciate it.\n. ",
    "luxerama": "@alexanderdean sorry, I didn't think you were going to add this, which is why I didn't follow up. I should however have notified you about this, apologies.\nWe have also realised that the \\copy would have worked, so we have slightly changed the way the Postgres loader works. currently its less than perfect as it tries to sniff if we are connecting to an RDS instance, exposing us to potential issues should AWS change the RDS end point structure.\nIf anyone is interested our current working solution to the RDS problem can be found here If you guys are interested in this we are more than happy to create a pull.\n. ",
    "ihortom": "@alexanderdean the first link seems invalid. Could you verify, please?\n. @alexanderdean, is the valid link will be https://github.com/snowplow/snowplow/blob/64a789273047d33c186cf00e2a0d8aabe52339f3/2-collectors/thrift-schemas/snowplow-raw-event/src/main/thrift/snowplow-raw-event.thrift now?\n. @alexanderdean - Done. Please, review the Thrift events format if the info is sufficient. If so the ticket could be closed. Also that section could cover yet another ticket #2231.\n. @alexanderdean - Updated as per recommendation.\n. @alexanderdean - I believe so: https://github.com/snowplow/snowplow/wiki/Setup-IAM-permissions-for-users-installing-Snowplow. Found no broken links. Must have been fixed since.. @alexanderdean - Added the page locally. For now it covers releases r65 through r76. Will add earlier releases soon.\n. @alexanderdean - Added releases r60 - r64 and slightly updated the layout as discussed as well as renamed the page\n. @alexanderdean - Added releases v0.9.5 - v0.9.14.\n. @alexanderdean - Added the remaining agreed releases v0.9.0 - v0.9.4. Could be closed now.\n. @alexanderdean I don't see anything broken here unless I'm looking at the wrong page.\n. @alexanderdean - updated locally\n. @alexanderdean - Updated screenshots for Cloudfront collector setup and added AWS CLI commands as an alternative approach. You could view the changes in local fork:\n1. Setup a bucket on S3 for the pixel\n2. Upload the tracking pixel\n3. Create a bucket for Cloudfront logs\n4. Create a Cloudfront distribution\n. @alexanderdean - Updated screenshots for Clojure collector setup. You could view the changes in local fork:\n1. Create a new application in Elastic Beanstalk and upload the WAR file into it\n2. Enable logging to S3\n3. Configuring the Clojure collector\n4. Additional configuration options\n. @alexanderdean - Updated the screenshots for Setting up PostgreSQL <- local copy\n. @alexanderdean - Looks like those are all the pages referencing AWS screenshots. The ticket could be closed. If anything missed accidentally, will be dealing on case-by-case basis. I am aware of one page with old screenshots (Troubleshooting jobs on Elastic MapReduce) but I do not have a suitable (failure) example to get it updated.\n. @alexanderdean / @yalisassoon - AWS screenshots were updated a while ago. Apart from that, would need to get a clarification/list on what is expected to be updated...\n. @alexanderdean - Removed cube related instructions. Added a warning that we are working on new data models and recipes as discussed with @bogaert. Local copy is here. Could be closed now.\n. @alexanderdean - I don't really see what needs to be updated here. The mentioned release addresses addition of the bad stream which is already referenced in the doc? Anything specific which I missed?\n. @alexanderdean / @chuwy - I added AWS CLI instructions to the relevant pages. Additionally, I created a new page (and set references to it) describing how to set up EC2 instance. All the instructions (with ocasional amendment) are taken from this issue page.\n. @alexanderdean updated locally but no permissions to push.\n. @alexanderdean  / @yalisassoon - Added POST as yet another method (local fork).\n. @alexanderdean could you, please, close this ticket?\n. @alexanderdean could you, please, close this ticket?\n. Updated locally\n. @alexanderdean  - Added reference to the Canonical event model, which describes the fields. Please, push and close.\n. @alexanderdean - Is there a way to track down any references to the page to be deleted to have them removed proactively as well since deleting this page could raise \"broken link\" issues?\n. Updated wiki:\n\nhttps://github.com/snowplow/snowplow/wiki/Create-a-new-application-in-Elastic-Beanstalk-and-upload-the-WAR-file-into-it\nhttps://github.com/snowplow/snowplow/wiki/Troubleshooting-Clojure-Collector-instances-to-prevent-data-loss. @alexanderdean Looks like it refers to README: \n. @fblundun would you know the correct link? The doc anchor refs to the broken link. It could be just misspelt or else.\n. Updated on local fork as per suggestion from @fblundun\n. @alexanderdean, this was fixed earlier, please, close. When updating the link didn't realise that there was actually a ticket opened for it.\n. @alexanderdean - Updated locally.\n. @alexanderdean - Done.\n. @alexanderdean - It was done locally a while ago but still hasn't been pushed to origin.\n. @alexanderdean - It was done locally a while ago but still hasn't been pushed to origin.\n. @alexanderdean - Deprecated the info and provided the link to the instructions from Amazon docs. It might not be sufficient but it's not something I could verify myself. Our own instructions could be provided when we, say, renew our certificate.\n. @alexanderdean you can review the the forked version of the page. Also I noticed the same problem with StorageLoader installation\n. @alexanderdean - I added ref to section 5.1.3 of RFC 6265. That includes other setup pages too, in particular Scala Collector docs.\n. @yalisassoon - Do you agree something needs to be rephrased? The excerpt refers to this section.\n. Updated locally\n. Done.\n. @alexanderdean / @fblundun - I corrected over 400 broken links (some are still evident only in my forked version). Once pushed, @fblundun could you run the validator again, please? I have dificulties running it myself in my environment.\n. @alexanderdean / @fblundun - Done locally. Hopefully, this is it.\n. For the sake of clarity, the page being referenced is this. @fblundun could you suggest what was meant here?\n. Updated locally\n. Removed the references to a non-existing (deleted) page.\n. Done locally\n. @alexanderdean - done locally. The main page to observe the changes is this one. I left the preambule as the new articles are to be written later on (as per your initial comment).\n. @alexanderdean - done locally (preview here)\n. @alexanderdean -  I \"laid a foundation\" for Glossary. Please, review and let me know if the format is OK. \n. @alexanderdean - Done locally. I guess there's no need to keep this ticket open. We just have to keep populating the Glossary as needed.\n. @alexanderdean - completed the matrix and populated the page. Before flashing out and wiring, could you review the page, please?\n. @alexanderdean - this could be closed.\n. @alexanderdean - Done.\n. @alexanderdean - Please, review the local copy. Additional examples of code for different trackers are yet to be added.\n. @alexanderdean / @bogaert / @yalisassoon  - I would keep the term \"event dictionary\" limiting its boundaries. When it comes to drawing screenshots and preparing JSON schemas it's building \"event dictionary\". When it comes to actual  (real world) implementation that would be called \"schema registry\". Thus \"event dictionary\" and \"schema registry\" are related but not mutually exclusive.\n. @alexanderdean - replaced the references to repository with registry. About event dictionaries: you do need to think ahead what you want to capture and draft the schemas accordingly (it cannot be called registry at this point). And that's an important phase. Users are free to skip it if the events are not complex.\n. @alexanderdean - the wiki section has been live for quite a while now. Could you, please, close the issue?\n. @alexanderdean - Updated the following 3 wiki pages:\nIP lookups enrichment\nCampaign attribution enrichment\nWeather enrichment\n. @alexanderdean - Updated the following wiki pages:\nCurrency conversion enrichment\nua-parser enrichment\nuser-agent-utils enrichment\nIP anonymization enrichment\nreferer parser enrichment\n. @alexanderdean - Updated the remaining pages:\nEvent fingerprint enrichment\nCookie extractor enrichment\nJavaScript script enrichment\n\nPlease, review them carefully as they were harder to figure out the answers.\nIf all is OK, the ticket could be closed.\n. @alexanderdean - Done.\n. @alexanderdean - Updated all the EmrEtlRunner setup pages to reflect the end of support for Ruby based runner. Additionally, did the same on StorageLoader setup pages.\n. @alexanderdean - Done. The local copy is here. Once approved and pushed, I will add the ref to the article on Kinesis-S3 Sink page.\n. Here's what I found about it:\n\nYou must specify the shard iterator type. For example, you can set the ShardIteratorType parameter to read exactly from the position denoted by a specific sequence number by using the AT_SEQUENCE_NUMBER shard iterator type, or right after the sequence number by using the AFTER_SEQUENCE_NUMBER shard iterator type, using sequence numbers returned by earlier calls to PutRecord, PutRecords, GetRecords, or DescribeStream. You can specify the shard iterator type TRIM_HORIZON in the request to cause ShardIterator to point to the last untrimmed record in the shard in the system, which is the oldest data record in the shard. Or you can point to just after the most recent record in the shard, by using the shard iterator type LATEST, so that you always read the most recent data in the shard.\n-- docs.aws.amazon.com\n\n@fblundun - I've no control over the page's title. It's automatically generated.\n. @alexanderdean - please, close (already live). Also added the ref to the page from Kinesis S3 sink page.\n. @alexanderdean  - Updated the page on my local fork\n. @alexanderdean - Updated the larger half of the doc. The new version of GTM is quite different. It's time consuming to update all the screenshots and instructions due to the sheer size of the article and the fact I have never used GTM before. You can either push what I currently modified or wait till I complete the whole page.\n. @alexanderdean - please, review and push (can be closed).\n. @alexanderdean - https://github.com/snowplow/snowplow/wiki/Integrating%20javascript%20tags%20with%20Google%20Tag%20Manager. Are there any more? The local copy is here.\n. @yalisassoon - could you, please, review the page. There are so many in the \"backlog\" because of this waiting to be pushed. The local copy I created is here.\nNote the last screenshot (preview) does not reflect the actual scenario discussed on the page but it is the representation of the current Google look. Thus if you do have (or can produce) that screenshot for  the actual scenario, please, share.\n/cc @alexanderdean / @fblundun \n. @fblundun - thanks for spotting this. Corrected.\n. @alexanderdean - updated the last screenshot (kindly provided by @yalisassoon)\n. @alexanderdean / @fblundun - I think that if there's a necessity to rename a (old) page, we should rather:\n1. Create a new one with the required name (and moved/amended content). \n2. The old page should remain with the note the page has been moved and provide the link to the new page.\n. That page and subject triggers the need to update the following pages too:\nhttps://github.com/snowplow/snowplow/wiki/setting-up-EmrEtlRunner\nhttps://github.com/snowplow/snowplow/wiki/Shredding\nhttps://github.com/snowplow/snowplow/wiki/4-Loading-shredded-types\nhttps://github.com/snowplow/snowplow/wiki/4-Self-hosting-Hadoop-Enrich\nWill take care of them.\n. @alexanderdean - Updated all the pages.\n. Done. @alexanderdean could you push the update, please?\n. @alexanderdean - Updated the following pages:\n- https://github.com/snowplow/snowplow/wiki/integrating-javascript-tags-onto-your-website\n- https://github.com/snowplow/snowplow/wiki/hosted-assets\nAlso added a comment how to ensure the latest version is loaded (if the page is behind).\nWill do the same to https://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker once get local copy synced with the master (have some changes in progress).\n. @alexanderdean - Updated the last page. The ticket could be reassigned once pushed.\n. @alexanderdean - I thought we were going to remove software section completely as we do not support neither hbase nor lingual...\nMany users provide some values in config.yml even though they do not use the software and that leads to failures.\n. @alexanderdean - Is it to reclaim some space back (to be available for shredding)?\n. @alexanderdean - Done. Local copy is here.\n. @alexanderdean - please, close (already live).\n. @alexanderdean - done.\n. @alexanderdean - could this be closed, please?\n. @alexanderdean - Updated. Also modified the table as it was confusing Parameter with its Description\n. @yalisassoon / @alexanderdean - Updated 2 pages (locally):\n- Contexts overview\n- 2-Specific-event-tracking-with-the-Javascript-tracker#315-custom-contexts\nLet me know if you think of any other pages. Otherwise push and close.\n. @alexanderdean - Done: http://discourse.snowplowanalytics.com/t/troubleshooting-clojure-collector-instances-to-prevent-data-loss/184\n. @alexanderdean - Fixed the broken links (locally).\n. @alexanderdean - modified locally.\n. @alexanderdean - modified locally.\n. @alexanderdean - Done.\nhttp://discourse.snowplowanalytics.com/t/how-to-setup-a-lambda-architecture-for-snowplow/249\n. @alexanderdean - Updated (locally). Please, push and close.\n. Actually it's already added but not referenced in the homepage for Snowplow Analytics SDK. Updating it now...\n. @alexanderdean - updated (locally). Please, push.\n. @alexanderdean - done.\n. @yalisassoon - I updated the Common Configuration article. However, to update the Setting up Redshift, I need a real example of (access to) SSL enabled cluster.\n. @alexanderdean - that article is all about screenshots. If that is what expected, I need to produce a screenshot of some client. If that is not required and a few sentences will do then I will completed the task.\n. done\n. corrected\n. I think it works without iglu: as well. Nonetheless, amended to avoid confusion.\n. @leonmaas - done. Also corrected similar typo in section https://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#315-custom-contexts\n. Done\n. Done\n. Done. Also corrected the same typo in Sidebar\n. @alexanderdean / @yalisassoon - I can see this in wiki: https://github.com/snowplow/snowplow/wiki/Configure-the-Scala-Stream-Collector#5-setting-the-cookie-duration. Is it not what you were looking for? Any other places to consider?. Done. Info added to the page https://github.com/snowplow/snowplow/wiki/Shredding (rather than a separate page)\n. Done.. @alexanderdean - Done: https://github.com/snowplow/snowplow/wiki/Setting-up-the-Cloudfront-collector#using-webhook-adapter. @alexanderdean - The wiki page does provide the link to other versions and explains how to get them:\n\nNote: Please, follow this link if you wish to get a different version of the runner. The distribution name follows the pattern snowplow_emr_{{RELEASE_VERSION}}.zip.\n\nDo you think something else needs to be done?. Done. Done. Done. Done. Done. Fixed. Done.\nhttps://github.com/snowplow/snowplow/wiki/1-General-parameters-for-the-Javascript-tracker#223-configuring-the-cookie-domain. Done.. Done. Done.. @alexanderdean - I actually can find some of the old posts on snowplow.github.com in my local repo. Maybe I could place those on wiki and update the references rather than delete them?. Done.. Fixed.. Fixed.. Added to \"Testing-the-Javascript-tracker-is-firing\" page only. Don't want to duplicate it on \"Javascript-tracker-setup\" as it links to \"Testing-the-Javascript-tracker-is-firing\" which is the most appropriate location for the reference to Snowplow Inspector.. https://github.com/snowplow/snowplow/wiki/Android-Tracker-Setup#34-proguard. @chuwy  - R97+ seems to work fine. I believe it's fixed now (can't be sure which release achieved this).. We no longer have anything similar to the Cookbook. Removed the references. @alexanderdean, looks like we do not have much choice as to drop enum:\n. . .\n    \"priority\": {\n        \"type\": [\"string\", \"null\"]\n    },\n    \"status\": {\n        \"type\": [\"string\"]\n    },\n    \"tags\": {\n        \"type\": [\"string\", \"null\"]\n    },\n    \"ticketType\": {\n        \"type\": [\"string\", \"null\"]\n    },\n    . . .\nThat also means amending the table com_zendesk_snowplow_ticket_updated_1 with the extended length of the corresponding VARCHAR fields to cater for longer (non-English) values.. Found many more of those on the same page. All fixed.. Done.. Updated.. Thanks.. Updated. Updated. Thanks.. Indeed, I think it makes sense to have a reference to skip steps as part of those names. For example,\n\n[staging] Elasticity S3DistCp Step: Raw {raw location} -> Raw Staging S3 (there is one per input bucket)\n[staging] Elasticity S3DistCp Step: Raw S3 -> Raw HDFS\n[enrich] Elasticity Spark Step: Enrich Raw Events\n[enrich] Elasticity S3DistCp Step: Enriched HDFS -> S3\n[enrich] Elasticity Custom Jar Step: Empty Raw HDFS\n[enrich] Elasticity S3DistCp Step: Enriched HDFS _SUCCESS -> S3\n[staging_stream_enrich] Elasticity S3DistCp Step: Stream Enriched {enriched location} -> Enriched Staging S3\n[shred] Elasticity S3DistCp Step: : Enriched S3 -> HDFS\n[shred] Elasticity Spark Step: Shred Enriched Events\n[shred] Elasticity S3DistCp Step: : Shredded HDFS -> S3\n[shred] Elasticity S3DistCp Step: Shredded HDFS _SUCCESS -> S3\n[archive_raw] Elasticity S3DistCp Step: Raw Staging S3 -> Raw Archive S3\n[rdb_load] Elasticity Custom Jar Step: Load {storage target} Storage Target (there is one per storage target)\n[archive_enriched] Elasticity S3DistCp Step: Enriched S3 -> Enriched Archive S3\n[archive_enriched] Elasticity S3DistCp Step: Enriched S3 -> S3 Enriched Archive\n[archive_shredded] Elasticity S3DistCp Step: Shredded S3 -> Shredded Archive S3\n[archive_shredded] Elasticity S3DistCp Step: Shredded S3 -> S3 Shredded Archive\n\nIt also could be more granular. Instead of just enrich it could be broken down into enrich-1, enrich-2, etc.. While running r112-rc3 every 2 minutes now and then we come across a failure with the error\nEMR Cluster must be in WAITING state before new job steps can be submitted - found RUNNING\nIt appears due to starting a new job on the persistent cluster while the cluster has not yet switched its status to WAITING. Also, 2 minutes interval is quite short to squeeze in an action like removing the lock. It seems that decreasing run frequency to every 5 mins is more plausible at the moment.\n. We also should keep in mind that X-Forwarded-For could have a few IP addresses listed. I guess the order is significant and we need to extract the 1st one:\nX-Forwarded-For: 105.167.230.20, 82.145.209.195. ",
    "kinabalu": "I haven't signed the CLA yet.  Been testing this in production though, and it doesn't appear to be processing records with a platform type of app or mob that were added as you would expect.  Would there be anything more to do to make those changes seen in production?\n. I should also mention, that the tests are all passing with the new types in them\n. Signed the CLA\n. @alexanderdean ok, so still not able to get this processed in production.  here's what I'm doing:\n1. Modify the BuildSettings.scala in 3-enrich/scala-common-enrich/project and change the version variable to be 0.1.0-SNAPSHOT\n2. Run sbt publish-local in 3-enrich/scala-common-enrich.  Packages now exist in ~/.ivy2\n3. Edit Dependencies.scala in 3-enrich/scala-hadoop-enrich/project and change the value of commonEnrich to 0.1.0-SNAPSHOT.\n4. Re-run everything.\nStill receiving:\n[\"Field [p]: [app] is not a supported tracking platform\"]\nI'm obviously missing a step here, thanks for your help\n. @alexanderdean that totally worked.  changes are working fine in production now.  Thanks!\n. ",
    "manjitkumar": "Can someone look into it and close the issue? It been here for like more than 3 years now. @alexanderdean @yalisassoon :(. ",
    "jonalmeida": "(Developing idea) We're grabbing the location for mobile devices, so maybe something that maps out when people are using the mobile while on the go, or at home.\n. Yeah, I'm still trying to wrap my head around what can be done that level. I'll keep pondering about a bit more.\n. We decided the key for the event id should be eid.\nI'll ask @fblundun on how to do this when I add the mobile context information.\n. Should be able to close this as well.\n. ",
    "kazjote": "Thanks for working on this! I am setting up snowplow Kinesis workflow and really need data on S3 to use with Spark.\nWhat is still missing here?\nI know a bit scala. Can I help somehow?\n. I have been building using feature/kinesis-s3-sink branch and still had to change excluded jars set in the build.sbt.\nI though that kinesis-s3-sink will write already enriched events as it was in the 4-storage directory. I will reevaluate my workflow options. Thanks! :+1: \n. I am working on it. These are my propsed json schemas:\nhttps://github.com/snowplow/iglu-central/compare/master...RetentionGrid:cookie_extractor?expand=1\nLet me know if they are ok.\n. :+1: \n. Thanks for your interest in the topic.\n1. I am using Spark to analyze data\n2. Anything that Spark can read will do\n3. The best would be split by appId\n. It works for me. For debugging purposes we are using mainly Ruby scripts - reading parquet files could be problematic with Ruby (however, still possible with JRuby) \n. I have just signed CLA - thanks!\n. :+1: it would make my life much easier... Counting columns in TSV files is quite challenging ;)\n. This enrichment would be very useful for us. Are there any estimates when it will be production ready?\n. Thanks for the answer! When do you expect the release of Scala Kinesis Enricher with this feature?\n. Thanks for working on this great project! :-)\n. I was thinking about this. However, I was afraid that in majority cases it would mean importing session cookies (the ones that keeps session id) into analytic platform. Because of this, I think that '*' filter option could be a trap rather than help.\n. > >  why gzip support exists in the first place\n\nThe GZIP support was added by @kazjote - so probably best to ask him about his use case (remember Kinesis S3 is not a Snowplow-exclusive project).\n\nOur use case is very simple. We are processing enriched events with Spark. We do not need to load it into Redshift or any other data store. Implementing gzip support in Kinesis S3 was the easiest way to persist enriched events in S3 at that time.\nHowever, now there seems to be easier way with AWS Firehose and AWS Lambda. You can move any data from Kinesis stream to Firehose using this Lambda function: https://github.com/awslabs/lambda-streams-to-firehose from AWS Labs.\nFirehose compresses Kinesis records and persists to S3. It is simplier as you don't have to run any process on your instances.. In order to assembly I had to modify this set on Ubuntu 14.10 / Java 7 while building yesterday. My set looks like:\nval excludes = Set(\n    \"junit-4.8.2.jar\",\n    \"jsp-2.1-6.1.14.jar\",\n    \"jasper-compiler-5.5.12.jar\",\n    \"jsp-api-2.1-6.1.14.jar\",\n    \"servlet-api-2.5-6.1.14.jar\",\n    \"commons-beanutils-1.7.0.jar\",\n    \"hadoop-lzo-0.4.19.jar\",\n    \"stax-api-1.0.1.jar\",\n    \"commons-collections-3.2.1.jar\"\n  )\n. ",
    "esurdam": "Hey @alexanderdean any update on this? Have everything setup and working but Postgres doesn't like to import the LZO'd files. Any alternate solutions on getting Kinesis enriched events loaded into Postgres? \n. ",
    "dashirov-ga": "What was the rationale for this?\nFound a reference to this issue in atomic.com_snowplowanalytics_snowplow_submit_form_1 DDL as a TODO item.. Was this de-scheduled to synchronize breaking changes with existing EMR/Hadoop tooling or for some other reason? Was this discussed anywhere on discourse or elsewhere?. I've noticed the enrichment downloads ( or attempts to download) the ip database into the current working directory of the running stream enrichment process. It silently fails to download,  if your start the process from a path that lacks sufficient permissions to download the database.\nChanging directory to a location writable by the running enrich process should fix your situation. Fixing the enrichment to download database files into a known and configurable location obviously is a better thing to do.. ",
    "tclass": "any updates on this or is there another way to exlucde events based on user-agent or IP?\n. This ticket is already a little bit old, but with the new zstd encoding, the results would be even better for some columns. https://aws.amazon.com/de/about-aws/whats-new/2017/01/amazon-redshift-now-supports-the-zstandard-high-data-compression-encoding-and-two-new-aggregate-functions/. I'm also waiting for this, because with EMR version 5.13 there're new instance types available https://docs.aws.amazon.com/emr/latest/ReleaseGuide/Spark-release-history.html once it's bumped it might be interesting for others too. I'm not sure how spark version and emr version are connected. ",
    "ganeshsp": "Hello Guys,\nCan you please help me to load shredded-tyes into postgresql?\nall events are getting enriched and shredded properly.\natomic.events are getting loaded properly . but other tables like link_click_1 are not loading.\nPlease help me out to resolve this. ",
    "alienware": "Hey guys,\nIs there an alternative way to generate tags?\n. Thanks a lot for pointing that out. Love the documentation. :)\n. ",
    "bugant": "Thank you @alexanderdean. We're going to sign the CLA, just after we got the OK from our employer company (just a matter of bureaucracy, we already had agreement on it)\n. @alexanderdean We have just submitted the CLA module\n. OK @potomak\nOn May 2, 2014 11:30 PM, \"Giovanni Cappellotto\" notifications@github.com\nwrote:\n\n@bugant https://github.com/bugant I'd like to close this PR to submit a\nnew one on top of master branch which is more up to date than develop,\nsee also #714 https://github.com/snowplow/snowplow/pull/714.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/pull/689#issuecomment-42081116\n.\n. \n",
    "potomak": "@pkallos I don't know if it's a good idea to include https://github.com/pkallos/amazon-kinesis-connectors as an external library as it seems more like an example project and it doesn't seem to be maintained at all by @awslabs.\nI also tried to put in this PR only needed classes and omitted others (for example BasicJsonTransformer.java or RedshiftTransformer.java).\n. Ok, you've convinced me, let's include it as a library.\n. @pkallos how can I add https://raw.githubusercontent.com/pkallos/amazon-kinesis-connectors/mvn-repo/ as a resolver?\n. @bugant I'd like to close this PR to submit a new one on top of master branch which is more up to date than develop, see also #714.\n. :+1: \n. It is. Nice job @pkallos.\n. ",
    "teachar": "Good\n. Goo\n. ",
    "ACOKing": "Thanks, and Come Check out my repository\n. ",
    "Kimblebrook": "Hi Alex - I'll get that sorted out with our legal guys on this side.\nOn Fri, May 16, 2014 at 1:01 PM, Alexander Dean notifications@github.comwrote:\n\nThanks Peter! I just checked and couldn't see Simply Business having\nsigned the CLA - https://github.com/snowplow/snowplow/wiki/CLA Could you\nget that signed and let me know?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/749#issuecomment-43323710\n.\n. Hi Alex\n\nWe should be signed up now - could you let me know if you have everything\nyou need?\nOn Fri, May 16, 2014 at 1:22 PM, Alexander Dean notifications@github.comwrote:\n\nThanks Stewart!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/snowplow/snowplow/issues/749#issuecomment-43325048\n.\n. I like the idea of making this generic - we use other internal ids for affiliates etc and presumably we could then use this for those business keys too, in a similar way to the multiple fields in the campaign_attribution in 0.9.9?\n. @alexanderdean is this for the gclid? I.e in the Google click ID or gee-see-ell-ID rather than gee-ell-see-ID as you have it here?\n\nIf so, then very helpful!\n. That's awesome - thanks!\n. Hi Yali\nWe've done some work on this, if you'd like to discuss detail. Very cool\ninitiative, by the way. Our approach was more about batch loading into\nRedShift, but we have figured out a fair amount about what the various\nreports mean and how the gclids work.\nRegards\nStewart\nOn Wed, Jan 21, 2015 at 9:49 AM, Yali notifications@github.com wrote:\n\nDepends on #1073 https://github.com/snowplow/snowplow/issues/1073\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1339#issuecomment-70810274.\n. Christian, out of interest, are you using the Snowplow pipeline to load the\ndata or do you load it directly to your analytics database?\n\nOn Wed, Jan 21, 2015 at 10:39 AM, lubasch notifications@github.com wrote:\n\nWe can add that it's really worth to wait up to 16-24 hours for full data\nbeing available in adwords via API. Otherwise your numbers will never match\nfor the later part of the day.\nCheers,\nChristian\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1339#issuecomment-70816538.\n. As it happens, that's how we're doing it too.\n. \n",
    "joaolcorreia": "Hi Alex, yes I have submitted the individual CLA.\n. Yes, please!. ",
    "gregorg": "PR #624 is much much faster :\nLoading Snowplow events into PostgreSQL LAB (PostgreSQL database)...\n   Load file 1/6 completed, 205549 records loaded successfully.\n   Load file 2/6 completed, 234642 records loaded successfully.\n   Load file 3/6 completed, 168862 records loaded successfully.\n   Load file 4/6 completed, 229375 records loaded successfully.\n   Load file 5/6 completed, 212009 records loaded successfully.\n   Load file 6/6 completed, 164249 records loaded successfully.\nLoad completed, 1214686 records.\nCompleted successfully\nbundle exec bin/snowplow-storage-loader -s download,delete,analyze,archive -c  336.52s user 35.54s system 35% cpu 17:34.94 total\nLoading Snowplow events into PostgreSQL LAB (PostgreSQL database)...\nOpening database connection ...\nRunning COPY command with data from: /home/snowplow/events/run=2014-05-27-20-21-55/part-00003\nResult of COPY is: PGRES_COMMAND_OK\nRunning COPY command with data from: /home/snowplow/events/run=2014-05-27-20-21-55/part-00000\nResult of COPY is: PGRES_COMMAND_OK\nRunning COPY command with data from: /home/snowplow/events/run=2014-05-27-20-21-55/part-00004\nResult of COPY is: PGRES_COMMAND_OK\nRunning COPY command with data from: /home/snowplow/events/run=2014-05-27-20-21-55/part-00001\nResult of COPY is: PGRES_COMMAND_OK\nRunning COPY command with data from: /home/snowplow/events/run=2014-05-27-20-21-55/part-00002\nResult of COPY is: PGRES_COMMAND_OK\nRunning COPY command with data from: /home/snowplow/events/run=2014-05-27-20-21-55/part-00005\nResult of COPY is: PGRES_COMMAND_OK\nCompleted successfully\nbundle exec bin/snowplow-storage-loader -s download,delete,analyze,archive -c  30.68s user 1.98s system 16% cpu 3:12.45 total\n. So I think we should close this PR and merge #624, right ? If so, I let you close this one ;)\n. I set \"/tmp\" for testing, but the purpose of this sink is to plug another storage without any AWS, like an internal Hadoop cluster or some micro-batch for Apache Spark, for example.\n. Yes, it make sense, however what is the best way to do without Kinesis ?\nSo, what's the best way to do that ? HTTP ---> scala-stream-collector ---->Redis-sink---->Redis-source---> scala-enrich-withoutkinesis --> HDFS\nI'm going to code this week ;)\n. Yes, I'm interested by adding Kafka support, however I'm new to scala, java ecosystem and Kafka. This PR is my first scala program ... So I'm scared about doing something in the wrong way.\nI have a question, why use raw events in the kafka stream and not thrift events ?\nDoes it make sense to create a new branch ?\n. Hi @alexanderdean, I'm going to write scala-kafka-enrich and see that really a lot of scala-kinesis-enrich will be duplicate. I think it should be better to add a kafka source and sink to scala-kinesis-enrich, don't you ?\n. Fine! I'm going to add a kafka source to scala-kinesis-enrich, which will read from a topic. \nCan I also add a HDFS sink to this enrich process ?\n. Indeed, I did not know camus.\nSo I have to add a kafka sink which will be sourced by camus, right ?\n. It's still an ugly patch, which should be properly fixed by bumping snowplowCommonEnrich to 0.5.0 or 0.6.0 ... Right now it's a small fix which fix a broken feature.\n. ",
    "pvdb": "Awesome turnaround :thumbsup: :smile:\n. > I've made these changes. Closing...\nJust wondering: where?  ie. what commit?\n. Thanks @fblundun - it's just me being thick... github Wikis always trip me over.\n. Just to confirm that we are having the same issue when switching from 2.4.2. to 2.4.8... would be good to understand why!  :smile:\n. > :g/will/s/will/might/g\n:smile: \n. > :g/will/s/will/might/g\n:smile: \n. > :g/will/s/will/might/g\n:smile: \n. ",
    "gs-akhan": "But there nothing like \nhttp://d21ey8j28ejz92.cloudfront.net/analytics/v1/sp.min.js in this repo \nThanks\n. Thanks, But what i am assuming is sp.min.js is the code that does all the tracking on browser. \nSo i was looking for unminified version of this..\n:)\n. aaaah.. So snowplow.js is the main file and it requires other modules internally and makes a complete..\nThe final sp.min.js is probably smaller than all files combined (Excluding the dependencies in package.json) \nso how does this all work ?\n. ",
    "GleasonK": "All done, no problem!\nOn Wed, Jun 18, 2014 at 9:39 AM, Alexander Dean notifications@github.com\nwrote:\n\nKevin this is awesome! Thanks so much for contributing this.\nI don't think you've signed a Contributor License Agreement yet - could\nyou sign one? Here is the link:\nhttps://github.com/snowplow/snowplow/wiki/CLA\nMany thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/841#issuecomment-46443921.\n. Let me know if there is anything I can do!\n\nOn Wed, Jun 18, 2014 at 12:06 PM, Alexander Dean notifications@github.com\nwrote:\n\nHuge! Thanks Kevin, let's get this prepped for release\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/841#issuecomment-46464161.\n. \n",
    "egonSchiele": "This is a bug in my library. Contracts aren't working on module functions. Once I changed module Cli to class Cli, contracts started getting checked. I've made a note of the issuehere and I will work on getting a fix out. Thanks for catching it!\n. ",
    "hslee16": "will do!\n. ",
    "quorak": "+1 \n2015-04-27 10:49:26 CEST STATEMENT:  COPY atomic.events FROM STDIN WITH CSV ESCAPE E'\\x02' QUOTE E'\\x01' DELIMITER '    ' NULL ''\n2015-04-27 10:49:27 CEST ERROR:  value too long for type character varying(255)\n2015-04-27 10:49:27 CEST CONTEXT:  COPY events, line 2841, column mkt_source: \"facebook.se/lp/limited/xxxxx.\"\n. ",
    "epantera": "Great, thanks Alex.\nTo give you a little bit more context of how it could be used here:\nWe plan to use Snowplow for tracking our Server-Side events. We will track all typed events happening in our business platform into unstructured events. We talk about 100 to 1000 different types of events.\nSo in a first step we want to track them all - then in a second step, if we decided to fine analyse ones of them, we will activate Shredding for them.\n. You Rock, thanks @alexanderdean \n. Hi @fblundun \nseems that we have an issue here with the r70 and the move to postgres jdbc driver.\nWe can't connect to Redshift.\nDid you already managed to connect to Redshift with r70 ?\nthx,\n. Hmm, seems to work better when adding ?ssl=true&sslfactory=org.postgresql.ssl.NonValidatingFactory\nto the connection_url in postgres_loader.rb\n. Hey @alexanderdean \nseems that the following approach is an elegant way to tune JVM;\n1) Create a custom bootstrap script file hosted in S3\neg: s3://viadeo-snowplow-processing/bootstrap-actions/snowplow-emr-customize-heap-size.sh with content :\n``` sh\n!/bin/sh\n/usr/share/aws/emr/scripts/configure-hadoop -m mapred.child.java.opts=-Xmx2G\n```\n2) Declare the custom bootstrap action script in the snowplow config file\n:emr:\n  ...\n  :bootstrap:\n    - s3://viadeo-snowplow-processing/bootstrap-actions/snowplow-emr-customize-heap-size.sh\nSimple and non intrusive - works well for us.\n. Nice move !\n. Hey @alexanderdean, that totally makes sens. I missed that point. \nAgreed, a kind of whitelist and/or blacklist filters seems the solution.\nWe will be proactive on that point, it's important for us. I'll try to propose you something.\nAny rough thoughts are welcome.\n. ",
    "kala725": "@alexanderdean : My basic question was that only, How to define that which event to shred, as a configuration option?\n. Thanks Fred. I will make the necessary changes and submit it again. Thanks again for the prompt reply. @fblundun \n. Sure Fred .\n. Thanks @alexanderdean for the feedback. I am fine with the first point and will make the changes as suggested, However about second point, my use case was a bit different. I want to store it in atomic.events table as an unstruct event and didn't wanted to create individual tables for some specific event types, So that I can do my analysis in a fast manner (user level targetting in a specific manner, my algo requires it.), also can do my research on some event types (which I want to shred) in more detailed fashion.\n. @alexanderdean , just a question. My colleage  @sambhavsharma contributed to snowplow. Can you tell, which CLA he signed, individual or corporate? \nThe pr was https://github.com/snowplow/referer-parser/pull/109\n. @alexanderdean : Made the changes suggested by you of taking the excluded entries from config. Updating the CLA.\n. @alexanderdean : Got it. A small help, Can you tell, how can I get it from predefined functions , if any . I am getting the complete s3_objectpath. Can I get the schema with some already defined function, or I have to write it.\nOne more thing to mention. I am not creating the json-schema in my current scenerio.\n. Thanks @fblundun @alexanderdean . However I did in a different fashion. Please review my last commit. \n. Hi @fblundun @alexanderdean , Just wanted to know that the code is good to go or requires modification ??\n. @fblundun @alexanderdean Thanks for the suggestion. Missed the backward compatibility completely. Now added that fix too. Also signed the CLA .\n. @alexanderdean : Is there anything needs to be done in this pr, or it is ready to merge.\n. Hey @alexanderdean, Actually my requirements are like this. Suppose  a case where I want filtered data from atomic.events to be indexded in solr/elastic search (Note here, I want filtered events to be indexed as per my requirement), Along with this, I want my analytics engine, to show/create reports for the all the data logged.\nSo in the above scenerio, which I am facing now (Hope others will reach this problem soon), How we can handle this.\nI thought of the above solution, where I can skip entry in the individual table (as not required) for some event types and also store them in atomic.events, which will lead to me a right way to go ahead with (after this , I will go ahead with the solr implementation plan).\nAlong with this, I am also going through other possible combinations. Wish to listen your opinions on the same.\n. ",
    "ngocthanhit": "Hi all, \nI am facing with this problem : \nBefore merging, remember that the contents of HDFS [1] [2] is not cleared out between \"runs\" in a persistent cluster, so you could easily end up reprocessing the same data...\n[1] hdfs:///local/snowplow/enriched-events/\n[2] hdfs:///local/snowplow/raw-events/\nSo the next running was  re-processing the old data. Did any one solve this issue ? \nThanks for read and help. \n. Thanks so much @alexanderdean. I need to implement those code now to use in my existing cluster. \n. Yes sure i will :+1: \n. Can you send me the file which process for  after enrich and shred, so i will try to add the setup for delelete everything from hdfs? I am too new with scala so still investigating your code now :) \nThanks\n. @alexanderdean  I can run with an existing cluster using feature/existing-emr branch and add the customer jar step for empty the HDFS path. So will i create an PR for your to process ? \nThanh/ \n. ",
    "nesbtesh": "@alexanderdean  How do you determine which event is from a bot?. ",
    "gisripa": "@alexanderdean  Didn't know about that. I just did.  Thanks\n. @alexanderdean  The section 4 indeed helped to figure it out. But it would be better to mention that jsonpath files needs to be uploaded even for snowplow authored jsons. \n. Hi @alexanderdean  When i leave the following configuration empty. The storage-loader fails. Didn't capture the error but says something like \"snow plow Contract violation for argument 2 of 2:.... jsonpaths expected String but nil \" \n :buckets:\n    :jsonpath_assets: \nI'll try to capture it and paste in a while. \n. Once I specify :jsonpath_assets:  relative to my s3 bucket. It works fine.  I guess #958 should be fine to track. \n. ",
    "OAGr": "Done.\nOn Tue, Aug 12, 2014 at 3:57 AM, Alexander Dean notifications@github.com\nwrote:\n\nThanks @OAGr https://github.com/OAGr ! You are right, those links were\nbroken. Can you sign our CLA if you haven't done so already?\nhttps://github.com/snowplow/snowplow/wiki/CLA\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/957#issuecomment-51899791.\n. \n",
    "benhoyt": "Just a plug for this: it'd be great to add a dvce_category field alongside the other dvce_* fields that's one of \"desktop\", \"mobile\", \"tablet\". We do this kind of query via GA all the time. As it is we plan to overload the app_id field and do things like \"site_desktop\", \"site_mobile\", \"site_tablet\".\n. Thanks! Yeah, that kind of thing would be great.\n. ",
    "gkushida": "Definitely willing to give it a shot. However I'm new to developing on Snowplow and not very familiar with Scala at all, so this may not be a quick-fix.\n. Absolutely - thanks for the fix. I'll get to this as soon as I can!\nOn Fri, Sep 19, 2014 at 12:55 PM, Alexander Dean notifications@github.com\nwrote:\n\nHey Grant @gkushida https://github.com/gkushida - I think I have a\nsolution for this. Can you test it out in a dev env?\nIt's here:\nhttp://d2io1hx8u877l0.cloudfront.net/2-collectors/clojure-collector/clojure-collector-0.8.0-standalone.war\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/970#issuecomment-56226349.\n. I tested a variety of combinations here - single-instance vs. load-balanced EB environments, calls directly to the environment URL vs. calls via CloudFront, and versions 0.6.x and 0.8.0. Note that 0.6.2 below is our version that includes the CloudWatch daemon but is otherwise identical to 0.6.0.\n\nIn summary, this fix seems to work for single-instance EB environments, but not for load-balanced environments:\n| version | environment type | call type | result |\n| --- | --- | --- | --- |\n| 0.6.x | single-instance | direct | broken (empty ip) |\n| 0.6.x | load-balanced | direct | WORKING |\n| 0.6.x | load-balanced | cloudfront | broken (multiple IPs logged) |\n| 0.8.0 | single-instance | direct | WORKING |\n| 0.8.0 | load-balanced | direct | BROKEN (load-balancer IP) |\n| 0.8.0 | load-balanced | cloudfront | BROKEN (load-balancer IP) |\nDetails:\nEB Load-Balanced ENV - direct calls to ELB URL\n0.6.x / load-balanced env / direct-to-load-balancer: WORKING!\napache: 172.31.elb.elb (76.217.me.me, 172.31.elb.elb) - - [20/Sep/2014:19:21:33 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"MY_USER_AGENT\"\ntomcat: 2014-09-20 19:21:33 - 37 76.217.me.me GET 172.31.elb.elb /i 200 - MY_USER_AGENT -&cv=clj-0.6.2-tom-0.0.4&nuid=69a3b5e8-1411-4bdb-a163-96534a0d147c\n0.8.0 / load-balanced env / direct-to-load-balancer: BROKEN (load-balancer IP)\napache: 172.31.elb.elb (76.217.me.me 172.31.elb.elb) - - [20/Sep/2014:19:08:37 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"MY_USER_AGENT\"\ntomcat: 2014-09-20 19:08:37 - 37 172.31.elb.elb GET 172.31.elb.elb /i 200 - MY_USER_AGENT &cv=clj-0.8.0-tom-0.1.0&nuid=69a3b5e8-1411-4bdb-a163-96534a0d147c\nEB Load-Balanced ENV - calls via CloudFront CDN\n0.6.x / load-balanced env / via cloudfront: BROKEN (multiple IPs)\napache: 172.31.elb.elb (76.217.me.me, 54.239.cf.cf, 172.31.elb.elb) - - [20/Sep/2014:19:21:19 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"MY_USER_AGENT\"\ntomcat: 2014-09-20 19:21:19 - 37 76.217.me.me, 54.239.cf.cf GET 172.31.elb.elb /i 200 - MY_USER_AGENT -&cv=clj-0.6.2-tom-0.0.4&nuid=8041f041-6520-4868-b578-8172f4812329\n0.8.0 / load-balanced env / via cloudfront: BROKEN (load-balancer IP)\napache: 172.31.elb.elb (76.217.me.me, 54.239.cf.cf, 172.31.elb.elb) - - [20/Sep/2014:19:10:27 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"MY_USER_AGENT\"\ntomcat: 2014-09-20 19:10:27 - 37 172.31.elb.elb GET 172.31.elb.elb /i 200 - MY_USER_AGENT &cv=clj-0.8.0-tom-0.1.0&nuid=917281ba-2e3f-4509-b0ed-8ac2fe237473\nEB Single-Instance ENV - direct calls to ELB URL\n0.6.x / single-instance env / direct-to-host: BROKEN (empty IP)\napache: 76.217.me.me (76.217.me.me) - - [20/Sep/2014:19:32:21 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"MY_USER_AGENT\"\ntomcat: 2014-09-20 19:32:21 - 37 - GET 76.217.me.me /i 200 - MY_USER_AGENT -&cv=clj-0.6.2-tom-0.0.4&nuid=dc332df0-8758-4c0b-bfd8-caddf4642a3c\n0.8.0 / single-instance env / direct-to-host: WORKING!\napache: 76.217.me.me (76.217.me.me) - - [20/Sep/2014:19:35:20 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"MY_USER_AGENT\"\ntomcat: 2014-09-20 19:35:27 - 37 76.217.me.me GET 76.217.me.me /i 200 - MY_USER_AGENT &cv=clj-0.8.0-tom-0.1.0&nuid=dc332df0-8758-4c0b-bfd8-caddf4642a3c\n. Looks like it works! I tested against CloudFront and directly against ELB load-balancers (did not bother to test a single-instance setup this time). In both cases the 0.8.0 collector writes the correct IP to the Tomcat log.\nThanks!\nBASELINE - 0.6.x\nCloudfront: curl http://.cloudfront.net/i\napache   172.31.elb.elb (64.60.me.me, 216.137.cf.cf, 172.31.elb.elb) - - [06/Oct/2014:16:43:14 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"curl/7.30.0\u201d\ntomcat    2014-10-06     16:43:14     -     37     64.60.me.me, 216.137.cf.cf     GET     172.31.elb.elb     /i     200     -     curl%2F7.30.0     -&cv=clj-0.6.2-tom-0.0.4&nuid=c439ecb3-35b4-4cc4-aa8d-11146616ebe9     -     -     -\nDirect: curl http://.elasticbeanstalk.com/i\napache   172.31.elb.elb (64.60.me.me, 172.31.elb.elb) - - [06/Oct/2014:16:44:31 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"curl/7.30.0\"\ntomcat    2014-10-06     16:44:31     -     37     64.60.me.me    GET     172.31.elb.elb     /i     200     -     curl%2F7.30.0     -&cv=clj-0.6.2-tom-0.0.4&nuid=57fe8507-80d2-4ea7-b1b8-be38bae292b1     -     -     -\nCANDIDATE - 0.8\nCloudfront: curl http://.cloudfront.net/i\napache   172.31.elb.elb (64.60.me.me, 216.137.cf.cf, 172.31.elb.elb) - - [06/Oct/2014:16:50:17 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"curl/7.30.0\u201d\ntomcat    2014-10-06     16:50:17     -     37     64.60.me.me     GET     172.31.elb.elb     /i     200     -     curl%2F7.30.0     &cv=clj-0.8.0-tom-0.1.0&nuid=33040f8a-08b8-4144-b39e-00cb34863e28     -     -     -     -     -\nDirect: curl http://.elasticbeanstalk.com/i\napache   172.31.elb.elb (64.60.me.me, 172.31.elb.elb) - - [06/Oct/2014:16:51:00 +0000] \"GET /i HTTP/1.1\" 200 37 \"-\" \"curl/7.30.0\u201d\ntomcat    2014-10-06     16:51:00     -     37     64.60.me.me     GET     172.31.elb.elb     /i     200     -     curl%2F7.30.0     &cv=clj-0.8.0-tom-0.1.0&nuid=ac02fafc-cef9-48fa-a8be-adf90fd6e140     -     -     -     -     -\n. ",
    "mortenstarfly": "Hi @alexanderdean - works for me.\nI'm on a single-instance Beanstalk. Also non-VPC.\nThanks!\n. Hi @alexanderdean \nI started to see IP's like 2001:981:b56d:2:1115:beb4:e3a8:cb92 in my log files once in a while. I breaks the storage loading process:\nCOMMIT;: ERROR:  value too long for type character varying(19)\nCONTEXT:  COPY events, line 249, column user_ipaddress: \"2001:981:b56d:2:1115:beb4:e3a8:cb92\"\nAny good ideas? \n. Hi @alexanderdean,\nI just saw that the problem is not entirely solved for me. I still see few internal IP's (~5 pr. day out of 100K events).\nHere's a sample from postgres:\nbb80ea89-893b-4bd9-a0d0-9f05220fd436    web 2014-11-10 03:01:12.503 2014-11-10 00:48:47 2014-11-10 00:48:51.064 page_view   127fb525-86b5-4bea-8131-0388b495598e    678396  cf  js-2.0.0    clj-0.8.0-tom-0.1.0 hadoop-0.7.0-common-0.6.0   195032  172.31.24.245   574101544   76a01d2e7a1d60d2    1   6ef85e81-66a3-4d2a-8641-38b00a84613a                                                http://graduateland.com/    Internships and Graduate Jobs - Graduateland        http    graduateland.com    80  /                                                                                                                                                                       Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.2; WOW64; Trident/7.0; .NET4.0E; .NET4.0C; .NET CLR 3.5.30729; .NET CLR 2.0.50727; .NET CLR 3.0.30729)  Internet Explorer   Internet Explorer       Browser TRIDENT en-GB   FALSE   TRUE    TRUE    FALSE   FALSE   FALSE   FALSE   FALSE   FALSE   TRUE    24  986 646 Windows Windows Microsoft Corporation   Europe/Berlin   Computer    FALSE   1024    768 utf-8   978 4373\nHere's the raw data for same event:\n2014-11-10  00:48:47    -   37  172.31.24.245   GET 172.31.24.245   /i  200 http://graduateland.com/    Mozilla%2F4.0+%28compatible%3B+MSIE+7.0%3B+Windows+NT+6.2%3B+WOW64%3B+Trident%2F7.0%3B+.NET4.0E%3B+.NET4.0C%3B+.NET+CLR+3.5.30729%3B+.NET+CLR+2.0.50727%3B+.NET+CLR+3.0.30729%29    e=pv&page=Internships%20and%20Graduate%20Jobs%20-%20Graduateland&dtm=1415580531064&tid=678396&vp=986x646&ds=978x4373&vid=1&duid=76a01d2e7a1d60d2&p=web&tv=js-2.0.0&fp=574101544&aid=bb80ea89-893b-4bd9-a0d0-9f05220fd436&lang=en-GB&cs=utf-8&tz=Europe%2FBerlin&tna=cf&f_pdf=0&f_qt=0&f_realp=0&f_wma=0&f_dir=0&f_fla=1&f_java=1&f_gears=0&f_ag=0&res=1024x768&cd=24&cookie=1&url=http%3A%2F%2Fgraduateland.com%2F&cv=clj-0.8.0-tom-0.1.0&nuid=6ef85e81-66a3-4d2a-8641-38b00a84613a -   -   -   -   -\nIt's not a big issue for me but just felt like reporting this.\n. ",
    "ilyakava": "@alexanderdean thanks! Just signed the individual CLA.\n. p.s. I also discovered the hard way today that the schema I have here is only guaranteed to be compatible with snowplow-common-enrich % \"0.2.0\" (I tried 0.4.0 here https://github.com/snowplow/snowplow/pull/1041)\n. as hinted in the error trace above, the error comes from here\nI've narrowed it down to being an overflow on scalazon's end - which itself uses the aws library. (confirmed ByteBuffer.wrap is working fine)\nAm now working on a fix that keeps track of how many records have been put and then closes and reconnects to the Kinesis stream.\n. I looked in the kinesis collector and saw that there was a new KinesisSink made on each request. This is in contrast to a single KinesisSink in the whole lifetime of scala enricher's infinite loop.\nI decided to make a new KinesisSink every several hundred puts to kinesis. Unfortunately, the error came back in (basically) the same form. FWIW, after running this many times, I noticed that the error occasionally comes after the 900th put to kinesis. But it is always either after the 1000th or the 900th put on the output kinesis stream...\nThe fault definitely includes scalazon:\n- if I do not create new instances of KinesisSink, the line in scalazon that puts the record gets a java.net.SocketException: Invalid argument\n- if I do create new instances of KinseisSink, the line in scalazon that lists available streams (used in the initialization of KinesisSink in snowplow) gets a java.net.SocketException: Invalid argument\nMisc notes:\nI didn't see any ability to refresh the connection in scalazon, and deleting and recreating the Kinesis stream don't help the problem (plus I realize now that that is not even a viable solution - since some snowplow storage app will need to read off of it at a later time)\n. For reference, the two varieties of errors (that I mention in my previous comment) are thrown after connecting to kinesis many times:\n- on list\n- on put\nIf you diff the two, the only two differences are:\nList\nat com.amazonaws.services.kinesis.AmazonKinesisClient.listStreams(AmazonKinesisClient.java:911)\n    at io.github.cloudify.scala.aws.kinesis.ClientImpl$$anonfun$execute$6.run$1(Client.scala:106)\n    at io.github.cloudify.scala.aws.kinesis.ClientImpl$$anonfun$execute$6.apply(Client.scala:116)\n    at io.github.cloudify.scala.aws.kinesis.ClientImpl$$anonfun$execute$6.apply(Client.scala:100)\nPut\nat com.amazonaws.services.kinesis.AmazonKinesisClient.putRecord(AmazonKinesisClient.java:491)\n    at io.github.cloudify.scala.aws.kinesis.ClientImpl$$anonfun$execute$7.apply(Client.scala:126)\n    at io.github.cloudify.scala.aws.kinesis.ClientImpl$$anonfun$execute$7.apply(Client.scala:119)\nAt this point, since the problem might be isolated to scalazon/amazon-kinesis-client, this issue should perhaps migrate to the scalazon/amazon-kinesis-client project. I'll write a little code to see if I can get the same errors without a project related to snowplow...\n. This looks related, so I made a post just in case: https://github.com/aws/aws-sdk-java/issues/239\n. as suggested in the aws issue: aws/aws-sdk-java#239 , the particular error traces I posted so far have to do with the number of opened BufferedInputStream inputs. I ran into the problem on OS X locally, and on heroku's ubuntu instances, both on java 1.7.0_55. Locally, I upgraded to 1.7.0_67, and this particular error went away. I was able to process several thousand more records in a consecutively than before, until...\nafter 10 minutes, com.amazonaws.http.AmazonHttpClient closed the connection full error here. This was despite me creating new instances of KinesisSink every several 100 puts, (which itself calls scalazon to create a new instance of the aws kinesis client)...\nPerhaps both these errors speak to problem's w/ the infinite worker loop without any shutdown/restart/recovery abilities. Next I'm going to look at the specifics of kinesis-redshift-sink since it shares the similarity of a single worker loop accessing kinesis, unlike scala-stream-collector\n. kk, resolved this...\nI refactored the snowplow method that puts to kinesis to use only aws-sdk-java to make the put, excluding this and this line from scalazon, just because they seemed unnecessary. After doing so, I got a new error trace, full here, starting with:\njava.io.FileNotFoundException: GeoLiteCity.dat (Too many open files)\nThis then led me to see that a new file path was being instantiated for each processed record. I also at that point found this PR. Indeed, just moving the new IpGeo val out of the enrichEvent method like done in the PR solved all my problems. In the end, all the (seemingly unrelated) previous errors I was having went away. I've been able to process 50k events consecutively since this change so I can confidently say my issue is closed.\nThanks for the help!\n. ~~nvm, checked again in production, and this update didn't change how the \"mob\" platform was ignored in the enrichment app. Not sure why since the bump to 0.4.0 was on Apr 29, 2014 while https://github.com/snowplow/snowplow/pull/524 was merged Apr 11, 2014~~\nBumping the scala common enrich here changed the schema for the event string (ordering and presence of fields) I am using for a later storage step here: https://github.com/snowplow/snowplow/pull/996 and broke my pipeline. I'm going to punt on this instead and just go back to using the web platform since that works fine\n. Why was this kept when it was moved from here?\nShouldn't this match the schema in scala-common-enrich?\n. These are the all the tab separated fields visible in the kinesis events steam after a vanilla deployment of the scala-enrich-kinesis app\n. The change is that MyS3Emitter is extended instead (which contains the newline addition)\n. The only changes here is the new line addition\n. @bugant if bufferMillisecondsLimit is in the properties files and is set, will it also effect how often the local buffer is purged? Nothing in your code seems to preclude this possibility from my perspective, but somehow bufferMillisecondsLimit doesn't seem to be effecting the purging for me in production, so I had to ask just in case...\n. ",
    "lekki": "@alexanderdean  thanks for the update, any idea when you're planning to release a fix?\n. ",
    "michaelpryor": "It's missing some other code too (that causes errors).  I think that whole example block should just be copied from http://snowplowanalytics.com/blog/2014/07/03/snowplow-javascript-tracker-2.0.0-released/\n. @fblundun is this the preferred method for submitting wiki edits or is there a way to do it as a pull request?\n. ",
    "danisola": "Hi Alex, I might be missing something, but if they should be encoded twice why then the colon and slashes of http:// aren't?\nI've also got some lines from the out-archive bucket that have gone through the storage phase, and they don't seem to be encoded twice either. Here's an example:\n2014-10-08  10:59:02    LHR5    474 195.195.248.230 GET d1f6ajd7ltcrsx.cloudfront.net   /i  200 https://www.simplybusiness.co.uk/welcome/insurance/hair-salon/?adb_id=1072775&ref_id=google_InsuranceShop_Salon_1072775&gclid=CM_-z6DznMECFQLJtAodJw0AK Mozilla/5.0%2520(compatible;%2520MSIE%25209.0;%2520Windows%2520NT%25206.1;%2520Trident/5.0) e=pv&page=Hair%2520%2526%2520beauty%2520salon%2520insurance%2520-%2520Simply%2520Business&dtm=1412765878378&tid=062498&vp=1680x955&ds=1680x2336&vid=1&duid=406908fd3e993c26&p=web&tv=js-2.0.1&fp=2571446246&lang=en-gb&cs=utf-8&tz=Europe%252FLondon&refr=http%253A%252F%252Fwww.google.co.uk%252Faclk%253Fsa%253Dl%2526ai%253DC1Q-I8hg1VKLyA-H57Qaxz4CYBrSF4dQF3OL_lCey_oaCxgEIABABKANQ9qS0igJgu46wg9AKoAHbxq7_A8gBAakCGvgP8Sj4uj6qBCdP0HignHsBB61p6t7SXiS9wkg9K7OR-XDrtK61V05GbmohcjoB43mAB425UZAHA6gHpr4bohNICjhABVIICgYSBAgAEAFoq-zfzYYNciMSIRDUq7HIBSACKAE4BECwktcGWAFo_v__________AYABARICCCwaCAoGZ29vZ2xl%2526sig%253DAOD64_0LLsKx85TYqxayZa7d-j8AasKZpw%2526rct%253Dj%2526frm%253D1%2526q%253D%2526ved%253D0CCEQ0Qw%2526adurl%253Dhttps%253A%252F%252Fwww.simplybusiness.co.uk%252Fwelcome%252Finsurance%252Fhair-salon%252F%25253Fadb_id%25253D1072775%252526ref_id%25253Dgoogle_InsuranceShop_Salon_1072775&f_java=1&res=1680x1050&cd=24&cookie=1&url=https%253A%252F%252Fwww.simplybusiness.co.uk%252Fwelcome%252Finsurance%252Fhair-salon%252F%253Fadb_id%253D1072775%2526ref_id%253Dgoogle_InsuranceShop_Salon_1072775%2526gclid%253DCM_-z6DznMECFQLJtAodJw0AKw    -   Hit 4SR2ztwifnME8XyQ4DF_ia4r2fhTHaG08-h5jJ0uCWCX2cKqEdVHzw==    d1f6ajd7ltcrsx.cloudfront.net   https   1463    0.001\nThanks!\n. Sorry for that, wrong repo...\n. Thanks Alex, I've added myself to the CLA!\n. Hi @fblundun, at Simply Business we successfully use GZIP. We didn't test out the LZO compression though... :-/\n. Thanks for Snowplow! ;)\n. Hi Alex. We did it this way because occasionally (2-3 times a month) we have S3DistCp jobs getting stuck. That's the syslog of the last SD3DistCp job that hung:\n2015-02-19 15:06:55,840 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Running with args: [Ljava.lang.String;@471719b6\n2015-02-19 15:07:00,511 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Using output path 'hdfs:/tmp/0deac8e1-16fe-4756-b3a1-b4a9e77a4c1f/output'\n2015-02-19 15:07:00,577 INFO com.amazon.elasticmapreduce.s3distcp.FileInfoListing (main): Opening new file: hdfs:/tmp/0deac8e1-16fe-4756-b3a1-b4a9e77a4c1f/files/1\n2015-02-19 15:07:00,736 INFO com.amazon.elasticmapreduce.s3distcp.S3DistCp (main): Created 1 files to copy 18 files \n2015-02-19 15:07:01,242 INFO org.apache.hadoop.mapred.JobClient (main): Default number of map tasks: null\n2015-02-19 15:07:01,243 INFO org.apache.hadoop.mapred.JobClient (main): Setting default number of map tasks based on cluster size to : 8\n2015-02-19 15:07:01,243 INFO org.apache.hadoop.mapred.JobClient (main): Default number of reduce tasks: 3\n2015-02-19 15:07:01,386 INFO org.apache.hadoop.security.ShellBasedUnixGroupsMapping (main): add hadoop to shell userGroupsCache\n2015-02-19 15:07:01,391 INFO org.apache.hadoop.mapred.JobClient (main): Setting group to hadoop\n2015-02-19 15:07:01,431 INFO org.apache.hadoop.mapred.FileInputFormat (main): Total input paths to process : 1\n2015-02-19 15:07:02,661 INFO org.apache.hadoop.mapred.JobClient (main): Running job: job_201502191448_0007\n2015-02-19 15:07:03,664 INFO org.apache.hadoop.mapred.JobClient (main):  map 0% reduce 0%\nIn my mind, the old approach and the PR approach aren't that different: in both cases the engineer will have to check what's going on if the data is not present when she/he wakes up.\n. No, we process our logs hourly and we haven't seen any spikes that make the processing time go over the threshold. In our experience, the processing time is much more dependent on the number of files (that is, days) than on the size of the files.\n. > Therefore, our plan is to adapt your PR to remove the jobflow-killing aspects and add in the check that a jobflow failure is a bootstrap failure before attempting a restart.\nIs fine. We'll keep the change it in our private repo as it's a big time saver for us.\n\nHave you filed a bug with AWS about the S3DistCp hangs?\n\nWe haven't, our experience with AWS is that every time we've complained about transient EMR errors they just tell us to retry the job. \n. Hey Alex,\nJust wanted to comment that we've come across another exception when starting the cluster that you might consider to catch and retry:\nRestClient::HTTPVersionNotSupported (505 HTTP Version Not Supported):\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/abstract_response.rb:48:in `return!'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:230:in `process_result'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:178:in `block in transmit'\n    /home/ec2-user/.rbenv/versions/1.9.3-p547/lib/ruby/1.9.1/net/http.rb:746:in `start'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:172:in `transmit'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:64:in `execute'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient/request.rb:33:in `execute'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/rest-client-1.6.7/lib/restclient.rb:72:in `post'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/elasticity-3.0.4/lib/elasticity/aws_request.rb:27:in `submit'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/elasticity-3.0.4/lib/elasticity/emr.rb:191:in `run_job_flow'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/elasticity-3.0.4/lib/elasticity/job_flow.rb:131:in `run'\n    /home/ec2-user/snowplow/releases/20150226101732/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:297:in `run'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/contracts-0.4/lib/contracts.rb:230:in `call'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/contracts-0.4/lib/contracts.rb:230:in `call_with'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/contracts-0.4/lib/decorators.rb:157:in `run'\n    /home/ec2-user/snowplow/releases/20150226101732/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:72:in `run_emr'\n    /home/ec2-user/snowplow/releases/20150226101732/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:56:in `run'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/contracts-0.4/lib/contracts.rb:230:in `call'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/contracts-0.4/lib/contracts.rb:230:in `call_with'\n    /home/ec2-user/snowplow/shared/bundle_emr-etl-runner/ruby/1.9.1/gems/contracts-0.4/lib/decorators.rb:157:in `run'\n    /home/ec2-user/snowplow/current/3-enrich/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in `<main>'\nRetrying the job worked well, so it seems to be a transient AWS error.\n. On job submission: https://github.com/snowplow/snowplow/blob/master/3-enrich%2Femr-etl-runner%2Flib%2Fsnowplow-emr-etl-runner%2Femr_job.rb#L288\n. Yes, having multiple actions would make it more general. I'll try to get some more time to do it. \n. BTW, notice that this change breaks quite a few tests Scala Hadoop Enrich, because they have custom contexts that don't have a schema. An example is CollectorPayload1LzoSpec, which has the custom context {\"page\":{\"url\":\"blog\"}}.\nIs it just a matter to fix those tests? Or are these non-schemed contexts valid?\n. Should I continue working on this branch for the other tickets? Or do you want me to open new PR that branch from this one?\n. The tests in Scala Hadoop Enrich fail, but before fixing them (there are a few of them...!) I wanted to push https://github.com/snowplow/snowplow/commit/8d809b6d5c746a5cecf412f5564a3cf6ef12efbd to make sure the new fields are in the correct position in EnrichedEvent.\n. I've moved the fields to the end of EnrichedEvent. Now the tests in Hadoop Enrich pass, but it's just because all the fields after 'document' are not being tested (currency, geolocation, etc)\n. Regarding https://github.com/snowplow/snowplow/issues/1801 and https://github.com/snowplow/snowplow/issues/1802, what are the migration script versions I should work on? Are they 0.5.0 to 0.6.0 for Postgres and 0.6.0 to 0.7.0 for Redshift?\n. I think I've finished with these tickets! Let me know if I've missed or you want me to change anything.\n. Thank you for the direction! :+1: \n. Without the new the compiler interprets that we want to use BadRow#apply, which uses Strings instead of ProcessingMessages.\nI just added the new because it was the smallest change to make the code compile. Otherwise I could create a new BadRow#apply that uses ProcessingMessages or I could both._2.map(_.toString).\n. ",
    "lubasch": "That's really great!\n. We can add that it's really worth to wait up to 16-24 hours for full data being available in adwords via API. Otherwise your numbers will never match for the later part of the day.\nCheers,\nChristian\n. We can add that it's really worth to wait up to 16-24 hours for full data being available in adwords via API. Otherwise your numbers will never match for the later part of the day.\nCheers,\nChristian\n. We decided to not touch the SP pipeline with this case, but import data straight into the db (Redshift). Kept everything a bit more separated at that times.\n. ",
    "dominickendrick": "Ah great, I look forward to the new version.\n. ah great, I'll close this pull request\n. ",
    "jbeemster": "I did copy it from the example outputs on their webhooks page so maybe they send through more for actual events.\n. Turns out with this one that Mandrill if failing to authenticate with a HEAD request will attempt with a POST.  So not a huge issue as none of the webhooks depend on this.\n. Sorry I should have let you know sooner! \n. All failing indexes, this is for a situation where we have gotten a valid List of jsons and we are trying to convert them to Validated RawEvents.\n. Yep no worries, I will move it now.\n. Sorry about this! Will fix that up in a branch now.\n. @alexanderdean Does this job need to go into the changelog?\n. Commit Ref: https://github.com/snowplow/snowplow/commit/9452b07bbb0b3c8e45086ea34245ea74487069ba\n. Commit Ref: https://github.com/snowplow/snowplow/commit/9452b07bbb0b3c8e45086ea34245ea74487069ba\n. We should be now yes, have not pushed the formatting to the other apps yet.\n. Hey @fblundun did we come to a conclusion on whether or not the badrow timestamp should be formatted properly.  I added formatting to the stream collector but not sure anymore if this had to be the case for everywhere?\n. Thanks Alex will make sure that that is the case.\n. Closing as it has been migrated.\n. The actual CloudWatch trigger gets included with the event so we should be able to determine what the event is about and why it happened.  But yes there will be a fair amount of interpretation..\n\"Trigger\": {\n      \"MetricName\": \"CPUUtilization\",\n      \"Namespace\": \"AWS/EC2\",   <- Namespace limits what metrics and dimensions are available\n      \"Statistic\": \"AVERAGE\",\n      \"Unit\": \"Percent\",\n      \"Dimensions\": [\n        {\n          \"name\": \"AutoScalingGroupName\", <- Tells us what dimensions this is for\n          \"value\": \"snowplow-autoscaling-group-name\"\n        }\n      ],\n      \"Period\": 60,\n      \"EvaluationPeriods\": 5,\n      \"ComparisonOperator\": \"LessThanThreshold\",\n      \"Threshold\": 60\n    }\nI think we will need to include quite extensive dictionaries of the available AWS dimensions and namespaces but it should be possible to figure out what service each event correlates to!\n. There is something else fun with Amazon SNS however.  We will need to have code that confirms that we want to Subscribe to the SNS Topic before it will publish anything.  On SNS Topic creation we will need to provide an endpoint, the POST will contain a SubscribeURL which just needs to be pinged to set it up.  \nAs the endpoint should be a Snowplow Collector will we be okay to embed this setup step somewhere in the Pipeline?\n. The subscribe URI is something that is generated for you on SNS Topic creation, it is included in a POST to whatever endpoint you specify.  Which could be: http://collector-acme/com.amazon/sns.  \nI am not sure I understand what you mean by notification URI?\n. I will see if we can change the endpoint after authenticating, that could be neater.\n. We have to use the Collector URI for both unfortunately.  There is no changing after authentication as far as I can see.\n. So I guess the question now is if we want to handle authorising the Subscription within common-enrich or if we want to try to do it earlier...\n. That could work as well! \n. It really is!\n. Hey @fblundun with this ticket where is the best place to be checking for the null value?  And are we only checking for null or also for empty strings as well?\nI see that the IP is being converted to a string here: https://github.com/snowplow/snowplow/blob/release/r67/2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors/scalastream/CollectorService.scala#L89\n...and if it were a null value it would no doubt throw a NPE.  Should it then instead be passing the ip through untouched and then processing it in the ResponseHandler?\n. I like the toOption option.  Is unknown an okay identifier for these cases?\n. We can add entropy but wouldn't it just make it harder to identify how many events are actually coming through as unknown?\n. Oh right yes it would just be for the partition key.  Never mind previous comment!\n. Hey @fblundun do you mean logging the error just here:\nhttps://github.com/snowplow/snowplow/blob/release/r67/3-enrich/scala-kinesis-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich.kinesis/sources/AbstractSource.scala#L183-L185\nOr was there a different spot that you were thinking of logging at?\n. That makes sense!  Okay I will add it to the KinesisSink only.  Thanks for the clarification.\n. We did indeed.  I think with the timeout kill and reset setup we need not worry about this ticket.\n. Does this ticket require any work or are we happy to leave it up to the back-off timings?\n. My issues got confused with my milestones sorry!\n. Exactly!  I figure it is meant to be used for both.\n. Easy fix no worries!  I doubt it would ever really come up as most people would use IAM or ENV for anything production...\n. Ahh.  No worries then will close these tickets then.\n. I did it already, the commit is in the commit to bump common enrich to 0.15.0.  It is all updated.  I can revert that section if you want?\n. No worries shall do.\n. Hey @denismo yes this does indeed fix not UTC as well!  Instead of it being a raw timestamp it is now being processed using this formatter: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/enrichments/EventEnrichments.scala#L39\nSo it should always be redshift compatible and in UTC from now on.\n. I think this could become part of the sink quite happily.  Each sink would be attempting to make the same requests but only one would be able to successfully hit the API in time (this claim will involve some testing).  \nWhichever sink gets the 200 response will then be elected to:\n- Create the new index\n- Re-alias to the new index so that new data is sent to the correct place\n- Forcemerge the old index; this makes seek time much faster on the old index as there is only one segment and all marked for deletion files will be removed.\nWe could also add options to automatically backup to a specific target (S3 to begin with) and how many old indexes to keep around at a time.\n. Hi @christoph-buente, the idea behind creating elasticsearch-housekeeper rather than adding this functionality directly into the sink is that:\n1. This functionality already exists in Curator, which also has versions with compatibility for all versions of Elasticsearch - which we then do not need to support.\n   - It has support for daily indexing and re-aliasing already\n2. These features are also required for the batch loading where we allow for loading of bad rows into Elasticsearch from EMR.\n3. Putting a daily housekeeping process inside the sink makes the sink harder to test and reason about.\nWhat you are suggesting as it being as an external scheduled job can already be done with Curator.  You can also already point the sink at an alias rather than a specific index.  So we end up gaining very little by including this directly into the sink.  You would just need to ensure that your cluster was prepared prior to running the sink.\nAny missing functionality or anything that is specific to Snowplow can then be added on top of Curator in our app.\n. Hey @christoph-buente unfortunately the sink only knows about what you have pointed it to.  So when you change the alias it will start sending to the new index.\nYou might be able to mitigate these effects by either having longer indexes (6/12/24 hours) or by writing a script to halt the rotation until the Sink has caught up.  You could check the most recent record against the current time and only rotate at this point.. Ideas on how to recreate:\n- Run the collector without an ELB in front\n. It is yes @BenFradet nothing that can be done apart from maybe raising a helpful error stating that fact?. There is an issue open already: https://github.com/awslabs/amazon-kinesis-connectors/issues/52\n. Link: https://blog.statusgator.io/introducing-web-hooks/\n. In the first one we still seem to be referencing the 1.1.0 release not the 2.0.0 that was recently published.\n\nhttps://github.com/snowplow/snowplow/wiki/Create-a-new-application-in-Elastic-Beanstalk-and-upload-the-WAR-file-into-it#alternative-approach-aws-cli\n\nThe second one has a few issues still - namely that the whole reason for adding Lifecycle hooks is to allow automated downscaling:\n\nhttps://github.com/snowplow/snowplow/wiki/Troubleshooting-Clojure-Collector-instances-to-prevent-data-loss#automatic-downscaling-should-be-disabled\n\n@ihortom can you grab me this week and we can go through this together?. Done\n. Hi @0xABAB, this error normally occurs if the ansible playbooks have been manually edited/modified after you have first attempted to provision.\nA fix for this is to delete the vagrant/.peru, vagrant/ansible and vagrant/oss-playbooks directories and to then to run: vagrant halt && vagrant up && vagrant provision again.\nPlease let me know if this helps you get the environment running!\n. You can run the servlet locally following the documentation found here. However we do not have any local infrastructure for running this collector locally inside tomcat.\nFor a local collector you can also look at the Scala Stream Collector which can be run locally and can be configured to output events to stdout rather than to a Kinesis Stream.\n. Hi @0xABAB this error has been fixed in this commit (https://github.com/snowplow/ansible-playbooks/commit/5f9e17cf5e53a539aaf9068527a876899f5c13b5).\nThe mirror unfortunately stopped working.  Please follow the steps in this ticket to resolve (https://github.com/snowplow/snowplow/issues/2408)\n. Hi @ranjithnaidu, for debugging help with Trackers could you please post in the user forum: https://groups.google.com/forum/#!forum/snowplow-user\nWe will also need more information on the issue to help you out such as:\n- Code sample of your implementation\n- StackTrace of the exception\nIf you could post all of the above to the user group we will be able to help you get it working!\n. Please post this in the https://groups.google.com/forum/#!forum/snowplow-user group.\n. This has been split up and done.\n. We didn't reset when we renamed kinesis-s3.\n. We could add Consul to the Collectors to allow them to communicate with each other?\n. It might not be a bad idea still to implement something like RocksDB on the box just in case the instance does get restarted so we are not depending on the in-memory storage?\n. This was more for Trackers that have no way of holding onto the event, no SQLite or local storage to hold them in.  In the case of the pixel tracker it needs to be accepted by the collector or it will be lost forever.\nSo under extreme stress the collector would potentially refuse events that can be stored on their host device but accept events that cannot be kept.\n. Oddly according to this the extra CPU cores tends to degrade performance: https://ruin.io/2014/benchmarking-virtualbox-multiple-core-performance/\nThe most important change for better performance is using NFS, especially in terms of packaging.\n. This page needs to be updated to point to these functions:\nhttps://github.com/snowplow/snowplow/wiki/2-Specific-event-tracking-with-the-Javascript-tracker#enhanced-ecommerce\n. This has been updated, please review @yalisassoon \n. Closing this ticket in favour of PR\n. Hey @alexanderdean, this ticket could be solved quite quickly by either:\n1. Doing robust testing of the 0.7.0 release and checking whether the current HTTP client works for 2.x\n2. Building a second binary with an updated dependency on the Jest Elasticsearch client. (This is preferable as their 2.x library does not work necessarily work backwards)\n. Closing as duplicate of https://github.com/snowplow/snowplow/issues/2512\n. I believe this had to do with the bug we uncovered in common enrich to do with base64 decoding.  So this might have been fixed already... we can leave it open to do some more digging however.\n. @alexanderdean we can define our own query-string parameters on the webhook.  Rather than hiding the schema in the form could we not just keep it with the webhooks?\n. No worries!  We have sample Unbounce data now and we will assume that schema={{ schema_uri }} is in the querystring.\n. Hi @rgabo, is this referring to the Java Tracker?\n. Okay @rgabo no worries, I will move this ticket to the appropriate repo then!  Agreed that it should be moved to trace as it really doesn't need to be that low.\n. Done sorry for the delay.\n. Hey @miike, we are actually going to add some more functionality to this PR so I will be taking it over from here.  Thanks again for the PR!\n. You can go ahead and close the PR.\n. It doesn't look like it is crashing his application just adding logs.\n. Easiest way to check would be to run that application locally yourself potentially?\n. No worries, let me know when M3 is live and I will bump and build the Kinesis artifacts.\n. So 1.8.1?\n. Hey @alexanderdean this only seems to affect the Clojure Collector Loader - should I rename the ticket to make this a bit more explicit?. But a trailing slash in the url would denote an invalid path to a file wouldn't it?  Or is this to allow the downloading of a folder rather than an individual file?\n. Ahh this makes sense.  When you have a trailing slash this happens:\nhttp://snowplow-hosted-assets.s3.amazonaws.com/third-party/maxmind//GeoLiteCity.dat\nWhich is an invalid url.\n. @alexanderdean this Iglu Scala Client has not been updated yet, which milestone would you like me to move this into?  Or should we wait for it to be released?. Hey @christoph-buente the example hocon is correct as far as I can tell.  My strategy would be to grab a fresh copy of the example hocon and move your old values across.  If that still does not get it working please feel free to create a discourse topic on it so we can help you debug this!\nThe first issue that pops out though is that:\nsink {\n  source = 'kinesis'\n  sink {\n    \"good\": 'elasticsearch'\n  }\n}\nThis portion is missing a bad sink, this should look like:\nsink {\n  source = 'kinesis'\n  sink {\n    \"good\": 'elasticsearch'\n    \"bad\": 'kinesis' (or stderr)\n  }\n}\n. Hey @pkallos @alexanderdean, I would have to dig through the KCL but my first feeling would be that a thread should be able to consume multiple ShardIterators - will need to test this though.\nI will need to set some time to actually experiment with the sink to see how it behaves in practice.  This is really interesting insight into how the buffer settings should be set though if it is indeed the case.\n. Closing this PR in favour of three distinct tickets:\n- https://github.com/snowplow/snowplow/issues/2880\n- https://github.com/snowplow/snowplow/issues/2879\n- https://github.com/snowplow/snowplow/issues/2878\n. The transport client already has support for this and the http client will.  There is nothing extra to be done for support for this particular version.  Can this ticket be closed @alexanderdean ?\n. Closing as support is already included for this version...\n. @alexanderdean the testing for this has gone very well under load and into an AWS Elasticsearch Service.  Is there anything code-wise that needs to be done before release?  I will start making the blog post now.\n. Last check through..\n. Bumping this back as this should be supported by release-manager.\nhttps://github.com/snowplow/release-manager/issues/14\n. Don't we already have this by setting the cookie age to 0 days?  Or do we want a more explicit setting?\n. Hi @KevinNZ83, I have cherry-picked the updates for the Transport client as part of this ticket:\n- https://github.com/snowplow/snowplow/issues/2525\n. Hey @alexanderdean yes it is a pretty big change.  I imagine this has something to do with more performant parsing of nested JSONs in ES 2.x.  As this error was picked up for a fieldName in a deeply nested JSON Value for a key.\nIn any case the blog post will contain a big angry warning about this change!\n. Hi @simplesteph I will have a look into this.  My understanding was that these limits were applied above the actual emitter within the Kinesis Connectors Library which supplies the records to the emitter.\nWill do some investigation!\n. Hi @simplesteph that does sound like an easy fix.  Could you provide the point where you had to add this stackTrace printing?\n. Well I still think the idea of the error is there but I do agree that without the stacktrace it could make it quite difficult to debug!  I will schedule this into the next milestone.\n. Agreed!  All of those should be available from the HOCON.\n. This has already been done:\nhttps://github.com/snowplow/snowplow/commit/e43b4d30dee99db999dc49815322e10ad4768d88. Is there anything to be done with this ticket @alexanderdean as it looks like the region has already been added?. Hi @juanstiza which version of Stream Enrich are you using which is causing this error?\n. This is a regression error in the latest release.  We attempted to dynamically fix URIs with accidental double slashes but have inadvertantly broken the s3 support here.\nA hack around this is to change your URL to look like the following:\n\"geo\": {\n                \"database\": \"GeoIPCity.dat\",\n                \"uri\": \"s3:///my-private-bucket\"\n            },\n            \"isp\": {\n                \"database\": \"GeoIPISP.dat\",\n                \"uri\": \"s3:///my-private-bucket\"\n            },\n            \"organization\": {\n                \"database\": \"GeoIPOrg.dat\",\n                \"uri\": \"s3:///my-private-bucket\"\n            }\nThe triple slash will be reduced to double slashes and it will then parse correctly.\n. https://github.com/snowplow/snowplow/issues/2744\n. This line needs to become:\nval cleanUri = new java.net.URI(uriFilePair._1.toString.replaceAll(\"(?<!(http:|https:|s3:))//\", \"/\"))\n. That does sound like authentication!  If you cannot get that issue resolved please post to the Discourse thread so we can help you debug it.\n. Hi @redsquare the binary has been moved to: http://dl.bintray.com/snowplow/snowplow-generic/snowplow_scala_stream_collector_0.8.0.zip\nCould you please advise where you got this link from so I can update the corresponding documentation?\n. Fixed!  Thanks for pointing that out.\n. Looks like we are already using 1.3.0 which is the latest I can find from here (what we are currently using): https://clojars.org/org.clojars.timewarrior/ua-parser\nAlso found here:\nhttps://mvnrepository.com/artifact/ua_parser/ua-parser/1.3.0\nThe library appears to have been split into many new repositories and the new one to watch is:\nhttps://github.com/ua-parser/uap-java\nHowever looks like we are using the latest and greatest of this library so should we push this back?\ncc/ @alexanderdean . Currently will allow you to attach a single volume of type (gp2, io1 or standard) to each of the core nodes in the JobFlow.\nIf the type is \"io1\" you will need to specify the \"volume_iops\" yourself, for gp2 and standard this is done for you.\nWe are not currently checking that the values passed are valid:\n\nThe type of volume being assigned.\nThe size of the volume.\n\nShould these checks be done in Elasticity or in our own library?\ncc/ @alexanderdean . Checks now being done by us, will only accept valid volume types and numbers greater than zero for volume_iops and volume_size.. Will have a look this afternoon - will spin up a test pipeline to smoke test all of it.. Hi @amolbarewar - support questions to our forum please: http://discourse.snowplowanalytics.com/. @alexanderdean we will need to get a 6.0.10 of Elasticity to build emr-etl-runner.  Would you like me to build these artifacts with a local copy for testing purposes before we get these changes merged in?. Quick update on some testing I have done:\n\nUsing c4.large with EBS volume could not get past the first S3DistCp step\nUsing m3.xlarge with EBS volume completed successfully\nUsing m4.xlarge with EBS volume completed successfully. Please use our Discourse for support: http://discourse.snowplowanalytics.com/. Hi @NielsKSchjoedt as the repo is comprised of many different projects we do not necessarily work on every project per release cycle.  If you look in the changelog you will see that the last EmrEtlRunner release was R83 (https://github.com/snowplow/snowplow/blob/master/CHANGELOG#L75).\n\nIn Bintray you will then find a zip file called snowplow_emr_r83_bald_eagle.zip.  \nLink: https://bintray.com/snowplow/snowplow-generic/download_file?file_path=snowplow_emr_r83_bald_eagle.zip\nIn future please use our  Discourse forum for support: http://discourse.snowplowanalytics.com/. Thats exactly right @alexanderdean - we have the ability to manually kill the cluster as the last step irrespective of the steps.. Moving to Tracker repository - answer on linked ticket.. Moving to Tracker repository - answer on linked ticket.. At this point the 2.x sinks do work with a 5.x sink but it would be good to get this properly tested and to use the latests libs supporting this.. Ahh yes I meant a 5.x cluster.  We are actually using this in production now without any issues.. Using the transport or http client @gincor ?. This does not seem to work at all anymore ... it appears variable substitution is not working as expected anymore.. Meaning that even after rebooting it is still cached.  It is cached to disk rather than in-memory.. Agreed number 3 is cleanest. I think this one can likely be closed as DynamoDB now offers on-demand scaling now which is much more cost effective for the spiky traffic that we have to deal with.. Part of the issue is likely due to how empty checks are performed - the function lists all files in the bucket to fetch the length.  Non-empty check only need to assert that some non-zero byte files are available rather than listing everything in the bucket.\n\nhttps://github.com/snowplow-archive/sluice/blob/master/lib/sluice/storage/s3/s3.rb#L128\n\nExample of this implemented in Scala:\n\nhttps://github.com/snowplow-devops/operations-server/blob/master/2-servers/operations-server/src/main/scala/com.snowplowanalytics.devops/operationsserver/implementations/clients/AwsClient.scala#L186-L230. Or possibly --ignore-lock - so that it can continue in circumstances that you control?. I guess the other-side of this is if we think that this script will have any adverse impact on the \"normal\" setup using either a NAT Instance or with EMR being able to communicate with Redshift without having to traverse a NAT.\n\nIf it will not have any impact it is much easier to add it to the existing script which means less updating for us internally and ensuring that others that stumble on this issue won't have to figure out what extra script they need to add into their config to have things working again.. Fundamentally no one can use the Java 8 with Tomcat 8 version 3.0.2 machine image.  Image 3.0.1 and lower are all fine.. Sounds good to me!. Memory usage is way down so yes it is considerably lower!  I think there is however scope for optimisation and further reducing the memory footprint in the codebase.  Maybe we can rename this ticket to \"find further memory optimisations\"?. Looking at the code what we would need to add to support this:\n\nAt this line we check whether a JobFlow already exists - if it does we use Elasticity::JobFlow.from_jobflow_id('jobflow ID', 'region') rather than Elasticity::JobFlow.new\nWe could quite easily write detection logic to fetch out a JobFlow ID using the unique name\nWe would need to be able to change the steps \"action_on_failure\" parameter - looks like these are all hardcoded at the moment but could easily be edited as they have public access\nWe would need a \"down\" command to terminate the persistent cluster\n\nThis could work in one of two ways:\n\nWe specify the want to use a persistent cluster and EMR detects whether one exists already; if it does it uses this; if it cannot find one it creates a new one\nWe have a distinct \"up\" command as with dataflow-runner\n\nIn any case we would need a distinct command to terminate a persistent cluster so that no external logic would be needed to manage this system.\n. @chuwy @BenFradet I think I have addressed all the main feedback now if you could both have another pass?  Would be great to get this into an RC this week so we can try it out with an internal pipeline.. One concern I have with the persistence is a note by S3DistCp (https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html) - namely:\n\nIf S3DistCp is unable to copy some or all of the specified files, the cluster step fails and returns a non-zero error code. If this occurs, S3DistCp does not clean up partially copied files.\n\nShould I be adding a step conditionally to always clean out any partial copies from HDFS?. Hey @BenFradet - it might be a good idea to use a more machine readable name for sure.  Historically all of our chatbot actions have used these rather bespoke names without any issues though so I do not think it would be that much of an issue to be fair.\nAre you thinking about using a UUID or did you have something else in mind for the name?. @BenFradet could you link me to the steps required to get an RC of this available in Bintray so we can try it out?. Hey @BenFradet so after a lot of chatting with AWS the only extra addition I would like to make are some HDFS state maintenance points - namely clearing out HDFS before any new steps are submitted.\nThis would involve updating your hdfs rmr script to:\n\nUse the -skipTrash argument so deleted files are not just moved to trash but are actually deleted\nAdd the ability to check for the existence of a directory before deleting - this lets us run cleanup steps for dirs that might not exist. Okay @BenFradet this is now working well in production with a test candidate and a pretty decent amount of traffic.  The last issue as I see it is that HDFS usage does creep up over time and things like failed s3-dist-cp commands will leave a heap of data floating around in the temporary directories.\n\nAs a way of maintaining this state for the long term my thinking is that we could have a \"cluster expiry\" of sorts where if we found a persistent cluster that had been around for a certain amount of time we would terminate it and redeploy.  This would also solve the issue of EC2 instances becoming unreliable overtime and needing to be replaced.\nReplacing the cluster even once per day would be acceptable to avoid needing to do any maintenance manually.. A new issue has come up which is that if the cluster is persistent all job steps are submitted straightaway as we go through the init phase where actually they need to be submitted after we have gone through all the potential no_op scenarios.\nWill be looking to store all steps into an array first before submission to mimic the same behaviour for persistent and ephemeral clusters.. Makes sense @chuwy thanks for the context.  After a good amount of investigation and chatting with AWS support I think I will just need to implement some manual cleanup steps for HDFS to ensure that it does not get out of hand.\nWill close this one for the moment.. I could probably take this further as well and use that VendorName val for all failure messages in the adapter. \n. .event contains the event type that was passed from mandrill.\n. It will always be message_delayed.  I guess it is pretty redundant information at this point...  For all of the adapters I send through the event type.  I can remove it from the event json before putting it in the unstruct event if you would like.\n. Should I make a package for this app to put the magic constant in?\n. I incorrectly told you to change it to camel case here.  The key should directly reflect what is coming from the event which in this case is with an underscore.\n. To be safe wrap the parse(body) in a try-catch block.  If you pass an invalid string to that function it will exception.\nTo be even safer you should probably also check that the json you parsed has children also.\nWe cannot assume anything about the structure of the data.\n. If you look in Adapter.scala the lookupSchema function actually already returns a neatly formatted error message for you.  So you can do: case Failure(err) => err.failNel instead.\nFor more accurate errors you should also use the second variation of lookupSchema which allows you to pass the the index of the event that failed.  https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/adapters/registry/Adapter.scala#L292-L308\n. If the JSON you parsed does not find event I believe this will throw an exception.  To be safer you can do this:\nval eventType: Option[String] = (itm \\\\ \"event\").extractOpt[String]\nThis removes the need to use eventType.some in your match statement as well.\n. With the final payload that you send in there are a few things we can manipulate beforehand:\n- The event key-pair can be removed as it is extra data for no gain.\n- The timestamp fields can be formatted from UNIX Epoch to something a bit more readable.\n  - 1415709559 -> 2014-11-11T12:39:19.000Z\n@alexanderdean can you think of any other manipulations that could be useful?\n. This function does both of those things: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/adapters/registry/MandrillAdapter.scala#L202-L219\n. @alexanderdean should this event schema have its name divide by an underscore: spamreport -> spam_report ?\n. You can move it into Adapter.scala; it is where we keep all the common functions.\n. The timestamp field could be a JString as well so would be good to add another case for that.\n. The timestamp won't always need to be turned into 'ms'.  Potentially pass another variable along to know what to do with the value?\n. Some events might use a different key for the 'event' variable.\n. This function is no longer in the Mandrill Adapter is it?\n. Could you add a case for a nested mixed timestamp:\njava\n\"\"\"{\"ts\":1415709559,\"nested\":{\"ts\":\"1415700000\"}}\"\"\"\n. They are indeed.  When you define a webhook endpoint in Mandrill or Mailchimp you could do something like this: `http://your-collector/com.sendgrid/v3?aid=email&cv=clj-0.6.0-tom-0.0.4\nSo every-time the webhook provider sends anything to us that query string will be sent with it.\n. The year needs to be bumped to 2015 again.\n. Can we move this function to Adapter.scala to cut down on code duplication.  Potentially also rename to parseJsonSafe.  This function can then be used in the PingdomAdapter as well or anywhere else we need to safely parse JSON\n. Why is Sendgrid mentioned here?\n. In place of wrapping the whole function in a try catch block we can use the parseJsonSafe from above.\n. I am not sure I understand the flow here.  Are we always expecting the body to be a Json Array or can it also be a single Json Object?  As this is a generic adapter it might be nicer to match against the parsed value as being either a JArray or JObject and to process accordingly.\nIf it is a JArray you know that there are multiple events to be processed.\nIf it is a JObject there is a single event to process.\n. This line makes sense when we know what fields in the payload actually are to do cleanup - as this Adapter has no knowledge about what Keys are going to be available it feels dangerous to attempt mutation.\n. A lot of webhooks end up sending a querystring within the payload.body rather than a properly formatted JSON.  To this end we should include a \"application/x-www-form-urlencoded\" content-type as well.\n. I was imagining the same but I guess there could be scope for a POST Json Array...\n. From what I can see the schema -> value pair would still be in the payload here?\n. The only situation I can think of is if the Webhook sent groups of the same event as an array.  That does seem unlikely to happen..\n. I guess I was thrown by it returning a List of RawEvents rather than a single object initially, as there is not actually any processing here to handle multiple events as of yet..\n. This function is returning a list of Validated RawEvents, however the actual body of the function is only ever going to return a single RawEvent in the List.  This should be changed to return just a Validated[RawEvent].\n. The payloadBodyToEvents function is doing a lot of extra work again that has already been done such as checking the Schema exists and extracting it.  This block only needs to:\n- Parse the JSON body safely.\n- Return either a NonEmptyList(String()).failure or a NonEmptyList(RawEvent()).success\nThere is no need to fetch a list of events as we are only expecting one event and as such no need to to pass it to the rawEventsListProcessor.\n. If the parse fails we need to match for Failure as well otherwise this will throw an exception.\nIt also might be a bit easier to follow the flow if this is changed to:\nscala\nparseJsonSafe(body) match {\n  case Failure(err) => err.failNel\n  case Success(parsed) => {\n    // create raw event here\n  }\n}\nThis flow is more in keeping with how the rest of the code is structured and removes the need to do pre-emptive returns like on line 73.\n. Could these all be indented the same or even moved back onto the same line?\n. Instead of defining the val and then matching you can directly match onto payloadBodyToEvent.  We do need the val afterwards so it is a bit cleaner this way.\n. Formatting here; cases should line up.. Formatting of the last row should line up with the others; if need be bump out all of the other rows to match.. Can we reorder PubSub to be after Kafka as the other two sinks are more for testing.. Is this variable being used?. Key is not being used, can this be noted in the scaladoc.. Is there any way send many events at once?  It becomes rather expensive to do an individual put for many millions of events.... Looks like there is one here as well \"datbase\". Will add it to my global and remove from here - you are right!. Following the same code pattern as @BenFradet implemented in his S3 class - happy to drop this back to a single public function.. At the moment I am ignoring that possibility and it will use the first page that is successful - it didn't seem like a good use of processing power to paginate through every single page of EMR clusters to look for that condition.\nI can however change it to first populate a full list of emr_clusters before applying the filtering logic?  Will just be a bit more expensive.. Fair enough!  Will drop down my local version of bundler back to 1.15.4.... Will try it out with the Elasticity function, thanks for flagging @chuwy !  If it works just a well I will remove this dependency and the bundler commit which should keep things a bit simpler!. We can - I just didn't see the need to generate another timestamp but happy to do so. We could do - TBH we should never hit that failure in practice as the Consul lock should prevent us from ever attempting to get a RUNNING cluster and we should never have a non-unique cluster.  If you think it would be useful happy to add one in.. Doh of course yes we can!. What would you prefer @BenFradet ?. Ended up implementing it to recursively find all clusters which are running or waiting with that name before checking so should be bullet proof now!. Implemented using Elasticity EMR class instead.. The EMR Cluster JSON always has both of those keys an unless Amazon drastically change their returned objects it should work fine.  This has all been tested through using a test rig running in the sandbox and works as expected.. Tested and working @BenFradet - you can execute any bash command using command-runner.jar - the only reason I could not use it for all the removals was that I needed to add if statement checks.. So the underlying script would accept N HDFS path arguments and loop through them?  Yeah that can be done easily enough.. It does it automatically so not sure its required - can add an exit 0 however.. Will do!. As this is run only after all steps are completed and only on successful completion I think it is pretty safe - will combine the ifs also.. ",
    "fail-fast": "please let me know in case you guys wanna me to create a PR for it..\n. ",
    "knservis": "Yes I have\n. @jbeemster What are these files? What is creating them? . Migrated to snowplow/iglu-central#661. Migrated to snowplow/iglu-central#656. @alexanderdean this seems ready, it just needs to be tested correct?. Is this still relevant given #3068 ?. @alexanderdean I suggest we keep the environment variables as: \nAWS_DEPLOY_ACCESS_KEY_ID\nand\nAWS_DEPLOY_SECRET_ACCESS_KEY\ninstead of:\nAWS_DEPLOY_ACCESS_KEY\nand\nAWS_DEPLOY_SECRET_KEY\nbut either way will work (I cannot add these keys as I don't have them). . - [x] @alexanderdean to update the secrets\n- [x] @knservis to compact the commits through rebase and submit PR. @alexanderdean Did you push your changes?. Submitted https://github.com/snowplow/snowplow/pull/3479. Would it make sense to suppress that field as part of the geoip enrichment? . Could we use environment variables?. As an alternative, instead of relying on s3distcp to delete the s3 objects it may be possible to delete them as a separate step after the job is successful (s3distcp provides a manifest that can be used for that). @alexanderdean It makes sense to me. I have added the algorithms that are supported by MessageDigest in java (which is what we are using for hashFunction argument at the moment) with a warning not to use MD2 (unless you have some very weird and specific requirement). Those are:\n\nMD2, the 128-bit algorithm MD2 (not-recommended due to performance see RFC6149)\nMD5, the 128-bit algorithm MD5\nSHA-1, the 160-bit algorithm SHA-1\nSHA-256, 256-bit variant of the SHA-2 algorithm\nSHA-384, 384-bit variant of the SHA-2 algorithm\nSHA-512, 512-bit variant of the SHA-2 algorithm\n\nAnd they are the ones allowed in the config schema: https://github.com/snowplow/iglu-central/issues/692. PR sent https://github.com/snowplow/snowplow/pull/3482. @BenFradet @chuwy @alexanderdean \nAfter the initial commit, there one way to ensure that this style continues to apply is making the scalafmt task a dependancy of the compile task in sbt. How do you feel about that?. @BenFradet In my experience compile is not noticeably affected. @BenFradet \nIt adds 3 sec on average to the SCE compile from pristine which on average takes 58.3 sec:\n```\nrepeat 3 { gpristine && time sbt compile >> /dev/null }\nFrom pristine:\n1. sbt compile >> /dev/null  212.17s user 6.64s system 385% cpu 56.754 total\n2. sbt compile >> /dev/null  225.30s user 7.03s system 389% cpu 59.717 total\n3. sbt compile >> /dev/null  216.93s user 7.48s system 388% cpu 57.688 total\nWith Scalafmt as compile task dep (sbt-scalafmt 1.13):\n\nsbt compile >> /dev/null  237.59s user 7.49s system 383% cpu 1:03.91 total\nsbt compile >> /dev/null  228.52s user 7.00s system 392% cpu 1:00.03 total\nsbt compile >> /dev/null  235.57s user 7.14s system 394% cpu 1:01.47 total\nI think that is tolerable. @alexanderdean With the igc release, tests pass now. I think we are done (except updating the versions and tagging). . @BenFradet done. IANAL but the way I understood the regulation, the protection is extended to the data subject even if only part of the tracked activity is in the EU. \nArticle 3:\n\n2.\u00a0This Regulation applies to the processing of personal data of data subjects who are in the Union by a controller or processor not established in the Union, where the processing activities are related to:\n(a) the offering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union; or \n(b) the monitoring of their behaviour as far as their behaviour takes place within the Union \n```\nI am really not sure what happens to the scope in (b) when part fo the activity is in the EU \nSee also recital 23 and 24:\n```\n(23) In order to ensure that natural persons are not deprived of the protection to which they are entitled under this Regulation, the processing of personal data of data subjects who are in the Union by a controller or a processor not established in the Union should be subject to this Regulation where the processing activities are related to offering goods or services to such data subjects irrespective of whether connected to a payment.\nIn order to determine whether such a controller or processor is offering goods or services to data subjects who are in the Union, it should be ascertained whether it is apparent that the controller or processor envisages offering services to data subjects in one or more Member States in the Union.\nWhereas the mere accessibility of the controller's, processor's or an intermediary's website in the Union, of an email address or of other contact details, or the use of a language generally used in the third country where the controller is established, is insufficient to ascertain such intention, factors such as the use of a language or a currency generally used in one or more Member States with the possibility of ordering goods and services in that other language, or the mentioning of customers or users who are in the Union, may make it apparent that the controller envisages offering goods or services to data subjects in the Union.\n```\n```\n(24) The processing of personal data of data subjects who are in the Union by a controller or processor not established in the Union should also be subject to this Regulation when it is related to the monitoring of the behaviour of such data subjects in so far as their behaviour takes place within the Union.\nIn order to determine whether a processing activity can be considered to monitor the behaviour of data subjects, it should be ascertained whether natural persons are tracked on the internet including potential subsequent use of personal data processing techniques which consist of profiling a natural person, particularly in order to take decisions concerning her or him or for analysing or predicting her or his personal preferences, behaviours and attitudes. \n```\n. @BenFradet the target branch should be the release/r9x-knossos branch right?. @BenFradet chery-picked to release branch. . We need to think about this in context of this comment: https://github.com/snowplow/iglu-central/pull/664#issuecomment-351312945 . @alexanderdean sha-512 is 128 characters long as a hex string.. Also the style guide is not sufficient probably. \nThings missing:\n\nLine length limit. \nBracket use guide (@BenFradet has a better idea on that)\nRefactoring common functionality\nFunction length guide\n\nThese will also help with the scalafmt (or other tool) configuration in #3496 . @chuwy It is not their fault. I have added some of the more exaggerated configurations (look at .scalafmt.conf align.tokens ). Overall we can play with other configurations and if they we need to switch them off somewhere as an exception we can always go //format: off.  Have a look here for configurations: http://scalameta.org/scalafmt/#Configuration. @alexanderdean It would have been too hard to see the changes otherwise. I can squash them together now that we are done.. Already merged. @BenFradet I am not sure what is happening with history (have merged master to both the release and the issue branches..). @BenFradet eventually they will end up in the same PR to master, but it is more clear to consider them in isolation initially. . @BenFradet The merged changes are now in the release branch. We'll look at them in the final PR. @BenFradet Thanks for the PR, very helpful. Can we please move all formatting comments to (https://github.com/snowplow/snowplow/issues/3496 - auto formatting). @BenFradet @chuwy I believe we are done with this PR right?. @chuwy Thanks for the review.. @BenFradet I did not realise that there is more to talk about. Is there something specific? All points have been addressed as far as I know. . @BenFradet Was there something that was not addressed? . @BenFradet @chuwy Sorry guys! Did not want to interrupt the review. I thought it was done.. In which case the test should not fail when there is a connection timeout. To me, when I am testing something unrelated and the whole suite fails because of an external service timeout, something is wrong. Is the OWM API not versioned? . @BenFradet Should we just include a PII section with an output stream name in the hocon?. @BenFradet I don't think there is something else. Is/should the user be able to select a different type of stream for the different outputs (e.g. good to kinesis, pii to kafka)? . @BenFradet can we come up with a better title? Or I could simply merge this to the corresponding issue: https://github.com/snowplow/snowplow/issues/3580. @alexanderdean Does that name capture the work? . Depends on https://github.com/snowplow/iglu-central/pull/708. @alexanderdean @yalisassoon Before releasing part 2 we may want to consider emitting a salt if it is finally added (We will need to modify iglu-central schemas for config and the event). See discussion here https://github.com/snowplow/snowplow/issues/3648 \nShould we hold off till we have made a decision?. @alexanderdean Yes the salt should not be emitted. It should only be in config.  . @BenFradet This is commit is not yet properly tested. I would like to add some test to stream enrich that will run the complete pipeline, including sending and receiving events based on something like https://github.com/mhart/kinesalite . Does that sound like a good idea to you?. @BenFradet I do not believe it will be a big time investment. From experience, I have set up similar tests before in 1-2 days. I am not partial to kinesis. It could be anything (now that I think about it maybe mock kafka is a better idea), but I think end to end testing is important to add soon. . Added https://github.com/snowplow/snowplow/issues/3639 (Not sure about title). @alexanderdean Does this belong to iglu-central?. @alexanderdean Done. Created https://github.com/snowplow/iglu-central/issues/702. @BenFradet Is this r100?. Incorrectly shows merged PR (old PR). Re-openning. @BenFradet Added scapegoat and scalastyle and made the test dependant on them (this can fail the tests). \nWe should look into the rules here http://www.scalastyle.org/rules-1.0.0.html  and make a decision on scalastyle-config.xml . @BenFradet All good for you here? . @alexanderdean \n\nDone for this and the target branch. \nDone on target branch (https://github.com/snowplow/snowplow/pull/3585/commits/feafbc7e29073072279a0198adad254954ab0346 on https://github.com/snowplow/snowplow/pull/3585)\n\nEpidaurus (https://en.wikipedia.org/wiki/Epidaurus). @chuwy Removed pattern match on List. Please have a look \n. Thanks @chuwy I took care of the last comments too. . @BenFradet I don't understand. There is a redirect to https.. \ud83d\udc4d  Build fixed by that change. We should probably add issues to upgrade sbt across the board anyway.. @BenFradet sounds good although we should change the milestone name if we are going to add this issue (as this is SE not SCE).. @alexanderdean This is for the next release right (i.e. gdpr-2)? . @chuwy \n\n\nYes it was used there. \n\n\nI thought we wanted a single formatting tool to ensure formatting automatically. Lets talk about it.. @BenFradet Fixed both. @BenFradet I don't know why the order is like that on GH. My local log has the correct order (changes, prep, rc commits). \n\n\nThat is what the log looks like to me:\n$ git log --pretty=oneline --abbrev-commit master.. | awk '!($1=\"\")' | sed 's/closes //' | sed 's/^ //'\nSE 0.14.0-rc5\nSCE 0.31.0-M5\nSE 0.14.0-rc4\nSCE 0.31.0-M4\nSE 0.14.0-rc3\nSCE 0.31.0-M3\nSE 0.14.0-rc2\nSCE 0.31.0-M2\nSSE 0.14.0-rc1\nSCE 0.31.0-M1\nPrepare for release\nScala Common Enrich: bump to 0.31.0 (#3598)\nScala Common Enrich: apply automated code formatting (#3532)\nScala Common Enrich: use automated code formatting (#3496)\nStream Enrich: use generated Settings for version in test (#3604)\nStream Enrich: bump to 0.14.0 (#3596)\nStream Enrich: bump scala-common-enrich to 0.31.0 (#3597)\nScala Common Enrich: add PII Enrichment (#3472)\nScala Common Enrich: remove unused version member in Enrichment trait (#3541)\nRedshift: widen sensitive columns in atomic.events to support pseudonymization (#3528)\nRedshift: widen se_label to 4,096 to support URLs etc (#196. @BenFradet According to @alexanderdean this is not a showstopper and a very unlikely case. I could fix it on jayway probably. . @BenFradet @chuwy I believe I fixed those issues. Have another look if you can. thanks. @chuwy Did you have a chance to have a look?. @chuwy @BenFradet thanks guys for the PR. Is anything else remaining @chuwy ?. @alexanderdean Do we need any changes here for your green tick? . @BenFradet Agreed not a great name (maybe GPubSub? ) but to me (again this may just be me) it would make things more intuitively readable, that is I would not need to go and verify for everything that I read, that it really is a specific product that variables are referring to and not some super class of all pub sub systems or some such generalisation. . @BenFradet The actual code (not test) change for this ticket is one line: https://github.com/snowplow/snowplow/pull/3633/commits/1c955e12c8b0740f0ca251f02406b74023be2790#diff-9e5742dff28d1d166dcb990bcdaa5a79R210. @BenFradet rebased. It should be clearer now. @BenFradet @chuwy Is there more to be done here? . @BenFradet Is something remaining for your green tick?. @chuwy @BenFradet Thanks for the review. Merging. @BenFradet Added failing case.. @BenFradet can you please verify that the hocon changes don't cause any issue, and that scalafmt is something we want here?. @BenFradet I mean this https://github.com/snowplow/snowplow/pull/3646/commits/f1addb6f3ddba4807251dbee8b07f86c8ce300ad#diff-35cd2b577f491cb5aeaf46ba8cead04eR23 . @BenFradet @chuwy This is now under review. Can you please have a look? . @BenFradet Can you please have another look? . @BenFradet Is there something else on this PR?. @BenFradet please have another look.. Looks like the emitted PII event should be an actual EnrichedEvent here is how I propose to populate it (sorry about the wide format) @alexanderdean \n(Most values are from the stream.good.PagePingWithContextSpec test):\n| Field name | Type | Internal value | Good event output | Pii event output |\n|------------|--------|-----------------------|--------------|-----------------|\n|app_id| String|\"CFe23a\"| Same | Same |\n|platform| String|\"web\"| Same| \"stream-enrich\" |\n|etl_tstamp| String|<timestamp>| Same | New generated |\n|collector_tstamp| String|\"2014-02-02 20:21:19.167\"| Same | Same |\n|dvce_created_tstamp| String|\"2014-02-02 20:21:19.329\"| Same | Same |\n|event| String|\"page_ping\"| Same | \"pii_transformation\" |\n|event_id| String|91c7b210-3851-44b9-a748-03ebb610d3a9| Same | New generated |\n|txn_id| String|\"576668\"| Same | None |\n|name_tracker| String|\"\"| Same | None |\n|v_tracker| String|\"js-0.13.1\"| Same | None |\n|v_collector| String|\"ssc-0.1.0-stdout\"| Same | Same |\n|v_etl| String|\"kinesis-0.14.0-common-0.32.0\"| Same | Same |\n|user_id| String|\"d1a21f2589511b4ed04ee297d88d950efb2612dc\"| Same | None |\n|user_ipaddress| String|\"850474a1f035479d332a5c2d2ad6fe4d07a3f722\"| Same | None |\n|user_fingerprint| String|\"1804954790\"| Same | None |\n|domain_userid| String|\"3c1757544e39bca4\"| Same | None |\n|domain_sessionidx| JInteger|\"25\"| Same | None |\n|network_userid| String|\"75a13583-5c99-40e3-81fc-541084dfc784\"| Same | None |\n|geo_country| String|\"\"| Same | None |\n|geo_region| String|\"\"| Same | None |\n|geo_city| String|\"\"| Same | None |\n|geo_zipcode| String|\"\"| Same | None |\n|geo_latitude| JFloat|\"\"| Same | None |\n|geo_longitude| JFloat|\"\"| Same | None |\n|geo_region_name| String|\"\"| Same | None |\n|ip_isp| String|\"\"| Same | None |\n|ip_organization| String|\"\"| Same | None |\n|ip_domain| String|\"\"| Same | None |\n|ip_netspeed| String|\"\"| Same | None |\n|page_url| String|\"file://file:///Users/alex/Development/dev-environment/demo/1-tracker/events.html/overridden-url/\"| Same | None |\n|page_title| String|\"Asynchronous website/webapp examples for snowplow.js\"| Same | None |\n|page_referrer| String|\"\"| Same | None |\n|page_urlscheme| String|\"file\"| Same | None |\n|page_urlhost| String|\"file\"| Same | None |\n|page_urlport| JInteger|\"80\"| Same | None |\n|page_urlpath| String|\"///Users/alex/Development/dev-environment/demo/1-tracker/events.html/overridden-url/\"| Same | None |\n|page_urlquery| String|\"\"| Same | None |\n|page_urlfragment| String|\"\"| Same | None |\n|refr_urlscheme| String|\"\"| Same | None |\n|refr_urlhost| String|\"\"| Same | None |\n|refr_urlport| JInteger|\"\"| Same | None |\n|refr_urlpath| String|\"\"| Same | None |\n|refr_urlquery| String|\"\"| Same | None |\n|refr_urlfragment| String|\"\"| Same | None |\n|refr_medium| String|\"\"| Same | None |\n|refr_source| String|\"\"| Same | None |\n|refr_term| String|\"\"| Same | None |\n|mkt_medium| String|\"\"| Same | None |\n|mkt_source| String|\"\"| Same | None |\n|mkt_term| String|\"\"| Same | None |\n|mkt_content| String|\"\"| Same | None |\n|mkt_campaign| String|\"\"| Same | None |\n|contexts| String|\"\"\"{\"schema\":\"iglu:com.snowplowanalytics.snowplow/contexts/jsonschema/1-0-0\",\"data\":[{\"schema\":\"iglu:com.snowplowanalytics.snowplow/web_page/jsonschema/1-0-0 data\":{\"id\":\"b05b31c3-81ac-4af5-92d1-113133968655\"}}]}\"\"\"| Same | \"\"\"{\"schema\":\"iglu:com.snowplowanalytics.snowplow/contexts/jsonschema/1-0-0\",\"data\":[{\"schema\":\"com.snowplowanalytics.snowplow/parent_event/jsonschema/1-0-0\",\"data\":{\"parentEventId\":\"91c7b210-3851-44b9-a748-03ebb610d3a9\"}}]}\"\"\" |\n|se_category| String|\"\"| Same | None |\n|se_action| String|\"\"| Same | None |\n|se_label| String|\"\"| Same | None |\n|se_property| String|\"\"| Same | None |\n|se_value | String|\"\"| Same | None|\n|unstruct_event | String|\"\"| Same | \"\"\"{\"schema\":\"iglu:com.snowplowanalytics.snowplow/unstruct_event/jsonschema/1-0-0\",\"data\":{\"schema\":\"iglu:com.snowplowanalytics.snowplow/pii_transformation/jsonschema/1-0-0\",\"data\":{\"pii\":{\"pojo\":[{\"fieldName\":\"user_ipaddress\",\"originalValue\":\"10.0.2.x\",\"modifiedValue\":\"850474a1f035479d332a5c2d2ad6fe4d07a3f722\"},{\"fieldName\":\"user_id\",\"originalValue\":\"alex 123\",\"modifiedValue\":\"d1a21f2589511b4ed04ee297d88d950efb2612dc\"}]},\"strategy\":{\"pseudonymize\":{\"hashFunction\":\"SHA-1\"}}}}}}\"\"\"|\n|tr_orderid| String|\"\"| Same | None |\n|tr_affiliation| String|\"\"| Same | None |\n|tr_total| String|\"\"| Same | None |\n|tr_tax| String|\"\"| Same | None |\n|tr_shipping| String|\"\"| Same | None |\n|tr_city| String|\"\"| Same | None |\n|tr_state| String|\"\"| Same | None |\n|tr_country| String|\"\"| Same | None |\n|ti_orderid| String|\"\"| Same | None |\n|ti_sku| String|\"\"| Same | None |\n|ti_name| String|\"\"| Same | None |\n|ti_category| String|\"\"| Same | None |\n|ti_price| String|\"\"| Same | None |\n|ti_quantity| JInteger|\"\"| Same | None |\n|pp_xoffset_min| JInteger|\"0\"| Same | None |\n|pp_xoffset_max| JInteger|\"0\"| Same | None |\n|pp_yoffset_min| JInteger|\"0\"| Same | None |\n|pp_yoffset_max| JInteger|\"0\"| Same | None |\n|useragent| String|\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:26.0) Gecko/20100101 Firefox/26.0\"| Same | None |\n|br_name| String|\"Firefox 26\"| Same | None |\n|br_family| String|\"Firefox\"| Same | None |\n|br_version| String|\"26.0\"| Same | None |\n|br_type| String|\"Browser\"| Same | None |\n|br_renderengine| String|\"GECKO\"| Same | None |\n|br_lang| String|\"en-US\"| Same | None |\n|br_features_pdf| JByte|\"0\"| Same | None |\n|br_features_flash| JByte|\"1\"| Same | None |\n|br_features_java| JByte|\"0\"| Same | None |\n|br_features_director| JByte|\"0\"| Same | None |\n|br_features_quicktime| JByte|\"1\"| Same | None |\n|br_features_realplayer| JByte|\"0\"| Same | None |\n|br_features_windowsmedia| JByte|\"0\"| Same | None |\n|br_features_gears| JByte|\"0\"| Same | None |\n|br_features_silverlight| JByte|\"0\"| Same | None |\n|br_cookies| JByte|\"1\"| Same | None |\n|br_colordepth| String|\"24\"| Same | None |\n|br_viewwidth| JInteger|\"1680\"| Same | None |\n|br_viewheight| JInteger|\"415\"| Same | None |\n|os_name| String|\"Mac OS X\"| Same | None |\n|os_family| String|\"Mac OS X\"| Same | None |\n|os_manufacturer| String|\"Apple Inc.\"| Same | None |\n|os_timezone| String|\"Europe/London\"| Same | None |\n|dvce_type| String|\"Computer\"| Same | None |\n|dvce_ismobile| JByte|\"0\"| Same | None |\n|dvce_screenwidth| JInteger|\"1920\"| Same | None |\n|dvce_screenheight| JInteger|\"1080\"| Same | None |\n|doc_charset| String|\"UTF-8\"| Same | None |\n|doc_width| JInteger|\"1680\"| Same | None |\n|doc_height| JInteger|\"415\"| Same | None |\n|tr_currency| String|\"\"| Same | None |\n|tr_total_base| String|\"\"| Same | None |\n|tr_tax_base| String|\"\"| Same | None |\n|tr_shipping_base| String|\"\"| Same | None |\n|ti_currency| String|\"\"| Same | None |\n|ti_price_base| String|\"\"| Same | None |\n|base_currency| String|\"\"| Same | None |\n|geo_timezone| String|\"\"| Same | None |\n|mkt_clickid| String|\"\"| Same | None |\n|mkt_network| String|\"\"| Same | None |\n|etl_tags| String|\"\"| Same | None |\n|dvce_sent_tstamp| String|\"\"| Same | None |\n|refr_domain_userid| String|\"\"| Same | None |\n|refr_dvce_tstamp| String|\"\"| Same | None |\n|derived_contexts| String|\"\"| Same | None |\n|domain_sessionid| String|\"\"| Same | None |\n|derived_tstamp| String|\"2014-02-02 20:21:19.167\"| Same| New generated timestamp |\n|event_vendor| String|\"com.snowplowanalytics.snowplow\" | Same | \"com.snowplowanalytics.snowplow\" |\n|event_name| String|\"page_ping\"| Same | \"pii_transformation\" |\n|event_format| String|\"jsonschema\"| Same | \"jsonschema\" |\n|event_version| String|\"1-0-0\" | Same | \"1-0-0\" |\n|event_fingerprint| String|\"\"| Same | None |\n|true_tstamp| String| \"\"| Same | None |\n|(pii) (SCE internal only field)| String | \"\"\"{\"schema\":\"iglu:com.snowplowanalytics.snowplow/pii_transformation/jsonschema/1-0-0\",\"data\":{\"pii\":{\"pojo\":[{\"fieldName\":\"user_ipaddress\",\"originalValue\":\"10.0.2.x\",\"modifiedValue\":\"850474a1f035479d332a5c2d2ad6fe4d07a3f722\"},{\"fieldName\":\"user_id\",\"originalValue\":\"alex 123\",\"modifiedValue\":\"d1a21f2589511b4ed04ee297d88d950efb2612dc\"}]},\"strategy\":{\"pseudonymize\":{\"hashFunction\":\"SHA-1\"}}}}\"\"\"| (not emitted as it is not part of the EnrichedEvent) | (not emitted as it is not part of the EnrichedEvent) |\nIs that correct (feel free to edit)?\nThe only question remaining in my mind is: to which can I add the parent event_id (UUID)? (I think that could be useful)\nEdit: added parent_event context. @alexanderdean @BenFradet The PII is now an enriched event. Can you please check that the fields (in the table above make sense)?. It seems like we will also need a new event to be included in the contexts field that will contain the parent event id. The format will be this https://github.com/snowplow/iglu-central/blob/master/schemas/com.snowplowanalytics.snowplow/duplicate/jsonschema/1-0-0 (except for the name and possibly the field name) . @BenFradet I have finished rebasing and all some changes that we missed on PR (moving integration test to separate module and making piistream optional). Can you please have another look to make sure I did not miss anything?. OK @alexanderdean and @BenFradet are you ok with this PR? . @BenFradet tests are sorted. Not sure what you mean about commits. @BenFradet tests are sorted. Not sure what you mean about commits. Hashing all possible values is of course possible and even trivial in some cases (e.g. IP), however I would like to understand the attack scenario in which this would happen. Would that scenario involve access to the entire dataset by an unauthorized party? Would it not be just as easy when you have that sort of access to also get the salt?\nThe initial thoughts about this functionality is that it help the data processor not have direct casual access to the name, email or IP address of the data subject without previous justification. When the data subject indicates that they do not wish for their identity to be used, then this data is hashed.  In a future release the unhashed value will be stored in a separate table along with the hash and access to it will be controlled.\nIn any case we could trivially add salt to the hash though the config, however in the case that a rogue actor of the data processor has decided to enumerate all the hashes because they have access, would they not just as likely have access to the salt?\nMy understanding is that salt is usually stored with the hash (in password hashes) and it is not supposed to be a secret. If it were to be kept a secret would it not make more sense to use actual cryptography (which may be implemented as an additional strategy in future releases)? Thoughts?. @falschparker82 We could easily add salt.\nJust a point on the IPv4 which has the smallest domain hence it's trivially enumerable: the PII enrichment runs after all other enrichments. If you have configured dropping the last octet of IPv4 and PII, the hashed value will be the masked value, which of course is even easier to enumerate, but does not uniquely identify the connected machine.\nCan you think of other dimensions that you use in Snowplow where someone can fully enumerate the domain?\nAbout the S3 logs, we are planning to address that issue too.\nThanks for your help. @alexanderdean It would make testing easier if we optionally have either \"ec2ParameterStore\" or \"localValue\", with the local value having the actual salt value. Do you see something wrong with that?. @alexanderdean @chuwy I have added the salt to v2-0-0 of the config https://github.com/snowplow/iglu-central/pull/708. I am not brave enough to change libraries for this for now (and deal with the fallout). I think it would need extended testing including measuring impact in client data. Bumping the library I think is the right thing for now and schedule a new library soon in 2018. What do you think @alexanderdean ?. @alexanderdean Sure. In fact I believe it is better that way. As an aside would it not be better to have those as environment variables anyway? . @BenFradet I am keen to tidy up all the branches on this release. Is the PR good enough to merge (after rebase)? \n. @BenFradet Thank you for the review. Can you see anything else?. @rbolkey Can you please change the base branch of the PR to release/gdpr-2 and rebase?. @rbolkey Thanks for that. Yes there have been issues with travis. @BenFradet is that related with issues you had with travis? . @yalisassoon and @alexanderdean Is there missing? \n. Closing this as a result of discussion here: https://github.com/snowplow/snowplow/pull/3669#issuecomment-382663457 . @BenFradet I am curious how that is not an issue either way as com.maxmind.geoip2 requires databind 2.9.3 (https://github.com/maxmind/GeoIP2-java/blob/v2.11.0/pom.xml) which is required by com.snowplowanalytics:scala-maxmind-iplookups_2.11:0.4.0. \nIf it works in some version (after building that way) I would not trust it as it probably works by coincidence. \nI had an issue just yesterday related to databind: https://github.com/snowplow/iglu-scala-client/issues/80\nOur version is quite old (5 years old) and it is bound to get evicted. Would releasing a com.snowplowanalytics:scala-maxmind-iplookups_2.11:0.4.0 with shaded databind work for somehow? \nIn assembly it is certainly possible (but painful) to rename dependencies.. . Could we mock out the service? . @chuwy @BenFradet Smoke testing went well (including with the rc1 of shredder). We have looked through all this code in previous PRs. Let me know if you spot something new. Thanks. @BenFradet Thanks not sure what is missing from the milestone. . @chuwy Thanks . @BenFradet Can you please look at the last commit before I copy it to all sources?. @BenFradet Thanks. Same approach for all the rest. (I will soon rebase). @BenFradet I think we are also done with the restructuring.. Anything else left? . @apiddubnyi This does not exactly answer your question, but did you know we have a PII pseudonymization enrichment that may help you comply with GDPR? This may be what you are looking for and you can read about it here: https://snowplowanalytics.com/blog/2018/02/27/snowplow-r100-epidaurus-released-with-pii-pseudonymization-support/. @BenFradet Let's move this to the issue-3636 branch. Sorry @BenFradet.. I meant in the release/gdpr-2 branch (after fixing the commit messages). We can change the PR to that branch instead of master\n. If we use a \"company-wide\" key as in this case we cannot throw away the key in order to act on a data subject request to forget them, but we can still remove the cipher from piinguin. This is addressing the specific need to control access within a company to the PII store without intervening in the data store where (when you use a hash and secret salt) you have a fixed length constant identifier which still protects the data subject. . That is a possible alternative. One possible motivation to do it in the enrichment is to also help secure the pii stream, however this is not a strong reason. It may be better in piinguin.. I think it makes no sense to do r106 first. Why don't you send a PR to release/gdpr-2 and I do the rest? If this is only the sample config then I can do it (Is that config tested?) Does it involve doc changes in the blog and the wiki? . @chuwy Ok then I can easily do that. The JSON path bug is unrelated to the shredder bug which lead to 1.13.1 but we can add that as other improvements. For the record I will repeat my point from our discussion here: This seems like a necessary workaround at the moment. At the same time I would add a ticket to undo this at some point. The reason is that when a user of stream enrich specifies a pii setting he does that, because he intends to emit pii events. The application should test that setting, including (as it does now) whether the pii enrich configuration is in conflict with that intention (i.e. if emitEvent is false). The fact that we have to undo this apparently correct test because Mini cannot update the hocon, seems a bit wrong, and even if undoing this test is not a big deal, maybe we should investigate whether something better could be done in Mini (after all the hocon is easy to fill in as we have done in the kafka intergation test). My 2c. @alexanderdean My understanding is that it is hard to update the hocon configuration on Snowplow Mini, but it is easy to update the enrichment configuration. As stream enrich checks both the enrichment (emitEvent:true) and the hocon (pii=blah to be present) that would be a problem for SP Mini because we would like to set the pii = standard_stream_name in the hocon and be able to set unset emitEvent in the enrichment. @BenFradet @jbeemster Is that a good summary of the motivation?. Yes @chuwy \n@alexanderdean I know.. . Should be map.. What do you suggest?. Don't you think that this will make the resulting long line less readable? Also what do you think about catching Exception? Is it not too general?. Good catch! Initially I did not think that this would be a very useful error, but now that I think about it again you are right. . Is there a way to include a reason the test was skipped with that syntax?. Actually the empty line above \"value\" is mandated by the RFC mentioned. . Fixed in https://github.com/snowplow/snowplow/issues/3532. Should this not be error (and terminate)? \n(Setting X-Forwarded-Proto to \"blah\" which is not supported and getting back the request protocol seems wrong). There are actually two ways to silence that compiler warning (workarounds):\nval token = redirectMacroConfig.placeholder.getOrElse(\"$\"+\"{SP_NUID}\") //Silence interpolator warning\nand \nval token = redirectMacroConfig.placeholder.getOrElse(s\"$${SP_NUID}\") //Silence interpolator warning. Same here about the compiler warning. . This is an error on formatting, but as far as I can tell formatting is coming from line 52.. Would it not be faster to test the range of tstamp? \n  . This requires a trailing newline according to https://github.com/snowplow/snowplow/blob/master/CONTRIBUTING.md. This should have a newline according to https://github.com/snowplow/snowplow/blob/master/CONTRIBUTING.md. I don't know if I missed something. Is there anything related to \"Scala Common Enrich: add adapter for Google Analytics (closes #3560)\" in commit in this file?. Extra space (I know you don't like those @BenFradet ). It would probably a lot simpler and faster to use a regex for composite fields\ne.g.\n\"(pr|il|cd|cm|cg)(\\\\d+)([a-zA-Z]*)(\\\\d*)([a-zA-Z]*)(\\\\d*)([a-zA-Z]*)$\".r. Are there any changes related to the commit message here?. Why not HALF_EVEN? Is this currency? HALF_EVEN has better statistical characteristics.. In which case HALF_EVEN will produce more accurate results on aggregate. I can't foresee any other implementations. Maybe it should be a sealed trait.. It fails schema validation: https://github.com/snowplow/iglu-central/pull/697. I am also adding a failure case, but it's probably redundant.. This is a private def and its only there to remove some clutter from the for comp. Why would we want this to be more generic? . Extracting the val is good. Mixing infix with postfix is bad in my opinion. Interesting. I will look into that. oops. Very true. True. It evolved into a val. . Maybe a constant.. \n. Can we add all formatting and alignment comments here: https://github.com/snowplow/snowplow/issues/3496 ?. I don't understand your comment. AdditionalProperties (or a property not in required). . This sounds like premature refactoring to me :-) Let's cross that river when we get there. Otherwise we can add a separate ticket to move it out and replace it in other places. No. Does it need to be? If yes we should add it to CONTRIBUTING as I had this question for every source file I read. . Good point. This is a maybe. \nIn fact your suggestion also suffers from type erasure:\nsrc/main/scala/com.snowplowanalytics.snowplow.enrich/common/enrichments/registry/PiiPseudonymizerEnrichment.scala:63: non-variable type argument com.snowplowanalytics.snowplow.enrich.common.ValidatedString in type pattern (String, String) => com.snowplowanalytics.snowplow.enrich.common.ValidatedString is unchecked since it is eliminated by erasure\n[warn]       case (inputField, (f: ((String, String) => ValidatedString), outputField: String)) if outputField == fieldName =>\n[warn]                                               ^\n[warn] one warning found\nalso I personally really don't find the val more readable. Finally \"andThen\" makes it much more explicit  to me that you are applying applyStartegy to the result.. good point. It's probably a good idea not to assume that this is obvious. To me its is not obvious why this comment and grouping is useful:\n```\n// Scalaz\nimport scalaz.\nimport Scalaz.\n// json4s\nimport org.json4s._\nimport org.json4s.jackson.JsonMethods\n```\nCan we resolve the why in the contributing guide?\n. This also gets an NPE:\n```\nscala> class PIIC{\n     |   val fieldName: String = \"not_\"\n     |   val isBrowser: Boolean = fieldName.startsWith(\"br_\")\n     | }\ndefined class PIIC\nscala>\nscala> object BrowserPII extends PIIC{\n     |   override val fieldName = \"br_useragent\"\n     | }\ndefined object BrowserPII\nscala> BrowserPII.isBrowser\njava.lang.NullPointerException\n  ... 45 elided\n```\nthese are sealed traits and the use is specific. Initalisation errors are annoying but defs with no argument are funny, and it seems that vals will not be a problem in this particular use.\nThis also seems to work:\n```\nscala> trait PIIT {\n     |   def fieldName: String\n     |   val isBrowser = fieldName.startsWith(\"br_\")\n     | }\ndefined trait PIIT\nscala>\nscala> object BrowserPIIT extends PIIT {\n     |   val fieldName = \"br_useragent\"\n     | }\ndefined object BrowserPIIT\nscala>\nscala> BrowserPIIT.isBrowser\njava.lang.NullPointerException\n  at PIIT$class.$init$(:13)\n  ... 44 elided\n. ^^^. I will add it to the test. Thanks. I like that library. We should go ahead and replace it in all SCE . I don't understand this: \"Use json4s' serialization instead.\" can you explain?. @chuwy What is the difference?. True. That is where your comment was. I am assuming you meant in the trait then?. Should it be: \"The configuration file does not conform to the configuration schema\"?. I don't understand exactly what you mean. It implements a trait. Random overrides don't compile.  . I thought of that too but the list is checked exactly above it. Maybe turn it into a Map? What would you do?. I could not find that in this version of \\/ but it's actually more specific than than NonFatal.. You cannot get AnyVal, given the interface of MapFunction.\n. A map to me implies that you are mapping to a Validation. Should it be \"The configuration does not conform to the config schema\"?. How should we solve it? Should we add a ClassTag? . (I think they use reflection and are therefore slow) . This should definitely go in a style guide. . Because it does not contain a useful message. In addition schema conformance in config is checked.. The config is checked against the config schema no?. json is just another field (it contains a json string instead of a scalar) in the POJO, so at this level they are both POJO mutators the way I see it. Maybe the we should rename the Pojo mutator to \"scalar mutator\" to make that clear, What do you think?. good point. Is this about \"=>\" alignment? If so it will not align them unless they are in consecutive lines and it needs to break the line here as it would be longer that the maximum configured.. scalar mutator sounds more accurate to me too. I am changing it.. \ud83d\udc4d . I tried splitting       }) via a newline to see if it will indent them differently and it joined them back. There must be a setting for that.. @chuwy It's easy for anyone to miss that especially if they haven't used json4s. In this case there is a single value and this is very convenient, but if there is a better, clearer way to get that value out I would like to know. . What you are saying is true, and another reason for me to not like json4s, however there is a guard in modifyObjectIfSchemaMatches, which is it only applies the modification if the schema matches. The only documented way that I could find on modifying a field does exactly that (in http://json4s.org/) and yes it does modify fields in multiple levels using pattern matching.  Using json4s, I am not sure what is a good alternative to avoid looking at all the fields recursively.. NM ^^^. Even though I did change that bit it was not to use iglu core. Can you give me an idea on how it would be used here? . Done. There is some sort of bug in the plugin when using it with ivy. I could only get it to work with coursier. . Agreed. In this instance the alignment is a bit weird. There are three options: \n1. Change settings in scalafmt for all files\n2. Switch off formatting locally there\n3. In this particular case, move theimport outputs.EnrichedEvent``` between the two multi imports so that scalafmt won't try to align them.. I think it is useful as a \"marker trait\" enabling things like the EnrichmentMap here: https://github.com/snowplow/snowplow/blob/362ebb527d4c392afc553615bd3c4a36230778b0/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/package.scala#L65\nbut I may be wrong. Any opportunity to remove reflection is worth looking into. I created https://github.com/snowplow/snowplow/issues/3610. It will affect formatting in all files. If you want to experiment with it, just modify .scalafmt.conf on the root of SCE and run sbt clean compile. Then you can see the changes with git diff.. @BenFradet I am not sure what the GH recommended max width is. 120 sounds good to me, although it will cause some reformatting (e.g. 3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/adapters/registry/UrbanAirshipAdapter.scala L53)\n. Not sure what you mean by moving the declarations up in this case. Can you be more specific?. Not a bad idea about the pii. The File was getting crowded and wanted a good solution to move all these constructs out. Pii seems like a good idea\n. As far as I can tell from bench marking this is not more performant. I tried your example and it was actually slower. . It is not but it should be used . As above. Probably. If you get the whole field, you may have more information which could be useful in a long config, but having only the name would make a better error message. Changing it. . This comment was lost here due to the update. I just found it in my emails. @chuwy What do you think about this formatting string? The objective is to keep the output at constant length. The byte array should have always the same length for the same algorithm. . That is done here: https://github.com/snowplow/snowplow/issues/3580 \n. I don't know. That file did not exist before 2017 but if we are talking about the project then yes. I don't know what the right years to put in are. @alexanderdean ?. Ok I'll change it then. Hey, sorry i did not respond to that comment. I thought that this is all resolved (https://github.com/snowplow/snowplow/pull/3599#issuecomment-365356824). Yes that was addressed along with all other formatting. I don't know all the settings, but I think it only sorts lists of imports from the same package. (e.g. import blah.{a, b, c} ). you mean for the types. Actually in the config they are called pojo and json. I can add the clarification. . We have asked scalafmt to align all ~ tokens (see .scalafmt.conf). It seems token here is aligned with something.. I am not sure what. I will try to see what happens if I remove the ~ from scalafmt config. Oh I see, it was supposed to be either. The result is this:\n-              ))            ~\n+              )) ~. Which alignment are you referring to in the above code?. @chuwy This doesn't seem to work:\niiPseudonymizerEnrichment.scala:88: ambiguous reference to overloaded definition,\n[error] both method md2Hex in object DigestUtils of type (x$1: String)String\n[error] and  method md2Hex in object DigestUtils of type (x$1: java.io.InputStream)String\n[error] match expected type ?\n[error]       case \"MD2\"     => { DigestUtils.md2Hex }.success. I don't think that was the problem:\nPiiPseudonymizerEnrichment.scala:82: ambiguous reference to overloaded definition,\n[error] both method md2Hex in object DigestUtils of type (x$1: String)String\n[error] and  method md2Hex in object DigestUtils of type (x$1: java.io.InputStream)String\n[error] match expected type ?\n[error]       case \"MD2\"     => { DigestUtils.md2Hex _ }.success\nThe errors seem to suggest that because md2Hex is polymorphic the type of the argument is required.. case \"MD2\" => { DigestUtils.md2Hex(_: Array[Byte]) }.success is better. Thanks . the <- token is aligned in an unbroken run but because there is an intervening = there are effectively three aligned blocks. Formatters are not as clever as to fully understand the language syntax I don't think. For all those cases one can use:\n// format off\n...\n// format on. In my opinion you probably get better overall results using a formatter to ensure consistent readability when a project gets above a few KLOC than not, even if you occasionally have to manually manage the format. It does most of the work for you, ensures consistency, and minimises formatting work so that people can focus on structure and potential defects. Consistent formatting is not a self-cause the way I see it but a means to an end, and that end is to improve maintainability and better discern potential defects. If teams can minimise the time spent on tasks that can be automated, is that not a win? . Should I add scalafmt here too (already doing it in stream enrich)? . Thanks for that. The point was to use the MasterCfSpec (the lines above were not necessary). If I undo the code changes, the test (on good) fails. I thought that means that it matches the content of the field. Let me have another look. About bad, probably the test is redundant currently but there should not be any changes in implementation in the future that cause this to fail. I can remove it... This works but due to a bug in specs2 (https://github.com/etorreborre/specs2/issues/416) Changed. I have no problem changing it. @BenFradet The original (458f2e8cf0) commit uses format instead fo string interpolation. Any reason to prefer format here? . Done. \ud83d\udc4d . ft: Throwable => B looks a bit weird to me. Is the purpose of the provided function to use a throwable and return just B? . would this work for this use: \n```\nimport scala.util.{ Either, Failure, Left, Right, Success, Try }\nimplicit def eitherToTryA <: Exception, B: Try[B] = {\n  either match {\n    case Right(obj) => Success(obj)\n    case Left(err) => Failure(err)\n}\n}\nimplicit def tryToEitherA: Either[Throwable, A] = {\n  obj match {\n    case Success(something) => Right(something)\n    case Failure(err) => Left(err)\n  }\n}\n```\n(source: https://stackoverflow.com/questions/22532357/either-to-try-and-vice-versa-in-scala) \nIt is much easier to understand, for me. \ud83d\udc4d . nit: newline . nit: newline. It looks like it does.. Hey alignment is more important right? \ud83d\ude04 . Are you referring to the properties above? (key.deserializer etc.). I'll try it out. . Yes we could, but the test would take a lot longer. As you are waiting for three outputs, you would need to wait for timeoutSec * 3 in total (and of course timeoutSec cannot be too small). \n. \ud83d\udc4d . I think nonEmpty will not do the right thing is if the field is actually null. \ud83d\udc4d . As I don't know of the use case you have in mind (another test?) can you please tell me where exactly (package? SpecHelpers?) they need be extracted? Alternatively if you have an additional use case, I can refactor and add the new test to this release (lets create a ticket in this milestone for that). I think using tests like that more could be great as they will improve coverage. . \ud83d\udc4d . This is easy to workaround by splitting the case. Not sure about limiting vertical alignments. Fixed here. There is a rule for that I have added for that in scalastyle, so soon we will get the check.. Fixed. Fixed. Fixed. Changed\n. Sure I can use the specs2 matcher. So that I understand the issue, can you please explain how it defeats the purpose of having futures in the first place?. I don't think I understand.\nCurrently there are three parallel background tasks that will complete when they receive the number of results that they are expecting. As this may never happen there needs to be a timeout. I am sure you realise that the future returns before the timeout when the result is ready. If we were waiting for the three results synchronously we would need to wait for at least as long as the timeout (45 sec). Actually the consumers return in about 2 sec. \nSo the objective here is to wait for a result that we don't know when it is coming (but usually pretty quickly in the successful case), but not wait forever, to do it in parallel for the three results expected so as not to delay the test unnecessarily. If the code in the future throws an exception, I see that exception in the Try. If the code times out it throws so the try is needed there. \nThese results are needed to complete the test evaluation so they need to be awaited. \nI am happy to use the specs2 matcher (which looks almost identical in code):\n/**\n   * when a Future contains a result, it can be awaited to return this result\n   */\n  implicit class futureAsResult[T : AsResult](f: Future[T]) {\n    def await: Result = await()\n    def await(retries: Int = 0, timeout: FiniteDuration = 1.seconds): Result = {\n      def awaitFor(retries: Int, totalDuration: FiniteDuration = 0.seconds): Result = {\n        try Await.result(f.map(value => AsResult(value)), timeout)\n        catch {\n          case e: TimeoutException =>\n            if (retries <= 0) Failure(s\"Timeout after ${totalDuration + timeout}\")\n            else awaitFor(retries - 1, totalDuration + timeout)\n          case other: Throwable    => throw other\n        }\n      }\n      awaitFor(retries)\n    }\n  }\nhowever since I still don't understand what you mean by \"lose the purpose of the future\" I don't know if it will address your concern. I think this will be easier over HipChat.. \n. Recapping @BenFradet we will use specs2 Matcher.await instead of Await.result to give the option to not block for the output of the job to whoever is implementing another kafka test in the future.. This was changed: https://github.com/snowplow/snowplow/pull/3646/files#diff-e405d498f6ff411d9e07f91778055e48R173 I think you are looking at an outdated version.. . Not sure if it's applicable, but set -e \"is considered harmful\" :-) \nSee http://mywiki.wooledge.org/BashFAQ/105. I am not sure how important that is here but you could instead do:\nids = Hash.new(0)\nand at L273:\nids[id] += 1\nand L278:\nduplicate_ids = ids.select { |_, c| c>1}.keys\nwill save a few iterations. \ud83d\udc4d . Here and L744:\nConsider changing the + unless to use a variable or the ternary operator to make more readable. . No that doesn't compile. Done (good point although readability is worse now if you ask me) . You mean if the schema is not a string? I suppose its conceivable. Fixed anyway. Given that the removal of package object for milestone 2.14 of the scala project hasn't been agreed yet and that we have another package object (package object common) I am inclined to wait till that gets agreed to modify this.. . ok. ok. Which one?. I will do a final rebase once we are done with this PR (I haven't looked at r105 yet). No. The default one caused the test to hang. In order to minimise test run time. . There are futures that have only some effect and don't return anything.. . ok. sure. This how it is done in scala-common-enrich (it uses an env var to override the default iglu repo). I am not sure how it should be done otherwise (I don't understand all/no env vars).. I am assuming that you are referring to the lines below if(successesTriggeredFlush == true.... Is that correct?. Ok I understood now. So we can definitely check pii for size, but the question is, what to do if the are too big (they are still PII so you we cant just send them to bad s that would leak PII)?. ok. @alexanderdean Any suggestions?. I can see the following options:\n\nDo nothing: As this is using the small enough Enriched Events to emit PII we can assume that the PII event is going to be smaller than the parent event. This is a reasonable assumption IMHO. \nUse the same limit and test PII separately which then creates the following options for oversize PII events: \n\n\nIf PII events are bigger than the limit, throw them away (will create data gaps in piinguin)\n\n\n\n\nSend them to the existing bad stream. That is problematic as it would leak PII to the bad stream, unless we somehow scrub the original values. Sounds like a much hassle for no actual benefit as there would still need to be a lot of manual work to recover.\n\n\n\n\nSend the parent event _id and an appropriate message to bad (but if we are doing that, why not send it to PII and have alternative handling where we look up the parent event and do PII backfilling).\n\n\n\n\nCome up with an EnrichedEvent fragmentation protocol to handle all cases where the event is too large, where they are fragmented and reassembled. This may seem like a good option and it certainly will avoid dropping data, but it is problematic in that most (all?) projects operate on single events at the moment.. Memo: https://github.com/snowplow/snowplow/issues/3755\n\n\n\nProposal: https://github.com/snowplow/snowplow/issues/3756\n\ud83d\udc4d . Fixed in release branch \n. Typo wihtoutCruft -> withoutCruft. Early version leftover. So what would you do instead? . Not quite sure what you mean. You mean creating the producer on NsqSource instantiation? . -X GET is probably  superfluous. I am assuming we should change the copyright year here. This looks like commented out code. We commonly call this output \"good\" instead of \"output\". . This is a very long def. I would move the trains to defs with descriptive names.. Undersized sounds like there is something wrong with them (unless I misunderstood the code and there is something wrong). I don't understand this number (/(4*10)) in this context (I am thinking of 39 max bytes for instance). Could we make it a constant with a descriptive name? Also, would it not be better to guarantee some fields make it to the truncated BadRow (for instance eventId)?. It looks like this should be handled by the def above resizeEnrichedEvent . Is that different than string.getBytes(UTF_8).size ?. This entry may exist but that does not mean it is a symlink. Also as per documentation notExists: \"method is not the complement of the exists method. Where it is not possible to determine if a file exists or not then both methods return false.\". ",
    "ludwigm": "This ticket is quite old and security best practices for AWS state to not hard-code those secret keys but to use IAM roles with instance profiles or other kind of temporary tokens. I tried to track this problem down and one thing I found is the lacking support for that in Sluice as already mentioned. Any updated on this? Is it on some roadmap? Quite an important feature for us to make the security right. Together with the documentation of the IAM permissions needed which do not confirm with the principle of least privilege it is even more of a problem: https://discourse.snowplowanalytics.com/t/what-is-the-minimum-viable-iam-policy-for-snowplow-operation/192. Awesome. Many thanks for the fast reply. That already helps us to plan ahead :). We also have a professional subscription and figured now that it caches and basically ignores updated files in S3. It was not totally obvious and would be great if this is added to the docs or even better a periodic check to S3 is added. We are for now cleaning the local files and restart the service to always have up to date data. ",
    "AALEKH": "Done !!\n. Waiting for your comment for this part specifically !! (Test Case for wrong AppId)\n. Implicit return does not seems to be a member of Unit, thus causes problem with toValidaitonNel\n. Ah, Sorry forgot to push one file\n. '''check''' is not needed here, Sorry I forgot to remove .\n. Ah, ok lat , long, timestamp will be from other schema.\n. ",
    "MrCurtis": "The basic functionality should be working on this branch - i.e. you should be able to stream from kinesis to bigquery assuming there is no problems with the data.\n. ",
    "natilivni": "Hi @alexanderdean, the filename and location are correct, but it needs a valid policy file syntax.  (reference is here: http://www.adobe.com/devnet/articles/crossdomain_policy_file_spec.html )\nThe most open crossdomain file looks like this:\nxml\n<cross-domain-policy>\n  <allow-access-from domain=\"*\" secure=\"false\" />\n</cross-domain-policy>\n. Perfect!  Thanks!\n. ",
    "achrefss": "I have the same exception using the Scala Collector with stdout config options set.\nAny Idea please?\n. ",
    "sr-ix": "Ahh, did not see that there was a new version of the collector.\nThe link to the CDN download on the Hosted Assets page still points to version 0.1.0\n. :tada: I like it.\n. ",
    "duncan": "@alexanderdean I haven't yet signed a CLA. Checking with our peeps to see if I can do that or if one of our officers need to do it.\nAlso, do y'all have any plans to pull the Scala collector out into its own repo? To deploy into our infra, we build from source and pulling in all of snowplow/snowplow and what not is a bit of a juggle. I don\u2019t mind it, but if it\u2019s going to go into its own repo\u2014like the trackers\u2014I won\u2019t put the time in on our side to work about it. \n. Ok, CLA is signed, so that should be good to go. As to the sub repos\u2014yeah. Dealing with submodules is a royal pain, so I\u2019m not actively arguing that you should do that... :smile: Just good to know that the things we need to do on our side are worthy to do now rather than later. Thanks! And thanks for the link to Peru. I'll give it a look.\n. I considered using /i as the health check but didn't want to generate events off that. :) Thanks @alexanderdean for bringing it forward and putting it in the release. I'll rebase our install!\n. getMilliseconds was deprecated. \n. ",
    "bogaert": "That is very interesting! Looking forward to this.\n. @alexanderdean Yep, this can be closed.. Indeed Alex. The purpose of this page view ID is to make the aggregation of page view and ping events into a single row easier. The page view event would have a new UUID, and each subsequent page ping on that page would have the same UUID (until a new page view event occurs).\nFor example: If a user opens 2 different pages in 2 tabs, and switches between them, the page ping events will alternate. While it's possible to roll these events up into the original page view event during the data modeling step, it's (needlessly) complicated to cover all edge cases in SQL. It's also an issue when the derived views are updated using only incremental data.\nThe page view UUID would make this all a lot simpler.\n. This has been implemented.\n. Interesting. I'll update the encoding when R69 is done.\nChristophe\nSnowplow Analytics http://snowplowanalytics.com/\nThe Roma Building, 32-38 Scrutton Street, London EC2A 4RQ, United Kingdom\n+44 (0) 203 589 6116\n+44 (0) 7598 006 851\nOn Wed, Jul 15, 2015 at 7:12 AM, Yali notifications@github.com wrote:\n\nLooks like for users with lots of custom context, the 'contexts' field\naccounts for almost 50% of the space taken, so fingers crossed switching\nthe encoding to lzo will result in big space savings...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1846#issuecomment-121502264.\n. For future reference: http://docs.aws.amazon.com/redshift/latest/dg/t_Verifying_data_compression.html\n\nIn almost all cases encode lzo has by far the best compression.\n. Depending on the user, it looks like updating the encoding can reduce disk usage between 15% (mobile app - so few columns in the main events table are used) and 45%.\n. Yep. Closing.. Replaced with a Discourse tutorial: http://discourse.snowplowanalytics.com/t/making-sql-data-models-incremental-to-improve-performance-tutorial/319\n. Closing because not relevant anymore.\n. Closing because not relevant anymore.\n. Closing because not relevant anymore.\n. Closing because not relevant anymore.\n. Closing because not relevant anymore.\n. Thanks @alexanderdean.\nI discussed this with @yalisassoon this morning, and the idea was to have this as a SQL Runner step after each load. Natural copies get deduplicated, while synthetic copies get moved to a separate events table.\n. I had another look at the original email, and I'm surprised by the versions\nnumbers in the GA screenshot (Opera is on version 28 now), so I would\nignore that part of the email. It appears that our parsing was correct in\nthe past, but the useragent changed when Opera moved to Blink/Chromium.\n\u200b\nThese parse correctly:\n- Opera/9.80 (Windows NT 6.1) Presto/2.12.388 Version/12.17\n- Opera/9.80 (Windows NT 6.2; WOW64) Presto/2.12.388 Version/12.17\n- Opera/9.80 (Android; Opera Mini/8.0.1739/35.6680; U; es) Presto/2.8.119 Version/11.10\nThese more recent ones return NULL:\n- Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60\n- Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60\n- Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60 (Edition Yx)\n- Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60\nChristophe\n. Thanks @fblundun.\n. That sounds good.\nNatural copies will have the same fingerprint, so that case is covered. What are the options when it comes to dealing with synthetic copies (in particular: same event ID, different event)? The first event will have passed through, but we might want to exclude it from analysis as well because something fishy is going on.\n. Yes. Closing.. Thanks @fblundun. I would only exclude dvce_sent_tstamp, unless there's anything I missed.\n. @fblundun dvce_tstamp is now also renamed in the SQL queries.\n. @alexanderdean Is the idea to keep this page? http://snowplowanalytics.com/no-js-tracker.html\nI don't think the form works anymore.\n. Just as an FYI. There are rare cases where events have the same event_fingerprint but a different event_id. I came across events sent in by this bot:\nMozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)\nMost fields (including dvce_created_tstamp) were NULL, except for ip_address and useragent (thus producing the same fingerprint).\n. There are a few more queries I want to add (see #2110). Expect this to be done later this morning.\n. This is now ready for release @alexanderdean (together with #2110).\n. Hi @bernardosrulzon. Thanks a lot for this PR. I have, however, closed it because these SQL queries will replaced with brand new ones in R72.\n. I thought this hadn't been done yet, but it is part of R71 (#2024).\n. That is done (cc @alexanderdean).\n. Hi @bernardosrulzon. Those changes look good to me (web-recalculate hasn't been updated in a while). It'd be great if you could:\n- rebase of master to resolve the merge conflicts\n- rebase the commits to map onto the following 2 tickets:\nhttps://github.com/snowplow/snowplow/issues/2274\nhttps://github.com/snowplow/snowplow/issues/2275\nThe changes will then be included in our next release.\nThanks for contributing!\n. Thanks @bernardosrulzon!\n. Hi @bernardosrulzon. These commits were included in R75: http://snowplowanalytics.com/blog/2016/01/02/snowplow-r75-long-legged-buzzard-released/#datamodel\n. > A Snowplow enrichment can run many millions of time per hour, effectively launching a DoS attack on a data source if we are not careful. The cache configuration attempts to minimize the number of lookups performed.\nThat makes sense - but I'm still a bit worried about Redshift. It support at most 50 concurrent queries (but is configured by default to only 5), so even running 500 queries would be a heavy load.\n. It broke 3 days ago when GitHub upgraded to the latest version of Jekyll.\n. We need to test whether this leads to tables that are more unsorted after inserting new events.\n. It'd be interesting to explore the impact of this patch: https://forums.aws.amazon.com/ann.jspa?annID=4157\n\nData loading enhancement. If you load your data in sort key order using a compound sort key with only one sort column, you might now reduce or even eliminate the need to vacuum as the COPY command automatically adds new rows in sort order to the table's sorted region\n\nThis should eliminate the need to vacuum if we continue to use the collector timestamp, and reduce (but not eliminate) this need if we were to switch to the derived timestamp.. Hi @bernardosrulzon. I agree that there is a lot of room for improvement. We will rethink some of these SQL data models in the near future, and, in the meantime, feel free to open a PR with these changes.\n. We can close this ticket @alexanderdean.. Should we replace \"unstructured events\" with \"self-describing events (also called unstructured events)\"? We'll have to start somewhere and this seems like a good place.\ncc @yalisassoon \n. We should keep the concept of event dictionaries (it's something few users do, and it hurts when someone else needs to consume the data).\nWhether we make it part of the registries or not is a separate discussion. I would let it live in the same place as the schemas.\n. The particular example is Firefox 4X being parsed as Firefox 4.\n. That all sounds good.\n. I understand the difference, but we wouldn't rename domain_userid to user_id if we were to move it to a context. These are all possible user identifiers, just like the IP address and user fingerprint.\nMost users will look at what the field is called, rather than look at the documentation (which can be hard to find). We should therefore make the field names as descriptive as possible, in my opinion.\n. I'd do the breaking change. It's a small thing, but something I have had to explain each time the client session context has come up. A good time to do the change might be to do it when we move domain_userid into the client_session context (and most users will have to update their queries).\nA couple of ideas:\n- install_userid (perhaps too specific to iOS and Android)\n- tracker_userid\n- snowplow_userid\n. This came up again this week - users that have more than one tracker struggle to understand what tracker supports what (so this would be a direct win for users).\n. Hi @bernardosrulzon - thanks for the contribution!\nThis is indeed a good way to speed up the queries once you have deduplicated the tables a first time. To do so, you'll still need to run the queries without any restrictions on the timestamp. Once that is done, almost all duplicates will indeed be in the last month or so.\nI'd be happy to merge these change, but I'd switch the default to having the restrictions commented out - what do you think? If you agree, could you commit these changes to the feature/web-models branch? That way it will go out with the next release. The folders in that branch have been restructured a little, but the deduplication queries remain the same.\n. Hi @bernardosrulzon.\nWe have built a new web model that does a lot more than the old model (e.g. it calculates scroll depth and page performance). I'll commit it to the feature/web-models repo later today. An incremental version will probably follow at some point, but we haven't started on that one yet.\n. Thanks @bernardosrulzon. I'll review the commits and merge the PR.\n. Hi @Germanaz0,\nThe best place to raise questions like this is on our forums: http://discourse.snowplowanalytics.com (GitHub issues are for tracking actual issues).\nChristophe\n. This was pushed back to a later release.. Hi @iaingray,\nWe started work on this several months ago but it was quickly put on hold. I believe we wrote most of the Python script that generates the deduplication queries and SQL Runner playbook (please confirm, @dilyand).\nHowever, there is a critical piece of extra work that needs to happen before we can release this and that's to significantly improve the deduplication queries. The two main shortcomings right now are:\n\npoor deduplication of the child tables (naive algorithm); and\nno solution for synthetic duplicates.\n\nThe child tables are currently deduplicated by simply grouping the fields in each child table. This method cannot distinguish between natural and synthetic duplicates (for which we need the event fingerprint).\nThe way the queries deal with synthetic duplicates (same event ID, different event fingerprint) is by simply quarantining them. We need a way to re-assign event ID in both the events table and child tables simultaneously.\nThis makes it a significantly larger exercise and\u2013given our improvements to the Hadoop-based deduplication process in Snowplow 86 and 88\u2013we decided to put this on hold.\nIf you're interested in having a crack at it, feel free to do so! Happy to advise along the way.. Hi @radubogdan. I don't think ON DELETE CASCADE is available in Redshift?. @alexanderdean Yep. Go ahead and close.. That's correct. Reassigning to @dilyand to commit a fix.. Hi @joshuacox,\nI get a 404 when I go to this page. Is it possible you meant to post a different link?\nIn any case, those views were removed in an earlier version of Snowplow. The recipes themselves were quite basic and no longer in line we how we think about event analytics. Instead, we have put our effort into developing much more powerful and useful event data models (like the one you linked to).\nYou can still find the original SQL queries in this post on Discourse: http://discourse.snowplowanalytics.com/t/basic-sql-recipes-for-web-data/25\nFor more information on event data modeling, check out this post: http://discourse.snowplowanalytics.com/t/what-is-event-data-modeling/56\nHope this helps!. Thanks! I have updated that section of the wiki to remove all references to the old recipes.\nIf you prefer, the views can be recreated the views by simply wrapping the queries in a CREATE VIEW statement. If you have any other questions about the views themselves or data models, please post them on our Discourse. Thanks :). Sounds good. I'll schedule this in.. How would we go about doing this?\n\nDo we need a \"release\" to take data modeling out of Snowplow core?\nDo we want a single \"data modeling\" repo or one per model? I'd start with one repo.. Sounds good. Will get this process started.. FYI, one of our customers asked for this exact feature last week. Running the risk that events fail validation when the API is unavailable makes the enrichment less attractive.. Super excited about this!. \n",
    "christiangda": "Hi Alexander, Thanks for your answer, you are right, I forgot mention that we don't have any error, we only have this information:\ngrunt> DUMP data;\n2015-02-20 08:50:51,827 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN\n2015-02-20 08:50:51,884 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, PartitionFilterOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter], RULES_DISABLED=[FilterLogicExpressionSimplifier]}\n2015-02-20 08:50:52,463 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n2015-02-20 08:50:52,464 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - fs.default.name is deprecated. Instead, use fs.defaultFS\n2015-02-20 08:50:52,629 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2015-02-20 08:50:52,629 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive\n2015-02-20 08:50:52,631 [main] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt__0001_m_000001_1' to file:/tmp/temp1145700320/tmp-1189644891/_temporary/0/task__0001_m_000001\n2015-02-20 08:50:52,648 [main] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n2015-02-20 08:50:52,657 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input paths to process : 1\n2015-02-20 08:50:52,661 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\nWe have searched in all the logs of process, system and pig, and we haven't any errors,  but when we run the command DUMP, we have no data !\nTools version\n\nApache Pig version 0.13.0 (r1606446) compiled Jun 29 2014, 02:27:58\nLinux hadoop-dev 3.13.0-45-generic #74-Ubuntu SMP Tue Jan 13 19:36:28 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nHadoop 2.4.0\nSubversion http://svn.apache.org/repos/asf/hadoop/common -r 1583262\nCompiled by jenkins on 2014-03-31T08:29Z\nCompiled with protoc 2.5.0\nFrom source with checksum 375b2832a6641759c6eaf6e3e998147\nThis command was run using /usr/local/hadoop-2.4.0/share/hadoop/common/hadoop-common-2.4.0.jar\n\n\n\nScala collector log trace\n08:33:31.495 [scala-stream-collector-akka.actor.default-dispatcher-13] DEBUG akka.io.TcpListener - New connection accepted\n08:33:31.502 [scala-stream-collector-akka.actor.default-dispatcher-12] DEBUG s.can.server.HttpServerConnection - Dispatching GET request to http://192.168.100.110:8080/i?e=ue&ue_px=eyJzY2hlbWEiOiJpZ2x1OmNvbS5zbm93cGxvd2FuYWx5dGljcy5zbm93cGxvdy91bnN0cnVjdF9ldmVudC9qc29uc2NoZW1hLzEtMC0wIiwiZGF0YSI6eyJkYXRhIjp7InVzZXJfaWQiOjk4ODMxOSwic2Vzc2lvbl9pZCI6MTQxMDYsImFjdGlvbiI6InNlc3Npb25fZW5kIiwic29mdHdhcmUiOiJGUkFNME5CR0pLIiwidmVyc2lvbiI6InYgMS4xLjE0IiwiY2FtcGFpZ24iOiJjYW1wYWlnbiIsIm9sZF92ZXJzaW9uIjoidiAxLjEuMTQifX19&dtm=1424421211442&tid=101143&vp=1855x993&ds=1855x993&vid=1&duid=954c0aa191bae4fa&p=web&tv=js-2.0.0&fp=1941551860&lang=en-us&cs=UTF-8&tz=Europe/London&f_pdf=1&f_qt=0&f_realp=0&f_wma=0&f_dir=0&f_fla=1&f_java=1&f_gears=0&f_ag=0&res=1920x1080&cd=24&cookie=1&url=file:///home/christian/Documents/Tuguu/LTV/SnowPlow/1-tracker/web-js-tracker/tgu-track.1.html to handler Actor[akka://scala-stream-collector/system/IO-TCP/selectors/$a/24#-340979721]\n08:33:31.503 [scala-stream-collector-akka.actor.default-dispatcher-13] INFO  c.s.s.c.s.sinks.KinesisSink - Writing Thrift record to Kinesis: CollectorPayload(schema:iglu:com.snowplowanalytics.snowplow/CollectorPayload/thrift/1-0-0, ipAddress:192.168.100.1, timestamp:1424421211503, encoding:UTF-8, collector:ssc-0.3.0-kinesis, userAgent:Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.111 Safari/537.36, path:/i, querystring:e=ue&ue_px=eyJzY2hlbWEiOiJpZ2x1OmNvbS5zbm93cGxvd2FuYWx5dGljcy5zbm93cGxvdy91bnN0cnVjdF9ldmVudC9qc29uc2NoZW1hLzEtMC0wIiwiZGF0YSI6eyJkYXRhIjp7InVzZXJfaWQiOjk4ODMxOSwic2Vzc2lvbl9pZCI6MTQxMDYsImFjdGlvbiI6InNlc3Npb25fZW5kIiwic29mdHdhcmUiOiJGUkFNME5CR0pLIiwidmVyc2lvbiI6InYgMS4xLjE0IiwiY2FtcGFpZ24iOiJjYW1wYWlnbiIsIm9sZF92ZXJzaW9uIjoidiAxLjEuMTQifX19&dtm=1424421211442&tid=101143&vp=1855x993&ds=1855x993&vid=1&duid=954c0aa191bae4fa&p=web&tv=js-2.0.0&fp=1941551860&lang=en-us&cs=UTF-8&tz=Europe%2FLondon&f_pdf=1&f_qt=0&f_realp=0&f_wma=0&f_dir=0&f_fla=1&f_java=1&f_gears=0&f_ag=0&res=1920x1080&cd=24&cookie=1&url=file%3A%2F%2F%2Fhome%2Fchristian%2FDocuments%2FTuguu%2FLTV%2FSnowPlow%2F1-tracker%2Fweb-js-tracker%2Ftgu-track.1.html, headers:[X-Forwarded-For: 192.168.100.1, Cookie: sp=c124c4b8-9f3a-475f-9491-af2d772e045b, Accept-Language: es, en-US, en, Accept-Encoding: gzip, deflate, sdch, User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.111 Safari/537.36, Accept: image/webp, */*;q=0.8, Host: 192.168.100.110:8080], hostname:192.168.100.110, networkUserId:c124c4b8-9f3a-475f-9491-af2d772e045b)\n08:33:31.508 [scala-stream-collector-akka.actor.default-dispatcher-9] DEBUG akka.io.TcpIncomingConnection - Closing connection due to IO error java.io.IOException: Connection reset by peer\n08:33:31.509 [scala-stream-collector-akka.actor.default-dispatcher-9] DEBUG s.can.server.HttpServerConnection - Connection was ErrorClosed(Connection reset by peer), awaiting TcpConnection termination...\n08:33:31.510 [scala-stream-collector-akka.actor.default-dispatcher-12] DEBUG s.can.server.HttpServerConnection - TcpConnection terminated, stopping\n08:33:31.672 [pool-1-thread-10] INFO  c.s.s.c.s.sinks.KinesisSink - Writing successful.\n08:33:31.672 [pool-1-thread-10] INFO  c.s.s.c.s.sinks.KinesisSink -   + ShardId: shardId-000000000000\n08:33:31.672 [pool-1-thread-10] INFO  c.s.s.c.s.sinks.KinesisSink -   + SequenceNumber: 49548077043996487777971171182938030608387103198025875458\nkinesis-lzo-s3-sink log trace\nfeb 20, 2015 8:34:31 AM com.amazonaws.services.kinesis.metrics.impl.DefaultCWMetricsPublisher publishMetrics\nINFORMACI\u00d3N: Successfully published 14 datums.\nfeb 20, 2015 8:34:41 AM com.amazonaws.services.kinesis.metrics.impl.DefaultCWMetricsPublisher publishMetrics\nINFORMACI\u00d3N: Successfully published 12 datums.\nfeb 20, 2015 8:34:52 AM com.amazonaws.services.kinesis.metrics.impl.DefaultCWMetricsPublisher publishMetrics\nINFORMACI\u00d3N: Successfully published 12 datums.\nfeb 20, 2015 8:34:57 AM com.amazonaws.services.kinesis.clientlibrary.lib.worker.Worker$WorkerLog info\nINFORMACI\u00d3N: Current stream shard assignments: shardId-000000000000\nfeb 20, 2015 8:34:57 AM com.amazonaws.services.kinesis.clientlibrary.lib.worker.Worker$WorkerLog info\nData files in S3 and downloaded to my FS\n-rw------- 1 devuser devuser 2877 Feb 20 08:44 2015-02-20-49548077043996487777971171182912643166175195023284371458-49548077043996487777971171182939239534206717827200581634.lzo\n-rw------- 1 devuser devuser    8 Feb 20 08:44 2015-02-20-49548077043996487777971171182912643166175195023284371458-49548077043996487777971171182939239534206717827200581634.lzo.index\nIt is possible to do that?,  I mean ,  load \"Thrift-Lzo SnowPlow format\" from Apache Pig using Elephant-bird library?\nThanks for your comments\n. Thanks again Alexander!, we will try your suggestion, and then we will inform to you the result!\n. jejeje, yes I know that this work, but what happens if I want to know about the \"ERROR\" log level?\n. nice, is time to say \n- It is a bug or not?\n- the documentation say the true or not?\n- the config file work for login level or not?\nThis is what I want to know!, not all the possible combination of \"trace log\" that is not in the documentation!.\nAnd your suggestion is not efficient for me!,  maybe it work for a develop environment.\n. ",
    "waterlink": "Actually the best way to utilize contracts is to include them in classes and modules that actually need them, explicitly. That way you don't need to change any code, except adding these include Contracts to classes/modules, that have contracts inside them; and removing this inclusion from global scope.\nI imagine that, if you would like to just namespace Contracts, then you will need to namespace any builtin contracts references too.\n. ",
    "sparrovv": "Got it, no worries.\n. Hey @alexanderdean I've submitted both CLA forms. Hope it's enough ?\n. ",
    "gabrielcrowdtilt": "I was failing on this for a couple hours while setting up for first time today:\n[main] INFO com.amazonaws.services.kinesis.clientlibrary.lib.worker.Worker - Initialization attempt 1\n[main] INFO com.amazonaws.services.kinesis.clientlibrary.lib.worker.Worker - Initializing LeaseCoordinator\n[main] ERROR com.amazonaws.services.kinesis.clientlibrary.lib.worker.Worker - Caught exception when initializing LeaseCoordinator\nThen I switched from iam to env and it started working right away. The same iam worked for scala-stream-collector-0.4.0 though. \n. ",
    "drewgillson": "Perfect, thanks @fblundun. I'm not sure how I missed that step (to set up the mapping) but I guess I did. Works like a charm.\n. ",
    "gabhi": "I was trying to following quickstart https://github.com/snowplow/snowplow#quickstart\nso i guess scala-common-enrich\n. yes it worked now. thanks @alexanderdean \n. ",
    "anishdevasia": "Thanks for quick reply.\nI am using following components to setup kinesis pipeline\n1. Scala Collector(snowplow-stream-collector-0.3.0)\n2. Scala Enrichment( snowplow-kinesis-enrich-0.3.0)\n3. S3 Sink (snowplow-lzo-s3-sink-0.1.0)\nAll these steps works perfectly and I am getting \"lzo\" files in S3.\nAt 4th setup where I try to load \"lzo\" files from S3 to Redshift using Storage loader I am getting the above reported error.\n. I think, I should have posted this issue in the forum first.\n. ",
    "falschparker82": "For this branch, I'm getting an\n```\n~/snowplow/3-enrich/emr-etl-runner$ bundle exec bin/snowplow-emr-etl-runner --config config/config.yml --enrichments config/enrichments\n[2015-04-09T15:07:32.385625 #7454] FATAL -- : \nContractError (Contract violation for return value:\n    Expected: {:logging=>{:level=>String}, :aws=>{:access_key_id=>String, :secret_access_key=>String}, :s3=>{:region=>String, :buckets=>{:assets=>String, :log=>String, :raw=>{:in=>String, :processing=>String, :archive=>String}, :enriched=>{:good=>String, :bad=>String, :errors=>String or }, :shredded=>{:good=>String, :bad=>String, :errors=>String or }}}, :emr=>{:ami_version=>String, :region=>String, :service_role=>String, :placement=>String or , :ec2_subnet_id=>String or , :ec2_key_name=>String, :bootstrap=>an array of String, :software=>{:hbase=>String or , :lingual=>String or }, :jobflow=>{:master_instance_type=>String, :core_instance_count=>Contracts::Num, :core_instance_type=>String, :task_instance_count=>Contracts::Num, :task_instance_type=>String, :task_instance_bid=>Contracts::Num or }}, :etl=>{:job_name=>String, :versions=>{:hadoop_enrich=>String, :hadoop_shred=>String}, :collector_format=>String, :continue_on_unexpected_error=>Contracts::Bool}, :iglu=>{:schema=>String, :data=>{:cache_size=>Contracts::Num, :repositories=>an array of {:name=>String, :priority=>Contracts::Num, :vendor_prefixes=>an array of String, :connection=>{:http=>{:uri=>String}}}}}},\n    Actual: {:logging=>{:level=>\"DEBUG\"}, :aws=>{:access_key_id=>\"AKIAIOVAVM4J6EDQ6FSA\", :secret_access_key=>\"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\"}, :s3=>{:region=>\"eu-west-1\", :buckets=>{:assets=>\"s3://snowplow-hosted-assets\", :log=>\"s3://mc-snowplow-etl/logs\", :raw=>{:in=>\"s3://mc-snowplow-logs\", :processing=>\"s3://mc-snowplow-logs/processing\", :archive=>\"s3://mc-snowplow-archive/raw\"}, :enriched=>{:good=>\"s3://mc-snowplow-logs-out/enriched/good\", :bad=>\"s3://mc-snowplow-logs-out/enriched/bad\"}, :shredded=>{:good=>\"s3://mc-snowplow-logs-out/shredded/good\", :bad=>\"s3://mc-snowplow-logs-out/shredded/bad\"}}}, :emr=>{:ami_version=>\"2.4.11\", :region=>\"eu-west-1\", :ec2_subnet_id=>\"subnet-923f9acb\", :ec2_key_name=>\"snowplow\", :bootstrap=>[], :software=>{:hbase=>nil, :lingual=>nil}, :jobflow=>{:master_instance_type=>\"m1.small\", :core_instance_count=>2, :core_instance_type=>\"m1.small\", :task_instance_count=>0, :task_instance_type=>\"m1.small\"}}, :etl=>{:job_name=>\"Snowplow ETL\", :versions=>{:hadoop_enrich=>\"0.14.0\", :hadoop_shred=>\"0.4.0\"}, :collector_format=>\"cloudfront\", :continue_on_unexpected_error=>false}, :iglu=>{:schema=>\"iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-0\", :data=>{:cache_size=>500, :repositories=>[{:name=>\"Iglu Central\", :priority=>0, :vendor_prefixes=>[\"com.snowplowanalytics\"], :connection=>{:http=>{:uri=>\"http://iglucentral.com\"}}}]}}}\n    Value guarded in: Snowplow::EmrEtlRunner::Cli::load_file\n    With Contract: Maybe, String => Hash\n    At: /home/ubuntu/snowplow/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/cli.rb:123 ):\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts.rb:69:in block in <class:Contract>'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts.rb:147:incall'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts.rb:147:in failure_callback'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts/decorators.rb:164:inrescue in block in common_method_added'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts/decorators.rb:157:in block in common_method_added'\n    bin/snowplow-emr-etl-runner:37:in'\n```\n. Running into the same problem here, appreciate any updates. Can we help fix anything?\n. Hi Alex - thanks so much for the update... we're just getting started with Snowplow and so far, I find it a very well designed product, having built analytics pipelines by hand myself. Just wanted to take the opportunity to say thanks for all your effort so far (and the whole team's of course).\n. That's great! Can we help testing somehow? I don't see a referenced branch in this issue or a recent commit from you?\n. Hmmmm... I'm getting this on feature/role-fix branch (same on releases/r64-palila) and after a git hard reset - do you know if I need to do anything else?\n```\n$:~/snowplow/3-enrich/emr-etl-runner$ bundle install --deployment\nFetching gem metadata from https://rubygems.org/.........\nUsing CFPropertyList 2.3.0\nUsing awrence 0.1.0\nUsing builder 3.2.2\nUsing contracts 0.7\nUsing multi_json 1.11.0\nUsing mime-types 2.4.3\nUsing netrc 0.10.3\nUsing rest-client 1.7.3\nUsing docile 1.1.5\nUsing simplecov-html 0.9.0\nUsing simplecov 0.9.2\nUsing tins 1.3.5\nUsing term-ansicolor 1.3.0\nUsing thor 0.19.1\nUsing coveralls 0.7.11\nUsing diff-lcs 1.2.5\nUsing excon 0.44.4\nUsing formatador 0.2.5\nUsing net-ssh 2.9.2\nUsing net-scp 1.2.1\nUsing fog-core 1.29.0\nUsing mini_portile 0.6.2\nUsing nokogiri 1.6.6.2\nUsing fog-xml 0.1.1\nUsing fog-atmos 0.1.0\nUsing fog-json 1.0.0\nUsing ipaddress 0.8.0\nUsing fog-aws 0.1.1\nUsing inflecto 0.0.2\nUsing fog-brightbox 0.7.1\nUsing fog-ecloud 0.0.2\nUsing fog-profitbricks 0.0.1\nUsing fog-radosgw 0.0.3\nUsing fog-riakcs 0.1.0\nUsing fog-sakuracloud 1.0.0\nUsing fog-serverlove 0.1.1\nUsing fog-softlayer 0.4.1\nUsing fog-storm_on_demand 0.1.0\nUsing fog-terremark 0.0.4\nUsing fission 0.5.0\nUsing fog-vmfusion 0.0.1\nUsing fog-voxel 0.0.2\nUsing fog 1.28.0\nUsing unf_ext 0.0.6\nUsing unf 0.1.4\nInstalling elasticity 4.0.5\nUsing rspec-core 2.99.2\nUsing rspec-expectations 2.99.2\nUsing rspec-mocks 2.99.3\nUsing rspec 2.99.0\nUsing sluice 0.2.1\nUsing bundler 1.7.6\nYour bundle is complete!\nIt was installed into ./vendor/bundle\n$:~/snowplow/3-enrich/emr-etl-runner$ bundle exec bin/snowplow-emr-etl-runner --config config/config.yml --enrichments config/enrichments\nF, [2015-04-13T15:56:12.265655 #11588] FATAL -- : \nContractError (Contract violation for return value:\n    Expected: {:logging=>{:level=>String}, :aws=>{:access_key_id=>String, :secret_access_key=>String}, :s3=>{:region=>String, :buckets=>{:assets=>String, :log=>String, :raw=>{:in=>String, :processing=>String, :archive=>String}, :enriched=>{:good=>String, :bad=>String, :errors=>String or }, :shredded=>{:good=>String, :bad=>String, :errors=>String or }}}, :emr=>{:ami_version=>String, :region=>String, :jobflow_role=>String, :service_role=>String, :placement=>String or , :ec2_subnet_id=>String or , :ec2_key_name=>String, :bootstrap=>an array of String, :software=>{:hbase=>String or , :lingual=>String or }, :jobflow=>{:master_instance_type=>String, :core_instance_count=>Contracts::Num, :core_instance_type=>String, :task_instance_count=>Contracts::Num, :task_instance_type=>String, :task_instance_bid=>Contracts::Num or }}, :etl=>{:job_name=>String, :versions=>{:hadoop_enrich=>String, :hadoop_shred=>String}, :collector_format=>String, :continue_on_unexpected_error=>Contracts::Bool}, :iglu=>{:schema=>String, :data=>{:cache_size=>Contracts::Num, :repositories=>an array of {:name=>String, :priority=>Contracts::Num, :vendor_prefixes=>an array of String, :connection=>{:http=>{:uri=>String}}}}}},\n    Actual: {:logging=>{:level=>\"DEBUG\"}, :aws=>{:access_key_id=>\"AKIAIOVAVM4J6EDQ6FSA\", :secret_access_key=>\"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\"}, :s3=>{:region=>\"eu-west-1\", :buckets=>{:assets=>\"s3://snowplow-hosted-assets\", :log=>\"s3://mc-snowplow-etl/logs\", :raw=>{:in=>\"s3://mc-snowplow-logs\", :processing=>\"s3://mc-snowplow-logs/processing\", :archive=>\"s3://mc-snowplow-archive/raw\"}, :enriched=>{:good=>\"s3://mc-snowplow-logs-out/enriched/good\", :bad=>\"s3://mc-snowplow-logs-out/enriched/bad\"}, :shredded=>{:good=>\"s3://mc-snowplow-logs-out/shredded/good\", :bad=>\"s3://mc-snowplow-logs-out/shredded/bad\"}}}, :emr=>{:ami_version=>\"2.4.11\", :region=>\"eu-west-1\", :ec2_subnet_id=>\"subnet-923f9acb\", :ec2_key_name=>\"snowplow\", :bootstrap=>[], :software=>{:hbase=>nil, :lingual=>nil}, :jobflow=>{:master_instance_type=>\"m1.small\", :core_instance_count=>2, :core_instance_type=>\"m1.small\", :task_instance_count=>0, :task_instance_type=>\"m1.small\"}}, :etl=>{:job_name=>\"Snowplow ETL\", :versions=>{:hadoop_enrich=>\"0.14.0\", :hadoop_shred=>\"0.4.0\"}, :collector_format=>\"cloudfront\", :continue_on_unexpected_error=>false}, :iglu=>{:schema=>\"iglu:com.snowplowanalytics.iglu/resolver-config/jsonschema/1-0-0\", :data=>{:cache_size=>500, :repositories=>[{:name=>\"Iglu Central\", :priority=>0, :vendor_prefixes=>[\"com.snowplowanalytics\"], :connection=>{:http=>{:uri=>\"http://iglucentral.com\"}}}]}}}\n    Value guarded in: Snowplow::EmrEtlRunner::Cli::load_file\n    With Contract: Maybe, String => Hash\n    At: /home/ubuntu/snowplow/3-enrich/emr-etl-runner/lib/snowplow-emr-etl-runner/cli.rb:123 ):\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts.rb:69:in block in <class:Contract>'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts.rb:147:incall'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts.rb:147:in failure_callback'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts/decorators.rb:164:inrescue in block in common_method_added'\n    /home/ubuntu/snowplow/3-enrich/emr-etl-runner/vendor/bundle/ruby/1.9.1/gems/contracts-0.7/lib/contracts/decorators.rb:157:in block in common_method_added'\n    bin/snowplow-emr-etl-runner:37:in'\n$:~/snowplow/3-enrich/emr-etl-runner$ git log -1\ncommit 7385cabac7c4c6f72907769261a973325391be14\nAuthor: Alex Dean alex@snowplowanalytics.com\nDate:   Mon Apr 13 08:41:14 2015 +0100\nEmrEtlRunner: added support for specifying EMR service role (closes #1595)\n\n$:~/snowplow/3-enrich/emr-etl-runner$ git status\nOn branch feature/role-fix\nYour branch is up-to-date with 'origin/feature/role-fix'.\nnothing to commit, working directory clean\n```\n. that one did the trick, thanks Alex!\n. Thanks @alexanderdean , I appreciate it\n. Hi there,\nof course, the attack surface around this case is already reduced (otherwise I also would have disclosed more responsibly). We're talking about a company internal perimeter here, as the data warehouse itself is usually protected.\nHowever, just for us for example, while we have a very limited amount of employees with privileges on the Snowplow pipeline, the data warehouse itself just has loosely guarded internal access (i.e. pretty much anyone can create an account and make a query). We do this, because we've pseudonymized or anonymized all PII on our own, even before R100.\nThe attack scenario in the aforementioned case involves an internal actor with this loosely guarded access. Currently, for example, the IP address is anonymized by truncating the last octet. This is pretty much irreversible. I just wanted to show, that pseudonymizing the IP address by hashing instead might actually make matters worse, as the values are now fully reversible by brute force.\nWe're already talking about the \"disgruntled employee/contractor\" case here, which is notoriously hard to defend against, but exporting data to third parties and analyzing the data in non-intended ways (already made more risky by the pending EU legislation) are also realities that happen in real life. \nSince the fix here should be rather easy to implement, it would limit this attack scenario to pipeline operators with access to the config \u2014 who usually have raw S3 logs access as well anyway.. Hi @knservis ,\nfirst off thanks for taking this issue serious, apparently you have thought about a lot of use cases on your own.\nHowever, at ~8.000.000.000.000.000 Hashes per day on a single, non-expensive GPU server, there's a lot of juicy targets out there. Numeric, ascending user_ids are definitely at risk, also sufficiently truncated and formatted geoip data and anything easily enumeratable (like user_agents or second-level timestamps) probably falls into this category. Anything UUID-like with sufficient entropy, (like the fingerprints, event_ids, UUIDv4 type user_ids etc.) is probably fine.. ",
    "ferrlin": "Refactoring highlights:\n- applied best practice to defining actors\n  -- props should be defined inside the companion object of the actor\n- code duplication on routes handled through directive composition technique\n- route composition\n- CollectorService as trait to be mixin to CollectorHandler (previously called CollectorServiceActor)\n. I've already signed the ICLA.\nRecent commit updates:\n- No test to confirm fix on cyclomatic complexity for createKinesisClient. Please review the code.\n- Two scalastyle warnings left which are,\n  snowplow.collectors.scalastream/ResponseHandler.scala:66:6: Method is longer than 50 lines\n  snowplow.collectors.scalastream/sinks/StdoutSink.scala:43:4: Regular expression matched 'println'\n. Updates:\nrebase from recent release of snowplow\n- CollectorConfig as trait for mixin\n- refactorings to fix scalastyle warning\nI'm thinking of adding some property based testing to traits with scalacheck. Is this something that snowplow would also consider?\n. Scalacheck testing\n- CollectorConfig\n- KinesisSink\nI would like to know if it makes sense to test KinesisSink. When doing property testing, it tries to connect to Amazon to create the client.\n[info] ! KinesisSink.check if stream exists: Exception raised on property evaluation.\n[info] > Exception: com.amazonaws.AmazonClientException: Unable to load AWS credentials from the /AwsCredentials.properties file on the classpath\n[info] ! KinesisSink.check raw event stored: Exception raised on property evaluation.\n[info] > Exception: com.amazonaws.AmazonClientException: Unable to load AWS credentials from the /AwsCredentials.properties file on the classpath\nI would like to know your thoughts on this to move forward.\nThanks,\n. I'll try if I could mock this instead. Thanks.\n. ",
    "rverma-nikiai": "@fblundun These configurations are no longer valid with es > 5.0 since the _timestamp and _ttl fields are gone. \nGetting below error\n{\n  \"error\": {\n    \"root_cause\": [\n      {\n        \"type\": \"mapper_parsing_exception\",\n        \"reason\": \"Failed to parse mapping [enriched]: [_timestamp] is removed in 5.0. As a replacement, you can use an ingest pipeline to add a field with the current timestamp to your documents.\"\n      }\n    ],\n    \"type\": \"mapper_parsing_exception\",\n    \"reason\": \"Failed to parse mapping [enriched]: [_timestamp] is removed in 5.0. As a replacement, you can use an ingest pipeline to add a field with the current timestamp to your documents.\",\n    \"caused_by\": {\n      \"type\": \"illegal_argument_exception\",\n      \"reason\": \"[_timestamp] is removed in 5.0. As a replacement, you can use an ingest pipeline to add a field with the current timestamp to your documents.\"\n    }\n  },\n  \"status\": 400\n}. @BenFradet Are we maintaining 2 repositories, an old one and a new one (bintray). I read somewhere that you are moving to https://snowplow.bintray.com/snowplow-maven, so expecting those artifacts there as well. Using the old repo url does solved my issue.. I am building a new sink in java 'cloudwatch logs sink' for bad stream and another enricher aws-batch. Thinking of using common libraries to build over it. Any guidelines would be highly appreciated :). ",
    "colmsnowplow": "This has recently become the focus for clients with a large number of subdomains, and whose subdomains will change over time (new products launching, or rolling out new tracking in phases across a large property set).. +1 just had a customer flag this. Actually this is a prime candidate for the insights UI, now that you mention it. I think most open source users would likely look at the schema before sending events/infer that this is required themselves.. Definitely. We already have the means to pass that information to the server-side via the getDomainUserInfo method, so if it was straightforward to have client-side tracking populate this cell in atomic.events then we'd take the heavy lifting out of data modeling.. From where I stand, my opinion would be option 2), for two reasons:\n\nMost implementation start by trying to measure user-behaviour and deliver benefit in the short term, rather than planning an integrated setup up-front. This is likely to begin with client-side tracking, and server-side is likely to be considered after that. \n\nSo in terms of pragmatics, it makes sense to send the client-side session ID to the server side.\n\nThis one's a little more high-level and potentially up for dispute, but I'm of the opinion that a good measurement is as simple and direct as possible - so if you're measuring a user interaction, you should measure it at the point that the user interacts, where possible. If you subscribe to this stance, it follows that any 'second level' (struggling for the right term here) information you take, like the session ID, should also be as close to the interaction as possible. \n\nI am biased on this, but for me there's a good argument for a 'client-side first' approach, where possible.. As with the session ID discussion, I'm open to dispute on this, but what I had in mind was the other way around.\nPage View ID generated client side -> Sent to server side -> setPageViewID method available for the relevant server-side events for that PageView.\nTo illustrate: Image a use case where you're a content-driven website, with a 'sign-up' box on every content page (or any website with a sign-up option on many/every page) . You might want to attribute the server side account creation event with the page view id for the page that led them to sign up.\nI had it in this order because I'm picturing a typical implementation beginning with client-side tracking, then server-side being considered post-fact, for practical reasons.\nThere may be pitfalls in thinking of it this way that I'm not taking into account.. >  the user should have some input into what occurs so this doesn't break existing JS enrichments that do mutate an event (e.g., warn, go to bad rows, hard fail etc).\nThis, in fact, isn't a bad idea for all similar enrichments. For example, if I want to use the API enrichment with a third party service I have no guarantee they'll output valid data every time, if I want to set up the SQL enrichment I may not be guaranteed that the other SQL DB will have 100% uptime or no concurrent process which causes a 404.\nThe option to simply not add the derived context in such cases would probably be useful.\n. @alexanderdean - actually now that you mention it you are correct, it's not the logic as I first thought. There's actually likely something fishy with the example in question. However I don't think it's down to a bug in the tracker.\nOn closer examination, for the specific use case in question, it looks like the device's clock went back in time between sending the first events and the second ones:\n|                               |                               |                               |                               | \n|-------------------------------|-------------------------------|-------------------------------|-------------------------------| \n| DVCE_CREATED_TSTAMP           | DVCE_SENT_TSTAMP              | COLLECTOR_TSTAMP              | DERIVED_TSTAMP                | \n| 2018-10-30 07:48:32.727 +0000 | 2018-10-30 07:48:33.360 +0000 | 2018-10-29 07:48:35.884 +0000 | 2018-10-29 07:48:35.251 +0000 | \n| 2018-10-30 07:48:32.877 +0000 | 2018-10-30 07:48:33.360 +0000 | 2018-10-29 07:48:35.884 +0000 | 2018-10-29 07:48:35.401 +0000 | \n| 2018-10-30 07:48:32.877 +0000 | 2018-10-30 07:48:33.360 +0000 | 2018-10-29 07:48:35.884 +0000 | 2018-10-29 07:48:35.401 +0000 | \n| 2018-10-30 07:48:38.355 +0000 | 2018-10-29 22:36:38.910 +0000 | 2018-10-29 22:36:41.274 +0000 | 2018-10-29 22:36:41.274 +0000 | \n| 2018-10-30 07:48:38.900 +0000 | 2018-10-29 22:36:38.910 +0000 | 2018-10-29 22:36:41.274 +0000 | 2018-10-29 22:36:41.274 +0000 | \n| 2018-10-30 07:48:37.190 +0000 | 2018-10-29 22:36:38.910 +0000 | 2018-10-29 22:36:41.274 +0000 | 2018-10-29 22:36:41.274 +0000 | \n| 2018-10-30 07:48:42.972 +0000 | 2018-10-29 22:36:38.910 +0000 | 2018-10-29 22:36:41.274 +0000 | 2018-10-29 22:36:41.274 +0000 | \n| 2018-10-30 07:48:37.207 +0000 | 2018-10-29 22:36:38.910 +0000 | 2018-10-29 22:36:41.274 +0000 | 2018-10-29 22:36:41.274 +0000 | \nThe user's location information changed also - so what we appear to be looking at here is a user going 'back in time' timezone-wise, and that caused an edge case. \n. > Only mostly fix because we can't detect other changes like a manual clock adjustment. But they must be extremely rare on mobile.\nI'd be amazed if this happened enough to be worth doing. Is it worth considering using NTP or GPS time for mobile? Not my area but they may be options for a device-neutral report of actual time?. Ah good to know! I searched snowplow/snowplow for it, didn't think of the incubator!. Ah that's why I couldn't find the issue. Sorry Ben Deleting this.. ",
    "andrioni": "(going to comment the main content later on)\nJust a note: I'm analyzing some of the enriched event TSVs, and apparently some fields (mkt_term, at least) are URL-decoded without any form of escaping for tabs, which is nasty whenever you have to process them later.\n. In fact, I found it on on the archive bucket, inside the enriched/good/ directory, but I can't find the event_id in Redshift.\n. It also happens with mkt_content, other fields might be affected.\n. The manual decommissioning process described here could probably be automated.\n. :+1:, this would make atomic.events table smaller and more manageable\n. :+1: \n. And then use a JSONPaths file to map it to the table schema? Sounds nice.\n. Yup, and maybe a Ring middleware (if viable, I have yet to think a bit more about it).\nInterested I am, but I am a bit newbie in Clojure, so the development might be a little slow.\n. ",
    "epylinkn": ":+1:\n. ",
    "mthomas": ":+1: \n. ",
    "danielzohar": ":+1: \n. We've seen about 45% reduction in size with one billion rows. down from 750GB to 400GB (approximately)\n. I'm interested in trying to tackle this. This field takes about 30% of our atomic.events storage. Can anyone point out what needs to be done in order to accomplish this? Thanks\n. That'll be great!\n. Is that for Docker containers? I'm happy to contribute the containers I've built previously which we use in production. I'm just in the process of building new, updated ones\n. @alexanderdean my pleasure! \nSo we currently have the containers at:\nhttps://github.com/Memrise/docker-snowplow-stream-elasticsearch-sink\nhttps://github.com/Memrise/docker-snowplow-stream-s3-sink\nhttps://github.com/Memrise/docker-snowplow-stream-collector\nI'm just in the process of upgrading to 78 rc2 and moving to Linux Alpine which will make the containers really small (about 40mb + JAR size = ~100MB).\nHow should we go from there? Do you want to open official repos on github.com/snowplow/ or do you want it hosted on this repo?\n. @alexanderdean I created a new repo that contains collector + enrich + elasticsearch-sink at https://github.com/Memrise/docker-snowplow\nHow would you like to proceed from here? I assume you'd like to take ownership. If that's the case, would you like to create a new repo on your account and I can a PR with those files?\n. thanks @alexanderdean \nI'll update and try to create a PR today.\nAs an aside, having separated JARs for each app would speed up build times and simplify the structure of the Dockerfiles a bit\n. Can't fork that repository. See https://github.com/snowplow/docker-snowplow/issues/8\n. yes @alexanderdean, sorry\nyou can see here that I need to download the entire zip file - https://github.com/Memrise/docker-snowplow/blob/master/base/Dockerfile\nthen in each docker file, I keep only the relevant JAR and delete the reset\n. ",
    "ninjabear": "While I'm here shall I fix this too @fblundun / @alexanderdean ? \n. Oh I see there's already a commit for it! The two changes are about to conflict\n. Hi @chuwy, thanks for writing this! I just used it to set up a clojure collector on elastic beanstalk.\nAs time has passed unfortunately Amazon have changed the elasticbeanstalk setup UI to the point where it no longer matches up with the guide here https://github.com/snowplow/snowplow/wiki/Setting-up-the-Clojure-collector. So having the CLI instructions has really helped as they seem to change less.\nI had to make a small alteration to get it to work as of today :-\naws elasticbeanstalk create-environment \\\n    --application-name analytics \\\n    --environment-name collector \\\n    --solution-stack-name \"64bit Amazon Linux 2015.09 v2.0.4 running Tomcat 8 Java 8\" \\\n    --version-label clojure-collector \\\n    | jq -r '.EnvironmentId'\nreturned \nA client error (InvalidParameterValue) occurred when calling the CreateEnvironment operation: No Solution Stack named '64bit Amazon Linux 2015.09 v2.0.4 running Tomcat 8 Java 8' found.\nSo using \naws elasticbeanstalk list-available-solution-stacks\nI could see it's been updated to 64bit Amazon Linux 2015.09 v2.0.6 running Tomcat 8 Java 8\nreplacing the solution-stack-name with this fixed it, however it didn't return the environment id. I found this by looking it up on the AWS web UI.\nEverything else worked fine (I hope!). Thanks! \n. I think at least in terms of the clojure collector it could become a one-stop setup script - the instructions could be something like \"run this\" - curl http://<something>/clojure-elasticbeanstalk.sh | bash. \nAside from saving the user a bit of time and possible headscratching it also gives the bonus of making the instructions provably correct - we can put them under test by running the script and quizzing aws afterwards. That would mean we don't need to play catchup with AWS quite so much.\n. See #2195 for current PR\n. @jbeemster I think that's everything?\n. I've updated the jsonpaths/ddl taking into account the jsonschema changes for https://github.com/snowplow/iglu-central/pull/277 here too. Hopefully that means this is ready to merge now, unless anyone has any other suggestions/comments!\n. joined up with sendgrid on #2195\n. joined up with sendgrid on #2195\n. Thanks @alexanderdean ! Will do\n. the CI results in this case are not relevant I don't think\n. Cool! Fingers crossed!\n. As a possibly relevant side note, I don't think the group by can work without advance knowledge of the filenames. I tried a group by of (.*) thinking that everything would be grouped into one file, however the result seemed to be that actually nothing was grouped. I could be wrong I was just going by the time it took my job to finish!\n. I've put the version bumping commits near the tip of this branch - with \"WIP\" on the end. I've updated the EmrEtlRunner sample config and the blogpost to specify a hadoop enrich version of 1.5.0 which is what I think it will become when shipped. \nI haven't changed the hadoop enrich version here \n. I've verified both the existing problem and that this PR fixes it now. One to watch out for is that the \"test your integration\" button sends the application/json content type and the regular API sends the extended version with the charset.\n. Looks good to me! Is there ever a situation where you'd want to write to more than one place? For example bad rows go to kinesis and get written to stderr too\n. The other thing we discussed is a switch that starts the collector writing to a file or S3 bucket - meaning that we can rerun those events later if required.\nThe number of events that haven't made it into kinesis or the last kinesis write timestamp would be good information to put on any healthchecks the collector provides.\nIn my experience the JVM can be pretty unhelpful when trying to store large amounts of information in memory, I'd find it interesting to know what happens exactly when the collector tries to cache a lot of events (do we already have a JVM imposed limit for cache size?).\n. I was just writing the same thing @fblundun, all of this can be done by treating the collector as a microservice with an api\n. I completely understand the need here - and this is how tools like Azkaban work. Each job can report back to the job runner, and that \"snapshot\" of where the job is being stored in a database like mysql. However, I think we should go about this in a different way for the following reasons:\n1. The task calling back to the job runner introduces a two way dependency - right now we have the control path flowing from Factotum to EmrEtlRunner. So if we change Factotum, EmrEtlRunner has no idea, and it doesn't actually need to know. Adding this in would mean EmrEtlRunner is coupled to some version of Factotum, and it also means we give any other vendor the ability to chain themselves to a version of Factotum. In order to keep factotum iterating without getting \"stuck\" with this kind of thing (ie. if I touch X then I have to edit X,Y,Z and persuade xyz to adopt my changes) we need to be quite strict about this control path \n2. It'd very likely introduce external dependencies (or anchor us to an operating system). If you really want to push on with this the way I'd probably go about it is to have EmrEtlRunner expose a \"debug port\" where it writes it's current state. That gets us around having to have a database, or using something like pipes. Using files is obviously possible too, but files are kind of error prone because of how the operating system works - flushing, caching etc. Doing it this way still doesn't give us a reliable (schema'd?) way to give this information out to snowbot (or others) either.\nSo my suggestion is..\nWe make it possible to run EmrEtlRunner steps/tasks using Factotum directly - ie introduce options into EmrEtlRunner to run a single step only, and export the \"dag\" side to Factotum. I think we should do it this way because:\n1) avoids problems above\n2) provides a path to maintain and or rewrite EmrEtlRunner in parts\n3) individual task logs and status (including stdin/out/err) will be fed back into Factotum for free (and in later versions of Factotum, the task results will very likely be available realtime as schema'd json)\nThe catch is this - the granularity of feedback will be limited by how small we can make tasks in EmrEtlRunner.\nWhat's your thoughts on this?\n. Awesome! One way to do it (in the spirit of #936) is to think of EmrEtlRunner more as service, so the pseudo steps are:\n1. EmrEtlRunner up\n   - any stateful setup\n2. exec step abc\n3. exec step xyz\n   ...\n4. EmrEtlRunner down \nThis can be accomplished pretty transparently to the user by having a pid file and the first invocation triggering the \"EmrEtlRunner up\" step and subsequent calls checking if an existing \"service\" for that cluster is available. The service times out after a while, or if a job on a different cluster is specified (the \"EmrEtlRunner down\" step).  This is how gradle works. The downside is it's a bit of a fiddle coding it, however it's a pattern that will always work with Factotum (and any other tool).\n. Sure - perhaps \"service\" isn't a great analogy from me, since it implies a \"global\" daemon. I'm thinking if you put the pid in the working directory, or with the configuration, only subsequent runs from that location would link to that process. \nIf we can remove the stateful requirement using some parameters, I think that'd be preferable! I'm just trying to say that we'll never get snookered by pulling DAGs out of applications and into Factotum - even if they are stateful. \n. Hey @chuwy I have patched this, also I removed the max-size restriction which was in error. Does it look right now?\nsee also https://github.com/snowplow/iglu-central/pull/354 for the updated json schema\n. @alexanderdean we can have some logic that establishes what needs building, using git diff --name-only or similar in the corresponding directories (so if something has changed under say, the EmrEtlRunner directory then we need to rebuild and repush that). \nI think we'd be starting to work against the grain though, traditionally one repository compiles to one artifact, even if that artifact is composed of N other artifacts - e.g. \"snowplow rXX is clojure-collector 0.1.0, stream-collector 0.2.0..\". \nWith custom crafted tags we're trying to give a tag a scope - but tags are a bookmark to a position in the whole repository. We're saying that everything in this repository at this point in time works together - this is the same paradigm that travis follows for better or worse.\nFor the example we're currently looking to implement, we'd push the tag r80-rc1, but would have bumped the kinesis version inside that - \"this is a candidate release of snowplow r80 - it uses a new kinesis elasticsearch sink\". Do you think this kind of system could work for us?\n. @alexanderdean It's not going to be a big problem if we keep the \"loose collection of artifacts\" direction of travel - however if we could split the repo into the collection of artifacts that we do update together (real-time/batch?) it would give us a couple of advantages downstream;\n- At some point, any number of engineers will try to \"download\" snowplow and get a bit confused about what bits they need. We can simplify that for them - especially if we include a install guide for the default setup inside the zip\n- When we're trying to upgrade a client to the latest version of snowplow we could download a single zip (product-version) which would include any migration scripts - the versions are guaranteed to run together (no need to check them again)\nI think if we don't split the project up with submodules (or other) then we should dig in and start a build script inside the snowplow/snowplow repo.\nI think this because if we have some non-standard build logic we need to capture it as neatly as possible, and keep it outside the build tool where we can. This means we'll remain fairly build tool agnostic, and we'll never lose the ability to build our product (surprisingly this does happen).\nPython is often used in this capacity - realistically we have a choice between python or ruby (using rake would be nice since build tools understand it). (We could also feasibly use gradle and a fair chunk of groovy - this has the advantage of having a bintray plugin provided by bintray). I'd like to avoid bash as much as possible since we can provision any interpreter we need and still provide the ability to build it yourself locally in vagrant.\nSo - looping back to your comments on hinting or automatically, if we write a custom build script we'll probably have both - the step to see if the artifacts have changed will be a validation step (we want to keep the tags and the releases married). This would prevent you from deploying anything that didn't have both parts (a tag, and changes).\nI think the way to look at the process deploying with crafted tags is, not that it's a button to deploy a release - but that the tag defines the release. Consider it from a support point of view - customer reports problem, they're using 0.1.0 of xyz - I need to check out that version in order to replicate the problem (you could download the artifact - but the source code is invaluable in figuring out the fix). If the tags are confusing, or have been deleted it makes things more difficult than it need be. We can also add multiple tags to the same commit, each will trigger a deploy, so we don't need the capacity to specify multiple releases in the same tag (I don't think?).\n. Sorry I didn't explain that well at all -\nI think the way to look at the process deploying with crafted tags is, not that it's a button to deploy a\nrelease - but that the tag defines the release\nWhat I mean by this is, if we had the history on the left:\n\nWe have the ability to single out specific releases of individual components. So there's some logic - just using git tag - that can tell us what's in a specific release. We can also then use tags to tell what the latest release of a specific component is, and what versions are in each \"r80\" style release. \nConsider it from a support point of view - customer reports problem, they're using 0.1.0 of xyz - I\nneed to check out that version in order to replicate the problem (you could download the artifact - \nbut the source code is invaluable in figuring out the fix)\nSo now - If I know the release, I can just check out r80 (and the versions of things will be correct) or if I  can see that they're using common-enrich 0.1.0 I can check out common-enrich/0.1.0.\nDirectly translating my ramble on \"tag buttons\" I mean that a tag is a release, we should make it as expressive as possible, and a 1:1 mapping with an artifact if we can. \nI don't think there's a real action from what I'm saying here - other than it's possible, and it may simplify working with the repository - and get us nearly all the way to representing the version matrix in source control. We should keep it simple for now!\n. @alexanderdean I'd go with the second option if possible. So we'd need to push a tag, like \"r80\" to get things started, then the build script can automatically establish what's changed, and what the versions now are (and push the corresponding tags back to the repo). \nThere is one catch with that though, and that is that we'd still need a way to bake in what should be in a release. So if it common-enrich has changed, we'll get that from the history - but many other things make up a release and they may not all have changes - they should still be \"reissued\" under the new release. Obviously this can change for different types of releases. \n. If I change hadoop-enrich, I could do it without upgrading common-enrich - would we want both to appear in the resulting release zip? If so then common-enrich would be \"reissued\" as part of the new release since it hasn't changed.\n. Hey @fblundun ! The simplest thing you can do is to enforce the tag version matches with the version in BuildSettings.scala prior to deployment - make the build fail if the tag and the version isn't correct (exiting with 1 in a bash script will do this).\nTo do this from a script, the tag is an environment variable provided by travis - $TRAVIS_TAG and you can grab the version of a SBT project using sbt version. Here's an example used in the scala tracker:\nhttps://github.com/snowplow/snowplow-scala-tracker/blob/master/.travis/deploy.sh#L17-L18\n. Proposal:\nWe have a self describing json in the root that tells the build what to run and what to release. This would change over time and would be updated manually by the person editing a specific component.\nThis would allow the person to select different items freehand and allow some complexity while enforcing a standard format.\n. +1! It would also be nice to use jdk8 as java 7 is no longer being patched\n. no probs, I've set them to as the event name appears in the source\n. @jbeemster I don't really want to duplicate that function - is there a common place we could put it?\n. Sure, this was an intellij format problem and it may creep back until I work out how to change the default. I'll keep watching out for it though! \n. Awesome! Cheers\n. It's not, no! I'm going to push what I've got as it fixes this, however its still WIP and contains a few eyebrow raisers still\n. Is it directly related to the Java Throwable? \nEither way thanks - I'll change this, thinking about it if I saw some Java catching throwable I'd say the same thing.\n. Is aid=email&cv=clj-0.6.0-tom-0.0.4&nuid= a set of get params? Sendgrid doesn't appear to send any of these, but they should be captured still\n. Ok cool, thanks @jbeemster \n. ah, sorry! Intellij hides that copyright notice at the top so I completely forgot about it. Fixed\n. Fixed!\n. hey @alexanderdean, this change:\nhadoop-0.14.0 -> hadoop-1.4.0\nI'm not sure what it means? I'm just fixing a conflict for R75 and want to make sure changes like this get percolated through properly\n. Oh cool so this is an external dependency, I just need to not revert it\n. ",
    "yuvalherziger": "Hi @alexanderdean , interesting, thanks for the information and the correction.\nSo since we've been working with PostgreSQL (for now), we may want to build scala-common-enrich and scala-hadoop-enrich with our own tweak to the size limit.\nWe've also noticed now that Kinesis has a hard limit of 50KB per record, that's also interesting in that respect.\nGenerally speaking, I acknowledge the fact that web tracking should remain on a strict diet, but it's  somewhat troubling to know that when a customer insists on sending over long lists of attributes with an event's context, it may be risky. I will stay tuned, thanks again for the info!\n. ",
    "jramos": "@fblundun - eureka! I've got a better idea: change your shared folder type from vboxfs (the default) to nfs in your Vagrantfile. E.g.\nconfig.vm.synced_folder \"~/src/\", \"/home/vagrant/src\", type: \"nfs\", create: true\nYou'll want to setup NFS in your VM, too. It's different depending on your distro and config, but here's what I did:\nbash\nsudo su\napt-get install nfs-kernel-server nfs-common\necho \"/home/vagrant   127.0.0.1(rw,no_subtree_check,all_squash,async)\" | tee -a /etc/exports\nservice nfs-kernel-server restart\nIf the NFS daemon launches successfully, halt and bring vagrant back up. You'll need to enter your administrator password on first launch.\nThe echo piece is needed because NFS won't start on Ubuntu unless there's a defined export. So I share /home/vagrant with myself.\nTotal time for a fresh sbt assembly went from ~8 minutes to ~60 seconds for me. Subsequent assemblies using the cache are around ~10s.\n. (FWIW, I'm using Scala 2.10.4, sbt 0.13.8, and sbt-assembly 0.13.0).\n. I can get this to compile and run with sbt, but I can't get it to assemble as a runnable JAR.\n$ ./target/scala-2.10/kinesis-redshift-sink-0.1.0 \nError: Invalid or corrupt jarfile ./target/scala-2.10/kinesis-redshift-sink-0.1.0\nAny ideas?\n. Unzipping the assembly gives me this:\n$ unzip kinesis-redshift-sink-0.1.0\nArchive:  kinesis-redshift-sink-0.1.0\nerror: End-of-centdir-64 signature not where expected (prepended bytes?)\n  (attempting to process anyway)\nwarning [kinesis-redshift-sink-0.1.0]:  43 extra bytes at beginning or within zipfile\n  (attempting to process anyway)\n...\n. Before this can be used with the 0.8.0 Redshift schema, the following change needs to be made:\nhttps://github.com/jramos/snowplow/commit/b1d779aff8d690b2cc85fc3ca8ca6d5cd05587d5\n. I also noticed one issue: when a COPY errors, for example when the fields are mapped to an incorrect type, Redshift reports that the load was a \"success\". You have to drill into the load details in the console to see that there were elements that didn't COPY over. Not sure of the best way to handle this yet...\n. Ah, I think MAXERROR 1000 is to blame.\n. I don't think 0.8.0 and produceCombinedContext will work together. The schema changes assume all contexts are shredded and context will be null.\n. I have a version I'm updating to work with r88. I will comment here when it's ready.. Will do. Thanks.\n. :+1: \n. Cool, thanks!\n. Actually, there's a debug log level that'll output the bad row. Thanks guys!\n. Sorry, this was a cluster issue. I fixed in ES.\n. :+1: \n. Okay, I'll try to make the updates.\n. Odd... I can't seem to reproduce in specs. At what point in the enrichment process are the adapters used? Maybe I'm looking for the app_id too soon.\n. That's what I thought. Here's a spec that shows it actually works:\n```\n    \"return correct json for sample event, including stripping out event keypair and fixing timestamp\" in {\n  val inputJson =\n    \"\"\"\n  [\n     {\n       \"email\": \"example@test.com\",\n       \"timestamp\": 1446549615,\n       \"smtp-id\": \"\\u003c14c5d75ce93.dfd.64b469@ismtpd-555\\u003e\",\n       \"event\": \"processed\",\n       \"category\": \"cat facts\",\n       \"sg_event_id\": \"sZROwMGMagFgnOEmSdvhig==\",\n       \"sg_message_id\": \"14c5d75ce93.dfd.64b469.filter0001.16648.5515E0B88.0\"\n      }\n  ]\"\"\"\n\n  val payload = CollectorPayload(Shared.api, toNameValuePairs(\"aid\" -> \"test\"), ContentType.some, inputJson.some, Shared.cljSource, Shared.context)\n\n  val expectedJson =\n    compact(\n      parse(\n        \"\"\"{\n          \"schema\":\"iglu:com.snowplowanalytics.snowplow/unstruct_event/jsonschema/1-0-0\",\n          \"data\":{\n            \"schema\":\"iglu:com.sendgrid/processed/jsonschema/1-0-0\",\n            \"data\":{\n                 \"email\": \"example@test.com\",\n                 \"timestamp\": \"2015-11-03T11:20:15.000Z\",\n                 \"smtp-id\": \"\\u003c14c5d75ce93.dfd.64b469@ismtpd-555\\u003e\",\n                 \"category\": \"cat facts\",\n                 \"sg_event_id\": \"sZROwMGMagFgnOEmSdvhig==\",\n                 \"sg_message_id\": \"14c5d75ce93.dfd.64b469.filter0001.16648.5515E0B88.0\"\n              }\n            }\n          }\n        }\"\"\")\n    )\n\n  val actual = SendgridAdapter.toRawEvents(payload)\n  actual must beSuccessful(\n    NonEmptyList(\n      RawEvent(Shared.api,\n        Map(\n          \"tv\" -> \"com.sendgrid-v3\",\n          \"e\" -> \"ue\",\n          \"p\" -> \"srv\",\n          \"aid\" -> \"test\",\n          \"ue_pr\" -> expectedJson // NB this includes removing the \"event\" keypair as redundant\n        ),\n        ContentType.some,\n        Shared.cljSource,\n        Shared.context)\n    )\n  )\n}\n\n```\nI'm stumped.\n. I checked with RequestBin, and the query string is passed.\n. Is it actually a bug in the collector service for post requests? I don't see a query string getting passed through there.\n. Yep, that's the issue. The post handler doesn't process the query string.\n. I knew this bug looked familiar... https://github.com/snowplow/snowplow/pull/2685\n\ud83d\ude04 . I signed it! \ud83e\udd47 . \ud83d\udc4d . ",
    "fwahlqvist": "Hi Alex, Have been using the set-up guide and so far got all the way up to Enrich, however as the elastic-mapreduce is outdated it would be great if that section could be prioritise.\n. ",
    "uniuser": "I have the same issue. Without the update to 0.1.2 I am not able to connect to frankfurt (eu-central-1).\n. ",
    "kytori": "yep really if i knowing this before to start deployment of all my stuff on eu-central-1 i dont waste so many time to move everything on ireland to be sure that is work , you have a date for this ?\n. waiting for your release i updated fog gems to last available and solved the problem (actually only on storage-loader processing logs for postgresql).\n. ",
    "gincor": "Hi, \nstill getting the \"The authorization mechanism you have provided is not supported. Please use AWS4-HMAC-SHA256.\", so I assume this issue is still open. Not asking for a release date, but could you update the \"3 releases\" guesstimate if you have the chance? \nthanks a lot\n. Hi Alex,\nthanks for the quick reply. Just wanted to add my (temporary) workaround which works well for testing and doesn't take a lot of time in case anyone with the same issue finds this.\nIf you setup everything for an unsupported region, all you have to do is manually sync the s3 bucket where your collector dumps the files to a bucket you create in a supported region and adjust the paths in your config.yaml. Same for your 'output bucket'. If you setup replication, I'd hope this would work pretty well automatically after the first setup/sync.\nbest regards from Berlin (also to @yalisassoon)\n. EMR apparently also has some issue in central-1 as juergenweber pointed out: https://groups.google.com/forum/#!msg/snowplow-user/9fuknNXXjjc/CMPjMXEjIgAJ\nrunning into the exact same thing. \n. @jbeemster are you saying I can give the 2.x sink a shot with an elasticsearch 5.x cluster? Not for production, but it should sort of work? Clarifying because either you mistyped ('2.x sinks do work with a 5.x sink') or I don't get what you mean. . Just FYI - couldn't make the 0.8 2.x sink work with an es 5.2.2 cluster - don't have the exact error at hand now, but it failed to find an es node at the given endpoint. It worked when I downgraded that exact endpoint to 2.4.4 and ran it with the exact same configuration file.. transport. I'm almost sure I tried it once or twice with http as well, but not 100%. I definitely checked curl server:9200 and that 9300 was open as well. I'll see if I can make some time later this week to investigate some more. . ",
    "jurgenweber": "That is quite slow and annoying if you collect a lot of data.\nSuper interested to see this live.\n. ",
    "jessealama": "+1\n. ",
    "jasonbosco": "Great! Thanks @alexanderdean\n. ",
    "digdeep": "Thanks for update Alex,\nOn the specifics of debugging, do bad rows for contexts still get loaded\nwith atomic events that are good? is this the reason it is used for\ndebugging to see what is causing the error?\nFor what it is worth, we use a basic install of Splunk for bad rows, super\nsimple to read in S3.\nOn Fri, Jun 5, 2015 at 4:17 AM, Alexander Dean notifications@github.com\nwrote:\n\nUpdate from @yalisassoon https://github.com/yalisassoon - these fields\nare in fact v useful for debugging bad rows. Therefore we can say that this\nticket is blocked by this ticket: #824\nhttps://github.com/snowplow/snowplow/issues/824, which provides an\nalternative way of debugging bad rows...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/1731#issuecomment-108997132.\n. \n",
    "grzegorzewald": "@alexanderdean: as far as I see in sources, If the record size is greater than 50 KB, it is rejected and not send to Kinesis. The limit is hardcoded (at least in Enrichment and ELasticsearch Storage in r65).\n. ",
    "szareiangm": "@BenFradet  @jbeemster  @alexanderdean Is the there something blocking this ticket? I see this ticket sitting in the backlog for a long time. This ticket is a large amount of work that in nature they are separated: \n1) The fist part that @chuwy mentioned as an example, is a far more simpler than the graph dependency of enrichments. However, it needs another nesting around enrichments to give them a name. This is actually very helpful, too. We can have  user defined names wrapping around enrichments to reuse them. \nTo avoid the issues with parsing Json config files, we can have the enrichment tasks names inside them as an optional field and convert the configuration to an array.\n[{\n\"userDefinedName\" : \"my_company_api_lookup\"   // usual API lookup configuration\n// the rest of the config for this API\n} , {\nuserDefinedName: \"acme_api_lookup\"    // another usual API lookup configuration\n// the rest of the config for this API\n}]\n2) For the second part, we need to tear apart the enrichment process and add a flexible/dynamic \ntask (enrichment) scheduler that reads its plan from a DAG file. This part can use the previous one, with calling tasks with the \"user-defined\" names instead of their vanilla name. To me, this is a huge overhaul. \nI think for the sake of getting things done, we should at least split this ticket into two. What do you guys think?. As you know this library has reached EOL. There are some points to discuss after a while:\n\n@alexanderdean on issue #3656 said that we don't swap the underlying library for the current enrichments. So we might stick to user-agent-utils.\n@alexanderdean (#1899, #1900, #1901) suggested that the result of this enrichment should be in its own context table instead of the atomic table.\nThere is a great contribution in #3810 to use flexible updatable regex file for ua_parser, the alternative. Now it is supercharged.\n\nI was thinking to recommend that we deprecate user-agent-utils fields and its enrichment in near future.  There is ua_parser as the alternative. During the next months  user-agent-utils would not get any updates and its output will be derived from ua_parser. This might cause a bit of messy data for analytics. \nWhat is your opinion about it, folks?. @alexanderdean I just thought giving heads up from now would be invaluable to the user community, so I propose this order:\n1. Provide a warning on startup that enrichment is eol (suggesting alternatives)\n2. Migrate the structure to a context (part of events refactor) -> and fix any code relying on this enrichment.\n3. Stop executing enrichment, and remove jar\nWhat do you think?. @alexanderdean Is there any specific issue ticket for the events refactor? \nPerhaps it should be defined as a Milestone and scattered issues get attached to it. The milestone is enormous as far as I understood.. Hello @alexanderdean \nYou are right. The benefit of this work, shows itself more on the side of extendibility and support for non-standard input payloads that collector has received. \nYes. I want to use Akka to process the payloads externally and format them as the expected rawEvent model for the Enricher. \nI started from Adapter as the entry point for enricher but enrichments were my next stop and I was going to ask after this. I saw almost every issue tickets for enrichments and EnrichmentManager. There is a room for improvement there to have the enrichments via TCP and Akka actors in a non-blocking fashion.. Hello @alexanderdean \nYou are right. The benefit of this work, shows itself more on the side of extendibility and support for non-standard input payloads that collector has received. \nYes. I want to use Akka to process the payloads externally and format them as the expected rawEvent model for the Enricher. \nI started from Adapter as the entry point for enricher but enrichments were my next stop and I was going to ask after this. I saw almost every issue tickets for enrichments and EnrichmentManager. There is a room for improvement there to have the enrichments via TCP and Akka actors in a non-blocking fashion.. Sure thing. Thanks.\nThe PR for this issue will be created soon in 1-2 weeks. I am finalizing some work in snowplow-elasticsearch-loader for now. . The build failed due to dependency fetching request timeout. can you guys trigger it again? . There we go. I am keen to take on this opportunity.\nPS: I could not find the flavor for API lookup in the docs or settings for it. Any URLs or hints?. If I could find some free time this week or the next, will do it. Busy with the other promise for optional enrichments.. duplicate of #3830 ? Although, I know that I delayed reviewing that milestone code but I managed to talk a look now. sorry about it. If you are doing it, please close my issue.. @BenFradet Sorry about late response. I want to move this discussion forward. \nI think we can work on a vanilla HTTP version, probably with POST. Is that okay with you?\nAre you sure about the gRPC? It comes with dependency on Proto3. https://grpc.io/docs/tutorials/basic/java.html Dependency on Protubuf is a whole new story, we have already Json/TSV serialization in the pipeline and you guys have tickets for Avro.. we have changed it as we agreed. can you please have a look?. About moving the example to the incubator, how do we do it? @BenFradet . @BenFradet The code is ready. I need to do one last end-to-end test for it and then we are good to go.. Sorry, wrong repo :(. What is your suggestion? The choice is based on the fact that input is an unstructured text and output should be in format of RawEvent to be compatible with the next steps.. Got it. I think I should pull out most of the remote-adapter-example functionalities and put them in the core and just keeping the part that creates unstruct event in there. So basically the steps would be these:\n1) RemoteAdapter extracts the collector-specific information in the payload, and just sends the business related values/params (content-type, payload.body and get parameters) to the webservice.\n2) Webservice does the work to transfer the data to a array of Json objects with one  field to fulfill RawEvent.parameters. Basically, running a function similar to  toUnstructEventParams in the remote Webservice, happens here.\n3) RemoteAdapter receives the payload, looks up the schema and merges it with step1 data into a RawEvent payload.. I will address it after the other comment. Okay. I have made the changes. \nThe enricher sends the data in base64(json(content-type, get-params, body)) format and receives base64(json(error, events)).\nPlease take a look and advise. If you agree on it, I will go for fixing the unit-tests and your other comment.. encoding and binary data. We can try json and see what would happen. so shall I remove base64? Also, please create the repo, so after resolving the comments, I will move the example there.. ",
    "jpoon": "+1\n. Ooh, that sounds promising! How far along is the Kafka+Samza version? \nAs @alexanderdean had mentioned, the other alternative is to support EventHub. My team has used EventHub successfully in the past pumping stuff in like logs and GPS data. I haven't personally used Kinesis so I can't compare.\nIf we can use the Kafka+Samza version, a good starting point might be to add support for sinking data to Azure Blob Storage (Azure equivalent to S3). Azure Blobs also have an HDFS interface to which we can run Hive / Pig jobs on top of the blobs.\n. As per the architecture you listed, just want to make sure we're on the same page with all our options: \nCollectors\n- Event Hub: Data can be pushed into Event Hub through the REST API or via AMQP. Apache Qpid has a JMS component that we could use to pull data in and out. Also came across some examples here.\n- Kafka: If we use Kafka, it's not cloud-specific so we could run it on an Azure VM and start partying.\nStorage\n- Azure Blob Storage. I couldn't find an existing solution out there for sinking Kafka/Event Hub to Azure Blob Storage. If we can't find one, I don't foresee this being too difficult to implement. (famous last words? :P) \n- SQL Data Warehouse. Not entirely sure when public preview will be open. Might be a good goal in the long term? \n- PostgreSQL. Reading the doc, apparently we can also sink data to PostgreSQL? It's possible to create an Azure VM running PostgreSQL so that can be a short-term goal to get things going.\nAnalytics/Compute\n- I haven't worked a whole lot with Hadoop. The Azure version of Hadoop is known as HDInsight which is essentially repackaged Hortonworks Data Platform (HDP). Let me try to find out if there is something simliar to Elasticity. \n. - Kafka or EventHub -> Azure Blob Storage. Have you found that a lot of people persist the raw events from Kinesis straight to S3? What is the difference between Amazon's version vs snowplow? In any case, agreed that it would be useful code. I wouldn't mind giving a crack at it once we decide which direction to go -> Kafka vs Event Hub.\n- SQL Data Warehouse. Let me poke around the company :)\n- HDInsight. HDInsight has native support for Azure Blob Storage (ie. no need to move it to an explicit HDFS store). Once the raw events are in Blob Storage, we can run MR jobs directly on top of it. As for management libraries, as you pointed out, there's the client .net library. Otherwise, as HDInsight deploys the Hortonworks Hadoop stack there is Ambari and WebHCat out of the box. \nI think what @thomaswrenn may have meant is in consideration of the options above, quickest path to supporting Azure. If that is the case, it might mean adding support for kafka or event hub and then using the persisting that data to PostgreSQL? What do you think? \n. > HDInsight + Azure Blob Storage\nBest practice is to have the data in Blob Storage. Azure did a massive amount of network upgrades a few years ago such that for MR jobs, perf characteristics for accessing data in Azure Blobs was near identical to the cluster's HDFS.\n\nHDInsight + YARN\n\nTotally :+1:. It's probably best to avoid any specific client libraries such that anything we do here can also apply elsewhere as well.\n\nPostgreSQL\n\nFair enough. \n. To throw another option into the ring, an alternative to building an explicit app for EventHub->Azure Blob Storage sink is to leverage another PaaS offering from Azure: Stream Analytics . We can use it to (1) read from event hub, (2) do some sort of processing, if any, (3) push it to azure blob storage. \nWith that in mind, we could build snowplow support for:\nevent hub -> stream analytics -> azure blob storage\n                              -> sql data warehouse\n. ",
    "thomaswrenn": "Our use case:\nI'm interested in trying out Snowplow but I want to do so in Azure because that's where we have a lot of free credits. \nWhat's the shortest path for me to do this on Azure?\nWe're a small team and just starting out with tracking user analytics so we want something quick to setup but where we'll own the data and will be easy to scale in the future without a migration.\n. Yeah, sorry, I severely miswrote. I mean\u2019t to phrase it as what the smallest set of implementations/ports would be to get something running on Azure like @jpoon said.\n. Do you have a recommended blog post or design document that explains the shredding?\n. ",
    "philbritton": "Azure Data Factory looks like the 'azure way' for managed ETL's, could this be a piece of the puzzle? https://azure.microsoft.com/en-us/services/data-factory/\n. ",
    "rgknp": "I would really like Snowplow support in Azure. \nNow Storm and Spark Streaming is available as part of Azure HDInsights. Also there is MPP version of Postgres (XtremeData) available in Azure Marketplace. So you could technically use Redshift loading engine with XtremeData without huge changes.\nSo probably this could be a viable alternative.\nevent hub -> storm/spark streaming -> azure blob storage\n                                                         -> Xtremedata (Postgres MPP)\n                                                         -> azure SQL Datawarehouse\nCan't wait to see Snowplow on Azure..\n. Yes, Azure now has EventProcessorHost component for receiving events from Event Hub. Below URL has complete implementation including code.\n[(https://azure.microsoft.com/en-us/documentation/articles/event-hubs-csharp-ephcs-getstarted/)]\n. @alexanderdean what is the performance you are looking for? In Stream Analytics you could do \"select * from  and output it to blob storage. Each streaming unit can give throughput of 1 MB/sec. You can add streaming units to scale up to 1GB/Sec.\n. @alexanderdean Stream Analytics supports Avro, CSV and JSON file formats. I am not aware of Thrift support. But you may want to look at Bijection APIs https://github.com/twitter/bijection for Thrift -> Avro conversion if needed.\nRecords will be written to blobs which can be in user defined containers. You could define path prefix, date and time format if you want data to be split into separate blobs. Each blob can hold max 1TB of data so it is a good idea to split records based on some kind of distribution. Splitting blobs based on date and time is most popular. More details can be found here https://azure.microsoft.com/en-us/documentation/articles/stream-analytics-define-outputs/\n. @alexanderdean I found kinesis-connector like libs for Azure. These are called data-pipeline libs. Written and supported by Microsoft.\nhttps://github.com/mspnp/data-pipeline\nhttps://github.com/mspnp/data-pipeline-storm\n. I am glad that worked out. Can't wait to have Snowplow on Azure.\n@alexanderdean do you have a timeframe for this?\n. @alexanderdean Do you have/plan to have Azure in your roadmap?\n. @alexanderdean I am with Microsoft. I am happy to help within my capabilities to help and expedite the port. I will send you email shortly.\n. ",
    "markusahlstrand": "@alexanderdean Not sure it fits your scenario but we're using stream analytics to stream data from event hubs to blob/table storage.\n. ",
    "lpgeiger": "Thanks. BTW can you give that page, or another, some more detail about how mechanics of the clojure connector. Does it use pixels? Does it use REST requests? As a newbie I don't see how the requests happen. \nThanks!\n. ",
    "danrama": "@alexanderdean CLA signed!\n. ",
    "denismo": "Also, would like to throw in that the timestamp (originally) was in local time, not UTC. Not sure whether the current fix handles that.\n. Hi Justin,\ndon't know why zip gives these errors. I did have a problem with JAR file\ninitially - there was no Main-Class entry in the MANIFEST so it was not\nself-executable. It should have been fixed though it's worth checking.\nDenis\nOn Fri, Sep 4, 2015 at 2:38 AM, Justin Ramos notifications@github.com\nwrote:\n\nUnzipping the assembly gives me this:\n$ unzip kinesis-redshift-sink-0.1.0\nArchive:  kinesis-redshift-sink-0.1.0\nerror: End-of-centdir-64 signature not where expected (prepended bytes?)\n  (attempting to process anyway)\nwarning [kinesis-redshift-sink-0.1.0]:  43 extra bytes at beginning or within zipfile\n  (attempting to process anyway)\n...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/1927#issuecomment-137537771.\n. BTW, unfortunately while using the drip-feeder in production we identified\nsome performance problems with it. It cannot insert the data at the rates\nthat would be sufficient for typical website loads. The fastest that we\ncould get it to run is 15 events/sec. So we'd advise to hold off using it\nin any production system until further update.\n\nDenis\nOn Fri, Sep 4, 2015 at 9:51 AM, Denis Mikhalkin <denis@digdeepdigital.com.au\n\nwrote:\nHi Justin,\ndon't know why zip gives these errors. I did have a problem with JAR file\ninitially - there was no Main-Class entry in the MANIFEST so it was not\nself-executable. It should have been fixed though it's worth checking.\nDenis\nOn Fri, Sep 4, 2015 at 2:38 AM, Justin Ramos notifications@github.com\nwrote:\n\nUnzipping the assembly gives me this:\n$ unzip kinesis-redshift-sink-0.1.0\nArchive:  kinesis-redshift-sink-0.1.0\nerror: End-of-centdir-64 signature not where expected (prepended bytes?)\n  (attempting to process anyway)\nwarning [kinesis-redshift-sink-0.1.0]:  43 extra bytes at beginning or within zipfile\n  (attempting to process anyway)\n...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/1927#issuecomment-137537771.\n. Cool, thanks. I've been experimenting with the lighter COPY-over-SSH\napproach (to avoid polluting S3 with temp files), with dynamic batching to\nensure predictable load. It is in POC right now, hopefully will have some\ntime to productize it soon.\n\n\nDenis\nOn Tue, Sep 8, 2015 at 8:47 PM, mark9white notifications@github.com wrote:\n\nWe have found the same performance issue - the feeder needs to be changed\nto upload to S3 then load from that into Redshift rather than using JDBC.\nWe've made fixes to enable it to be run as a JAR and a few other things\nat: https://github.com/myinsiders/snowplow\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/1927#issuecomment-138514235.\n. 1. produceCombinedContext is pretty important, so even with 0.8.0 it is required.\n2. Yes, MAXERROR is intentional - we don't want to abort the transaction just because there is an error  in some schema. Those errors do happen - e.g. often when new schemas are added. Because dripfeeding is a continuous process the new schemas are picked up immediately but often the first few events are the test events (and tests fail) so this was introduced. \n\nIn a typical client, you might have 5 to 10 schemas per transaction, and because this is not a batch (with possible manual recovery) process, it is better to produce stl_load_errors for some schemas, while allowing all other schemas (and the main table) to go in. At least it was so for our clients. The limit should of course be configurable though.\n. Yes, this looks better\nOn Tue, Dec 16, 2014 at 4:25 AM, Phil Kallos notifications@github.com\nwrote:\n\nIn\n2-collectors/scala-stream-collector/src/main/scala/com.snowplowanalytics.snowplow.collectors.scalastream/sinks/AbstractSink.scala\nhttps://github.com/snowplow/snowplow/pull/1254#discussion-diff-21840818:\n\n@@ -36,9 +36,13 @@ trait AbstractSink {\n   // Serialize Thrift SnowplowRawEvent objects,\n   // and synchronize because TSerializer doesn't support multi-threaded\n   // serialization.\n-  private val thriftSerializer = new TSerializer\n-  def serializeEvent(event: SnowplowRawEvent): Array[Byte] =\n-    this.synchronized {\n-      thriftSerializer.serialize(event)\n-  private val thriftSerializer = new ThreadLocal[TSerializer]\n\nHow about instead\nprivate val thriftSerializer = new ThreadLocal[TSerializer] {\n override def initialValue = new TSerializer()\n}\nSo the initialization is handled outside of the serializeEvent code?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/1254/files#r21840818.\n. \n",
    "vceron": "We used this script proposed in AWS Labs and we gained 50% of atomic.events disk space (from 530Gb to 250Gb) with 1.07 billion rows.\nThe output sql script is similar to Gabriel's script\n. > The problem must be something to do with threads\nYes, now I truly believe it is linked to threads usage in Sluice.\nNow I've no problems with processing raw files but with the storage in archive.\nThis morning i identified 4 missing files for yesterday's runs (1h, 9h, 12h, 21h).\ngrep -i \"archive/raw\" | grep MOVE | egrep \"30-01|30-09|30-12|30-21\"\nDoing the previous grep on EmrEtlRunner output I found that some files wrere overwritten, independently of the instance.\n```\n[Mon Jan 30 08:15:05 UTC 2017] (t5)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-01.us-east-1.i-096c6fe1.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-01.us-east-1.i-096c6fe1.txt.gz\n[Mon Jan 30 08:15:05 UTC 2017] (t7)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-01.us-east-1.i-d5bf5d5z.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-01.us-east-1.i-050292a4.txt.gz\n[Mon Jan 30 08:15:05 UTC 2017] (t0)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-01.us-east-1.i-050292a4.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-01.us-east-1.i-050292a4.txt.gz\n[Mon Jan 30 09:52:04 UTC 2017] (t8)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-09.us-east-1.i-d5bf5d5z.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-09.us-east-1.i-d5bf5d5z.txt.gz\n[Mon Jan 30 09:52:04 UTC 2017] (t5)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-09.us-east-1.i-050292a4.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-09.us-east-1.i-096c6fe1.txt.gz\n[Mon Jan 30 09:52:04 UTC 2017] (t6)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-09.us-east-1.i-096c6fe1.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-09.us-east-1.i-096c6fe1.txt.gz\n[Mon Jan 30 12:52:05 UTC 2017] (t6)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-12.us-east-1.i-d5bf5d5z.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-12.us-east-1.i-d5bf5d5z.txt.gz\n[Mon Jan 30 12:52:05 UTC 2017] (t4)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-12.us-east-1.i-050292a4.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-12.us-east-1.i-050292a4.txt.gz\n[Mon Jan 30 12:52:05 UTC 2017] (t5)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-12.us-east-1.i-096c6fe1.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-12.us-east-1.i-d5bf5d5z.txt.gz\n[Mon Jan 30 23:15:05 UTC 2017] (t4)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-21.us-east-1.i-050292a4.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-21.us-east-1.i-096c6fe1.txt.gz\n[Mon Jan 30 23:15:05 UTC 2017] (t5)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-21.us-east-1.i-096c6fe1.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-21.us-east-1.i-096c6fe1.txt.gz\n[Mon Jan 30 23:15:05 UTC 2017] (t6)    MOVE snowplow-processing/processing/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-21.us-east-1.i-d5bf5d5z.txt.gz -> snowplow-archive/raw/2017-01-30/var_log_tomcat8_rotated_localhost_access_log.2017-01-30-21.us-east-1.i-d5bf5d5z.txt.gz\n```\nI'm looking forward to install the R87\n. Not yet @BenFradet. We'll update the stack to the last version in some days (a couple of weeks). . The  jruby update to 9.1.6 in r87-chichen-itza helped into, or even fixed, this issue.\n\nFrom the graph : \n- before mid-July we had the r86 with the raw files copying issue.\n- from mid-July until start of August the indicator was broken, as the pattern in S3 bucket changed (to run=xxxxx)\n- from August the indicator was fixed and we notice all the raw files are OK. Here you have a gist to monitor the raw files if needed. So the changes will allow to stage everything (raw, enriched, shredded) from the EMR cluster as for the raw files @BenFradet ?\nAt least for the last 8 days it is OK @alexanderdean, ill keep monitoring it for some weeks before concluding it is solved.. I signed it!. So they will not have this format any more @BenFradet  ?\nvar_log_tomcat8_rotated_localhost_access_log.2017-08-10-19.us-east-1.i-06e3148.txt.gz. OK\nDo you have a command/script to identify missing raw files instead ?\nhttps://gist.github.com/vceron/014dd82cec201862dc8da1020cbd9a3f#file-monitor-snowplow-raw-files-from-collector-L13 . I have a question for you @alexanderdean & @chuwy . Why a lifespan of 6 months has been set?  Is it possible to modify this parameter in a recent release ? \nIt's been 4 days since my DynamoDB table has been created in order to enable cross-batch de-duplication and these are my numbers : \n- Storage size (in bytes) | 2.00 \n- Item count | 18,495,836\n. Manifest ? Do you mean, in Redshift ? \nNot, this number comes from the table details in DynamoDB.. No, I didn't use the Event Manifest Populator, all these items are from a fresh table (today I already have 33M items for ~3.5GB).\nHere is the relate issue : https://github.com/snowplow/snowplow-rdb-loader/issues/84. Is there a wiki page which explains how to build our own hadoop_shred (r88 :-| )  @chuwy ? \nAnd I guess that all asset are taken from the same bucket (s3.bucket.assets), isn't it ? . ",
    "sdboer78": "dup https://github.com/snowplow/snowplow/issues/729\n. ",
    "andrebrov": "Hi guys, any news on this feature?. ",
    "sphinks": "@BenFradet is any news regarding monitoring componenets of Stream Pipeline, especially Stream Enricher.. ",
    "derdeka": "As Amazon does not support AMI 3.7 #1959 for new clusters at any locations anymore and 3.8 #1834 will not be supported, what is the current workaround to get emrrtlrunner running?\n. Same problem here. This make it impossible to debug why data is invalid. We have millions of this lines:\n{\"line\":\"[B@c4ba42b\",\"errors\":[\"error: instance type (object) does not match any allowed primitive type\n. ",
    "robert2d": "fixed in https://github.com/rslifka/elasticity/pull/120 but yet to be merged\n. https://github.com/rslifka/elasticity/pull/141 has been merged \ud83d\ude04 . ",
    "mark9white": "We have found the same performance issue - the feeder needs to be changed to upload to S3 then load from that into Redshift rather than using JDBC.\nWe've made fixes to enable it to be run as a JAR and a few other things at: https://github.com/myinsiders/snowplow\n. ",
    "aldrinleal": "@jramos, you must use:\n$ java -cp <path to jar> com.snowplowanalytics.snowplow.storage.kinesis.redshift.SinkApp --config <config>\nOr switch to Java 8. Its a limitation of older vms, where the MANIFEST wound't be read if past 65K files in the .jar file.\nLink: http://stackoverflow.com/a/27250943/39261\n. @denismo, I found a small gotcha: When dedup is off (I do when debugging), events simply doesn't get logged. Here's how I solved it (as a patch, sorry):\n```\ndiff --git a/4-storage/kinesis-redshift-sink/src/main/scala/com.snowplowanalyti\nindex a150709..a4129ce 100644\n--- a/4-storage/kinesis-redshift-sink/src/main/scala/com.snowplowanalytics.snow\n+++ b/4-storage/kinesis-redshift-sink/src/main/scala/com.snowplowanalytics.snow\n@@ -13,6 +13,7 @@\n package com.snowplowanalytics.snowplow.storage.kinesis.redshift\nimport java.sql.BatchUpdateException\n+import java.util\n import java.util.Properties\nimport com.snowplowanalytics.snowplow.storage.kinesis.redshift.limiter.RatioFl\n@@ -72,11 +73,9 @@ class RedshiftEmitter(config: KinesisConnectorConfiguration,\n     RatioFlushLimiter.totalKinesisRecords.addAndGet(buffer.getRecords.size)\n     val errors: mutable.MutableList[EmitterInput] = new mutable.MutableList[Em\n     try {\n-      if (deduplicate) {\n-        Deduplicator.deduplicate(buffer.getRecords)\n-      } else {\n-        buffer.getRecords\n-      }.foreach { record =>\n+      val source: Iterable[EmitterInput] = if (deduplicate) Deduplicator.dedupicate(buffer.getRecords) else buffer.getRecords\n+\n+      source.foreach { record =>\n         try {\n           shredder.shred(record._1)\n         }\n```\n. ",
    "cdimitroulas": "is this still being worked on and if so what is the current status?. ",
    "bhattdeepak90": "This approach is not working for AMI version 4.3.0, can you please suggest any other method to increase java heap space.\nThe error I am getting is : \"/mnt/var/lib/bootstrap-actions/1/emr-customize-bootsize_unix.sh: line 2: /usr/share/aws/emr/scripts/configure-hadoop: No such file or directory\".\nI think the script configure-hadoop which we are trying to access is not available in this AMI version. Please share the workaround.\nThanks!\nDeepak Bhatt. I have asked the same on Discourse. Please help..\nThanks\n. ",
    "diamondo25": "done\n. ",
    "ajitkshirsagar": "Is this feature planned for the roadmap?. ",
    "danwald": "just did\nOn Wed, Sep 30, 2015 at 3:37 AM Alexander Dean notifications@github.com\nwrote:\n\nThanks! Have you signed our CLA?\nhttps://github.com/snowplow/snowplow/wiki/CLA\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/2064#issuecomment-144220998.\n. Updated Duke...\nCheers,\n- danny\n\nOn Wed, Jan 27, 2016 at 2:52 AM Duke notifications@github.com wrote:\n\nHi @danwald https://github.com/danwald,\nCan you please rebase your branch with master to @alexanderdean\nhttps://github.com/alexanderdean accept it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/2064#issuecomment-175277846.\n. \n",
    "dukex": "Hi @danwald, \nCan you please rebase your branch with master to @alexanderdean accept it?\n. @alexanderdean Do you see any trouble in this pull request?\n. +1\n. +1 \n. @alexanderdean done!\n. @alexanderdean I saw the tests is falling on openjdk, how to fix it?\n. ",
    "xkidro": "snowplow-kinesis-enrich-0.6.0 , updated post right now\n. I want to know if the Scala Elasticsearch sink AWS authentication applies to Elasticsearch also not only for the kinesis stream. I am guessing since it's already authenticated with AWS it should just work.\nI am deploying the reconfigured elasticsearch-sink.conf as we speak, just wanted to know if anyone has tried this.\n. in case anyone is wondering it doesn't work :)\nI even tried a Lambda function with Kinesis Event source but that doesn't work either \n``` javascript\nvar AWS = require('aws-sdk');\nvar path = require('path');\nvar esDomain = {\n    region: 'us-west-2',\n    endpoint: 'search-xxxxx.us-west-2.es.amazonaws.com',\n    index: 'snowplow',\n    doctype: 'enriched'\n};\nvar endpoint = new AWS.Endpoint(esDomain.endpoint);\nvar creds = new AWS.EnvironmentCredentials('AWS');\nexports.handler = function(event, context) {\n    console.log(JSON.stringify(event, null, '  '));\n    event.Records.forEach(function(record) {\n        var jsonDoc = new Buffer(record.kinesis.data, 'base64');\n        postToES(jsonDoc.toString(), context);\n    });\n}\nfunction postToES(doc, context) {\n    var req = new AWS.HttpRequest(endpoint);\nreq.method = 'POST';\nreq.path = path.join('/', esDomain.index, esDomain.doctype);\nreq.region = esDomain.region;\nreq.headers['presigned-expires'] = false;\nreq.headers['Host'] = endpoint.host;\nreq.body = doc;\n\nvar signer = new AWS.Signers.V4(req , 'es');  // es: service code\nsigner.addAuthorization(creds, new Date());\n\nvar send = new AWS.NodeHttpClient();\nsend.handleRequest(req, null, function(httpResp) {\n    var respBody = '';\n    httpResp.on('data', function (chunk) {\n        respBody += chunk;\n    });\n    httpResp.on('end', function (chunk) {\n        console.log('Response: ' + respBody);\n        context.succeed('Lambda added document ' + doc);\n    });\n}, function(err) {\n    console.log('Error: ' + err);\n    context.fail('Lambda failed with error ' + err);\n});\n\n}\n```\n. Hello @fblundun thank you for the clarification, I have a question, it's not really related with this but it is related to the other issue I posted, didn't really study these but would it be possible for the sink to use the http api instead of the transport client? Does the kinesis connectors library support this? If so, would it be hard to modify the sink to work with that? I really want to use the new AWS elasticsearch service but it doesn't support the transport client :(\n. Awesome thanks!\n. ",
    "timelf123": "Does gmail actually respect that header? I was under the impression that they cache all images no matter what\n. ",
    "bernardosrulzon": "Done!\n. No problem, thanks!\n. @ninjabear That's awesome, thanks so much! Quick question - does this PR also support the unique_args argument, used to pass custom parameters? That'd be super useful to measure the impact of specific campaigns, A/B Tests, etc.\n. @bogaert Done! I've reopened as #2276 as I made a big mess out of the commits :disappointed: \n. @bogaert Awesome, thanks!\n. @bogaert Awesome, thanks!\n. Ah, good to see you're working on this! Makes sense - I'll close this PR.\nCheers!\n. A side question for the experts: Snowplow is attempting the redirect users to a URL with spaces, which AFAIK is not best practice - any chances this breaks something for end users?\n. Good point. I think we can encode the first part of the URL http://..., but not the {keyword} variable, which may contain special characters.\nReference: https://www.en.advertisercommunity.com/t5/AdWords-Tracking-and-Reporting/How-do-I-remove-spaces-between-keywords-when-using-ValueTrack/td-p/427714#\n. Sounds good to me!\nOn Nov 18, 2016 10:16 AM, \"Tiago Portela\" notifications@github.com wrote:\n\nSounds perfect \ud83d\udc4d\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/snowplow/snowplow/issues/2913#issuecomment-261518777,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGoiBlEvCYhlL_NEYSMFS__q_gS6eoxRks5q_ZcVgaJpZM4KV4Wz\n.\n. Awesome - that makes perfect sense.\n\nBTW - Are you guys working on a new web-incremental model? I've built most of the queries, would be happy to open another PR if you're not working on it.\n. Closed in favour of #2926\n. Hey @bogaert - I've commited on your feature/web-models. Git newbie here, please check if that worked as you expected.\n. Hey - thanks for the fast reply!\n\nIt's set to false\nHow can we check? It's probably not the case though, as we didn't see any events with that random in uri_redirect. @alexanderdean The event does get through the Hadoop Shred test suite. I'll try to re-process these events in our production pipeline to check whether this is an intermittent issue. Any other ideas?\n\nThanks!!. I think I found the culprit. I had recently updated the configs to use 1 core instance + 3 task instances, trying to lower EMR costs. It turns out the spot instances were responsible for the loss of events.\nDuring this debugging, I used a fixed set of raw files and run the enrich/shred process with different configs. Using spot instances, I lost 5-15% of events - the lost events were random. I'm not sure if this problem affects other types of events - the case described above was the easiest for me to verify, so I only tested uri_redirect. Using core instances, all events came through.\nFor the record, the dataset had 546 unique events to be enriched and shredded. These were the experiments:\n1. [Control] 1x m3.2xlarge for master, 1x m3.2xlarge for core, 3x m3.2xlarge for task (with bid = 0.532)\nResult: 546 unique events in enrich (as expected). 523 unique events in shred (4.2% loss)\n\n\n[Same configs as control, different EMR job] 1x m3.2xlarge for master, 1x m3.2xlarge for core, 3x m3.2xlarge for task (with bid = 0.532)\nResult: 546 unique events in enrich (as expected). 477 unique events in shred (12.6% loss). Lost events were different than previous run. This proves the point that the event loss behavior is somewhat random,\n\n\n[Removing task instances] 1x m3.2xlarge for master, 3x m3.2xlarge for core\nResult: 546 unique events in enrich (as expected). 546 unique events in shred. Passed!\n\n\n[Downsizing master] 1x m1.medium for master, 3x m3.2xlarge for core\nResult: 546 unique events in enrich (as expected). 546 unique events in shred. Passed!\n\n\n@alexanderdean Any ideas why this is happening and what the next steps would be?. Good points. From the monitoring charts, the task instances took ~5min more to provision & bootstrap (vs. core). Otherwise, all other charts point to a healthy job - no reboots, no decommissions, no unhealthy nodes, etc. I set the bid pretty high exactly to avoid that.. Exactly! Hadoop Shred kicked off only 22 mins into the job.. N. Virginia (us-east-1d). The S3 buckets are in S\u00e3o Paulo, though.. Cool, thanks! :). That makes a lot of sense @alexanderdean.\nI guess the key issue is whether Snowplow is ready for spot instances or there's additional development involved in making it resistant to instance termination/rebooting - which would be especially troublesome on the Shred Step as data would already be inserted into DynamoDB.\nAny thoughts on that? /cc @BenFradet . That's the point @chuwy - when a job fails in Shred and we re-process data using EmrEtlRunner, a new job is created with an updated etl_tstamp, which causes the previous events to be discarded as duplicates.\nUnless we can make the cluster resist to an instance shutdown/reboot without failing.. ",
    "mattmueller": ":+1: \n. ",
    "snowindy": "My config looks like this:\naws:\n  access_key_id: XXX\n  secret_access_key: XXX\n  s3:\n    region: us-east-1\n    buckets:\n      assets: s3://snowplow-hosted-assets\n      log: s3://my-snowplow-bucket/log\n      raw:\n        in:\n        - s3://elasticbeanstalk-us-east-1-XXX/resources/environments/logs/publish/e-XXX\n        processing: s3://my-snowplow-bucket/processing\n        archive: s3://my-snowplow-bucket/archive\n      enriched:\n        good: s3://my-snowplow-bucket/out\n        bad: s3://my-snowplow-bucket/bad\n        errors: s3://my-snowplow-bucket/errors\n        archive: s3://my-snowplow-bucket/storage-archive\n      shredded:\n        good: s3://my-snowplow-bucket/out\n        bad: s3://my-snowplow-bucket/bad\n        errors: s3://my-snowplow-bucket/errors\n        archive:\n      jsonpath_assets:\n  emr:\n    ami_version: 3.7.0\n    region: us-east-1\n    jobflow_role: EMR_EC2_DefaultRole\n    service_role: EMR_DefaultRole\n    placement: us-east-1b\n    ec2_key_name: emr-keypair\n    bootstrap: []\n    software:\n      lingual: '1.1'\n    jobflow:\n      master_instance_type: m1.medium\n      core_instance_count: 2\n      core_instance_type: m1.medium\n      task_instance_count: 0\n      task_instance_type: m1.medium\n      task_instance_bid: 0.015\n    bootstrap_failure_tries: 3\ncollectors:\n  format: clj-tomcat\nenrich:\n  job_name: Snowplow ETL\n  versions:\n    hadoop_enrich: 1.1.0\n    hadoop_shred: 0.5.0\n  continue_on_unexpected_error: false\n  output_compression: NONE\nstorage:\n  download:\n    folder: /var/storageloader\n  targets:\n  - name: My PostgreSQL database\n    type: postgres\n    host: localhost\n    database: xxxxx\n    port: 5432\n    table: atomic.events\n    username: postgres\n    password: xxxx\n    maxerror:\n    comprows:\n    ssl_mode: disable\nmonitoring:\n  tags: {}\n  logging:\n    level: DEBUG\n  snowplow:\n. I updated issue body bottom with result of 2 commands execution.\n. ",
    "dennisatspaceape": "I can only speak for my use-case. But quite often when the atomic events schema is initially created the VARCHAR lengths are set to something reasonable at the time. As things evolve, developers make changes, SDK's are updated the values can change and no longer subscribe to the targets columns specifications. Unfortunately, it's not easy to detect this in advance especially if it comes from a third party.\nIn our current setup we have two columns which are being truncated on a regular basis which are used by SQL queries. We only noticed the truncation after another application I wrote flagged that the target columns were too small.\nIn an ideal world I'd like the TRUNCATECOLUMNS option to be at the column level (possibly in ETL code). There are some things which I really don't care about and can be truncated and others like transaction_id (this was being truncated for us) which must never be truncated and warrants further investigation. Being able to turn TRUNCATECOLUMNS off would at least cause the load to fail (if > maxerrors) and provide some alerting/visibility that a problem exists.\nThis is definitely not a perfect solution, but for me there is a difference between hiding a single failure and several thousand failures a day. Perhaps there is a better way of doing this in Open Source Snowplow?\n. ",
    "miike": "Done!\n. I think this new commit hopefully addresses and fixes most of the comments made above.\n. @jbeemster No worries. Do you still want me to address the comments above or should I close this PR?\n. Akka-http 10.1.0-RC1 has basic preview support for HTTP/2 but it's missing a few additional API endpoints and the API isn't stable yet.\nJava 9 (Scala 2.13 - targeting Q1 2018 release) contains ALPN support which may possibly be tricky to add in earlier versions.. @jramos I must have missed that previous issue when searching through the Github issues!\n@BenFradet The change to the code here is trivial but I'm struggling to write a test case for it. The tests in PostSpec only seem to test for setting cookie values back on the HttpRequest object and the actual creation of the payload only seems to be tested by manually constructing a responseHandler.cookie object rather than making a POST request. Any thoughts on the best/idiomatic way to do this?. In that case it looks like #2685 should solve the issue @jramos . @BenFradet My mistake, I was testing this locally and forgot to move the file. Fixed.. Yes - this would supersede #3063 (at least the collector/pubsub component of it).. Thanks for the quick turnaround on the review @BenFradet. Latest commit should hopefully fix all those issues.. Thanks @BenFradet, exciting!. Excited about ZSTD being in here! :tada:. I'd never heard of TJSON before - the spec looks quite interesting.. I've been having a look at S3 encryption recently.\nThere's three types of S3 encryption supported: customer provided (SSE-C), Key Management based (SSE-KMS) and S3 managed (SSE-S3).\nSSE-C isn't supported by EMR or Redshift COPY so we can eliminate having to deal with that at the moment [1], [2].\nNeither SSE-KMS or SSE-S3 required modifying the COPY command to include an ENCRYPTED argument so this doesn't require any changes to the way Redshift is loaded [3].\nThe --s3ServerSideEncryption flag may work with no changes to the EMR cluster for SSE-S3 but we may need to add an extra Encryption parameter for EMRFS to support SSE-KMS [4] - it looks like there's at least 1 additional step to support SSE-KMS [5]\n[1] https://docs.aws.amazon.com/emr/latest/DeveloperGuide/emr-emrfs-encryption.html\n[2] https://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html\n[3] https://docs.aws.amazon.com/redshift/latest/dg/c_loading-encrypted-files.html\n[4] https://docs.aws.amazon.com/emr/latest/DeveloperGuide/emr-emrfs-encryption.html\n[5] https://docs.aws.amazon.com/emr/latest/DeveloperGuide/emr-create-security-configuration.html#emr-security-configuration-encryption. Love it!\nIs it worth adding a parameter to igluctl that allows setting the strategy within a schema?\nThis would mean that:\n1) It's possible to visually inspect what fields are being pseudoanonymized in a schema\n2) The appropriate DDL could generate VARCHAR(128) or similar depending on the strategy set\nIs it overkill to add a salt/shared secret option to the configuration? In circumstances where there might be reasonably low cardinality (such as SSN) it would be possible for a determined individual to deanonymize with a dictionary/rainbow tables attack (though this would add some degree of complexity and is less related to GDPR directly).. Ah great - missed that one!. Sure thing @BenFradet, done!. This would be amazing!. That's a really interesting approach!. That's a fair question and I'm not too sure. I'm open to any suggestions on how I could relax those assumptions to make it more inclusive?. :+1: For circe - I've had some good experiences with it when using Scio/Dataflow.. @razius Could you list all the fields you've seen that are subject to the translation issue and I'll patch and update them?. @razius Could you list all the fields you've seen that are subject to the translation issue and I'll patch and update them?. @wheller This config is shared between multiple collectors (e.g., the Scala stream collector for real time) which logs higher up in a bucket. There is some mention of the path required here for emr-etl-runner but it's a few years old now.. Agreed - it isn't semantically ideal from a REST point of view but could also make it quite a bit easier to support POST queries to GraphQL APIs.. This is a great addition! Should save some pain when using some webhooks that batch events in arrays.. One of the tests is failing at the moment. I'll fix this once we've confirmed where we want to put the configuration option.. Was it because of the greenlet dependency? I believe @mhadam has fixed this in the yet to be released 0.8.1 https://github.com/snowplow/snowplow-python-tracker/pull/209. What's the plan with event recovery? Is there going to be a new Spark version / overhaul?. Well this is exciting.. Can mutation be added as an option (defaulting to false in the JS config?) as I suspect there's quite a few JS enrichments mutating events even if that's not ideal.. That make sense though it feels like the user should have some input into what occurs so this doesn't break existing JS enrichments that do mutate an event (e.g., warn, go to bad rows, hard fail etc).. Could we add a random delay parameter to this?\nThis would help avoid any thundering herd scenarios in which a load balancer may become unhealthy and then when it returns again a large number of clients flush their local buffers simultaneously.. Looks like we missed an instance here. com.vero => com.getvero. I'm not too sure on the semantics of this. For Redshift we need to create an entirely new table whereas for Postgres we can just run an ALTER COLUMN.. Good catch, I've changed this.. I tried this in 10.5 and it seemed to work. Should I try this on other versions?. I could be incorrect but I though that Postgres DDLs are transactional so this would be rolled back.. ",
    "bcharp": "Any update ? I'm interested too.\n. Well, it made sense in my mind with this specific mindset : As snowplow is always using the most standard way to do things with AWS, Firehose is surely on the way to enable me to have a streaming pipeline directly from my events to Redshift. But, in fact, the more I read on the streaming pipeline, the more I believe that it's actually not made to insert data into Redshift. \n. I'm currently upgrading our snowplow stack and it worked well before R83 (at least in our case). In R83, we've got clojure collector in europe and emr, etc.. in US Standard, it doesn't work. I think it made sense to be able to have collector in multiple region and only one pipeline.. ",
    "johnrobertfitz": "It's kind of hacky, but I was able to get streaming to Redshift every minute by doing: \nScala Collector -> Log File Buffer\nLog File Buffer -> Scala Enrich\nScala Enrich -> Enriched Log Files\nEriched Log Files -> Firehose via Kinesis Firehose Agent\nFirehose -> Redshift (5MB buffer 60 second interval)\n. ",
    "harmy": "@johnrobertfitz Thanks for sharing, I am also intertested in this subject, would you please show me some more details? like how can scala enrich do with log file buffer?. ",
    "fingerco": "This is also a rough hack, but I thought I'd post it here in case it'd help someone towards their own solution. I was able to modify the kinesis-tee project to push (TSV -> JSON transformed) events to Firehose instead of pushing to another Kinesis stream:\nhttps://github.com/snowplow/kinesis-tee/compare/master...fingerco:firehose\nWhich makes our current process:\nScala Collector -> Raw Kinesis\nRaw Kinesis -> Scala Enrich\nScala Enrich -> Enriched Kinesis\nEnriched Kinesis -> Kinesis Tee (JSON Transform)\nKinesis Tee -> Firehose\nFirehose -> Redshift\nI think there's some value to having a hosted storage loader, that I don't have to manage or worry about. At least on a small team. Firehose also gives a nice overlay to how things are going overall:\n\nI don't know whether it's useful enough to warrant official support. But, it helps a little towards sleeping soundly at night!. ",
    "jdub": "A year later, Firehose is available in most AWS regions, and there still seems to be some interest on the forums and elsewhere. Is this still a wontfix?. ",
    "mitulshah44": "My EmrEtlRunner config.yml file:\n```\nlogging:\n  level: DEBUG # You can optionally switch to INFO for production\naws:\n  # Credentials can be hardcoded or set in environment variables\n  access_key_id: AK*ZA\n  secret_access_key: Ro*toZ\n  s3:\n    region: us-east-1\n    buckets:\n      assets: s3://snowplow-hosted-assets # DO NOT CHANGE unless you are hosting the jarfiles etc yourself in your own bucket\n      jsonpath_assets: # If you have defined your own JSON Schemas, add the s3:// path to your own JSON Path files in your own bucket here\n      log: s3n://mf-snowplow-logs\n      raw:\n        in:\n          - s3n://mf-snowplow-logs        # e.g. s3://my-archive-bucket/raw\n        processing: s3n://mf-snowplow-logs/processing\n        archive: s3://mf-snowplow-archive/raw    # e.g. s3://my-archive-bucket/raw\n      enriched:\n        good: s3://mf-snowplow-out/enriched/good       # e.g. s3://my-out-bucket/enriched/good\n        bad: s3://mf-snowplow-out/enriched/bad        # e.g. s3://my-out-bucket/enriched/bad\n        errors: s3://mf-snowplow-out/enriched/errors     # Leave blank unless :continue_on_unexpected_error: set to true below\n        archive: s3://mf-snowplow-out/enriched/archive  # Where to archive enriched events to, e.g. s3://my-out-bucket/enriched/archive\n      shredded:\n        good: s3://mf-snowplow-out/shredded/good       # e.g. s3://my-out-bucket/shredded/good\n         bad: s3://mf-snowplow-out/shredded/bad        # e.g. s3://my-out-bucket/shredded/bad\n        errors: s3://mf-snowplow-out/shredded/errors     # Leave blank unless :continue_on_unexpected_error: set to true below\narchive: s3://metafunding-snowplow-emretlrunner/shredded/archive    # Not required for Postgres currently\nemr:\n    ami_version: 3.7.0      # Don't change this\n    region: us-east-1        # Always set this\n    jobflow_role: EMR_EC2_DefaultRole # Created using $ aws emr create-default-roles\n    service_role: EMR_DefaultRole     # Created using $ aws emr create-default-roles\n    placement: us-east-1c     # Set this if not running in VPC. Leave blank otherwise\n    ec2_subnet_id: subnet-e904f6b0  # Set this if running in VPC. Leave blank otherwise\n    ec2_key_name: demo-iqr-snowplow\n    bootstrap: []           # Set this to specify custom boostrap actions. Leave empty otherwise\n    software:\n      hbase:                # To launch on cluster, provide version, \"0.92.0\", keep quotes\n      lingual:              # To launch on cluster, provide version, \"1.1\", keep quotes\n    # Adjust your Hadoop cluster below\n    jobflow:\n      master_instance_type: m1.medium\n      core_instance_count: 2\n      core_instance_type: m1.medium\n      task_instance_count: 0 # Increase to use spot instances\n      task_instance_type: m1.medium\n      task_instance_bid: 0.015 # In USD. Adjust bid, or leave blank for non-spot-priced (i.e. on-demand) task instances\n    bootstrap_failure_tries: 3 # Number of times to attempt the job in the event of bootstrap failures\ncollectors:\n  format: cloudfront # Or 'clj-tomcat' for the Clojure Collector, or 'thrift' for Thrift records, or 'tsv/com.amazon.aws.cloudfront/wd_access_log' for Cloudfront access logs\nenrich:\n  job_name: Snowplow ETL # Give your job a name\n  versions:\n    hadoop_enrich: 1.2.0 # Version of the Hadoop Enrichment process\n    hadoop_shred: 0.5.0 # Version of the Hadoop Shredding process\n  continue_on_unexpected_error: false # Set to 'true' (and set :out_errors: above) if you don't want any exceptions thrown from ETL\n  output_compression: NONE # Compression only supported with Redshift, set to NONE if you have Postgres targets. Allowed formats: NONE, GZIP\nstorage:\n  download:\n    folder: # Postgres-only config option. Where to store the downloaded files. Leave blank for Redshift\n  targets:\n  format: cloudfront # Or 'clj-tomcat' for the Clojure Collector, or 'thrift' for Thrift records, or 'tsv/com.amazon.aws.cloudfront/wd_access_log' for Cloudfront access logs\nenrich:\n  job_name: Snowplow ETL # Give your job a name\n  versions:\n    hadoop_enrich: 1.2.0 # Version of the Hadoop Enrichment process\n    hadoop_shred: 0.5.0 # Version of the Hadoop Shredding process\n  continue_on_unexpected_error: false # Set to 'true' (and set :out_errors: above) if you don't want any exceptions thrown from ETL\n  output_compression: NONE # Compression only supported with Redshift, set to NONE if you have Postgres targets. Allowed formats: NONE, GZIP\nstorage:\n  download:\n    folder: # Postgres-only config option. Where to store the downloaded files. Leave blank for Redshift\n  targets:\n    - name: \"My PostgreSQL database\"\n      type: postgres\n      host: 127.0.0.1 # Hostname of database server\n      database: snowplow # Name of database\n      port: 5432 # Default Postgres port\n      ssl_mode: disable # One of disable (default), require, verify-ca or verify-full\n      table: atomic.events\n      username: power_user\n      password: password\n      maxerror: # Not required for Postgres\n      comprows: # Not required for Postgres\nmonitoring:\n  tags: {} # Name-value pairs describing this job\n  logging:\n    level: DEBUG # You can optionally switch to INFO for production\n  snowplow:\n    method: get\n    app_id: snowplow12 # e.g. snowplow\n    collector: dlf****1m3.cloudfront.net \n```\nI'm using same config file for storageloader. Is anything missed or wrong ?\n. what should I put in storage.download.folder? I'm using PostgreSQL setting up in same AWS EC2 instance. \ncan you please provide me one example for it?\n. I paste the same code but getting this error\nDownload folder 'mydownloadfolder/' not found\nMy mydownloadfolder is exists in config directory at snowplow/4-storage/storage-loader/config/\nwhat's the wrong now?\n. @fblundun Thanks buddy, process is started using absolute path, but got error again at the end of process.\nDownloading Snowplow events...\n  downloading files from s3://mf-snowplow-out/enriched/good/ to /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/\n(t2)    DOWNLOAD mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/part-00000 +-> /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/run=2015-12-01-09-46-29/part-00000\n(t1)    DOWNLOAD mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/part-00001 +-> /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/run=2015-12-01-09-46-29/part-00001\n      +/> /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/run=2015-12-01-09-46-29/part-00000\n      +/> /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/run=2015-12-01-09-46-29/part-00001\nLoading Snowplow events into My PostgreSQL database (PostgreSQL database)...\nOpening database connection ...\nRunning COPY command with data from: /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/run=2015-12-01-09-46-29/part-00001\nRunning COPY command with data from: /home/ubuntu/snowplow/4-storage/storage-loader/config/mydownloadfolder/run=2015-12-01-09-46-29/part-00000\nI, [2015-12-02T10:52:31.245000 #1352]  INFO -- : SnowplowTracker::Emitter initialized with endpoint http://dlfshzdhg01m3.cloudfront.net:80/i\nI, [2015-12-02T10:52:31.310000 #1352]  INFO -- : Attempting to send 1 request\nI, [2015-12-02T10:52:31.317000 #1352]  INFO -- : Sending GET request to http://dlfshzdhg01m3.cloudfront.net:80/i...\nI, [2015-12-02T10:52:31.626000 #1352]  INFO -- : GET request to http://dlfshzdhg01m3.cloudfront.net:80/i finished with status code 200\nArchiving Snowplow events...\n  moving files from s3://mf-snowplow-out/enriched/good/ to s3://mf-snowplow-out/enriched/archive/\n(t1)    MOVE mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/part-00000 -> mf-snowplow-out/enriched/archive/run=2015-12-01-09-46-29/part-00000\n(t3)    MOVE mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/part-00001 -> mf-snowplow-out/enriched/archive/run=2015-12-01-09-46-29/part-00001\n      +-> mf-snowplow-out/enriched/archive/run=2015-12-01-09-46-29/part-00001\n      +-> mf-snowplow-out/enriched/archive/run=2015-12-01-09-46-29/part-00000\n      x mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/part-00001\n      x mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/part-00000\n  moving files from s3://mf-snowplow-out/enriched/good/ to s3://mf-snowplow-out/enriched/archive/\n(t0)    MOVE mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/_SUCCESS -> mf-snowplow-out/enriched/archive/run=2015-12-01-09-46-29/_SUCCESS\n      +-> mf-snowplow-out/enriched/archive/run=2015-12-01-09-46-29/_SUCCESS\n      x mf-snowplow-out/enriched/good/run=2015-12-01-09-46-29/_SUCCESS\nUnexpected error: Contract violation for argument 1 of 1:\n    Expected: String,\n    Actual: nil\n    Value guarded in: Sluice::Storage::S3::Location::initialize\n    With Contract: String => Sluice::Storage::S3::Location\n    At: /usr/local/rvm/gems/jruby-1.7.19/gems/sluice-0.2.2/lib/sluice/storage/s3/location.rb:35 \n/usr/local/rvm/gems/jruby-1.7.19/gems/contracts-0.7/lib/contracts.rb:69:in `Contract'\norg/jruby/RubyProc.java:271:in `call'\n/usr/local/rvm/gems/jruby-1.7.19/gems/contracts-0.7/lib/contracts.rb:147:in `failure_callback'\n/usr/local/rvm/gems/jruby-1.7.19/gems/contracts-0.7/lib/contracts/decorators.rb:164:in `common_method_added'\n/usr/local/rvm/gems/jruby-1.7.19/gems/contracts-0.7/lib/contracts/decorators.rb:159:in `common_method_added'\n/home/ubuntu/snowplow/4-storage/storage-loader/deploy/snowplow-storage-loader!/storage-loader/lib/snowplow-storage-loader/s3_tasks.rb:91:in `archive_files_of_type'\n/home/ubuntu/snowplow/4-storage/storage-loader/deploy/snowplow-storage-loader!/storage-loader/lib/snowplow-storage-loader/s3_tasks.rb:68:in `archive_files'\nfile:/home/ubuntu/snowplow/4-storage/storage-loader/deploy/snowplow-storage-loader!/storage-loader/bin/snowplow-storage-loader:63:in `(root)'\norg/jruby/RubyKernel.java:1091:in `load'\nfile:/home/ubuntu/snowplow/4-storage/storage-loader/deploy/snowplow-storage-loader!/META-INF/main.rb:1:in `(root)'\norg/jruby/RubyKernel.java:1072:in `require'\nfile:/home/ubuntu/snowplow/4-storage/storage-loader/deploy/snowplow-storage-loader!/META-INF/main.rb:1:in `(root)'\n/tmp/jruby990240094077490356extract/jruby-stdlib-1.7.20.1.jar!/META-INF/jruby.home/lib/ruby/shared/rubygems/core_ext/kernel_require.rb:1:in `(root)'\n. @fblundun for scheduling of Emretlrunner ask to set path for RUNNER_ENRICHMENTS=/path/to/your-enrichment-jsons\ncan you please suggest me where the enrichment-jsons is stored generally? I'm not able to find json file.\n. ",
    "aragnon": "The cause of all of this is that simply doing vagrant ssh as the instructions suggest is wrong, because incorrect/exotic settings on the client machine leak into the virtual machine. To fix this properly, it would be better to configure the virtual machine to not use the locale settings of the client by default and to have some custom configuration file for those people who want to do things differently \n. ",
    "claudiomartins": "+1 on this \nThis is incredibly important for multi-layered enhancers - we are starting to work with multiple types of events (struct/unstruct) that would be very useful to created dependencies among them and multiple enhancers of the same type (mostly API).. +1 for Postgres shredding - we extensively use Postgres for all our needs and would help to not have to run another ETL to export data out of Redshift into Postgres.. ",
    "thomasintveld": "The ability to use multiple enrichments of the same type in parallel is very important \u2014\u00a0and one could argue that SQL Enrichment is not fully supported without it. I.e. you now have to choose whether you enrich events with user-data, or with data about the page they are viewing. +1 on this. ",
    "tdevitt": "+1\n. +1\n. I noticed sql-runner appears to still be using collector_tstamp instead of derived_tstamp as well, so you guys may want to update that when you get to this issue\n. ",
    "RyanCodes": "@chuwy @alexanderdean can this enrichment be used?. @chuwy thanks for getting back to me! where would one start to construct a PR is there documentation somewhere that can help guide me to the proper repository?. ",
    "0xABAB": "Wow, that's fast. Do you use some special tooling to respond that fast? \n. Very nice.\n. IMHO, this spec is much clearer: http://tools.ietf.org/html/rfc2965 . The language used to describe domain matching in RFC 6265 is terrible. \n. https://github.com/snowplow/snowplow/wiki/javascript-tracker#wiki-events (another example, only to demonstrate the size of the issue). I agree that a github-wiki-link-validator should be part of your infrastructure. \n. This did work, but what I would like to do is run the Clojure collector in this Virtualbox VM in Tomcat 8. How can I do that? \n. Detecting the number of CPUs from Ruby is just a library call away: \nSee: https://github.com/grosser/parallel\nI don't really see any reason for hard-coded constants. In short, I don't see any problem here. Using the time to write a comment could also have been used to just fix the problem in the first place. \n. ",
    "digitaltouch": "I believe this should address everything. I can issue PR if you would like - or wait until the milestone completion gets closer. \n. ",
    "rbkn": "Thanks Alex!\n. ",
    "jgautheron": "@alexanderdean any update on this?. Hi! @alexanderdean do you still believe it would make a good fit for Snowplow?. ",
    "ranjithnaidu": "From Activity's Constructor\npublic MyActivity(){\n    SnowPlowAnalyticsManager.initTracker(context);\n}\nAnayltics manager\nimport android.content.Context;\nimport com.appleete.haat.FabricApplication;\nimport com.snowplowanalytics.snowplow.tracker.Emitter;\nimport com.snowplowanalytics.snowplow.tracker.Subject;\nimport com.snowplowanalytics.snowplow.tracker.Tracker;\nimport com.snowplowanalytics.snowplow.tracker.emitter.HttpMethod;\nimport com.snowplowanalytics.snowplow.tracker.emitter.RequestSecurity;\nimport com.snowplowanalytics.snowplow.tracker.events.Unstructured;\nimport com.snowplowanalytics.snowplow.tracker.payload.SelfDescribingJson;\nimport java.text.SimpleDateFormat;\nimport java.util.Calendar;\nimport java.util.HashMap;\nimport java.util.Map;\n/*\n- Created by Ranjithnaidu on 03/02/16.\n  /\n  public class SnowPlowAnalyticsManager {\npublic static final String TRACKER_NAME_SPACE = \"snowplowTracker\";\n  public static final String APP_ID = \"haat_snowplow\";\n  private static final String SNOWPLOW_PRODUCT_REACH = \"haat\";\n  private static final String SNOWPLOW_ACTION_LOAD = \"load\";\n  private static final String SNOWPLOW_EVENT_CARD_VIEW = \"card_view\";\n  public static Context appContext = null;\n  public static final boolean useBase64 = true;\n  public static final String USER_ID = \"1009\";\n  public static final String TRACKER_URI = \"\";\nprivate static final String SNOWPLOW_KEY_PRODUCT_NAME = \"$1$product_name\";\n  private static final String SNOWPLOW_KEY_ACTION_NAME= \"$1$action_name\";\n  private static final String SNOWPLOW_KEY_EVENT_NAME = \"$1$event_name\";\n  private static final String SNOWPLOW_KEY_LOCALITY = \"$1$locality\";\n  private static final String SNOWPLOW_KEY_PRODUCT_ID = \"$1$product_id\";\n  private static final String SNOWPLOW_KEY_SUBSCRIPTION_ID = \"$1$subscription_id\";\n  private static final String SNOWPLOW_DATE_TIME_FORMAT = \"yyyy-MM-dd'T'HH:mm:ss\";\n  private static final String SNOWPLOW_KEY_EVENT_DATE = \"$1$event_date\";\npublic static Tracker mTracker;\n  public static Emitter e1;\npublic static void initTracker(Context context){\n      appContext = HaatApplication.getContext();\n  //        try {\n          e1 = new Emitter.EmitterBuilder(TRACKER_URI, context).build();\n          e1.setHttpMethod(HttpMethod.GET);\n          e1.setRequestSecurity(RequestSecurity.HTTP);\nmTracker = new Tracker\n              .TrackerBuilder(e1, TRACKER_NAME_SPACE, APP_ID, context)\n              .base64(useBase64)\n              .subject(new Subject.SubjectBuilder().build())\n              .build();\n//        }catch (Exception e){\n  //            e.printStackTrace();\n  //        }\n  }\npublic static void sendReachEvent(){\n```\n  Map _event = new HashMap<>();\n  _event.put(SNOWPLOW_KEY_PRODUCT_NAME, SNOWPLOW_PRODUCT_REACH);\n  _event.put(SNOWPLOW_KEY_ACTION_NAME, SNOWPLOW_ACTION_LOAD);\n  _event.put(SNOWPLOW_KEY_PRODUCT_ID, 265);\n  _event.put(SNOWPLOW_KEY_SUBSCRIPTION_ID, 5999);\nCalendar c = Calendar.getInstance();\n  SimpleDateFormat df = new SimpleDateFormat(SNOWPLOW_DATE_TIME_FORMAT);\n_event.put(SNOWPLOW_KEY_EVENT_DATE, df.format(c.getTime()));\nSelfDescribingJson eventData = new SelfDescribingJson(\"null\").setData(_event);\n  Unstructured unstructuredEvent = Unstructured.builder().eventData(eventData).build();\n// Set user id\n  mTracker.getSubject().setUserId(USER_ID);\n// Send data\n  mTracker.track(unstructuredEvent);\n  ```\n}\n}\nStackTrace of the exception\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime: FATAL EXCEPTION: main\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime: Process: com.practo.fabric, PID: 29350\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime: java.lang.RuntimeException: Unable to start activity ComponentInfo{com.practo.fabric/com.practo.fabric.result.SearchResultActivity}: java.lang.IllegalStateException: Can\u2019t create emitter\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2325)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2387)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.access$800(ActivityThread.java:151)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1303)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.os.Handler.dispatchMessage(Handler.java:102)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.os.Looper.loop(Looper.java:135)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.main(ActivityThread.java:5254)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.reflect.Method.invoke(Native Method)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.reflect.Method.invoke(Method.java:372)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:903)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:698)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:  Caused by: java.lang.IllegalStateException: Can\u2019t create emitter\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.snowplowanalytics.snowplow.tracker.a$a.a(Emitter.java:237)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.misc.af.a(SnowplowAnalyticsManager.java:51)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.search.c$c.(ProductSearch.java:769)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.search.c.(ProductSearch.java:105)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.search.h.a(SearchFactory.java:18)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.result.c.onCreate(SearchResultFragment.java:170)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.Fragment.performCreate(Fragment.java:1939)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.o.a(FragmentManager.java:1029)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.o.a(FragmentManager.java:1248)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.f.run(BackStackRecord.java:738)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.o.i(FragmentManager.java:1613)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.l.p(FragmentController.java:330)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.j.onStart(FragmentActivity.java:547)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.result.SearchResultActivity.onStart(SearchResultActivity.java:113)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.Instrumentation.callActivityOnStart(Instrumentation.java:1236)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.Activity.performStart(Activity.java:6006)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2288)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2387)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.access$800(ActivityThread.java:151)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1303)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.os.Looper.loop(Looper.java:135)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.main(ActivityThread.java:5254)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.reflect.Method.invoke(Native Method)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:903)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:698)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:  Caused by: java.lang.NoSuchMethodException:  [class com.snowplowanalytics.snowplow.tracker.a$a]\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.Class.getConstructor(Class.java:531)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.Class.getDeclaredConstructor(Class.java:510)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.snowplowanalytics.snowplow.tracker.a$a.a(Emitter.java:234)\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.misc.af.a(SnowplowAnalyticsManager.java:51)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.search.c$c.(ProductSearch.java:769)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.search.c.(ProductSearch.java:105)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.search.h.a(SearchFactory.java:18)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.result.c.onCreate(SearchResultFragment.java:170)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.Fragment.performCreate(Fragment.java:1939)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.o.a(FragmentManager.java:1029)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.o.a(FragmentManager.java:1248)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.f.run(BackStackRecord.java:738)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.o.i(FragmentManager.java:1613)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.l.p(FragmentController.java:330)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.support.v4.app.j.onStart(FragmentActivity.java:547)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.practo.fabric.result.SearchResultActivity.onStart(SearchResultActivity.java:113)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.Instrumentation.callActivityOnStart(Instrumentation.java:1236)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.Activity.performStart(Activity.java:6006)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2288)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2387)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.access$800(ActivityThread.java:151)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1303)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.os.Looper.loop(Looper.java:135)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at android.app.ActivityThread.main(ActivityThread.java:5254)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.reflect.Method.invoke(Native Method)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:903)\u00a0\n02-11 05:13:34.577 29350-29350/com.practo.fabric E/AndroidRuntime:     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:698)\u00a0\n. thanks @jbeemster \n. ",
    "khalidjaz": "Fixed your comment, I just signed the CLA :)\n. Done thank you so much @chuwy and @alexanderdean!\n. ",
    "ironsideshu": "That will be done by the custom DataWorks endpoint in the backend. The calls to create and run DataWorks activities has to run through Bluemix right now, hence the reason for the DataWorks proxy. Whenever IBM decides to drop that limitation to DataWorks, I can rework the code.\n. @alexanderdean yup, I can add a tidbit on what is needed to setup DashDB and DataWorks! for the atomic events table definition, a small problem:\nSchemas can only be created by DashDB admins, and you don't get such a privilege unless you subscribe to their enterprise tiers (tier plans: https://console.ng.bluemix.net/catalog/services/dashdb). The schema that is assigned to the user is of the same name as the autogenerated username that DashDB assigns upon the creation and binding of the service. And that leads to my question: for the DashDB's atomic-def.sql, how do you want me to set it up--towards the users who are going to use it for the Entry level plan or the Enterprise tiers? Alternatively, I can write a short ruby script that can create the atomic-def.sql given some parameters they input. \nThe difference in atomic-def.sql between the Entry and Enterprise tiers are minimal--in Entry, no creation of the atomic schema and the schema that is used is the username (in uppercase), and in Enterprise, the atomic schema would be created and used. For now, I'm just going to assume Enterprise plans.\n. Closing due to limitations of DataWorks' API. Moved onto the DashDB strategy #2908.\n. Refer to #2908\n. @alexanderdean closed out the other PR and uploaded the code to the custom jar for the EMR step that loads from the temporary HDFS -> DashDB: https://github.com/ironsideshu/SnowplowDashDBConn\nShould I have added to the existing fork and created another PR from that instead?\n. Hey @alexanderdean - just pushed the new updates that includes the code to the DashDB load jar and the DashDB ddl for atomic.events. Let me know if there's you need any clarification or help, or if there are any prevalent mistakes I need to fix!\n. Hey @alexanderdean ! Yes, ideally we would want to commericalize the service and roll out different tiers just like how OpenWeatherMap operates (http://openweathermap.org/price). We would need this to be an external lookup since we want to allow the users the ability to upload their own shape boundaries or points (for proximity searches). I'll keep you posted on our plans!\n. yup, you're right. copy and pasting has failed me :(\n. I'm actually not privy of Contracts so I was following redshift_loader as an example. From that class, in load_events_and_shredded_types, the Contract is Hash, Hash => nil, so I figured that snowplow_tracking_enabled is optional? \n. duly noted!\n. yup, you're right! made the change\n. done!\n. I believe I tried this before and had to settle for SMALLINT since BOOLEAN isn't a support column data type in DashDB/DB2. Check out bullet # 2: https://www.ibm.com/support/knowledgecenter/en/SS6NHC/com.ibm.swg.im.dashdb.apdv.sqlpl.doc/doc/c0053651.html\n. This is a little tricky because I cannot on my current test instance since I'm running an 'Entry' tier of DashDB that's on a shared cluster. I'm assuming the higher tiers that run on dedicated nodes can because they have this detailed page on creating indices: http://www.ibm.com/support/knowledgecenter/SS6NHC/com.ibm.swg.im.dashdb.sql.ref.doc/doc/r0000919.html\nWhen I tried creating an index (on say, event_id), a message a spat back telling me the \"operation failed because the operation is not supported with the type of the specified table.\"\n. I say, leave it out for now and maybe add an optional script later that adds indices if the user is running a dedicated instance?\n. ",
    "Graham-M": "Is this still valid?. And London (eu-west-2) whilst you're there, please and thank you.. ",
    "juanstiza": "Hi! I'm having the same issue. In my case it will start working on Ansible playbooks for a while, then a \"done!\" shows up. However the only way of getting out of there is by CTRL+C. I've left it way more than 30 minutes. I'll see if I can get a copy of the output.\n. Here is the output: \n==> default: ===================\n==> default: CONFIGURING ANSIBLE\n==> default: -------------------\n==> default: ... done\n==> default: ==========================================\n==> default: RUNNING PLAYBOOKS WITH ANSIBLE*\n==> default: * no output while each playbook is running\n==> default: ------------------------------------------\n==> default: \n==> default: Setting up Ansible to run out of checkout...\n==> default: \n==> default: PATH=/vagrant/vagrant/ansible/bin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\n==> default: PYTHONPATH=/vagrant/vagrant/ansible/lib:\n==> default: ANSIBLE_LIBRARY=/vagrant/vagrant/ansible/library:/usr/share/ansible/\n==> default: MANPATH=/vagrant/vagrant/ansible/docs/man:\n==> default: \n==> default: Remember, you may wish to specify your host file with -i\n==> default: Done!\n. @alexanderdean thanks a bunch!. High quality internet connection is not something available here right now, I shall wait!\n. Ha! Just found out what the problem is, I got mixed up between stream-enrich and scala-common-enrich. My bad!\n. I downloaded this file: https://dl.bintray.com/snowplow/snowplow-generic/snowplow_kinesis_r84_stellers_sea_eagle.zip and ran the snowplow-stream-enrich-0.9.0 binary.\n. @alexanderdean ah! great, I'll check! Thanks!!\n. Hi, I tested that change and it throws the same The bucket name parameter must be specified when requesting an object error. I'll forward this to Discourse.\n. @jbeemster thanks. I tested the fix and now I'm getting a Unable to calculate a request signature: Unable to calculate a request signature: Empty key, but I believe that might be related to authentication... somehow.\n. ",
    "13scoobie": "I apologize - was making a change locally to our fork for testing and code review and had selected snowplow as comparison branch.\nThanks!\n-dave\n. ",
    "ezefranca": "https://github.com/contact?report=0xABAB\n. ",
    "gyurist": "It seems like the same issue affects the clojure collector as well. \nSince no explicit path is set the default browser behavior is used which sets the sp cookie path to /com.snowplowanalytics.snowplow.\n. ",
    "dannyeuu": "Cool!\n. ",
    "mariussoutier": "I've checked the two parameters before the enrichment process - they are already problematic when extracting from the raw parameters (e.g. rawEvent.parameters.get(\"url\")). Is this something you've already seen?\n. I'm already using common enrich. I also haven't found a lot code that handles these kind of problems.\n. Nevermind, Snowplow is working fine. It's the system that produces the URLs that is inconsistent with encodings.\n. Sure, thanks for your support.\n. ",
    "justingourley": "Guys, thanks for digging in to this. I like option 3 the most as it feels the most flexible. We're scheduling the emr and storage loader tasks synchronously anyway so adding a 3rd step (if desired and maybe some users do not) would seem to be an easy option. Also, it can be tested/used independently of running emr runner which is great when we're running in dev and breaking things/clearing out s3 etc. \n. ",
    "simplesteph": "Kafka-Connect for sure, has a hard dependency on integrating with the schema registry, see http://discourse.snowplowanalytics.com/t/discussion-improvement-kafka-optionally-support-avro-and-the-schema-registry/1127. It's avro. The process is as such.\nDefine your avro schema (optionally register it on your schema registry, although the code should do it for you).\nCreate your generic avro record with your data\nWrite to Kafka using the AvroSerializer (see serializer here: http://docs.confluent.io/3.2.0/schema-registry/docs/serializer-formatter.html#serializer)\nThe link here basically shows you how to write avro data in java: http://docs.confluent.io/3.2.0/schema-registry/docs/serializer-formatter.html#serializer\nMore doc here: http://docs.confluent.io/3.2.0/schema-registry/docs/intro.html#quickstart. @alexanderdean sound counter intuitive right? :)\nLet's have a deeper look at:\nhttp://docs.confluent.io/3.2.0/schema-registry/docs/serializer-formatter.html#serializer\nProperties props = new Properties();\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,\n          io.confluent.kafka.serializers.KafkaAvroSerializer.class);\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n          io.confluent.kafka.serializers.KafkaAvroSerializer.class);\nprops.put(\"schema.registry.url\", \"http://localhost:8081\");\nAs you can see, the serializer isn't just any Avro serializer, it's io.confluent.kafka.serializers.KafkaAvroSerializer.class. Now you can look at the implementation here https://github.com/confluentinc/schema-registry/blob/master/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroSerializer.java, but what it needs is the schema.registry.url, see https://github.com/confluentinc/schema-registry/blob/master/avro-serializer/src/main/java/io/confluent/kafka/serializers/AbstractKafkaAvroSerDeConfig.java#L33\nWhat this serializer does is split the schema from your avro record, stores the schema in the schema registry (that's why you need to provide a url), and then stores the avro record in Kafka. Basically your record is split between Kafka and the schema registry. A consumer needs to use a special deserializer (provided by Confluent again) to retrieve the schema from the registry and add it to the avro record it receives. \nDoes that help?\n. @alexanderdean you should try it out to get more comfortable with it, okay? :)\nThe documentation of what's stored in Kafka is here: http://docs.confluent.io/3.2.0/schema-registry/docs/serializer-formatter.html#wire-format \n```\nThe wire format currently has only a couple of components:\nBytes   Area    Description\n0   Magic Byte  Confluent serialization format version number; currently always 0.\n1-4 Schema ID   4-byte schema ID as returned by the Schema Registry\n5-...   Data    Avro serialized data in Avro\u2019s binary encoding. The only exception is raw bytes, which will be written directly without any special Avro encoding.\nNote that all components are encoded with big-endian ordering, i.e. standard network byte order.\n```\nit's simpler than you think, just include the packages, and one can read / write to kafka using the serializer as it's documented in the example links I sent before. Kinesis sink around line 98, I added the following:\n} catch {\n      case rnfe: ResourceNotFoundException => false\n      case e: Exception => e.printStackTrace(); false\n    }\nElasticSearchSenderHTTP.scala around line 181\n```\ncase e: Exception => {\n          e.printStackTrace()\n          Log.error(\"ElasticsearchEmitter threw an unexpected exception \", e)\n```\nOverall anytime you try catch I believe you should somehow display a clear idea of the error.\n. Yeah the idea of the error is here but somehow the error never got displayed, i.e. in my logs I would get \n\"ElasticsearchEmitter threw an unexpected exception <blank>\n. @alexanderdean would love to get a chance to discuss https://github.com/snowplow/snowplow/pull/3189 before documentation is written. My PR has a few breaking changes but streamlines the whole kafka piece, while providing integration with secure kafka setups. So I looked at this and it is going to be more complicated than expected. \nWe are losing information along the way, because we do Kafka Consumer -> Processing -> Kafka Producer\nThe producer doesn't know the offsets of the original records, and therefore we can't commit there. \nI'm not sure if any offsets are committed in Kinesis or how it's achieved today, but I guess you would have the exact same issues. The behaviour is therefore somewhat consistent (although there is the risk of losing data)\nThis sounds like a perfect use case to use Akka Streams though: http://doc.akka.io/docs/akka/2.4.14/scala/stream/index.html\nIt has a reactive Kafka adapter: https://github.com/akka/reactive-kafka\nSomeone put together a reactive stream for Kinesis too: https://github.com/timeoutdigital/akka-streams-kinesis\nThat will make the framework way more robust in my opinion. \n. If you use Akka Streams framework, I believe you can easily achieve that :) Just need to create a Source interface that has a checkpoint call which can get called when the job is done. \nAn example to look at (at least for inspiration) can be found here: http://doc.akka.io/docs/akka-stream-kafka/current/consumer.html#offset-storage-in-kafka. I think it's just the naming, on my end. I feel more confident when seeing this param with the kafka config, otherwise I'd have to dig up the code and see if it was app-name. \nMaybe the documentation will clear that up?. Hi @chuwy . Yup it works when I open the subfolder with intellij, but not if I open the top level, so I guess that's an IntelliJ limitation. . done. I signed it!. closing in favour of https://github.com/snowplow/snowplow/pull/3189. Alright done. I left a few goodies in there, including a revamp of the sample config using the true semantics from typesafe config (instead of {{ }} which definitely won't work). \nLet me know your review, but I'd love to see this merged for the next release so I don't have to maintain my own Snowplow in the meantime.\nThe whole purpose of this was to fix a few issues and enable security configurations for Kafka. Just realized I'm addressing https://github.com/snowplow/snowplow/issues/2994 as well. \nThis jaas can be set by properties sasl.jaas.config as of kafka clients 0.10.2 (https://kafka.apache.org/documentation/#security_client_dynamicjaas)\nThe expected jaas is \nKafkaClient {\n    com.ibm.messagehub.login.MessageHubLoginModule required\n    serviceName=\"kafka\"\n    username=\"username\"\n    password=\"password\";\n};. @alexanderdean @jbeemster have you had a chance to look at this?. @alexanderdean @jbeemster ping\n. @BenFradet feel free to use what you like in this for your R92, but this should cover a bunch of stuff. Hi @BenFradet , thanks for the feedback. I understand this may be more than the tickets being fixed. I don't use Snowplow anymore unfortunately, so I won't be editing this PR further. I'm happy to give you control of my branch if you want to do further commits on this branch, or you can cherry pick what you need in your own PR. \nThe idea was \n1) to add dynamic config (which allows security)\n2) properly configure Kafka defaults (minus the batch.size param)\n3) refactor the config for extensibility. \nGood luck, hope that helped!. very important fix here, can we please merge ASAP? . @BenFradet no not really. If you don't support this upgrade, you can't support security by passing the jaas conf as a property from producerProps (sasl.jaas.config added in 10.2). Ah true, that's bytes. byteLimit.toString then, right? And buffer.memory removed?. that's important that it's this there if you intend to allow dynamic settings and therefore support security. naming a typesafe config with .sample is invalid. It messes with the compiler, and other stuff when you pass configs as arguments to your programs (especially when using docker java base image). To prevent users from making the same mistake that took me about 4 hours to solve, after debugging typesafe config itself, I'd rather rename this to config.sample.conf. . ",
    "esquire900": "Thank you for your quick reply @fblundun & @alexanderdean.\nPlease note that this also affects lots of other browsers (mostly mobile), fixing this bug in our snowplow script seems to increase traffic with almost 40%.\n. ",
    "danielmohacsi": "Any status update on this?. ",
    "snowplowcla": "Closing, this went into R97 Knossos.. Closing, this went into R97 Knossos.. Closing, this went into R97 Knossos.. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n. Confirmed! @KevinNZ83 has signed the Individual Contributor License Agreement. Thanks so much.\n. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n. Confirmed! @nakulgan has signed the Individual Contributor License Agreement. Thanks so much.\n. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n. Confirmed! @StaymanHou has signed the Individual Contributor License Agreement. Thanks so much.\n. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\n. Confirmed! @pkallos has signed the Individual Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @acgray has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. @rgabo has signed the Software Grant and Corporate Contributor License Agreement. @acgray has signed the Software Grant and Corporate Contributor License Agreement. . Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @vceron has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @clarkeandrew has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @zcei has signed the Individual Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @jatinder85 has signed the Individual Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @simplesteph has signed the Individual Contributor License Agreement. Thanks so much. @simplesteph has signed the Individual Contributor License Agreement. @bernardosrulzon has signed the CLA. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @jramos has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Confirmed! @fridiculous has signed the Individual Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @cpoepke has signed the Individual Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @ashwinGokhale has signed the Individual Contributor License Agreement. Thanks so much.. @ashwinGokhale has signed the Individual Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. @acgray has signed the Software Grant and Corporate Contributor License Agreement. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @premjg has signed the Individual Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Confirmed! @ksnabb has signed the Individual Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. @rbolkey has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @telyukov has signed the Individual Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. @rbolkey has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. @miike has signed the Software Grant and Corporate Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @thalesmello has signed the Individual Contributor License Agreement, thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @jspc has signed the Individual Contributor License Agreement. Thanks so much. @rbolkey has signed the Software Grant and Corporate Contributor License Agreement. @rbolkey has signed the Software Grant and Corporate Contributor License Agreement. @misterpig has signed the Software Grant and Corporate Contributor License Agreement. @misterpig has signed the Software Grant and Corporate Contributor License Agreement. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @romansalin has signed the Individual Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. @arihantsurana has signed the Software Grant and Corporate Contributor License Agreement. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. @userkci has signed the Software Grant and Corporate Contributor License Agreement. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. @userkci has signed the Software Grant and Corporate Contributor License Agreement. @szareiangm  has signed the Software Grant and Corporate Contributor License Agreement. @szareiangm  has signed the Software Grant and Corporate Contributor License Agreement. Confirmed! @kingo55  has signed the Individual Contributor License Agreement. Thanks so much. @szareiangm  has signed the Software Grant and Corporate Contributor License Agreement. @aparra  has signed the Software Grant and Corporate Contributor License Agreement. @jankoulaga  has signed the Software Grant and Corporate Contributor License Agreement. @misterpig  has signed the Software Grant and Corporate Contributor License Agreement. @miike  has signed the Software Grant and Corporate Contributor License Agreement. @miike  has signed the Software Grant and Corporate Contributor License Agreement. @szareiangm  has signed the Software Grant and Corporate Contributor License Agreement. @misterpig  has signed the Software Grant and Corporate Contributor License Agreement. @misterpig  has signed the Software Grant and Corporate Contributor License Agreement. Confirmed! @christopher-fredregill  has signed the Individual Contributor License Agreement. Thanks so much. Thanks for your pull request. Is this your first contribution to a Snowplow open source project? Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn more and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.. Confirmed! @0xE282B0  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @0xE282B0  has signed the Individual Contributor License Agreement. Thanks so much. @christoph-buente  has signed the Software Grant and Corporate Contributor License Agreement. Confirmed! @arunma  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @arunma  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @tonicebrian  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @arunma  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @arunma  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @mirkoprescha  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @mirkoprescha  has signed the Individual Contributor License Agreement. Thanks so much. Confirmed! @tonicebrian  has signed the Individual Contributor License Agreement. Thanks so much. ",
    "shin-nien": "@alexanderdean I don't mind adding it to the ES sink but it'll take a few days to get round to doing/testing it.\n. @alexanderdean wasn't sure if you got an update - ES sink updated. Ready for merge.\n. @morgante if it helps in the meantime, the patched binaries I've been using are here: https://bintray.com/endource/Snowplow\nThey were built off r81 I think...\n. Just needs this PR merged: https://github.com/snowplow/kinesis-s3/pull/67. I'm not familiar with the Kafka sink but iirc it was a generic one line\nchange so you could look at submitting a PR.\nOn 22 Aug 2017 4:55 am, \"Mohammad Mazraeh\" notifications@github.com wrote:\n\nCan we have it in Kafka sink as well ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/2798#issuecomment-323912274,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAuUmT6aKt3MFPfZvdoOV6qxBGM5dcuxks5salETgaJpZM4JHSg5\n.\n. \n",
    "morgante": "Can anything be done to help move this along? Right now it's the only thing blocking us from using the JAR directly.\n. ",
    "brycesteinhoff": "@jbeemster Could this call be added to the Kinesis S3 sink as well?. ",
    "MohammadMazraeh": "Can we have it in Kafka sink as well ?. ",
    "Germanaz0": "I'm using the cloujure collector and I am not sure if the pipeline works too, I am following those tutorials, creating schemas, uploading jsponpaths, and the data for the custom schema is not added.\nI'm also using all strings for the schema to avoid errors, and still, cannot see the iglu webhook working.\nThanks\n. ",
    "philsch": "@alexanderdean done\n. ",
    "kazysgurskas": "Running EMR on eu-central-1 seems to not work at all. I get no errors and it fails on Elasticity Setup Hadoop Debugging. Is this region supported? \n. Tried out attaching EBS for EMR workers. \ncore_instance_ebs:\n  volume_size: 100\n  volume_type: \"gp2\"\nGot this:\n13:58:44 TypeError (no implicit conversion of Symbol into Integer):\n13:58:44     org/jruby/RubyArray.java:1457:in `[]'\n13:58:44     uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:141:in `block in initialize'\n13:58:44     org/jruby/RubyKernel.java:1746:in `tap'\n13:58:44     uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:139:in `block in initialize'\n13:58:44     org/jruby/RubyKernel.java:1746:in `tap'\n13:58:44     uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/emr_job.rb:137:in `initialize'\n13:58:44     uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:in `send_to'\n13:58:44     uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in `call_with'\n13:58:44     uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:in `block in redefine_method'\n13:58:44     uri:classloader:/emr-etl-runner/lib/snowplow-emr-etl-runner/runner.rb:68:in `run'\n13:58:44     uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_reference.rb:43:in `send_to'\n13:58:44     uri:classloader:/gems/contracts-0.11.0/lib/contracts/call_with.rb:76:in `call_with'\n13:58:44     uri:classloader:/gems/contracts-0.11.0/lib/contracts/method_handler.rb:138:in `block in redefine_method'\n13:58:44     uri:classloader:/emr-etl-runner/bin/snowplow-emr-etl-runner:39:in `<main>'\n13:58:44     org/jruby/RubyKernel.java:973:in `load'\n13:58:44     uri:classloader:/META-INF/main.rb:1:in `<main>'\n13:58:44     org/jruby/RubyKernel.java:955:in `require'\n13:58:44     uri:classloader:/META-INF/main.rb:1:in `(root)'\n13:58:44     uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1:in `<main>'. ",
    "RajeshHegde": "BigQuery storage loader\nIntroduction\nBigQuery is Google's fully managed, petabyte scale, low cost analytics data warehouse. BigQuery is serverless, there is no infrastructure to manage and you don't need a database administrator, so you can focus on analyzing data to find meaningful insights, use familiar SQL, and take advantage of pay-as-you-go model.\nSetup BigQuery\n\nInstall BigQuery CLI - https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu\nCreate a project in Google Console\nMake sure you have access right to create BigQuery dataset and upload data\n\nConfiguration\nYou can add BigQuery as one of the storage targets.\n```\n\nname: \"Name of the project\"\n  type: bigquery\n  schema: /path/to/json/schema/file\n  database: \"Name of dataset\"\n  table: \"Prefix to date partitioned table\"\n  maxerror: 1000 # Stop loading on first error, or increase to permit more load errors\n  replace: false\n  timezone: \"Timezone\"\n  processing_dir: /path/to/tmp/directory/for/processing\n\n```\nNew config options:\nschema:  BigQuery require a JSON schema which describes the structure of the table. \n**Note:  Currently we use a python script to generate schema from iglu schema files. We can write it in ruby so that it will be easy to follow.\ndatabase:  Dataset is similar to database which can have collection of tables.\ntable: Currently we are partitioning the data into date based tables. Since Google charges based on the amount of data it queried, by partitioning the data makes more cost optimised. But BigQuery have a limitation of number of table can be queried at a time, we are combining to form a monthly table at the end of the month. If the tables are partitioned by date then BigQuery have functions to query across multiple tables be date range.\nreplace:  Loading the new data will erase the old entries if set true, else append\ntimezone:  Timezone by which data needs to be split into date table. Collector timestamp will be in UTC, timezone will be used to split the data into local timezone date.\nprocessing_dir:  Temporary directory to process enriched events into date tables. \n**Note: this is temp documentation, will keep refining and updating.\n. We have integrated BigQuery with one of the earlier version of snowplow. Now we tried to upgrade to latest version and we are stuck in building the executable. It throws the following error, \nLoadError: no such file to load -- pg\n  require at org/jruby/RubyKernel.java:944\n  require at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:55\n    <top> at uri:classloader:/storage-loader/lib/snowplow-storage-loader/bigquery_loader.rb:1\n  require at org/jruby/RubyKernel.java:944\n  require at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:55\n   (root) at uri:classloader:/jruby/kernel/kernel.rb:1\n    <top> at uri:classloader:/jruby/kernel/kernel.rb:13\n  require at org/jruby/RubyKernel.java:944\n   (root) at uri:classloader:/storage-loader/lib/snowplow-storage-loader.rb:27\n    <top> at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1\n     load at org/jruby/RubyKernel.java:962\n    <top> at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:55\n  require at org/jruby/RubyKernel.java:944\n   (root) at uri:classloader:/storage-loader/bin/snowplow-storage-loader:20\n    <top> at uri:classloader:/META-INF/main.rb:1\nERROR: org.jruby.embed.EvalFailedException: (LoadError) no such file to load -- pg\nWould be great if you can show us some lights on whats going wrong.\nNote: Im using vagrant on Mac and ran build.sh. Tried changing JRuby version also.\n. Steps to reproduce:\n- Pull snowplow/snowplow master\n- vagrant up\n- vagrant ssh\n- cd 4-storage/storage-loader\n- ./build.sh\nIt will throw the following error\nYour Ruby version is 2.2.3, but your Gemfile specified 1.9.3\nNow change the JRuby version in build.sh\n```\nrvm install jruby-1.7.23\nbash -l -c 'rvm use jruby-1.7.23'\n```\nRun ./build.sh again,\nvagrant@snowplow:/vagrant/4-storage/storage-loader$ ./deploy/snowplow-storage-loader\n it will throw following error\nLoadError: no such file to load -- pg\n  require at org/jruby/RubyKernel.java:944\n  require at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:55\n    <top> at uri:classloader:/storage-loader/lib/snowplow-storage-loader/bigquery_loader.rb:1\n  require at org/jruby/RubyKernel.java:944\n  require at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:55\n   (root) at uri:classloader:/jruby/kernel/kernel.rb:1\n    <top> at uri:classloader:/jruby/kernel/kernel.rb:13\n  require at org/jruby/RubyKernel.java:944\n   (root) at uri:classloader:/storage-loader/lib/snowplow-storage-loader.rb:27\n    <top> at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1\n     load at org/jruby/RubyKernel.java:962\n    <top> at uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:55\n  require at org/jruby/RubyKernel.java:944\n   (root) at uri:classloader:/storage-loader/bin/snowplow-storage-loader:20\n    <top> at uri:classloader:/META-INF/main.rb:1\nERROR: org.jruby.embed.EvalFailedException: (LoadError) no such file to load -- pg\n. Thanks @alexanderdean \n. There was a problem in my bigquery_loader script. Now I'm able to build the executable. Your Ruby version is 2.2.3, but your Gemfile specified 1.9.3 issue still exists, but we can solve this by specifying jruby version in build.sh\nThanks @alexanderdean\n. @alexanderdean  I have added schema generator script at 4-storage/storage-loader/lib/snowplow-storage-loader/schema_generator.py, please suggest if script need to be kept in any specific path.\nUsage\ncd 4-storage/storage-loader/lib/snowplow-storage-loader/\npython schema_generator.py --iglu '/path/to/iglu/schemas/directory' > schema.json\nOptions:\n-i or --iglu :  Path to the parent directory of all iglu schemas, \n. @alexanderdean Currently we don't use custom or derived contexts so these scripts support only unstructured events. If you can tell me how to handle contexts we can think of including it in the PR.\n. Hi @alexanderdean,\nIts been long time since PR is pending, can you please give an update on this, anything we need to do to merge this PR?. Thanks for the update @alexanderdean.. ",
    "sbonami": "@alexanderdean: Signed the Corporate Contributor one.\n. ",
    "agentgt": "I will see if I can do a PR next week. This is roughly what our stuff does (we have different names for some stuff events -> event).\n``` sql\nALTER TABLE event RENAME TO event_v04;\n-- Copy schema\nCREATE TABLE event AS (SELECT * from event_v04 limit 0);\n-- FORCE programmatic dispatch to table (v9.2).\n-- A trigger could be used to route but this forces correct\n-- handling of struct and unstruct\nALTER TABLE event ADD CONSTRAINT event_no_insert CHECK (false) no inherit;\n-- CREATE sub tables\nCREATE table event_page_view (\n       CHECK ( event = 'page_view' )\n) INHERITS (event);\nCREATE table event_page_ping (\n       CHECK ( event = 'page_ping' )\n) INHERITS (event);\nCREATE table event_unstruct (\n       CHECK ( event = 'unstruct' )\n) INHERITS (event);\nCREATE table event_struct (\n       CHECK ( event = 'struct' )\n) INHERITS (event);\nINSERT INTO event_page_view (SELECT * from event_v04 where event='page_view');\nINSERT INTO event_page_ping (SELECT * from event_v04 where event='page_ping');\nINSERT INTO event_unstruct (SELECT * from event_v04 where event='unstruct');\nINSERT INTO event_struct (SELECT * from event_v04 where event='struct');\n```\nNow when you do something like:\nsql\nSELECT * from event where event.event = 'page_view'\nPostgresql will only look at the event_page_view table. \nWe may not use the check constraint on the event field but rather the event_name and event_vendor in the future now that the schema 4-tuple info is now in the events (vendor,name,format,version).\nIf you did that approach you would create a table per schema for unstructs and have them inherit. \nThe table name would be something like:\nscala\ntableName = event.eventVendor.replaceAll(\"\\\\.\", \"_\") + \"_\" event.eventName\nBut that might get you some unwieldy table names :)\n. All the basic columns still live logically in atomic.events (ie the structured ones). Physically though I think postgresql duplicates them but I don't know the internals well enough.\nThe event_unstruct could contain additional columns perhaps flattened from schemas. For example we do something like this for the link_click schema:\n``` sql\nALTER TABLE event_unstruct\nADD COLUMN link_click_element_id character varying(256);\nALTER TABLE event_unstruct\nADD COLUMN link_click_element_target character varying(128);\nALTER TABLE event_unstruct\nADD COLUMN link_click_target_url character varying(2048);\nALTER TABLE event_unstruct\nADD COLUMN link_click_element_content character varying(1024);\n```\nYou won't see these fields though unless you directly query event_unstruct (I think) but event_unstruct would contain all the atomic.events columns. You can think of this as partitioning by event type (in fact I think Postgresql calls it partitioning).\nNow the advantage to this approach besides the aforementioned performance is you guys can continuously make updates to the events table and all the inheriting tables will pick up the updates. Obviously there are some caveats that are in the postgresql documentation so it would be wise to check those out.\nYou can of course partition out later on and migrate. The only fairly tricky thing is that because we are adding columns you must insert into the tables directly (Many examples don't do this because they are partitioning by time for cube analysis and thus can rely on a trigger. The sub tables are only for performance). Thus the ETL storage process would have to know how to dispatch to the correct table (we do this on our own but I believe you guys have a mechanism for this with jsonpath and what not).\n. ",
    "KevinNZ83": "I signed it!\n. ",
    "nakulgan": "I signed it!\n. ",
    "StaymanHou": "Thanks for the prompt reply. I signed the CLA!\nBTW, looks like the snowplow-storage-loader file is just a jar file. So I guess I can just patch the code in the jar file?\n. ",
    "darrengruber": "Hi all, thanks for looking into this issue. I am experiencing the same problem. When I try to validate the fix (which should work) I run into what is probably an unrelated problem.\nOS: Debian GNU/Linux 8\nRuby: jruby 9.0.5.0 (2.2.3) 2016-01-26 7bee00d OpenJDK 64-Bit Server VM 25.111-b14 on 1.8.0_111-8u111-b14-2~bpo8+1-b14 +jit [linux-amd64]\nSnowplow version: r85-metamorphosis\nGems:  \n```\n LOCAL GEMS \naws-sdk (2.5.8)\naws-sdk-core (2.5.8)\naws-sdk-resources (2.5.8)\nbuilder (3.2.2)\nbundler (1.13.6)\nCFPropertyList (2.3.2)\ncontracts (0.11.0)\ncoveralls (0.8.15)\ndiff-lcs (1.2.5)\ndocile (1.1.5)\nexcon (0.52.0)\nfission (0.5.0)\nfog (1.25.0)\nfog-brightbox (0.11.0)\nfog-core (1.42.0)\nfog-json (1.0.2)\nfog-profitbricks (0.0.5)\nfog-radosgw (0.0.5)\nfog-sakuracloud (1.7.5)\nfog-softlayer (1.1.4)\nfog-terremark (0.1.0)\nfog-vmfusion (0.1.0)\nfog-voxel (0.1.0)\nfog-xml (0.1.2)\nformatador (0.2.5)\ninflecto (0.0.2)\nipaddress (0.8.3)\njar-dependencies (0.2.3)\njdbc-postgres (9.4.1206)\njmespath (1.3.1)\njruby-jars (9.1.4.0)\njruby-openssl (0.9.15 java)\njruby-rack (1.1.20)\njson (2.0.2 java, 1.8.0 java)\nmime-types (3.1)\nmime-types-data (3.2016.0521)\nminitest (5.4.1)\nmulti_json (1.12.1)\nnet-ssh (2.9.4)\nnokogiri (1.6.8 java)\nopennebula (5.0.2)\nplissken (0.1.0)\npower_assert (0.2.3)\npsych (2.0.15 java)\nracc (1.4.13 java)\nrake (11.2.2, 10.1.0)\nrbvmomi (1.9.2)\nrdoc (4.1.2)\nrspec (2.99.0)\nrspec-core (2.99.2)\nrspec-expectations (2.99.2)\nrspec-mocks (2.99.4)\nrubyzip (1.2.0)\nsimplecov (0.12.0)\nsimplecov-html (0.10.0)\nsluice (0.4.0)\nsnowplow-tracker (0.5.2)\nterm-ansicolor (1.3.2)\ntest-unit (3.0.3)\nthor (0.19.1)\ntins (1.12.0)\ntrollop (2.1.2)\nwarbler (2.0.3)\n```\nInput YML:  \naws:\n  access_key_id: <redacted>\n  secret_access_key: <redacted>\n  s3:\n    region: us-east-1\n<snip>\nHere's how I patched and rebundled (done inside a Docker container but it shouldn't effect the outcome):\ncurl -sSL https://github.com/snowplow/snowplow/archive/r85-metamorphosis.tar.gz \\\n    | tar xz --strip-components=1 -C . \\\n    && curl -sSL https://github.com/snowplow/snowplow/pull/2888.diff | patch -p1 \\\n    && cd 4-storage/storage-loader \\\n    && bundle install \\\n    && rake\nThen when I try to run it:  \ndeploy# ./snowplow-storage-loader --config /snowplow/config.yml\nDownloading Snowplow events...\nUnexpected error: undefined method `host=' for #<Fog::Storage::AWS::Real:0x23ade612>\nuri:classloader:/storage-loader/lib/snowplow-storage-loader/s3_tasks.rb:41:in `download_events'\nuri:classloader:/storage-loader/bin/snowplow-storage-loader:42:in `<main>'\norg/jruby/RubyKernel.java:973:in `load'\nuri:classloader:/META-INF/main.rb:1:in `<main>'\norg/jruby/RubyKernel.java:955:in `require'\nuri:classloader:/META-INF/main.rb:1:in `(root)'\nuri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1:in `<main>'\nThis doesn't look related to this patch, I just wanted to see if anybody else was able to validate the fix for postgres storage loading.\nThanks!\n. @alexanderdean - I was having a lot of problems patching the fatjar. All attempts to use jar uf snowplow-storage-loader file_to_patch simply replaced everything in the jar with file_to_patch. What is the best way to patch this jar?\n. ",
    "theon": "No problem!\n. FYI: I have moved the scala-uri project over to https://github.com/lemonlabsuk/scala-uri and the latest version is 0.5.0 now.. ",
    "matogertel": "Any update on this ?. If you're using the JavaScript tracker, I've added some code to our tracker initialization script to fix the \"invalid\" urls. Essentially, it url-escapes all # characters after the first one.\nThis is the general idea:\nvar url = window.location.href;\nvar matches = url.match(/#/g);\nif (matches && matches.length>1) {\n    var fixed = url.split('#').slice(0,1).join('') + '#' + url.split('#').slice(1).join('%23');\n    snowplow('setCustomUrl',fixed);\n}. ",
    "DrGomi": "Cheers, just wanted to tell you guys that christoph is not the only one waiting for this fix.\nWe are also having this issue in our ionic v1 based hybrid app.. ",
    "gregbonnette": "@alexanderdean Any luck with the RFC for this PR?\n. Good talk on this issue and some potential solutions...  https://spark-summit.org/east-2017/events/spark-and-object-stores-what-you-need-to-know/. ",
    "Rajadas": "My issue is similar but slightly different.\nSome links we track need placeholders like this ##ASPID##, others like this %%ASPID%%. The exact placeholder tag %%ASPID%% will then be replaced by some values, if they exist, so encoding the tag is not an option.\nThe first case has no problem at all, 302 Redirect, the second one gets stuck in the collector, 400 Bad Request, without the expected redirection.\n. Hey @alexanderdean if there's a way to flag that the content of u= parameter contains non-escaped characters, that could work.\n. Sounds perfect \ud83d\udc4d\n. ",
    "joho": "Hi there. Is there any plan/schedule on when this will get addressed? It's a blocker on some security work we'd like to do at work.. ",
    "mhadam": "Done!\nhttps://github.com/snowplow/snowplow/wiki/iOS-Tracker#5-tracking-single-users-spsubject. Thanks @iur-kaia for pointing this out, but we've started using a new site for documentation.\nHere's a link: http://docs.snowplowanalytics.com/open-source/snowplow/trackers/objective-c-tracker/1.0.0/. ",
    "redsquare": "Hi @jbeemster - from your hosted assets page https://github.com/snowplow/snowplow/wiki/Hosted-assets. Thanks for the revised link.\n. Ive got a hack by hosting a js file on the collector subdomain that allows me to hit the health endpoint and read the cookie however this would be a smarter solution and available to all. Tried to implement it but my scala fu is zero currently (c# guy). ",
    "userkci": "FYI, they have finally started publishing to Maven Central with version 1.4.0: ua-parser/uap-java#1\nThis would be extremely useful, as the UA parsing ruleset currently embedded in the enrichment jar is from 2013.. It looks to me that you can initialize the parser with an InputStream, so I don't think a PR is required for ua-parser.\nWe have a need for this change and I'll have some free time in the next week, so hopefully can provide a PR for this change shortly.. As if the title isn't a tipoff, this addresses issue #3793.\nsnowplow/iglu-central#791 is the corresponding schema update.. I've updated the configuration to track the changes to the schema, as referenced in the iglu-central PR.  With the error handling, I've reworked the parser initialization since both Parser() and Parser(InputStream) can throw.  The parser is now wrapped in a validation.  This obviously then had a knock-on effect on the user agent parsing.  I went with reporting the failure on parse, but it unfortunately means all events will fail on this.  Not really content with this, but not sure how to best to handle here.. I've made the updates, including the styling.  Because if I'm updating, why not anyway? ;). ",
    "joshuacox": "I was just wondering if I could replace Amazon Kinesis with Kafka.   Is everything working but undocumented?. sorry I must've pasted the wrong link\n@bogaert I think this section of the wiki needs to be changed which still references the recipes:\n282 [[/images/warning.png]] | Note we are working on new data models and recipes that are more specific than we currently have. They will make use of all our latest features and enhancements.\n283 ---|:---\n284 \n285 First, let's create the `atomic.events` table, where the actual Snowplow data will live. The SQL for creating the atomic schema and table can be found [here][postgres-table-def]. Either copy and paste that SQL into PSQL / Navicat, or      you can run that file into PSQL at the command line. To do this, navigate to your Snowplow repo, then:\n286 \n287   $ cd 4-storage/postgres-storage/sql\n288   $ psql -h <HOSTNAME> -U power_user -d snowplow -p <PORT> -f atomic-def.sql\n289 \n290 You'll need to substitute in your credentials for `<HOSTNAME>` and `<PORT>`. Make sure you use the `power_user` credentials to create new tables etc.\n291 \n292 Now that you've created your `atomic.events` table, you're in a position to the different views. This can also be done at the command line:\n293 \n294   $ psql -h <HOSTNAME> -U power_user -d snowplow -p <PORT> -f recipes/recipes-basic.sql\n295   $ psql -h <HOSTNAME> -U power_user -d snowplow -p <PORT> -f recipes/recipes-catalog.sql\n296   $ psql -h <HOSTNAME> -U power_user -d snowplow -p <PORT> -f recipes/recipes-customers.sql\n297 \n298 Finally, you need to grant access relevant access permissions to your different users to the different tables and views.\n--- snowplow.wiki/setup-guide/4-setup-alternative-data-stores/postgres/Setting-up-PostgreSQL.md . @bogaert of note I do not  believe the links you posted answer this question. \nThe links you posted show how to query an existing database, whereas that section of the wiki is about creating the different views, and those sql files are not there. . anyone stumbling in here from searching similar terms should go here. ",
    "sokser": "Done here: http://discourse.snowplowanalytics.com/t/request-signature-not-matching/804. I was trying to compile from the terminal command line in OSX. Is there no way to do this from outside of vagrant?. ",
    "asabourin": "Following up on that issue we're seeing lot of similar warnings caused by the iOS/Android Facebook apps custom user agents: \n\n00:58:02.290 [scala-stream-collector-akka.actor.default-dispatcher-9] WARN  s.can.server.HttpServerConnection - Illegal request header: Illegal 'User-Agent' header: Invalid input '[', expected ' ', '\\t', LWS, Comment or ProductVersionComment (line 1, pos 156):\nMozilla/5.0 (Linux; Android 6.0.1; SM-G930F Build/MMB29K; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/53.0.2785.124 Mobile Safari/537.36 [FB_IAB/FB4A;FBAV/103.0.0.20.72;]\n                                                                                                                                                           ^\n00:58:02.367 [scala-stream-collector-akka.actor.default-dispatcher-9] WARN  s.can.server.HttpServerConnection - Illegal request header: Illegal 'User-Agent' header: Invalid input '[', expected ' ', '\\t', LWS, Comment or ProductVersionComment (line 1, pos 113):\nMozilla/5.0 (iPhone; CPU iPhone OS 10_0_2 like Mac OS X) AppleWebKit/602.1.50 (KHTML, like Gecko) Mobile/14A456 [FBAN/FBIOS;FBAV/60.0.0.37.141;FBBV/34183777;FBRV/0;FBDV/iPhone8,1;FBMD/iPhone;FBSN/iPhone OS;FBSV/10.0.2;FBSS/2;FBCR/OPTUS;FBID/phone;FBLC/en_GB;FBOP/5]\n                                                                                                                ^\n00:58:02.842 [scala-stream-collector-akka.actor.default-dispatcher-5] WARN  s.can.server.HttpServerConnection - Illegal request header: Illegal 'User-Agent' header: Invalid input '[', expected ' ', '\\t', LWS, Comment or ProductVersionComment (line 1, pos 101):\nMozilla/5.0 (iPad; CPU OS 7_1 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Mobile/11D167 [FBAN/FBIOS;FBAV/62.0.0.43.141;FBBV/36454510;FBRV/0;FBDV/iPad2,5;FBMD/iPad;FBSN/iPhone OS;FBSV/7.1;FBSS/1;FBCR/;FBID/tablet;FBLC/en_GB;FBOP/5]. Cool yes it was an easy fix but I thought that might be confusing for people sticking with Kinesis so  good to have it documented here. Thanks for the quick reply!. \n",
    "mrwacky42": "This would be very nice, since so far, the rest of the Snowplow stack supports IAM role profiles.. ",
    "nitin02": "Hello, Is there any milestone or progress on this request?\nSigning is definitely a necessary component as it allows for the sink to scale to any number of machines based on input load (with aws).. ",
    "spy-tech": "@alexanderdean I was wrong. I will check how we exactly do things. Not a Snowplow issue. . ",
    "acgray": "Thanks! Now I guess I should fix the bug in the KCL that stopped the exception from propagating in the first place... \ud83d\ude44. I signed it!. Limiting the AWS deps to only the necessary components (i.e. Kinesis) solves this for me - will submit a PR shortly.... This passes but still uses my private fork of Scalazon with the upgraded SDK version. I've submitted a PR with those changes and I'll update this PR once they get released.... No worries, that suits me just fine!\nOn Thu, Aug 10, 2017 at 5:42 PM, Ben Fradet notifications@github.com\nwrote:\n\n@acgray https://github.com/acgray\nUnfortunately, we''re moving away from Scalazon since it doesn't seem to\nbe maintained anymore (cf #3300\nhttps://github.com/snowplow/snowplow/issues/3300 and #3341\nhttps://github.com/snowplow/snowplow/issues/3341). Moreover, the sdk\nversions will be updated as part of #3274\nhttps://github.com/snowplow/snowplow/pull/3274.\nThanks a lot for your contribution anyway!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/3082#issuecomment-321554471,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA1z1jM8p22UR0OSykfe25S1EyrYYuQkks5sWwi6gaJpZM4LwcZz\n.\n. It's not a huge issue but we mirror snowplow-hosted-assets internally and also have our own Iglu central fork, and it would be nice to be able to use those globally, just to keep things clean 'n' tidy.\n\n(Edit: it also seems inconsistent for Iglu central schemas to be read from snowplow-hosted-assets rather than from Iglu central's jsonpaths folder, and equally from a specific jsonpaths_assets folder rather than from the Iglu repo in the same way as the jsonschema itself, but I'm not sure to what extent this is supported by the Iglu client atm.  Happy to contribute if you agree with that direction!). Correct - however if we used the aws.s3.buckets.jsonpath_assets setting for our snowplow-hosted-assets mirror we would have to add our custom jsonpaths to the same bucket alongside the mirrored assets under 4-storage/redshift-storage/jsonpaths which isn't very clean (I'd rather our mirror was a pristine copy).. @BenFradet I replaced the 2nd gist to censor our bucket names, perhaps you caught the old version\nThe correct link is https://gist.github.com/acgray/6d2d5a4ffdd0c77396e38799781708c4. Ah great.  I'll keep an eye out for the release.. ",
    "colobas": "@alexanderdean @jbeemster  splitted them!. Shoud this be sink & source, or is there going to be another issue for the source?. got it. my bad, I'm fixing it. By createTopic here it means creating the Topic object, which represents the real Topic, so if the topic doesn't exist this will fail. It does not create a new topic on the Cloud.. ",
    "iaingray": "Is anyone working on this? I'd be interested to if not.. Okay, just so I'm clear, the script should generate a playbook to run the following queries on whatever event/unstruct/context tables exist within the db it's run against?\nhttps://github.com/snowplow/snowplow/tree/master/5-data-modeling/deduplication-queries. ",
    "NirSivan": "I have tried this configuration multiple times and that did not fix the issue, \nI have also tried with S3:// and with http: on my private buckets after verifying that the server have access to them and that the resources on them are available.\nLike I said my best assumption is that something did not load up properly on my first configuration, then it got stuck in this situation where no matter if the setup is correct or not, the result is the error.\nPutting the file in the enrichments folder was sort of a last resort,but it did the work\nHope this helps anyone. \n. ",
    "thiagogsr": "It is working for me after it:\nhttp://discourse.snowplowanalytics.com/t/ip-lookup-arrayindexoutofboundsexception/870/21?u=thiagogsr. ",
    "clarkeandrew": "We have signed the corporate agreement and I should be listed on it.\n. It would definitly be worth considering adding support for Spot Blocks (https://aws.amazon.com/blogs/aws/new-ec2-spot-blocks-for-defined-duration-workloads/)  and instance fleets in general in the next release. The cost savings are pretty substantial if you have a job that has consistent timing.. ",
    "geetanshjindal": "please help guys, TIA. ",
    "zcei": "Done \u2705 . Shall I change the PR base to release/r88-angkor-wat then?. ",
    "liamtarpey": "Hi @alexanderdean & @chuwy \nI've been deconstructing your snippet to try to understand it better.\nFrom what I understand the script is doing two main things:\n\nCreating a function on window.snowplow which stores queued events until the script is loaded (q)\nCreating a script tag which injects the snowplow script\n\nWithout having to rewrite much of your script, the below solution would work for us:\n```\nconst loadSnowplow = () => {\n  ;(function(p,l,o,w,i,n,g){n=l.createElement(o);g=l.getElementsByTagName(o)[0];n.async=1;\n  n.src=w;g.parentNode.insertBefore(n,g)}}\n  (window,document,\"script\",\"//d1fc8wv8zag5ca.cloudfront.net/2.7.2/sp.js\",\"snowplow\"));\n};\nconst attachSnowplow = () => {\n  if(!window['snowplow']) {\n    window.GlobalSnowplowNamespace = window.GlobalSnowplowNamespace || [];\n    window.GlobalSnowplowNamespace.push('snowplow');\n    window['snowplow'] = function() {\n      ( window['snowplow'].q = window['snowplow'].q || []).push(arguments)\n    };\n    window['snowplow'].q = window['snowplow'].q || [];\n  }\n}\nattachSnowplow();\nwindow.addEventListener('load', loadSnowplow);\n```\nCan you foresee any issues with us using this method? \n. Thanks @chuwy much appreciated :). ",
    "jatinder85": "Hi @alexanderdean ok sounds good. I can submit PR, is there any dev docs on submitting PRs?\nYeah nvm the buffer semantics are identical.. Ok cool. Here u go: #3177 . I have signed the agreement.\nOn Fri, Mar 31, 2017 at 3:54 PM, Snowplow CLA bot notifications@github.com\nwrote:\n\nThanks for your pull request. Is this your first contribution to a\nSnowplow open source project? Before we can look at your pull request,\nyou'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://github.com/snowplow/snowplow/wiki/CLA to learn\nmore and sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/snowplow/snowplow/pull/3177#issuecomment-290851569,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABzbFeJilBwKNaH4JXOXIpGbcHvp3P1fks5rrYQ_gaJpZM4MwOr_\n.\n. \n",
    "tyomo4ka": "@alexanderdean\nOk. Thanks!\nAsked there http://discourse.snowplowanalytics.com/t/emr-shred-step-failed-with-exitcode-1/1090. ",
    "hose314": "I'm running into the same issue and thinking where to implement analytics on native or js threads. Existing node/js snowplow trackers are not working out of the box in react-native due to the some incompatible deps. \nAfter some experiments with React Native we decided to run in production. Our existing infrastructure tightly depends on snowplow, so we cannot move to another analytics provider and don't want to. So the question is writing native bridge or rewrite js (with support to save events on disc and react-native-device-info package). I checked that core tracker itself is okay with react-native. Node.js tracker falls with request module due to the it's lies on node environment. The issue could be solved by checking if lib runs under node (but it's senseless because package is called snowplow nodejs tracker) then use some request alternative. But my suggestion is to migrate from request to axios. I can open PR if you think it's appropriate solution.. @chuwy from snowplow-javascript-tracker i have used only core component and everything is okay there. I've considered snowplow-nodejs-tracker as you mentioned as commonjs track and i see in docs that i can use my own emitter logic:\njs\nvar t = tracker([e], 'myTracker', 'myApp', false);. @chuwy thanks, got it. l've bit confused because snowplow-tracker-core npm page points to snowplow-javascript-tracker. \nEventually, what you think about moving to axios in snowplow-nodejs-tracker? . @sunshineo My suggestion is to plug native side tracker. The main advantaged is that the crash events also will be collected to the store . ",
    "sunshineo": "Hello, what's the situation now? NodeJS tracker good to go in React Native?. ",
    "stdfalse": "\n\n[x] Getting started with data modeling\n\n\n[x] http://snowplowanalytics.com/analytics/index.html\n\n[x] http://snowplowanalytics.com/analytics/event-dictionaries-and-data-models/data-modeling.html\n[x] https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/ may be this: https://github.com/snowplow/snowplow/tree/master/5-data-modeling/web-model/redshift\n[x] https://github.com/snowplow/snowplow/tree/master/5-data-modeling/sql-runner/redshift/playbooks/full.yml may be this: https://github.com/snowplow/snowplow/blob/master/5-data-modeling/web-model/sql-runner/playbook/web-model.yml.tmpl. - [x] Getting started analyzing Snowplow data\n[x] http://snowplowanalytics.com/analytics/index.html\n[x] https://github.com/snowplow/snowplow/tree/master/5-analytics\n[x] https://github.com/snowplow/snowplow/tree/master/5-analytics/redshift\n\n[x] https://github.com/snowplow/snowplow/tree/master/5-analytics/postgresql/recipes\n\n\n[x] Setting up Looker\n\n\n[x] https://github.com/snowplow/snowplow/tree/master/5-analytics/looker-analytics\n\n\n[x] Setting up ChartIO\n\n\n[x] http://snowplowanalytics.com/analytics/index.html\n\n\n[ ] Setting up Excel to analyze Snowplow data\n\n[ ] http://snowplowanalytics.com/analytics/index.html\n\n[ ] http://snowplowanalytics.com/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html\n\n\n[ ] Setting up Tableau to analyze your Snowplow data\n\n[ ] http://snowplowanalytics.com/analytics/tools-and-techniques/converting-snowplow-data-into-a-format-suitable-for-olap.html#what\n[ ] http://snowplowanalytics.com/analytics/catalog-analytics/measuring-and-comparing-product-page-performance.html#visualising\n[ ] http://snowplowanalytics.com/analytics/catalog-analytics/measuring-and-comparing-content-page-performance.html#single-user-journey\n\n[ ] http://snowplowanalytics.com/analytics/\n\n\n[ ] Setting up R to perform more sophisticated analysis on your Snowplow data\n\n[ ] http://snowplowanalytics.com/analytics/index.html\n\n[ ] http://snowplowanalytics.com/analytics/tools-and-techniques/get-started-analysing-snowplow-data-with-r.html\n\n\n[x] Setting up Qubole to analyze Snowplow data using Apache Hive\n\n[x] http://snowplowanalytics.com/analytics/index.html. Looks good!\n\n\n[staging] s3-dist-cp: Raw S3 -> HDFS\n\nIt's a part of enrich step and requires --resume-from enrich arg. \n\n[enrich] s3-dist-cp: Stream Enriched {enriched stream location} -> Enriched Staging S3\n\nIt's a staging_stream_enrich.\nWhat about the maintenance step: ...Empty Raw HDFS?  In https://github.com/snowplow/snowplow/pull/3933 we are going to introduce some more to clean up  HDFS. Should we include them?. > Also pinging @ihortom @stdfalse for their input on the description above.\nThe logic looks good to me.. We have noticed strange behavior on RestClient::RequestTimeout issues: in transient mode EMR cluster either not starting or loosing connection and executing all steps, in persistent mode it's executing steps until the disconnect and isn't reporting anything about completed steps. An example:\nstdout:\nD, [2019-01-17T03:35:37.280603 #29694] DEBUG -- : Initializing EMR jobflow\nstderr:\nuri:classloader:/gems/avro-1.8.1/lib/avro/schema.rb:350: warning: constant ::Fixnum is deprecated\nuri:classloader:/gems/json-schema-2.7.0/lib/json-schema/util/array_set.rb:18: warning: constant ::Fixnum is deprecated\nRestClient::RequestTimeout: Request Timeout\n                  transmit at uri:classloader:/gems/rest-client-1.8.0/lib/restclient/request.rb:427\n...\nlast emr status:\n| Name                                                                                                 | Status    | Start time                        | Elapsed time     | Elapsed (s) | \n| ---------------------------------------------------------------------------------------------------- | --------- | --------------------------------- | ---------------- | ----------- | \n| Elasticity Spark Step: Shred Enriched Events                                                         | COMPLETED | Thu Jan 17 2019 03:37:57 UTC+0000 |  0 h,  1 m, 56 s | 116         | \n| Elasticity S3DistCp Step: Enriched S3 -> HDFS                                                        | COMPLETED | Thu Jan 17 2019 03:37:27 UTC+0000 |  0 h,  0 m, 28 s | 28          | \n| Elasticity S3DistCp Step: Stream Enriched s3a://xxx-kinesis-s3-enriched/main/ -> Enriched Staging S3 | COMPLETED | Thu Jan 17 2019 03:36:55 UTC+0000 |  0 h,  0 m, 30 s | 30          | \n| Elasticity Custom Jar Step: Empty HDFS trash                                                         | COMPLETED | Thu Jan 17 2019 03:36:47 UTC+0000 |  0 h,  0 m,  6 s | 6           | \n| Elasticity Custom Jar Step: Empty Snowplow HDFS                                                      | COMPLETED | Thu Jan 17 2019 03:36:27 UTC+0000 |  0 h,  0 m, 18 s | 18          |\nAs you can see, it has just stopped after shred step and no indication about this in logs. Is it possible to fetch this info or make it to execute all steps so it won't fail in the middle of the way?. ",
    "fridiculous": "I signed it!. ",
    "mrosack": "Attaching example enriched events here since I couldn't on the forum and I'm probably going to delete that bucket: part-00001.gz\n. ",
    "cpoepke": "CLA is signed!. ",
    "arunma": "@BenFradet - Hopefully, it goes through the reviews successfully.  Do you think we should also add a couple of tickets for the enrich for EventHub source and sink?. I would like to give this is a try, unless someone's already working on it. @alexanderdean \n. Thanks @BenFradet. Will pick that up.  I sincerely hope that it would be similar to Kinesis. :-). I am sorry if this issue is already resolved. I saw no activity for a month and thought I would give it a shot. . @BenFradet Tried upgrading to Spark to 2.4.0, as suggested. Bumped into two issues and I would like to know your thoughts on this.\n\nCaused by: com.fasterxml.jackson.databind.JsonMappingException: Incompatible Jackson version: 2.8.4\n\nManaged to get through this issue by adding the following Overrides\nbuild.sbt\n.settings(\n    dependencyOverrides ++= Seq(\n      Dependencies.Overrides.jacksonCore,\n      Dependencies.Overrides.jacksonDatabind,\n      Dependencies.Overrides.jacksonModuleScala,\n      Dependencies.Overrides.json4s\n    )\n  )\nDependencies.scala\nobject Overrides {\n    val jacksonCore = \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.8.4\"\n    val jacksonDatabind = \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.8.4\"\n    val jacksonModuleScala = \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.8.4\"\n    val json4s = \"org.json4s\" %% \"json4s-jackson\" % \"3.2.11\"\n  }\nHowever, I am not convinced if this is the best way to approach this. Can't google a better option too.\n\nAfter the above overrides, a couple of testcases fail - CljTomcatPingdomEventSpec and CljTomcatPagerdutyEventSpec, I notice that with Spark 2.4, the results are empty string (\"\") instead of the expected nulls. I am not familiar with the code yet and I would appreciate your inputs on this.\n\nval expected = List(\n    \"uptime\",\n    \"srv\",\n    etlTimestamp,\n    \"2014-10-09 16:28:31.000\",\n    null,\n    \"unstruct\",\n    null, // We can't predict the event_id\n    null,\nOn debug window\nactual = {String[131]@12670} \n 0 = \"uptime\"\n 1 = \"srv\"\n 2 = \"2001-09-09 01:46:40.000\"\n 3 = \"2014-10-09 16:28:31.000\"\n 4 = \"\"\"\"\n 5 = \"unstruct\"\n 6 = \"30d288f6-b2bf-44db-9fe8-7872ae3d4713\"\n 7 = \"\"\"\"\n 8 = \"\"\"\". Can we safely assume that this issue is closed? I see that the \"child\" issues are closed.  . Seems like Travis failed with apt-get update.   Is there a way I could trigger the build again?\nUsing Scala 2.11.12\n3.34s$ sudo apt-get -qq update\nW: http://ppa.launchpad.net/couchdb/stable/ubuntu/dists/trusty/Release.gpg: Signature by key 15866BAFD9BCC4F3C1E0DFC7D69548E1C17EAB57 uses weak digest algorithm (SHA1)\nW: GPG error: https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 6B05F25D762E3157\nW: The repository 'https://packagecloud.io/github/git-lfs/ubuntu trusty InRelease' is not signed.\nW: There is no public key available for the following key IDs:\n6B05F25D762E3157  \nE: Failed to fetch http://security.ubuntu.com/ubuntu/dists/trusty-security/main/source/Sources.gz  Hash Sum mismatch\nE: Failed to fetch http://security.ubuntu.com/ubuntu/dists/trusty-security/main/binary-amd64/Packages.gz  Hash Sum mismatch\nE: Some index files failed to download. They have been ignored, or old ones used instead.\nThe command \"sudo apt-get -qq update\" failed and exited with 100 during .\nYour build has been stopped..  Absolutely! Will get it to you in a few hours. . @BenFradet Tried upgrading to Spark to 2.4.0, as suggested. Bumped into two issues and I would like to know your thoughts on this. \n\nCaused by: com.fasterxml.jackson.databind.JsonMappingException: Incompatible Jackson version: 2.8.4\n\nManaged to get through this issue by adding the following Overrides\nbuild.sbt\n.settings(\n    dependencyOverrides ++= Seq(\n      Dependencies.Overrides.jacksonCore,\n      Dependencies.Overrides.jacksonDatabind,\n      Dependencies.Overrides.jacksonModuleScala,\n      Dependencies.Overrides.json4s\n    )\n  )\nDependencies.scala\nobject Overrides {\n    val jacksonCore = \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.8.4\"\n    val jacksonDatabind = \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.8.4\"\n    val jacksonModuleScala = \"com.fasterxml.jackson.module\" %% \"jackson-module-scala\" % \"2.8.4\"\n    val json4s = \"org.json4s\" %% \"json4s-jackson\" % \"3.2.11\"\n  }\nHowever, I am not convinced if this is the best way to approach this. Can't google a better option too.\n\nAfter the above overrides, a couple of testcases fail - CljTomcatPingdomEventSpec and CljTomcatPagerdutyEventSpec, I notice that with Spark 2.4, the results are empty string (\"\") instead of the expected nulls. I am not familiar with the code yet and I would appreciate your inputs on this. \n\nval expected = List(\n    \"uptime\",\n    \"srv\",\n    etlTimestamp,\n    \"2014-10-09 16:28:31.000\",\n    null,\n    \"unstruct\",\n    null, // We can't predict the event_id\n    null,\nOn debug window\nactual = {String[131]@12670} \n 0 = \"uptime\"\n 1 = \"srv\"\n 2 = \"2001-09-09 01:46:40.000\"\n 3 = \"2014-10-09 16:28:31.000\"\n 4 = \"\"\"\"\n 5 = \"unstruct\"\n 6 = \"30d288f6-b2bf-44db-9fe8-7872ae3d4713\"\n 7 = \"\"\"\"\n 8 = \"\"\"\". Thanks for your quick turnaround,  @BenFradet. Will carry out the review comments over the weekend.. Please let me know if this is alright. Since there's no more scalify and buildSettings=basicSettings, I just removed buildSettings @BenFradet . Absolutely !. ",
    "ashwinGokhale": "I signed it!. @BenFradet Sure. A user might want a null partition key for keyless messaging, so if the partitionKey property is specified, but the value is an empty string, the key will be null.. ",
    "devesh-shetty": "Thanks @BenFradet! I shall ping you once it's done . ",
    "aldemirenes": "@BenFradet Sure Ben, sorry for missing about this.  . There are multiple tools on the NSQ project and NSQ server named as nsqd which means that NSQ Daemon. Also, there are other tools like nsqlookupd which finds correct nsqd instance from the topic name. Therefore, I used nsqd-, instead of nsq-. Actually there are two different tools for NSQ, one of them is nsqd and other is nsqlookupd. These two tools should run on the same machine therefore I used one host name which is nsq-host for both of them. Since they are two different tool, both of them have different port number.. Every event string must be converted to the byte array. After converting event, I send them with \"produce\". I did not see any reason to get together them again and sent with produceMulti. Is there any reason which you think using produceMulti is better solution ?. Since tool name is nsqlookupd, I think it is better in this way. What you think ?. There are channel fields in the NSQ additionally. I thought that keeping channel and topic name separated can be confusing therefore I put all of the topic and channel fields under NSQ section.. I think buffer related fields are put in the NSQConfig only. I can not find anywhere which they are used.. Sorry, since I did not see the direct usage of these field, I thought they are not used. I will make necessary changes then.  . > output_buffer_size (nsqd v0.2.21+) the size in bytes of the buffer nsqd will use when writing to this client.\nThis part is from here\nAs I understand, these buffer fields are used while nsqd is writing to the clients. I think there is no buffering option for producer. It is obvious from here too because NSQProducer class does not take any NSQConfig argument and creates default NSQConfig which buffer fields are null in the default NSQConfig.  I will add buffering to stream-enrich but I think this is not possible for stream-collector. What do you think ? . Right now, nothing. . Sure we can I just did not see it necessary because they can handle fairly good amount of request and snowplow mini is real time, buffering in this tools would break being real time. However, if you want, I can add the buffering.  . ",
    "nishanth6": "Does any one knows how should I have to run in GitBash command shell.. Hello @chuwy ,\nI have already posted it in\n\n\nUnexpected error while running the Storage Loader\n\n\nHow to run Storage Loader in PostgreSQL database\n\n\nBut no one replied for the LoadError: no such file to load -- jruby_pageant. \nI need a solution for running storage loader even without configuring .yml & .json files.\nI Thank for answering the below question\nHow to run StorageLoader in standalone system without connecting to external server eg: Amazon AWS \nKindly mention if there are any alternate solutions.\nThanking You,\nNishanth\n. ",
    "clayheaton": "I just read it and spent a few minutes trying to fix the issue, unsuccessfully. Sounds like the provisioning script needs to be updated to a more recent version of GPG? I'm not behind a firewall that would cause a problem with this. . ",
    "mpapis": "Please use GPG v2 for downloading the keys, RVM itself will work fine with both GPG v1 and v2.. ",
    "premjg": "Signed it!. ",
    "waspesi": "Sorry, new to this. Thanks!\nWayne Aspesi\nMarketing Analytics Consultant | Data Analytics\nT: (413) 744-3262\nMassMutual\n1295 State Street  | MIP F205  |  Springfield, MA 01111\nMassMutual.com | RetireSmart |  Facebook | Twitter | LinkedIn\nPlease use below link to submit requests:\nData Analytics Requestshttp://mminfo.private.massmutual.com/RequestCenter/myservices/navigate.do?query=serviceid&sid=1481&layout=&userid_for_service_search=12167&\nMassachusetts Mutual Life Insurance Company (MassMutual), Springfield, MA 01111-0001,\nand its affiliated US insurance companies.\nFrom: Ben Fradet notifications@github.com\nReply-To: snowplow/snowplow reply@reply.github.com\nDate: Thursday, September 14, 2017 at 11:40 AM\nTo: snowplow/snowplow snowplow@noreply.github.com\nCc: Wayne Aspesi waspesi@massmutual.com, Author author@noreply.github.com\nSubject: [EXTERNAL]Re: [snowplow/snowplow] SP.js error (#3423)\nHello, could you create the issue in the repository dedicated to the javscript tracker: https://github.com/snowplow/snowplow-javascript-trackerhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_snowplow_snowplow-2Djavascript-2Dtracker&d=DwMCaQ&c=BX7Y4KpGhcDnIsrgHKqkbfoiiDvjhxwuYUpcrPD7xrE&r=C4McflgI92zqG9lb7tPW3IzxJb9VtYbhJbc1YMJhjn4&m=bN3seaGNdEpCBU_mgnQHpaRecEg_P1UYUiKhIJyK8TE&s=rpRwNDPyl9fmI0ZXXnd3NbeMsObIGAF7CEkR7k7_KV8&e=? Thanks.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_snowplow_snowplow_issues_3423-23issuecomment-2D329522307&d=DwMCaQ&c=BX7Y4KpGhcDnIsrgHKqkbfoiiDvjhxwuYUpcrPD7xrE&r=C4McflgI92zqG9lb7tPW3IzxJb9VtYbhJbc1YMJhjn4&m=bN3seaGNdEpCBU_mgnQHpaRecEg_P1UYUiKhIJyK8TE&s=8HdS2XKWBP4OpB4VCvleViUY-v4kNymsCrzdjJDvmdg&e=, or mute the threadhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AcNJUDkSsJ1fyk4wo1DoLwc5yW-5FpByKEks5siUjcgaJpZM4PXxzb&d=DwMCaQ&c=BX7Y4KpGhcDnIsrgHKqkbfoiiDvjhxwuYUpcrPD7xrE&r=C4McflgI92zqG9lb7tPW3IzxJb9VtYbhJbc1YMJhjn4&m=bN3seaGNdEpCBU_mgnQHpaRecEg_P1UYUiKhIJyK8TE&s=RNxJ5hDTz0n8XA5_mQu6SgJkmBe5vYa9rqf4CVpvFhs&e=.\nThis e-mail transmission may contain information that is proprietary, privileged and/or confidential and is intended exclusively for the person(s) to whom it is addressed. Any use, copying, retention or disclosure by any person other than the intended recipient or the intended recipient's designees is strictly prohibited. If you are not the intended recipient or their designee, please notify the sender immediately by return e-mail and delete all copies\n. ",
    "frankcash": "is there any progress or acceptable way of doing this that is endorsed by the snowplow team right now \ud83d\udcaf . ",
    "ksnabb": "all done. ",
    "jankoulaga": "@BenFradet Maybe then the release docs should be updated, saying that this feature only works for GET requests.\nNow, about the ContentType... I was testing my hack on 0-8-0 with cookie bounce(that's the version we used to submit the Pull Request for the cookie bounce), and my Content-Type application/x-www-form-urlencoded would pass fine through the collector, but then i guess during enrichment it would end up in the bad stream due to:\n{\n  \"level\": \"error\",\n  \"message\": \"Content type of application/x-www-form-urlencoded provided, expected one of: application/json, application/json; charset=utf-8, application/json; charset=UTF-8\"\n}\nWhat i'm seeing right now in the enricher, is that it actually could work with the latest release. \nI'll try to whip up some pull requests for a POST cookie bounce(collector & JS tracker), if you guys are interested...\n. Alright, i'll try to push something in the following week.\nUnfortunately, my findings on POST redirects are purely empirical. What i did was a JS fiddle sending a xhr POST to a collector, just so i can prove that it was FF that's not behaving as it shuold.\nFirst thing i tried was exactly the way how Snowplow JS tracker does it(with Json content type):\nrq = new XMLHttpRequest();    \n//THIS IS EXACTLY HOW SNOWPLOW DOES IT\nrq.open(\"POST\", redirectingURI, true);\nrq.withCredentials = true;\nrq.setRequestHeader('Content-Type', 'application/json; charset=utf-8');\nrq.send(stringifiedJson);\nThe result of that was that in Chrome, the redirect was followed, data was submitted with the n3pc redirect; in FF, the redirect was not followed and the xhr threw an error; In Safari, the redirect was followed, however, the n3pc redirect contained no body.\nThen by accident i commented out the content type setting, and the redirect worked in FF. The same request(without the content type) again didn't work in Safari, as the body was not added to the redirect request.\nAnd then, after some reading about why 307 was introduced(one of the reason was posting form redirects), i tried using application/x-www-form-urlencoded and it worked.\n. @BenFradet makes perfect sense. I'll prepare another solution soon.. @BenFradet i've taken a look into the code. The DNT cookie comparison is done on HttpCookie objects, which are returned as such from the collectorConfig. Now there will be a need for some slight refactoring on that, so i wanted to see which approach would you like best in order to speed this thing up.\nTo me it seems like the doNotTrackCookie function should return an object that builds the regex Pattern once constructed and evaluates whether the cookie from the request should result as a DNT request. However, that's not the pattern used in the code, and i don't want to introduce something that doesn't fit the overall code.\nAny suggestions? . Makes sense completely! Will do! Thanks.. ",
    "telyukov": "I signed it!. ",
    "petervcook": "Use case: apply anonymization or pseudonymization (or other GDPR features) only to users who are based in the EU.\nI\u2019m thinking something along the lines of if the visitor is in one of the 28 member states then apply certain rules.. ",
    "SakuraSound": "Not sure if this is a good place to put this, but it would also be nice to do scrubbing/hashing of some of the elements of the Geo-IP enrichment using the PII enrichment (like dropping latitude/longitude, zip code, etc.). ",
    "dilyand": "@alexanderdean Was there anything you wanted me to do with this?. I used an extra schema registry for testing, which we might want to remove before release: https://github.com/snowplow/snowplow/blob/scala-enrich-sendgrid/3-enrich/spark-enrich/src/test/scala/com.snowplowanalytics.snowplow.enrich.spark/EnrichJobSpec.scala#L246-L256. ",
    "pawanpatil08": "Sure thanks ..\nI have not seen this  last notification . ",
    "wheller": "I suspected something had changed recently.  But as you can see from that bug report, leningen isn't exactly friendly to newcomers trying to debug.. Yeah I did read the FAQ too,  but was discouraged at \"If this happens it is strongly recommended to add an :exclusion and report a bug with the dependency which does this.\"  and leningen leaving it up to you to figure out which one that is.. Oh, and I'm planning on plugging this into a CI chain that will rebuild when someone makes a change to our repo, so charging ahead doing something under the category of \"never do this\" seemed... unwise. At least before asking for help. :-). I'd suggest putting a link to that from here...\nhttps://github.com/snowplow/snowplow/wiki/1-Installing-EmrEtlRunner#4-configuration\nSince it already has two different links to the sample config and a config doc page that don't mention that at all.  And as a new user learning SnowPlow that's the path I came to learning about the configuration.\n. ",
    "worace": "Hi there, I found this issue via google while searching for solutions to the same problem. I eventually figured out a a way to track down transitive non-TLS deps using Maven. I wrote up the steps I used here: http://worace.works/2018/05/29/identify-non-https-transitive-leiningen-dependencies/.\nWould be happy if this helps anyone as I have been dealing with this issue for a while now.. ",
    "thalesmello": "I signed it . ",
    "arthur78": "Sorry, please ignore... ",
    "keanerobinson": "Apologies if there already is one - I couldn't find it! \nI'm working on getting this live on iOS with Lifecake next week. I'll see if there's anything we can use that Filippo builds!. Closing in favour of Obj C Tracker issue 349.. Closing in favour of Obj C tracker issue 349.. I elaborated more on how I think this should work in the iOS issue.. thanks @alexanderdean will do!. ",
    "razius": "Yes, I agree.\nI did the following in the Trigger template to make it work (horrible, I know):\n\"status\": \"{% if ticket.status == 'Nuevo' or ticket.status == 'Neu' or ticket.status == '\u65b0\u5efa' or ticket.status == 'Nowe' or ticket.status == 'Nouveau' or ticket.status == 'Novo' or ticket.status == 'Nuovo' or ticket.status == 'Nou' or ticket.status == '\u041d\u043e\u0432\u044b\u0439' or ticket.status == '\u05d7\u05d3\u05e9\u05d4' or ticket.status == 'Ny' or ticket.status == '\u062c\u062f\u064a\u062f\u0629' or ticket.status == '\u65b0\u898f' or ticket.status == '\uc2e0\uaddc' or ticket.status == 'Nov\u00fd' or ticket.status == 'Uusi' or ticket.status == 'Yeni' or ticket.status == '\u041d\u043e\u0432' or ticket.status == 'Nieuw' or ticket.status == '\u00daj' or ticket.status == '\u041d\u043e\u0432\u0430' %}New{% elsif ticket.status == 'Abierto' or ticket.status == 'Offen' or ticket.status == '\u5df2\u958b\u555f' or ticket.status == '\u5df2\u5f00\u542f' or ticket.status == 'Otwarte' or ticket.status == 'Ouvert' or ticket.status == 'Aberto' or ticket.status == 'Aperto' or ticket.status == 'Deschis' or ticket.status == '\u041e\u0442\u043a\u0440\u044b\u0442' or ticket.status == '\u05e4\u05ea\u05d5\u05d7\u05d4' or ticket.status == '\u00c5pen' or ticket.status == '\u0645\u0641\u062a\u0648\u062d\u0629' or ticket.status == '\u30aa\u30fc\u30d7\u30f3' or ticket.status == '\ub4f1\ub85d' or ticket.status == 'Otev\u0159en\u00fd' or ticket.status == 'Avattu' or ticket.status == 'A\u00e7\u0131k' or ticket.status == '\u00d6ppen' or ticket.status == '\u041e\u0442\u0432\u043e\u0440\u0435\u043d' or ticket.status == '\u00c5ben' or ticket.status == 'Nyitott' or ticket.status == '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0430' %}Open{% elsif ticket.status == 'Pendiente' or 'Wartend' or '\u672a\u6c7a' or '\u5f85\u56de\u5e94' or 'Oczekuj\u0105ce' or 'En attente' or 'Pendente' or 'In attesa' or '\u00cen a\u0219teptare' or '\u0412 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0438' or '\u05d1\u05d4\u05de\u05ea\u05e0\u05d4' or 'Venter p\u00e5 tilbakemelding' or '\u0642\u064a\u062f \u0627\u0644\u0639\u0645\u0644' or '\u4fdd\u7559\u4e2d' or '\ubcf4\ub958' or '\u010cekaj\u00edc\u00ed' or 'Odottaa' or 'Beklemede' or 'V\u00e4ntar' or '\u0427\u0430\u043a\u0430\u0449' or 'Venter' or 'In afwachting' or 'F\u00fcgg\u0151ben' or '\u0412 \u043e\u0447\u0456\u043a\u0443\u0432\u0430\u043d\u043d\u0456' %}Pending{% elsif ticket.status == 'En espera' or ticket.status == 'Angehalten' or ticket.status == '\u66ab\u505c' or ticket.status == '\u6682\u505c' or ticket.status == 'Wstrzymane' or ticket.status == 'En pause' or ticket.status == 'Em espera' or ticket.status == 'In sospeso' or ticket.status == '\u00cen pauz\u0103' or ticket.status == '\u041d\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0438' or ticket.status == '\u05de\u05d5\u05d7\u05d6\u05e7\u05ea' or ticket.status == 'P\u00e5 vent' or ticket.status == '\u0645\u0639\u0644\u0642\u0629' or ticket.status == '\u5f85\u6a5f\u4e2d' or ticket.status == '\ub300\uae30' or ticket.status == 'Pozastaven\u00fd' or ticket.status == 'Pidossa' or ticket.status == 'Ask\u0131da' or ticket.status == 'Parkerat' or ticket.status == '\u0421\u043f\u0440\u044f\u043d' or ticket.status == 'I bero' or ticket.status == 'Geparkeerd' or ticket.status == 'Felf\u00fcggesztve ' %}On-hold{% elsif ticket.status == 'Resuelto' or ticket.status == 'Gel\u00f6st' or ticket.status == '\u5df2\u89e3\u6c7a' or ticket.status == '\u5df2\u89e3\u51b3' or ticket.status == 'Za\u0142atwione' or ticket.status == 'R\u00e9solu' or ticket.status == 'Resolvido' or ticket.status == 'Risolto' or ticket.status == 'Rezolvat' or ticket.status == '\u0412\u044b\u043f\u043e\u043b\u043d\u0435\u043d' or ticket.status == '\u05e4\u05ea\u05d5\u05e8\u05d4' or ticket.status == 'L\u00f8st' or ticket.status == '\u0645\u062d\u0644\u0648\u0644\u0629' or ticket.status == '\u89e3\u6c7a\u6e08\u307f' or ticket.status == '\ud574\uacb0' or ticket.status == 'Vy\u0159e\u0161en\u00fd' or ticket.status == 'Ratkaistu' or ticket.status == '\u00c7\u00f6z\u00fcld\u00fc' or ticket.status == 'L\u00f6st' or ticket.status == '\u0418\u0437\u043f\u044a\u043b\u043d\u0435\u043d' or ticket.status == 'Opgelost' or ticket.status == 'Megoldva' or ticket.status == '\u0412\u0438\u0440\u0456\u0448\u0435\u043d\u0430' %}Solved{% elsif ticket.status == 'Cerrado' or ticket.status == 'Geschlossen' or ticket.status == '\u5df2\u95dc\u9589' or ticket.status == '\u5df2\u5173\u95ed' or ticket.status == 'Zamkni\u0119te' or ticket.status == 'Clos' or ticket.status == 'Fechado' or ticket.status == 'Chiuso' or ticket.status == '\u00cenchis' or ticket.status == '\u0417\u0430\u043a\u0440\u044b\u0442' or ticket.status == '\u05e1\u05d2\u05d5\u05e8\u05d4' or ticket.status == 'Avsluttet' or ticket.status == '\u0645\u063a\u0644\u0642\u0629' or ticket.status == '\u7d42\u4e86' or ticket.status == '\uc885\ub8cc' or ticket.status == 'Uzav\u0159en\u00fd' or ticket.status == 'Suljettu' or ticket.status == 'Kapal\u0131' or ticket.status == 'St\u00e4ngt' or ticket.status == '\u0417\u0430\u0442\u0432\u043e\u0440\u0435\u043d' or ticket.status == 'Lukket' or ticket.status == 'Gesloten' or ticket.status == 'Bez\u00e1rva' or ticket.status == '\u0417\u0430\u043a\u0440\u0438\u0442\u0430' %}Closed{% else %}{{ ticket.status }}{% endif %}\",\nAlso, ticketType seems to share the same problem.\n. Yes, I agree.\nI did the following in the Trigger template to make it work (horrible, I know):\n\"status\": \"{% if ticket.status == 'Nuevo' or ticket.status == 'Neu' or ticket.status == '\u65b0\u5efa' or ticket.status == 'Nowe' or ticket.status == 'Nouveau' or ticket.status == 'Novo' or ticket.status == 'Nuovo' or ticket.status == 'Nou' or ticket.status == '\u041d\u043e\u0432\u044b\u0439' or ticket.status == '\u05d7\u05d3\u05e9\u05d4' or ticket.status == 'Ny' or ticket.status == '\u062c\u062f\u064a\u062f\u0629' or ticket.status == '\u65b0\u898f' or ticket.status == '\uc2e0\uaddc' or ticket.status == 'Nov\u00fd' or ticket.status == 'Uusi' or ticket.status == 'Yeni' or ticket.status == '\u041d\u043e\u0432' or ticket.status == 'Nieuw' or ticket.status == '\u00daj' or ticket.status == '\u041d\u043e\u0432\u0430' %}New{% elsif ticket.status == 'Abierto' or ticket.status == 'Offen' or ticket.status == '\u5df2\u958b\u555f' or ticket.status == '\u5df2\u5f00\u542f' or ticket.status == 'Otwarte' or ticket.status == 'Ouvert' or ticket.status == 'Aberto' or ticket.status == 'Aperto' or ticket.status == 'Deschis' or ticket.status == '\u041e\u0442\u043a\u0440\u044b\u0442' or ticket.status == '\u05e4\u05ea\u05d5\u05d7\u05d4' or ticket.status == '\u00c5pen' or ticket.status == '\u0645\u0641\u062a\u0648\u062d\u0629' or ticket.status == '\u30aa\u30fc\u30d7\u30f3' or ticket.status == '\ub4f1\ub85d' or ticket.status == 'Otev\u0159en\u00fd' or ticket.status == 'Avattu' or ticket.status == 'A\u00e7\u0131k' or ticket.status == '\u00d6ppen' or ticket.status == '\u041e\u0442\u0432\u043e\u0440\u0435\u043d' or ticket.status == '\u00c5ben' or ticket.status == 'Nyitott' or ticket.status == '\u0412\u0456\u0434\u043a\u0440\u0438\u0442\u0430' %}Open{% elsif ticket.status == 'Pendiente' or 'Wartend' or '\u672a\u6c7a' or '\u5f85\u56de\u5e94' or 'Oczekuj\u0105ce' or 'En attente' or 'Pendente' or 'In attesa' or '\u00cen a\u0219teptare' or '\u0412 \u043e\u0436\u0438\u0434\u0430\u043d\u0438\u0438' or '\u05d1\u05d4\u05de\u05ea\u05e0\u05d4' or 'Venter p\u00e5 tilbakemelding' or '\u0642\u064a\u062f \u0627\u0644\u0639\u0645\u0644' or '\u4fdd\u7559\u4e2d' or '\ubcf4\ub958' or '\u010cekaj\u00edc\u00ed' or 'Odottaa' or 'Beklemede' or 'V\u00e4ntar' or '\u0427\u0430\u043a\u0430\u0449' or 'Venter' or 'In afwachting' or 'F\u00fcgg\u0151ben' or '\u0412 \u043e\u0447\u0456\u043a\u0443\u0432\u0430\u043d\u043d\u0456' %}Pending{% elsif ticket.status == 'En espera' or ticket.status == 'Angehalten' or ticket.status == '\u66ab\u505c' or ticket.status == '\u6682\u505c' or ticket.status == 'Wstrzymane' or ticket.status == 'En pause' or ticket.status == 'Em espera' or ticket.status == 'In sospeso' or ticket.status == '\u00cen pauz\u0103' or ticket.status == '\u041d\u0430 \u0443\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u0438' or ticket.status == '\u05de\u05d5\u05d7\u05d6\u05e7\u05ea' or ticket.status == 'P\u00e5 vent' or ticket.status == '\u0645\u0639\u0644\u0642\u0629' or ticket.status == '\u5f85\u6a5f\u4e2d' or ticket.status == '\ub300\uae30' or ticket.status == 'Pozastaven\u00fd' or ticket.status == 'Pidossa' or ticket.status == 'Ask\u0131da' or ticket.status == 'Parkerat' or ticket.status == '\u0421\u043f\u0440\u044f\u043d' or ticket.status == 'I bero' or ticket.status == 'Geparkeerd' or ticket.status == 'Felf\u00fcggesztve ' %}On-hold{% elsif ticket.status == 'Resuelto' or ticket.status == 'Gel\u00f6st' or ticket.status == '\u5df2\u89e3\u6c7a' or ticket.status == '\u5df2\u89e3\u51b3' or ticket.status == 'Za\u0142atwione' or ticket.status == 'R\u00e9solu' or ticket.status == 'Resolvido' or ticket.status == 'Risolto' or ticket.status == 'Rezolvat' or ticket.status == '\u0412\u044b\u043f\u043e\u043b\u043d\u0435\u043d' or ticket.status == '\u05e4\u05ea\u05d5\u05e8\u05d4' or ticket.status == 'L\u00f8st' or ticket.status == '\u0645\u062d\u0644\u0648\u0644\u0629' or ticket.status == '\u89e3\u6c7a\u6e08\u307f' or ticket.status == '\ud574\uacb0' or ticket.status == 'Vy\u0159e\u0161en\u00fd' or ticket.status == 'Ratkaistu' or ticket.status == '\u00c7\u00f6z\u00fcld\u00fc' or ticket.status == 'L\u00f6st' or ticket.status == '\u0418\u0437\u043f\u044a\u043b\u043d\u0435\u043d' or ticket.status == 'Opgelost' or ticket.status == 'Megoldva' or ticket.status == '\u0412\u0438\u0440\u0456\u0448\u0435\u043d\u0430' %}Solved{% elsif ticket.status == 'Cerrado' or ticket.status == 'Geschlossen' or ticket.status == '\u5df2\u95dc\u9589' or ticket.status == '\u5df2\u5173\u95ed' or ticket.status == 'Zamkni\u0119te' or ticket.status == 'Clos' or ticket.status == 'Fechado' or ticket.status == 'Chiuso' or ticket.status == '\u00cenchis' or ticket.status == '\u0417\u0430\u043a\u0440\u044b\u0442' or ticket.status == '\u05e1\u05d2\u05d5\u05e8\u05d4' or ticket.status == 'Avsluttet' or ticket.status == '\u0645\u063a\u0644\u0642\u0629' or ticket.status == '\u7d42\u4e86' or ticket.status == '\uc885\ub8cc' or ticket.status == 'Uzav\u0159en\u00fd' or ticket.status == 'Suljettu' or ticket.status == 'Kapal\u0131' or ticket.status == 'St\u00e4ngt' or ticket.status == '\u0417\u0430\u0442\u0432\u043e\u0440\u0435\u043d' or ticket.status == 'Lukket' or ticket.status == 'Gesloten' or ticket.status == 'Bez\u00e1rva' or ticket.status == '\u0417\u0430\u043a\u0440\u0438\u0442\u0430' %}Closed{% else %}{{ ticket.status }}{% endif %}\",\nAlso, ticketType seems to share the same problem.\n. status for which you can get the translations from here and ticketType for which I couldn't find the translations so I've just set it to null. \nDo you want to do the language mapping or drop the validation?. ",
    "jspc": "Actually- this PR is crap; I'm blindly assuming that all non-404 requests are fine.\nLet me fix this.. @BenFradet - thanks for assigning yourself to this PR. I've sorted the thing I caught earlier.. I have signed the CLA @snowplowcla . I was in two minds about the relative import, but I'm happy to follow your lead.. ",
    "rahulrsingh09": "@BenFradet  thanks. @BenFradet  thanks. ",
    "tmacedo": "@alexanderdean that was the case initially but https://github.com/snowplow/scala-maxmind-iplookups/pull/35/commits/4ffd84b78469a367f7cf1c756b71d4aafddd49f6 actually changed the IpLocation class.. ",
    "misterpig": "Marketo documentation this way --->. Of course, happy to have the Marketo documentation to go in the Snowplow wiki. Cheers!. Not a problem, thanks for your help.. Hey @BenFradet , sorry for not getting back to you I was away and didn't have access to my laptop. I worked on the Spark enrich test spec awhile back and ran into some issues and unfortunately haven't had time to go back to it. . Hi @chuwy I'm not clear on what it is I'm to do here.. oops, the case matching should actually be:\nscala\ncase Some(\"user_updated\") => parsed\nreformatParameters is called once on all cases of formattedEvent to convert the field \"triggered_at\".. @chuwy made a similar comment I THINK in the Marketo adapter but I'm not sure what needs to be changed. this function was \"borrowed\" from the cleanupJsonEventValues method as is\nthe function is called only when it matches the case of\nscala\ncase (\"triggered_at\", JInt(x)\")\nso I'm not sure if any possible errors could arise if some integer values exist for that field to be converted\nlet me know what you guys think. Heyo @BenFradet , I found that if I don't \"return\" early here on failure then running the test specs spits out an error instead of a failure.\nE.g. passing in a bad json payload \"hello\" when tested shows:\n! toRawEvents must return a success for a valid for all event types payload body being passed\n[error]  JsonParseException: : Unrecognized token 'hello': was expecting ('true', 'false' or 'null')\n[error]  at [Source: java.io.StringReader@42b5aa30; line: 1, column: 11]  (JsonParser.java:1524)\nversus\nx toRawEvents must return a success for a valid for all event types payload body being passed\n[error]  'Failure(NonEmptyList(Twilio event failed to parse into JSON: [Unrecognized token 'hello': was expecting ('true', 'false' or 'null')\n[error]   at [Source: java.io.StringReader@38849b9f; line: 1, column: 11]]))' is not Success with value'NonEmptyList(.... I'm running into some trouble here. The val 'eventType' is a string which will fail when passed into lookupSchema as it requires a Option[] object. I could get around this by using\nlookupSchema(Some(eventType).... How should I return the JValue in 'formattedEvent' so it can be passed into 'toUnstructEventParams' without using .get?. ",
    "asamoilich": "Ok, will do. thx!. ",
    "freyfogle": "@tonicebrian is available to work on this, will follow up with Yali via email. ",
    "weipe007": "Hi @BenFradet @asgergb, as this issue is not fixed yet. Is there any work around?\nWe are using Spark 2.1.0 and snowplow maxmind scala lib 0.2.0 before.  But after we updated to 0.4.0, it always complain below error when I run either unit test or in Spark.\nDowngrade geoip2 to 2.5.0 or shading the jar will not help on this. Could you help to advise?\nThanks\nMartin\n18/05/03 03:55:27 WARN KafkaUtils: overriding receive.buffer.bytes to 65536 see KAFKA-3135\n18/05/03 03:55:32 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 114, 10.128.96.40, executor 0): java.lang.NoSuchMethodError: com.fasterxml.jackson.databind.node.ArrayNode.(Lcom/fasterxml/jackson/databind/node/JsonNodeFactory;Ljava/util/List;)V\n    at com.maxmind.db.Decoder.decodeArray(Decoder.java:272)\n    at com.maxmind.db.Decoder.decodeByType(Decoder.java:156)\n    at com.maxmind.db.Decoder.decode(Decoder.java:147)\n    at com.maxmind.db.Decoder.decodeMap(Decoder.java:281)\n    at com.maxmind.db.Decoder.decodeByType(Decoder.java:154)\n    at com.maxmind.db.Decoder.decode(Decoder.java:147)\n    at com.maxmind.db.Decoder.decode(Decoder.java:87)\n    at com.maxmind.db.Reader.(Reader.java:132)\n    at com.maxmind.db.Reader.(Reader.java:116)\n    at com.maxmind.geoip2.DatabaseReader.(DatabaseReader.java:66). Thanks @BenFradet ! Do you have any hard version requirement in snowplow scala-maxmind-lookups 0.4? \nI kept on seeing it need jackson 2.9.3 when run the unit test. \nMy build.sbt(0.13) is like this:\n==========================================\nscalaVersion := \"2.11.6\"\nresolvers += \"SnowPlow Repo\" at \"http://maven.snplow.com/releases/\"\nlibraryDependencies <++= libraryVersions { version => Seq (\n  \"com.typesafe\"             % \"config\"                         % version('typesafe),\n  \"org.scalatest\"            % \"scalatest_2.11\"                 % \"2.2.2\" % \"test\",\n  \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.6.7\",\n  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.6.7\",\n  \"com.fasterxml.jackson.module\" % \"jackson-module-scala_2.11\" % \"2.6.7\",\n  \"com.maxmind.geoip2\"       % \"geoip2\"    %                \"2.5.0\",\n  \"com.snowplowanalytics\"   %% \"scala-maxmind-iplookups\"        % \"0.4.0\",\n  \"commons-io\"               % \"commons-io\"                     % \"2.5\"\n)}. Thanks @BenFradet! After checking using sbt-dependency-graph, I was able to get it work. Just adding below to build.sbt is fine for me.\ndependencyOverrides +=   \"com.fasterxml.jackson.core\" % \"jackson-core\" % \"2.6.7\"\ndependencyOverrides +=  \"com.fasterxml.jackson.core\" % \"jackson-databind\" % \"2.6.7\"\ndependencyOverrides +=  \"com.fasterxml.jackson.module\" % \"jackson-module-scala_2.11\" % \"2.6.7\"\ndependencyOverrides +=  \"com.fasterxml.jackson.core\" % \"jackson-annotations\" % \"2.6.7\"\ndependencyOverrides +=  \"com.maxmind.geoip2\"       % \"geoip2\"\n. ",
    "bsweger": "Thanks--the new wording makes things clearer and is much appreciated!. ",
    "asgergb": "@BenFradet Good idea, I'll try the official artifacts.\n@alexanderdean Sure sounds like it if you can't reproduce the error. We're just doing a simple sbt \"project kinesis\" assembly, however, we are using a custom Docker image. There's really only Scala installed in that image except for some configuration files, though, and it has worked fine for the last ~1.5 years.\nEdit: You can actually see our build steps here. Dockerfile-stream-enrich looks for the assembled JAR file and adds it and the relevant configuration files to the image.\nDouble edit: Changed the URL to point to a specific commit where we build from source instead of using the official artifacts.. Using the official artifact works. For now I have simply changed our build setup to use those instead. It will make the build simpler, and although we will lose the flexibility gained from building directly from source, it will work fine for us for now.\nWe are still using our custom Docker image so the only change I made is switching from sbt \"project kinesis\" assembly and a copy of the jar to the Docker image to using wget artifact.zip and unzipping. Hence, the build process does not work as expected for us. I will close the issue as there's not much point investigating it unless we run into the problem again later.\nAnyway, thanks for your time!. I can still reproduce this from a completely fresh clone and build. Here's a log file. The referenced configuration is here.\nLet me know if you need any more info.. Sure, I'll do that as soon as possible. We have a holiday coming up but I'll try to get around to it.. FYI I have tried the official artifacts for R103 (collector 0.13.0 and enricher 0.16.0) as part of #3744, and the result is the same with roughly half of all events being duplicated.. @BenFradet Here you go: https://discourse.snowplowanalytics.com/t/stream-enrich-duplicated-enriched-events-in-r103-3745/1986. ",
    "romansalin": "@BenFradet Thank you for the fix, but I have the same problem with \"project kafka\". I guess we should add this dependency as a common one for all projects.. @BenFradet, thanks, I signed the CLA.. Yes, I have tested it and it worked fine without jacksonCbor as a common dependency. I'm going to revert it.. ",
    "rzats": "@BenFradet @chuwy just finished this round of QA, should be ready for another review. No, this is the config schema https://github.com/snowplow/iglu-central/pull/766/files\n(same for every other comment below). This one should actually be pointing to the response POJO, will fix. Will fix this. Is there a way to do integration tests that interact directly with EnrichmentManager and send in Snowplow events? (instead of method params). There's no flatten method for a scalaz validation as far as I can tell.. What's the way to do this without a for comp?. It should work with trailing slashes: https://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/test/scala/com.snowplowanalytics.snowplow.enrich.common/utils/conversionUtilsSpecs.scala#L73. These are the parameter names used by the IAB client: https://github.com/snowplow/iab-spiders-and-robots-java-client/blob/master/src/main/java/com/snowplowanalytics/iab/spidersandrobotsclient/IabClient.java#L77\nI could of course rename this but that would be a bit inconsistent.. The reason this is converted into a scala case class is because those types of Java enums seem to have some issues when they're serialized via Extraction.decompose(). I could convert the IabResponse into an IabEnrichmentResponse in getIab instead but I don't think that would make much of a difference.. Should I use a case class then?. Seems to work fine (the multiple slashes are simply added to the URI with no errors). I've added a normalize call here just in case - that should remove the extra slashes.. Good point, I'll move this step to be executed just after the IP lookups enrichment - that shouldn't be affected by anonymization. Thanks for spotting - those changes were apparently lost after rebasing, will fix. I'll also be doing some smoke testing today which should help prevent issues like these. ",
    "eperinan": "I am interested in tackling this issue. You can assign it to me if you wish @BenFradet and I will try to do it in the next weeks :muscle: \nIf I have any question or concern I will let you know.. ",
    "saj1th": "@eperinan - are you working on it? Would you need a helping hand ?. ",
    "apiddubnyi": "Actually I have no sensitive data except IP addresses.\nSo IP anonymization is enought for me.. ",
    "arihantsurana": "I signed it!\n. @BenFradet rearranged commits into two clean ones.. I think I have isolated all the pieces requiring override:\nhttps://github.com/snowplow/snowplow-s3-loader/blob/bdb6ca547467ad62774af5c1148291cfc57e6cbf/src/main/scala/com.snowplowanalytics.snowplow.storage.kinesis/s3/SinkApp.scala#L147-L150\nhttps://github.com/snowplow/snowplow-s3-loader/blob/bdb6ca547467ad62774af5c1148291cfc57e6cbf/src/main/scala/com.snowplowanalytics.snowplow.storage.kinesis/s3/SinkApp.scala#L140\nhttps://github.com/snowplow/snowplow-s3-loader/blob/bdb6ca547467ad62774af5c1148291cfc57e6cbf/src/main/scala/com.snowplowanalytics.snowplow.storage.kinesis/s3/SinkApp.scala#L94. Done for both.. Done for both.. ",
    "jwhansome": "Your model is accurate. It returns json. Example request/response:.\n```\nhttps://history.openweathermap.org/data/2.5/history/city?lat=1&lon=1&appid=[redacted]&type=hour&start=1528430820&cnt=24\n{\"message\":\"New city\",\"cod\":\"200\",\"city_id\":2302357,\"calctime\":0.028488455}\n```\nJust change the start parameter to a recent unix timestamp to reproduce. It may not require a full hour to provide a weather result, so to be sure, the more recent the better.. I should probably note, that for us, this was causing the majority of events to fail during enrichment. For those running this enrichment, especially in streaming environment, and further, with a low geoPrecision setting, it has a very large impact.\nAs for a solutions, I wonder if it would make sense to leave the cache key the same, but offset the OWM request start [and end] one hour prior. This should prevent the 'New city' response from ever happening...just a thought. Additionally, 'New city' errors should probably be retried similar to timeout errors.\nHope this is helpful information. . I agree, this is a reasonable short term fix. You're probably implying this, but to be clear, the 'New city' error should be retried if it's retrieved from the cache, or skip being cached. \nI wonder if you couldn't also evaluate another endpoint with OWM, current weather by coordinates perhaps (api.openweathermap.org/data/2.5/weather)? I'm unsure how/if the hourly historical data is getting used, but it seems that if the cached responses persist for the day (in the case of stream enrich), you're likely to only ever get the first hour anyways. I assume this would eliminate the 'New city' response, but have not verified.\nWithin our recovery process, we exclude certain enrichments due to potential transient errors that fail enrichment -- so this approach in general would eliminate much of our reprocessing. Consider this a definite up vote on \"optional\" enrichments.\nThank you for your attention to this issue.. ",
    "anistal": "Thanks for your fast answer @BenFradet \nI just have created this PR: https://github.com/snowplow/snowplow/pull/3832\nAlso I would like to inform that fields of the event (that it is a tsv) don't match with the database schema that it is defined here: https://github.com/snowplow/snowplow/blob/master/4-storage/redshift-storage/sql/atomic-def.sql\nI think that columns described in this sql script should have the same fields than this class: \nhttps://github.com/snowplow/snowplow/blob/master/3-enrich/scala-common-enrich/src/main/scala/com.snowplowanalytics.snowplow.enrich/common/outputs/EnrichedEvent.scala\nOtherwise the copy command fails. I also did modifications in this sql script and I could make another PR for it.. Reading the documentation deeply, I have realized that I misunderstood how Snowplow components were working and my PR has not sense.\nI have understood that (correct me if I am wrong, please):\n\nI have to publish events in Kinesis with the \"stream-enrich\" component without andy kind of break line.\n\nI have to use the \"snowplow-s3-loader\" to load data from my Kinesis topic to S3; furthermore this component as the documentation says:\n\nThe records are treated as byte arrays containing UTF-8 encoded strings (whether CSV, JSON or TSV). New lines are used to separate records written to a file.\n\n\n\nFinally, I should use  the \"snowplow-rdb-loader\" to load data from S3 to my Redshift cluster. \n\n",
    "dnedev": "Hey @chuwy, you are right. Adding /api at the end of the resolver URI for HTTP fixed the issue. Thanks!. ",
    "darrenhaken": "Sorry to jump in and ask a silly question, is this to allow the stream processor to work as a Lambda?\nTrying to assume so based on the PR name :smiley:. ",
    "aparra": "I am also interested in this feature for the same reason: send large payload to enrichment endpoint. \nSo I was taking a look at the code to create a PR supporting this feature. However there is one point that requires a deal: cache. Current implementation offers an in-memory cache mechanism where the key is the URL - it makes sense for GET requests where the parameters are in the URL. Also it is an interesting feature when you are using a third-party API.\nBut now with the plan to support POST calls - with post body - we need new deal for cache key. Maybe we could have a function that creates an UUID using URL and Request Body: (URL, Option[Request Body]) => UUID.\nI have one more point related to cache. I think cache should be optional. For example if you are controlling the API, maybe you would like to take care of cache mechanism. I think optional cache brings more flexibility to use this kind of enrichment.\nWhat do you think @miike @alexanderdean ?. I've created a PR to handle #3885 to support POST calls with body but it does not switch cache to optional.. @alexanderdean Yes, currently it is 1.\njson\n                \"cache\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"size\": {\n                            \"type\": \"integer\",\n                            \"minimum\": 1\n                        },\n                        \"ttl\": {\n                            \"type\": \"integer\",\n                            \"minimum\": 0,\n                            \"maximum\": 86400\n                        }\n                    },\n                    \"additionalProperties\": false,\n                    \"required\": [\"size\", \"ttl\"]\n                }\nI think in this case, we need only create a new version of JSON Schema when \"size\": { \"type\": \"integer\", \"minimum\": 0 }.\nCurrent implementation of cache supports size = 0:\n```scala\nval cache = Cache(size = 0, ttl = 0)\nval key   = ApiRequestEnrichment.cacheKey(url = \"http://api.acme.com/url\", body = None)\ncache.put(key, JInt(42).success)\ncache.get(key) must beNone and (cache.actualLoad must beEqualTo(0))\n```. Great @BenFradet! iglu-central's PR: https://github.com/snowplow/iglu-central/pull/835. @BenFradet I've applied pattern matching based on http method defined in json schema.\nscala\nmethod match {\n  case \"POST\" | \"PUT\" => Some(Serialization.write(context)(DefaultFormats))\n  case \"GET\"          => None\n}\nWhat do you think?. Hey @BenFradet, do you have any updated for this PR? . Hey @BenFradet. Holidays are always welcome! Well I didn't parameterised  content-type because it is always going to have a json body.. ",
    "palash25": "Hi @BenFradet I would like to work on this issue. . Oh I didn't see any commit referenced on the issue thread so I thought it was open for contributions. No worries.. ",
    "jrluis": "Interesting link on this issue https://denibertovic.com/posts/handling-permissions-with-docker-volumes/. ",
    "melgenek": "@BenFradet Changed the source of scala to build info generate file.\nThe java version is not known until jar file is ran but now it is taken from a predefined env property java.version, not by reflection. https://docs.oracle.com/javase/8/docs/api/java/lang/System.html#getProperties--. There is no public toSeconds and nanos give the best precision after division. Actually constant values are better to be declared using final modifier. \nhttps://www.scala-lang.org/files/archive/spec/2.12/05-classes-and-objects.html\nNote, however, that constant value definitions do require an explicit final modifier, even if they are defined in a final class or object.\nThe final variables are set during compile time, while non-final members of object are just method calls.\nConsider such example\n```\nobject Sample {\n  def main(args: Array[String]): Unit = {\n    println(Constants.str1)\n    println(Constants.str2)\n  }\n}\nobject Constants {\n  val str1 = \"String1\"\n  final val str2 = \"String2\"\n}\n``\nThen compilescalac -print Sample.scala`\nAnd you can see the difference\n[[syntax trees at end of                   cleanup]] // Sample.scala\npackage <empty> {\n  object Sample extends Object {\n    def main(args: Array[String]): Unit = {\n      scala.this.Predef.println(Constants.str1());\n      scala.this.Predef.println(\"String2\")\n    };\n    def <init>(): Sample.type = {\n      Sample.super.<init>();\n      ()\n    }\n  };\n  object Constants extends Object {\n    private[this] val str1: String = _;\n    <stable> <accessor> def str1(): String = Constants.this.str1;\n    final <stable> <accessor> def str2(): String(\"String2\") = \"String2\";\n    def <init>(): Constants.type = {\n      Constants.super.<init>();\n      Constants.this.str1 = \"String1\";\n      ()\n    }\n  }\n}. There is no information metrics. https://prometheus.io/docs/concepts/metric_types/\nThis one is chosen just to set static value. Thank you for the hint. I have left only the version, as it is only one /metrics per service and the name is not necessary. This is commented on purpose. \nBy default built-in buckets are used. If they does not meet the needs of application then they can be configured via hocon.\nThe parameter durationBuckets is specified as optional and is allowed to be omitted. added some info. ",
    "0xE282B0": "I signed it!. Thank you for your feedback! I will have a look at stream-enrich.. I've applied the changes from https://github.com/snowplow/snowplow/pull/3941 and squashed the commits.. Unfortunately, Travis could not build. Since I could not reproduce it, I tried to trigger a rebuild and accidentally deleted the branch. \ud83d\ude06. When I remove all quotes like this: compression.type = snappy it results in: \"compression\" -> \"{\"type\":\"snappy\"}\". Since the producer config key contains a dot it needs to be quoted.\n. Yes, of course.. Sure.. Yes, too eager while copying. My bad.. Good point. However, since some values are set from other values in the configuration file, I would suggest only specifying the remaining (fixed) values and referring to the other locations.\nLike this: https://gist.github.com/0xE282B0/d91b8fac239c360cfd4c8e58901f87b0#file-config-hocon-sample-L94\nDid I get you right?. I've added all values that are hardcoded in the createProperties(...) methods of the source and sink.\nFor the other values which are collected from other parts of the config I added a comment. Maybe it's a bit to crowdy.. ",
    "cpwc": "@BenFradet I'm running on Kinesis. However, I'm still hitting that exception. Added the specific project to the issue description.. ",
    "mirkoprescha": "I created a new PR (https://github.com/snowplow/snowplow/pull/3981)\nI will also validate the update on stream enricht. Hi @BenFradet ,\nUnfortunately updating enricher for kafka requires more effort, as the Integration Test is relying on an outdated library. I am not able to do it until Wednesday (and currently I don't have pressure from my side to get it updated).\nLuckely, there is no dependency to the collector.. ",
    "tonicebrian": "Hi, I just wanted to touch base and ask if the PR is fit.. @benjben Hopefully now everything is fixed.. Hi @benjben I've just checked and that test doesn't work locally for me neither. \nSince I haven't touched that folder I was worried that I was bringing that version from the OpenCage scala driver but it isn't a dependency of our driver.\nI've used plugin https://github.com/jrudolph/sbt-dependency-graph and it reports that Jackson 2.8 is being imported by other snowplow libraries like com.snowplowanalytics:snowplow-thrift-raw-event:0.1.0, com.snowplowanalytics:iglu-scala-client_2.11:0.5.0 and others. \nHave a look at output of sbt dependencyTree. \u2714 cebrian@T480:~/.../OpenCage/snowplow/3-enrich/spark-enrich [master] $ sbt dependencyTree\n[info] Loading settings from idea.sbt,dependency-graph.sbt ...\n[info] Loading global plugins from /home/cebrian/.sbt/1.0/plugins\n[info] Loading settings from plugins.sbt ...\n[info] Loading project definition from /home/cebrian/Freelancer/OpenCage/snowplow/3-enrich/spark-enrich/project\n[info] Loading settings from build.sbt ...\n[info] Set current project to snowplow-spark-enrich (in build file:/home/cebrian/Freelancer/OpenCage/snowplow/3-enrich/spark-enrich/)\n[success] Total time: 1 s, completed Mar 14, 2019 4:06:25 PM\n[info] com.snowplowanalytics:snowplow-spark-enrich_2.11:1.16.0 [S]\n[info]   +-com.github.scopt:scopt_2.11:3.5.0 [S]\n[info]   +-com.hadoop.gplcompression:hadoop-lzo:0.4.19 (evicted by: 0.4.20)\n[info]   +-com.hadoop.gplcompression:hadoop-lzo:0.4.20\n[info]   | +-commons-logging:commons-logging:1.1.1 (evicted by: 1.2)\n[info]   | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | +-commons-logging:commons-logging:1.2\n[info]   |\n[info]   +-com.maxmind.geoip2:geoip2:2.5.0\n[info]   | +-com.google.http-client:google-http-client:1.21.0\n[info]   | | +-com.google.code.findbugs:jsr305:1.3.9 (evicted by: 2.0.1)\n[info]   | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | +-org.apache.httpcomponents:httpclient:4.0.1 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.3 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.1 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.0.1 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | |   +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | |   +-commons-codec:commons-codec:1.11\n[info]   | |   +-commons-logging:commons-logging:1.2\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |\n[info]   | +-com.maxmind.db:maxmind-db:1.1.0\n[info]   |   +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   |   | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   |   | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   |   | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   |   | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   |   |\n[info]   |   +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   |     +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   |     +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   |     +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   |     +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   |\n[info]   +-com.snowplowanalytics:collector-payload-1:0.0.0 [S]\n[info]   | +-org.apache.thrift:libthrift:0.9.1\n[info]   |   +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   |   +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   |   +-org.apache.commons:commons-lang3:3.7\n[info]   |   +-org.apache.httpcomponents:httpclient:4.2.5 (evicted by: 4.5.5)\n[info]   |   +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   |   | +-commons-codec:commons-codec:1.11\n[info]   |   | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   |   | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   |   | +-commons-logging:commons-logging:1.2\n[info]   |   | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   |   |\n[info]   |   +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   |   | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   |   | +-commons-codec:commons-codec:1.11\n[info]   |   | +-commons-logging:commons-logging:1.2\n[info]   |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   |   |\n[info]   |   +-org.apache.httpcomponents:httpcore:4.2.4 (evicted by: 4.4.9)\n[info]   |   +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   |   +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   |   +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   |   +-org.slf4j:slf4j-api:1.7.25\n[info]   |\n[info]   +-com.snowplowanalytics:iglu-scala-client_2.11:0.5.0 [S]\n[info]   | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.8.11.2)\n[info]   | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | |\n[info]   | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | |\n[info]   | +-com.github.fge:json-schema-validator:2.2.3\n[info]   | | +-com.github.fge:json-schema-core:1.2.1\n[info]   | | | +-com.github.fge:jackson-coreutils:1.6\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.8.11.2)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | |\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | |\n[info]   | | | | +-com.github.fge:msg-simple:1.1\n[info]   | | | | | +-com.github.fge:btf:1.2\n[info]   | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | |\n[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | |\n[info]   | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | | | | +-com.google.guava:guava:20.0\n[info]   | | | |\n[info]   | | | +-com.github.fge:uri-template:0.9\n[info]   | | | | +-com.github.fge:msg-simple:1.1\n[info]   | | | | | +-com.github.fge:btf:1.2\n[info]   | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | |\n[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | |\n[info]   | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | | | | +-com.google.guava:guava:20.0\n[info]   | | | |\n[info]   | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | +-org.mozilla:rhino:1.7R4\n[info]   | | |\n[info]   | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | +-com.googlecode.libphonenumber:libphonenumber:6.0\n[info]   | | +-javax.mail:mailapi:1.4.3\n[info]   | | | +-javax.activation:activation:1.1\n[info]   | | |\n[info]   | | +-joda-time:joda-time:2.3 (evicted by: 2.9.9)\n[info]   | | +-joda-time:joda-time:2.9.9\n[info]   | | +-net.sf.jopt-simple:jopt-simple:4.6 (evicted by: 5.0.3)\n[info]   | | +-net.sf.jopt-simple:jopt-simple:5.0.3\n[info]   | |\n[info]   | +-com.twitter:util-collection_2.11:18.2.0 [S]\n[info]   | | +-com.twitter:util-core_2.11:18.2.0 [S]\n[info]   | |   +-com.twitter:util-function_2.11:18.2.0 [S]\n[info]   | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | |\n[info]   | +-com.twitter:util-collection_2.11:6.20.0 (evicted by: 18.2.0)\n[info]   | +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | +-org.apache.commons:commons-lang3:3.7\n[info]   | +-org.json4s:json4s-jackson_2.11:3.2.11 [S]\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.3.1 (evicted by: 2.8.11.2)\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | |\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | |\n[info]   | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | |   +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | |   +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | |   +-org.scala-lang:scalap:2.11.11\n[info]   | |     +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | |       +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |       +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | |       +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | |\n[info]   | +-org.json4s:json4s-scalaz_2.11:3.2.11 [S]\n[info]   | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | | | +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | | | +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | | | +-org.scala-lang:scalap:2.11.11\n[info]   | | |   +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | | |     +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |     +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-org.scalaz:scalaz-core_2.11:7.0.6 (evicted by: 7.0.9)\n[info]   | | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | |\n[info]   | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   |\n[info]   +-com.snowplowanalytics:snowplow-common-enrich_2.11:0.35.0 [S]\n[info]   | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.8.11.2)\n[info]   | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | |\n[info]   | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | |\n[info]   | +-com.github.fge:json-schema-validator:2.2.3\n[info]   | | +-com.github.fge:json-schema-core:1.2.1\n[info]   | | | +-com.github.fge:jackson-coreutils:1.6\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.8.11.2)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | |\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | |\n[info]   | | | | +-com.github.fge:msg-simple:1.1\n[info]   | | | | | +-com.github.fge:btf:1.2\n[info]   | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | |\n[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | |\n[info]   | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | | | | +-com.google.guava:guava:20.0\n[info]   | | | |\n[info]   | | | +-com.github.fge:uri-template:0.9\n[info]   | | | | +-com.github.fge:msg-simple:1.1\n[info]   | | | | | +-com.github.fge:btf:1.2\n[info]   | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | |\n[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | |\n[info]   | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | | | | +-com.google.guava:guava:20.0\n[info]   | | | |\n[info]   | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | +-org.mozilla:rhino:1.7R4\n[info]   | | |\n[info]   | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | +-com.googlecode.libphonenumber:libphonenumber:6.0\n[info]   | | +-javax.mail:mailapi:1.4.3\n[info]   | | | +-javax.activation:activation:1.1\n[info]   | | |\n[info]   | | +-joda-time:joda-time:2.3 (evicted by: 2.9.9)\n[info]   | | +-joda-time:joda-time:2.9.9\n[info]   | | +-net.sf.jopt-simple:jopt-simple:4.6 (evicted by: 5.0.3)\n[info]   | | +-net.sf.jopt-simple:jopt-simple:5.0.3\n[info]   | |\n[info]   | +-com.github.ua-parser:uap-java:1.4.0\n[info]   | | +-org.apache.commons:commons-collections4:4.1\n[info]   | | +-org.yaml:snakeyaml:1.20\n[info]   | |\n[info]   | +-com.jayway.jsonpath:json-path:2.4.0\n[info]   | | +-net.minidev:json-smart:2.3\n[info]   | | | +-net.minidev:accessors-smart:1.2\n[info]   | | |   +-org.ow2.asm:asm:5.0.4\n[info]   | | |\n[info]   | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-com.opencagedata:scala-opencage-geocoder_2.11:1.1.0 [S]\n[info]   | | +-com.github.tomakehurst:wiremock:2.20.0\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | |\n[info]   | | | +-com.flipkart.zjsonpatch:zjsonpatch:0.4.4\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.10 (evicted by: 2.8.11.2)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | |   +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | |   +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | |   +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | |   +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | |\n[info]   | | | +-com.github.jknack:handlebars-helpers:4.0.7\n[info]   | | | | +-com.github.jknack:handlebars:4.0.7\n[info]   | | | |   +-org.antlr:antlr4-runtime:4.7.1\n[info]   | | | |   +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | | | |   +-org.apache.commons:commons-lang3:3.7\n[info]   | | | |   +-org.slf4j:slf4j-api:1.6.4 (evicted by: 1.7.25)\n[info]   | | | |   +-org.slf4j:slf4j-api:1.7.12 (evicted by: 1.7.25)\n[info]   | | | |   +-org.slf4j:slf4j-api:1.7.25\n[info]   | | | |\n[info]   | | | +-com.github.jknack:handlebars:4.0.7\n[info]   | | | | +-org.antlr:antlr4-runtime:4.7.1\n[info]   | | | | +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | | | | +-org.apache.commons:commons-lang3:3.7\n[info]   | | | | +-org.slf4j:slf4j-api:1.6.4 (evicted by: 1.7.25)\n[info]   | | | | +-org.slf4j:slf4j-api:1.7.12 (evicted by: 1.7.25)\n[info]   | | | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | | | |\n[info]   | | | +-com.google.guava:guava:20.0\n[info]   | | | +-com.jayway.jsonpath:json-path:2.4.0\n[info]   | | | | +-net.minidev:json-smart:2.3\n[info]   | | | | | +-net.minidev:accessors-smart:1.2\n[info]   | | | | |   +-org.ow2.asm:asm:5.0.4\n[info]   | | | | |\n[info]   | | | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | | | |\n[info]   | | | +-junit:junit:3.8.1 (evicted by: 4.12)\n[info]   | | | +-junit:junit:4.12\n[info]   | | | | +-org.hamcrest:hamcrest-core:1.3\n[info]   | | | |\n[info]   | | | +-net.sf.jopt-simple:jopt-simple:5.0.3\n[info]   | | | +-org.apache.commons:commons-lang3:3.7\n[info]   | | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | | +-commons-codec:commons-codec:1.11\n[info]   | | | | +-commons-logging:commons-logging:1.2\n[info]   | | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | | |\n[info]   | | | +-org.eclipse.jetty:jetty-server:9.2.24.v20180105\n[info]   | | | | +-javax.servlet:javax.servlet-api:3.1.0\n[info]   | | | | +-org.eclipse.jetty:jetty-http:9.2.24.v20180105\n[info]   | | | | | +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | | |\n[info]   | | | | +-org.eclipse.jetty:jetty-io:9.2.24.v20180105\n[info]   | | | |   +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | |\n[info]   | | | +-org.eclipse.jetty:jetty-servlet:9.2.24.v20180105\n[info]   | | | | +-org.eclipse.jetty:jetty-security:9.2.24.v20180105\n[info]   | | | |   +-org.eclipse.jetty:jetty-server:9.2.24.v20180105\n[info]   | | | |     +-javax.servlet:javax.servlet-api:3.1.0\n[info]   | | | |     +-org.eclipse.jetty:jetty-http:9.2.24.v20180105\n[info]   | | | |     | +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | |     |\n[info]   | | | |     +-org.eclipse.jetty:jetty-io:9.2.24.v20180105\n[info]   | | | |       +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | |\n[info]   | | | +-org.eclipse.jetty:jetty-servlets:9.2.24.v20180105\n[info]   | | | | +-org.eclipse.jetty:jetty-continuation:9.2.24.v20180105\n[info]   | | | | +-org.eclipse.jetty:jetty-http:9.2.24.v20180105\n[info]   | | | | | +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | | |\n[info]   | | | | +-org.eclipse.jetty:jetty-io:9.2.24.v20180105\n[info]   | | | | | +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | | |\n[info]   | | | | +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | |\n[info]   | | | +-org.eclipse.jetty:jetty-webapp:9.2.24.v20180105\n[info]   | | | | +-org.eclipse.jetty:jetty-servlet:9.2.24.v20180105\n[info]   | | | | | +-org.eclipse.jetty:jetty-security:9.2.24.v20180105\n[info]   | | | | |   +-org.eclipse.jetty:jetty-server:9.2.24.v20180105\n[info]   | | | | |     +-javax.servlet:javax.servlet-api:3.1.0\n[info]   | | | | |     +-org.eclipse.jetty:jetty-http:9.2.24.v20180105\n[info]   | | | | |     | +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | | |     |\n[info]   | | | | |     +-org.eclipse.jetty:jetty-io:9.2.24.v20180105\n[info]   | | | | |       +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | | |\n[info]   | | | | +-org.eclipse.jetty:jetty-xml:9.2.24.v20180105\n[info]   | | | |   +-org.eclipse.jetty:jetty-util:9.2.24.v20180105\n[info]   | | | |\n[info]   | | | +-org.slf4j:slf4j-api:1.7.12 (evicted by: 1.7.25)\n[info]   | | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | | | +-org.xmlunit:xmlunit-core:2.5.1\n[info]   | | | +-org.xmlunit:xmlunit-legacy:2.5.1\n[info]   | | |   +-junit:junit:3.8.1 (evicted by: 4.12)\n[info]   | | |   +-junit:junit:4.12\n[info]   | | |   | +-org.hamcrest:hamcrest-core:1.3\n[info]   | | |   |\n[info]   | | |   +-org.xmlunit:xmlunit-core:2.5.1\n[info]   | | |\n[info]   | | +-com.softwaremill.sttp:async-http-client-backend-future_2.11:1.2.3 [S]\n[info]   | | | +-com.softwaremill.sttp:async-http-client-backend_2.11:1.2.3 [S]\n[info]   | | | | +-com.softwaremill.sttp:core_2.11:1.2.3 [S]\n[info]   | | | | +-org.asynchttpclient:async-http-client:2.5.2\n[info]   | | | |   +-com.sun.activation:javax.activation:1.2.0\n[info]   | | | |   +-com.typesafe.netty:netty-reactive-streams:2.0.0\n[info]   | | | |   | +-io.netty:netty-handler:4.1.13.Final (evicted by: 4.1.27.Final)\n[info]   | | | |   | +-io.netty:netty-handler:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | | | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | | |\n[info]   | | | |   | | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   | | | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | | |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | | |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | | |   |\n[info]   | | | |   | | |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | | |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | | |\n[info]   | | | |   | | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |   |\n[info]   | | | |   | |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-org.reactivestreams:reactive-streams:1.0.0 (evicted by: 1.0.2)\n[info]   | | | |   | +-org.reactivestreams:reactive-streams:1.0.2\n[info]   | | | |   |\n[info]   | | | |   +-io.netty:netty-codec-http:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   |   +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   |     +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   |     | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |     |\n[info]   | | | |   |     +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   |       +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |\n[info]   | | | |   +-io.netty:netty-codec-socks:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   |   +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   |     +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   |     | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |     |\n[info]   | | | |   |     +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   |       +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |\n[info]   | | | |   +-io.netty:netty-handler-proxy:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-codec-http:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   | |   +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | |     +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | |     | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |     |\n[info]   | | | |   | |     +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | |       +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-codec-socks:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   | |   +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | |     +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | |     | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |     |\n[info]   | | | |   | |     +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | |       +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |   |\n[info]   | | | |   |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |\n[info]   | | | |   +-io.netty:netty-handler:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |   |\n[info]   | | | |   | |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |   |\n[info]   | | | |   |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |\n[info]   | | | |   +-io.netty:netty-resolver-dns:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-codec-dns:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-codec:4.1.27.Final\n[info]   | | | |   | |   +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | |     +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | |     | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |     |\n[info]   | | | |   | |     +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | |       +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |   |\n[info]   | | | |   |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |\n[info]   | | | |   +-io.netty:netty-transport-native-epoll:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | +-io.netty:netty-transport-native-unix-common:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   | |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |   |\n[info]   | | | |   | |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   | |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-io.netty:netty-transport:4.1.27.Final\n[info]   | | | |   |   +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   |   | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |   |\n[info]   | | | |   |   +-io.netty:netty-resolver:4.1.27.Final\n[info]   | | | |   |     +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   |\n[info]   | | | |   +-org.asynchttpclient:async-http-client-netty-utils:2.5.2\n[info]   | | | |   | +-com.sun.activation:javax.activation:1.2.0\n[info]   | | | |   | +-io.netty:netty-buffer:4.1.27.Final\n[info]   | | | |   | | +-io.netty:netty-common:4.1.27.Final\n[info]   | | | |   | |\n[info]   | | | |   | +-org.slf4j:slf4j-api:1.7.25\n[info]   | | | |   |\n[info]   | | | |   +-org.reactivestreams:reactive-streams:1.0.2\n[info]   | | | |   +-org.slf4j:slf4j-api:1.7.25\n[info]   | | | |\n[info]   | | | +-com.softwaremill.sttp:core_2.11:1.2.3 [S]\n[info]   | | |\n[info]   | | +-com.softwaremill.sttp:circe_2.11:1.2.3 [S]\n[info]   | | | +-com.softwaremill.sttp:core_2.11:1.2.3 [S]\n[info]   | | | +-io.circe:circe-core_2.11:0.11.0 [S]\n[info]   | | | | +-io.circe:circe-numbers_2.11:0.11.0 [S]\n[info]   | | | | +-org.typelevel:cats-core_2.11:1.5.0 [S]\n[info]   | | | |   +-org.typelevel:cats-kernel_2.11:1.5.0 [S]\n[info]   | | | |   +-org.typelevel:cats-macros_2.11:1.5.0 [S]\n[info]   | | | |   | +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | | |   |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | | |   |\n[info]   | | | |   +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | | |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | | |\n[info]   | | | +-io.circe:circe-core_2.11:0.9.3 (evicted by: 0.11.0)\n[info]   | | | +-io.circe:circe-parser_2.11:0.9.3 [S]\n[info]   | | |   +-io.circe:circe-core_2.11:0.11.0 [S]\n[info]   | | |   | +-io.circe:circe-numbers_2.11:0.11.0 [S]\n[info]   | | |   | +-org.typelevel:cats-core_2.11:1.5.0 [S]\n[info]   | | |   |   +-org.typelevel:cats-kernel_2.11:1.5.0 [S]\n[info]   | | |   |   +-org.typelevel:cats-macros_2.11:1.5.0 [S]\n[info]   | | |   |   | +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |   |   |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |   |   |\n[info]   | | |   |   +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |   |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |   |\n[info]   | | |   +-io.circe:circe-core_2.11:0.9.3 (evicted by: 0.11.0)\n[info]   | | |   +-io.circe:circe-jawn_2.11:0.9.3 [S]\n[info]   | | |     +-io.circe:circe-core_2.11:0.11.0 [S]\n[info]   | | |     | +-io.circe:circe-numbers_2.11:0.11.0 [S]\n[info]   | | |     | +-org.typelevel:cats-core_2.11:1.5.0 [S]\n[info]   | | |     |   +-org.typelevel:cats-kernel_2.11:1.5.0 [S]\n[info]   | | |     |   +-org.typelevel:cats-macros_2.11:1.5.0 [S]\n[info]   | | |     |   | +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |     |   |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |     |   |\n[info]   | | |     |   +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |     |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |     |\n[info]   | | |     +-io.circe:circe-core_2.11:0.9.3 (evicted by: 0.11.0)\n[info]   | | |     +-org.spire-math:jawn-parser_2.11:0.11.1 [S]\n[info]   | | |\n[info]   | | +-com.softwaremill.sttp:core_2.11:1.2.3 [S]\n[info]   | | +-io.circe:circe-core_2.11:0.11.0 [S]\n[info]   | | | +-io.circe:circe-numbers_2.11:0.11.0 [S]\n[info]   | | | +-org.typelevel:cats-core_2.11:1.5.0 [S]\n[info]   | | |   +-org.typelevel:cats-kernel_2.11:1.5.0 [S]\n[info]   | | |   +-org.typelevel:cats-macros_2.11:1.5.0 [S]\n[info]   | | |   | +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |   |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |   |\n[info]   | | |   +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-io.circe:circe-generic-extras_2.11:0.11.0 [S]\n[info]   | | | +-io.circe:circe-generic_2.11:0.11.0 [S]\n[info]   | | |   +-com.chuusai:shapeless_2.11:2.3.3 [S]\n[info]   | | |   | +-org.typelevel:macro-compat_2.11:1.1.1 [S]\n[info]   | | |   |\n[info]   | | |   +-io.circe:circe-core_2.11:0.11.0 [S]\n[info]   | | |     +-io.circe:circe-numbers_2.11:0.11.0 [S]\n[info]   | | |     +-org.typelevel:cats-core_2.11:1.5.0 [S]\n[info]   | | |       +-org.typelevel:cats-kernel_2.11:1.5.0 [S]\n[info]   | | |       +-org.typelevel:cats-macros_2.11:1.5.0 [S]\n[info]   | | |       | +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |       |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |       |\n[info]   | | |       +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | | |         +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-io.circe:circe-generic_2.11:0.11.0 [S]\n[info]   | |   +-com.chuusai:shapeless_2.11:2.3.3 [S]\n[info]   | |   | +-org.typelevel:macro-compat_2.11:1.1.1 [S]\n[info]   | |   |\n[info]   | |   +-io.circe:circe-core_2.11:0.11.0 [S]\n[info]   | |     +-io.circe:circe-numbers_2.11:0.11.0 [S]\n[info]   | |     +-org.typelevel:cats-core_2.11:1.5.0 [S]\n[info]   | |       +-org.typelevel:cats-kernel_2.11:1.5.0 [S]\n[info]   | |       +-org.typelevel:cats-macros_2.11:1.5.0 [S]\n[info]   | |       | +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | |       |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | |       |\n[info]   | |       +-org.typelevel:machinist_2.11:0.6.6 [S]\n[info]   | |         +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | |\n[info]   | +-com.snowplowanalytics:collector-payload-1:0.0.0 [S]\n[info]   | | +-org.apache.thrift:libthrift:0.9.1\n[info]   | |   +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | |   +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | |   +-org.apache.commons:commons-lang3:3.7\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.2.5 (evicted by: 4.5.5)\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | |   | +-commons-codec:commons-codec:1.11\n[info]   | |   | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | |   | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | |   | +-commons-logging:commons-logging:1.2\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   |\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | |   | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | |   | +-commons-codec:commons-codec:1.11\n[info]   | |   | +-commons-logging:commons-logging:1.2\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   |\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.2.4 (evicted by: 4.4.9)\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | |   +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-com.snowplowanalytics:iab-spiders-and-robots-client:0.1.0\n[info]   | | +-commons-io:commons-io:2.5\n[info]   | | +-commons-net:commons-net:3.6\n[info]   | | +-org.apache.commons:commons-csv:1.4\n[info]   | | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.7\n[info]   | |\n[info]   | +-com.snowplowanalytics:iglu-scala-client_2.11:0.5.0 [S]\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.8.11.2)\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | |\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | |\n[info]   | | +-com.github.fge:json-schema-validator:2.2.3\n[info]   | | | +-com.github.fge:json-schema-core:1.2.1\n[info]   | | | | +-com.github.fge:jackson-coreutils:1.6\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-databind:2.2.3 (evicted by: 2.8.11.2)\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | | |\n[info]   | | | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | | | |\n[info]   | | | | | +-com.github.fge:msg-simple:1.1\n[info]   | | | | | | +-com.github.fge:btf:1.2\n[info]   | | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | | |\n[info]   | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | |\n[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | | | | | +-com.google.guava:guava:20.0\n[info]   | | | | |\n[info]   | | | | +-com.github.fge:uri-template:0.9\n[info]   | | | | | +-com.github.fge:msg-simple:1.1\n[info]   | | | | | | +-com.github.fge:btf:1.2\n[info]   | | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | | |\n[info]   | | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | |\n[info]   | | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | | | | | +-com.google.guava:guava:20.0\n[info]   | | | | |\n[info]   | | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | | +-org.mozilla:rhino:1.7R4\n[info]   | | | |\n[info]   | | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | | | +-com.googlecode.libphonenumber:libphonenumber:6.0\n[info]   | | | +-javax.mail:mailapi:1.4.3\n[info]   | | | | +-javax.activation:activation:1.1\n[info]   | | | |\n[info]   | | | +-joda-time:joda-time:2.3 (evicted by: 2.9.9)\n[info]   | | | +-joda-time:joda-time:2.9.9\n[info]   | | | +-net.sf.jopt-simple:jopt-simple:4.6 (evicted by: 5.0.3)\n[info]   | | | +-net.sf.jopt-simple:jopt-simple:5.0.3\n[info]   | | |\n[info]   | | +-com.twitter:util-collection_2.11:18.2.0 [S]\n[info]   | | | +-com.twitter:util-core_2.11:18.2.0 [S]\n[info]   | | |   +-com.twitter:util-function_2.11:18.2.0 [S]\n[info]   | | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-com.twitter:util-collection_2.11:6.20.0 (evicted by: 18.2.0)\n[info]   | | +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.7\n[info]   | | +-org.json4s:json4s-jackson_2.11:3.2.11 [S]\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.3.1 (evicted by: 2.8.11.2)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | |\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | |\n[info]   | | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | | |   +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | | |   +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | | |   +-org.scala-lang:scalap:2.11.11\n[info]   | | |     +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | | |       +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |       +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | |       +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-org.json4s:json4s-scalaz_2.11:3.2.11 [S]\n[info]   | | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | | | | +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | | | | +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | | | | +-org.scala-lang:scalap:2.11.11\n[info]   | | | |   +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | | | |     +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | | |     +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | | |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | | |\n[info]   | | | +-org.scalaz:scalaz-core_2.11:7.0.6 (evicted by: 7.0.9)\n[info]   | | | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   | | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | |\n[info]   | | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | |\n[info]   | +-com.snowplowanalytics:referer-parser_2.11:0.3.0 [S]\n[info]   | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.yaml:snakeyaml:1.10 (evicted by: 1.20)\n[info]   | | +-org.yaml:snakeyaml:1.20\n[info]   | |\n[info]   | +-com.snowplowanalytics:scala-forex_2.11:0.5.0 [S]\n[info]   | | +-com.twitter:util-collection_2.11:18.2.0 [S]\n[info]   | | | +-com.twitter:util-core_2.11:18.2.0 [S]\n[info]   | | |   +-com.twitter:util-function_2.11:18.2.0 [S]\n[info]   | | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-com.twitter:util-collection_2.11:6.34.0 (evicted by: 18.2.0)\n[info]   | | +-joda-time:joda-time:2.3 (evicted by: 2.9.9)\n[info]   | | +-joda-time:joda-time:2.9.9\n[info]   | | +-org.codehaus.jackson:jackson-mapper-asl:1.9.7\n[info]   | | | +-org.codehaus.jackson:jackson-core-asl:1.9.7\n[info]   | | |\n[info]   | | +-org.joda:joda-convert:1.2 (evicted by: 1.8.2)\n[info]   | | +-org.joda:joda-convert:1.8.2\n[info]   | | +-org.joda:joda-money:0.9\n[info]   | |\n[info]   | +-com.snowplowanalytics:scala-maxmind-iplookups_2.11:0.4.0 [S]\n[info]   | | +-com.twitter:util-collection_2.11:18.2.0 [S]\n[info]   | | | +-com.twitter:util-core_2.11:18.2.0 [S]\n[info]   | | |   +-com.twitter:util-function_2.11:18.2.0 [S]\n[info]   | | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | |\n[info]   | +-com.snowplowanalytics:scala-weather_2.11:0.3.0 [S]\n[info]   | | +-com.twitter:util-collection_2.11:18.2.0 [S]\n[info]   | | | +-com.twitter:util-core_2.11:18.2.0 [S]\n[info]   | | |   +-com.twitter:util-function_2.11:18.2.0 [S]\n[info]   | | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |   +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-com.twitter:util-collection_2.11:6.39.0 (evicted by: 18.2.0)\n[info]   | | +-joda-time:joda-time:2.9.9\n[info]   | | +-org.joda:joda-convert:1.8.2\n[info]   | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | | | +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | | | +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | | | +-org.scala-lang:scalap:2.11.11\n[info]   | | |   +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | | |     +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |     +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-org.json4s:json4s-jackson_2.11:3.2.11 [S]\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.3.1 (evicted by: 2.8.11.2)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | |\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | |\n[info]   | | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | | |   +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | | |   +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | | |   +-org.scala-lang:scalap:2.11.11\n[info]   | | |     +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | | |       +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |       +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | |       +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-org.scalaj:scalaj-http_2.11:2.3.0 [S]\n[info]   | |\n[info]   | +-com.snowplowanalytics:schema-sniffer-1:0.0.0 [S]\n[info]   | | +-org.apache.thrift:libthrift:0.9.1\n[info]   | |   +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | |   +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | |   +-org.apache.commons:commons-lang3:3.7\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.2.5 (evicted by: 4.5.5)\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | |   | +-commons-codec:commons-codec:1.11\n[info]   | |   | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | |   | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | |   | +-commons-logging:commons-logging:1.2\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   |\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | |   | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | |   | +-commons-codec:commons-codec:1.11\n[info]   | |   | +-commons-logging:commons-logging:1.2\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   |\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.2.4 (evicted by: 4.4.9)\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | |   +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-com.snowplowanalytics:snowplow-thrift-raw-event:0.1.0 [S]\n[info]   | | +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.7\n[info]   | | +-org.apache.thrift:libthrift:0.9.0 (evicted by: 0.9.1)\n[info]   | | | +-commons-lang:commons-lang:2.4\n[info]   | | | +-commons-lang:commons-lang:2.5 (evicted by: 2.4)\n[info]   | | | +-org.apache.httpcomponents:httpclient:4.1.3 (evicted by: 4.5.5)\n[info]   | | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | | +-commons-codec:commons-codec:1.11\n[info]   | | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | | +-commons-logging:commons-logging:1.2\n[info]   | | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | | |\n[info]   | | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | | +-commons-codec:commons-codec:1.11\n[info]   | | | | +-commons-logging:commons-logging:1.2\n[info]   | | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | | |\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.1.3 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | | +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | | |\n[info]   | | +-org.apache.thrift:libthrift:0.9.1\n[info]   | |   +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | |   +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | |   +-org.apache.commons:commons-lang3:3.7\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.2.5 (evicted by: 4.5.5)\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | |   | +-commons-codec:commons-codec:1.11\n[info]   | |   | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | |   | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | |   | +-commons-logging:commons-logging:1.2\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   |\n[info]   | |   +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | |   | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | |   | +-commons-codec:commons-codec:1.11\n[info]   | |   | +-commons-logging:commons-logging:1.2\n[info]   | |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   |\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.2.4 (evicted by: 4.4.9)\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | |   +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |   +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | |   +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-commons-codec:commons-codec:1.11\n[info]   | +-commons-io:commons-io:2.4 (evicted by: 2.5)\n[info]   | +-commons-io:commons-io:2.5\n[info]   | +-eu.bitwalker:UserAgentUtils:1.21\n[info]   | +-io.gatling:jsonpath_2.11:0.6.4 [S]\n[info]   | | +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.3 (evicted by: 1.0.4)\n[info]   | | +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |\n[info]   | +-io.lemonlabs:scala-uri_2.11:0.5.0 [S]\n[info]   | | +-com.chuusai:shapeless_2.11:2.3.2 [S] (evicted by: 2.3.3)\n[info]   | | | +-org.typelevel:macro-compat_2.11:1.1.1 [S]\n[info]   | | |\n[info]   | | +-com.chuusai:shapeless_2.11:2.3.3 [S]\n[info]   | | | +-org.typelevel:macro-compat_2.11:1.1.1 [S]\n[info]   | | |\n[info]   | | +-io.spray:spray-json_2.11:1.3.2 [S]\n[info]   | | +-org.parboiled:parboiled_2.11:2.1.4 [S]\n[info]   | |   +-com.chuusai:shapeless_2.11:2.3.2 [S] (evicted by: 2.3.3)\n[info]   | |   | +-org.typelevel:macro-compat_2.11:1.1.1 [S]\n[info]   | |   |\n[info]   | |   +-com.chuusai:shapeless_2.11:2.3.3 [S]\n[info]   | |     +-org.typelevel:macro-compat_2.11:1.1.1 [S]\n[info]   | |\n[info]   | +-joda-time:joda-time:2.2 (evicted by: 2.9.9)\n[info]   | +-joda-time:joda-time:2.3 (evicted by: 2.9.9)\n[info]   | +-joda-time:joda-time:2.9.9\n[info]   | +-mysql:mysql-connector-java:5.1.39\n[info]   | +-org.apache.commons:commons-lang3:3.4 (evicted by: 3.7)\n[info]   | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | +-org.apache.commons:commons-lang3:3.7\n[info]   | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | +-commons-codec:commons-codec:1.11\n[info]   | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | +-commons-logging:commons-logging:1.2\n[info]   | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |\n[info]   | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | +-commons-codec:commons-codec:1.11\n[info]   | | +-commons-logging:commons-logging:1.2\n[info]   | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | |\n[info]   | +-org.apache.maven:maven-artifact:3.2.2\n[info]   | | +-org.codehaus.plexus:plexus-utils:3.0.17\n[info]   | |\n[info]   | +-org.joda:joda-convert:1.2 (evicted by: 1.8.2)\n[info]   | +-org.joda:joda-convert:1.8.2\n[info]   | +-org.json4s:json4s-jackson_2.11:3.2.11 [S]\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.3.1 (evicted by: 2.8.11.2)\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.6.4 (evicted by: 2.8.11.2)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.6.0 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.6.4 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | |\n[info]   | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | |\n[info]   | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | |   +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | |   +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | |   +-org.scala-lang:scalap:2.11.11\n[info]   | |     +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | |       +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |       +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | |       +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | |\n[info]   | +-org.json4s:json4s-scalaz_2.11:3.2.11 [S]\n[info]   | | +-org.json4s:json4s-core_2.11:3.2.11 [S]\n[info]   | | | +-com.thoughtworks.paranamer:paranamer:2.6\n[info]   | | | +-org.json4s:json4s-ast_2.11:3.2.11 [S]\n[info]   | | | +-org.scala-lang:scalap:2.11.11\n[info]   | | |   +-org.scala-lang:scala-compiler:2.11.11 [S]\n[info]   | | |     +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | | |     +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | | |     +-org.scala-lang:scala-reflect:2.11.11 [S]\n[info]   | | |\n[info]   | | +-org.scalaz:scalaz-core_2.11:7.0.6 (evicted by: 7.0.9)\n[info]   | | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   | |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   | |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   | |\n[info]   | +-org.postgresql:postgresql:9.4.1208.jre7\n[info]   | +-org.scalaj:scalaj-http_2.11:2.3.0 [S]\n[info]   | +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]   |   +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]   |   +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]   |\n[info]   +-com.snowplowanalytics:snowplow-thrift-raw-event:0.1.0 [S]\n[info]   | +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | +-org.apache.commons:commons-lang3:3.7\n[info]   | +-org.apache.thrift:libthrift:0.9.0 (evicted by: 0.9.1)\n[info]   | | +-commons-lang:commons-lang:2.4\n[info]   | | +-commons-lang:commons-lang:2.5 (evicted by: 2.4)\n[info]   | | +-org.apache.httpcomponents:httpclient:4.1.3 (evicted by: 4.5.5)\n[info]   | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpcore:4.1.3 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-org.apache.thrift:libthrift:0.9.1\n[info]   |   +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   |   +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   |   +-org.apache.commons:commons-lang3:3.7\n[info]   |   +-org.apache.httpcomponents:httpclient:4.2.5 (evicted by: 4.5.5)\n[info]   |   +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   |   | +-commons-codec:commons-codec:1.11\n[info]   |   | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   |   | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   |   | +-commons-logging:commons-logging:1.2\n[info]   |   | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   |   |\n[info]   |   +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   |   | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   |   | +-commons-codec:commons-codec:1.11\n[info]   |   | +-commons-logging:commons-logging:1.2\n[info]   |   | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   |   |\n[info]   |   +-org.apache.httpcomponents:httpcore:4.2.4 (evicted by: 4.4.9)\n[info]   |   +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   |   +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   |   +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   |   +-org.slf4j:slf4j-api:1.7.25\n[info]   |\n[info]   +-com.twitter.elephantbird:elephant-bird-core:4.14\n[info]   | +-com.google.guava:guava:11.0.1 (evicted by: 20.0)\n[info]   | | +-com.google.code.findbugs:jsr305:1.3.9 (evicted by: 2.0.1)\n[info]   | | +-com.google.code.findbugs:jsr305:2.0.1\n[info]   | |\n[info]   | +-com.google.guava:guava:16.0.1 (evicted by: 20.0)\n[info]   | +-com.google.guava:guava:20.0\n[info]   | +-com.google.protobuf:protobuf-java:2.4.1\n[info]   | +-com.googlecode.json-simple:json-simple:1.1\n[info]   | +-com.hadoop.gplcompression:hadoop-lzo:0.4.19 (evicted by: 0.4.20)\n[info]   | +-com.hadoop.gplcompression:hadoop-lzo:0.4.20\n[info]   | | +-commons-logging:commons-logging:1.1.1 (evicted by: 1.2)\n[info]   | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | +-commons-logging:commons-logging:1.2\n[info]   | |\n[info]   | +-com.twitter.elephantbird:elephant-bird-hadoop-compat:4.14\n[info]   | +-commons-lang:commons-lang:2.4\n[info]   | +-org.apache.thrift:libthrift:0.7.0 (evicted by: 0.9.1)\n[info]   | | +-commons-lang:commons-lang:2.4\n[info]   | | +-commons-lang:commons-lang:2.5 (evicted by: 2.4)\n[info]   | | +-javax.servlet:servlet-api:2.5\n[info]   | | +-org.apache.httpcomponents:httpclient:4.0.1 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.3 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.1 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.0.1 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | | +-org.slf4j:slf4j-api:1.6.4 (evicted by: 1.7.25)\n[info]   | | +-org.slf4j:slf4j-api:1.7.12 (evicted by: 1.7.25)\n[info]   | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-org.apache.thrift:libthrift:0.9.0 (evicted by: 0.9.1)\n[info]   | | +-commons-lang:commons-lang:2.4\n[info]   | | +-commons-lang:commons-lang:2.5 (evicted by: 2.4)\n[info]   | | +-org.apache.httpcomponents:httpclient:4.1.3 (evicted by: 4.5.5)\n[info]   | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpcore:4.1.3 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-org.apache.thrift:libthrift:0.9.1\n[info]   | | +-org.apache.commons:commons-lang3:3.1 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.5 (evicted by: 3.7)\n[info]   | | +-org.apache.commons:commons-lang3:3.7\n[info]   | | +-org.apache.httpcomponents:httpclient:4.2.5 (evicted by: 4.5.5)\n[info]   | | +-org.apache.httpcomponents:httpclient:4.3.3 (evicted by: 4.5.5)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-codec:commons-codec:1.6 (evicted by: 1.11)\n[info]   | | | +-commons-logging:commons-logging:1.1.3 (evicted by: 1.2)\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpclient:4.5.5\n[info]   | | | +-commons-codec:commons-codec:1.10 (evicted by: 1.11)\n[info]   | | | +-commons-codec:commons-codec:1.11\n[info]   | | | +-commons-logging:commons-logging:1.2\n[info]   | | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | |\n[info]   | | +-org.apache.httpcomponents:httpcore:4.2.4 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.3.2 (evicted by: 4.4.9)\n[info]   | | +-org.apache.httpcomponents:httpcore:4.4.9\n[info]   | | +-org.slf4j:slf4j-api:1.5.8 (evicted by: 1.7.25)\n[info]   | | +-org.slf4j:slf4j-api:1.7.25\n[info]   | |\n[info]   | +-org.slf4j:slf4j-api:1.6.4 (evicted by: 1.7.25)\n[info]   | +-org.slf4j:slf4j-api:1.7.12 (evicted by: 1.7.25)\n[info]   | +-org.slf4j:slf4j-api:1.7.25\n[info]   |\n[info]   +-org.scalaz:scalaz-core_2.11:7.0.9 [S]\n[info]     +-org.scala-lang.modules:scala-parser-combinators_2.11:1.0.4 [S]\n[info]     +-org.scala-lang.modules:scala-xml_2.11:1.0.5 [S]\n[info]\n[success] Total time: 1 s, completed Mar 14, 2019 4:06:26 PM. Uuups good catch. Let me investigate how that test dependency sneaked into the published artifact. Long Story Short, when packaging SBT treats IntegrationTest dependencies as regular dependencies contrary to what it does with Test dependencies that don't get packaged. Locally sbt dependencyTree was correctly not reporting that dependency but it had sneaked in.\nHopefully there aren't any more surprises.. @benjben  thanks for pointing out. Now it is in master https://github.com/OpenCageData/scala-opencage-geocoder/commit/c85988860ec3ba8b8046d266e500bc8b9511a71d. Tests running locally in SBT hung in my computer. The only thing that make them pass is to run them sequentially. Maybe you don't see it because Travis is too slow and the race condition doesn't manifest. I will remove it from the PR anyway.. Good catch!!. I was following guidelines from WeatherEnrichment were only valid calls to the API are being implemented in the tests.\nI've added testing for roundCoordinate.. ",
    "benjben": "Thanks for integrating the changes @tonicebrian !\nThat looks good to me. However, the unit tests for spark-enrich fail, the spark job can't initialize, with the error:\n[error] Caused by: com.fasterxml.jackson.databind.JsonMappingException: Incompatible Jackson version: 2.8.11-2\nOn your laptop, if you run sbt publishLocal for scala-common-enrich and then run the unit tests for spark-enrich, does it work?. Hi @tonicebrian ,\nCan you please copy/paste the output of sbt dependencyTree here, so that anyone that would jump in this PR can also see?\nThe unit tests of spark-enrich are working without OpenCage integration in Scala common enrich.. [info]   | +-com.opencagedata:scala-opencage-geocoder_2.11:1.1.0 [S]\n[info]   | | +-com.github.tomakehurst:wiremock:2.20.0\n[info]   | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\n[info]   | | | +-com.fasterxml.jackson.core:jackson-databind:2.8.11.2\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.0 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-annotations:2.8.11\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.10 (evicted by: 2.8.11)\n[info]   | | | | +-com.fasterxml.jackson.core:jackson-core:2.8.11\nOpenCage does bring a version of jackson.. LGTM.\n@tonicebrian you pushed v1.1.1 from your laptop? I can't see it on Github.. The original plan was to use only scala-uri for parsing (in a very relaxed way) and not java.net.URI.create() any more.\nAlthough this was working well, a consequence was that some URIs were encoded differently as before, thus breaking unit tests in jobs using SCE.\nIt's possible to fix the unit tests but they probably don't cover all the cases that we have in our customers' pipelines and updating the output of a function that is used for every single event in all the pipeline is risky.\nSo it was decided to continue to use Java parsing first and in case of failure a very relaxed Scala parsing (no percent decoding).. What goes wrong when the tests run in parallel?. Don't we want the function never to throw an exception, but instead return the error message is something went wrong?. I hesitated about it, but I think that it should be left as a possibility to do a strict URI parsing. But we can have a relaxed URI parsing by default and make it strict optionally, instead of the other way around. WDYT?\nFor example see this. Actually, the choice of relaxed parsing or not would still need to go all the way from the configuration to the function:\n$ grep -R stringToUri\nenrichments/EnrichmentManager.scala:    val refererUri = CU.stringToUri(event.page_referrer)\nenrichments/registry/IpLookupsEnrichment.scala:      .stringToUri(uri + \"/\" + database)\nenrichments/registry/IabEnrichment.scala:      .stringToUri(uri + (if (uri.endsWith(\"/\")) \"\" else \"/\") + database)\nenrichments/registry/UaParserEnrichment.scala:      .stringToUri(uri + (if (uri.endsWith(\"/\")) \"\" else \"/\") + database)\nenrichments/web/PageEnrichments.scala:      case (Some(r), None)    => CU.stringToUri(r)\nenrichments/web/PageEnrichments.scala:      case (None, Some(t))    => CU.stringToUri(t)\nenrichments/web/PageEnrichments.scala:      case (Some(r), Some(t)) => CU.stringToUri(t) // Tracker URL takes precedence\nutils/ConversionUtils.scala:  def stringToUri(uri: String, relaxed: Boolean = true): Validation[String, Option[URI]] =\nAlthough it's technically possible, I'm not sure that this is worth it to make it happen. Can you think of any case where we wouldn't want a relaxed URI parsing? If not I think that we should just always use relaxed parsing, as you suggested.. WeatherEnrichment => update copy/paste.. Same.. Would be nice to add a bit more details. For instance OpenCage API makes it possible to convert coordinates to and from places (e.g. 51.4266,-0.0798 -> Shoreditch, London). Please separate imports.. Same.. Update copy/paste.. scale should be passed as a parameter.. No need to create response.. You never populate the cache, you only read from it.. 2018 -> 2019.. Would be nice to add the input latitude and longitude in the error message (if opencage doesn't already do it).. weather. Could be lazy as well.. request -> response.. `com.opencagedata.geocoder.OpenCageResponse`  -> [[OpenCageResponse]]. At this stage the request to OpenCage has already been done, getGeocodeFromResponse would be more clear.. geocode2JValue -> geocodeToJValue.. By mocking OpenCageClient.reverseGeocode() you could test most of your code without even querying OpenCage.\nOpenCage also provides API keys that you can use for testing, see here.\nroundCoordinate should be unit tested.. Neat! But are you referring to\n1) Scala 2.12's Try.toEither ?\n2) cats's Either.catchNonFatal ? (scalaz used here)\nOk I will remove java URI parsing then. (I also created this issue regarding the fact that scala-uri re-encodes already correctly percent-encoded URIs.). I can't have something as neat because:\n1) null url needs to be taken care of before parseTry\n2) a String is returned, and not a Throwable. "
}